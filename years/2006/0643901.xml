<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Breaking the phonetic code: novel acoustic-lexical modeling techniques for robust automatic speech recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/15/2006</AwardEffectiveDate>
<AwardExpirationDate>11/30/2012</AwardExpirationDate>
<AwardTotalIntnAmount>502952.00</AwardTotalIntnAmount>
<AwardAmount>502952</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Spontaneous speech, accented speech, and speech in noise continue to &lt;br/&gt;provide automatic speech recognition (ASR) technology with significant &lt;br/&gt;challenges; error rates of ASR systems are still unacceptably high for &lt;br/&gt;these types of speech.  This project establishes a consistent &lt;br/&gt;framework that seeks to cope with all of these conditions. The novel &lt;br/&gt;approach to phonetic variability investigated here views the problem &lt;br/&gt;as one of phonetic information underspecification: some subset of &lt;br/&gt;information that the listener receives will be missing or uncertain. &lt;br/&gt;Lexical access is thus a phonetic code-breaking problem --- how can a &lt;br/&gt;system accumulate phonetic cues in each of these conditions to &lt;br/&gt;recognize words on the basis of incomplete evidence? &lt;br/&gt; &lt;br/&gt;The research program of this project takes a multidisciplinary &lt;br/&gt;approach to integrating linguistic theory with speech recognition &lt;br/&gt;technology; discriminative statistical models of linguistic features &lt;br/&gt;are employed to model nonlinear, overlapping phonological effects &lt;br/&gt;observed in speech.  The framework allows derivation of new linguistic &lt;br/&gt;insights through analysis of trained systems. &lt;br/&gt; &lt;br/&gt;The educational program fosters interdisciplinary research (with &lt;br/&gt;cross-disciplinary graduate seminars) and increases participation of &lt;br/&gt;underrepresented students in Computer Science by introducing language &lt;br/&gt;technology topics early into the undergraduate curriculum and &lt;br/&gt;encouraging undergraduate research. &lt;br/&gt; &lt;br/&gt;Apart from cultivating a new way of thinking about pronunciation &lt;br/&gt;variation for ASR, the broader impacts of this research are to provide &lt;br/&gt;collaborative resources for the ASR and linguistics communities to &lt;br/&gt;discuss in tutorial and workshop settings.  Addressing noise, accent, &lt;br/&gt;and speaking style in a consistent framework will also improve ASR &lt;br/&gt;technology for many who are underserved by current systems.</AbstractNarration>
<MinAmdLetterDate>12/06/2006</MinAmdLetterDate>
<MaxAmdLetterDate>12/03/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0643901</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Fosler-Lussier</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric Fosler-Lussier</PI_FULL_NAME>
<EmailAddress>fosler@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142924890</PI_PHON>
<NSF_ID>000182577</NSF_ID>
<StartDate>12/06/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University Research Foundation -DO NOT USE</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888734</PhoneNumber>
<StreetAddress>1960 KENNY RD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>071650709</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY RESEARCH FOUNDATION, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName>Columbus</CityName>
<StateCode>OH</StateCode>
<ZipCode>432101016</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~98316</FUND_OBLG>
<FUND_OBLG>2008~95111</FUND_OBLG>
<FUND_OBLG>2009~99076</FUND_OBLG>
<FUND_OBLG>2010~103218</FUND_OBLG>
<FUND_OBLG>2011~107231</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main scientific premise of this project is that the way that we perceive speech in the face of noise and varying accents can bethought of as problem of breaking a phonetic code: humans perceive incomplete evidence that they are able to reassemble into messages. Computer models of speech for the process of getting computers to recognize what was said (the Automatic Speech Recognition problem) could be improved by including evidence combination techniques (a form of machine learning). &nbsp;Moreover, the ability to think about these machine learning techniques in an interdisciplinary way (combining insights from linguistics and computer science) can lead to new ways to think about general problems in linguistics.</p> <p><br />The main outcomes of the project included two general findings (summarized from roughly 25 publications):</p> <p><span>First, statistical methods called Conditional Random Fields (<span>CRFs</span></span>) that are relatively new to the Automatic Speech Recognition field have been shown to be effective combiners of linguistic information. &nbsp;For example, one view of speech sounds represents the sound patterns as whole blocks in time (known as phones); these are the traditional building blocks of <span><span>ASR</span></span> systems. &nbsp;However, these sounds can be broken into "phonological feature" categories -- a multi-dimensional representation of speech sounds. &nbsp;<span><span>CRFs</span></span><span> are shown to be a much more effective combination method of these different representations of speech than the tradition Hidden Markov Model (<span>HMM</span></span>), and can decrease the errors made by a system much more than either representation alone. &nbsp;Our explorations examined how we can think about feature combinations both within short, local windows of speech, or over longer timescales.</p> <p><br />A second outcome was a new method of thinking about how phonetic information is impacted by noise. &nbsp;When the human ear hears speech and noise together, some frequencies of the speech are blocked by the noise, in a process called masking; this is similar to the visual phenomenon of distant objects being partially obscured by closer objects in the line of sight. &nbsp;Noise severely degrades <span><span>ASR</span></span> performance (i.e., it increases the error). &nbsp;Previous methods tried to estimate what parts of the signal were noise-masked, and reconstruct the underlying speech. &nbsp;However, our research showed that, surprisingly, treating the masked components as completely absent was a better strategy than other reconstruction techniques. &nbsp;It is likely better to focus on mask estimation rather than reconstruction. &nbsp;Tying into the phonetic code aspect of the project, we found that one could improve mask estimation by using information from a speech recognition system, and then using machine learning techniques similar to <span><span>CRFs</span></span> to improve prediction of the mask. &nbsp;Another thread of research showed how <span><span>CRFs</span></span> could be used directly for mask prediction; this overall approach shows how we may be able to think about speech recognition and speech enhancement as complimentary, cooperative processes.</p> <p><br />The educational outcomes of this project included new techniques for teaching advanced machine learning concepts in speech and language technology classes, and helping students in linguistics disciplines&nbsp;utilize machine learning in their own dissertations.</p> <p>In terms of outreach, a tutorial on this material was presented at a major international conference, and three tutorial-style journal articles co-authored by the PI were influenced by this grant. &nbsp;The project fostered interdisciplinary research by being an example project presented at the new Buckeye Language Network, and members of the project participated in the <span><span>Ohi...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main scientific premise of this project is that the way that we perceive speech in the face of noise and varying accents can bethought of as problem of breaking a phonetic code: humans perceive incomplete evidence that they are able to reassemble into messages. Computer models of speech for the process of getting computers to recognize what was said (the Automatic Speech Recognition problem) could be improved by including evidence combination techniques (a form of machine learning).  Moreover, the ability to think about these machine learning techniques in an interdisciplinary way (combining insights from linguistics and computer science) can lead to new ways to think about general problems in linguistics.   The main outcomes of the project included two general findings (summarized from roughly 25 publications):  First, statistical methods called Conditional Random Fields (CRFs) that are relatively new to the Automatic Speech Recognition field have been shown to be effective combiners of linguistic information.  For example, one view of speech sounds represents the sound patterns as whole blocks in time (known as phones); these are the traditional building blocks of ASR systems.  However, these sounds can be broken into "phonological feature" categories -- a multi-dimensional representation of speech sounds.  CRFs are shown to be a much more effective combination method of these different representations of speech than the tradition Hidden Markov Model (HMM), and can decrease the errors made by a system much more than either representation alone.  Our explorations examined how we can think about feature combinations both within short, local windows of speech, or over longer timescales.   A second outcome was a new method of thinking about how phonetic information is impacted by noise.  When the human ear hears speech and noise together, some frequencies of the speech are blocked by the noise, in a process called masking; this is similar to the visual phenomenon of distant objects being partially obscured by closer objects in the line of sight.  Noise severely degrades ASR performance (i.e., it increases the error).  Previous methods tried to estimate what parts of the signal were noise-masked, and reconstruct the underlying speech.  However, our research showed that, surprisingly, treating the masked components as completely absent was a better strategy than other reconstruction techniques.  It is likely better to focus on mask estimation rather than reconstruction.  Tying into the phonetic code aspect of the project, we found that one could improve mask estimation by using information from a speech recognition system, and then using machine learning techniques similar to CRFs to improve prediction of the mask.  Another thread of research showed how CRFs could be used directly for mask prediction; this overall approach shows how we may be able to think about speech recognition and speech enhancement as complimentary, cooperative processes.   The educational outcomes of this project included new techniques for teaching advanced machine learning concepts in speech and language technology classes, and helping students in linguistics disciplines utilize machine learning in their own dissertations.  In terms of outreach, a tutorial on this material was presented at a major international conference, and three tutorial-style journal articles co-authored by the PI were influenced by this grant.  The project fostered interdisciplinary research by being an example project presented at the new Buckeye Language Network, and members of the project participated in the OhioSpeaks workshop.  The PI presented research from this and related projects on Capitol Hill as part of the 2008 Coalition for National Science Funding research day.  The PI also gave a talk on how machine learning can improve autism research at the Central Ohio Autism Society.                Last Modified: 05/11/2013       Submitted by: Eric Fosler-Lussier]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
