<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Algebraic aspects of modern coding theory</AwardTitle>
<AwardEffectiveDate>07/01/2006</AwardEffectiveDate>
<AwardExpirationDate>06/30/2010</AwardExpirationDate>
<AwardTotalIntnAmount>147651.00</AwardTotalIntnAmount>
<AwardAmount>147651</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tomek Bartoszynski</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This project concerns the class of Low Density Parity Check (LDPC)&lt;br/&gt;codes.  LDPC codes come equipped with an iterative message-passing&lt;br/&gt;algorithm which operates on a certain bipartite graph associated to&lt;br/&gt;the code.  The algorithm acts locally on the graph, and this results&lt;br/&gt;in both its greatest strength (low complexity) and its greatest&lt;br/&gt;weakness (nonoptimality).  The PI's focus is on understanding this&lt;br/&gt;weakness.  Because the algorithm acts locally, it cannot distinguish&lt;br/&gt;if it is acting on the graph itself or on some finite unramified cover&lt;br/&gt;of the graph.  This leads to the notion of "pseudo-codewords", which&lt;br/&gt;arise from codewords in codes corresponding to the covers and which&lt;br/&gt;compromise the decoder.  Thus to understand the performance of LDPC&lt;br/&gt;codes, we must understand these pseudo-codewords; most of the problems&lt;br/&gt;the PI considers stem from the desire to understand pseudo-codewords&lt;br/&gt;of LDPC codes.  In previous joint work, the PI has given two&lt;br/&gt;characterizations of the pseudo-codewords of an LDPC code: via the&lt;br/&gt;so-called "fundamental cone" and via the edge zeta function of a&lt;br/&gt;certain graph attached to the code.  While the fundamental cone&lt;br/&gt;characterization is valid for all LDPC codes, the zeta function&lt;br/&gt;characterization is satisfactory only in the special case of cycle&lt;br/&gt;codes.  The PI will further her study of the fundamental cone and the&lt;br/&gt;development of a zeta function characterization for pseudo-codewords&lt;br/&gt;of general LDPC codes.  Additional targets of study are non-binary&lt;br/&gt;LDPC codes and their pseudo-codewords as well as the connections&lt;br/&gt;between LDPC codes and another class of graph-based codes: turbo&lt;br/&gt;codes.&lt;br/&gt;&lt;br/&gt;Whenever information is transmitted across a channel, errors are bound&lt;br/&gt;to occur.  By adding redundancy to the data, many of these errors can&lt;br/&gt;be corrected.  If the information is thought of as strings 0's and 1's&lt;br/&gt;of fixed length, then the codewords are strings of 0's and 1's of&lt;br/&gt;length some longer length, where the difference in lengths represents&lt;br/&gt;the amount of redundancy which was added.  A collection of codewords&lt;br/&gt;is called a code.  A large part of classical coding theory is&lt;br/&gt;concerned with finding the trade-offs between three fundamental&lt;br/&gt;parameters of a code: its length, its number of codewords, and its&lt;br/&gt;minimum Hamming distance, i.e., the minimum number of positions in&lt;br/&gt;which any two distinct codewords differ.  While any code can correct&lt;br/&gt;all errors of weight at most rougly half its minimum distance, most&lt;br/&gt;codes can correct many errors of substantially higher weight.  It is&lt;br/&gt;the goal of modern coding theory to find those representations of&lt;br/&gt;codes that admit decoding algorithms that allow for correction of all&lt;br/&gt;the error patterns that the code can correct --- not only those which&lt;br/&gt;have weight at most roughly half the minimum distance.  One of the&lt;br/&gt;greatest achievements of modern coding theory so far is the discovery&lt;br/&gt;and subsequent development of the class of Low Density Parity Check&lt;br/&gt;(LDPC) codes.  The usefulness of these codes stems from the fact that&lt;br/&gt;they come equipped with a very efficient decoding algorithm which&lt;br/&gt;operates on a certain bipartite graph associated to the code.  The&lt;br/&gt;main goal of this project is to further the understanding of the&lt;br/&gt;theoretical performance of this decoding algorithm, especially through&lt;br/&gt;the study of the so-called "pseudo-codewords" which arise from codes&lt;br/&gt;associated to finite covers of the bipartite graph.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>04/13/2006</MinAmdLetterDate>
<MaxAmdLetterDate>04/13/2006</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0602332</AwardID>
<Investigator>
<FirstName>Judy</FirstName>
<LastName>Walker</LastName>
<EmailAddress>judy.walker@unl.edu</EmailAddress>
<StartDate>04/13/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
</Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>1264</Code>
<Text>ALGEBRA,NUMBER THEORY,AND COM</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
