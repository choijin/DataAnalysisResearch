<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Architectural Techniques and Tools for Adaptive Active Storage Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2007</AwardEffectiveDate>
<AwardExpirationDate>03/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>303493.00</AwardTotalIntnAmount>
<AwardAmount>412000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The ability to embed computation on the I/O path creates exciting possibilities for accelerating a wide spectrum of data intensive applications, ranging from content-based retrieval of images stored on a personal computer to fusion of massive geospatial datasets. The presence of processing power at the storage devices, coupled with the location of these devices in the system, also opens up the opportunity to provide storage-centric computing services that run directly on the I/O path. &lt;br/&gt;However, harnessing a large amount of processing power on the I/O path  at a  small energy cost requires extensive architectural support. This research project develops a storage-centric architecture in which the entire I/O path is treated as a programmable and reconfigurable computational substrate. The research addresses several cross-cutting issues in electro-mechanical design, processor microarchitecture, and parallel programming. An integral component of this project includes the development of simulation tools and a hardware testbed for research and education in the area of adaptive active storage systems.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>04/06/2007</MinAmdLetterDate>
<MaxAmdLetterDate>04/04/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0643925</AwardID>
<Investigator>
<FirstName>Sudhanva</FirstName>
<LastName>Gurumurthi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sudhanva Gurumurthi</PI_FULL_NAME>
<EmailAddress>gurumurthi@cs.virginia.edu</EmailAddress>
<PI_PHON>4349822227</PI_PHON>
<NSF_ID>000245538</NSF_ID>
<StartDate>04/06/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia Main Campus]]></Name>
<CityName>CHARLOTTESVILLE</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044195</ZipCode>
<StreetAddress><![CDATA[P.O.  BOX 400195]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7352</Code>
<Text>COMPUTING PROCESSES &amp; ARTIFACT</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~42132</FUND_OBLG>
<FUND_OBLG>2008~198449</FUND_OBLG>
<FUND_OBLG>2010~100830</FUND_OBLG>
<FUND_OBLG>2011~70589</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We are in the era of data-centric computing. Many applications, such as big-data, high-performance computing (HPC), and social networking generate and process massive amounts of data. The design of the memory and storage system plays a pivotal role in determining the performance and energy usage characteristics for servers and data centers that run such applications. The goal of this NSF CAREER award was to explore the architecture design space of the storage system for such applications. This included extensions to conventional server and storage device architectures, exploring the role of emerging memory technologies, and developing tools that facilitate design-space exploration.</p> <p>The initial thrust of&nbsp;our research effort was to explore&nbsp;new architectures to reduce&nbsp;the cost of data movement between processors and the storage system. The first step was to explore "processing-in-storage" architectures, where data-intense computation is performed in or close to the data storage medium in a disk drive. We found that there is possible to increase the amount of processing power inside a disk drive and still operate within the power budget by trading off energy usage between the spindle and the electronics. To enable even higher performance, we next looked at disk drive architectures that provided parallelism inside the drive. The latter effort also showed promise as a technique to significantly reduce the energy usage of a storage system. We also developed techniques for effective energy reduction of conventional disk-based storage systems.</p> <p>As flash memory based Solid State Drives (SSDs) started becoming more common in the marketplace, the research&nbsp;emphasis shifted from hard drives to&nbsp;SSDs. While flash memory provides low access latency for reads, it still poses write bottlenecks. Moreover, flash memory has a limited lifetime and the memory cells can wear out after repeated writes and erases. We studied&nbsp;the reliability&nbsp;problem and showed that it is possible to achieve orders of magnitude higher endurance than those quoted in manufacturer datasheets by leveraging the self-recovery property inherent to flash. We&nbsp;showed that using a high-endurance non-volatile memory, such as Spin Transfer Torque RAM (STT-RAM), as a write merge-buffer in the SSD can significantly boost both performance and reliability.&nbsp;We also developed tools that allow detailed study of flash memory tradeoffs.</p> <p>We&nbsp;expanded our&nbsp;research from storage&nbsp;to examine&nbsp;non-volatility at other layers of the memory hierarchy. In particular, we looked at the use of STT-RAM as caches and main memory. While STT-RAM allows for high density and has virtually no leakage power (since data is not stored in the form of charges), it is slower than SRAM and the write-energy is high. We showed that it is possible to address these problems by reducing the retention time of the STT-RAM cells. Although reducing the non-volatility can be problematic for long-term storage, caches and main memory house data for much shorter duration of time than storage and therefore such a tradeoff will be acceptable. We also developed other circuit-level techniques to reduce write latency and energy. Finally, we developed a STT-RAM memory design tool.&nbsp;A key&nbsp;broader impact of this research was the development of a tutorial series on non-volatile memory with colleagues at IBM Research. These tutorials were given at major architecture conferences and at the Non-Volatile Memories Workshop in 2011, where there were over 140 registered attendees. We also co-authored a Synthesis Lecture on Computer Architecture book on this topic.</p><br> <p>            Last Modified: 05/09/2013<br>      Modified by: Sudhanva&nbsp;Gurumurthi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We are in the era of data-centric computing. Many applications, such as big-data, high-performance computing (HPC), and social networking generate and process massive amounts of data. The design of the memory and storage system plays a pivotal role in determining the performance and energy usage characteristics for servers and data centers that run such applications. The goal of this NSF CAREER award was to explore the architecture design space of the storage system for such applications. This included extensions to conventional server and storage device architectures, exploring the role of emerging memory technologies, and developing tools that facilitate design-space exploration.  The initial thrust of our research effort was to explore new architectures to reduce the cost of data movement between processors and the storage system. The first step was to explore "processing-in-storage" architectures, where data-intense computation is performed in or close to the data storage medium in a disk drive. We found that there is possible to increase the amount of processing power inside a disk drive and still operate within the power budget by trading off energy usage between the spindle and the electronics. To enable even higher performance, we next looked at disk drive architectures that provided parallelism inside the drive. The latter effort also showed promise as a technique to significantly reduce the energy usage of a storage system. We also developed techniques for effective energy reduction of conventional disk-based storage systems.  As flash memory based Solid State Drives (SSDs) started becoming more common in the marketplace, the research emphasis shifted from hard drives to SSDs. While flash memory provides low access latency for reads, it still poses write bottlenecks. Moreover, flash memory has a limited lifetime and the memory cells can wear out after repeated writes and erases. We studied the reliability problem and showed that it is possible to achieve orders of magnitude higher endurance than those quoted in manufacturer datasheets by leveraging the self-recovery property inherent to flash. We showed that using a high-endurance non-volatile memory, such as Spin Transfer Torque RAM (STT-RAM), as a write merge-buffer in the SSD can significantly boost both performance and reliability. We also developed tools that allow detailed study of flash memory tradeoffs.  We expanded our research from storage to examine non-volatility at other layers of the memory hierarchy. In particular, we looked at the use of STT-RAM as caches and main memory. While STT-RAM allows for high density and has virtually no leakage power (since data is not stored in the form of charges), it is slower than SRAM and the write-energy is high. We showed that it is possible to address these problems by reducing the retention time of the STT-RAM cells. Although reducing the non-volatility can be problematic for long-term storage, caches and main memory house data for much shorter duration of time than storage and therefore such a tradeoff will be acceptable. We also developed other circuit-level techniques to reduce write latency and energy. Finally, we developed a STT-RAM memory design tool. A key broader impact of this research was the development of a tutorial series on non-volatile memory with colleagues at IBM Research. These tutorials were given at major architecture conferences and at the Non-Volatile Memories Workshop in 2011, where there were over 140 registered attendees. We also co-authored a Synthesis Lecture on Computer Architecture book on this topic.       Last Modified: 05/09/2013       Submitted by: Sudhanva Gurumurthi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
