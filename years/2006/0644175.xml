<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Scalable Rendering for Visual Realism in Scale-Complex Scenes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2007</AwardEffectiveDate>
<AwardExpirationDate>01/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>456625</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Lawrence Rosenblum</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>CAREER: Scalable Rendering for Visual Realism in Scale-Complex Scenes&lt;br/&gt;PI: Kavita Bala&lt;br/&gt;&lt;br/&gt;A fundamental challenge in computer graphics is to create interactive virtual environments that accurately depict the complex natural scenes of the real world.  These virtual environments are vital for a wide variety of applications, including e-commerce, education, industrial design and architectural planning, games and movies, safety analysis and virtual training, and cultural heritage.  Realistically simulating the visual appearance of the real world is extremely challenging because scenes of interest have complex geometry, material, and lighting interacting across a wide range of physical scales, ranging from millimeter-sized surface bumps to large-scale structure. We call such scenes scale-complex.  Current rendering methods are blind to scale, making it infeasible to realistically simulate the complex paths along which light reflects and scatters in such scale-complex scenes.  This project develops a novel framework for realistically rendering images of scale-complex scenes. Importantly, the framework supports rich illumination phenomena and rendering effects such as indirect illumination, participating media, subsurface scattering, motion blur, and depth-of-field.&lt;br/&gt;&lt;br/&gt;For the proposed framework to be scalable, it must perform well even with growing complexity of the scene and of simulated illumination phenomena. This project explores the following new approaches: (a) a unified treatment of all illumination phenomena and rendering effects, (b) novel multiresolution representations coupled with perceptual metrics based on early vision and higher level vision to eliminate computation where it is not visually important, (c) new methods for accurately computing illumination detail as needed, with illumination-driven simplification of geometry and material, and (d) new hybrid CPU/GPU algorithms for interactive performance.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/22/2007</MinAmdLetterDate>
<MaxAmdLetterDate>02/28/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0644175</AwardID>
<Investigator>
<FirstName>Kavita</FirstName>
<LastName>Bala</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kavita Bala</PI_FULL_NAME>
<EmailAddress>kb@cs.cornell.edu</EmailAddress>
<PI_PHON>6072551383</PI_PHON>
<NSF_ID>000179613</NSF_ID>
<StartDate>01/22/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148502820</ZipCode>
<StreetAddress><![CDATA[373 Pine Tree Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7352</Code>
<Text>COMPUTING PROCESSES &amp; ARTIFACT</Text>
</ProgramElement>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~99597</FUND_OBLG>
<FUND_OBLG>2008~104409</FUND_OBLG>
<FUND_OBLG>2009~86462</FUND_OBLG>
<FUND_OBLG>2010~104962</FUND_OBLG>
<FUND_OBLG>2011~61195</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>An image of a scene conveys critical visual information about the objects in<br />the scene (e.g., their shape and material), and the lighting and context of the<br />scene.&nbsp; Many applications depend critically on such images to predict visual<br />appearance.&nbsp; For example, in industrial design and architectural design, it is<br />useful to visualize how objects and buildings look before they are constructed<br />at full scale and considerable cost. This capability can encourage creative<br />design, and let designers and users to achieve the appearance they desire.<br />These applications share in common their need for a very high standard of<br />visual fidelity; they must accurately predict exactly how objects will truly<br />appear in the real world before they are ever built or made.&nbsp; Other application<br />areas that require this level of visual fidelity include virtual cultural heritage,<br />virtual training, and ecommerce.&nbsp; <br /><br />Achieving realism requires (1) detailed models of the shape and material of<br />each object, and (2) accurate simulation of how light interacts with these<br />objects and materials, and then propagates in the scene.&nbsp; Realistic rendering<br />of such scenes is a grand challenge because accurately simulating the physics of light is prohibitively expensive.&nbsp; Our project builds on a key insight to address this challenge: we use knowledge of how human beings perceive the world to focus computational effort only where it contributes to an image, and eliminate computation where the result will not be perceived by a human<br />observer. <br /><br /><strong>Impact. </strong>This new way of coupling perceptual knowledge with computational algorithms has changed how graphics algorithms are designed today.&nbsp; The impact of this work has been significant. Our scalable rendering technology, lightcuts, has been adopted by industry. Autodesk, the market leader in design software, uses Lightcuts (our research) as the core rendering engine of major design and modeling software products that are commercially available.&nbsp; Millions of images (and growing) have been rendered by users and designers using our software technology for pre-visualization.<br /><br />Over the course of this project we have pursued three major directions of<br />research.&nbsp; <br /><br /><strong>1. Micron resolution representations of detailed shape.</strong> We have developed techniques to construct micron resolution models of everyday materials like cloth. This work leverages technologies like CT imaging to acquire the geometric details that are crucial to making materials like silk and velvet achieve their characteristic visual appearance. <br /><br /><strong>2. Scaling to complex scenes. </strong>Rendering images of models that can range<br />from micron-resolution detail to city-size geometry requires computational<br />algorithms that can scale to this range of sizes.&nbsp; The key technology developed in this project is Lightcuts, a scalable rendering algorithm that uses<br />knowledge of human perception to drive efficient image generation rendering<br />algorithms.&nbsp;&nbsp;&nbsp; Lightcuts supports complex effects like motion blur, volumetric<br />rendering, and depth-of-field.&nbsp; While these complex effects increase the cost<br />of rendering, they often decrease the perceptual salience of scene features.<br />Our key insight was to use inexpensive lighting approximations aggressively where perceptual salience is reduced.&nbsp; Thus, lightcuts simulates the physics of light using multi-scale approximations that are perceptually accurate, but can scale from micron resolution to city scale. <br /><br />Prior perceptual methods cannot predict when such approximations are possible without doing the very work of rendering the image for comparison.&nbsp; We show how to select approximations by predicting and bounding per...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ An image of a scene conveys critical visual information about the objects in the scene (e.g., their shape and material), and the lighting and context of the scene.  Many applications depend critically on such images to predict visual appearance.  For example, in industrial design and architectural design, it is useful to visualize how objects and buildings look before they are constructed at full scale and considerable cost. This capability can encourage creative design, and let designers and users to achieve the appearance they desire. These applications share in common their need for a very high standard of visual fidelity; they must accurately predict exactly how objects will truly appear in the real world before they are ever built or made.  Other application areas that require this level of visual fidelity include virtual cultural heritage, virtual training, and ecommerce.    Achieving realism requires (1) detailed models of the shape and material of each object, and (2) accurate simulation of how light interacts with these objects and materials, and then propagates in the scene.  Realistic rendering of such scenes is a grand challenge because accurately simulating the physics of light is prohibitively expensive.  Our project builds on a key insight to address this challenge: we use knowledge of how human beings perceive the world to focus computational effort only where it contributes to an image, and eliminate computation where the result will not be perceived by a human observer.   Impact. This new way of coupling perceptual knowledge with computational algorithms has changed how graphics algorithms are designed today.  The impact of this work has been significant. Our scalable rendering technology, lightcuts, has been adopted by industry. Autodesk, the market leader in design software, uses Lightcuts (our research) as the core rendering engine of major design and modeling software products that are commercially available.  Millions of images (and growing) have been rendered by users and designers using our software technology for pre-visualization.  Over the course of this project we have pursued three major directions of research.    1. Micron resolution representations of detailed shape. We have developed techniques to construct micron resolution models of everyday materials like cloth. This work leverages technologies like CT imaging to acquire the geometric details that are crucial to making materials like silk and velvet achieve their characteristic visual appearance.   2. Scaling to complex scenes. Rendering images of models that can range from micron-resolution detail to city-size geometry requires computational algorithms that can scale to this range of sizes.  The key technology developed in this project is Lightcuts, a scalable rendering algorithm that uses knowledge of human perception to drive efficient image generation rendering algorithms.    Lightcuts supports complex effects like motion blur, volumetric rendering, and depth-of-field.  While these complex effects increase the cost of rendering, they often decrease the perceptual salience of scene features. Our key insight was to use inexpensive lighting approximations aggressively where perceptual salience is reduced.  Thus, lightcuts simulates the physics of light using multi-scale approximations that are perceptually accurate, but can scale from micron resolution to city scale.   Prior perceptual methods cannot predict when such approximations are possible without doing the very work of rendering the image for comparison.  We show how to select approximations by predicting and bounding perceptual error.  The result is a huge reduction in computational cost: 3--6 orders of magnitude. This algorithm has been widely adopted in industry.   3. Perceptual knowledge. The insight that drives our work is that an understanding of human perception can be used to design algorithms that automatically focus computation where it impacts perceived image fidelity.  T...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
