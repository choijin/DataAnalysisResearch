<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Making music documents accessible in musical terms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2007</AwardEffectiveDate>
<AwardExpirationDate>12/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>490669.00</AwardTotalIntnAmount>
<AwardAmount>506669</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Making Music Documents Accessible in Musical Terms&lt;br/&gt;One of the key problems facing us in the 21st century is information retrieval and management. Finding ways to automatically index, label, and access multimedia content (such as music documents) in meaningful ways is an open research question that increases in importance as multimedia databases proliferate and grow. Music collections, such as the 3.5 million recordings in Apple Computer's iTunes repository, comprise one of the most popular categories of on-line multimedia content.&lt;br/&gt;For scholars, musicians and even casual listeners, the music document is only the beginning, a tool to initiate the task at hand. Musicians may be interested in remixing a musical recording even though all they have available is the final mix. Scholars may wish to analyze the harmonies in a piece. Others may want karaoke that follows the singer's expressive timing, or a way to remove the sound of an unwanted cell phone ring from a recording of their daughter's flute recital.&lt;br/&gt;The objective of this research is to develop two key facilitating technologies to enable these kinds of interactions: score alignment and source separation.  Score alignment, involves aligning an audio performance and to the events in a machine-readable music score. When aligned to a score, a performance can be addressed by melodic and harmonic content. We propose to advance the state-of-the-art by enabling a machine to follow partially specified scores (such as Jazz lead sheets). This alignment require significant inference about likely surface structures (the note sequence in an improvised solo) from deeper structural descriptions in the score (the chords in a lead sheet). This will enable alignment of entire classes of music, such as much Jazz, Pop and Rock, that cannot currently be aligned to scores.&lt;br/&gt;The second technology, source separation, is the process of isolating individual source signals, given mixtures of the source signals. With source separation, individual instruments and sounds can be accessed, identified and manipulated in ways beyond the power of commercial audio search and editing software. We will advance the field through score-informed separation, as well as new iterative methods for approximating source models from acoustic mixtures.&lt;br/&gt;The idea is to develop a synergistic system for music-information-retrieval and interaction that uses multiple document modalities (written scores, audio files, MIDI) to infer more about the music structure than is possible using a single modality.&lt;br/&gt;This research will impact the signal-processing community (source separation), the music information retrieval community (music indexing and search) and the artificial intelligence community (tools for intelligent abstraction of real-world data). To broadly disseminate the work, demonstration tools will be made available over the internet and results will be published in relevant journals and conferences. The PI is committed to involving undergraduates and members of historically underrepresented groups in research, working with the SROP and UROP programs to make this happen. The PI also teaches the course "Machine Perception of Music" where research results will be disseminated to a wide variety of students.</AbstractNarration>
<MinAmdLetterDate>12/18/2006</MinAmdLetterDate>
<MaxAmdLetterDate>05/24/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0643752</AwardID>
<Investigator>
<FirstName>Bryan</FirstName>
<LastName>Pardo</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bryan A Pardo</PI_FULL_NAME>
<EmailAddress>pardo@northwestern.edu</EmailAddress>
<PI_PHON>8474917184</PI_PHON>
<NSF_ID>000275342</NSF_ID>
<StartDate>12/18/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~98835</FUND_OBLG>
<FUND_OBLG>2008~102370</FUND_OBLG>
<FUND_OBLG>2009~101021</FUND_OBLG>
<FUND_OBLG>2010~96431</FUND_OBLG>
<FUND_OBLG>2011~108012</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>One of the key problems facing us in the 21st century is information retrieval and management. Finding ways to automatically index, label, and access multimedia content (such as music documents) in meaningful ways is an open research question that increases in importance as multimedia databases proliferate and grow. Music collections, such as the 3.5 million recordings in Apple Computer&rsquo;s iTunes repository, comprise one of the most popular categories of on-line multimedia content.</p> <p><br />For scholars, musicians and even casual listeners, the music document is only the beginning, a tool to initiate the task at hand. Musicians may be interested in remixing a musical recording even though all they have available is the final mix. Scholars may wish to analyze the harmonies in a piece. Others may want karaoke that follows the singer&rsquo;s expressive timing, or a way to remove the sound of an unwanted cell phone ring from a recording of their daughter&rsquo;s flute recital.</p> <p><br />The objective of this research was to develop two key facilitating technologies to enable these kinds of interactions: score alignment and source separation.&nbsp; Score alignment, involves aligning an audio performance and to the events in a machine-readable music score. When aligned to a score, a performance can be addressed by melodic and harmonic content. We proposed to advance the state-of-the-art by enabling a machine to follow partially specified scores (such as Jazz lead sheets). This alignment requires significant inference about likely surface structures (the note sequence in an improvised solo) from deeper structural descriptions in the score (the chords in a lead sheet). This enables alignment of entire classes of music, such as much Jazz, Pop and Rock, that could not be aligned to scores prior to our work.&nbsp; We developed new fundamental algorithms to allow such alignment, enabling new kinds of editing and remixing of audio that were not previously possible for music that have a loosely specified lead sheet, but not a note-for-note score, as is found in classical music.</p> <p><br />The second technology, source separation, is the process of isolating individual source signals, given mixtures of the source signals. With source separation, individual instruments and other sounds (such as human voices) can be accessed, identified and manipulated in ways beyond the power of commercial audio search and editing software. We advanced the field both through music score-informed separation, as well as repetition-based source separation. This work has also resulted in a patent application for repetition-based source separation. We further anticipate the source separation algorithms we use can, in the future, be applied to non-music-based problems, such as enhancement of audio for hearing aids.</p> <p><br />One goal of this work was to make steps towards a synergistic system for music-information-retrieval and interaction that uses multiple document modalities (written scores, audio files, MIDI) to infer more about the music structure than is possible using a single modality. We have developed a demonstration tool that separates a mixture of music (such as a string quartet) into individual tracks by aligning the audio to a MIDI file containing the written score. This allows remixing of the audio even when individually recorded tracks do not exist (e.g. live recording of a string quartet to a stereo microphone).</p> <p><br />This research impacts the signal-processing community (source separation), the music information retrieval community (music indexing and search) and the artificial intelligence community (tools for intelligent abstraction of real-world data). To broadly disseminate the work, demonstration tools for source separation, score alignment and audio manipulation are available over the internet at music.cs.northwestern.edu and results have been published in...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ One of the key problems facing us in the 21st century is information retrieval and management. Finding ways to automatically index, label, and access multimedia content (such as music documents) in meaningful ways is an open research question that increases in importance as multimedia databases proliferate and grow. Music collections, such as the 3.5 million recordings in Apple ComputerÆs iTunes repository, comprise one of the most popular categories of on-line multimedia content.   For scholars, musicians and even casual listeners, the music document is only the beginning, a tool to initiate the task at hand. Musicians may be interested in remixing a musical recording even though all they have available is the final mix. Scholars may wish to analyze the harmonies in a piece. Others may want karaoke that follows the singerÆs expressive timing, or a way to remove the sound of an unwanted cell phone ring from a recording of their daughterÆs flute recital.   The objective of this research was to develop two key facilitating technologies to enable these kinds of interactions: score alignment and source separation.  Score alignment, involves aligning an audio performance and to the events in a machine-readable music score. When aligned to a score, a performance can be addressed by melodic and harmonic content. We proposed to advance the state-of-the-art by enabling a machine to follow partially specified scores (such as Jazz lead sheets). This alignment requires significant inference about likely surface structures (the note sequence in an improvised solo) from deeper structural descriptions in the score (the chords in a lead sheet). This enables alignment of entire classes of music, such as much Jazz, Pop and Rock, that could not be aligned to scores prior to our work.  We developed new fundamental algorithms to allow such alignment, enabling new kinds of editing and remixing of audio that were not previously possible for music that have a loosely specified lead sheet, but not a note-for-note score, as is found in classical music.   The second technology, source separation, is the process of isolating individual source signals, given mixtures of the source signals. With source separation, individual instruments and other sounds (such as human voices) can be accessed, identified and manipulated in ways beyond the power of commercial audio search and editing software. We advanced the field both through music score-informed separation, as well as repetition-based source separation. This work has also resulted in a patent application for repetition-based source separation. We further anticipate the source separation algorithms we use can, in the future, be applied to non-music-based problems, such as enhancement of audio for hearing aids.   One goal of this work was to make steps towards a synergistic system for music-information-retrieval and interaction that uses multiple document modalities (written scores, audio files, MIDI) to infer more about the music structure than is possible using a single modality. We have developed a demonstration tool that separates a mixture of music (such as a string quartet) into individual tracks by aligning the audio to a MIDI file containing the written score. This allows remixing of the audio even when individually recorded tracks do not exist (e.g. live recording of a string quartet to a stereo microphone).   This research impacts the signal-processing community (source separation), the music information retrieval community (music indexing and search) and the artificial intelligence community (tools for intelligent abstraction of real-world data). To broadly disseminate the work, demonstration tools for source separation, score alignment and audio manipulation are available over the internet at music.cs.northwestern.edu and results have been published in relevant journals and conferences.       Last Modified: 01/30/2013       Submitted by: Bryan A Pardo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
