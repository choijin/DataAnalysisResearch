<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Fundamental Research, Design and Evaluation of Auditory and Multimodal Graphs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2007</AwardEffectiveDate>
<AwardExpirationDate>01/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>500686.00</AwardTotalIntnAmount>
<AwardAmount>536686</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Regardless of advances in technology, a continual challenge to science, math, and engineering (SMET) education for blind students, and for their careers, is the task of examining and interpreting data, typically accomplished with visual graphs by sighted users.  While there have been many projects aimed at enhancing SMET education and scientific access for the blind by adding sound in some manner, there has been very little systematic study of how, exactly, to create effective auditory graphs.  Without such auditory display tools (in combination with visual graphs), there is little chance for visually impaired individuals to participate in science, especially when collaborative discussions are part of the learning or discovery process.  In this project, the PI will conduct a program of research and empirical validation into the most effective ways to design and develop auditory graphs, train users to interpret them, and evaluate their effectiveness and appropriateness alongside visual graphs.  The PI's preliminary findings indicate that both sighted and visually impaired students and scientists can make use of the auditory display of quantitative information, so long as the displays are sophisticated and well designed, the listeners are trained, and the designer is aware of any differences that may exist in how the various listeners tend to interpret what they hear.  A multimodal display should be even more effective, especially when there are both sighted and blind participants in a classroom or research team.  Building on these insights, the PI will first perform fundamental experiments aimed at determining the optimal methods for providing contextual cues to auditory graphs, analogous to the axis or tick marks in a visual graph.  Once these auditory graph design guidelines have been determined, open source software will be developed to produce both auditory and visual graphs simultaneously, so that teachers or others working with data need not duplicate their efforts.  The PI will then conduct experiments to determine the most effective ways to train listeners to interpret the auditory graphs, since it is crucial to overcome the lack of experience that most students, sighted or blind, have with auditory displays.  Online training materials will also be developed in order to ensure that the teachers who are likely to create and utilize these sophisticated auditory graphs have the requisite knowledge and skills.  The training methods and materials will be evaluated and validated in an experiment and focus group with teachers who work with sighted and visually impaired students.  Finally, the efficacy of the enhanced auditory+visual graphs will be validated by comparing the learning outcomes for students who are taught science concepts with the aid of auditory vs. auditory+visual graphs.  All of these studies will involve the participation of teachers and both visually impaired and sighted students.  &lt;br/&gt;&lt;br/&gt;Broader Impact:  Well-designed auditory displays should be invaluable for both sighted and visually impaired students.  Having multiple formats for the data displays will allow all users to maximize learning, and have a shared basis for collaboration.  The ongoing research projects will be tied in to university level classes, so that students can learn by taking part in the research efforts; the results and findings will then be rolled into the materials for subsequent classes in computing, psychology, and assistive technology.  Visually impaired participants will be a major constituent in focus groups, experiments, and prototype evaluation; this will encourage them to become involved in future R &amp; D efforts that focus on technologies of use and of interest to them.  Project outcomes will be disseminated through the PI's various partner organizations and international research collaborators; web sites will make the software, training, and research materials freely available worldwide.</AbstractNarration>
<MinAmdLetterDate>01/25/2007</MinAmdLetterDate>
<MaxAmdLetterDate>03/09/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0644076</AwardID>
<Investigator>
<FirstName>Bruce</FirstName>
<LastName>Walker</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bruce Walker</PI_FULL_NAME>
<EmailAddress>bruce.walker@psych.gatech.edu</EmailAddress>
<PI_PHON>4048948265</PI_PHON>
<NSF_ID>000492288</NSF_ID>
<StartDate>01/25/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 NORTH AVE NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~100058</FUND_OBLG>
<FUND_OBLG>2008~200012</FUND_OBLG>
<FUND_OBLG>2010~120131</FUND_OBLG>
<FUND_OBLG>2011~100485</FUND_OBLG>
<FUND_OBLG>2012~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 10"> <div class="layoutArea"> <div class="column"> <p><span>Our research has led to a better understanding of how to design auditory and multimodal graphs, and how to  train people to use them. We have learned about how different people interpret auditory  graphs, and how to use this to make better displays, especially for learners with vision loss. We have rolled all these findings into  our software, which allows users to make effective and usable auditory graphs. We are now  applying this in a practical educational setting (math education for blind students).</span></p> <p><span>Our research</span>&nbsp;will make math and science education more accessible, which will enable  more visually impaired students to gain STEM skills, and thereby enhance their career  opportunities.&nbsp;The software tools we are developing are also relevant and useful for visually impaired  individuals who are already working in math, science, and data-intense fields, which opens up the range of careers available to people with vision loss.</p> <p><span>It is also true that our tools are useful for students and scientists, regardless of visual abilities, and may lead to  new science discoveries due to the different analysis methods that are now more available (in  particular, multimodal representations of data).&nbsp;</span></p> <p>Our research has inspired the development of several additional software tools, and has led to the development of enhanced auditory  interfaces for other&nbsp;important software, such as popular spreadsheet software programs. These potential new products may have broad impact  in the data analysis market, and on the design and development of all kinds of multimodal  information displays.&nbsp;</p> </div> </div> </div> <p>&nbsp;</p><br> <p>            Last Modified: 05/16/2013<br>      Modified by: Bruce&nbsp;Walker</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Our research has led to a better understanding of how to design auditory and multimodal graphs, and how to  train people to use them. We have learned about how different people interpret auditory  graphs, and how to use this to make better displays, especially for learners with vision loss. We have rolled all these findings into  our software, which allows users to make effective and usable auditory graphs. We are now  applying this in a practical educational setting (math education for blind students).  Our research will make math and science education more accessible, which will enable  more visually impaired students to gain STEM skills, and thereby enhance their career  opportunities. The software tools we are developing are also relevant and useful for visually impaired  individuals who are already working in math, science, and data-intense fields, which opens up the range of careers available to people with vision loss.  It is also true that our tools are useful for students and scientists, regardless of visual abilities, and may lead to  new science discoveries due to the different analysis methods that are now more available (in  particular, multimodal representations of data).   Our research has inspired the development of several additional software tools, and has led to the development of enhanced auditory  interfaces for other important software, such as popular spreadsheet software programs. These potential new products may have broad impact  in the data analysis market, and on the design and development of all kinds of multimodal  information displays.              Last Modified: 05/16/2013       Submitted by: Bruce Walker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
