<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NetSE:  Large:  Collaborative Research:  Exploiting Multi-Modality for Tele-Immersion</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>366399.00</AwardTotalIntnAmount>
<AwardAmount>366399</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Providing an environment that offers both immersion and interaction is a tough research challenge. Ensuring a reasonable Quality of Experience (QoE) in using these environments installed in geographically distributed cities is even a tougher challenge. This project considers a collaborative, immersive, and interactive environment that not only supports 3D rendering of the participants? video but also other modalities such as Body Sensor Network (BSN) data that can offer highly precise data about a person?s physical movements (as well as physiological data). While creating this environment, one needs to consider the various bottlenecks that choke the data streams carrying the immersive and interactive information: reconstruction delay, ultra-high throughput needed, packet loss, and rendering delays. &lt;br/&gt;&lt;br/&gt;The main aim of this project is to design and develop collaborative, multi-modal immersive environments with higher frame rates and frame quality by carrying out research tasks that can take advantage of information from other modalities and handle these bottlenecks.&lt;br/&gt;&lt;br/&gt;In a typical tele-immersive environment, participants can see themselves in the locally rendered 3D view and see participants in the remote environments as well. Since the local rendering delays are much smaller, participants can see themselves earlier and in a more smooth fashion compared to the rendering of remote participants that suffers from communication delays and packet losses. This aspect of varying delays among the immersive participants can potentially cause problems during dynamic interactions and affect their QoE. Answers to questions such as what type of problems can be caused and how the participants handle them depend on the application domain of the immersive environments. To study the QoE and validate (with usability studies) the collaborative, immersive environment, a tele-rehabilitation application will be deployed in multiple cities: Berkeley, California; 2 sites in Dallas, Texas; and Urbana-Champaign, Illinois. &lt;br/&gt;&lt;br/&gt;Intellectual Merits of this project are (i) The resource adaptation framework for streaming multi-source, multi-destination, multi-rate, multi-modal data incorporates supervisory hybrid control theory based fine-grained resource management, multi-modal coarse-grained management, and a multi-modal multicasting approach. (ii) Graphics Processing Unit (GPU)-based 3D reconstruction and compression algorithms. These algorithms facilitate reconstruction of 3D data points based on 3D camera array data and compress them at a faster pace than their CPU-based counterparts. (iii) GPU-based rendering algorithm of 3D data on the receiver side. This algorithm will handle potential data loss in 3D camera data streams using skeletal information from BSN data streams. (iv) Identification and measurement of Quality of Experience (QoE) metrics and using those metrics to derive Quality of Service (QoS) parameters. The derived QoS parameters will then help the resource adaptation framework to modify its decisions at run-time. This project aims to have transformative aspects in the new set of algorithms that exploits multi-modality while incorporating a feedback based on Quality of Experience for functions such as streaming, 3D reconstruction, and rendering.&lt;br/&gt;&lt;br/&gt;Broader Impacts: This project promises significant impact in the fields of education and pervasive health care by providing augmented abilities to carry out intricate programs such as tele-rehabilitation with increased correctness and flexibility. This can also lead to improved productivity in the society considering the ability of health-care professionals to potentially handle a larger population (in remote places) as well as considering the possibility of the affected persons to become independent and productive faster. The project also ensures the results from the proposed research will be incorporated into the courses being taught. 3 women PhD students and 6 under-graduate students (2 are minority students) already working with the investigators of this project. Serious efforts will be undertaken to continue their involvement in this project. Apart from refereed conference and journal publications, the developed software, collected data, and research results will be shared with other researchers through a dedicated website (after ensuring satisfaction of HIPAA regulations).</AbstractNarration>
<MinAmdLetterDate>09/23/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1012194</AwardID>
<Investigator>
<FirstName>Klara</FirstName>
<LastName>Nahrstedt</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Klara Nahrstedt</PI_FULL_NAME>
<EmailAddress>klara@cs.uiuc.edu</EmailAddress>
<PI_PHON>2172446624</PI_PHON>
<NSF_ID>000222062</NSF_ID>
<StartDate>09/23/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207406</ZipCode>
<StreetAddress><![CDATA[1901 South First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7794</Code>
<Text>NETWORK SCIENCE &amp; ENGINEERING</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~60430</FUND_OBLG>
<FUND_OBLG>2011~80655</FUND_OBLG>
<FUND_OBLG>2012~72782</FUND_OBLG>
<FUND_OBLG>2013~75383</FUND_OBLG>
<FUND_OBLG>2014~77149</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Intellectual Merit: </strong>As the population continues to age, the need for physical therapy services increases. 3D Tele-immersion (3DTI) technology allows full-body, multi-modal interaction among geographically dispersed users and opens a variety of opportunities for cyber-collaborative activities such telehealth and tele-physiotherapy. We have investigated 3D tele-immersion for tele-physiotherapy, considering audio, 3D video, and wearable sensors such as motion sensors (accelerometer and gyroscopes), EMG sensor and heartbeat sensor with the following outcomes:</p> <p>First: Design and implementation of the <strong>CyPhy (Cyber-Physiotherapy) system</strong>.&nbsp; CyPhy exploits the richness of healthcare semantics, and encompasses correlated multimodal information and health-related users&rsquo; attention focus, to deliver high quality real-time content between therapist and patient sites and enable a virtual reality experience in remote healthcare. CyPhy consists of five entities that serve five different user roles: therapist, patient, trainer, legal and administrative staff and offline reviewers. The five entities interconnect via Internet. We emulate all entities in our research laboratory to validate the cyber-system, i.e., we do not experiment with real patients since our goal is to validate the feasibility of 3DTI for potential health-care environments.</p> <p>Patient and trainer are <em>immersive CyPhy sites</em>. Patient and trainer interact with each other through audio and 3D multi-camera visual communication. Through these cameras, CyPhy captures and sends 3D user model to the other site to render a synchronized virtual world, where trainer and patient(s) interact with each other. Besides audiovisual devices, the patient wears medical sensors, and streams real-time sensory data to therapist &nbsp;along 3D video stream. The therapist and the offline viewer sites are <em>non-immersive CyPhy sites. </em>The therapist observes the movement of patient simultaneously as the patient interacts with the trainer. Since the image of patient is in 3D form, therapist can change view arbitrarily to focus on suspected injured body parts naturally by moving her position and view direction.</p> <p>Second: <strong>CyPhy Kit</strong>. CyPhy Kit is a set of wearable health devices, attached to a compression suit, and four Kinect cameras for the patient to put on during the home-based physiotherapy. CyPhy enables light-weighted recording studio at home combining the CyPhy kit, patient&rsquo;s home computer, scree, speaker and wireless access point. On a daily basis, CyPhy streams to the patient a pre-recorded exercise demonstration via 3D video, prescribed by the therapist. Patient follows exercises and have this rehabilitation session recorded with the CyPhy kit. After CyPhy records the home-based rehabilitation, CyPhy uploads the recordings to the patient&rsquo;s electronic health record cloud. Therapist then views the recordings whenever she is available.</p> <p>Third: <strong>FreeViewer</strong> and <strong>OmniViewer</strong>. &nbsp;&ldquo;<strong>Free Viewer&rdquo; on immersive sites</strong> provides 360<sup>o</sup> viewing for patient and trainer and adopts adaptive 3D point cloud capturing and activity recognition to lower substantially the resource requirement. The intuition is that at any given time, the viewer does not require all camera data to render her view. If we can acquire the interested view and the orientation of the captured object, we can filter out most of the unneeded content and achieve resource-efficient content capturing. FreeViewer uses the activity-aware adaptive compression (A3C) for activity capturing. A3C uses the Morphing-Based Frame Synthesis (MBFS) to boost up video frame rate by injecting synthesized frames into the received content. Thus, CyPhy reduces transmission rate by 25% to save substantial amount of bandwidth. <strong>&ldquo;OmniViewer&rdquo; for non-immersive sites</strong> uses four 3D cameras to enable 360<sup>o</sup> view. CyPhy encodes each 3D stream with multiple bitrates as a normal 2D stream, containing a pair of color frame and depth frame (enhanced DASH protocol). The user is then able to watch the performer from any angle.</p> <p>Forth: <strong>Smart Storage</strong> and <strong>Auto Summary Generation</strong>. Since the patient&rsquo;s physiotherapy recordings require large storage, we have developed a <strong>compression scheme</strong> to utilize not only inter-frame coding, but also exploit similarity between rehabilitation videos. We reached 1:1000 compression ratio, which is one magnitude order better than the compression ratio of existing inter-frame approaches (1:100).</p> <p>CyPhy performs content analysis to detect patient&rsquo;s movement anomalies. To make the viewing of detected anomalies easier, we provide <strong>auto summary generation</strong> of patient&rsquo;s movement anomalies. Our lab experiments show that we achieve an anomaly detection with 82.9% recall and 41.1% precision. This implies that when CyPhy uses deep content analysis, our content analysis module can help the auto summary creation mechanisms to discover 80% anomalies when it only has to go through 20% of the entire dataset.</p> <p><strong>Broader Impact:</strong> Together with UT Dallas and UC Berkeley collaborators, we have explored a large number of 3DTI technologies in a holistic manner that work in a resource-constrained sensing, computing and communication eco-system. We have communicated with CIMIT (Consortia for Improving Medicine with Innovation &amp; Technology) and shared our CyPhy knowledge. We have presented our work at leading multimedia venues, HCESC (Health-Care Engineering Systems Center) Symposium with Peoria OSF Hospital researchers present, &nbsp;in relevant classes, and &nbsp;during the UIUC Engineering Open House.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/05/2017<br>      Modified by: Klara&nbsp;Nahrstedt</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1012194/1012194_10048508_1486241491435_Figure-CyPyKit--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1012194/1012194_10048508_1486241491435_Figure-CyPyKit--rgov-800width.jpg" title="CyPhy Kit"><img src="/por/images/Reports/POR/2017/1012194/1012194_10048508_1486241491435_Figure-CyPyKit--rgov-66x44.jpg" alt="CyPhy Kit"></a> <div class="imageCaptionContainer"> <div class="imageCaption">CyPhy Kit Components: (a) set of multi-modal sensors, (b) emulated patient's home environment, (c) compression suit with medical sensors</div> <div class="imageCredit">Shanon Chen</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Klara&nbsp;Nahrstedt</div> <div class="imageTitle">CyPhy Kit</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1012194/1012194_10048508_1486243064982_Figure-Detection--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1012194/1012194_10048508_1486243064982_Figure-Detection--rgov-800width.jpg" title="Detection of Patient's Movement Anomaly"><img src="/por/images/Reports/POR/2017/1012194/1012194_10048508_1486243064982_Figure-Detection--rgov-66x44.jpg" alt="Detection of Patient's Movement Anomaly"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Detection of Patient's Movement Anomaly: (a) test data from different exercises, (b) results of anomaly detection  where blue dots are original frames capturing shoulder exercise; orange dots are frames capturing anomalies</div> <div class="imageCredit">Shannon Chen</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Klara&nbsp;Nahrstedt</div> <div class="imageTitle">Detection of Patient's Movement Anomaly</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit: As the population continues to age, the need for physical therapy services increases. 3D Tele-immersion (3DTI) technology allows full-body, multi-modal interaction among geographically dispersed users and opens a variety of opportunities for cyber-collaborative activities such telehealth and tele-physiotherapy. We have investigated 3D tele-immersion for tele-physiotherapy, considering audio, 3D video, and wearable sensors such as motion sensors (accelerometer and gyroscopes), EMG sensor and heartbeat sensor with the following outcomes:  First: Design and implementation of the CyPhy (Cyber-Physiotherapy) system.  CyPhy exploits the richness of healthcare semantics, and encompasses correlated multimodal information and health-related users? attention focus, to deliver high quality real-time content between therapist and patient sites and enable a virtual reality experience in remote healthcare. CyPhy consists of five entities that serve five different user roles: therapist, patient, trainer, legal and administrative staff and offline reviewers. The five entities interconnect via Internet. We emulate all entities in our research laboratory to validate the cyber-system, i.e., we do not experiment with real patients since our goal is to validate the feasibility of 3DTI for potential health-care environments.  Patient and trainer are immersive CyPhy sites. Patient and trainer interact with each other through audio and 3D multi-camera visual communication. Through these cameras, CyPhy captures and sends 3D user model to the other site to render a synchronized virtual world, where trainer and patient(s) interact with each other. Besides audiovisual devices, the patient wears medical sensors, and streams real-time sensory data to therapist  along 3D video stream. The therapist and the offline viewer sites are non-immersive CyPhy sites. The therapist observes the movement of patient simultaneously as the patient interacts with the trainer. Since the image of patient is in 3D form, therapist can change view arbitrarily to focus on suspected injured body parts naturally by moving her position and view direction.  Second: CyPhy Kit. CyPhy Kit is a set of wearable health devices, attached to a compression suit, and four Kinect cameras for the patient to put on during the home-based physiotherapy. CyPhy enables light-weighted recording studio at home combining the CyPhy kit, patient?s home computer, scree, speaker and wireless access point. On a daily basis, CyPhy streams to the patient a pre-recorded exercise demonstration via 3D video, prescribed by the therapist. Patient follows exercises and have this rehabilitation session recorded with the CyPhy kit. After CyPhy records the home-based rehabilitation, CyPhy uploads the recordings to the patient?s electronic health record cloud. Therapist then views the recordings whenever she is available.  Third: FreeViewer and OmniViewer.  "Free Viewer" on immersive sites provides 360o viewing for patient and trainer and adopts adaptive 3D point cloud capturing and activity recognition to lower substantially the resource requirement. The intuition is that at any given time, the viewer does not require all camera data to render her view. If we can acquire the interested view and the orientation of the captured object, we can filter out most of the unneeded content and achieve resource-efficient content capturing. FreeViewer uses the activity-aware adaptive compression (A3C) for activity capturing. A3C uses the Morphing-Based Frame Synthesis (MBFS) to boost up video frame rate by injecting synthesized frames into the received content. Thus, CyPhy reduces transmission rate by 25% to save substantial amount of bandwidth. "OmniViewer" for non-immersive sites uses four 3D cameras to enable 360o view. CyPhy encodes each 3D stream with multiple bitrates as a normal 2D stream, containing a pair of color frame and depth frame (enhanced DASH protocol). The user is then able to watch the performer from any angle.  Forth: Smart Storage and Auto Summary Generation. Since the patient?s physiotherapy recordings require large storage, we have developed a compression scheme to utilize not only inter-frame coding, but also exploit similarity between rehabilitation videos. We reached 1:1000 compression ratio, which is one magnitude order better than the compression ratio of existing inter-frame approaches (1:100).  CyPhy performs content analysis to detect patient?s movement anomalies. To make the viewing of detected anomalies easier, we provide auto summary generation of patient?s movement anomalies. Our lab experiments show that we achieve an anomaly detection with 82.9% recall and 41.1% precision. This implies that when CyPhy uses deep content analysis, our content analysis module can help the auto summary creation mechanisms to discover 80% anomalies when it only has to go through 20% of the entire dataset.  Broader Impact: Together with UT Dallas and UC Berkeley collaborators, we have explored a large number of 3DTI technologies in a holistic manner that work in a resource-constrained sensing, computing and communication eco-system. We have communicated with CIMIT (Consortia for Improving Medicine with Innovation &amp; Technology) and shared our CyPhy knowledge. We have presented our work at leading multimedia venues, HCESC (Health-Care Engineering Systems Center) Symposium with Peoria OSF Hospital researchers present,  in relevant classes, and  during the UIUC Engineering Open House.             Last Modified: 02/05/2017       Submitted by: Klara Nahrstedt]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
