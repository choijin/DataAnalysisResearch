<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Picturing Motion: Analyzing Multidimensional Time-Varying Data through Perceptually Accurate Exploratory Visualization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2011</AwardEffectiveDate>
<AwardExpirationDate>01/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>467027.00</AwardTotalIntnAmount>
<AwardAmount>499027</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The goal of this research project is to transform the way that complex time-varying scientific phenomena are analyzed using computers.  The project makes possible a new style of aggregate and comparative analysis of large, high-resolution scientific motion databases.  The approach combines the high-bandwidth channel of the human visual system with computational algorithms and interactive data exploration.&lt;br/&gt;&lt;br/&gt;The experimental research is integrated with educational efforts, including training computer scientist students in interdisciplinary research via a new course, a new writing-focused curriculum, and broadly disseminated lesson plans.  Artists and designers are direct participants in the research, opening up new career paths in the sciences for visually creative students.  Hands-on interactive visualizations engage 7-12th grade minority and underprivileged students in scientific computing.  &lt;br/&gt;&lt;br/&gt;The results include experimentally grounded guidelines for perceptually accurate visualization, new techniques for coupling dimensionality reduction with illustrative data visualization, and tested human-computer interfaces for exploring spatially complex multidimensional data.  These results are significant because they lead to new understandings of the coupled relationship between people and computing, specifically helping scientists move from vast, complex datasets to new insights.  The broader impacts of the work lie in applications to multiple disciplines including musculoskeletal biomechanics, evolutionary biology, and geospatial science. Through specific applications, the work yields insights that could lead to improved treatments for spinal disorders and diseases in children and adults, and inform new theories of historical diversification among animals.  Project results will be disseminated via open source software, videos, publications, and demos, all available on the project web site (http://www.cs.umn.edu/~keefe/NSFCAREER).</AbstractNarration>
<MinAmdLetterDate>01/13/2011</MinAmdLetterDate>
<MaxAmdLetterDate>05/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1054783</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Keefe</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel F Keefe</PI_FULL_NAME>
<EmailAddress>keefe@cs.umn.edu</EmailAddress>
<PI_PHON>6126267508</PI_PHON>
<NSF_ID>000513431</NSF_ID>
<StartDate>01/13/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 OAK ST SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~104111</FUND_OBLG>
<FUND_OBLG>2012~90499</FUND_OBLG>
<FUND_OBLG>2013~92994</FUND_OBLG>
<FUND_OBLG>2014~112202</FUND_OBLG>
<FUND_OBLG>2015~99221</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project team has completed a research project with the goal of creating and understanding new tools and techniques for analyzing time-varying scientific data (i.e., scientific phenomena that involve motion) using computer graphics data visualization. The research has been applied to some of today&rsquo;s most complex datasets, such as biomechanical analysis of the human spine. The results fit into a bigger picture of research that aims to understand how humans can work most effectively with computers in this new age of big data.</p> <p>Intellectual Merit:</p> <p>The project has made intellectual contributions to knowledge, theory, and practice in computer science.&nbsp; These results have been well received by the computer science research community as well as by collaborators in other scientific disciplines.&nbsp; Project results were published in top venues for Computer Science, Data Visualization, Virtual Reality, Clinical Biomechanics, and Art.&nbsp;</p> <p>The project took a data-visualization approach to analyzing complex motion data.&nbsp; In doing so, it made possible a new style of aggregate and comparative analysis of large, high-resolution scientific motion databases.&nbsp; First, experimentally grounded guidelines for perceptually accurate visualization of motion data in virtual reality environments were developed.&nbsp; Then, using these as a theoretical basis, new computational techniques for coupling dimensionality reduction algorithms with interactive and illustrative data visualization were created and evaluated with domain-science collaborators.&nbsp; Finally, building upon the best of the results, novel human-computer interface techniques were developed and tested in order to make it easier to analyze these spatially complex data in the display environments (e.g., virtual reality) that were determined to be best suited for motion data analysis tasks.&nbsp;</p> <p>Together, these technical and theoretical advances now make possible a new paradigm of motion data analysis called &ldquo;trend-centric motion analysis&rdquo;.&nbsp; Rather than visualizing a single motion at a time, this new approach helps scientists analyze trends across sets of hundreds of similar motions (e.g., hundreds of patients performing similar neck/back exercises).&nbsp; This is important because as science moves increasingly toward a data-intensive paradigm of discovery, experiments and simulations lead not only to more complex, multidimensional data but also to a new ability to scale up to collecting many instances of similar data.&nbsp; Although visualization has previously proven useful for analyzing a single instance of time-varying data, it is a different task to analyze trends and anomalies across a large collection.&nbsp; It is not possible to simply superimpose hundreds of 4D data visualizations on top of each other &ndash; the result would be far too cluttered to read; so, new strategies like &ldquo;trend-centric motion analysis&rdquo; are urgently needed.</p> <p>Broader Impacts:</p> <p>The work has already impacted the biomechanics domain, where collaborators are using the new visualization algorithms and techniques.&nbsp; Fourteen computer science students, 4 Ph.D. and 10 undergraduates (7 with REU support) have been supported in part by the project and trained in science and engineering methods.&nbsp; In addition two fine arts students have participated in the scientific research.&nbsp;&nbsp; The project&rsquo;s integrated education plan led to new visualization curriculum and helped to establish a vibrant new research lab, including interdisciplinary collaborators (students and faculty) in the sciences and arts.&nbsp; Each year of the project, the research team participated in multiple outreach activities, including hosting K-12 students for special research presentations and hands-on demonstrations in collaboration with programs, such as TRIO Upward Bound, Eureka!, and Discover STEM, that are designed to reach groups underserved and/or underrepresented in science. &nbsp;Finally, open source software, videos, publications, and other results were widely disseminated and are archived on the project website.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/24/2018<br>      Modified by: Daniel&nbsp;F&nbsp;Keefe</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586258968_trend-centric-teaser--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586258968_trend-centric-teaser--rgov-800width.jpg" title="Trend-Centric Motion Visualization"><img src="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586258968_trend-centric-teaser--rgov-66x44.jpg" alt="Trend-Centric Motion Visualization"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Trend-Centric Motion Visualization system makes it possible to analyze a collection of hundreds of similar motions to find common trends and anomalies.  For example, here, a collection data from 200 neck circumduction exercises is visualized using a 2D trend line linked with multiple 3D views.</div> <div class="imageCredit">Daniel Keefe</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;F&nbsp;Keefe</div> <div class="imageTitle">Trend-Centric Motion Visualization</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586054520_Jackson-2012-BELIV--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586054520_Jackson-2012-BELIV--rgov-800width.jpg" title="Visualizing Helical Axes of Motion"><img src="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586054520_Jackson-2012-BELIV--rgov-66x44.jpg" alt="Visualizing Helical Axes of Motion"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A sweep surface is created to visualize the quality of motion for patients with neck pain as they perform a circumduction exercise.  The visualization reveals "kinks" in the motion that could be used to suggest an optimal course of treatment.  Time advances from left to right.</div> <div class="imageCredit">Daniel Keefe</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;F&nbsp;Keefe</div> <div class="imageTitle">Visualizing Helical Axes of Motion</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586572468_Kim-2017-3dui-Biomechanics-Poster--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586572468_Kim-2017-3dui-Biomechanics-Poster--rgov-800width.jpg" title="2D/3D Shape Matching in VR"><img src="/por/images/Reports/POR/2018/1054783/1054783_10068585_1524586572468_Kim-2017-3dui-Biomechanics-Poster--rgov-66x44.jpg" alt="2D/3D Shape Matching in VR"></a> <div class="imageCaptionContainer"> <div class="imageCaption">In biomechanics, high-resolution motion capture can be accomplished with X-ray movies, but this provides only a flattened view of the anatomy.  Using a new virtual reality user interface, 3D bone models can be aligned in space to recreate each frame of the motion in true 3D, enabling new science.</div> <div class="imageCredit">Daniel Keefe</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;F&nbsp;Keefe</div> <div class="imageTitle">2D/3D Shape Matching in VR</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project team has completed a research project with the goal of creating and understanding new tools and techniques for analyzing time-varying scientific data (i.e., scientific phenomena that involve motion) using computer graphics data visualization. The research has been applied to some of today?s most complex datasets, such as biomechanical analysis of the human spine. The results fit into a bigger picture of research that aims to understand how humans can work most effectively with computers in this new age of big data.  Intellectual Merit:  The project has made intellectual contributions to knowledge, theory, and practice in computer science.  These results have been well received by the computer science research community as well as by collaborators in other scientific disciplines.  Project results were published in top venues for Computer Science, Data Visualization, Virtual Reality, Clinical Biomechanics, and Art.   The project took a data-visualization approach to analyzing complex motion data.  In doing so, it made possible a new style of aggregate and comparative analysis of large, high-resolution scientific motion databases.  First, experimentally grounded guidelines for perceptually accurate visualization of motion data in virtual reality environments were developed.  Then, using these as a theoretical basis, new computational techniques for coupling dimensionality reduction algorithms with interactive and illustrative data visualization were created and evaluated with domain-science collaborators.  Finally, building upon the best of the results, novel human-computer interface techniques were developed and tested in order to make it easier to analyze these spatially complex data in the display environments (e.g., virtual reality) that were determined to be best suited for motion data analysis tasks.   Together, these technical and theoretical advances now make possible a new paradigm of motion data analysis called "trend-centric motion analysis".  Rather than visualizing a single motion at a time, this new approach helps scientists analyze trends across sets of hundreds of similar motions (e.g., hundreds of patients performing similar neck/back exercises).  This is important because as science moves increasingly toward a data-intensive paradigm of discovery, experiments and simulations lead not only to more complex, multidimensional data but also to a new ability to scale up to collecting many instances of similar data.  Although visualization has previously proven useful for analyzing a single instance of time-varying data, it is a different task to analyze trends and anomalies across a large collection.  It is not possible to simply superimpose hundreds of 4D data visualizations on top of each other &ndash; the result would be far too cluttered to read; so, new strategies like "trend-centric motion analysis" are urgently needed.  Broader Impacts:  The work has already impacted the biomechanics domain, where collaborators are using the new visualization algorithms and techniques.  Fourteen computer science students, 4 Ph.D. and 10 undergraduates (7 with REU support) have been supported in part by the project and trained in science and engineering methods.  In addition two fine arts students have participated in the scientific research.   The project?s integrated education plan led to new visualization curriculum and helped to establish a vibrant new research lab, including interdisciplinary collaborators (students and faculty) in the sciences and arts.  Each year of the project, the research team participated in multiple outreach activities, including hosting K-12 students for special research presentations and hands-on demonstrations in collaboration with programs, such as TRIO Upward Bound, Eureka!, and Discover STEM, that are designed to reach groups underserved and/or underrepresented in science.  Finally, open source software, videos, publications, and other results were widely disseminated and are archived on the project website.          Last Modified: 04/24/2018       Submitted by: Daniel F Keefe]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
