<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:  Telling the Story of a Visual World: Event Classification and Integrated Image Understanding</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>06/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>498615.00</AwardTotalIntnAmount>
<AwardAmount>514615</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The ability to make meaning out of a visual world, such as recognizing objects, scenes and semantically meaningful activities and events, is a cornerstone of artificial intelligence. In computer vision, very important progress has been made recently in object and scene level recognition. But such tasks are often performed without an integrated and coherent description of the scene. Moreover, very few current algorithms are capable of further interpreting higher level semantic meanings of an image such as an event or activity. The goal of this project is to achieve event classification via an integrated image understanding given a single unknown image.&lt;br/&gt;This project aims to push the frontier of integrated and descriptive understanding of images through the development of sophisticated learning frameworks suitable for training algorithms by using a large amount of real-world data such as the ones from the Internet. High accuracy performance, minimal human supervision, flexibility and scalable learning will be the focus of this endeavor.  This project?s theoretical framework ties together several areas of computer vision, offers interesting model representations for the machine learning field, and connects more semantically driven visual recognition problem with the natural language processing field. &lt;br/&gt;The results are vital for image understanding technology for the visually- impaired; automatic annotation of images for large digital library as well as the next generation of image retrieval engines; and translation, education and rehabilitation technology for language students and medical patients (such as aphasia, stroke, etc.). &lt;br/&gt;The project?s long-term educational plan focuses on bringing the latest visual computation and cognition research directly into the classroom and the community at large, with an emphasis on reaching the underrepresented groups of students. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/08/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/04/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1000845</AwardID>
<Investigator>
<FirstName>Fei-Fei</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Fei-Fei Li</PI_FULL_NAME>
<EmailAddress>feifeili@cs.stanford.edu</EmailAddress>
<PI_PHON>6507253860</PI_PHON>
<NSF_ID>000070465</NSF_ID>
<StartDate>01/08/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[450 Jane Stanford Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~52158</FUND_OBLG>
<FUND_OBLG>2010~120876</FUND_OBLG>
<FUND_OBLG>2011~112201</FUND_OBLG>
<FUND_OBLG>2012~229380</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-a93ab04c-dbfd-a4c0-be51-1905e752822b"><span>This project focuses on high-level visual recognition. In particular, addressing the tasks of modeling and understanding objects, scene categories, and events in images. Towards these goals, there are three main issues that we focus on: representation, performance, and flexible and scalable learning. In the area of representation, we have developed methods to extract features from images that are robust and expressive, allowing us to build more complex and semantic models on top. Among these methods, we have developed the ObjectBank feature for scene understanding, probabilistic models understand multiple view of 3D objects, Grouplets that focus on the fine details of actions in images, and extensive evaluations of neural network models that are able to automatically learn features. Building on top of the image representations we have developed, we have also built many models that are able to achieve highly accurate classification/recognition performance, while simultaneously being able to provide a more detailed understanding of image content and relationships. As examples, we have developed several probabilistic graphical models capable of jointly reasoning about various aspects of scenes, random forests that are able to perform effective action classification, and neural networks that have achieved impressive performance on complex tasks such as image captioning. We have also introduced several benchmark datasets that allow us to evaluate the performance of methods for these tasks, helping to promote research and make comparisons between methods easier. Finally, in the area of flexible and scalable learning, we have introduced several interesting scenarios that can take advantage of large amounts of data with little to no annotation, and developed methods that can work effectively in these scenarios. In particular, we have placed a big emphasis on learning with weak supervision, which allows us to avoid laborious pixel-level manual annotation, while being able to take advantage of the large amounts of data on the Internet that are already weakly labeled. Together, our works have resulted in numerous publications in major research venues, and we have strived to make our results and code publicly available to help promote further research.</span></span></p><br> <p>            Last Modified: 03/02/2015<br>      Modified by: Fei-Fei&nbsp;Li</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project focuses on high-level visual recognition. In particular, addressing the tasks of modeling and understanding objects, scene categories, and events in images. Towards these goals, there are three main issues that we focus on: representation, performance, and flexible and scalable learning. In the area of representation, we have developed methods to extract features from images that are robust and expressive, allowing us to build more complex and semantic models on top. Among these methods, we have developed the ObjectBank feature for scene understanding, probabilistic models understand multiple view of 3D objects, Grouplets that focus on the fine details of actions in images, and extensive evaluations of neural network models that are able to automatically learn features. Building on top of the image representations we have developed, we have also built many models that are able to achieve highly accurate classification/recognition performance, while simultaneously being able to provide a more detailed understanding of image content and relationships. As examples, we have developed several probabilistic graphical models capable of jointly reasoning about various aspects of scenes, random forests that are able to perform effective action classification, and neural networks that have achieved impressive performance on complex tasks such as image captioning. We have also introduced several benchmark datasets that allow us to evaluate the performance of methods for these tasks, helping to promote research and make comparisons between methods easier. Finally, in the area of flexible and scalable learning, we have introduced several interesting scenarios that can take advantage of large amounts of data with little to no annotation, and developed methods that can work effectively in these scenarios. In particular, we have placed a big emphasis on learning with weak supervision, which allows us to avoid laborious pixel-level manual annotation, while being able to take advantage of the large amounts of data on the Internet that are already weakly labeled. Together, our works have resulted in numerous publications in major research venues, and we have strived to make our results and code publicly available to help promote further research.       Last Modified: 03/02/2015       Submitted by: Fei-Fei Li]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
