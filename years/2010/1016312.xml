<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI:  Small:  Perceptually Grounded Learning of Instructional Language</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is developing methods that allow a computer to automatically learn to understand and generate instructions in human language. Traditional approaches to natural-language learning require linguistic experts to laboriously annotate large numbers of sentences with detailed information about their grammar and meaning. In this project, instructional language is initially learned by simply observing humans following instructions given by other humans.  Once the system has learned reasonably well from observation, it also actively participates in the learning process by following human-given instructions itself, or giving its own instructions to humans and observing their behavior. The approach is being evaluated on its ability to interpret and generate English instructions for navigating in a virtual environment (e.g. "Go down the hall and turn left after you pass the chair.").  A novel machine learning method infers a probable formal meaning for a sentence from the resulting actions performed by a human follower, and then existing language-learning methods are used to acquire a language interpreter and generator.  The learned system is being evaluated in a range of virtual environments, testing its ability to follow human-provided natural language instructions to achieve prescribed goals, as well as to generate natural language instructions that humans can successfully follow to find specific destinations. The methods developed for this project will contribute to the development of virtual agents in games and educational simulations that learn to interpret and generate English instructions, and eventually aid the development of robots that can learn to interpret human language instruction from observation.</AbstractNarration>
<MinAmdLetterDate>07/28/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016312</AwardID>
<Investigator>
<FirstName>Raymond</FirstName>
<LastName>Mooney</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raymond J Mooney</PI_FULL_NAME>
<EmailAddress>mooney@cs.utexas.edu</EmailAddress>
<PI_PHON>5124719558</PI_PHON>
<NSF_ID>000308265</NSF_ID>
<StartDate>07/28/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787595316</ZipCode>
<StreetAddress><![CDATA[3925 W Braker Lane, Ste 3.340]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~149233</FUND_OBLG>
<FUND_OBLG>2011~300767</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;Intellectual Merit: This project explored grounded language learning by computers. Most research in natural-language processing attempts to comprehend text in isolation; however, fully understanding human language requires capturing the relationship between language and the world. Grounded language learning attempts to acquire knowledge that connects language to perception and action. This project primarily explored grounded language learning for the task of direction following in virtual worlds. It resulted in the development a series of increasingly capable computer systems for learning to follow natural-language navigation instructions by simply observing humans following such instructions in a virtual environment. Extensive experiments demonstrated the ability of these approaches to learn to follow natural-language instructions in two diverse languages (English and Mandarin Chinese) from such weak, ambiguous supervision with no prior linguistic knowledge.&nbsp; This grant also partially supported additional gounded-language research on video description, including using text-mining to aid activity recognition and generate simple sentences for describing single-activity videos.&nbsp; Experimental results on actual short YouTube videos demonstrated the system's abiltiy to accurately describe videos with descriptive&nbsp; English sentences and to improve its descriptions through the use of linguistic knowledge automatically acquired from large text corpora.The project produced 8 major scientific conference papers, and two PhD theses.</p> <p>Broader Impacts: The developed methods will aid the development of virtual agents and robots that automatically learn to accept human instruction in natural language. The methods developed for describing videos in natural-language will aid the development of systems for video search, descriptive video services for the visually imparied, and automated surveillance. Navigation instruction data for the project has been made publicly available and other researchers have already used it to develop and evaluate grounded learning for instructional language. The system has supported the education of two PhD and four Masters students in Computer Science in the high-demand areas of natural-language processing and machine learning. These graduates are now working in the U.S. technology industry for Google, Microsoft and several Internet start-ups.</p><br> <p>            Last Modified: 11/24/2014<br>      Modified by: Raymond&nbsp;J&nbsp;Mooney</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2014/1016312/1016312_10019266_1416765332746_instruction-following--rgov-214x142.jpg" original="/por/images/Reports/POR/2014/1016312/1016312_10019266_1416765332746_instruction-following--rgov-800width.jpg" title="Following English  directions"><img src="/por/images/Reports/POR/2014/1016312/1016312_10019266_1416765332746_instruction-following--rgov-66x44.jpg" alt="Following English  directions"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A screenshot of the system following English directions in a virtual environment after learning.</div> <div class="imageCredit">Raymond J. Mooney</div> <div class="imageSubmitted">Raymond&nbsp;J&nbsp;Mooney</div> <div class="imageTitle">Following English  directions</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Intellectual Merit: This project explored grounded language learning by computers. Most research in natural-language processing attempts to comprehend text in isolation; however, fully understanding human language requires capturing the relationship between language and the world. Grounded language learning attempts to acquire knowledge that connects language to perception and action. This project primarily explored grounded language learning for the task of direction following in virtual worlds. It resulted in the development a series of increasingly capable computer systems for learning to follow natural-language navigation instructions by simply observing humans following such instructions in a virtual environment. Extensive experiments demonstrated the ability of these approaches to learn to follow natural-language instructions in two diverse languages (English and Mandarin Chinese) from such weak, ambiguous supervision with no prior linguistic knowledge.  This grant also partially supported additional gounded-language research on video description, including using text-mining to aid activity recognition and generate simple sentences for describing single-activity videos.  Experimental results on actual short YouTube videos demonstrated the system's abiltiy to accurately describe videos with descriptive  English sentences and to improve its descriptions through the use of linguistic knowledge automatically acquired from large text corpora.The project produced 8 major scientific conference papers, and two PhD theses.  Broader Impacts: The developed methods will aid the development of virtual agents and robots that automatically learn to accept human instruction in natural language. The methods developed for describing videos in natural-language will aid the development of systems for video search, descriptive video services for the visually imparied, and automated surveillance. Navigation instruction data for the project has been made publicly available and other researchers have already used it to develop and evaluate grounded learning for instructional language. The system has supported the education of two PhD and four Masters students in Computer Science in the high-demand areas of natural-language processing and machine learning. These graduates are now working in the U.S. technology industry for Google, Microsoft and several Internet start-ups.       Last Modified: 11/24/2014       Submitted by: Raymond J Mooney]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
