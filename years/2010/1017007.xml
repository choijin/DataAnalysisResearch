<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Interactive Perception for Manipulating Non-Rigid Objects</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>417908.00</AwardTotalIntnAmount>
<AwardAmount>417908</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Traditional robotics research has adopted a sense-plan-act paradigm in which it is assumed that the sensors are capable of providing enough information in order to decide the next course of action.  Humans and animals, however, frequently adopt a different approach, such as shuffling through a pile of unknown objects in order to identify an item of interest hidden beneath the pile.  This project explores the concept of interactive perception (or manipulated-guided sensing), in which successive manipulations of objects in an environment are used to increase vision-based understanding of that environment, and vice versa.  In particular, the project involves developing appropriate low-order models of highly non-rigid structures such as fabrics and textiles; constructing algorithms to perform real-time vision-based sensing of such objects in cluttered, unstructured environments; and building prototype robotic hardware for testing the resulting models and algorithms.  The research forms an integral part of next-generation household service robots performing everyday tasks such as sorting and folding laundry.</AbstractNarration>
<MinAmdLetterDate>08/19/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/19/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1017007</AwardID>
<Investigator>
<FirstName>Ian</FirstName>
<LastName>Walker</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ian D Walker</PI_FULL_NAME>
<EmailAddress>iwalker@clemson.edu</EmailAddress>
<PI_PHON>8646567209</PI_PHON>
<NSF_ID>000373849</NSF_ID>
<StartDate>09/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ian</FirstName>
<LastName>Walker</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ian D Walker</PI_FULL_NAME>
<EmailAddress>iwalker@clemson.edu</EmailAddress>
<PI_PHON>8646567209</PI_PHON>
<NSF_ID>000373849</NSF_ID>
<StartDate>08/19/2010</StartDate>
<EndDate>09/19/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stanley</FirstName>
<LastName>Birchfield</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stanley T Birchfield</PI_FULL_NAME>
<EmailAddress>stb@clemson.edu</EmailAddress>
<PI_PHON>8646565912</PI_PHON>
<NSF_ID>000256446</NSF_ID>
<StartDate>08/19/2010</StartDate>
<EndDate>09/19/2013</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Clemson University</Name>
<CityName>CLEMSON</CityName>
<ZipCode>296345701</ZipCode>
<PhoneNumber>8646562424</PhoneNumber>
<StreetAddress>230 Kappa Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042629816</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CLEMSON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042629816</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Clemson University]]></Name>
<CityName>CLEMSON</CityName>
<StateCode>SC</StateCode>
<ZipCode>296345701</ZipCode>
<StreetAddress><![CDATA[230 Kappa Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~417908</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project examined automated sensing and manipulation (and their interaction) of non-rigid objects. In particular, we investigated the problem of identifying and handling articles of clothing when arbitrarily crumpled and piled, as found when a dryer is unloaded when doing laundry. This part of the laundry process is particularly difficult to automate due to the unstructured initial arrangement of the clothes, and hard to predict movements of articles of clothing under manipulation - in both cases due to their inherent lack of rigid elements. Our research demonstrated how &ldquo;interactive perception&rdquo;, or successive operations of manipulating and then sensing non-rigid clothing items, improves the ability to classify individual items (as shirts, pants, shorts, etc.). Using a simple manipulator and inexpensive RGBD sensing (a Microsoft Kinect sensor), we further developed and implemented several algorithms for identification and classification and tracking of flexible objects such as clothing. In support of this work, we built and archived a large indexed database of RGBD clothing images from real laundry baskets across a range of configurations. Further research concentrated on the robotic manipulation of flexible surfaces, and on 3D mapping of environments using RGBD sensors. We additionally conducted basic research demonstrating how sensing of active manipulation of articulated objects can be used to predict and reconstruct how they articulate (i.e. find their hinge and rotation axes).</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/05/2014<br>      Modified by: Ian&nbsp;D&nbsp;Walker</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project examined automated sensing and manipulation (and their interaction) of non-rigid objects. In particular, we investigated the problem of identifying and handling articles of clothing when arbitrarily crumpled and piled, as found when a dryer is unloaded when doing laundry. This part of the laundry process is particularly difficult to automate due to the unstructured initial arrangement of the clothes, and hard to predict movements of articles of clothing under manipulation - in both cases due to their inherent lack of rigid elements. Our research demonstrated how "interactive perception", or successive operations of manipulating and then sensing non-rigid clothing items, improves the ability to classify individual items (as shirts, pants, shorts, etc.). Using a simple manipulator and inexpensive RGBD sensing (a Microsoft Kinect sensor), we further developed and implemented several algorithms for identification and classification and tracking of flexible objects such as clothing. In support of this work, we built and archived a large indexed database of RGBD clothing images from real laundry baskets across a range of configurations. Further research concentrated on the robotic manipulation of flexible surfaces, and on 3D mapping of environments using RGBD sensors. We additionally conducted basic research demonstrating how sensing of active manipulation of articulated objects can be used to predict and reconstruct how they articulate (i.e. find their hinge and rotation axes).          Last Modified: 08/05/2014       Submitted by: Ian D Walker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
