<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Spontaneous 4D-Facial Expression Corpus for Automated Facial Image Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>44216.00</AwardTotalIntnAmount>
<AwardAmount>44216</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Currently, few publically available, annotated databases exist. Those that do are limited to 2D static images or video of posed facial behavior. Further development is stymied by lack of adequate training data. Because posed and un-posed (aka ?spontaneous?) facial expressions differ along several dimensions including complexity, well annotated video of un-posed facial behavior is needed. Moreover, because the face is a three-dimensional deformable object, 2D video is insufficient. A 3D video archive is needed. &lt;br/&gt;&lt;br/&gt;This project develops a 3D video corpus of spontaneous facial and vocal expression in a diverse group of young adults. Well-validated emotion inductions elicit expressions of emotion and paralinguistic communication. Sequence-level ground truth is obtained via participant self-report. Frame-level ground-truth is obtained via facial action unit coding using the Facial Action Coding System. The project promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action. &lt;br/&gt;&lt;br/&gt;The project promotes research on next-generation affective computing with applications in security, law-enforcement, biomedicine, behavior science, entertainment and education. The multimodal 3D video database and its metadata are for the research community for new algorithm development, assessment, comparison, and evaluation.</AbstractNarration>
<MinAmdLetterDate>08/24/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/24/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1051169</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Cohn</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey F Cohn</PI_FULL_NAME>
<EmailAddress>jeffcohn@pitt.edu</EmailAddress>
<PI_PHON>4126248825</PI_PHON>
<NSF_ID>000211710</NSF_ID>
<StartDate>08/24/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pittsburgh</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152133203</ZipCode>
<PhoneNumber>4126247400</PhoneNumber>
<StreetAddress>300 Murdoch Building</StreetAddress>
<StreetAddress2><![CDATA[3420 Forbes Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004514360</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF PITTSBURGH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004514360</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pittsburgh]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133203</ZipCode>
<StreetAddress><![CDATA[300 Murdoch Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~44216</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>EAGER: Spontaneous 4D-Facial Expression Corpus for</p> <p>Automated Facial Image Analysis</p> <p>Lijun Yin, State University of New York at Binghamton</p> <p>Jeffrey F Cohn, University of Pittsburgh</p> <p>NSF Directorate/Division: CISE/ Division of Information &amp; Intelligent Systems,</p> <p>Program Officer: Dr. Jie Yang</p> <p>NSF Award Numbers: IIS-1051103 (Binghamton) and IIS-1051169 (Pittsburgh)</p> <p><span style="text-decoration: underline;">Project Outcomes Report:</span></p> <p><span style="white-space: pre;"> </span>Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Further development is stymied by lack of adequate annotated training data. Because posed and un-posed (aka &ldquo;spontaneous&rdquo;) facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed.&nbsp; Moreover, because the face is a three-dimensional deformable object, 2D video is insufficient. A 3D video archive is needed.</p> <p><span style="white-space: pre;"> </span>Thanks to support from NSF grants IIS-1051103 (PI: Dr. Lijun Yin of Binghamton University) and IIS-1051169 (PI: Dr. Jeff Cohn of University of Pittsburgh), a collaborative, inter-institutional research team from Binghamton University and University of Pittsburgh has developed a new 4D video (3D + time) database of spontaneous facial expression (Zhang, Yin, Cohn, Canavan, Reale, Horowitz, &amp; Liu, 2013). Participants were 23 women and 18 men from diverse ancestries that include Asian, African-American, Hispanic/Latino, and Euro-American.&nbsp; Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication.</p> <p><span style="white-space: pre;"> </span>The database includes multiple types of metadata to maximize its information value and usefulness. Sequence-level ground truth was obtained via participant self-report. Frame-level annotation of facial actions was obtained using manual FACS coding (Facial Action Coding System) (<a title="Ekman, 2002 #1014" href="#_ENREF_2">Ekman, Friesen, &amp; Hager, 2002</a>). Sixty-six fiduciary facial landmarks were hand labeled in approximately 5% of video frames.&nbsp; The landmarks then were used to train person-specific active appearance models (<a title="Baker, 2004 #3311" href="#_ENREF_1">Baker, Gross, &amp; Matthews, 2004</a>). Facial features were tracked in both 2D and 3D domains using both person-specific (AAM) and generic (constrained local models, or CLM) (<a title="Lucey, 2009 #4137" href="#_ENREF_3">Lucey, Wang, Cox, Sridharan, &amp; Cohn, 2009</a>) approaches.</p> <p>To provide benchmarks for automatic facial action unit detection, the project team developed a unique space-time feature &ndash; referred to as a Nebular feature &ndash; to represent facial actions and detect expressions. Unlike traditional methods that use two-dimensional images or posed 3D facial models, the Nebula feature (Reale, Zhang, &amp; Yin, 2013) uses high-resolution 3D motion models to detect subtle shape and appearance deformations on a 3D surface. This method can recognize facial expressions not only by individual models, but also by their dynamic actions over time. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.</p> <p><span style="white-space: pre;"> </span>This 4D spontaneous facial expression database is the first of its kind for spontaneous facial expression research. It will be released to the research community in April 2013 at the IEEE International Conference ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ EAGER: Spontaneous 4D-Facial Expression Corpus for  Automated Facial Image Analysis  Lijun Yin, State University of New York at Binghamton  Jeffrey F Cohn, University of Pittsburgh  NSF Directorate/Division: CISE/ Division of Information &amp; Intelligent Systems,  Program Officer: Dr. Jie Yang  NSF Award Numbers: IIS-1051103 (Binghamton) and IIS-1051169 (Pittsburgh)  Project Outcomes Report:   Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Further development is stymied by lack of adequate annotated training data. Because posed and un-posed (aka "spontaneous") facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed.  Moreover, because the face is a three-dimensional deformable object, 2D video is insufficient. A 3D video archive is needed.   Thanks to support from NSF grants IIS-1051103 (PI: Dr. Lijun Yin of Binghamton University) and IIS-1051169 (PI: Dr. Jeff Cohn of University of Pittsburgh), a collaborative, inter-institutional research team from Binghamton University and University of Pittsburgh has developed a new 4D video (3D + time) database of spontaneous facial expression (Zhang, Yin, Cohn, Canavan, Reale, Horowitz, &amp; Liu, 2013). Participants were 23 women and 18 men from diverse ancestries that include Asian, African-American, Hispanic/Latino, and Euro-American.  Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication.   The database includes multiple types of metadata to maximize its information value and usefulness. Sequence-level ground truth was obtained via participant self-report. Frame-level annotation of facial actions was obtained using manual FACS coding (Facial Action Coding System) (Ekman, Friesen, &amp; Hager, 2002). Sixty-six fiduciary facial landmarks were hand labeled in approximately 5% of video frames.  The landmarks then were used to train person-specific active appearance models (Baker, Gross, &amp; Matthews, 2004). Facial features were tracked in both 2D and 3D domains using both person-specific (AAM) and generic (constrained local models, or CLM) (Lucey, Wang, Cox, Sridharan, &amp; Cohn, 2009) approaches.  To provide benchmarks for automatic facial action unit detection, the project team developed a unique space-time feature &ndash; referred to as a Nebular feature &ndash; to represent facial actions and detect expressions. Unlike traditional methods that use two-dimensional images or posed 3D facial models, the Nebula feature (Reale, Zhang, &amp; Yin, 2013) uses high-resolution 3D motion models to detect subtle shape and appearance deformations on a 3D surface. This method can recognize facial expressions not only by individual models, but also by their dynamic actions over time. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.   This 4D spontaneous facial expression database is the first of its kind for spontaneous facial expression research. It will be released to the research community in April 2013 at the IEEE International Conference on Automatic Face and Gesture Recognition for use in algorithm development and testing.  A complete description of the database together with benchmark findings will be published in the conference proceedings Zhang, Yin, &amp; Cohn et al., 2013). Based on our past experience, we anticipate that the database will serve as a valuable and well-utilized benchmark for research and development in biometrics, security, biomedicine, psychology, affective computing, computer graphics, and related fields.  References:  B...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
