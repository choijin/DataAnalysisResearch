<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Development of a Fully Instrumented Self-Sensing and Self-Regulating Data Center</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>1342299.00</AwardTotalIntnAmount>
<AwardAmount>1342299</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Proposal #: 10-40666&lt;br/&gt;PI(s):  Ghose, Kanad; Gopala, Kardik; Murray, Brice T; Sammakia, Bahgat G; &lt;br/&gt;Institution: SUNY - Binghamton &lt;br/&gt;Title:  MRI/Dev.: Fully Instrumented Self-Sensing and Self-Regulating Data Center&lt;br/&gt;Project Proposed:&lt;br/&gt;The project, building an instrument for next generation data centers where energy consumption and management is treated as a first class resource (together with CPU, communications, etc) that needs to be scheduled and managed explicitly to maximize the overall energy-efficiency of the data center, takes a new approach for predicting local and global thermal conditions with new control paradigms. Studying the most energy-efficient use of the cooling resources, the proposed instrument specifically involves:&lt;br/&gt;- Experimental, scaled down data center of Linux servers with dynamic cooling facilities, modified kernels. and scheduling components; &lt;br/&gt;- Large number of temperature and airflow sensors and power meters, along with software instrumentation; &lt;br/&gt;- Floor plenum based chilled air cooling system used to provide nominal cooling and remotely controlled computer room air conditioners (CRACs) that provide a quickly adjustable and directed cooling facility;&lt;br/&gt;- Software components for the facility that permit research into the development of a wide variety of techniques, both traditional and innovative, for improving the data center energy efficiency; and &lt;br/&gt;- Generation and recording live data on workload levels, power consumption, temperature, and airflow distributions. &lt;br/&gt;Broader Impacts: &lt;br/&gt;This unique facility primarily provides solutions for energy efficiency in data centers. The project intensively involves minority, women, and undergraduate students in their research. The instrument contributes to prepare all students for rewarding careers in the technology, management, and use of data centers.</AbstractNarration>
<MinAmdLetterDate>09/17/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/17/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1040666</AwardID>
<Investigator>
<FirstName>Bahgat</FirstName>
<LastName>Sammakia</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bahgat G Sammakia</PI_FULL_NAME>
<EmailAddress>bahgat@binghamton.edu</EmailAddress>
<PI_PHON>6077776880</PI_PHON>
<NSF_ID>000170927</NSF_ID>
<StartDate>09/17/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bruce</FirstName>
<LastName>Murray</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bruce T Murray</PI_FULL_NAME>
<EmailAddress>bmurray@binghamton.edu</EmailAddress>
<PI_PHON>6077776561</PI_PHON>
<NSF_ID>000202940</NSF_ID>
<StartDate>09/17/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kanad</FirstName>
<LastName>Ghose</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kanad Ghose</PI_FULL_NAME>
<EmailAddress>ghose@cs.binghamton.edu</EmailAddress>
<PI_PHON>6077774608</PI_PHON>
<NSF_ID>000165096</NSF_ID>
<StartDate>09/17/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kartik</FirstName>
<LastName>Gopalan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kartik Gopalan</PI_FULL_NAME>
<EmailAddress>kartik@binghamton.edu</EmailAddress>
<PI_PHON>6077773751</PI_PHON>
<NSF_ID>000176936</NSF_ID>
<StartDate>09/17/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Binghamton</Name>
<CityName>BINGHAMTON</CityName>
<ZipCode>139026000</ZipCode>
<PhoneNumber>6077776136</PhoneNumber>
<StreetAddress>4400 VESTAL PKWY E</StreetAddress>
<StreetAddress2><![CDATA[PO Box 6000]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>22</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY22</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>090189965</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Binghamton]]></Name>
<CityName>BINGHAMTON</CityName>
<StateCode>NY</StateCode>
<ZipCode>139026000</ZipCode>
<StreetAddress><![CDATA[4400 VESTAL PKWY E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>22</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY22</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~1342299</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Data centers are a ubiquitous part of our daily life and are used to support a wide variety of web-based services and represent the computing and storage infrastructures behind clouds.&nbsp; In the US, data centers consume about 100 billion KiloWatt-Hours of energy annually and stress out the available capacities of the existing power grids.</p> <p>Data centers have two main sources of electricity consumption: the bulk of this is in running the IT equipment (such as servers, networking infrastructures and storage) which eventually gets converted to heat.&nbsp; The other major source of electricity consumption is in providing the cooling necessary for the IT equipment, whose operating temperatures need to be limited to safe levels for avoiding any damage.&nbsp; Modern data centers are usually overprovisioned in terms of both IT capacity and cooling capacity for dealing with unexpected load increases and the consequential heat that is generated from increased server activity. Such overprovisioning leads to decreased operational efficiencies and higher operating costs.&nbsp; In reality, typical data center workload levels are far lower than the peak load that they can handle, and server utilization is low.&nbsp; Consequently, data centers are not energy proportional, as the energy consumed does not vary directly with the workload level.</p> <p>A fully instrumented data center laboratory was established as the main outcome of this award, using support from this award, internal funds and contribution from industrial partners.&nbsp; This data center laboratory has the scale of a mid-sized data center with seven rows of servers of various types and different types of contemporary cooling systems (air cooling, chilled water cooling and warm water cooling).&nbsp; This laboratory enables experimentally validated investigations to discover practical solutions&nbsp;for reducing the energy wastage in typical data centers.&nbsp;&nbsp; A specific outcome was the development of techniques that automatically and dynamically provide IT and cooling capacity that is just adequate to deal with the current workload.&nbsp; The technique predicts the workload sufficiently in advance to permit the adjustment of the number of active servers and the cooling system with negligible impact on the delivered performance.</p> <p>Software and hardware instrumentation developed for this facility permits all critical data related to thermal conditions and performance be collected in real-time down to the level of individual servers.&nbsp; This data is analyzed to ascertain the efficacy of various solutions targeted by the project, including: (a) efficiency improvement of various types of cooling systems, (b) workload scheduling techniques for servers, (c) development of data center benchmarks and metrics that correlate energy efficiency with performance and, (d) the development of holistic control techniques for automatically managing the IT and cooling system capacity of typical data centers in a way that tracks instantaneous workload variations.</p> <p>Examples findings of specific investigations carried out are as follows.</p> <ul> <li>The energy required to operate a data center can be      reduced dramatically by provisioning the load handling capacity of the      activated servers to match a predicted load level, where the prediction is      based on the history of the load seen on recently-activated servers.&nbsp;      Typical server energy savings realized using the technique developed were      well in excess of 30% without any noticeable service quality degradation.</li> <li>Tail latency prolongation with tight server      provisioning can be reduced dramatically by taking into account the recent      errors made in predicting a load decrease and using a timeout interval to      delay server deactivations.</li> <li>Accurate but fast models at the room and rack level for      data centers ca...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Data centers are a ubiquitous part of our daily life and are used to support a wide variety of web-based services and represent the computing and storage infrastructures behind clouds.  In the US, data centers consume about 100 billion KiloWatt-Hours of energy annually and stress out the available capacities of the existing power grids.  Data centers have two main sources of electricity consumption: the bulk of this is in running the IT equipment (such as servers, networking infrastructures and storage) which eventually gets converted to heat.  The other major source of electricity consumption is in providing the cooling necessary for the IT equipment, whose operating temperatures need to be limited to safe levels for avoiding any damage.  Modern data centers are usually overprovisioned in terms of both IT capacity and cooling capacity for dealing with unexpected load increases and the consequential heat that is generated from increased server activity. Such overprovisioning leads to decreased operational efficiencies and higher operating costs.  In reality, typical data center workload levels are far lower than the peak load that they can handle, and server utilization is low.  Consequently, data centers are not energy proportional, as the energy consumed does not vary directly with the workload level.  A fully instrumented data center laboratory was established as the main outcome of this award, using support from this award, internal funds and contribution from industrial partners.  This data center laboratory has the scale of a mid-sized data center with seven rows of servers of various types and different types of contemporary cooling systems (air cooling, chilled water cooling and warm water cooling).  This laboratory enables experimentally validated investigations to discover practical solutions for reducing the energy wastage in typical data centers.   A specific outcome was the development of techniques that automatically and dynamically provide IT and cooling capacity that is just adequate to deal with the current workload.  The technique predicts the workload sufficiently in advance to permit the adjustment of the number of active servers and the cooling system with negligible impact on the delivered performance.  Software and hardware instrumentation developed for this facility permits all critical data related to thermal conditions and performance be collected in real-time down to the level of individual servers.  This data is analyzed to ascertain the efficacy of various solutions targeted by the project, including: (a) efficiency improvement of various types of cooling systems, (b) workload scheduling techniques for servers, (c) development of data center benchmarks and metrics that correlate energy efficiency with performance and, (d) the development of holistic control techniques for automatically managing the IT and cooling system capacity of typical data centers in a way that tracks instantaneous workload variations.  Examples findings of specific investigations carried out are as follows.  The energy required to operate a data center can be      reduced dramatically by provisioning the load handling capacity of the      activated servers to match a predicted load level, where the prediction is      based on the history of the load seen on recently-activated servers.       Typical server energy savings realized using the technique developed were      well in excess of 30% without any noticeable service quality degradation. Tail latency prolongation with tight server      provisioning can be reduced dramatically by taking into account the recent      errors made in predicting a load decrease and using a timeout interval to      delay server deactivations. Accurate but fast models at the room and rack level for      data centers can be used to control the cooling system and predict the      transient behavior of data centers from a thermal perspective. In a tightly provisioned data center, the cooling      sy...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
