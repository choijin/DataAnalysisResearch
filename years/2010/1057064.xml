<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Numerical Multilinear Algebra and Its Applications - From Matrices to Tensors</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2011</AwardEffectiveDate>
<AwardExpirationDate>02/28/2017</AwardExpirationDate>
<AwardTotalIntnAmount>550000.00</AwardTotalIntnAmount>
<AwardAmount>550000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The PI will extend extensive studies in numerical linear algebra to the numerical multilinear algebra setting, which complements and enriches the underlying linear framework.   The PI's prior work has laid foundations for this new subject via (1) mapping the boundary between the possible and impossible, the computable and non-computable; and (2) extending several matrix notions to tensors (e.g. eigenvalues, singular values, Schatten, Ky Fan norms, Perron-Frobeniuis theorem). This project takes the next step -- developing the requisite algorithms for problems that sit within these boundaries. While almost all 'algorithms' currently in use for tensor problems lack correctness and convergence guarantees, the ones developed for this project strive to be algorithms in the true sense of the word, i.e. meeting the basic requirement of convergence to a true solution and not just a stationary or fixed point. This is in general not possible but the PI will (i) identify large classes of interesting cases for which efficient, provably convergent algorithms exist; and (ii) exploit multilinearity and tap the existing rich collection of linear techniques. These design principles and goals will be applied to algorithmic development of all following problems: (a) Low rank tensor approximations; (b) eigenvalues and singular values for tensors; (c) systems of multilinear equations in the exact and least-squares sense. The PI will also demonstrate the utility of numerical multilinear algebra by proposing several new tools in telecommunications, bioinformatics, and neuroimaging.&lt;br/&gt;&lt;br/&gt;Almost all engineering, scientific, and statistical computing problems may ultimately be reduced to a handful of standard problems in linear algebra. &lt;br/&gt;Therefore one may argue that computational linear algebra is the workhorse of computations in science and engineering. This project is about expanding this fundamental arsenal of tools by going from "linear" to "multilinear". To achieve this, one must first realize that while there are natural extensions of the aforementioned handful of linear problems, there are also subtle difficulties and even impossibilities associated with these multilinear analogues. The crux of the project is to carve out a substantial tractable subclass of problems that are nevertheless still useful in applications. The project would also examine three concrete applications to brain imaging, cellular phone communication, and analytical chemistry.</AbstractNarration>
<MinAmdLetterDate>01/13/2011</MinAmdLetterDate>
<MaxAmdLetterDate>01/13/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1057064</AwardID>
<Investigator>
<FirstName>Lek-Heng</FirstName>
<LastName>Lim</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lek-Heng Lim</PI_FULL_NAME>
<EmailAddress>lekheng@galton.uchicago.edu</EmailAddress>
<PI_PHON>7737024263</PI_PHON>
<NSF_ID>000150703</NSF_ID>
<StartDate>01/13/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606372612</ZipCode>
<StreetAddress><![CDATA[6054 South Drexel Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~550000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="text-align: left;">In this project, we laid the foundations for numerical multilinear algebra, established the role of tensors within computational mathematics, and examined their applications to the design of fastest/stablest algorithms and to brain imaging. We will identify what we think are the five major contributions of this project as well as two auxiliary contributions that are potential topics for further developments.</p> <p style="text-align: left;">The first major contribution of this project is in the complexity of various generalizations of standard problems in numerical linear algebra. We showed that the higher-order multilinear generalizations of linear systems, least squares, matrix rank, matrix decomposition, eigenvalues, singular values, norms, determinants,&nbsp;positive definiteness, etc, are all NP-hard to compute (or intractable in ways other than NP-hardness).</p> <p style="text-align: left;">The second major contribution is in extending various matrix notions to hypermatrices, i.e., coordinate representations of tensors. These include eigenvalues, singular values, nuclear and spectral norms, and a Perron-Frobenius theory for nonnegative hypermatrices.</p> <p style="text-align: left;">The third major contribution is in relating existing studies in computational mathematics to tensors. Among other things, we related numerical stability to the tensor nuclear norm, complexity of structured matrix operations to tensor rank, self-concordance to positive definiteness of a tensor, etc. These allow us to show that deciding self-concordance is NP-hard, and to find provably fastest or stablest algorithms for various algebraic operations. Specific results include the fastest algorithms for symmetric matrix-vector multiply, BTTB matrix-vector multiply, and many other structured-matrix-vector multiplications, as well as an algorithm that is simultaneously fastest and stablest for&nbsp;multiplying a pair of complex numbers (the first improvement of the renown 200-year old algorithm of Gauss). The framework that we introduced also allows us to view existing algorithms (e.g., the fast integer multiplication algorithms of Karatsuba, Toom&ndash;Cook, Sch#onhage&ndash;Strassen, F#urer) in new light and thoroughly demystify them.</p> <p style="text-align: left;">The fourth major contribution is in our systematic study of nonnegative tensor rank using semialgebraic geometry, establishing properties such the uniqueness of decomposition and approximation, the typical and maximal nonnegative ranks, etc, and showing that every nonnegative tensor rank decomposition correspond to the&nbsp;decomposition of a joint probability density into conditional probability densities satisfying the naive Bayes hypothesis with minimal support.</p> <p style="text-align: left;">The fifth major contribution is in modeling diffusion-weighted MRI imaging problems as a problem of decomposing a H-PSD&nbsp;4-tensor. We showed that the problem in spatial dimensions, i.e., 3x3x3x3, is well-behaved in many ways and that when applied to real-world data, gives the most accurate reconstruction of diffusion MRI measurements of the human brain to date, capturing fine details in the neural fiber bundles of a living subject that are missed by other existing models.</p> <p style="text-align: left;">Our most significant contributions aside from the above are: (i) Quantifying the topology of tensor rank: determining questions of path-connectedness, fundamental and higher homotopy groups for sets defined by tensor rank, border rank, multilinear rank, and their symmetric counterparts. (ii) Laying down a rigorous definition of rank associated with tensor networks and studying its many properties and showing that the notion of matrix rank may essentially be generalized in a way that gives as many different notions of ranks as there are undirected graphs. These two other contributions we hope to further develop in future projects.</p><br> <p>            Last Modified: 05/01/2018<br>      Modified by: Lek-Heng&nbsp;Lim</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1057064/1057064_10068486_1525194708275_cc-posdef--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1057064/1057064_10068486_1525194708275_cc-posdef--rgov-800width.jpg" title="brain map"><img src="/por/images/Reports/POR/2018/1057064/1057064_10068486_1525194708275_cc-posdef--rgov-66x44.jpg" alt="brain map"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Images of neural fibers of a living person reconstructed from diffusion MRI measurements using an order-4 tensor model.</div> <div class="imageCredit">M. Ankele, L.-H. Lim, S. Groeschel, and T. Schultz</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Lek-Heng&nbsp;Lim</div> <div class="imageTitle">brain map</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1057064/1057064_10068486_1525194966647_complexity--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1057064/1057064_10068486_1525194966647_complexity--rgov-800width.jpg" title="most tensor problems are NP-hard"><img src="/por/images/Reports/POR/2018/1057064/1057064_10068486_1525194966647_complexity--rgov-66x44.jpg" alt="most tensor problems are NP-hard"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Most tensor problems are NP-hard.</div> <div class="imageCredit">C. Hillar and L.-H. Lim</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Lek-Heng&nbsp;Lim</div> <div class="imageTitle">most tensor problems are NP-hard</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[In this project, we laid the foundations for numerical multilinear algebra, established the role of tensors within computational mathematics, and examined their applications to the design of fastest/stablest algorithms and to brain imaging. We will identify what we think are the five major contributions of this project as well as two auxiliary contributions that are potential topics for further developments. The first major contribution of this project is in the complexity of various generalizations of standard problems in numerical linear algebra. We showed that the higher-order multilinear generalizations of linear systems, least squares, matrix rank, matrix decomposition, eigenvalues, singular values, norms, determinants, positive definiteness, etc, are all NP-hard to compute (or intractable in ways other than NP-hardness). The second major contribution is in extending various matrix notions to hypermatrices, i.e., coordinate representations of tensors. These include eigenvalues, singular values, nuclear and spectral norms, and a Perron-Frobenius theory for nonnegative hypermatrices. The third major contribution is in relating existing studies in computational mathematics to tensors. Among other things, we related numerical stability to the tensor nuclear norm, complexity of structured matrix operations to tensor rank, self-concordance to positive definiteness of a tensor, etc. These allow us to show that deciding self-concordance is NP-hard, and to find provably fastest or stablest algorithms for various algebraic operations. Specific results include the fastest algorithms for symmetric matrix-vector multiply, BTTB matrix-vector multiply, and many other structured-matrix-vector multiplications, as well as an algorithm that is simultaneously fastest and stablest for multiplying a pair of complex numbers (the first improvement of the renown 200-year old algorithm of Gauss). The framework that we introduced also allows us to view existing algorithms (e.g., the fast integer multiplication algorithms of Karatsuba, Toom&ndash;Cook, Sch#onhage&ndash;Strassen, F#urer) in new light and thoroughly demystify them. The fourth major contribution is in our systematic study of nonnegative tensor rank using semialgebraic geometry, establishing properties such the uniqueness of decomposition and approximation, the typical and maximal nonnegative ranks, etc, and showing that every nonnegative tensor rank decomposition correspond to the decomposition of a joint probability density into conditional probability densities satisfying the naive Bayes hypothesis with minimal support. The fifth major contribution is in modeling diffusion-weighted MRI imaging problems as a problem of decomposing a H-PSD 4-tensor. We showed that the problem in spatial dimensions, i.e., 3x3x3x3, is well-behaved in many ways and that when applied to real-world data, gives the most accurate reconstruction of diffusion MRI measurements of the human brain to date, capturing fine details in the neural fiber bundles of a living subject that are missed by other existing models. Our most significant contributions aside from the above are: (i) Quantifying the topology of tensor rank: determining questions of path-connectedness, fundamental and higher homotopy groups for sets defined by tensor rank, border rank, multilinear rank, and their symmetric counterparts. (ii) Laying down a rigorous definition of rank associated with tensor networks and studying its many properties and showing that the notion of matrix rank may essentially be generalized in a way that gives as many different notions of ranks as there are undirected graphs. These two other contributions we hope to further develop in future projects.       Last Modified: 05/01/2018       Submitted by: Lek-Heng Lim]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
