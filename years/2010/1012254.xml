<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Large: Collaborative Research:  Compact Representations and Efficient Algorithms for Distributed Geometric Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>432699.00</AwardTotalIntnAmount>
<AwardAmount>448539</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Across many fields of science, engineering, and business, massive data sets are being generated at unprecedented rate by high-bandwidth sensors and cameras, large-scale simulations, or web-enabled large scale data collection.  Much of this data has a geometric character, either directly or indirectly.   For example, second generation LiDARs can map the earth's surface at 15-20 cm resolution; the Large Synoptic Telescope is set to produce about 30 terabytes of data each night; thirteen hours of video are uploaded to YouTube every minute; Facebook manages over 40 billion photos requiring more than one petabyte of data.&lt;br/&gt;&lt;br/&gt;These data sets provide tremendous opportunities to enable novel capabilities that were unimaginable a few years ago.  Capitalizing on these opportunities, however, and transforming these massive amounts of heterogeneous data into useful information for vastly different types of applications and users requires solving challenging algorithmic problems.  An effective way of addressing this challenge is by designing efficient methods for producing informative yet succinct summaries of such geometric data sets.  These summaries must work at multiple scales, and allow a wide variety of queries to be answered approximately but efficiently.  The goal of this project is to study the theoretical underpinnings of compact representations and efficient algorithms for organizing, summarizing, cross-correlating, interlinking, and querying large distributed geometric data sets.&lt;br/&gt;&lt;br/&gt;This project will design methods for computing summaries of many kinds of flavors, all with provable properties.  Summaries can be combinatorial and metric (core sets and kernels), algebraic (linear sketches), topological (persistence diagrams), feature-based, and structural (encoding self-similarities in the data).  The properties they aim to capture extend from low-level metric attributes, such as the diameter or width of a point set, to higher-level attributes revealing the internal structure of the data, as in the detection of symmetries and repeated patterns.  This processing must be done in the presence of uncertainty in data coming from sensors, and optimize multiple performance measures, including communication cost for data distributed across multiple locations in a network.  Another key aspect of this project is that it aims to understand not individual data sets in isolation but rather the inter-relationships and correspondences among different data sets, and to do so by communicating only summary information, without even having all the data in one place. &lt;br/&gt;&lt;br/&gt;This work touches upon many topics in theoretical computer science and applied mathematics including low-distortion embeddings, compressive sensing, transportation metrics, spectral graph theory or harmonic analysis, machine learning, and computational topology.</AbstractNarration>
<MinAmdLetterDate>08/22/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/13/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1012254</AwardID>
<Investigator>
<FirstName>Pankaj</FirstName>
<LastName>Agarwal</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pankaj K Agarwal</PI_FULL_NAME>
<EmailAddress>pankaj@cs.duke.edu</EmailAddress>
<PI_PHON>9196606540</PI_PHON>
<NSF_ID>000462514</NSF_ID>
<StartDate>08/22/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress><![CDATA[2200 W. Main St, Suite 710]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7929</Code>
<Text>COMPUTATIONAL GEOMETRY</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7929</Code>
<Text>COMPUTATIONAL GEOMETRY</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~107699</FUND_OBLG>
<FUND_OBLG>2011~161972</FUND_OBLG>
<FUND_OBLG>2012~163028</FUND_OBLG>
<FUND_OBLG>2013~15840</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this collaborative project, with Leo Guibas (Stanford) and Piotr Indyk (MIT), was to design methods for computing compact summaries of massive data sets, such as large collections of images or audio signals. Such summaries preserve important properties of the data, but are significantly smaller than the full data sets. These summaries makes it possible to analyze and process them much more efficiently, leading to time, space, or energy savings.</p> <p>As a result of this project, several such summaries and associated algorithms have been discovered. Some of the summaries we discovered were general-purpose summaries (e.g. core sets) while others were targeted to specific applications. Many graduate and undergraduate students and two postdoctoral fellows were partially supported by this project. The PI had organized a workshop on &ldquo;big data&rdquo; in 2012, in which many talks had touched upon the topic of this project.</p> <p>Our contributions include the following:</p> <p><strong>Core sets:</strong> Core-sets are small subsets of the data with the property that the value of a given objective function of interest computed over the core-set is approximately the same as the value corresponding to the whole data set. The existence of small-size core-sets for a wide range of problems was proved &nbsp;by the PI (in collaboration with Har Peled and Varadarajan) about fifteen years ago. In this project, algorithms were developed for computing core sets with certain properties. First, we described a method for maintaining the core set that is stable under dynamic updates, i.e., as a new input object is inserted or an existing one is deleted, we can update the corset with O(1) changes in the core-set.&nbsp; We presented algorithms for maintaining core sets in a distributed fashion, which requires merging multiple core sets to compute the corset of the overall data set. We also adapted core sets to answer queries on large data sets, where one is willing to trade efficiency with accuracy &ndash; core sets enable us to reduce the size of the data structure for answering queries at the cost of answering a query approximately; the approximation error is tunable and can be set by the user.</p> <p><strong>Summaries for path planning:</strong> The introduction of sampling-based planners enabled solving motion-planning problems that were previously infeasible or inefficient. Specifically, for multi-query scenarios, planners such as the Probabilistic Roadmap Planner (PRM) approximate the connectivity of the free space by taking random samples from the Configuration Space (C-space) and connecting near-by free configurations when possible. The resulting data structure, the roadmap, is a graph where vertices represent configurations in the free space and edges are collision-free paths connecting two such configurations. We discovered algorithms for constructing a compact representation of the roadmap graph, without sacrificing the desirable guarantees on path quality.</p> <p><strong>Summarizing trajectory data:</strong> The continuing deployment of GPS devices has created an unprecedented amount of spatio-temporal trajectory data. These datasets offer great opportunities for enhancing our understanding of human mobility patterns, thus benefit many applications such as location-based services (LBS) and transportation system planning. However, due to the large size of the data and various sources of uncertainty, such as inaccurate GPS measurements and low sampling rates, an effective analysis of trajectory data is quite challenging.&nbsp; An interesting problem is to identify long paths that are traversed by many trajectories.&nbsp; We call these potions of paths as <em>fragments</em>.&nbsp; The canonical paths represent mobility patterns that are shared among many trajectories.&nbsp; We developed algorithms for computing compact representations of trajectories using fragme...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this collaborative project, with Leo Guibas (Stanford) and Piotr Indyk (MIT), was to design methods for computing compact summaries of massive data sets, such as large collections of images or audio signals. Such summaries preserve important properties of the data, but are significantly smaller than the full data sets. These summaries makes it possible to analyze and process them much more efficiently, leading to time, space, or energy savings.  As a result of this project, several such summaries and associated algorithms have been discovered. Some of the summaries we discovered were general-purpose summaries (e.g. core sets) while others were targeted to specific applications. Many graduate and undergraduate students and two postdoctoral fellows were partially supported by this project. The PI had organized a workshop on "big data" in 2012, in which many talks had touched upon the topic of this project.  Our contributions include the following:  Core sets: Core-sets are small subsets of the data with the property that the value of a given objective function of interest computed over the core-set is approximately the same as the value corresponding to the whole data set. The existence of small-size core-sets for a wide range of problems was proved  by the PI (in collaboration with Har Peled and Varadarajan) about fifteen years ago. In this project, algorithms were developed for computing core sets with certain properties. First, we described a method for maintaining the core set that is stable under dynamic updates, i.e., as a new input object is inserted or an existing one is deleted, we can update the corset with O(1) changes in the core-set.  We presented algorithms for maintaining core sets in a distributed fashion, which requires merging multiple core sets to compute the corset of the overall data set. We also adapted core sets to answer queries on large data sets, where one is willing to trade efficiency with accuracy &ndash; core sets enable us to reduce the size of the data structure for answering queries at the cost of answering a query approximately; the approximation error is tunable and can be set by the user.  Summaries for path planning: The introduction of sampling-based planners enabled solving motion-planning problems that were previously infeasible or inefficient. Specifically, for multi-query scenarios, planners such as the Probabilistic Roadmap Planner (PRM) approximate the connectivity of the free space by taking random samples from the Configuration Space (C-space) and connecting near-by free configurations when possible. The resulting data structure, the roadmap, is a graph where vertices represent configurations in the free space and edges are collision-free paths connecting two such configurations. We discovered algorithms for constructing a compact representation of the roadmap graph, without sacrificing the desirable guarantees on path quality.  Summarizing trajectory data: The continuing deployment of GPS devices has created an unprecedented amount of spatio-temporal trajectory data. These datasets offer great opportunities for enhancing our understanding of human mobility patterns, thus benefit many applications such as location-based services (LBS) and transportation system planning. However, due to the large size of the data and various sources of uncertainty, such as inaccurate GPS measurements and low sampling rates, an effective analysis of trajectory data is quite challenging.  An interesting problem is to identify long paths that are traversed by many trajectories.  We call these potions of paths as fragments.  The canonical paths represent mobility patterns that are shared among many trajectories.  We developed algorithms for computing compact representations of trajectories using fragments. We then proposed models based on these summaries to measure similarity between a pair of trajectories and to detect anomalous trajectories.  Contour trees: Contour tree can be regarded as a...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
