<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:  An Integrated Framework for Multimodal Music Search and Discovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2011</AwardEffectiveDate>
<AwardExpirationDate>01/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>550000.00</AwardTotalIntnAmount>
<AwardAmount>550000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A revolution in music production and distribution has made millions of songs instantly available to virtually anyone, on the Internet. However, a listener looking for "dark electronica with cello" or "music like U2's", without knowing a relevant artist or song name, or a musicologist wanting to search through large amounts of unknown ethnic music, would face serious challenges. Novel music search and discovery technologies are required to help users find the desired content.&lt;br/&gt;&lt;br/&gt;The non-text-based, multimodal character of Internet-wide information about music (audio clips, lyrics, web documents, images, band networks, etc.) poses a new and difficult challenge to existing database technology that depends on unimodal, text-based data-structures. This project addresses two fundamental research questions at the core of addressing this challenge: (1) The automated annotation of (non-text-based) audio content with descriptive keywords; and (2) the automated integration of the heterogeneous content of multimodal databases, to improve music search and discovery on the Internet or in a personal database. The resulting architecture leverages the automation and scalability of machine learning with the effectiveness of human computation, engaging music professionals or enthusiasts around the world.&lt;br/&gt;&lt;br/&gt;The research addresses questions at the core of multimedia information retrieval in general, enabling the design of a new generation of expressive and flexible retrieval systems for multimodal databases, with applications to music discovery, video retrieval, indexing multimedia content on the home PC, etc.&lt;br/&gt;&lt;br/&gt;The results of this project, including a software library and annotated music data sets, will be incorporated in ongoing education and outreach activities and disseminated via the project website (http://cosmal.ucsd.edu/~gert/CAREER.html) to enhance research and education in music information retrieval.</AbstractNarration>
<MinAmdLetterDate>01/19/2011</MinAmdLetterDate>
<MaxAmdLetterDate>04/23/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1054960</AwardID>
<Investigator>
<FirstName>Gert</FirstName>
<LastName>Lanckriet</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gert Lanckriet</PI_FULL_NAME>
<EmailAddress>gert.lanckriet@gmail.com</EmailAddress>
<PI_PHON>8585346976</PI_PHON>
<NSF_ID>000487588</NSF_ID>
<StartDate>01/19/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~107180</FUND_OBLG>
<FUND_OBLG>2012~112274</FUND_OBLG>
<FUND_OBLG>2013~118708</FUND_OBLG>
<FUND_OBLG>2014~101934</FUND_OBLG>
<FUND_OBLG>2015~109904</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project advanced fundamental research at the core of music search and recommendation. As millions of songs are now instantly available to millions of people around the world, the novel music search and discovery technologies invented during this project are making it easier for music listeners to find the music they enjoy in a given moment. The outcomes of this project advanced fundamental research in different areas: (i) the automated understanding of non-text based music content, i.e., &rdquo;Can we make computers listen to and understand audio clips, like humans do?&rdquo;, (ii) the understanding of user context based on smart phone and smart watch sensor measurements and (iii) how to leverage other than audio information to improve the understanding and retrieval of music content. Jointly, all of these are advancing our ability to recommend the right music to a listener at the right time. The research outcomes address questions at the core of multimedia information retrieval in general, with wide-spread practical applications &mdash; including, e.g., image and video search and recommendation, surveillance systems, copyright infringement detection. This project and the inherent fun-factor of music also catalyzed other, educational projects with high school, undergraduate and graduate students, introducing those students to research at UCSD.</p><br> <p>            Last Modified: 07/31/2020<br>      Modified by: Gert&nbsp;Lanckriet</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project advanced fundamental research at the core of music search and recommendation. As millions of songs are now instantly available to millions of people around the world, the novel music search and discovery technologies invented during this project are making it easier for music listeners to find the music they enjoy in a given moment. The outcomes of this project advanced fundamental research in different areas: (i) the automated understanding of non-text based music content, i.e., "Can we make computers listen to and understand audio clips, like humans do?", (ii) the understanding of user context based on smart phone and smart watch sensor measurements and (iii) how to leverage other than audio information to improve the understanding and retrieval of music content. Jointly, all of these are advancing our ability to recommend the right music to a listener at the right time. The research outcomes address questions at the core of multimedia information retrieval in general, with wide-spread practical applications &mdash; including, e.g., image and video search and recommendation, surveillance systems, copyright infringement detection. This project and the inherent fun-factor of music also catalyzed other, educational projects with high school, undergraduate and graduate students, introducing those students to research at UCSD.       Last Modified: 07/31/2020       Submitted by: Gert Lanckriet]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
