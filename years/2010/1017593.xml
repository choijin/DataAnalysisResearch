<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small:  A Computational Theory of Perceptual Integration in Multimodal Multitasking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>499591.00</AwardTotalIntnAmount>
<AwardAmount>515591</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Much contemporary computing is done in multitasking environments in which multiple visual and auditory displays compete for a person?s perceptual, cognitive (decision), and motor (movement) processing.  In some task domains, such as air-traffic control, emergency vehicle dispatching, and in-car navigation, multitasking cannot be avoided, and people need to interleave secondary tasks such as navigation with primary life-critical tasks such as driving.  To maximize overall human effectiveness, user interfaces intended for life-critical and time-critical complex multitasking need to be designed to account for a person?s ability to monitor and respond to multiple information sources in parallel.  And yet, there is little or no practical scientific theory to explain the human abilities, limitations, and strategies for multimodal (auditory and visual) multitasking.&lt;br/&gt;&lt;br/&gt;This project develops the science base needed for predictive modeling of human abilities to integrate across multiple modalities to accomplish multiple tasks in parallel.  The project will develop a theory of Perceptual Integration in Multimodal Multitasking with rigorous, detailed, high-fidelity computational cognitive modeling of carefully collected human data, including detailed eye movement data, for tasks that are positioned between the lab for high resolution tasks and data and the real world to insure practical application. The modeling will emphasize the role of central executive cognitive decisions for managing perceptual processing, moving the eyes, and coordinating motor responses to interleaved task demands.&lt;br/&gt;&lt;br/&gt;The project benefits society by studying and revealing the limitations to human multimodal multitasking performance at the core, and by providing theory that can be put to practice in the design of mission control centers, subway dispatching centers, emergency rooms, and computer systems for vehicles.  The theory will lead to practical design decisions that will improve the safety of subways, nuclear power plants, highways, hospitals, and vehicles.</AbstractNarration>
<MinAmdLetterDate>06/24/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/16/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1017593</AwardID>
<Investigator>
<FirstName>Michal</FirstName>
<LastName>Young</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michal T Young</PI_FULL_NAME>
<EmailAddress>michal@cs.uoregon.edu</EmailAddress>
<PI_PHON>5413464140</PI_PHON>
<NSF_ID>000434200</NSF_ID>
<StartDate>08/17/2012</StartDate>
<EndDate>09/25/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anthony</FirstName>
<LastName>Hornof</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anthony J Hornof</PI_FULL_NAME>
<EmailAddress>hornof@cs.uoregon.edu</EmailAddress>
<PI_PHON>5033461372</PI_PHON>
<NSF_ID>000298770</NSF_ID>
<StartDate>09/25/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Oregon Eugene</Name>
<CityName>Eugene</CityName>
<ZipCode>974035219</ZipCode>
<PhoneNumber>5413465131</PhoneNumber>
<StreetAddress>5219 UNIVERSITY OF OREGON</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079289626</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF OREGON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049793995</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Oregon Eugene]]></Name>
<CityName>Eugene</CityName>
<StateCode>OR</StateCode>
<ZipCode>974035219</ZipCode>
<StreetAddress><![CDATA[5219 UNIVERSITY OF OREGON]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~499591</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project developed a new&nbsp;understanding of multitasking through parallelized strategy exploration and individualized cognitive modeling. Human multitasking often involves complex task interactions and subtle tradeoffs which might be best understood through detailed computational cognitive modeling, yet traditional cognitive modeling approaches may not explore a sufficient range of task strategies to reveal the true complexity of multitasking behavior. The project identified a systematic approach for exploring a large number of strategies using a computer-cluster-based parallelized modeling system. The project&nbsp; demonstrates the efficacy of the approach for investigating and revealing the effects of different microstrategies on human performance, both within and across individuals, for a time-pressured multimodal dual task. The modeling results demonstrate that multitasking performance is not simply a matter of interleaving cognitive and sensorimotor processing but is instead heavily influenced by the selection of subtask microstrategies.</p> <div class="page" title="Page 2"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>The project developed a computational model of "active vision" for visual search in human&ndash;computer interaction. Human visual search plays an important role in many human&ndash;computer interaction (HCI) tasks. Better models of visual search are needed not just to predict overall performance outcomes, such as whether people will be able to find the information needed to complete an HCI task, but to understand the many human processes that interact in visual search, which will in turn inform the detailed design of better user interfaces. This project provided a detailed instantiation, in the form of a computational cognitive model, of a comprehensive theory of human visual processing known as &lsquo;&lsquo;active vision&rsquo;&rsquo; (Findlay &amp; Gilchrist, 2003). The computational model was built using the Executive Process-Interactive Control cognitive architecture. Eye-tracking data from three experiments inform the development and validation of the model. The modeling asks&mdash;and at least partially answers&mdash;the four questions of active vision: (a) What can be perceived in a fixation? (b) When do the eyes move? (c) Where do the eyes move? (d) What information is integrated between eye movements? Answers include: (a) Items nearer the point of gaze are more likely to be perceived, and the visual features of objects are sometimes misidentified. (b) The eyes move after the fixated visual stimulus has been processed (i.e., has entered working memory). (c) The eyes tend to go to nearby objects. (d) Only the coarse spatial information of what has been fixated is likely maintained between fixations. The model developed to answer these questions has both scientific and practical value in that the model gives HCI researchers and practitioners a better understanding of how people visually interact with computers, and provides a theoretical foundation for predictive analysis tools that can predict aspects of that interaction.&nbsp;</span></p> <p>The project established a easier way to conduct of&nbsp;post-hoc spatial recalibration of eye tracking data. The gaze locations reported by eye trackers often contain error resulting from a variety of sources. Such error is of increasing concern to eye tracking researchers, and several techniques have been introduced to clean up the error. These methods, however, either compensate only for error caused by a particular source (such as pupil dilation) or require the error to be somewhat constant across space and time. This project introduced a method that is applicable to error generated from a variety of sources and that is resilient to the change in error across the display. A study showed that, at least in some cases, although the change in error across the display appears to be random it in fact follows a consistent pattern which can be modeled using quadratic equations. The parameters of these equations can be estimated using linear regression on the error vectors between recorded fixations and possible target locations. The resulting equations can then be used to clean up the error. This regression-based approach is much easier to apply than some of the previously published methods. The method was applied to the data of a visual search experiment, and the results show that the regression-based error correction works very well.</p> </div> </div> </div> </div> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/05/2017<br>      Modified by: Anthony&nbsp;J&nbsp;Hornof</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project developed a new understanding of multitasking through parallelized strategy exploration and individualized cognitive modeling. Human multitasking often involves complex task interactions and subtle tradeoffs which might be best understood through detailed computational cognitive modeling, yet traditional cognitive modeling approaches may not explore a sufficient range of task strategies to reveal the true complexity of multitasking behavior. The project identified a systematic approach for exploring a large number of strategies using a computer-cluster-based parallelized modeling system. The project  demonstrates the efficacy of the approach for investigating and revealing the effects of different microstrategies on human performance, both within and across individuals, for a time-pressured multimodal dual task. The modeling results demonstrate that multitasking performance is not simply a matter of interleaving cognitive and sensorimotor processing but is instead heavily influenced by the selection of subtask microstrategies.      The project developed a computational model of "active vision" for visual search in human&ndash;computer interaction. Human visual search plays an important role in many human&ndash;computer interaction (HCI) tasks. Better models of visual search are needed not just to predict overall performance outcomes, such as whether people will be able to find the information needed to complete an HCI task, but to understand the many human processes that interact in visual search, which will in turn inform the detailed design of better user interfaces. This project provided a detailed instantiation, in the form of a computational cognitive model, of a comprehensive theory of human visual processing known as ??active vision?? (Findlay &amp; Gilchrist, 2003). The computational model was built using the Executive Process-Interactive Control cognitive architecture. Eye-tracking data from three experiments inform the development and validation of the model. The modeling asks&mdash;and at least partially answers&mdash;the four questions of active vision: (a) What can be perceived in a fixation? (b) When do the eyes move? (c) Where do the eyes move? (d) What information is integrated between eye movements? Answers include: (a) Items nearer the point of gaze are more likely to be perceived, and the visual features of objects are sometimes misidentified. (b) The eyes move after the fixated visual stimulus has been processed (i.e., has entered working memory). (c) The eyes tend to go to nearby objects. (d) Only the coarse spatial information of what has been fixated is likely maintained between fixations. The model developed to answer these questions has both scientific and practical value in that the model gives HCI researchers and practitioners a better understanding of how people visually interact with computers, and provides a theoretical foundation for predictive analysis tools that can predict aspects of that interaction.   The project established a easier way to conduct of post-hoc spatial recalibration of eye tracking data. The gaze locations reported by eye trackers often contain error resulting from a variety of sources. Such error is of increasing concern to eye tracking researchers, and several techniques have been introduced to clean up the error. These methods, however, either compensate only for error caused by a particular source (such as pupil dilation) or require the error to be somewhat constant across space and time. This project introduced a method that is applicable to error generated from a variety of sources and that is resilient to the change in error across the display. A study showed that, at least in some cases, although the change in error across the display appears to be random it in fact follows a consistent pattern which can be modeled using quadratic equations. The parameters of these equations can be estimated using linear regression on the error vectors between recorded fixations and possible target locations. The resulting equations can then be used to clean up the error. This regression-based approach is much easier to apply than some of the previously published methods. The method was applied to the data of a visual search experiment, and the results show that the regression-based error correction works very well.                    Last Modified: 05/05/2017       Submitted by: Anthony J Hornof]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
