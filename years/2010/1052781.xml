<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Communication, Perturbation, and Early Development</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>96029.00</AwardTotalIntnAmount>
<AwardAmount>104029</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Laura Namy</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Young infants typically form lasting, emotional attachments to their caregivers. The strength and type of these attachments are related to emotional well-being and cognitive development. This project will explore how face-to-face interactions between infants and adults contribute to this important aspect of child development.&lt;br/&gt;&lt;br/&gt;During early interactions, infants and parents form expectations about one another. Will a smile be answered with a bigger smile, for example, or with no smile at all? If the parent is asked to stop interacting and just look at her infant, will the infant smile or vocalize in an attempt to repair the interaction?  Do these early patterns of interaction predict the infant's later security of attachment--their ability to be comforted after a brief separation from the parent? To answer these questions, seventy-five infants and their mothers will participate in a standard "Face-To-Face/Still-Face"&lt;br/&gt;procedure at four months. Their security of attachment will then be assessed at&lt;br/&gt;12 months.&lt;br/&gt;&lt;br/&gt;It is difficult to measure early interactive behavior-and human behavior more generally-objectively and efficiently. To address this challenge, the project's interdisciplinary team of psychological and computer scientists will implement automated, quantitative measurements of behavior in the Face-To-Face/Still-Face procedure. Automated facial image analysis and pattern recognition approaches will be used to produce objective, continuous measurements of infant and mother facial expression, head motion, gaze direction, and vocalizations. Precise measurement of this multimodal suite of infant and mother behaviors will be used to tackle a fundamental scientific problem: Modeling the structure of early interaction and its relation to later development.&lt;br/&gt;&lt;br/&gt;This is a promising approach to understanding threats to typical development and learning associated with risk factors such as maternal depression and disorders such as autism. To maximize the project's impact, the team will make a database of audiovisual recordings, automated measurements, and pattern recognition and modeling software available to other scientists.</AbstractNarration>
<MinAmdLetterDate>06/14/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1052781</AwardID>
<Investigator>
<FirstName>Mohammad</FirstName>
<LastName>Mahoor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mohammad Mahoor</PI_FULL_NAME>
<EmailAddress>mmahoor@du.edu</EmailAddress>
<PI_PHON>3038713745</PI_PHON>
<NSF_ID>000511471</NSF_ID>
<StartDate>06/14/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Denver</Name>
<CityName>Denver</CityName>
<ZipCode>802104711</ZipCode>
<PhoneNumber>3038712000</PhoneNumber>
<StreetAddress>2199 S. University Blvd.</StreetAddress>
<StreetAddress2><![CDATA[Ofc of Research & Sponsored Prog]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431760</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLORADO SEMINARY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431760</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Denver]]></Name>
<CityName>Denver</CityName>
<StateCode>CO</StateCode>
<ZipCode>802104711</ZipCode>
<StreetAddress><![CDATA[2199 S. University Blvd.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1698</Code>
<Text>DS -Developmental Sciences</Text>
</ProgramElement>
<ProgramElement>
<Code>7750</Code>
<Text>CDI TYPE I</Text>
</ProgramElement>
<ProgramReference>
<Code>1698</Code>
<Text>DS-Developmental Sciences</Text>
</ProgramReference>
<ProgramReference>
<Code>7750</Code>
<Text>CDI TYPE I</Text>
</ProgramReference>
<ProgramReference>
<Code>7752</Code>
<Text>CDI NON SOLICITED RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~96029</FUND_OBLG>
<FUND_OBLG>2014~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Automated recognition of spontaneous facial expressions and action units (AUs) defined by Facial Action Coding System (FACS) has remained a challenging problem in computer vision. Especially measuring facial action units of children is more challenging compared to adults due to sever head motion and lack of facial textures in children's face. Such measurements are used by our collaborators in psychology departments to study infant's emotion development and its effect during mother-infant face-to-face communication. In this collaborative project the DU team concentrated on developing novel and reliable methods for detecting and measuring facial action units in infants&rsquo; facial videos. We learned that simultaneous recognition of AUs are more reliable than measuring AUs individually. The target AUs are those essential to positive and negative infant's emotions such as AU-6&nbsp; (cheek raiser), AU-12 (lip corner puller), and AU-20 (lip stretcher). Methods such as Multi-task learning and structural learning based on Support Vector Machines were developed and applied in our research and we learned that these machine learning methods are more reliable than methods that only targets AUs individually. In our studies we also examined different feature descriptors such as HOG, LBPH, and Gabor for facial image representations and we learned that perhaps a combination of features would work better than single features for AU detection and measurement.</p><br> <p>            Last Modified: 09/21/2015<br>      Modified by: Mohammad&nbsp;Mahoor</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Automated recognition of spontaneous facial expressions and action units (AUs) defined by Facial Action Coding System (FACS) has remained a challenging problem in computer vision. Especially measuring facial action units of children is more challenging compared to adults due to sever head motion and lack of facial textures in children's face. Such measurements are used by our collaborators in psychology departments to study infant's emotion development and its effect during mother-infant face-to-face communication. In this collaborative project the DU team concentrated on developing novel and reliable methods for detecting and measuring facial action units in infantsÃ† facial videos. We learned that simultaneous recognition of AUs are more reliable than measuring AUs individually. The target AUs are those essential to positive and negative infant's emotions such as AU-6  (cheek raiser), AU-12 (lip corner puller), and AU-20 (lip stretcher). Methods such as Multi-task learning and structural learning based on Support Vector Machines were developed and applied in our research and we learned that these machine learning methods are more reliable than methods that only targets AUs individually. In our studies we also examined different feature descriptors such as HOG, LBPH, and Gabor for facial image representations and we learned that perhaps a combination of features would work better than single features for AU detection and measurement.       Last Modified: 09/21/2015       Submitted by: Mohammad Mahoor]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
