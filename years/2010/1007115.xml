<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ExTENCI: Extending Science Through Enhanced National Cyberinfrastructure</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>2143231.00</AwardTotalIntnAmount>
<AwardAmount>2143231</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Barry I. Schneider</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>As the conduct of scientific and engineering research and the provisioning of cyberinfrastructure (CI) to support that research evolve and mature, new opportunities for leverage and technology sharing and codevelopment to provide increasing levels of support and services to the computationally-enabled research community are emerging.  TeraGrid (TG) and Open Science Grid (OSG) have been working over the past year to identify such opportunities, and  this project (ExTENCI: Extending science Through Enhanced National CyberInfrastructure)  will in part advance the support and capabilities provided to a set of key research activities that represent pathfinders in their respective communities in harnessing CI resources and services. &lt;br/&gt;&lt;br/&gt;ExTENCI will work with several representative research applications for which a limited but sustained CI effort will significantly increase the science they deliver. These applications span a range from large collaborative science to small research groups and include earthquake engineering, biology, protein structure, physics, and environmental studies. The  team will work with specialists from these research areas to exploit enhancements made in four technology areas: (1) Workflow and client tools that permit an application to exploit either TeraGrid and OSG resources, to use both simultaneously or to utilize new resources (e.g., clouds); (2) Distributed file systems operating across wide area networks that simplify access to and delivery of data; (3) Virtual machine technologies that can hide the complexity of application environments and allow them to run in developing environments such as clouds; (4) New job submission paradigms that utilize distributed grid resources more efficiently. &lt;br/&gt;&lt;br/&gt;One  important  goal of ExTENCI will enable the Southern California Earthquake Center (SCEC) to advance their hazard prediction curves by providing extended workflow capabilities and a new distributed storage solution, to support data transfer and to allow integrated use of OSG and TeraGrid resources appropriate for each part of the workflow. The extended workflow capabilities will also permit protein 3-D structures to be determined for significantly longer amino acid sequences.  The distributed storage mechanism will also significantly improve access by U.S. universities to Large Hadron Collider data and will simplify the sharing of simulated data by institutions participating in the Lattice Quantum Chromodynamics (LQCD) project. Additionally, ExTENCI's goal is to provide improved Virtual Machine (VM) and job submission capabilities to extend the range of CI resources available to applications as diverse as experiments at the Relativistic Heavy Ion Collider (RHIC) and oil reservoir simulations dependent on the Ensemble Kalman Filter algorithm.</AbstractNarration>
<MinAmdLetterDate>07/24/2010</MinAmdLetterDate>
<MaxAmdLetterDate>02/27/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1007115</AwardID>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Avery</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul R Avery</PI_FULL_NAME>
<EmailAddress>avery@phys.ufl.edu</EmailAddress>
<PI_PHON>3523929264</PI_PHON>
<NSF_ID>000182353</NSF_ID>
<StartDate>07/24/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ralph</FirstName>
<LastName>Roskies</LastName>
<PI_MID_INIT>Z</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ralph Z Roskies</PI_FULL_NAME>
<EmailAddress>roskies@psc.edu</EmailAddress>
<PI_PHON>4122684960</PI_PHON>
<NSF_ID>000196046</NSF_ID>
<StartDate>07/24/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Katz</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel S Katz</PI_FULL_NAME>
<EmailAddress>dskatz@illinois.edu</EmailAddress>
<PI_PHON>2172448000</PI_PHON>
<NSF_ID>000312962</NSF_ID>
<StartDate>07/24/2010</StartDate>
<EndDate>02/27/2012</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>GAINESVILLE</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress><![CDATA[1 UNIVERSITY OF FLORIDA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1221</Code>
<Text>HEP-High Energy Physics</Text>
</ProgramElement>
<ProgramElement>
<Code>7245</Code>
<Text>PHYSICS GRID COMPUTING</Text>
</ProgramElement>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>1221</Code>
<Text>ELEMENTARY PARTICLE ACCEL USER</Text>
</ProgramReference>
<ProgramReference>
<Code>7245</Code>
<Text>PHYSICS GRID COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7476</Code>
<Text>ETF</Text>
</ProgramReference>
<ProgramReference>
<Code>7684</Code>
<Text>STRATEGIC TECHNOLOGIES FOR CI</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~1229109</FUND_OBLG>
<FUND_OBLG>2011~914122</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><h2>Summary</h2> <p>The national Cyberinfrastructure (CI) consists of a nationwide network of high performance supercomputers originally provided by TeraGrid (TG) and now provided by the Extreme Science and Engineering Discovery Environment (XSEDE) and high throughput (large numbers of powerful microcomputer resources) provided by the Open Science Grid (OSG).&nbsp; The CI computers are located at universities and national laboratories.&nbsp; There are also experimental and commercial &ldquo;cloud&rdquo; computers that run applications embedded in Virtual Machines that contain the entire software system (e.g. Microsoft Windows or Linux).&nbsp; The CI is funded by the NSF, Department of Energy, and universities.&nbsp; Scientists, researchers, and engineers use the CI to do research to solve scientific and engineering problems using simulations.&nbsp; They also study and analyze data from experiments (such as from the Large Hadron Collider - LHC) and satellites (such as weather, ocean, land and climate data). &nbsp;ExTENCI&rsquo;s primary goal was to develop and provide production quality enhancements to the CI to enable specific science applications to more easily use both the OSG and TG/XSEDE or broaden access to a capability to both TG/XSEDE and OSG users.&nbsp; ExTENCI made enhancements in four areas: 1) Workflow and client tools that permit a software application to use OSG, XSEDE and cloud resources; 2) Distributed file systems that provide easy and fast access to scientific data across a wide area network connecting universities and national laboratories; 3) Virtual Machine technologies that hide complexity of applications and allow them to run on different cloud environments; and 4) Job submission paradigms that distribute work to computing resources.&nbsp; Eight institutions provided the ExTENCI project staff.&nbsp;</p> <h2>Intellectual Merit</h2> <p>ExTENCI enhanced the capability and expanded use in four areas and five tools in the CI:</p> <p>1)&nbsp;&nbsp; Workflow and Client Tools</p> <p>Swift &ndash; a parallel scripting language for running applications on the CI &ndash; was enhanced to:&nbsp; 1) dispatch work to either XSEDE or OSG; 2) increase the capacity to handle a larger number of simultaneous jobs; 3) take advantage of OSG&rsquo;s glidein pilot job system, and 4) make use of OSG&rsquo;s data services.&nbsp;</p> <p>2)&nbsp;&nbsp; Distributed File Systems</p> <p>Lustre/WAN &ndash; a filesystem that provides transparent access to data located at another location &ndash; was enhanced by adding Kerberos computer network user authentication security.&nbsp; ExTENCI set up hardware and software to support Lustre/WAN services at five sites and installed Lustre servers at two sites.&nbsp; Performance analysis/tuning was completed to enable CMS (LHC high energy physics experiment) science applications at two Florida Universities (FIU and FSU) to run on data from the University of Florida and Fermilab.&nbsp; Three major science applications were tested in this environment.</p> <p>3)&nbsp;&nbsp; Virtual Machines (VMs) and Cloud Technologies</p> <p>HTCondor &ndash; a specialized workload management system for compute-intensive jobs and the basis of the OSG - was improved to better support cloud services.&nbsp; ExTENCI created new tools for creating, managing, distributing VMs to clouds; invented a Cloud Dashboard that enables users to start, interact with, and stop VMs via a browser; and created two new cloud services.</p> <p>4)&nbsp;&nbsp; Job Submission Paradigms</p> <p>SAGA/BigJob &ndash; a standardized language for defining and running applications on the CI &ndash; was enhanced by designing and implementing an HTCondor adaptor to provide an interface to OSG thereby enabling SAGA and BigJob users to send application jobs to either XSEDE or OSG resources.&nbsp; BigJob was made more reliable and provided with a Gateway to enable scientists to set up pilot-based...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Summary  The national Cyberinfrastructure (CI) consists of a nationwide network of high performance supercomputers originally provided by TeraGrid (TG) and now provided by the Extreme Science and Engineering Discovery Environment (XSEDE) and high throughput (large numbers of powerful microcomputer resources) provided by the Open Science Grid (OSG).  The CI computers are located at universities and national laboratories.  There are also experimental and commercial "cloud" computers that run applications embedded in Virtual Machines that contain the entire software system (e.g. Microsoft Windows or Linux).  The CI is funded by the NSF, Department of Energy, and universities.  Scientists, researchers, and engineers use the CI to do research to solve scientific and engineering problems using simulations.  They also study and analyze data from experiments (such as from the Large Hadron Collider - LHC) and satellites (such as weather, ocean, land and climate data).  ExTENCIÆs primary goal was to develop and provide production quality enhancements to the CI to enable specific science applications to more easily use both the OSG and TG/XSEDE or broaden access to a capability to both TG/XSEDE and OSG users.  ExTENCI made enhancements in four areas: 1) Workflow and client tools that permit a software application to use OSG, XSEDE and cloud resources; 2) Distributed file systems that provide easy and fast access to scientific data across a wide area network connecting universities and national laboratories; 3) Virtual Machine technologies that hide complexity of applications and allow them to run on different cloud environments; and 4) Job submission paradigms that distribute work to computing resources.  Eight institutions provided the ExTENCI project staff.  Intellectual Merit  ExTENCI enhanced the capability and expanded use in four areas and five tools in the CI:  1)   Workflow and Client Tools  Swift &ndash; a parallel scripting language for running applications on the CI &ndash; was enhanced to:  1) dispatch work to either XSEDE or OSG; 2) increase the capacity to handle a larger number of simultaneous jobs; 3) take advantage of OSGÆs glidein pilot job system, and 4) make use of OSGÆs data services.   2)   Distributed File Systems  Lustre/WAN &ndash; a filesystem that provides transparent access to data located at another location &ndash; was enhanced by adding Kerberos computer network user authentication security.  ExTENCI set up hardware and software to support Lustre/WAN services at five sites and installed Lustre servers at two sites.  Performance analysis/tuning was completed to enable CMS (LHC high energy physics experiment) science applications at two Florida Universities (FIU and FSU) to run on data from the University of Florida and Fermilab.  Three major science applications were tested in this environment.  3)   Virtual Machines (VMs) and Cloud Technologies  HTCondor &ndash; a specialized workload management system for compute-intensive jobs and the basis of the OSG - was improved to better support cloud services.  ExTENCI created new tools for creating, managing, distributing VMs to clouds; invented a Cloud Dashboard that enables users to start, interact with, and stop VMs via a browser; and created two new cloud services.  4)   Job Submission Paradigms  SAGA/BigJob &ndash; a standardized language for defining and running applications on the CI &ndash; was enhanced by designing and implementing an HTCondor adaptor to provide an interface to OSG thereby enabling SAGA and BigJob users to send application jobs to either XSEDE or OSG resources.  BigJob was made more reliable and provided with a Gateway to enable scientists to set up pilot-based application runs via a browser interface.  Broad Impact  ExTENCI produced enhancements to the CI that were used by researchers in the science areas of glass material modeling, protein modeling, theoretical chemistry, earth systems science, molecular biophysics, high energy physics, ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
