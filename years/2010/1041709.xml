<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Augment the Kraken! Increasing the scientific capability and capacity of the NSF's most powerful supercomputer</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2011</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>2850000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Barry I. Schneider</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>1041709 &lt;br/&gt;Andrews&lt;br/&gt;This award 12 cabinets of compute nodes, with an additional 144 TF and 18 TB of memory, to Kraken, taking it to 1,174 TF and 147 TB. This will increase its node count from 8,256 to 9,408 by adding 1,152 nodes. With the addition of 1,152 nodes, Kraken will have significant room left to run other jobs simultaneously with 8,192 node jobs. Indeed, it will be possible to run a 8,192 job and a 1,024 node job at the same time. Given the ?bi-modal? distribution of the user jobs, with most job sizes at either 8,192 and above, or at 1,024 and below, this will greatly improve the responsiveness of the machine for users.  The Kraken upgrade will allow users to extend their codes by scaling them up to the largest NSF supercomputer, with over 110,000 cores, implementing new models that could not have been run before this upgrade. There already a number of users scaling their applications to the full size of the current Kraken, and some of them are already prepared to use a larger system. Additionally, this will help facilitate scaling for applications already chosen to run on the Track 1 Blue Waters system, and prepare them for that very large machine.</AbstractNarration>
<MinAmdLetterDate>08/20/2010</MinAmdLetterDate>
<MaxAmdLetterDate>04/28/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1041709</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Zacharia</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Zacharia</PI_FULL_NAME>
<EmailAddress>zachariat@ornl.gov</EmailAddress>
<PI_PHON>8655744897</PI_PHON>
<NSF_ID>000354743</NSF_ID>
<StartDate>08/20/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Patricia</FirstName>
<LastName>Kovatch</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patricia A Kovatch</PI_FULL_NAME>
<EmailAddress>patricia.kovatch@mssm.edu</EmailAddress>
<PI_PHON>2126598531</PI_PHON>
<NSF_ID>000506224</NSF_ID>
<StartDate>04/28/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Patricia</FirstName>
<LastName>Kovatch</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patricia A Kovatch</PI_FULL_NAME>
<EmailAddress>patricia.kovatch@mssm.edu</EmailAddress>
<PI_PHON>2126598531</PI_PHON>
<NSF_ID>000506224</NSF_ID>
<StartDate>08/20/2010</StartDate>
<EndDate>04/28/2011</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Philip</FirstName>
<LastName>Andrews</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Philip L Andrews</PI_FULL_NAME>
<EmailAddress>pandrew2@utk.edu</EmailAddress>
<PI_PHON>8659743466</PI_PHON>
<NSF_ID>000507500</NSF_ID>
<StartDate>08/20/2010</StartDate>
<EndDate>04/28/2011</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName>Knoxville</CityName>
<StateCode>TN</StateCode>
<ZipCode>379163801</ZipCode>
<StreetAddress><![CDATA[1331 CIR PARK DR]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7619</Code>
<Text>Innovative HPC</Text>
</ProgramElement>
<ProgramReference>
<Code>7619</Code>
<Text>EQUIPMENT ACQUISITIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~2850000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Kraken is the most popular and powerful system within the NSF&rsquo;s TeraGrid organization, and provides approximately 50% of the total available resources. It is an extremely usable and scalable system, delivering over 80% of peak on the High Performance Linpack benchmark, and ranked as the world&rsquo;s top Academic supercomputer, third overall, in the most recent Top500 list. Kraken was augmented with 12 cabinets of compute nodes, with an additional 144 TF and 18 TB of memory, taking it to 1,174 TF and 147 TB. This increased its node count from 8,256 to 9,408 by adding 1,152 nodes. With the shortfall in resources, the NSF scientific community badly needed this additional compute capacity, and the addition of 12 cabinets provided even more than that.</p> <p>&nbsp;</p> <p>Kraken runs a large proportion of &ldquo;full machine&rdquo; jobs, many of which use exactly 8,192 nodes. Previously, with only 8,256 compute nodes total, very little room was left on the system for smaller jobs. With the addition of 1,152 nodes, Kraken now has significant room left to run other jobs simultaneously with 8,192 node jobs. We now run a 8,192 job and a 1,024 node job at the same time. Given the &ldquo;bi-modal&rdquo; distribution of the user jobs, with most job sizes at either 8,192 and above, or at 1,024 and below, this has greatly improve the responsiveness of the machine for users. This was a very cost-effective upgrade, taking advantage of the already installed support infrastructure for Kraken, and it will operate the upgrade throughout the presently scheduled Kraken lifetime, i.e. through April 2012. This Kraken upgrade has allowed users to extend their codes by scaling them up to the largest NSF supercomputer, with over 110,000 cores, implementing new models that could not have been run before this upgrade.</p> <p>&nbsp;</p> <p>The larger machine has enabled easier access for classes and courses on Kraken. In particular, the great increase in headroom above the popular 8,192 node threshold has made it much easier to fit educational work into the machine while it is being used for capability computing. In the past, we have been unable to provide time for some classes because it would have prevented time-sensitive jobs at 8,192 nodes from running, but with the extra headroom, it is possible to cater to more of these requests.</p> <p>&nbsp;</p> <p>Under this award, Kraken enabled science successfully and reliably for the TeraGrid from February 2011 to June 2011. In this time, Kraken delivered over 359 million hours to scientists in 266,804 jobs, with over 97% uptime and 95% utilization. Over US 20 institutions used time on this machine, as allocated by the TeraGrid Resource Allocations Committee. This machine was used in classes at University of Tennessee-Knoxville, Brown University and Truman State University. NICS also participated in the Virtual School for Computational Science and Engineering, the TeraGrid/DEISA Summer School in HPC, the USA Science and Engineering Festival, Tapia, the National Society of Black Engineers, ESPCoR, the TeraGrid 2010 and Supercomputing 2010 conferences.</p><br> <p>            Last Modified: 09/26/2011<br>      Modified by: Patricia&nbsp;A&nbsp;Kovatch</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Kraken is the most popular and powerful system within the NSFÆs TeraGrid organization, and provides approximately 50% of the total available resources. It is an extremely usable and scalable system, delivering over 80% of peak on the High Performance Linpack benchmark, and ranked as the worldÆs top Academic supercomputer, third overall, in the most recent Top500 list. Kraken was augmented with 12 cabinets of compute nodes, with an additional 144 TF and 18 TB of memory, taking it to 1,174 TF and 147 TB. This increased its node count from 8,256 to 9,408 by adding 1,152 nodes. With the shortfall in resources, the NSF scientific community badly needed this additional compute capacity, and the addition of 12 cabinets provided even more than that.     Kraken runs a large proportion of "full machine" jobs, many of which use exactly 8,192 nodes. Previously, with only 8,256 compute nodes total, very little room was left on the system for smaller jobs. With the addition of 1,152 nodes, Kraken now has significant room left to run other jobs simultaneously with 8,192 node jobs. We now run a 8,192 job and a 1,024 node job at the same time. Given the "bi-modal" distribution of the user jobs, with most job sizes at either 8,192 and above, or at 1,024 and below, this has greatly improve the responsiveness of the machine for users. This was a very cost-effective upgrade, taking advantage of the already installed support infrastructure for Kraken, and it will operate the upgrade throughout the presently scheduled Kraken lifetime, i.e. through April 2012. This Kraken upgrade has allowed users to extend their codes by scaling them up to the largest NSF supercomputer, with over 110,000 cores, implementing new models that could not have been run before this upgrade.     The larger machine has enabled easier access for classes and courses on Kraken. In particular, the great increase in headroom above the popular 8,192 node threshold has made it much easier to fit educational work into the machine while it is being used for capability computing. In the past, we have been unable to provide time for some classes because it would have prevented time-sensitive jobs at 8,192 nodes from running, but with the extra headroom, it is possible to cater to more of these requests.     Under this award, Kraken enabled science successfully and reliably for the TeraGrid from February 2011 to June 2011. In this time, Kraken delivered over 359 million hours to scientists in 266,804 jobs, with over 97% uptime and 95% utilization. Over US 20 institutions used time on this machine, as allocated by the TeraGrid Resource Allocations Committee. This machine was used in classes at University of Tennessee-Knoxville, Brown University and Truman State University. NICS also participated in the Virtual School for Computational Science and Engineering, the TeraGrid/DEISA Summer School in HPC, the USA Science and Engineering Festival, Tapia, the National Society of Black Engineers, ESPCoR, the TeraGrid 2010 and Supercomputing 2010 conferences.       Last Modified: 09/26/2011       Submitted by: Patricia A Kovatch]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
