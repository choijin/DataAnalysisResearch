<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Integrating dynamic decision making with neurocontrollers by combining system and cognitive sciences</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>136645.00</AwardTotalIntnAmount>
<AwardAmount>136645</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Paul Werbos</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Project Summary&lt;br/&gt;&lt;br/&gt;The objective of this research is to develop new neural network structures to solve optimal control problems with dynamic decision making. These problems are quite complex since the system dynamics could switch modes at unknown times based on event based decision making. The approach is to develop the decision-making paradigms from cognitive science principles but their mathematical representations will use Decision Field Theory. Their solutions contained in neural networks will interact with another set of networks that embed solutions to the related optimal control problem formulated in an approximate dynamic programming framework.&lt;br/&gt;&lt;br/&gt;Intellectual Merit &lt;br/&gt;&lt;br/&gt;This research seeks to find unified controller solutions to problems which have both continuous and discrete elements in them. It is expected that the mathematical cognitive science ideas developed will lead to new representations and problem solving structures in computational neuroscience and control. The work proposed in this effort seeks to accomplish these objectives by offering a transformative approach that integrates concepts from system science and cognitive science. &lt;br/&gt;&lt;br/&gt;Broader Impact &lt;br/&gt;&lt;br/&gt;Abstractions and solution structures developed through this research can be used in consequence or emergency management systems like managing the aftermath of an earthquake, retrieving an impaired aircraft to stability and sustainable motion and landing, and managing multiple assets and allocation in striking responses to threats. Decision making structures resulting from this research can make tremendous impact on human-machine interactions too. For example, driver aid systems can be developed to augment human perception and enhance their cognition when they drive under impaired conditions.</AbstractNarration>
<MinAmdLetterDate>09/24/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/24/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1002188</AwardID>
<Investigator>
<FirstName>Jerome</FirstName>
<LastName>Busemeyer</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jerome R Busemeyer</PI_FULL_NAME>
<EmailAddress>jbusemey@indiana.edu</EmailAddress>
<PI_PHON>8128554882</PI_PHON>
<NSF_ID>000276216</NSF_ID>
<StartDate>09/24/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474013654</ZipCode>
<StreetAddress><![CDATA[509 E 3RD ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7607</Code>
<Text>EPCN-Energy-Power-Ctrl-Netwrks</Text>
</ProgramElement>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~136645</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>One of the key factors that provide the ability of human experts to make complex dynamic decisions is their facility in seamlessly moving between a discrete plan of large scale goals and continuous control over behavior to achieve these goals. My colleague, S. N. Balakrishnan, and I proposed to build on this key property and develop autonomous systems that have this facility to seamlessly move back and forth between discrete plans and smooth control.</p> <p>To accomplish this goal we examined human decision making and planning behavior in dynamic decision task called a predator - prey or goal seeking task. In this task, one agent (e.g. a predator) must seek a moving goal (e.g. a prey). To make the task amenable to mathematical modeling, we used a discrete time and discrete state virtual world called a grid world, in which the agents and goals move from cell in one time step to another cell in a large table of cells in the grid world. There are obstructions and penalties located in the grid world that the agent has to learn to avoid in order to capture to the goal. This predator prey problem is an example of a large class of dynamic decision problems called Markov decision problems.</p> <p>Optimal solutions to Markov decision problems can sometimes be solved using a a mathematical method called dynamic programming. However, sometimes the problem is too large to solve using this method. In any case, dynamic programming is not something a human could easily perform in a natural way. &nbsp;Instead, in many complex applications, it is necessary to use learning models that learn the optimal path from experience with a large number of learning trials in the environment. Models which learn to solve Markov decision problems from experience are called reinforcement learning models. &nbsp;</p> <p>One of the important issues for reinforcement learning models is the choice of exploring new paths to find a better path or exploiting previously learned paths to maximize the probability of catching the goal. This is called the exploration-exploitation problem in reinforcement learning. One of the traditional methods for doing this is to used a rule called the soft max rule that chooses steps in a path probabilistically. The problem with this traditional method is that it's performance is not robust, because it is highly dependent on a parameter called a temperature parameter, and the proper value for this parameter is usually difficult to determine for different environments.</p> <p>We developed a new solution to the exploration-exploitation problem using a new theory of decision making called quantum probability theory. This theory applies the mathematical principle from quantum theory to human decision making. We developed a new quantum reinforcement learning model that used a quantum rule for selecting steps along a path. The quantum algorithm proved to be more robust than the traditional soft max method.</p> <p>In fact, when two predators (one using the traditional soft max rule, and the other using the quantum rule) are placed in the same grid world to catch a single prey, the quantum algorithm most frequently outperforms and catches the prey before the traditional soft max agent.</p> <p>We also examined human performance on these tasks to see which learning model matches human behavior the best. We found that humans are capable of much faster replanning after being blocked than is possible by reinforcement learning models. This finding has motivated us to develop a different kind of learning model capable of rapid replanning.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/20/2014<br>      Modified by: Jerome&nbsp;R&nbsp;Busemeyer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ One of the key factors that provide the ability of human experts to make complex dynamic decisions is their facility in seamlessly moving between a discrete plan of large scale goals and continuous control over behavior to achieve these goals. My colleague, S. N. Balakrishnan, and I proposed to build on this key property and develop autonomous systems that have this facility to seamlessly move back and forth between discrete plans and smooth control.  To accomplish this goal we examined human decision making and planning behavior in dynamic decision task called a predator - prey or goal seeking task. In this task, one agent (e.g. a predator) must seek a moving goal (e.g. a prey). To make the task amenable to mathematical modeling, we used a discrete time and discrete state virtual world called a grid world, in which the agents and goals move from cell in one time step to another cell in a large table of cells in the grid world. There are obstructions and penalties located in the grid world that the agent has to learn to avoid in order to capture to the goal. This predator prey problem is an example of a large class of dynamic decision problems called Markov decision problems.  Optimal solutions to Markov decision problems can sometimes be solved using a a mathematical method called dynamic programming. However, sometimes the problem is too large to solve using this method. In any case, dynamic programming is not something a human could easily perform in a natural way.  Instead, in many complex applications, it is necessary to use learning models that learn the optimal path from experience with a large number of learning trials in the environment. Models which learn to solve Markov decision problems from experience are called reinforcement learning models.    One of the important issues for reinforcement learning models is the choice of exploring new paths to find a better path or exploiting previously learned paths to maximize the probability of catching the goal. This is called the exploration-exploitation problem in reinforcement learning. One of the traditional methods for doing this is to used a rule called the soft max rule that chooses steps in a path probabilistically. The problem with this traditional method is that it's performance is not robust, because it is highly dependent on a parameter called a temperature parameter, and the proper value for this parameter is usually difficult to determine for different environments.  We developed a new solution to the exploration-exploitation problem using a new theory of decision making called quantum probability theory. This theory applies the mathematical principle from quantum theory to human decision making. We developed a new quantum reinforcement learning model that used a quantum rule for selecting steps along a path. The quantum algorithm proved to be more robust than the traditional soft max method.  In fact, when two predators (one using the traditional soft max rule, and the other using the quantum rule) are placed in the same grid world to catch a single prey, the quantum algorithm most frequently outperforms and catches the prey before the traditional soft max agent.  We also examined human performance on these tasks to see which learning model matches human behavior the best. We found that humans are capable of much faster replanning after being blocked than is possible by reinforcement learning models. This finding has motivated us to develop a different kind of learning model capable of rapid replanning.          Last Modified: 11/20/2014       Submitted by: Jerome R Busemeyer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
