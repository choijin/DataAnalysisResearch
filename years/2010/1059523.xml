<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>UNCERTAINTY REDUCTION: The Guiding Principle of Attentional Allocation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2011</AwardEffectiveDate>
<AwardExpirationDate>02/28/2015</AwardExpirationDate>
<AwardTotalIntnAmount>407905.00</AwardTotalIntnAmount>
<AwardAmount>407905</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Attention is a central process in cognition. When people search through the environment for information relevant to their current goal, they are selectively processing perceptual information. Crucial to our understanding of attentional selection is determining what guides this selection. Until recently it has been observed that spatial locations are important for attentional guidance. However, recent evidence suggests that under some circumstances objects, not spatial locations, guide attentional selection. The full understanding of why under some circumstances such guidance is possible has remained elusive. This program of research introduces and tests a new unifying framework, uncertainty reduction, within which to examine factors that influence object-based attentional guidance. The generality of this framework is investigated, in several different paradigms, by systematically testing its predictive powers in two domains of uncertainty: internal uncertainty reduction as manipulated by reward and external uncertainty reduction as manipulated by varying the scope of attentional set.  In order to understand uncertainty reduction and its effects on object-based attentional guidance, behavioral profiles as well as the neural underpinnings of this mechanism are examined.&lt;br/&gt;                Whether reading a book in the peace and quiet of your living room or driving a car in traffic, the ability to pay attention to important aspects of the environment is an integral part of our lives and, ultimately, is crucial to our success and survival. A better understanding of critical factors that determine how attentional selection is deployed is important to diverse fields such as the design of interactive and ergonomic panels and enhanced training programs across multiple industries, ranging from training drivers and machine operators to security personnel (e.g., airport baggage screeners) to neurologists reading X-rays for evidence of cancer.</AbstractNarration>
<MinAmdLetterDate>02/18/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1059523</AwardID>
<Investigator>
<FirstName>Sarah</FirstName>
<LastName>Shomstein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sarah Shomstein</PI_FULL_NAME>
<EmailAddress>shom@gwu.edu</EmailAddress>
<PI_PHON>2029945957</PI_PHON>
<NSF_ID>000535750</NSF_ID>
<StartDate>02/18/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520086</ZipCode>
<StreetAddress><![CDATA[1922 F Street NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7969</Code>
<Text>FY 2010 Funding for PTR</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~89507</FUND_OBLG>
<FUND_OBLG>2012~318398</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There you are in the supermarket tracking down the ingredients for making your famous lasagna when your cell phone rings interrupting you. There are two kinds of attention at work here. Your active search for specific ingredients among the multitude of products in the supermarket unfolds by way of a voluntary top-down attentional system. When your cell phone rings, however, a stimulus determined bottom-up attention grabbing system stops you in your tracks. Without a fine balance between these two attentional systems, you would vacillate during the day between a kind of &ldquo;tunnel vision&rdquo; and a complete inability to focus on anything. And even as you do that balancing act, you still are totally unaware of the vast majority of stimuli relentlessly coming your way. To attend to anything is to simultaneously ignore mostly everything else. How do we manage that?</p> <p>&nbsp;Crucial to our understanding of attentional selection is determining what guides this selection. Until recently it has been observed that spatial locations are important for attentional guidance. However, recent evidence suggests that under some circumstances objects, and not spatial locations, guide attentional selection. The full understanding of why under some circumstances such guidance is possible has remained elusive. This program of research introduced, and tested, a new unifying framework within which to examine factors that influence object-based attentional was investigated, in several different paradigms, by systematically testing its predictive powers in two domains of uncertainty: internal uncertainty reduction as manipulated by reward (Aim 1) and external uncertainty reduction as manipulated by varying the scope of attentional set (Aim 2).</p> <p>&nbsp;Over the period covered under the proposal we have made several important contributions to our understanding of attentional selection. To summarize it briefly, we demonstrated that while attentional selection based on spatial information is a mandatory process in the human visual system, attentional selection based on object-based representations is a default setting. Object-based attentional selection contributes to attentional selection if, and only if, there is high degree of uncertainty in the visual input. To get back to the supermarket example, we demonstrated that if you are in a familiar environment (low degree of uncertainty), whether you are picking out ingredients, or picking up your cell phone, the selection of information is primarily based on spatial location of the incoming sensory stimuli. Importantly, however, if you are in the new and unfamiliar supermarket, and you do not remember whether your phone is in your jacket pocket, in your jeans, or in your computer bag (high degree of uncertainty), selection of information is going to be based on both spatial locations and object-based representations. &nbsp;</p> <p>&nbsp;More detailed findings can be found in 13 papers (7 published, 6 forthcoming), as well as over 25 conference presentations. &nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/05/2015<br>      Modified by: Sarah&nbsp;Shomstein</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There you are in the supermarket tracking down the ingredients for making your famous lasagna when your cell phone rings interrupting you. There are two kinds of attention at work here. Your active search for specific ingredients among the multitude of products in the supermarket unfolds by way of a voluntary top-down attentional system. When your cell phone rings, however, a stimulus determined bottom-up attention grabbing system stops you in your tracks. Without a fine balance between these two attentional systems, you would vacillate during the day between a kind of "tunnel vision" and a complete inability to focus on anything. And even as you do that balancing act, you still are totally unaware of the vast majority of stimuli relentlessly coming your way. To attend to anything is to simultaneously ignore mostly everything else. How do we manage that?   Crucial to our understanding of attentional selection is determining what guides this selection. Until recently it has been observed that spatial locations are important for attentional guidance. However, recent evidence suggests that under some circumstances objects, and not spatial locations, guide attentional selection. The full understanding of why under some circumstances such guidance is possible has remained elusive. This program of research introduced, and tested, a new unifying framework within which to examine factors that influence object-based attentional was investigated, in several different paradigms, by systematically testing its predictive powers in two domains of uncertainty: internal uncertainty reduction as manipulated by reward (Aim 1) and external uncertainty reduction as manipulated by varying the scope of attentional set (Aim 2).   Over the period covered under the proposal we have made several important contributions to our understanding of attentional selection. To summarize it briefly, we demonstrated that while attentional selection based on spatial information is a mandatory process in the human visual system, attentional selection based on object-based representations is a default setting. Object-based attentional selection contributes to attentional selection if, and only if, there is high degree of uncertainty in the visual input. To get back to the supermarket example, we demonstrated that if you are in a familiar environment (low degree of uncertainty), whether you are picking out ingredients, or picking up your cell phone, the selection of information is primarily based on spatial location of the incoming sensory stimuli. Importantly, however, if you are in the new and unfamiliar supermarket, and you do not remember whether your phone is in your jacket pocket, in your jeans, or in your computer bag (high degree of uncertainty), selection of information is going to be based on both spatial locations and object-based representations.     More detailed findings can be found in 13 papers (7 published, 6 forthcoming), as well as over 25 conference presentations.            Last Modified: 05/05/2015       Submitted by: Sarah Shomstein]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
