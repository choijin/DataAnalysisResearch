<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: New Techniques for Recognition and Visualization of Inscriptions on Papyrological Documents</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>99904.00</AwardTotalIntnAmount>
<AwardAmount>99904</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will apply innovative techniques of automated recognition to digital facsimiles of ancient Greek papyri. The project combines three new technological approaches in attempting to analyze the oldest surviving papyrus texts from the 3rd century BC. The first is the automated, statistical evaluation of letter-forms on ancient texts. The second is advanced automated workflows for high-definition digital imaging that result in registered datasets of multi-spectral 2d images aligned to 3d models of a document. And the third is new capabilities in digital library infrastructure that allow automated discovery and retrieval of digital images in registration with textual transcriptions based on generic queries and at a fine level of granularity. All of these involve high levels of computation combined with innovative methodologies. If successful the work can significantly extend and expand existing research capabilities for recovering content from ancient inscriptions, in cases where text is regularized with discrete characters, and dating of the source-documents is made easier by virtue of archaeological context.</AbstractNarration>
<MinAmdLetterDate>08/13/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1041949</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Blackwell</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher W Blackwell</PI_FULL_NAME>
<EmailAddress>Christopher.Blackwell@furman.edu</EmailAddress>
<PI_PHON>8642943468</PI_PHON>
<NSF_ID>000257133</NSF_ID>
<StartDate>08/13/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Furman University</Name>
<CityName>Greenville</CityName>
<ZipCode>296131000</ZipCode>
<PhoneNumber>8642943470</PhoneNumber>
<StreetAddress>3300 Poinsett Highway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044937407</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FURMAN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044937407</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Furman University]]></Name>
<CityName>Greenville</CityName>
<StateCode>SC</StateCode>
<ZipCode>296131000</ZipCode>
<StreetAddress><![CDATA[3300 Poinsett Highway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~99904</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Background</strong></p> <p>This project brought together computer scientists and scholars of ancient Greek to develop methods for measuring &ldquo;similiarity&rdquo; across ancient handwritten documents.&nbsp;</p> <p>We sought to focus on two very significant ancient works: Aristotle&rsquo;s &ldquo;Constitution of the Athenians&rdquo; and Alcman&rsquo;s &ldquo;Parthenaion&rdquo;. The first is a political science essay by the philosopher and scientist <a title="Aristotle on Wikipedia" href="http://en.wikipedia.org/wiki/Aristotle">Aristotle</a> from the 4th Century BCE describing the workings of Athenian Democracy. The second is a poem celebrating love and marriage by <a title="Alcman on Wikipedia" href="http://en.wikipedia.org/wiki/Alcman">the poet Alcman</a>. In both cases, these ancient works of literature survive <em>only</em> on a single, fragmentary papyrus.</p> <p>The first challenge was getting images of these texts, which are owned by European institutions. As publicly funded researchers, we insisted that we be allowed to share the digital images of these texts to the public free of charge. Negotiations took several years. In the meantime, we developed algorithms for detecting similiarity among handwritten manuscripts of the<a href="http://en.wikipedia.org/wiki/Iliad"> <em>Iliad</em>, Homer&rsquo;s poem about the Trojan War</a>. These were thought to have been written in Constantinople around 950 CE, but <a title="Escorial Upsilon 1.1" href="http://beta.hpcc.uh.edu/tomcat/hmtcite/browseimg?urn=urn:cite:hmt:e3img&amp;limit=9&amp;offset=18">one resides in a library near Madrid, Spain</a>, and <a title="The Venetus B" href="http://beta.hpcc.uh.edu/tomcat/hmtcite/browseimg?urn=urn:cite:hmt:vbimg">one in a library in Venice, Italy</a>.</p> <p>We chose the papyrus documents and the two <em>Iliad</em> manuscripts because human readers have noted their similiarity. The goal was to apply numerical values to this subjective sense of &ldquo;similarity&rdquo;.</p> <p><strong>Outcome: Sharing Information</strong></p> <p>In order to work with large collections of digital images, collaborating between the United States and Greece, we developed&nbsp;<a title="Four URLs" href="http://folio.furman.edu/projects/cite/four_urls.html">a digital-library infrastructure that allows citation of images and parts of images</a>. &ldquo;Citation&rdquo; is important, because it allows us to compare particular regions of images, efficiently, in a way that is not dependant on any particular technology.</p> <p>If we wanted to point to the word &phi;&iota;&lambda;&omicron;&iota;&sigmaf; (&ldquo;to the dear ones&rdquo;) on the Alcman papyrus, we can identify an image with a URN, e.g. <strong>&nbsp;urn:cite:fufolioimg:alcman.gp100644</strong>, and add a &ldquo;region of interest&rdquo;, e.g. <strong>@0.1471,0.5067,0.0601,0.0334</strong>. By passing that citation to an image-service, we can see&nbsp;<a title="An Excerpt" href="http://folio.furman.edu/citeservlet/images?request=GetBinaryImage&amp;w=6000&amp;urn=urn:cite:fufolioimg:alcman.gp100644@0.1471,0.5067,0.0601,0.0334">a excerpt of an image</a>, or we can see<a title="Image ROI in context" href="http://folio.furman.edu/citeservlet/images?request=GetIIPMooViewer&amp;urn=urn:cite:fufolioimg:alcman.gp100644@0.1471,0.5067,0.0601,0.0334"> the whole image, with the excerpt highlighted</a>.</p> <p>Because we were able to negotiate open-content licenses with the British Museum and the Louvre, <a title="The Project Site at Folio.furman.edu" href="http://folio.furman.edu/projects/AthPol/index.html">we are able to make these unique documents available to anyone who would like to study them, using this efficient networked architecture</a>.</p> <p><strong>Outcome: Measuring &ldquo;Similarity&rdquo;</strong></p> <p>The <a title="Pattern Recognition Pub" href="http://www.sciencedirect.com/science/article/pii/S0031320313000563">first publication of...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Background  This project brought together computer scientists and scholars of ancient Greek to develop methods for measuring "similiarity" across ancient handwritten documents.   We sought to focus on two very significant ancient works: AristotleÆs "Constitution of the Athenians" and AlcmanÆs "Parthenaion". The first is a political science essay by the philosopher and scientist Aristotle from the 4th Century BCE describing the workings of Athenian Democracy. The second is a poem celebrating love and marriage by the poet Alcman. In both cases, these ancient works of literature survive only on a single, fragmentary papyrus.  The first challenge was getting images of these texts, which are owned by European institutions. As publicly funded researchers, we insisted that we be allowed to share the digital images of these texts to the public free of charge. Negotiations took several years. In the meantime, we developed algorithms for detecting similiarity among handwritten manuscripts of the Iliad, HomerÆs poem about the Trojan War. These were thought to have been written in Constantinople around 950 CE, but one resides in a library near Madrid, Spain, and one in a library in Venice, Italy.  We chose the papyrus documents and the two Iliad manuscripts because human readers have noted their similiarity. The goal was to apply numerical values to this subjective sense of "similarity".  Outcome: Sharing Information  In order to work with large collections of digital images, collaborating between the United States and Greece, we developed a digital-library infrastructure that allows citation of images and parts of images. "Citation" is important, because it allows us to compare particular regions of images, efficiently, in a way that is not dependant on any particular technology.  If we wanted to point to the word &phi;&iota;&lambda;&omicron;&iota;&sigmaf; ("to the dear ones") on the Alcman papyrus, we can identify an image with a URN, e.g.  urn:cite:fufolioimg:alcman.gp100644, and add a "region of interest", e.g. @0.1471,0.5067,0.0601,0.0334. By passing that citation to an image-service, we can see a excerpt of an image, or we can see the whole image, with the excerpt highlighted.  Because we were able to negotiate open-content licenses with the British Museum and the Louvre, we are able to make these unique documents available to anyone who would like to study them, using this efficient networked architecture.  Outcome: Measuring "Similarity"  The first publication of our results describes the algorithms we applied to character-forms from these manuscripts; it also includes mathematical proofs of the propositions on which the algorithms are based. The work involved the following steps:  Assembling a catalogue of letterforms.  Capturing their outlines (difficult with faded handwritten texts, where the ink often "bleeds" into the underlying parchment). Instructing the computer to transform the outline of a letter from one manuscript so that it matches the outline of that letter in another manuscript, deforming the outline in both width and height at many points in its curve. Analyzing the amount of effort, and the nature of effort, required to force one outline onto another. Comparing the resulting values with values from a control set, and calculating a statistical result.   The initial outcome was very promising. From a double-blind set of anonymized image fragments from manuscripts, the algorithm correctly recognized images that were from the same manuscript (a good first step), and suggested that samples from the Venetian and Spanish Iliads might also be the same.  Further Work  We secured images of the Alcman fragment only in the summer of 2013; we will test our algorithms against the two papyri next. Since "similiarity" is a matter of context, we plan to extend these algorithms to consider not only individual letters, but pairs of letters in their linguistic context. By aligning our image-citations with textual-citations, we can i...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
