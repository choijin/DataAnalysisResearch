<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Multisensory Perceptual Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>238675.00</AwardTotalIntnAmount>
<AwardAmount>238675</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Cleary</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Ladan Shams, University of California at Los Angeles&lt;br/&gt;Aaron Seitz, University of California at Riverside&lt;br/&gt;&lt;br/&gt;COLLABORATIVE RESEARCH: MULTISENSORY PERCEPTUAL LEARNING&lt;br/&gt;&lt;br/&gt;ABSTRACT&lt;br/&gt;&lt;br/&gt;In daily life, we frequently experience correlated sensations across our different sensory modalities. For example, as we climb up the stairs, we receive sensations to our auditory, visual, tactile, and vestibular systems that are all related to the experience of stair-climbing. These types of multisensory experiences are a key aspect of how we interact with, and learn about, the world around us. Through our experience with the world, our sensory abilities undergo refinements that allow us to optimize our performance in the tasks that we perform. These sensory refinements involve fine-tuning of processing in each of our sensory modalities, but equally importantly, in how we merge information across modalities. While much research has focused on how learning can take place within each individual sensory system, the learning of how information is combined across the senses has been largely neglected. The PIs will conduct a series of experiments in which they can track visual, auditory, and auditory-visual multisensory learning in parallel, and discriminate among different theories of multisensory processing and learning. Behavioral and neuroimaging methods will be combined to shed light on the roles that different brain areas, and the interactions between brain areas, play in the process of multisensory learning. Altogether these studies will provide fundamental insights into how our sensory systems work together and refine their interactions to best operate in the tasks that we perform.&lt;br/&gt;&lt;br/&gt;This project will be the first systematic investigation of multisensory perceptual learning. It will also be the first study of changes in interaction between brain areas that may occur as a result of sensory learning. Altogether, this study promises to provide foundational knowledge regarding the brain mechanisms involved in multisensory learning as well as the mechanisms of learning in general. Understanding multisensory learning can contribute to the development of more effective strategies for learning. These strategies can be utilized to enhance learning for typically-developed children and adults, as well as to facilitate learning and communication for individuals with deprivation in one sense (e.g., individuals with low-vision or low-hearing, patients with cochlear implants or undergoing macular degeneration or cataract surgeries). They can also contribute to devising remedial programs for dyslexia, which appears to involve deficits in combining information across the senses.</AbstractNarration>
<MinAmdLetterDate>03/31/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/16/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1057625</AwardID>
<Investigator>
<FirstName>Aaron</FirstName>
<LastName>Seitz</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aaron R Seitz</PI_FULL_NAME>
<EmailAddress>aseitz@ucr.edu</EmailAddress>
<PI_PHON>9518276422</PI_PHON>
<NSF_ID>000195088</NSF_ID>
<StartDate>03/31/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Riverside</Name>
<CityName>RIVERSIDE</CityName>
<ZipCode>925210217</ZipCode>
<PhoneNumber>9518275535</PhoneNumber>
<StreetAddress>Research &amp; Economic Development</StreetAddress>
<StreetAddress2><![CDATA[245 University Office Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>44</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA44</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>627797426</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Riverside]]></Name>
<CityName>RIVERSIDE</CityName>
<StateCode>CA</StateCode>
<ZipCode>925210217</ZipCode>
<StreetAddress><![CDATA[Research &amp; Economic Developm]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>44</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA44</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~76175</FUND_OBLG>
<FUND_OBLG>2012~162500</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Normal-first">Perception is multisensory by default. At any given moment the human nervous system receives information from multiple sensory modalities, and has to integrate these signals in order to achieve a unified percept of the environment. In the realm of perception research, on the other hand, perception has traditionally been viewed as a modular function with different sensory modalities operating independently. However, psychophysical and neurophysiological studies indicate strong interactions between the perceptual processes of different modalities that influence the earliest stages of perceptual processing.</p> <p class="Normal-indent">Here we investigated processes of multisensory learning. Results of multiple studies conducted in the project period explored different aspects of multisensory learning.</p> <p class="Normal-indent">In one series of studies we investigated crossmodal transfer where training a task in one modality can transfer to sensitivity improvements in another sensory modality. For example training visual orientation discrimination transfers to haptic performance on the same task. In some cases training in one sense is even more effective that training that task in the target sense. For example training on an auditory rhythm task produces better visual learning than training directly on the visual task. These studies help us understand how the different senses interact and suggest novel training approaches to target training to the sensory modality that is most appropriate to learn a given task regardless of what modality typical task performance will be based upon.</p> <p class="Normal-indent">In other studies we examined impacts of training multiple senses together. These studies help inform us how the senses can work together during a normal learning process. In tasks for which vision provides the clearest information (such as distance estimation) we found that training on an auditory-visual distance estimation task produced substantial shift in perceived distance estimation from just auditory cues suggesting that visual training is a useful method of training auditory distance estimation. In the other direction we found that the addition of auditory location cues during a visual therapy designed to improve acuity and contrast sensitivity contributed to substantial improvements of visual processing that transferred to real world visual tasks (such as reading and playing baseball). Together these results show that multisensory training, when properly conducted, leads to superior learning.</p> <p class="Normal-indent">Together, our results show that understanding multisensory learning can contribute to the development of more effective strategies for learning. These strategies can be utilized to enhance learning for typically-developed children and adults, as well as to facilitate learning and communication for individuals with sensory or cognitive deficits.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/17/2014<br>      Modified by: Aaron&nbsp;R&nbsp;Seitz</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Perception is multisensory by default. At any given moment the human nervous system receives information from multiple sensory modalities, and has to integrate these signals in order to achieve a unified percept of the environment. In the realm of perception research, on the other hand, perception has traditionally been viewed as a modular function with different sensory modalities operating independently. However, psychophysical and neurophysiological studies indicate strong interactions between the perceptual processes of different modalities that influence the earliest stages of perceptual processing. Here we investigated processes of multisensory learning. Results of multiple studies conducted in the project period explored different aspects of multisensory learning. In one series of studies we investigated crossmodal transfer where training a task in one modality can transfer to sensitivity improvements in another sensory modality. For example training visual orientation discrimination transfers to haptic performance on the same task. In some cases training in one sense is even more effective that training that task in the target sense. For example training on an auditory rhythm task produces better visual learning than training directly on the visual task. These studies help us understand how the different senses interact and suggest novel training approaches to target training to the sensory modality that is most appropriate to learn a given task regardless of what modality typical task performance will be based upon. In other studies we examined impacts of training multiple senses together. These studies help inform us how the senses can work together during a normal learning process. In tasks for which vision provides the clearest information (such as distance estimation) we found that training on an auditory-visual distance estimation task produced substantial shift in perceived distance estimation from just auditory cues suggesting that visual training is a useful method of training auditory distance estimation. In the other direction we found that the addition of auditory location cues during a visual therapy designed to improve acuity and contrast sensitivity contributed to substantial improvements of visual processing that transferred to real world visual tasks (such as reading and playing baseball). Together these results show that multisensory training, when properly conducted, leads to superior learning. Together, our results show that understanding multisensory learning can contribute to the development of more effective strategies for learning. These strategies can be utilized to enhance learning for typically-developed children and adults, as well as to facilitate learning and communication for individuals with sensory or cognitive deficits.          Last Modified: 06/17/2014       Submitted by: Aaron R Seitz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
