<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NetSE: Medium: Collaborative Research: Auditing Internet Content for Credibility, Fairness, and Privacy</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2011</AwardEffectiveDate>
<AwardExpirationDate>04/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>549452.00</AwardTotalIntnAmount>
<AwardAmount>549452</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Millions of users are accessing a portion of billions of Web pages and other content on the Internet on a daily basis. While the networking research community and the public in general are well-aware of the net neutrality problem, i.e., how to develop regulatory policies and auditing mechanisms to prevent Internet Service Providers from discriminating against various applications, very little effort is invested in enabling content neutrality. In particular, it is not a secret that almost every browsing click we make is collected by either Web- or ISP-based 'information collectors and aggregators', and that our profiles are used for online advertising. Still, no public auditing mechanisms, capable of detecting and informing end users about such practices, exist in this emerging area. Take search engines as another example. How does one know that information available on these services is not biased (or will become biased in the future) for commercial, political, or any other reason? More generally, no public auditing systems are capable of monitoring the scope and effectiveness of advertising or spam campaigns on the Internet. The PIs argue that all these questions fundamentally affect fairness at all levels, information credibility, and user privacy, all of which have significant relevance for the future development of the Internet and beyond.&lt;br/&gt;&lt;br/&gt;The PIs will build a set of methodologies and tools unified in a system capable of (i) enabling auditing mechanisms for the Web advertising domain, (ii) monitoring search engines? services and revealing their neutrality, and (iii) independently determining a Web site's popularity and checking for the truthfulness of advertised popularity. Further, the PIs plan to deploy the developed system on the PlanetLab wide area network testbed to perform long term monitoring of content neutrality in the Internet.&lt;br/&gt;&lt;br/&gt;Broader Impact: By informing the public of how online advertisers and search engines behave, the auditing system will enable fair competition and preclude monopolies, oligopolies, or collusions, eventually helping to improve online information credibility and user privacy. Even if no bias nor privacy-violating practices were to exist in today's Internet (unfortunately, the PI's preliminary results show that this is not the case), the auditing system and methodologies will always be a barrier for anyone who would believe that applying such approaches might be done without public knowledge and repercussions. By exposing biased content behavior, the auditing system will be capable of revealing discriminating acts that can happen for commercial, social, political, religious, or any other reason, ultimately facilitating the development of free and open society. The PIs will design and disseminate easy-to-use browser extensions and plug-ins that will be capable of assisting end users in detecting content neutrality violations. Education is an integral part of this award. The insights and tools derived from this project will be integrated into the current undergraduate and graduate curricula.</AbstractNarration>
<MinAmdLetterDate>03/28/2011</MinAmdLetterDate>
<MaxAmdLetterDate>04/12/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1064595</AwardID>
<Investigator>
<FirstName>Aleksandar</FirstName>
<LastName>Kuzmanovic</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aleksandar Kuzmanovic</PI_FULL_NAME>
<EmailAddress>akuzma@northwestern.edu</EmailAddress>
<PI_PHON>8474675519</PI_PHON>
<NSF_ID>000491611</NSF_ID>
<StartDate>03/28/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Douglas</FirstName>
<LastName>Downey</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Douglas C Downey</PI_FULL_NAME>
<EmailAddress>dougd@allenai.org</EmailAddress>
<PI_PHON>8474913710</PI_PHON>
<NSF_ID>000534948</NSF_ID>
<StartDate>03/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7794</Code>
<Text>NETWORK SCIENCE &amp; ENGINEERING</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~549452</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>Broader Impact:</p> <p>Millions of users are accessing a portion of billions of Web pages and other content on the Internet on a daily basis. While the networking research community and the public in general are well-aware of the <em>net neutrality </em>problem, <em>i.e.</em>, how to develop regulatory policies and auditing mechanisms to prevent Internet Service Providers from discriminating against various applications, very little<em> </em>effort is invested in enabling <em>content neutrality</em>. In particular, how do we know that search engines and other content providers are not biased (or will become biased in the future) for commercial, political, or any other reason? Certainly, the most persistent critique of search engines is the fear that they create reality instead of reflecting it. More generally, no public auditing systems were capable of monitoring and understanding when and how numerous online entities track users. The key contribution of this project lies in developing some of the first methods for auditing content neutrality and designing systems for monitoring user tracking. In particular, we developed content auditing systems for the following three domains.</p> <p>&nbsp;</p> <p>Intellectual Merit:</p> <p><em>Enabling auditing mechanisms for the Web advertising domain. </em>We developed novel monitoring methods capable of accurately detecting when a user is targeted on individual or group bases, which includes detecting both <em>behavioral ad targeting </em>and <em>location-based ad</em> <em>targeting</em>. A common insight of our research is that there exists a significant lack of coordination among different entities in the online advertising domain, even in scenarios when different entities belong to the same organization. Our system was among the first to analyze and reveal the properties and the deployment of location-based and behavioral targeting methods at various levels of granularity. To detect behavioral targeting, we developed inconsistency tests for cases when cookies are utilized by an online service and when they are removed by the client.</p> <p>&nbsp;</p> <p><em>Auditing search engines. </em>Auditing search engines and detecting potential bias is a challenging research problem for several reasons. Most significantly, there exists no accepted notion of what unbiased ranking for a given search term represents. Indeed, each search engine has its own set of internal ranking rules and policies that produce a given ranking outcome. We have addressed this problem in the following ways. (a) By detecting search-engine response <em>inconsistency</em>, in the context of a single search engine, among different keywords and topics. To detect bias, we train our model on benign keywords and then evaluate if the learned model deviates for less-benign keywords, potential bias targets. (b) We continued our efforts on building a Platform for Analyzing Web Search engines (PAWS). PAWS measures content emphasis, i.e., the degree to which differences across search engines&rsquo; rankings correlate with features of the ranked content. We find that neither Google nor Bing are emphasizing positive sentiments toward their company&rsquo;s own products; that <em>both</em> search engines <em>do </em>rank statistically significantly higher pages when it contains the engine company&rsquo;s ads, as opposed to competitor ads; that there exists statistically significant news search bias across different hosts, indicating that the two search engines often prefer different hosts.</p> <p>&nbsp;</p> <p><em>Auditing Web sites&rsquo; popularity and Internet campaigns</em>. We developed methodologies to audit Web sites&rsquo; popularity at <em>scale </em>and to monitor advertising and spam campaigns on the Internet. Among others, we developed the first system that enables an endpoint to <em>remotely</em> measure traffic of a Website. In addition, we conduc...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Broader Impact:  Millions of users are accessing a portion of billions of Web pages and other content on the Internet on a daily basis. While the networking research community and the public in general are well-aware of the net neutrality problem, i.e., how to develop regulatory policies and auditing mechanisms to prevent Internet Service Providers from discriminating against various applications, very little effort is invested in enabling content neutrality. In particular, how do we know that search engines and other content providers are not biased (or will become biased in the future) for commercial, political, or any other reason? Certainly, the most persistent critique of search engines is the fear that they create reality instead of reflecting it. More generally, no public auditing systems were capable of monitoring and understanding when and how numerous online entities track users. The key contribution of this project lies in developing some of the first methods for auditing content neutrality and designing systems for monitoring user tracking. In particular, we developed content auditing systems for the following three domains.     Intellectual Merit:  Enabling auditing mechanisms for the Web advertising domain. We developed novel monitoring methods capable of accurately detecting when a user is targeted on individual or group bases, which includes detecting both behavioral ad targeting and location-based ad targeting. A common insight of our research is that there exists a significant lack of coordination among different entities in the online advertising domain, even in scenarios when different entities belong to the same organization. Our system was among the first to analyze and reveal the properties and the deployment of location-based and behavioral targeting methods at various levels of granularity. To detect behavioral targeting, we developed inconsistency tests for cases when cookies are utilized by an online service and when they are removed by the client.     Auditing search engines. Auditing search engines and detecting potential bias is a challenging research problem for several reasons. Most significantly, there exists no accepted notion of what unbiased ranking for a given search term represents. Indeed, each search engine has its own set of internal ranking rules and policies that produce a given ranking outcome. We have addressed this problem in the following ways. (a) By detecting search-engine response inconsistency, in the context of a single search engine, among different keywords and topics. To detect bias, we train our model on benign keywords and then evaluate if the learned model deviates for less-benign keywords, potential bias targets. (b) We continued our efforts on building a Platform for Analyzing Web Search engines (PAWS). PAWS measures content emphasis, i.e., the degree to which differences across search enginesÆ rankings correlate with features of the ranked content. We find that neither Google nor Bing are emphasizing positive sentiments toward their companyÆs own products; that both search engines do rank statistically significantly higher pages when it contains the engine companyÆs ads, as opposed to competitor ads; that there exists statistically significant news search bias across different hosts, indicating that the two search engines often prefer different hosts.     Auditing Web sitesÆ popularity and Internet campaigns. We developed methodologies to audit Web sitesÆ popularity at scale and to monitor advertising and spam campaigns on the Internet. Among others, we developed the first system that enables an endpoint to remotely measure traffic of a Website. In addition, we conducted research on automatically identifying low-credibility content on the Web: social-media spam. The approach is based on the insight that social media spam content is often generated from template-based mechanisms, which automatically creates short text sequences that both carry the spammerÆs messag...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
