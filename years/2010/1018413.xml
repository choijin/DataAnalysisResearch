<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small:  Depth Perception in Near- and Medium-Field Augmented Reality</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>499998.00</AwardTotalIntnAmount>
<AwardAmount>499998</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Project Abstract&lt;br/&gt;HCC: Small: Depth Perception in Near- and Medium-Field Augmented Reality&lt;br/&gt;&lt;br/&gt;Augmented reality (AR) is a technology where users see computer-generated, virtual objects superimposed on their view of the world.  A large number of compelling AR applications have been proposed and developed in areas such as manufacturing, logistics, maintenance, situation awareness, heads-up maps, training, tourism, archeology, theater, choreography, architecture, and image-guided surgery.  A common challenge for all of these AR applications is placing virtual objects at a specific depth in the real world.  Because of inherent engineering limitations, AR displays do not show virtual objects with the same depth cues as real objects.  Furthermore, some AR applications involve x-ray vision, which is the display of virtual objects that exist behind opaque surfaces.  For these applications, the depth cues necessarily conflict, but still yield useful information.  This leads to the scientific problems that this project is studying: (1) how depth perception operates with the conflicting depth cues of AR, and (2) new methods that can more effectively convey AR depth need to be developed and validated. &lt;br/&gt; &lt;br/&gt;The investigators are addressing these problems through two major project tracks.  The first track is empirically studying how the depth of computer-generated images is perceived for near-field distances within arm?s reach and medium-field distances of 1.5 to 30 meters.  The second track is implementing and empirically testing several new eye tracker-based AR depth presentation methods.  These include vergence depth rendering, where the vergence angle of the eyes controls which AR objects are visible, and simulated depth-of-field, where objects at different depths than the vergence angle are blurred.  This simulates depth-of-field blurring in an AR display with a fixed focal depth.  The project?s experiments adopt empirical methods from the long history of depth perception research and use control conditions that allow validation through comparison with this literature.</AbstractNarration>
<MinAmdLetterDate>07/31/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1018413</AwardID>
<Investigator>
<FirstName>J. Edward</FirstName>
<LastName>Swan II</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>J. Edward Swan II</PI_FULL_NAME>
<EmailAddress>swan@cse.msstate.edu</EmailAddress>
<PI_PHON>6623124411</PI_PHON>
<NSF_ID>000356075</NSF_ID>
<StartDate>07/31/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Mississippi State University</Name>
<CityName>MISSISSIPPI STATE</CityName>
<ZipCode>397629662</ZipCode>
<PhoneNumber>6623257404</PhoneNumber>
<StreetAddress>PO Box 6156</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Mississippi</StateName>
<StateCode>MS</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MS03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>075461814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MISSISSIPPI STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>075461814</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Mississippi State University]]></Name>
<CityName>MISSISSIPPI STATE</CityName>
<StateCode>MS</StateCode>
<ZipCode>397629662</ZipCode>
<StreetAddress><![CDATA[PO Box 6156]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Mississippi</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MS03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~499998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to measure how humans perceive the depth location of virtual objects, when those objects are displayed by augmented reality (AR) technology.&nbsp; There are many compelling application of AR technology, among them image-guided surgery, manufacturing, maintenance, heads-up maps, training, tourism, archeology, theater, and architecture, to name but a few.&nbsp; However, many of these applications will not be fully realized until we have a better understanding of how AR users perceive the depth location of AR objects.&nbsp;</p> <p>In this project, we sought to measure AR depth perception at two different distance levels: medium-field (1.5 to 30 meters) and near-field (out to 1.5 meters).&nbsp; Both distance categories are important for different AR applications.&nbsp; The primary activity was a series of experiments, where we presented real objects, AR-presented virtual objects, and sometimes objects presented in virtual reality (VR).&nbsp; Observers then indicated the depth at which they perceived the objects.&nbsp; We used two different depth measurement methods: visually open-loop techniques, and visually closed-loop techniques.&nbsp; Each method has tradeoffs, but using both allowed us to connect our results to a large body of existing depth perception literature, and also provided results that are relevant for AR applications. &nbsp;In addition to experiments, we built a number of specialized devices, which allowed us to very accurately measure near-field AR depth.</p> <p>At medium-field distances, we found that the VR depth underestimation problem, which has been widely replicated by many groups, is very unlikely to affect AR applications, because observers use their view of the real world to calibrate their motions within virtual environments, and to connect virtual objects to real world locations.&nbsp; At near-field distances, we found that the fixed focal distance of an AR display negatively affects perceived depth, to a degree that will compromise medical AR applications.&nbsp; However, focusing at the midpoint of the working volume fixes this problem.&nbsp; We also found that the brightness of the virtual object matters; it needs to be similar to the brightness of surrounding real objects.&nbsp; Finally, we found that older observers (over 40), who likely wear reading glasses, are just as accurate as younger observers (under 40).&nbsp;</p> <p>This project has substantially added to what is known about how observers perceive the depth of AR-presented virtual objects.&nbsp; In addition to the intellectual merit of gaining this understanding, the results are important for AR applications, such as in medicine, where the accuracy of perceived depth is important.&nbsp; Finally, this project has contributed to the training of three graduate students: two are now assistant professors of computer science, and the third is an engineer at Google's California campus.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/29/2015<br>      Modified by: &nbsp;</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/1018413/1018413_10019479_1451406702814_Outcomes-Image-5--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/1018413/1018413_10019479_1451406702814_Outcomes-Image-5--rgov-800width.jpg" title="Augmented Reality Haploscope Near-Field Experimental Apparatus"><img src="/por/images/Reports/POR/2015/1018413/1018413_10019479_145140...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to measure how humans perceive the depth location of virtual objects, when those objects are displayed by augmented reality (AR) technology.  There are many compelling application of AR technology, among them image-guided surgery, manufacturing, maintenance, heads-up maps, training, tourism, archeology, theater, and architecture, to name but a few.  However, many of these applications will not be fully realized until we have a better understanding of how AR users perceive the depth location of AR objects.   In this project, we sought to measure AR depth perception at two different distance levels: medium-field (1.5 to 30 meters) and near-field (out to 1.5 meters).  Both distance categories are important for different AR applications.  The primary activity was a series of experiments, where we presented real objects, AR-presented virtual objects, and sometimes objects presented in virtual reality (VR).  Observers then indicated the depth at which they perceived the objects.  We used two different depth measurement methods: visually open-loop techniques, and visually closed-loop techniques.  Each method has tradeoffs, but using both allowed us to connect our results to a large body of existing depth perception literature, and also provided results that are relevant for AR applications.  In addition to experiments, we built a number of specialized devices, which allowed us to very accurately measure near-field AR depth.  At medium-field distances, we found that the VR depth underestimation problem, which has been widely replicated by many groups, is very unlikely to affect AR applications, because observers use their view of the real world to calibrate their motions within virtual environments, and to connect virtual objects to real world locations.  At near-field distances, we found that the fixed focal distance of an AR display negatively affects perceived depth, to a degree that will compromise medical AR applications.  However, focusing at the midpoint of the working volume fixes this problem.  We also found that the brightness of the virtual object matters; it needs to be similar to the brightness of surrounding real objects.  Finally, we found that older observers (over 40), who likely wear reading glasses, are just as accurate as younger observers (under 40).   This project has substantially added to what is known about how observers perceive the depth of AR-presented virtual objects.  In addition to the intellectual merit of gaining this understanding, the results are important for AR applications, such as in medicine, where the accuracy of perceived depth is important.  Finally, this project has contributed to the training of three graduate students: two are now assistant professors of computer science, and the third is an engineer at Google's California campus.              Last Modified: 12/29/2015       Submitted by:]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
