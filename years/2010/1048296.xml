<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CiC (RDDC):  Continuous Bulk Processing in the Cloud</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>360000.00</AwardTotalIntnAmount>
<AwardAmount>360000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research explores an alternative data processing architecture that&lt;br/&gt;fundamentally improves computing efficiency to reduce costs and provide&lt;br/&gt;enhanced data mining capabilities for cloud computing.  The next major&lt;br/&gt;advancements in IT, medical, and science informatics will largely be&lt;br/&gt;dictated by our ability to store, manage, and analyze large amounts of&lt;br/&gt;information.  For many important problems, advances in data acquisition&lt;br/&gt;tools are rapidly increasing data generation rates, exceeding our ability&lt;br/&gt;to manage and process the data they produce.&lt;br/&gt;&lt;br/&gt;This proposal investigates novel architectures for continuous bulk data&lt;br/&gt;processing, which support data-intensive applications that perform complex&lt;br/&gt;multi-step computations over successive batches of input data (e.g., from&lt;br/&gt;scientific or medical instruments), allowing analytics to simply be&lt;br/&gt;updated, not recomputed, when new data arrives.  The research seeks to&lt;br/&gt;raise incremental processing as a first-class abstraction, significantly&lt;br/&gt;lowering the barrier of data-intensive projects to take advantage of cloud&lt;br/&gt;computing.  Towards that end, the work will develop data analysis portal&lt;br/&gt;architectures to increase the reach of on-demand data analytics across&lt;br/&gt;various application domains.  As a case study, it will develop a portal for&lt;br/&gt;fatty-liver disease, allowing care givers on-demand access to medical&lt;br/&gt;analytics to improve care, reduce cost, and improve clinical outcomes for&lt;br/&gt;this important disease.</AbstractNarration>
<MinAmdLetterDate>03/28/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/28/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1048296</AwardID>
<Investigator>
<FirstName>Kenneth</FirstName>
<LastName>Yocum</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kenneth G Yocum</PI_FULL_NAME>
<EmailAddress>kyocum@cs.ucsd.edu</EmailAddress>
<PI_PHON>8588223287</PI_PHON>
<NSF_ID>000367876</NSF_ID>
<StartDate>03/28/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rohit</FirstName>
<LastName>Loomba</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rohit Loomba</PI_FULL_NAME>
<EmailAddress>roloomba@ucsd.edu</EmailAddress>
<PI_PHON>8585344896</PI_PHON>
<NSF_ID>000565910</NSF_ID>
<StartDate>03/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>8010</Code>
<Text>Computing in the Cloud</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~360000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The next major advancements in IT, medical, and science informatics will largely be dictated by our ability to store, manage, and analyze large amounts of information. &nbsp;For many important problems, advances in data acquisition tools are rapidly increasing data generation rates, exceeding our ability tomanage and process the data they produce. This rapid data ``deluge'' gives rise to many exciting data mining opportunities. Here, large, daily data arrivals (e.g., from scientific or medical instruments) must be combined with data derived from previous batches or iterated upon to produce results. Cloud computing promises to radically reduce the cost and complexity of performing these large-scale analytics by providing storage, network, and processing resources commensurate with the size of the data.&nbsp;</p> <p>In this environment, a primary bottleneck to leveraging big data is developing algorithms(analytics) to process and understand data. &nbsp;&nbsp;Many processing error cases arise when running at scale on large input data sets; errors may occur after tens to thousands of machine hours. &nbsp;This proposal addresses this problem by investigating how to understand, debug, and tune analytics.&nbsp;</p> <p><strong>Intellectual Merit:</strong></p> <p>The goals of this work were to develop semantics and systems for continuously processing bulk data, identify methods for debugging and tuning large scale analytics, and evaluate the ideas with prototype software.</p> <ul> <li>The work explored how to add continuous data processing semantics to an existing relational scripting language (Pig) originally designed for batch processing systems like Hadoop MapReduce. &nbsp;This reduced the latency of operations, such as aggregations, that incrementally ingest new data.&nbsp;</li> <li>The research resulted in data provenance models for MapReduce and graph processing systems. &nbsp;Data provenance can be used to determine the set of inputs used to produce each new data item -- the PI's explored how to use such provenance to debug and tune analytics. &nbsp;The work produced provenance collection systems that could efficiently extract this fine-grain (per output record) data flow information from a variety of data-intensive computing systems including Hadoop, Hyracks, Spark, and GraphLab.</li> <li>The project developed new methods for employing provenance to improve analytics through debugging and data cleaning. &nbsp; &nbsp;This approach allows a data scientist to record and then inspect provenance to discover potential errors in the analytic. This research illustrated that one could capture fine-grain provenance in systems like Hadoop with sizea nd space overheads less than 50%. &nbsp;Having the entire provenance "at hand" allows a data scientist to debug and tune large-scale analytics in an interactive manner. In addition, this work has shown that provenance may be used for data cleaning sophisticated multi-step non-relationaltransforms (i.e., de novo genome assembly). &nbsp; This technique leveraged an ability to remove data from a pipeline and we used it to improve de novo genetic assembly when input data sets are polluted with DNA from multiple organisms.</li> <li>Finally, the work investigated how to use fine-grain provenance to understand and improve graph processing analytics. Exploiting provenance information in this setting raised significant technical challenges, stemming from the fact that provenance data attains big data scales even when it pertains to analytics over modest input graphs. &nbsp;In response to this challenge, the research gave rise to a number of novel technical solutions, including: (i) a formal semantic model of provenance in parallel graph processing frameworks and (ii) a low-overhead provenance capture and storage system. &nbsp;The work also began an investigation into a declarative provenance query language and a set of provenance queries th...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The next major advancements in IT, medical, and science informatics will largely be dictated by our ability to store, manage, and analyze large amounts of information.  For many important problems, advances in data acquisition tools are rapidly increasing data generation rates, exceeding our ability tomanage and process the data they produce. This rapid data ``deluge'' gives rise to many exciting data mining opportunities. Here, large, daily data arrivals (e.g., from scientific or medical instruments) must be combined with data derived from previous batches or iterated upon to produce results. Cloud computing promises to radically reduce the cost and complexity of performing these large-scale analytics by providing storage, network, and processing resources commensurate with the size of the data.   In this environment, a primary bottleneck to leveraging big data is developing algorithms(analytics) to process and understand data.   Many processing error cases arise when running at scale on large input data sets; errors may occur after tens to thousands of machine hours.  This proposal addresses this problem by investigating how to understand, debug, and tune analytics.   Intellectual Merit:  The goals of this work were to develop semantics and systems for continuously processing bulk data, identify methods for debugging and tuning large scale analytics, and evaluate the ideas with prototype software.  The work explored how to add continuous data processing semantics to an existing relational scripting language (Pig) originally designed for batch processing systems like Hadoop MapReduce.  This reduced the latency of operations, such as aggregations, that incrementally ingest new data.  The research resulted in data provenance models for MapReduce and graph processing systems.  Data provenance can be used to determine the set of inputs used to produce each new data item -- the PI's explored how to use such provenance to debug and tune analytics.  The work produced provenance collection systems that could efficiently extract this fine-grain (per output record) data flow information from a variety of data-intensive computing systems including Hadoop, Hyracks, Spark, and GraphLab. The project developed new methods for employing provenance to improve analytics through debugging and data cleaning.    This approach allows a data scientist to record and then inspect provenance to discover potential errors in the analytic. This research illustrated that one could capture fine-grain provenance in systems like Hadoop with sizea nd space overheads less than 50%.  Having the entire provenance "at hand" allows a data scientist to debug and tune large-scale analytics in an interactive manner. In addition, this work has shown that provenance may be used for data cleaning sophisticated multi-step non-relationaltransforms (i.e., de novo genome assembly).   This technique leveraged an ability to remove data from a pipeline and we used it to improve de novo genetic assembly when input data sets are polluted with DNA from multiple organisms. Finally, the work investigated how to use fine-grain provenance to understand and improve graph processing analytics. Exploiting provenance information in this setting raised significant technical challenges, stemming from the fact that provenance data attains big data scales even when it pertains to analytics over modest input graphs.  In response to this challenge, the research gave rise to a number of novel technical solutions, including: (i) a formal semantic model of provenance in parallel graph processing frameworks and (ii) a low-overhead provenance capture and storage system.  The work also began an investigation into a declarative provenance query language and a set of provenance queries that can help identify opportunities for optimizing graph analytics.     Broader Impacts:  A key producer of big data is the medical community, though dealing with medical data is often complex.  One goal of this work ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
