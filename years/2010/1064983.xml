<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>G&amp;V: Medium: Collaborative Research: Contact-Based Human Motion Acquisition and Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>384809.00</AwardTotalIntnAmount>
<AwardAmount>384809</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>To date, motion capture technologies suffer from three major limitations. First, the hardware devices are restrictive, cumbersome, and expensive. Second, most techniques only record the kinematic information of the movement, rather than underlying dynamic properties or control mechanisms. Third, the current technique fails to capture the interaction between the subject and the environment. Without the information of contacts, reconstructing motion that consists of complex contact phenomena is nearly impossible. This project develops a new motion acquisition and reconstruction technique that solves all three problems aforementioned. The new technique combines the force sensors and a single video camera to reconstruct full-body poses, joint torques, and contact forces in an unconstrained setting. In contrast to expensive lab equipment, the proposed system consists of a pair of low-cost, non-intrusive force-sensing shoes and a single consumer-level video camera that can be used to acquire motions difficult to capture in the lab. This acquisition technology enables new design of motion controllers by leveraging a large amount of real-world contact data. The research also develops new data representations and novel algorithms for intelligent and efficient motion planning and evaluates the developed motion controllers by simulating a human figure performing challenging balanced activities in a novel and unpredicted environment. &lt;br/&gt;&lt;br/&gt;The project is tightly integrated with education components in both Georgia Tech and Texas A&amp;M. The research of this project lends itself well to solve important real-world problems for computer graphics. The results from this project would impact research in video gaming, sports training, remote health care, biped robots, and virtual characters, etc.</AbstractNarration>
<MinAmdLetterDate>03/25/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/25/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1064983</AwardID>
<Investigator>
<FirstName>C. Karen</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>C. Karen Liu</PI_FULL_NAME>
<EmailAddress>karenliu@cs.stanford.edu</EmailAddress>
<PI_PHON>4048252745</PI_PHON>
<NSF_ID>000430108</NSF_ID>
<StartDate>03/25/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 NORTH AVE NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~384809</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>During the period of this award, we have accomplished the goal to develop a new technology that combines the force sensors and low-end RGBD cameras to reconstruct full-body poses, joint torques, and contact forces in an unconstrained setting. We validated our hypothesis that the proposed hybrid motion capture system would allow us to combine advantages of vision and force sensors while avoiding their disadvantages. The <strong>intellectual merit</strong> produced by this project including publications in Transactions on Graphics (TOG), International Conference on Intelligent Robots and Systems (IROS), International Conference on Robotics and Automation (ICRA) and Symposium on Computer Animation (SCA), two PhD theses, as well as a US provisional patent. The inexpensive and deployable nature of the system also allows us to easily reconfigure and customize to other research projects and applications. This is an enabling feature of our technology in terms of making <strong>broader impacts</strong> beyond computer animation and robotics. Based on our findins, we built a cutomized system for our colleagues at the School of Biomedical in Georgia Tech for locomotion balance studies. The low-cost, portable motion capture system allows the biomechanics researchers to test whether a simple beam-walking task could quantify differences in walking balance proficiency across a range of sensorimotor abilities. In addition, our technology was used in a new assistive technology which develops a modular and cost-effective physical device to to assist older adults or motor impaired individuals performing the task of stair negotiation. This energy-recycling smart stairs takes the advantage of our highly deployable pressure sensing technique to activate the latching mechanism customized for each individual user.&nbsp; We also exploited the motion reconstruction method for validating the physical system. Lastly, this award supported the PI&rsquo;s efforts in promoting high/middel-school <strong>education and diversity</strong>. These efforts include lecturing for Middle and High School Teacher Workshop in Georgia, organizing lab demos for middle school children during field trips, and giving a presentation at Grace Hopper Conference in 2013.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/29/2015<br>      Modified by: C. Karen&nbsp;Liu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ During the period of this award, we have accomplished the goal to develop a new technology that combines the force sensors and low-end RGBD cameras to reconstruct full-body poses, joint torques, and contact forces in an unconstrained setting. We validated our hypothesis that the proposed hybrid motion capture system would allow us to combine advantages of vision and force sensors while avoiding their disadvantages. The intellectual merit produced by this project including publications in Transactions on Graphics (TOG), International Conference on Intelligent Robots and Systems (IROS), International Conference on Robotics and Automation (ICRA) and Symposium on Computer Animation (SCA), two PhD theses, as well as a US provisional patent. The inexpensive and deployable nature of the system also allows us to easily reconfigure and customize to other research projects and applications. This is an enabling feature of our technology in terms of making broader impacts beyond computer animation and robotics. Based on our findins, we built a cutomized system for our colleagues at the School of Biomedical in Georgia Tech for locomotion balance studies. The low-cost, portable motion capture system allows the biomechanics researchers to test whether a simple beam-walking task could quantify differences in walking balance proficiency across a range of sensorimotor abilities. In addition, our technology was used in a new assistive technology which develops a modular and cost-effective physical device to to assist older adults or motor impaired individuals performing the task of stair negotiation. This energy-recycling smart stairs takes the advantage of our highly deployable pressure sensing technique to activate the latching mechanism customized for each individual user.  We also exploited the motion reconstruction method for validating the physical system. Lastly, this award supported the PIÃ†s efforts in promoting high/middel-school education and diversity. These efforts include lecturing for Middle and High School Teacher Workshop in Georgia, organizing lab demos for middle school children during field trips, and giving a presentation at Grace Hopper Conference in 2013.          Last Modified: 12/29/2015       Submitted by: C. Karen Liu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
