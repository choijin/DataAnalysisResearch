<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: A New Voice Source Model: From Glottal Areas to Better Speech Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>458000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of the proposed research is to develop and evaluate a new voice&lt;br/&gt;source model based on physiological observations of the vocal folds of 30 adult speakers. Shortcomings of existing source models can be in part attributed to the way in which they were developed: based on limited data from a few speakers, without direct physiological observations, and without perceptual validation. A larger dataset would help in not only developing a source model that could account for a range of voice qualities within and across speakers, but also result in an understanding of how and which model parameter(s) are speaker and/or gender specific. Model development will consider the perceptual effects of the model's parameters from the earliest stages.&lt;br/&gt;A better source model might also improve the performance of speech processing algorithms such as text-to-speech synthesis (TTS). Typically in the development of such algorithms, the emphasis has been on acoustic features related to the speech spectral envelope. The acoustics of the voice source, on the other hand, have received less attention. &lt;br/&gt;The proposed work involves: 1) recording high-speed images of vocal fold&lt;br/&gt;vibrations with simultaneous audio recordings from 15 male and 15 female speakers, 2) extracting glottal area functions from the images to parameterize a new voice source model, 3) performing perception experiments to uncover which model parameters are perceptually salient, and 4) using the new voice source model in TTS. &lt;br/&gt;The project's interdisciplinary team (with expertise in modeling, synthesis, recognition, phonetics, and psycholinguistics) is uniquely qualified to conduct this transformative research.</AbstractNarration>
<MinAmdLetterDate>07/29/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/15/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1018863</AwardID>
<Investigator>
<FirstName>Patricia</FirstName>
<LastName>Keating</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patricia A Keating</PI_FULL_NAME>
<EmailAddress>keating@humnet.ucla.edu</EmailAddress>
<PI_PHON>3107946316</PI_PHON>
<NSF_ID>000092027</NSF_ID>
<StartDate>07/29/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Abeer</FirstName>
<LastName>Alwan</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abeer A Alwan</PI_FULL_NAME>
<EmailAddress>alwan@ee.ucla.edu</EmailAddress>
<PI_PHON>3102062231</PI_PHON>
<NSF_ID>000090848</NSF_ID>
<StartDate>07/29/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jody</FirstName>
<LastName>Kreiman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jody Kreiman</PI_FULL_NAME>
<EmailAddress>jkreiman@ucla.edu</EmailAddress>
<PI_PHON>3107940102</PI_PHON>
<NSF_ID>000441064</NSF_ID>
<StartDate>07/29/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>LOS ANGELES</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951406</ZipCode>
<StreetAddress><![CDATA[10889 Wilshire Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~163326</FUND_OBLG>
<FUND_OBLG>2011~294674</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we investigate how the human vocal folds vibrate when producing sound. To that end, we used a high-speed camera to record vocal cord vibrations for 9 speakers (4 males and 5 females). These recordings were done simultaneously with microphone recordings of the speech sounds so that the images and voice samples can be compared.&nbsp; These data are important for understanding how voice is produced, and how changes in voice production affect the sound of a person&rsquo;s voice.</p> <p><strong>Outcomes:</strong></p> <p><strong>1) Produced a database of high-speed digital image recordings of vocal fold vibrations with simultaneous acoustic recordings. </strong>This database will be made freely available next year through the Linguistic Data Consortium.</p> <p><strong>2) Developed a software tool, the &lsquo;Glottaltopogram&rsquo;, which is available freely on our website for anyone to use. </strong>With sampling rates of thousands of frames per second, high-speed videoendoscopy produces a large amount of data that is difficult to analyze subjectively.<strong> </strong>The software is a method of analyzing high-speed images of vocal fold vibrations. This tool reveals the overall synchronization of the vibrational patterns of the vocal folds over the entire laryngeal area. Experimental results showed that this method is effective in visualizing pathological and normal vocal fold vibratory patterns.</p> <p>&nbsp;<strong>3) Analyzed Physiological and Acoustic Measurements. </strong>We examined the relationships between physiological measurements of the voice source and acoustic measurements recorded by the microphone. We also analyzed inter- and intra-speaker variability.&nbsp;Based on the analyses and motivated by data from high-speed laryngeal videoendoscopy, a new voice source model was developed to capture perceptually-important aspects. This model could be used to improve synthetic speech voices.</p> <p>&nbsp;<strong>4) Conducted Perceptual Validation of Model Parameters.</strong>&nbsp;Many glottal source models have been proposed, but none has been systematically validated perceptually. Our new model, along with four other source models, was fitted to 40 natural voice sources (20 male and 20 female). We generated synthetic copies of the voices using each modeled source pulse, with all other synthesis parameters held constant, and then conducted experiments in which listeners assessed the extent of perceived similarity between the target voice samples and each copy. Results showed that the proposed model provided a more accurate fit and a better perceptual match to the target than did the other models</p> <p>&nbsp;</p> <p><strong>Broader Impact:</strong> The project has provided valuable research experience for two&nbsp;PhD students in Electrical Engineering, and a PhD student in Linguistics. These students have been exposed to issues related to signal acquisition,&nbsp;high-speed imaging, image and acoustic analysis, voice source modeling,&nbsp;speech synthesis, perceptual experiments, cross-linguistic issues, and&nbsp;intra- and inter-speaker variation in speech production.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>The project also included community outreach in the form of participation in World Voice Day 2014. Public exhibits on the UCLA campus, staffed by project participants and by students, presented scientific, engineering, health, and artistic aspects of the study of the human voice. The event was very successful and was covered in the campus newspaper.</p> <p>&nbsp;</p> <p>All publications have been posted on our webpage and presented at leading conferences in Engineering, Computer Science, Speech Science and Linguistics (e.g., ICASSP, Interspeech, ASA) and journals. The high-speed video analysis tool "GTG tool" has been posted on the project's webpage.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/20/2015<br>      Modif...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we investigate how the human vocal folds vibrate when producing sound. To that end, we used a high-speed camera to record vocal cord vibrations for 9 speakers (4 males and 5 females). These recordings were done simultaneously with microphone recordings of the speech sounds so that the images and voice samples can be compared.  These data are important for understanding how voice is produced, and how changes in voice production affect the sound of a personÆs voice.  Outcomes:  1) Produced a database of high-speed digital image recordings of vocal fold vibrations with simultaneous acoustic recordings. This database will be made freely available next year through the Linguistic Data Consortium.  2) Developed a software tool, the æGlottaltopogramÆ, which is available freely on our website for anyone to use. With sampling rates of thousands of frames per second, high-speed videoendoscopy produces a large amount of data that is difficult to analyze subjectively. The software is a method of analyzing high-speed images of vocal fold vibrations. This tool reveals the overall synchronization of the vibrational patterns of the vocal folds over the entire laryngeal area. Experimental results showed that this method is effective in visualizing pathological and normal vocal fold vibratory patterns.   3) Analyzed Physiological and Acoustic Measurements. We examined the relationships between physiological measurements of the voice source and acoustic measurements recorded by the microphone. We also analyzed inter- and intra-speaker variability. Based on the analyses and motivated by data from high-speed laryngeal videoendoscopy, a new voice source model was developed to capture perceptually-important aspects. This model could be used to improve synthetic speech voices.   4) Conducted Perceptual Validation of Model Parameters. Many glottal source models have been proposed, but none has been systematically validated perceptually. Our new model, along with four other source models, was fitted to 40 natural voice sources (20 male and 20 female). We generated synthetic copies of the voices using each modeled source pulse, with all other synthesis parameters held constant, and then conducted experiments in which listeners assessed the extent of perceived similarity between the target voice samples and each copy. Results showed that the proposed model provided a more accurate fit and a better perceptual match to the target than did the other models     Broader Impact: The project has provided valuable research experience for two PhD students in Electrical Engineering, and a PhD student in Linguistics. These students have been exposed to issues related to signal acquisition, high-speed imaging, image and acoustic analysis, voice source modeling, speech synthesis, perceptual experiments, cross-linguistic issues, and intra- and inter-speaker variation in speech production.         The project also included community outreach in the form of participation in World Voice Day 2014. Public exhibits on the UCLA campus, staffed by project participants and by students, presented scientific, engineering, health, and artistic aspects of the study of the human voice. The event was very successful and was covered in the campus newspaper.     All publications have been posted on our webpage and presented at leading conferences in Engineering, Computer Science, Speech Science and Linguistics (e.g., ICASSP, Interspeech, ASA) and journals. The high-speed video analysis tool "GTG tool" has been posted on the project's webpage.             Last Modified: 10/20/2015       Submitted by: Abeer A Alwan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
