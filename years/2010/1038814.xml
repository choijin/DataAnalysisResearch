<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Supporting and Enhancing the HPC Challenge Benchmark for Hybrid-Multicore  Computers</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2011</AwardEffectiveDate>
<AwardExpirationDate>02/28/2015</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Thomas F. Russell</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The tight coupling that now exists between accelerating progress in nearly every critical scientific field and increasing the availability of high performance computing (HPC) means that agencies committed to supporting leading edge research will have to continue, or even expand, their investment in high end computing capacity. Unfortunately, the radical changes in system architecture that new extreme scale platforms are bringing with them (massive concurrency, processor heterogeneity, constrained power budgets, and complex memory architectures) are also crippling our ability to make such investment decisions in a rational and well-informed way. In particular, tools such as the HPC Challenge (HPCC) benchmark suite, which have played a vitally important role in evaluating the impact of system design choices on the performance of scientific applications, are not as well adapted as they need to be to the incredible complexities of emerging supercomputing platforms. Our proposed plan for supporting and enhancing the HPCC benchmark for hybrid-multicore computers is designed to maintain and extend the effectiveness of this essential tool.&lt;br/&gt;&lt;br/&gt;Accordingly, our plan for updating and enhancing HPCC will focus on three main areas of activity: 1) investigate new rules for benchmark execution; 2) improve scalability of benchmark for future architectures; and 3) expand community interaction.&lt;br/&gt;&lt;br/&gt;In terms of contributing to national cyberinfrastructure, the planned work builds upon and strengthens our ongoing collaborations with NSF Centers and HPC vendors. We are involved in the NSF Centers and have a wide collection of users and applications to draw on. These facilities contribute software and expertise to HPCC and will provide an early operational testbed for new technologies and application concepts to be developed for HPCC. Responses to our plans from vendors have been encouraging. With their cooperation, we will produce software that will be transitioned to the commercial software vendors to complete the development that we have initiated and provide these new capabilities to their customers in the research community.</AbstractNarration>
<MinAmdLetterDate>03/09/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/09/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1038814</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Dongarra</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack J Dongarra</PI_FULL_NAME>
<EmailAddress>dongarra@icl.utk.edu</EmailAddress>
<PI_PHON>8659748295</PI_PHON>
<NSF_ID>000299281</NSF_ID>
<StartDate>03/09/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Piotr</FirstName>
<LastName>Luszczek</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Piotr Luszczek</PI_FULL_NAME>
<EmailAddress>luszczek@icl.utk.edu</EmailAddress>
<PI_PHON>8659743466</PI_PHON>
<NSF_ID>000296500</NSF_ID>
<StartDate>03/09/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName>Knoxville</CityName>
<StateCode>TN</StateCode>
<ZipCode>379163801</ZipCode>
<StreetAddress><![CDATA[1331 CIR PARK DR]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramElement>
<Code>K514</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>5936</Code>
<Text>GERMANY (F.R.G.)</Text>
</ProgramReference>
<ProgramReference>
<Code>7684</Code>
<Text>STRATEGIC TECHNOLOGIES FOR CI</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Large supercomputer installations require benchmarking to test their usefulness for scientific applications. These supercomputers are much larger than a personal computer or machines that you might find at your local university computer lab, and they often feature exotic technologies to make them work efficiently at that scale. Consequently, it is quite an endeavor to write a complex scientific application for these brand new machines, and there is always a question of whether these applications will run well enough at scale to justify such unique and expensive hardware. Scientific benchmarks break this vicious cycle. On the one hand, the benchmarks represent the applications well enough so that if the benchmark runs well, then the application is also likely to run well. On the other hand, the benchmarks are simple enough to implement quickly on brand new supercomputer hardware &ndash; much faster than implementing the entire scientific application. HPC Challenge is an example of such a scientific benchmark, and represents a large number of scientific applications because it contains tests that are very typical of how scientific applications use a computer&rsquo;s main memory.</p> <p>The outcome of this NSF-funded effort was to make sure that HPC Challenge (HPCC) runs well on modern computer hardware. The update of the HPCC code was necessary because a lot has changed since the introduction of the benchmark over 10 years ago. The changes include introduction of multicore CPUs and hardware accelerators such as GPUs and coprocessors. In addition to making HPCC run well on this new hardware, the project fixed bugs discovered by HPCC users and modernized the source code to use new programming language features. As a result, HPCC now runs not just on thousands of CPUs, but also on tens and hundreds of thousands of CPUs&mdash;a number characteristic of modern supercomputers.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/27/2015<br>      Modified by: Jack&nbsp;J&nbsp;Dongarra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Large supercomputer installations require benchmarking to test their usefulness for scientific applications. These supercomputers are much larger than a personal computer or machines that you might find at your local university computer lab, and they often feature exotic technologies to make them work efficiently at that scale. Consequently, it is quite an endeavor to write a complex scientific application for these brand new machines, and there is always a question of whether these applications will run well enough at scale to justify such unique and expensive hardware. Scientific benchmarks break this vicious cycle. On the one hand, the benchmarks represent the applications well enough so that if the benchmark runs well, then the application is also likely to run well. On the other hand, the benchmarks are simple enough to implement quickly on brand new supercomputer hardware &ndash; much faster than implementing the entire scientific application. HPC Challenge is an example of such a scientific benchmark, and represents a large number of scientific applications because it contains tests that are very typical of how scientific applications use a computerÃ†s main memory.  The outcome of this NSF-funded effort was to make sure that HPC Challenge (HPCC) runs well on modern computer hardware. The update of the HPCC code was necessary because a lot has changed since the introduction of the benchmark over 10 years ago. The changes include introduction of multicore CPUs and hardware accelerators such as GPUs and coprocessors. In addition to making HPCC run well on this new hardware, the project fixed bugs discovered by HPCC users and modernized the source code to use new programming language features. As a result, HPCC now runs not just on thousands of CPUs, but also on tens and hundreds of thousands of CPUs&mdash;a number characteristic of modern supercomputers.          Last Modified: 05/27/2015       Submitted by: Jack J Dongarra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
