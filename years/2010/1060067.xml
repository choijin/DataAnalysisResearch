<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>230755.00</AwardTotalIntnAmount>
<AwardAmount>230755</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bradley D. Keister</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This award supports the development of software tools for advanced algorithms on a cluster of high performance graphics processing units(GPUs).  The initial goal of this library will focus on a single driver application, the fundamental numerical study of Nuclear Forces (Lattice Quantum Chromodynamics: LQCD), and a second target application, the numerical simulation of the exciting nano-technology of graphene.  These are both multi-fermion problems well suited to solution via multi-scale algorithms on many-core architectures.  This pilot project draws on experience gained by the small team at Boston University and Harvard in developing Dirac solvers for lattice field theory.  Two building blocks from prior research are (1) the construction of an adaptive multigrid (MG) solver for the Wilson Dirac operator of LQCD, which demonstrates a 20x speedup compared to the best Krylov solvers in production code and (2) a highly optimized Krylov solver for the same operator on GPUs (but without multigrid), realizing a 10x improvement in price/performance over traditional clusters.  The library will unite these feature and generalize their domain of applicability.  &lt;br/&gt;&lt;br/&gt;As an example of the broader impact, it is estimated that combining these two technologies (MG algorithms and GPU architectures) will yield a 100-fold improvement in price/performance for the most compute-intensive component of LQCD simulations.  Such an advance would be truly transformative, making an immediate impact in nuclear and particle physics.  At the same time, it will serve as a prototype of the more generic problem of mapping hierarchical algorithms onto heterogeneous architectures, a challenge of paramount importance on the path to the exascale.  The software library will be designed to bring similar benefits to graphene technology and to evolve to accommodate additional target application and additional domain decomposition algorithm to mitigate the communication bottleneck of Exascale designs.  The award will provide partial support for two postdoctoral scholars who play essential roles in this project.</AbstractNarration>
<MinAmdLetterDate>09/07/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1060067</AwardID>
<Investigator>
<FirstName>Lincoln</FirstName>
<LastName>Greenhill</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lincoln J Greenhill</PI_FULL_NAME>
<EmailAddress>greenhill@cfa.harvard.edu</EmailAddress>
<PI_PHON>6174957194</PI_PHON>
<NSF_ID>000175585</NSF_ID>
<StartDate>09/07/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385369</ZipCode>
<StreetAddress><![CDATA[1033 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramReference>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramReference>
<ProgramReference>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~150966</FUND_OBLG>
<FUND_OBLG>2011~79789</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Intellectual Merit.</strong>&nbsp; The focus of the awarded project was sustainable software development and engineering for scientific problems that involve very-large scale computing.&nbsp; In this context, sustainability refers to maintenance and growth over many years, for which open development, ultimately involving a community of&nbsp; developers, is a common model.&nbsp; Two problem were pursued, both of which involved research and construction of optimized software libraries for execution using breakthrough technologies provided by Graphics Processing Units (GPUs).&nbsp; GPUs are co-processors that are capable of processing thousands of threads in parallel, making them potentially hundreds of times more efficient than common CPUs when problems can be broken down into many small pieces.&nbsp; Efficiency depends acutely on the details of the problem posed, how it is factored (i.e., broken down), and the implementations in code.&nbsp; Optimal solutions are those whereby for a fixed problem size, the addition of more processing power reduces the time taken for complete a calculation, because the problem can be broken down into smaller pieces almost endlessly.&nbsp; This is known as "strong scaling." &nbsp;</p> <p>The first computational problem involved the Multigrid technique that is used in Quantum Chromodynamics (QCD) calculations.&nbsp; These are used in nuclear physics to predict and aid in interpretation of results from&nbsp; experiments with accelerators, such as the Large Hadron Collider.&nbsp; QCD calculations are among the largest (civilian) consumers of supercomputer time annually, worldwide.&nbsp; The project contributed a new "solver" benefiting users of Multigrid in the open QUDA library to which much of the research community has moved in the last several years and maintains collectively.&nbsp; The solver enabled scaling of calculations up to 256 GPUs.&nbsp; At the time, this corresponded to on the order of a million billion calculations per second.</p> <p>The second computation problem that can be factored for parallel computation is cross correlation of fast data streams that is done in real time, enabling widely distributed arrays of radio telescopes or antennas to make sharp images of the sky.&nbsp; The investigators' prior <em>xGPU</em> open library was built to handle the most computationally intensive step in cross correlation, for which it is in use with a number of new-generation eperiments and facilities internationally&nbsp; (LEDA, LWA, MWA, PAPER, and HERA) that run at a few tens of million million calculations per second.&nbsp; Since correlators are typically special purpose machines, the open question is whether in the context of a general purpose supercomputer and for the full calculation that is cross correlation (including&nbsp; Fourier transforms and networked transpose operations) can <em>xGPU</em> and accompanying operations run efficiently and scale strongly.&nbsp; Work with the DOE Titan supercomputing cluster answered this: yes, for up to 8000 antennas or about ten million billion computations per second.&nbsp;&nbsp; Beyond this, machine memory limitations and the process of scattering and gathering data on the network, with generic tools, becomes problematic.&nbsp; Nonetheless, the largest existing "antenna farm" for astronomy has only about 500 antennas.&nbsp; Cross correlation, end-to-end, by a "conventional" supercomputer had not been attempted before, and success at these large scales is surprising.&nbsp; The complex mathematics of constructing sky images has been known to be well suited and factorable for highly parallel calculations (though it is not known to scale strongly).&nbsp; The results obtained here suggest that correlation and image construction at extreme computational scales could be combined to run in real time on a single machine.</p> <p>The process of developing&nbsp; software pipelines for Tita...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit.  The focus of the awarded project was sustainable software development and engineering for scientific problems that involve very-large scale computing.  In this context, sustainability refers to maintenance and growth over many years, for which open development, ultimately involving a community of  developers, is a common model.  Two problem were pursued, both of which involved research and construction of optimized software libraries for execution using breakthrough technologies provided by Graphics Processing Units (GPUs).  GPUs are co-processors that are capable of processing thousands of threads in parallel, making them potentially hundreds of times more efficient than common CPUs when problems can be broken down into many small pieces.  Efficiency depends acutely on the details of the problem posed, how it is factored (i.e., broken down), and the implementations in code.  Optimal solutions are those whereby for a fixed problem size, the addition of more processing power reduces the time taken for complete a calculation, because the problem can be broken down into smaller pieces almost endlessly.  This is known as "strong scaling."    The first computational problem involved the Multigrid technique that is used in Quantum Chromodynamics (QCD) calculations.  These are used in nuclear physics to predict and aid in interpretation of results from  experiments with accelerators, such as the Large Hadron Collider.  QCD calculations are among the largest (civilian) consumers of supercomputer time annually, worldwide.  The project contributed a new "solver" benefiting users of Multigrid in the open QUDA library to which much of the research community has moved in the last several years and maintains collectively.  The solver enabled scaling of calculations up to 256 GPUs.  At the time, this corresponded to on the order of a million billion calculations per second.  The second computation problem that can be factored for parallel computation is cross correlation of fast data streams that is done in real time, enabling widely distributed arrays of radio telescopes or antennas to make sharp images of the sky.  The investigators' prior xGPU open library was built to handle the most computationally intensive step in cross correlation, for which it is in use with a number of new-generation eperiments and facilities internationally  (LEDA, LWA, MWA, PAPER, and HERA) that run at a few tens of million million calculations per second.  Since correlators are typically special purpose machines, the open question is whether in the context of a general purpose supercomputer and for the full calculation that is cross correlation (including  Fourier transforms and networked transpose operations) can xGPU and accompanying operations run efficiently and scale strongly.  Work with the DOE Titan supercomputing cluster answered this: yes, for up to 8000 antennas or about ten million billion computations per second.   Beyond this, machine memory limitations and the process of scattering and gathering data on the network, with generic tools, becomes problematic.  Nonetheless, the largest existing "antenna farm" for astronomy has only about 500 antennas.  Cross correlation, end-to-end, by a "conventional" supercomputer had not been attempted before, and success at these large scales is surprising.  The complex mathematics of constructing sky images has been known to be well suited and factorable for highly parallel calculations (though it is not known to scale strongly).  The results obtained here suggest that correlation and image construction at extreme computational scales could be combined to run in real time on a single machine.  The process of developing  software pipelines for Titan drove recognition of the importance and need for new infrastructure for development of chains of data processing applications (a.k.a. pipelines) in supercomputing. There is no open, lightweight, modular framework for development and operation of hi...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
