<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Ember: a critical science and engineering enabling SMP resource</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2010</AwardEffectiveDate>
<AwardExpirationDate>02/29/2012</AwardExpirationDate>
<AwardTotalIntnAmount>3232158.00</AwardTotalIntnAmount>
<AwardAmount>3232158</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Barry I. Schneider</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>1012087&lt;br/&gt;Towns&lt;br/&gt;Funding is provided for the new shared memory resource EMBER at UIUC.  The new machine will be part of the TeraGrid/XD "family" of machines.  This resource is essential to enabling many electronic structure calculations as well as computations in fluid dynamics.  In all these cases, a large shared memory system is critical.  In addition, the new system will allow larger scale computational experiments in OpenMP and provides an excellent entree for students desiring to learn parallel programming.</AbstractNarration>
<MinAmdLetterDate>02/26/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1012087</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Towns</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John W Towns</PI_FULL_NAME>
<EmailAddress>jtowns@ncsa.illinois.edu</EmailAddress>
<PI_PHON>2172443228</PI_PHON>
<NSF_ID>000400201</NSF_ID>
<StartDate>02/26/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Pflugmacher</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael R Pflugmacher</PI_FULL_NAME>
<EmailAddress>mikep@ncsa.uiuc.edu</EmailAddress>
<PI_PHON>2173332187</PI_PHON>
<NSF_ID>000312037</NSF_ID>
<StartDate>02/26/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207406</ZipCode>
<StreetAddress><![CDATA[1901 South First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>7619</Code>
<Text>Innovative HPC</Text>
</ProgramElement>
<ProgramReference>
<Code>7619</Code>
<Text>EQUIPMENT ACQUISITIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~3232158</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Ember was a shared memory resource offering access to 16.4 TF of large shared memory SGI UltraViolet systems with a total of 1,536 processor cores and 8 TB of memory. The system had 170 TB of storage in a CxFS filesystem with 13.5 GB/s I/O bandwidth. Ember was configured to run applications with moderate to high levels of parallelism (16-384 processors) requiring the advantages of the shared memory environment. &nbsp;Ember provided a critical resource to continue to support the limited opportunity for advancing research requiring a scalable shared memory architecture otherwise unavailable to the research community. As part of NSF's portfolio of high-end computing resources coordinated by the TeraGrid today and subsequently by the Extreme Sicnce and Engineering Discovery Environment (XSEDE), the system will provided a broad and diverse community of researchers with a unique resource. Ember was specifically targeted to support applications especially computational chemistry, and computational solid and fluid mechanics requiring the large-scale shared memory architecture that is extremely rare amongst institutional and departmental resources and even then only at small scale.</p> <p>Ember emerged as an essential resource for science and engineering research due to its unique combination of large shared memory nodes, balanced I/O performance, and the availability of third party application codes&mdash;in particular in the areas of computational chemistry and computational solid and fluid mechanics&mdash;that was not found anywhere else in the TeraGrid and subsequent XSEDE portfolio of resources. For computational chemistry, large shared memory nodes that can reliably run long running ab-initio calculations enable material property predictions that are more efficient than the equivalent distributed memory, semi-direct algorithms. Computational solid and fluid mechanics codes use efficient in-core solvers, exploiting the large memory, which enable projects such as the characterization of human bone microstructures pertinent to fracture initiation and arrest. Ember also enabled projects such as the NSF-funded Extreme OpenMP, which is investigating application scalability with the turbulence code Gen-IDLEST as well as scalable OpenMP implementations in experimental compilers.</p> <p>Ember will provided a computing resource to support the development and execution of a number of important applications that push the boundaries of computational science and engineering and are inherently dependent on a large shared memory hardware architecture. Throughout its operational period, Ember supported significant scientific advances in a broad range of areas including planetary astronomy (<a href="http://www.ncsa.illinois.edu/News/Stories/disks/">http://www.ncsa.illinois.edu/News/Stories/disks/</a>), hydrogen fuel cell design (<a href="http://www.ncsa.illinois.edu/News/Stories/ammonia/">http://www.ncsa.illinois.edu/News/Stories/ammonia/</a>), molecular structure (<a href="http://www.ncsa.illinois.edu/News/Stories/recoupled/">http://www.ncsa.illinois.edu/News/Stories/recoupled/</a>), and semi-conductor materials design (<a href="http://www.ncsa.illinois.edu/News/Stories/graphene/">http://www.ncsa.illinois.edu/News/Stories/graphene/</a>).</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/08/2012<br>      Modified by: John&nbsp;W&nbsp;Towns</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Ember was a shared memory resource offering access to 16.4 TF of large shared memory SGI UltraViolet systems with a total of 1,536 processor cores and 8 TB of memory. The system had 170 TB of storage in a CxFS filesystem with 13.5 GB/s I/O bandwidth. Ember was configured to run applications with moderate to high levels of parallelism (16-384 processors) requiring the advantages of the shared memory environment.  Ember provided a critical resource to continue to support the limited opportunity for advancing research requiring a scalable shared memory architecture otherwise unavailable to the research community. As part of NSF's portfolio of high-end computing resources coordinated by the TeraGrid today and subsequently by the Extreme Sicnce and Engineering Discovery Environment (XSEDE), the system will provided a broad and diverse community of researchers with a unique resource. Ember was specifically targeted to support applications especially computational chemistry, and computational solid and fluid mechanics requiring the large-scale shared memory architecture that is extremely rare amongst institutional and departmental resources and even then only at small scale.  Ember emerged as an essential resource for science and engineering research due to its unique combination of large shared memory nodes, balanced I/O performance, and the availability of third party application codes&mdash;in particular in the areas of computational chemistry and computational solid and fluid mechanics&mdash;that was not found anywhere else in the TeraGrid and subsequent XSEDE portfolio of resources. For computational chemistry, large shared memory nodes that can reliably run long running ab-initio calculations enable material property predictions that are more efficient than the equivalent distributed memory, semi-direct algorithms. Computational solid and fluid mechanics codes use efficient in-core solvers, exploiting the large memory, which enable projects such as the characterization of human bone microstructures pertinent to fracture initiation and arrest. Ember also enabled projects such as the NSF-funded Extreme OpenMP, which is investigating application scalability with the turbulence code Gen-IDLEST as well as scalable OpenMP implementations in experimental compilers.  Ember will provided a computing resource to support the development and execution of a number of important applications that push the boundaries of computational science and engineering and are inherently dependent on a large shared memory hardware architecture. Throughout its operational period, Ember supported significant scientific advances in a broad range of areas including planetary astronomy (http://www.ncsa.illinois.edu/News/Stories/disks/), hydrogen fuel cell design (http://www.ncsa.illinois.edu/News/Stories/ammonia/), molecular structure (http://www.ncsa.illinois.edu/News/Stories/recoupled/), and semi-conductor materials design (http://www.ncsa.illinois.edu/News/Stories/graphene/).          Last Modified: 06/08/2012       Submitted by: John W Towns]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
