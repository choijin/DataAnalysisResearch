<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>02/28/2015</AwardExpirationDate>
<AwardTotalIntnAmount>374088.00</AwardTotalIntnAmount>
<AwardAmount>374088</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This award supports the development of software tools for advanced algorithms on a cluster of high performance graphics processing units(GPUs).  The initial goal of this library will focus on a single driver application, the fundamental numerical study of Nuclear Forces (Lattice Quantum Chromodynamics: LQCD), and a second target application, the numerical simulation of the exciting nano-technology of graphene.  These are both multi-fermion problems well suited to solution via multi-scale algorithms on many-core architectures.  This pilot project draws on experience gained by the small team at Boston University and Harvard in developing Dirac solvers for lattice field theory.  Two building blocks from prior research are (1) the construction of an adaptive multigrid (MG) solver for the Wilson Dirac operator of LQCD, which demonstrates a 20x speedup compared to the best Krylov solvers in production code and (2) a highly optimized Krylov solver for the same operator on GPUs (but without multigrid), realizing a 10x improvement in price/performance over traditional clusters.  The library will unite these feature and generalize their domain of applicability.  &lt;br/&gt;&lt;br/&gt;As an example of the broader impact, it is estimated that combining these two technologies (MG algorithms and GPU architectures) will yield a 100-fold improvement in price/performance for the most compute-intensive component of LQCD simulations.  Such an advance would be truly transformative, making an immediate impact in nuclear and particle physics.  At the same time, it will serve as a prototype of the more generic problem of mapping hierarchical algorithms onto heterogeneous architectures, a challenge of paramount importance on the path to the exascale.  The software library will be designed to bring similar benefits to graphene technology and to evolve to accommodate additional target application and additional domain decomposition algorithm to mitigate the communication bottleneck of Exascale designs.  The award will provide partial support for two postdoctoral scholars who play essential roles in this project.</AbstractNarration>
<MinAmdLetterDate>09/07/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/15/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1060012</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Brower</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard C Brower</PI_FULL_NAME>
<EmailAddress>brower@bu.edu</EmailAddress>
<PI_PHON>6173536052</PI_PHON>
<NSF_ID>000460814</NSF_ID>
<StartDate>09/07/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Claudio</FirstName>
<LastName>Rebbi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Claudio Rebbi</PI_FULL_NAME>
<EmailAddress>rebbi@bu.edu</EmailAddress>
<PI_PHON>6173539058</PI_PHON>
<NSF_ID>000454046</NSF_ID>
<StartDate>09/07/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lorena</FirstName>
<LastName>Barba</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lorena A Barba</PI_FULL_NAME>
<EmailAddress>labarba@gwu.edu</EmailAddress>
<PI_PHON>2029943715</PI_PHON>
<NSF_ID>000517032</NSF_ID>
<StartDate>09/07/2010</StartDate>
<EndDate>09/29/2010</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName>BOSTON</CityName>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress><![CDATA[881 COMMONWEALTH AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramReference>
<ProgramReference>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~249034</FUND_OBLG>
<FUND_OBLG>2011~125054</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Multigrid Accelerates Discovery in Quark Physics&nbsp;</p> <p><br />Quantum Chromodynamics (QCD) is the fundamental theory of the nuclear force capable in principle to unravel the mysteries of how quarks bind to form the protons, neutrons and all nuclear matter. But it is a very difficult mathematical task to predict its consequences except at very high energies. In 1974, Noble laureate, Ken Wilson, pioneered the numerical formulation of QCD on a 4-dimensional a space-time lattice (K. Wilson "Confinement of quarks". Physical Review D 10 (8):2445.) to overcome this difficulty with large scale computing. However he also projected that Petascale or even Exascale architectures and radically new algorithms would be required before lattice QCD simulations achieve useful interactions with experiments. We are just now a the cusp of Wilson's prophesied revolutionary transition.<br /><br />The SHARE project has demonstrated one significant step in Wilson's dream. &nbsp;Following his renormalization group ideas in combination with the classic Multigrid techniques of applied mathematics, a new adaptive linear Dirac solver has been discovered that already sees a 25-fold increase in convergence rate. This SHARE project has married this algorithms to the recent multi-core hardware revolution powered by the NVIDIA GPU that hosts 1000s of arithmetic units on a single chip. The result is to add at least another factor of 5 in increased speed at fixed cost. Together this achieves better than a 100-fold reduction in cost allowing for enhanced &nbsp;predictive power to lattice QCD. &nbsp;As these multi-scale and multi-core technologies mature in the coming several years, lattice QCD promises to &nbsp;transform our understanding of the structure and interaction of nuclei and the high energy proton collisions of the Large Hadron Collider need to explore the Higgs regime. &nbsp;Both are essential components &nbsp;to advancing research in High Energy and Nuclear Physics. This basic approach of marrying multi-scale algorithms and multi-core chips is bound to interact with similar &nbsp;computational challenges in other areas of basic research as well as in application to develop new materials based on strongly correlated Dirac electrons.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/30/2015<br>      Modified by: Richard&nbsp;C&nbsp;Brower</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/1060012/1060012_10037733_1433024224673_lattice_2014_MG_GPU--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/1060012/1060012_10037733_1433024224673_lattice_2014_MG_GPU--rgov-800width.jpg" title="Mapping MG to GPUs"><img src="/por/images/Reports/POR/2015/1060012/1060012_10037733_1433024224673_lattice_2014_MG_GPU--rgov-66x44.jpg" alt="Mapping MG to GPUs"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The multigrid algorithm has  heirarchy of lattices from fine resolution scales  to  renormalized low  resolution scales  that  the natural maps between the multi-core GPU to the few core CPU architecture (Lattice 2014 report of Brower, Cheng and Clark/speaker)</div> <div class="imageCredit">Slide authored by Mike Clark of NVIDIA</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Richard&nbsp;C&nbsp;Brower</div> <div class="imageTitle">Mapping MG to GPUs</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[                Multigrid Accelerates Discovery in Quark Physics    Quantum Chromodynamics (QCD) is the fundamental theory of the nuclear force capable in principle to unravel the mysteries of how quarks bind to form the protons, neutrons and all nuclear matter. But it is a very difficult mathematical task to predict its consequences except at very high energies. In 1974, Noble laureate, Ken Wilson, pioneered the numerical formulation of QCD on a 4-dimensional a space-time lattice (K. Wilson "Confinement of quarks". Physical Review D 10 (8):2445.) to overcome this difficulty with large scale computing. However he also projected that Petascale or even Exascale architectures and radically new algorithms would be required before lattice QCD simulations achieve useful interactions with experiments. We are just now a the cusp of Wilson's prophesied revolutionary transition.  The SHARE project has demonstrated one significant step in Wilson's dream.  Following his renormalization group ideas in combination with the classic Multigrid techniques of applied mathematics, a new adaptive linear Dirac solver has been discovered that already sees a 25-fold increase in convergence rate. This SHARE project has married this algorithms to the recent multi-core hardware revolution powered by the NVIDIA GPU that hosts 1000s of arithmetic units on a single chip. The result is to add at least another factor of 5 in increased speed at fixed cost. Together this achieves better than a 100-fold reduction in cost allowing for enhanced  predictive power to lattice QCD.  As these multi-scale and multi-core technologies mature in the coming several years, lattice QCD promises to  transform our understanding of the structure and interaction of nuclei and the high energy proton collisions of the Large Hadron Collider need to explore the Higgs regime.  Both are essential components  to advancing research in High Energy and Nuclear Physics. This basic approach of marrying multi-scale algorithms and multi-core chips is bound to interact with similar  computational challenges in other areas of basic research as well as in application to develop new materials based on strongly correlated Dirac electrons.                Last Modified: 05/30/2015       Submitted by: Richard C Brower]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
