<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:  Bayesian Models for Lexicalized Grammars</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2011</AwardEffectiveDate>
<AwardExpirationDate>01/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>500001.00</AwardTotalIntnAmount>
<AwardAmount>500001</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Natural language processing (NLP) is a key technology for the digital age. At the core of most NLP systems is a parser, a program which identifies the grammatical structure of sentences. Parsing is an essential prerequisite for language understanding. But despite significant progress in recent decades, accurate wide-coverage parsing for any genre or language remains an unsolved problem. The broader impact of this CAREER project will be to advance the state of art in NLP technology through the development of more accurate statistical parsing models.  &lt;br/&gt;&lt;br/&gt;Since language is highly ambiguous, parsers require a statistical model which assigns the highest probability to the correct structure of each sentence.  The accuracy of current parsers is limited by the amount of available training data on which their models can be trained, and by the amount of information the models take into account. This CAREER project aims to advance parsing by developing novel methods of indirect supervision to overcome the lack of labeled training data, as well as new kinds of models which incorporate information about the prior linguistic context in which sentences appear. It employs Bayesian techniques, which give robust estimates and allow rich parametrization, and applies them to lexicalized grammars, which provide a compact representation of the syntactic properties of a language.&lt;br/&gt;This CAREER project will also train graduate students in natural language processing and develop materials that can be used to teach middle and high school students about NLP  and to inspire them to pursue an education in computer science.</AbstractNarration>
<MinAmdLetterDate>01/14/2011</MinAmdLetterDate>
<MaxAmdLetterDate>01/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1053856</AwardID>
<Investigator>
<FirstName>Julia</FirstName>
<LastName>Hockenmaier</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Julia C Hockenmaier</PI_FULL_NAME>
<EmailAddress>juliahmr@illinois.edu</EmailAddress>
<PI_PHON>2173332187</PI_PHON>
<NSF_ID>000237005</NSF_ID>
<StartDate>01/14/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207406</ZipCode>
<StreetAddress><![CDATA[1901 South First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~128524</FUND_OBLG>
<FUND_OBLG>2012~128147</FUND_OBLG>
<FUND_OBLG>2013~81685</FUND_OBLG>
<FUND_OBLG>2014~80691</FUND_OBLG>
<FUND_OBLG>2015~80954</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In order to understand natural language, it is necessary to identify the correct grammatical structure of sentences. &nbsp;Syntactic parsing is therefore an integral part of many Natural Language Processing (NLP) systems. Since each sentence has a myriad of possible structures, parsers rely on statistical models that identify the most likely structures. These models are typically trained (estimated) on so-called treebanks, large corpora of syntactically annotated text. However, treebanks are expensive and labor intensive to produce, and not available for all languges or genres, while raw text is often abundant. It is therefore important to understand how well such statistical models can be estimated from text that is is not annotated or only annotated with very limited information (e.g. if the words are labeled with their parts of speech). However, this is not just a question of statistics, but also of linguistics: there are many different grammar formalisms that differ in the kind of syntactic representations they define. Most work on grammar induction, or unsupervised parsing, the task we have focused on for this project, is based on impoverished variants of context free grammar (which return phrase structure trees labeled with arbitrary categories) or dependency grammar (which return unlabeled projective dependency trees that lack linguistic interpretability). Our project was based on a different, linguistically expressive, formalism called combinatory categorial grammar (CCG). CCG &nbsp;is a so-called mildly context-sensitive formalism (which means it can represent more complex structures than context-free grammars or projective dependency grammars) that is based on combinatory logic. Its representations explicitly capture function-argument relations between constituents (e.g. between verbs and their subjects or objects). The use of CCG has provided us with several advantages over other approaches for unsupervised grammar induction: 1) we were able to show that CCG permits a much simpler statistical model for unsupervised induction than context-free or dependency grammars (in fact, our model is as simple as that for finite-state Hidden-Markov Models, even though those are even less expressive than context-free grammars); 2) our parser is much more robust (accurate) on longer sentences, 3) our approach returns linguistically interpretable analyses that identify head-argument and head-modifier relations between words and constituents, making it potentially directly useful for any downstream applications that require syntactic analyses, 4) the linguistic interpretability of our system's parses has made it possible for us to perform an extensive error analysis of the kinds of constructions that can or cannot be identified correctly without supervision, and 5) we were also able to show that the output of our syntactic parser can be used either under its original unsupervised training regime, or when provided with additional weak supervision for semantic parsing in the same fashion as a supervised CCG parser can. We believe that our error analysis and our study on semantic parsing with unsupervised and weakly supervised syntactic parsers carry important lessons for unsupervised and weakly supervised parsing with any grammar formalism, as they shed light on the kind of annotation that needs to be provided for such systems to return correct (and useful) syntactic structures.This project has trained one PhD student and one MS student. We have presented our work at international conferences and at invited talks at a number of universities.&nbsp;</p><br> <p>            Last Modified: 12/21/2018<br>      Modified by: Julia&nbsp;C&nbsp;Hockenmaier</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In order to understand natural language, it is necessary to identify the correct grammatical structure of sentences.  Syntactic parsing is therefore an integral part of many Natural Language Processing (NLP) systems. Since each sentence has a myriad of possible structures, parsers rely on statistical models that identify the most likely structures. These models are typically trained (estimated) on so-called treebanks, large corpora of syntactically annotated text. However, treebanks are expensive and labor intensive to produce, and not available for all languges or genres, while raw text is often abundant. It is therefore important to understand how well such statistical models can be estimated from text that is is not annotated or only annotated with very limited information (e.g. if the words are labeled with their parts of speech). However, this is not just a question of statistics, but also of linguistics: there are many different grammar formalisms that differ in the kind of syntactic representations they define. Most work on grammar induction, or unsupervised parsing, the task we have focused on for this project, is based on impoverished variants of context free grammar (which return phrase structure trees labeled with arbitrary categories) or dependency grammar (which return unlabeled projective dependency trees that lack linguistic interpretability). Our project was based on a different, linguistically expressive, formalism called combinatory categorial grammar (CCG). CCG  is a so-called mildly context-sensitive formalism (which means it can represent more complex structures than context-free grammars or projective dependency grammars) that is based on combinatory logic. Its representations explicitly capture function-argument relations between constituents (e.g. between verbs and their subjects or objects). The use of CCG has provided us with several advantages over other approaches for unsupervised grammar induction: 1) we were able to show that CCG permits a much simpler statistical model for unsupervised induction than context-free or dependency grammars (in fact, our model is as simple as that for finite-state Hidden-Markov Models, even though those are even less expressive than context-free grammars); 2) our parser is much more robust (accurate) on longer sentences, 3) our approach returns linguistically interpretable analyses that identify head-argument and head-modifier relations between words and constituents, making it potentially directly useful for any downstream applications that require syntactic analyses, 4) the linguistic interpretability of our system's parses has made it possible for us to perform an extensive error analysis of the kinds of constructions that can or cannot be identified correctly without supervision, and 5) we were also able to show that the output of our syntactic parser can be used either under its original unsupervised training regime, or when provided with additional weak supervision for semantic parsing in the same fashion as a supervised CCG parser can. We believe that our error analysis and our study on semantic parsing with unsupervised and weakly supervised syntactic parsers carry important lessons for unsupervised and weakly supervised parsing with any grammar formalism, as they shed light on the kind of annotation that needs to be provided for such systems to return correct (and useful) syntactic structures.This project has trained one PhD student and one MS student. We have presented our work at international conferences and at invited talks at a number of universities.        Last Modified: 12/21/2018       Submitted by: Julia C Hockenmaier]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
