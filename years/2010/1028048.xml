<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Recurrent Deep Learning Machines</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>295151.00</AwardTotalIntnAmount>
<AwardAmount>295151</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Paul Werbos</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The objective of this research is to develop a new paradigm of deep learning machine - those with a feedback structure. Feedbacks bring to computing nodes current or past information contained in neighboring or larger receptive fields of other computing nodes from the same or higher layers for forming better local representations or features. Such information is required for processing dynamical data and for maximizing generalization capabilities on static data. The approach of this research is to select or design deep and recurrent architectures, develop generative and discriminative learning techniques, and integrating the risk-averting method of convexifying training criteria into training recurrent deep learning machines. &lt;br/&gt;&lt;br/&gt;    Intellectual Merit: Recurrent neural networks are irreplaceable for applications involving dynamical data and are fundamentally better than feedforward networks even on static data. However, difficulty in training recurrent networks has stifled development and understanding of them. The proposed research is expected to help remove this difficulty, bring forth the full power of recurrent neural networks, and boost interests in neural networks in general, which have unfortunately and undeservedly fallen out of favor in recent years.&lt;br/&gt;&lt;br/&gt;    Broader Impact: Recurrent deep learning machines are powerful for static and dynamical classification and regression, including image and video recognition, analysis and compression; nonlinear system identification/control; signal processing/filtering; and critical system health/fault monitoring/detection. Therefore, the proposed work will contribute greatly to medical instrumentation, computer/robot/information technology, wireless telecommunication, national defense, and homeland security. Recurrent deep learning machines will ecome an important component in the graduate education in engineering and computer science</AbstractNarration>
<MinAmdLetterDate>08/27/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1028048</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Lo</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James T Lo</PI_FULL_NAME>
<EmailAddress>jameslo@umbc.edu</EmailAddress>
<PI_PHON>4104552432</PI_PHON>
<NSF_ID>000415714</NSF_ID>
<StartDate>08/27/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland Baltimore County</Name>
<CityName>Baltimore</CityName>
<ZipCode>212500002</ZipCode>
<PhoneNumber>4104553140</PhoneNumber>
<StreetAddress>1000 Hilltop Circle</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>061364808</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND BALTIMORE COUNTY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland Baltimore County]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212500002</ZipCode>
<StreetAddress><![CDATA[1000 Hilltop Circle]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7607</Code>
<Text>EPCN-Energy-Power-Ctrl-Netwrks</Text>
</ProgramElement>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~295151</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>An objective of the project is to solve the local-minimum problem in training recurrent deep learning machines. A method of convexifying the error criterion was proposed for data fitting&nbsp;but had difficulties with computer shift register overflow and selection of the risk-sensitivity index.</p> <p>The efforts in the project to resolve such difficulties resulted in the followng methods, which are believed to have solved the local minimum problem for all practical purpose:</p> <p>1. The NRAE method (at a fixed value of &lambda;): The NRAE training method was numerically tested for a large number of values of &lambda; in the range 10?-10&sup1;&sup1;. The success rate of the method is about 50% for &lambda; in the range of 10?-10? and increases to about 75% as &lambda; increases to the range of 10&sup1;?-10&sup1;&sup1;. The method fails to work when &lambda; exceeds 10&sup1;&sup1;.</p> <p>The method and numerical&nbsp;results were reported in "Overcoming the Local-Minimum Problem in Training Multilayer Perceptrons with the NRAE Training Method", <em>Advances in Neural Networks - ISNN 2012</em>, J. Wang, G.G. Yen, and M.M. Polycarpou (Eds.), pp. 440-447, Springer-Verlag Berlin Heidelberg, 2012 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).</p> <p>2. The NRAE-MSE method: To improve the success rates of the NRAE training method, we developed the NRAE-MSE training method. For a value of &lambda; in the range of 10?-10&sup1;&sup1;, we train the MLP with the NRAE method, but we take excursions from training with C_{&lambda;}(w) to training the MLP with MSE criterion Q(w) from time to time. Once Q(w) &asymp; 0 is reached in such an excursion, the training is stopped and declared a success. This method, which is called the NRAE-MSE training method, achieved 100% success rate in all the numerical tests conducted.</p> <p>This method and numerical&nbsp;results were reported in "Overcoming the Local-Minimum Problem in Training Multilayer Perceptrons by the NRAE-MSE Training Method", <em>Advances in Neural Networks - ISNN 2013</em>, Chengan Guo, Zeng-Guang Hou, Zhigang Zeng (Eds.), pp. 440-447 Springer-Verlag Berlin Heidelberg, 2013 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).</p> <p>The NRAE method and the NRAE-MSE method discussed above were described in detail in a journal paper entitled "The Normalized Risk-Averting Error Criterion for Avoiding Nonglobal Local Minima in Training Neural Networks", to appear in <em>Neurocomputing</em>, which is available online at http://dx.doi.org/10.1016/j.neucom.2013.11.056 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).</p> <p>3. The GDC (gradual deconvexification) method: It was found that the greater &lambda; is, the flatter the lanscape of the NRAE is. At a very large &lambda;, training often stagnant, and it is misinterpreted as falling into a local minimum. This observation motivated an alternative method: We the training with a large &lambda;, say 5,000, and gradually decrease it by a certain percentage, say 10%,, whenever the training error is not reduced enough after a certain number of epochs, down to 0.</p> <p>This method, called the GDC method, has the advantage of having a very small chance to fall into a nonglobal local minmum at large values of &lambda; and continuing to improve the training error as &lambda; decreases. Our tests show the GDC method works 100% on all of our test examples.</p> <p>The GDC method and numerical&nbsp;results were reported in "Overcoming the Local-Minimum Problem in Training Multilayer Perceptrons by Gradual Deconvexification", <em>Proceedings of International Joint Conference on Neural Networks</em>, pp. 635-640, Dallas, Texas, USA, August 4-9, 2013 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).</p> <p>4. The GDC pairwise training method: In the above three training methods, the BFGS optimization method was used. When the dimension of the feature vector input to the MLP is large, the BFGS method req...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ An objective of the project is to solve the local-minimum problem in training recurrent deep learning machines. A method of convexifying the error criterion was proposed for data fitting but had difficulties with computer shift register overflow and selection of the risk-sensitivity index.  The efforts in the project to resolve such difficulties resulted in the followng methods, which are believed to have solved the local minimum problem for all practical purpose:  1. The NRAE method (at a fixed value of &lambda;): The NRAE training method was numerically tested for a large number of values of &lambda; in the range 10?-10&sup1;&sup1;. The success rate of the method is about 50% for &lambda; in the range of 10?-10? and increases to about 75% as &lambda; increases to the range of 10&sup1;?-10&sup1;&sup1;. The method fails to work when &lambda; exceeds 10&sup1;&sup1;.  The method and numerical results were reported in "Overcoming the Local-Minimum Problem in Training Multilayer Perceptrons with the NRAE Training Method", Advances in Neural Networks - ISNN 2012, J. Wang, G.G. Yen, and M.M. Polycarpou (Eds.), pp. 440-447, Springer-Verlag Berlin Heidelberg, 2012 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).  2. The NRAE-MSE method: To improve the success rates of the NRAE training method, we developed the NRAE-MSE training method. For a value of &lambda; in the range of 10?-10&sup1;&sup1;, we train the MLP with the NRAE method, but we take excursions from training with C_{&lambda;}(w) to training the MLP with MSE criterion Q(w) from time to time. Once Q(w) &asymp; 0 is reached in such an excursion, the training is stopped and declared a success. This method, which is called the NRAE-MSE training method, achieved 100% success rate in all the numerical tests conducted.  This method and numerical results were reported in "Overcoming the Local-Minimum Problem in Training Multilayer Perceptrons by the NRAE-MSE Training Method", Advances in Neural Networks - ISNN 2013, Chengan Guo, Zeng-Guang Hou, Zhigang Zeng (Eds.), pp. 440-447 Springer-Verlag Berlin Heidelberg, 2013 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).  The NRAE method and the NRAE-MSE method discussed above were described in detail in a journal paper entitled "The Normalized Risk-Averting Error Criterion for Avoiding Nonglobal Local Minima in Training Neural Networks", to appear in Neurocomputing, which is available online at http://dx.doi.org/10.1016/j.neucom.2013.11.056 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).  3. The GDC (gradual deconvexification) method: It was found that the greater &lambda; is, the flatter the lanscape of the NRAE is. At a very large &lambda;, training often stagnant, and it is misinterpreted as falling into a local minimum. This observation motivated an alternative method: We the training with a large &lambda;, say 5,000, and gradually decrease it by a certain percentage, say 10%,, whenever the training error is not reduced enough after a certain number of epochs, down to 0.  This method, called the GDC method, has the advantage of having a very small chance to fall into a nonglobal local minmum at large values of &lambda; and continuing to improve the training error as &lambda; decreases. Our tests show the GDC method works 100% on all of our test examples.  The GDC method and numerical results were reported in "Overcoming the Local-Minimum Problem in Training Multilayer Perceptrons by Gradual Deconvexification", Proceedings of International Joint Conference on Neural Networks, pp. 635-640, Dallas, Texas, USA, August 4-9, 2013 (James Ting-Ho Lo, Yichuan Gui and Yun Peng).  4. The GDC pairwise training method: In the above three training methods, the BFGS optimization method was used. When the dimension of the feature vector input to the MLP is large, the BFGS method requires a very large amount of memory. Hence, the GDC pairwise training method, which used a pairwise gradient descent optimization method, was developed. The method was tested for...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
