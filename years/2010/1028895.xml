<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDI-Type II: Collaborative Research: Perception of Scene Layout by Machines and Visually Impaired Users</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>700937.00</AwardTotalIntnAmount>
<AwardAmount>700937</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>01060000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OIA</Abbreviation>
<LongName>Office of Integrative Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Stephen Meacham</SignBlockName>
<PO_EMAI>smeacham@nsf.gov</PO_EMAI>
<PO_PHON>7032927599</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project investigates computational methods for object detection, spatial scene construction, and natural language spatial descriptions derived from real-time visual images to describe prototypical indoor spaces (e.g., rooms, offices, etc.). The primary application of this research is to provide blind or visually impaired users with spatial information about their surroundings that may otherwise be difficult to obtain from non-visual sensing. Such knowledge will assist in development of accurate cognitive models of the environment and will support better informed execution of spatial behaviors in everyday tasks. &lt;br/&gt;&lt;br/&gt;A second motivation for the work is to contribute to the improvement of spatial capacities for computers and robots. Computers and robots are similarly "blind" to images unless they have been provided some means to "see" and understand them. Currently, no robotic system is able to reliably perform high-level processing of spatial information on the basis of image sequences, e.g., to find an empty chair in a room, which not only means finding an empty chair in an image, but also localizing the chair in the room, and performing an action of reaching the chair. The guiding tenet of this research is that a better understanding of spatial knowledge acquisition from visual images and concepts of spatial awareness by humans can also be applied to reducing the ambiguity and uncertainty of information processing by autonomous systems. &lt;br/&gt;&lt;br/&gt;A central contribution of this work is to make the spatial information content of visual images available to the visually impaired, a rapidly growing demographic of our aging society. In an example scenario a blind person and her guide dog are walking to her doctor's office, an office which she has not previously visited. At the office she needs information for performing some essential tasks such as finding the check-in counter, available seating, or the bathroom. No existing accessible navigation systems are able to describe the spatial parameters of an environment and help detect and localize objects in that space. Our work will provide the underlying research and elements to realize such a system.</AbstractNarration>
<MinAmdLetterDate>09/20/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/20/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.083</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1028895</AwardID>
<Investigator>
<FirstName>Mary-Kate</FirstName>
<LastName>Beard-Tisdale</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mary-Kate Beard-Tisdale</PI_FULL_NAME>
<EmailAddress>kate.beard@maine.edu</EmailAddress>
<PI_PHON>2075812147</PI_PHON>
<NSF_ID>000198393</NSF_ID>
<StartDate>09/20/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nicholas</FirstName>
<LastName>Giudice</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicholas Giudice</PI_FULL_NAME>
<EmailAddress>nicholas.giudice@maine.edu</EmailAddress>
<PI_PHON>2075812187</PI_PHON>
<NSF_ID>000503809</NSF_ID>
<StartDate>09/20/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Reinhard</FirstName>
<LastName>Moratz</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Reinhard Moratz</PI_FULL_NAME>
<EmailAddress>moratz@spatial.maine.edu</EmailAddress>
<PI_PHON>2075811484</PI_PHON>
<NSF_ID>000516326</NSF_ID>
<StartDate>09/20/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maine</Name>
<CityName>ORONO</CityName>
<ZipCode>044695717</ZipCode>
<PhoneNumber>2075811484</PhoneNumber>
<StreetAddress>5717 Corbett Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maine</StateName>
<StateCode>ME</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>ME02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>186875787</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MAINE SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071750426</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maine]]></Name>
<CityName>ORONO</CityName>
<StateCode>ME</StateCode>
<ZipCode>044695717</ZipCode>
<StreetAddress><![CDATA[5717 Corbett Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maine</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>ME02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7751</Code>
<Text>CDI TYPE II</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>5936</Code>
<Text>GERMANY (F.R.G.)</Text>
</ProgramReference>
<ProgramReference>
<Code>5979</Code>
<Text>Europe and Eurasia</Text>
</ProgramReference>
<ProgramReference>
<Code>7721</Code>
<Text>FROM DATA TO KNOWLEDGE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~700937</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Perception of Scene Layout by Machines and Visually Impaired Users</p> <p>This project investigated approaches for converting visual images (RGB and depth) of indoor scenes to outputs that could be accessed by blind or low vision persons.&nbsp; The outcomes of the project are new image processing sequences that sequentially build meaning into images of indoor scenes. The images are analyzed initially for planar segments that can be interpreted as walls, tabletops, etc. and shapes.&nbsp; The processing uses some metric information such as the depth differences in the image but a novel contribution is the use of qualitative spatial properties to describe objects and relationships between objects in the scene such as adjacent, left of, or near.&nbsp; Such qualitative spatial properties can adequately describe a scene in the presence of incomplete information. The project developed new efficient qualitative shape descriptions and measures of shape similarity that can support quick detection of similar shapes such as chairs. Once a scene is built up in terms of identifiable objects and relations between them we need a way to describe the scene in non-visual terms. To that end the project developed a vibro-audio interface (VAI) for a standard tablet/ smart phone that provides combined tactile and audio cues for a user as they move a finger over the screen.&nbsp; The project tested the psychophysical parameters of this interface (line width, line separation, and join angles) with blindfolded subjects to assure it&rsquo;s functionality. In all tests the VAI performed as well as gold standard tactile maps.&nbsp; The functionality of the VAI was tested for non-visual methods for panning and zooming to allow access to information extending beyond the screen size with tests indicating no significant difference in image comprehension between information obtained from a single image versus the situation in which information had to be obtained from panning and zooming. The VAI was also tested for environmental transfer: that is, can a user transfer information obtained from the interface and use it to perform tasks in the actual environment. Blindfolded participants were able to correctly move and point to objects in a scene learned from the VAI interface.&nbsp; Another set of tests evaluated natural language expressions for describing indoor scenes, particularly the use of spatial prepositions. The objective was to find optimal, in the sense of concise and reliable expressions that convey an indoor scene description. Such information is important for building reliable indoor navigation assistance comparable to the concise navigation instructions provided for outdoor vehicle and pedestrian navigation. Human participants were asked to choose or provide spatial prepositions they felt best described sets of qualitative spatial relationships such as adjacent or meets, and near in virtual reality scenes that were controlled for specific relations between furniture or furniture and room structures such as walls and doors. Results will contribute to construction of a natural language generation aid that provides contextual cues for preferred preposition use or acceptable preposition substitution. &nbsp;While the components have not been tied together in a comprehensive implementation package, the key pieces have been developed and tested. With the availability of hand held devices combining RGB and depth images, the essential components are in place to implement an effective indoor navigation assistant for blind and low vision users.</p> <p>&nbsp;</p> <p>The project provided an interdisciplinary training opportunity for several graduate students including three MS students, three PhD students who will complete their degrees in 2017, and optional practical training for one post graduate student.</p><br> <p>            Last Modified: 01/02/2017<br>      Modified by: Mary-Kate&nbsp;Beard-Tisdale</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1028895/1028895_10047016_1483376790351_Tracingcorridormapusingtwo-fingerpantechnique--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1028895/1028895_10047016_1483376790351_Tracingcorridormapusingtwo-fingerpantechnique--rgov-800width.jpg" title="Tracing lines on vibro audio Interface"><img src="/por/images/Reports/POR/2017/1028895/1028895_10047016_1483376790351_Tracingcorridormapusingtwo-fingerpantechnique--rgov-66x44.jpg" alt="Tracing lines on vibro audio Interface"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Example of  testing line tracing with the vibro-audio interface implementation on a tablet.</div> <div class="imageCredit">Hari Palani, PhD student Univeristy of Maine</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Mary-Kate&nbsp;Beard-Tisdale</div> <div class="imageTitle">Tracing lines on vibro audio Interface</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Perception of Scene Layout by Machines and Visually Impaired Users  This project investigated approaches for converting visual images (RGB and depth) of indoor scenes to outputs that could be accessed by blind or low vision persons.  The outcomes of the project are new image processing sequences that sequentially build meaning into images of indoor scenes. The images are analyzed initially for planar segments that can be interpreted as walls, tabletops, etc. and shapes.  The processing uses some metric information such as the depth differences in the image but a novel contribution is the use of qualitative spatial properties to describe objects and relationships between objects in the scene such as adjacent, left of, or near.  Such qualitative spatial properties can adequately describe a scene in the presence of incomplete information. The project developed new efficient qualitative shape descriptions and measures of shape similarity that can support quick detection of similar shapes such as chairs. Once a scene is built up in terms of identifiable objects and relations between them we need a way to describe the scene in non-visual terms. To that end the project developed a vibro-audio interface (VAI) for a standard tablet/ smart phone that provides combined tactile and audio cues for a user as they move a finger over the screen.  The project tested the psychophysical parameters of this interface (line width, line separation, and join angles) with blindfolded subjects to assure it?s functionality. In all tests the VAI performed as well as gold standard tactile maps.  The functionality of the VAI was tested for non-visual methods for panning and zooming to allow access to information extending beyond the screen size with tests indicating no significant difference in image comprehension between information obtained from a single image versus the situation in which information had to be obtained from panning and zooming. The VAI was also tested for environmental transfer: that is, can a user transfer information obtained from the interface and use it to perform tasks in the actual environment. Blindfolded participants were able to correctly move and point to objects in a scene learned from the VAI interface.  Another set of tests evaluated natural language expressions for describing indoor scenes, particularly the use of spatial prepositions. The objective was to find optimal, in the sense of concise and reliable expressions that convey an indoor scene description. Such information is important for building reliable indoor navigation assistance comparable to the concise navigation instructions provided for outdoor vehicle and pedestrian navigation. Human participants were asked to choose or provide spatial prepositions they felt best described sets of qualitative spatial relationships such as adjacent or meets, and near in virtual reality scenes that were controlled for specific relations between furniture or furniture and room structures such as walls and doors. Results will contribute to construction of a natural language generation aid that provides contextual cues for preferred preposition use or acceptable preposition substitution.  While the components have not been tied together in a comprehensive implementation package, the key pieces have been developed and tested. With the availability of hand held devices combining RGB and depth images, the essential components are in place to implement an effective indoor navigation assistant for blind and low vision users.     The project provided an interdisciplinary training opportunity for several graduate students including three MS students, three PhD students who will complete their degrees in 2017, and optional practical training for one post graduate student.       Last Modified: 01/02/2017       Submitted by: Mary-Kate Beard-Tisdale]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
