<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: Learning in Worst-Case Noise Models</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>499864.00</AwardTotalIntnAmount>
<AwardAmount>499864</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jack S. Snoeyink</SignBlockName>
<PO_EMAI>jsnoeyin@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Machine Learning algorithms are ubiquitous in computer science with important applications to data-mining, classification, and ranking.  These algorithms are typically applied to data sets that contain a sizable fraction of noisy training examples.  This project focuses on developing learning algorithms that can succeed in the presence of noisy data sets that have been corrupted in a potentially adversarial or malicious manner.  Algorithms that can tolerate these types of worst-case noise are critical for the depolyment of complex machine learning systems, as real-world data sets (for example, data related to spam detection) are often noisy in unpredictable ways.  Previous work on learning in the presence of noise focused on models with strong assumptions on how the noise is applied (e.g., independently for each data point).&lt;br/&gt;&lt;br/&gt;The intellectual merit of this project lies in understanding the computational complexity of optimization problems associated with learning in worst-case noise models.  More specifically, the project will design algorithms that can find a classifier whose error is competitive with the best function from a large class of concepts.  In order to design these algorithms, the project will prove new structural results on how well classes of Boolean functions can be approximated with respect to a variety of well-studied probability distributions.  Additionally, the project will explore hardness results for learning functions with respect to adversarial noise via reductions to notoriously difficult problems in cryptography and computational complexity.&lt;br/&gt;&lt;br/&gt;The broader impact of this project is the potential to realize more powerful classification tools in a variety of application areas in the sciences such as computational biology (e.g., protein detection) and linguistics (e.g., text categorization).  Additionally, the PI will develop a new graduate course that furthers the relationship between computational and statistical methods in machine learning theory.</AbstractNarration>
<MinAmdLetterDate>09/21/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/19/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1018829</AwardID>
<Investigator>
<FirstName>Adam</FirstName>
<LastName>Klivans</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adam R Klivans</PI_FULL_NAME>
<EmailAddress>klivans@cs.utexas.edu</EmailAddress>
<PI_PHON>6173699964</PI_PHON>
<NSF_ID>000284027</NSF_ID>
<StartDate>09/21/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787595316</ZipCode>
<StreetAddress><![CDATA[3925 W Braker Lane, Ste 3.340]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~499864</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Machine Learning systems are now pervasive in the sciences and engineering. &nbsp;Tools for making automated predictions have exciting applications in Biology (drug discovery), Medicine (automated diagnoses), Industry (Self-Driving Cars), and Finance (risk management and trading). &nbsp;At the core of these applications are algorithms for automatically finding structure in data. &nbsp;One of the major challenges in developing these algorithms is grappling with noisy data. &nbsp;The main thrust of this proposal is the development of new algorithms and hardness results for problems related to overcome noise in data. &nbsp;Additionally, we have developed new mathematical tools to relate this problems to other fields in science and mathematics.&nbsp;</p> <p>A common primitive for machine learning systems is an algorithm for learning a linear separator, or halfspace. &nbsp;We have given new algorithms for learning these function classes in environments where an adversary may corrupt a substantial fraction of the data in an adversarial fraction. &nbsp;Much previous work in learning theory focused on the case when data is noiseless or is independently corrupted. &nbsp;A much more realistic setting is the *agnostic* model of learning, where there is no structure placed on the noise. &nbsp; For halfspaces and for many of their generalizations, we have given new mathematical tools to understand when these problems are tractable and when they are most likely hard in the worst-case. &nbsp;These are useful for many common machine learning tools used in practice, such as Support Vector Machines.&nbsp;</p> <p>Our results inform the learning theory (and machine learning community more generally) as to the feasibility of various concrete learning tasks. &nbsp;</p> <p>More concrete examples of our results include:</p> <p>** The first nontrivial bounds for learning polynomial threshold functions (higher degree linear separators) with respect to the uniform distribution in the agnostic model.</p> <p>** Improved bounds for learning intersections of halfspaces with respect to the uniform distribution in the agnostic model and a new "Invariance Principle" to relate learning over different domains.</p> <p>** The first nontrivial algorithms for learning submodular functions in the agnostic model (without queries).&nbsp;</p> <p>** The first algorithms for learning intersections of halfspaces with respect to log-concave distributions and a new, distribution-independent notion of noise stability (moment-matching).</p> <p>** The first hardness results for learning a single halfspace in a noisy model with respect to Gaussian distributions.&nbsp;</p> <p>** New relationships between learning and proving lower bounds in computational complexity theory.</p> <p>** New results relating learning techniques from adaptive data analysis to derandomization.&nbsp;</p> <p>From an educational perspective, we have developed a 'flipped' class for mathematics for computer scientists whose tools are related to the research undertaken in this project. &nbsp;A new data-mining curriculum has also been created.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/16/2017<br>      Modified by: Adam&nbsp;R&nbsp;Klivans</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Machine Learning systems are now pervasive in the sciences and engineering.  Tools for making automated predictions have exciting applications in Biology (drug discovery), Medicine (automated diagnoses), Industry (Self-Driving Cars), and Finance (risk management and trading).  At the core of these applications are algorithms for automatically finding structure in data.  One of the major challenges in developing these algorithms is grappling with noisy data.  The main thrust of this proposal is the development of new algorithms and hardness results for problems related to overcome noise in data.  Additionally, we have developed new mathematical tools to relate this problems to other fields in science and mathematics.   A common primitive for machine learning systems is an algorithm for learning a linear separator, or halfspace.  We have given new algorithms for learning these function classes in environments where an adversary may corrupt a substantial fraction of the data in an adversarial fraction.  Much previous work in learning theory focused on the case when data is noiseless or is independently corrupted.  A much more realistic setting is the *agnostic* model of learning, where there is no structure placed on the noise.   For halfspaces and for many of their generalizations, we have given new mathematical tools to understand when these problems are tractable and when they are most likely hard in the worst-case.  These are useful for many common machine learning tools used in practice, such as Support Vector Machines.   Our results inform the learning theory (and machine learning community more generally) as to the feasibility of various concrete learning tasks.    More concrete examples of our results include:  ** The first nontrivial bounds for learning polynomial threshold functions (higher degree linear separators) with respect to the uniform distribution in the agnostic model.  ** Improved bounds for learning intersections of halfspaces with respect to the uniform distribution in the agnostic model and a new "Invariance Principle" to relate learning over different domains.  ** The first nontrivial algorithms for learning submodular functions in the agnostic model (without queries).   ** The first algorithms for learning intersections of halfspaces with respect to log-concave distributions and a new, distribution-independent notion of noise stability (moment-matching).  ** The first hardness results for learning a single halfspace in a noisy model with respect to Gaussian distributions.   ** New relationships between learning and proving lower bounds in computational complexity theory.  ** New results relating learning techniques from adaptive data analysis to derandomization.   From an educational perspective, we have developed a 'flipped' class for mathematics for computer scientists whose tools are related to the research undertaken in this project.  A new data-mining curriculum has also been created.                Last Modified: 01/16/2017       Submitted by: Adam R Klivans]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
