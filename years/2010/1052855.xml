<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: A Bayesian model of phonetic and phonotactic effects in cross-language speech production</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>174878.00</AwardTotalIntnAmount>
<AwardAmount>174878</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A fundamental aspect of human cognition is the capacity to perceive and produce language.  Studies of non-native speech processing provide some of the most striking evidence bearing on this capacity: when humans attempt to perceive or produce words containing foreign sounds or sound sequences, they show systematic patterns of correct and incorrect performance. Prior research has established that different non-native structures elicit different rates and types of error; it has been hypothesized that these differences can be explained by a combination of grammatical, perceptual, and articulatory factors. The main goals of this project are to provide carefully controlled experimental evaluations of these factors, and to develop an explicit, probabilistic model of how they interact in human performance. Particular experimental issues to be investigated are: (1) what phonetic characteristics humans are most sensitive to when processing non-native sounds; (2) how the quality of the input and ambient acoustics affect non-native perception and production; and (3) whether learning word meaning can modulate sensitivity to detailed properties of non-native sounds. The computational model builds on a growing body of work suggesting that human perception and action reflect optimal Bayesian inference conditioned on prior expectations and noisy sensory measurements. The relevant prior reflects knowledge of the native language; the model predicts that non-native structures that are more similar to those in the native language should be processed with greater accuracy. The model also predicts that non-native sounds with robust perceptual properties should be processed more accurately, even if their prior probabilities are low. The development of this model, which will be made available to other researchers, will promote the role of phonology and phonetics within the broader context of cognitive science research. Because our experiments examine the impact of classroom acoustics and talker variation on non-native sound processing, this project also has ramifications for foreign language pedagogy.</AbstractNarration>
<MinAmdLetterDate>08/15/2011</MinAmdLetterDate>
<MaxAmdLetterDate>02/10/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1052855</AwardID>
<Investigator>
<FirstName>Lisa</FirstName>
<LastName>Davidson</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lisa B Davidson</PI_FULL_NAME>
<EmailAddress>lisa.davidson@nyu.edu</EmailAddress>
<PI_PHON>2129928761</PI_PHON>
<NSF_ID>000186579</NSF_ID>
<StartDate>08/15/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121019</ZipCode>
<StreetAddress><![CDATA[70 WASHINGTON SQUARE S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7298</Code>
<Text>COLLABORATIVE RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~174878</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Everyone has the experience of listening to an unfamilar language, and finding the words of that language difficult to hear and say correctly. Previous research has established that this difficulty results from a 'tuning' of the cognitive system for language to native (as opposed to foreign) patterns of speech, but the precise way in which native tuning accounts for cross-language perception and production errors has remained unclear. This project considered three hypotheses about how such errors could arise: in the mental decoding of sound waves into consonants and vowels; in the application of more abstract knowledge about how consonants and vowels are arranged into words (compare <em>blick</em>, which is a possible word of English, with <em>bnick</em>, which would be a possible word of Russian but not English); or in the implementation of a mental plan for speech into physical movements of the tongue, lips, and other parts of the vocal tract. Several experiments manipulated fine-grained acoustic details of the foreign words that people heard, the tasks that people were asked to perform (e.g., repeating a foreign word vs. spelling it), and other methodological details (e.g., listening to words in a quiet sound booth vs. in an ordinary college classroom; listening to one speaker of the unfamiliar language vs. multiple speakers). The results support a robust role for more peripheral cognitive processes, such as acoustic decoding and articulatory encoding, with a more limited contribution from abstract knowledge of consonant / vowel arrangement. This conclusion was drawn on the basis of analysis with modern Bayesian statistical methods and on careful consideration of each possible cognitive factor. The findings of this project could transform the scientific study of cross-language speech processing, and have an impact on foreign accent teaching, by focusing attention on how the detailed physical properties of sounds influence their mental representation. Some specific implications are that errors in pronouncing words of an unfamiliar language may not be paralleled by errors in spelling, that hearing a new language in a classroom may be more beneficial than listening to it in a sound-attenuated environment, and that hearing multiple speakers of a foreign language can lead to higher levels of correct performance than hearing one speaker. In addition to these general impacts, the project provided significant training in the statistical analysis of speech for many undergraduate and graduate research assistants, and the findings have been disseminated to the scientific community in the form of several publications and conference presentations.</p><br> <p>            Last Modified: 12/29/2017<br>      Modified by: Lisa&nbsp;B&nbsp;Davidson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Everyone has the experience of listening to an unfamilar language, and finding the words of that language difficult to hear and say correctly. Previous research has established that this difficulty results from a 'tuning' of the cognitive system for language to native (as opposed to foreign) patterns of speech, but the precise way in which native tuning accounts for cross-language perception and production errors has remained unclear. This project considered three hypotheses about how such errors could arise: in the mental decoding of sound waves into consonants and vowels; in the application of more abstract knowledge about how consonants and vowels are arranged into words (compare blick, which is a possible word of English, with bnick, which would be a possible word of Russian but not English); or in the implementation of a mental plan for speech into physical movements of the tongue, lips, and other parts of the vocal tract. Several experiments manipulated fine-grained acoustic details of the foreign words that people heard, the tasks that people were asked to perform (e.g., repeating a foreign word vs. spelling it), and other methodological details (e.g., listening to words in a quiet sound booth vs. in an ordinary college classroom; listening to one speaker of the unfamiliar language vs. multiple speakers). The results support a robust role for more peripheral cognitive processes, such as acoustic decoding and articulatory encoding, with a more limited contribution from abstract knowledge of consonant / vowel arrangement. This conclusion was drawn on the basis of analysis with modern Bayesian statistical methods and on careful consideration of each possible cognitive factor. The findings of this project could transform the scientific study of cross-language speech processing, and have an impact on foreign accent teaching, by focusing attention on how the detailed physical properties of sounds influence their mental representation. Some specific implications are that errors in pronouncing words of an unfamiliar language may not be paralleled by errors in spelling, that hearing a new language in a classroom may be more beneficial than listening to it in a sound-attenuated environment, and that hearing multiple speakers of a foreign language can lead to higher levels of correct performance than hearing one speaker. In addition to these general impacts, the project provided significant training in the statistical analysis of speech for many undergraduate and graduate research assistants, and the findings have been disseminated to the scientific community in the form of several publications and conference presentations.       Last Modified: 12/29/2017       Submitted by: Lisa B Davidson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
