<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: A Human-Level, Real-Time, Integrated Agent</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>01/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>448090.00</AwardTotalIntnAmount>
<AwardAmount>464090</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project is developing and integrating statistical and symbolic methods of Artificial Intelligence in an agent architecture and evaluating the agent in a competitive domain, notably the real-time strategy game StarCraft. Real-time strategy (RTS) games provide several interesting research challenges including real-time decision making, enormous state spaces and imperfect information. StarCraft is a popular commercial RTS game that has several professional gaming leagues, and therefore ideal for evaluating the performance of AI agents. Professional StarCraft players reason about and react to strategic decisions at multiple levels of abstraction, sometimes executing over 300 game actions per minute, so developing competition-level StarCraft agents presents extraordinary challenges.  &lt;br/&gt;&lt;br/&gt;More specifically, the project is using novel supervised and unsupervised learning algorithms to automatically learn domain knowledge from collections of professional gameplay traces; the agent is being implemented within the reactive planning architecture ABL (A Behavior Language). The ABL reactive planner provides the glue for integrating multiple, heterogeneous reasoners within a real-time execution environment. &lt;br/&gt;&lt;br/&gt;This work is expected to make significant contributions to the understanding of decision making processes  in a complex, real-time domain. This understanding will contribute to the development of robust, intelligent systems that can be deployed within real-world environments. This work will motivate AI researchers to build integrated agent architectures. As a well-known game with very high-level professional play, research in StarCraft AI has the potential to attract significant attention to AI research. The StarCraft competition being hosted by our lab  has attracted significant interest both within and outside academia, and at the high-school, undergraduate and graduate level. Thus, this work has the potential to raise general public awareness in research in human-level AI, and will encourage high-school students to pursue careers in computer science and game design.</AbstractNarration>
<MinAmdLetterDate>08/26/2010</MinAmdLetterDate>
<MaxAmdLetterDate>04/19/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1018954</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Mateas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Mateas</PI_FULL_NAME>
<EmailAddress>michaelm@soe.ucsc.edu</EmailAddress>
<PI_PHON>8314591789</PI_PHON>
<NSF_ID>000426854</NSF_ID>
<StartDate>08/26/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Arnav</FirstName>
<LastName>Jhala</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arnav H Jhala</PI_FULL_NAME>
<EmailAddress>ahjhala@ncsu.edu</EmailAddress>
<PI_PHON>9195136698</PI_PHON>
<NSF_ID>000547005</NSF_ID>
<StartDate>08/26/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Cruz</Name>
<CityName>Santa Cruz</CityName>
<ZipCode>950641077</ZipCode>
<PhoneNumber>8314595278</PhoneNumber>
<StreetAddress>1156 High Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>125084723</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA CRUZ</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Cruz]]></Name>
<CityName>Santa Cruz</CityName>
<StateCode>CA</StateCode>
<ZipCode>950641077</ZipCode>
<StreetAddress><![CDATA[1156 High Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~448090</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Resarch Outcomes :</p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica} --> <ol> <li>Using StarCraft as an application domain, we have implemented an agent in ABL to&nbsp; demonstrate the ability of reactive planning to support authoring of complex intelligentagents. By presenting new idioms, we have shown concretely how a reactive planninglanguage like ABL provides the structure required to support multi-scale game AI. Our reactive planning agent reasons at multiple scales and across many concurrent goalsin order to perform well in the StarCraft domain. Different threads communicate with eachother and cooperate to give rise to effective unit control in which complicated multi-unitbehaviors are explicit rather than emergent. The explicit property of our higher-leveltactics allows complicated strategic reasoning processes to deal directly with these tactics,instead of trying to manipulate the state of individual units to give rise to some desiredemergent behavior. Using this uni?ed reasoning architecture, our multi-scale agent is ableto play competitively against the built-in StarCraft AI. The agent was tested against all three races on three professional gaming maps thatencourage different styles of gameplay. The agent achieved a win rate of over 60% against all of the races.</li> <li>We developed an approach for using vector fields to effectively spread units prior to engaging an enemy. Our technique copes with uncertainty in RTS games by requiring only an approximation of the opponent&rsquo;s location. We evaluate our implementation in a micromanagement domain of Starcraft Broodwar analogous to the first tournament of the 2010 AIIDE Starcraft AI Competition. In our evaluations a total of 2,400 simulations were run over four different scenarios that varied the user-controlled race, the size of the armies, and unit composition. In each scenario we paired a vector field bot using either imperfect or perfect information against the Native Starcraft AI. In addition, we paired the Native Starcraft AI against itself as a baseline comparison.The Native Starcraft Bot performed as anticipated in scenarios where both players controlled equal units &ndash; winning around 50% of the time against itself. In these scenarios our vector field bots using imperfect and perfect information won 90% and 97% of the time, respectively. In scenarios where the units were not equal the Native Starcraft AI won less frequently against itself, suggesting a disadvantageous unit composition. The vector field bot using perfect information won 150% more frequently than the baseline, and using imperfect information won 120% more frequently.</li> <li>We use case-based goal formulation (Weber, Mateas, and Jhala 2010b) to select goals for the agent to pursue and anticipate the actions of adversaries. The components are integrated within a reactive planner that manages the active goals of the agent. We apply learning from demonstration for GDA to an agent that plays the real-time strategy (RTS) game StarCraft. Using the GDA model enables the agent to emulate a subset of the cognitive processes necessary for RTS gameplay. RTS games are an excellent domain for AI research because they contain many real-world properties (Buro 2003), enable investigation of integrative AI approaches (Laird and VanLent 2001), and provide huge data sets available for analysis in the form of professional player replays (Weber and Mateas 2009). We extract examples from professional replays enabling the agent to estimate hidden game state, anticipate opponent actions, and select goals to pursue. Our results show that integrating these processes into the agent using the GDA model greatly improves the gameplay performance of the system, while reducing the domain engineering task.</li> </ol> <p>&nbsp;</p> <p>Mentoring Outcomes :</p> <ol> <li>Dr. Ben Weber completed his PhD with funding from the award and successfully transitioned to research in industry where he currently serves as Senior Data Scientist at Twich.</li> <li>Dr. Peter Mawhorter who was partially funded on the project currently is at Pomona College after finished PhD.</li> <li>Michael Leece and Trevor Santarra successfully defended their thesis proposals and have plans to join Sentient Systems and Unity AI Research Lab.</li> <li>Dustin Escoffery, Adrian Young, Wesley Souza, and Daniel Montalvo who worked as MS and REU students on the project have successfully graduated and transitioned to industry positions at Blizzard, Microsoft, and Zynga.</li> </ol> <p>&nbsp;</p> <p>The overall broader impact of the project has been significant. The games program at UCSC has significantly grown and generated interest in Engineering majors for undergraduate students. The StarCraft competition continues to encourage submissions for game AI from teams globally.&nbsp;</p><br> <p>            Last Modified: 08/07/2017<br>      Modified by: Arnav&nbsp;H&nbsp;Jhala</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Resarch Outcomes :   Using StarCraft as an application domain, we have implemented an agent in ABL to  demonstrate the ability of reactive planning to support authoring of complex intelligentagents. By presenting new idioms, we have shown concretely how a reactive planninglanguage like ABL provides the structure required to support multi-scale game AI. Our reactive planning agent reasons at multiple scales and across many concurrent goalsin order to perform well in the StarCraft domain. Different threads communicate with eachother and cooperate to give rise to effective unit control in which complicated multi-unitbehaviors are explicit rather than emergent. The explicit property of our higher-leveltactics allows complicated strategic reasoning processes to deal directly with these tactics,instead of trying to manipulate the state of individual units to give rise to some desiredemergent behavior. Using this uni?ed reasoning architecture, our multi-scale agent is ableto play competitively against the built-in StarCraft AI. The agent was tested against all three races on three professional gaming maps thatencourage different styles of gameplay. The agent achieved a win rate of over 60% against all of the races. We developed an approach for using vector fields to effectively spread units prior to engaging an enemy. Our technique copes with uncertainty in RTS games by requiring only an approximation of the opponent?s location. We evaluate our implementation in a micromanagement domain of Starcraft Broodwar analogous to the first tournament of the 2010 AIIDE Starcraft AI Competition. In our evaluations a total of 2,400 simulations were run over four different scenarios that varied the user-controlled race, the size of the armies, and unit composition. In each scenario we paired a vector field bot using either imperfect or perfect information against the Native Starcraft AI. In addition, we paired the Native Starcraft AI against itself as a baseline comparison.The Native Starcraft Bot performed as anticipated in scenarios where both players controlled equal units &ndash; winning around 50% of the time against itself. In these scenarios our vector field bots using imperfect and perfect information won 90% and 97% of the time, respectively. In scenarios where the units were not equal the Native Starcraft AI won less frequently against itself, suggesting a disadvantageous unit composition. The vector field bot using perfect information won 150% more frequently than the baseline, and using imperfect information won 120% more frequently. We use case-based goal formulation (Weber, Mateas, and Jhala 2010b) to select goals for the agent to pursue and anticipate the actions of adversaries. The components are integrated within a reactive planner that manages the active goals of the agent. We apply learning from demonstration for GDA to an agent that plays the real-time strategy (RTS) game StarCraft. Using the GDA model enables the agent to emulate a subset of the cognitive processes necessary for RTS gameplay. RTS games are an excellent domain for AI research because they contain many real-world properties (Buro 2003), enable investigation of integrative AI approaches (Laird and VanLent 2001), and provide huge data sets available for analysis in the form of professional player replays (Weber and Mateas 2009). We extract examples from professional replays enabling the agent to estimate hidden game state, anticipate opponent actions, and select goals to pursue. Our results show that integrating these processes into the agent using the GDA model greatly improves the gameplay performance of the system, while reducing the domain engineering task.      Mentoring Outcomes :  Dr. Ben Weber completed his PhD with funding from the award and successfully transitioned to research in industry where he currently serves as Senior Data Scientist at Twich. Dr. Peter Mawhorter who was partially funded on the project currently is at Pomona College after finished PhD. Michael Leece and Trevor Santarra successfully defended their thesis proposals and have plans to join Sentient Systems and Unity AI Research Lab. Dustin Escoffery, Adrian Young, Wesley Souza, and Daniel Montalvo who worked as MS and REU students on the project have successfully graduated and transitioned to industry positions at Blizzard, Microsoft, and Zynga.      The overall broader impact of the project has been significant. The games program at UCSC has significantly grown and generated interest in Engineering majors for undergraduate students. The StarCraft competition continues to encourage submissions for game AI from teams globally.        Last Modified: 08/07/2017       Submitted by: Arnav H Jhala]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
