<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TC:Large:Collaborative Research:Anonymizing Textual Data and its Impact on Utility</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>574430.00</AwardTotalIntnAmount>
<AwardAmount>574430</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Data Protection laws that exempt data that is not individually&lt;br/&gt;identifiable have led to an explosion in anonymization research.&lt;br/&gt;Unfortunately, how well current de-identification and anonymization&lt;br/&gt;techniques control risks to privacy and confidentiality is not well&lt;br/&gt;understood.  Neither is the usefulness of anonymized data for real-world&lt;br/&gt;applications.  The project addresses anonymization on three fronts:&lt;br/&gt;&lt;br/&gt;1) Textual data, even when explicit identifiers are removed (names,&lt;br/&gt;dates, locations), can contain highly identifiable information.  For&lt;br/&gt;example, a sample of chief complaint fields from the Indiana Network&lt;br/&gt;for Patient Care (INPC) found several instances of "phantom limb&lt;br/&gt;pain".  Amputees can be visually identifiable, but the HIPAA Safe&lt;br/&gt;Harbor rules do not list this as "identifying information".  Any&lt;br/&gt;policy explicitly listing all types of identifying data is likely to&lt;br/&gt;fail.  Through a joint effort with computer science and linguistics,&lt;br/&gt;the project is developing new methods to remove specific details from&lt;br/&gt;text while preserving meaning, eliminating such highly identifiable&lt;br/&gt;information without a priori knowledge of what would be identifying.&lt;br/&gt;&lt;br/&gt;2) Current anonymization research is based on unproven measures of&lt;br/&gt;identifiability.  Through a re-identification challenge on synthetic&lt;br/&gt;data (but based on real healthcare data), the project is evaluating&lt;br/&gt;the efficacy of these measures.  Interdisciplinary teams of students&lt;br/&gt;are given challenge problems - anonymized data with hypothetical&lt;br/&gt;healthcare data - and asked to make (hypothetical) inferences about&lt;br/&gt;health information of individuals.  The results can be used to&lt;br/&gt;calibrate the effectiveness of different anonymization measures.&lt;br/&gt;&lt;br/&gt;3) The utility of anonymized data has been a concern among research:&lt;br/&gt;Does anonymized data provide credible research results?  By partnering&lt;br/&gt;with healthcare studies at the Kinsey Institute and Purdue University&lt;br/&gt;School of Nursing, the project is comparing analyses on original data&lt;br/&gt;with analyses on anonymized data, and evaluating the impact of types&lt;br/&gt;of anonymization on research results.  A related issue is determining&lt;br/&gt;the impact on data collection: Are individuals more candid in their&lt;br/&gt;responses if they know data will be anonymized?  Outcomes are broadening&lt;br/&gt;the scope of research that can be performed on anonymized data, while&lt;br/&gt;ensuring that researchers know when access to individually identifiable&lt;br/&gt;data (with attendant restrictions and safeguards) is needed.&lt;br/&gt;&lt;br/&gt;Through these tasks, the project is advancing our ability to utilize&lt;br/&gt;the wealth of data we now collect for the benefit of society, while&lt;br/&gt;ensuring individual privacy is protected.&lt;br/&gt;&lt;br/&gt;For further information see the project web site at the URL:&lt;br/&gt;http://projects.cerias.purdue.edu/TextAnon</AbstractNarration>
<MinAmdLetterDate>09/02/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/02/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1012081</AwardID>
<Investigator>
<FirstName>Stephanie</FirstName>
<LastName>Sanders</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephanie A Sanders</PI_FULL_NAME>
<EmailAddress>sanders@indiana.edu</EmailAddress>
<PI_PHON>8128550101</PI_PHON>
<NSF_ID>000197344</NSF_ID>
<StartDate>09/02/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Raquel</FirstName>
<LastName>Hill</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raquel L Hill</PI_FULL_NAME>
<EmailAddress>ralhill@indiana.edu</EmailAddress>
<PI_PHON>8128565807</PI_PHON>
<NSF_ID>000260075</NSF_ID>
<StartDate>09/02/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474013654</ZipCode>
<StreetAddress><![CDATA[509 E 3RD ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~574430</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The sharing of medical data has many possible benefits, which include: the creation of unified data visualizations for clinicians, the development of predictive and diagnostic support systems, reductions in institutional costs, and improvements in medical care. Medical data are often not shared with external parties because even when transformations that are intended to preserve individual privacy are applied to raw data, e.g. when data is de-identified per US HIPAA Safe Harbor rules or when data is transformed per the UK ICO&rsquo;s Anonymization Guidelines, sharing of such data may still introduce privacy risks. This is because privacy does not only depend on the transformed data but also on context-specific information, such as shared demographic data of subjects, presence of fields that may be linked across other existing databases, social relationships among subjects, and profiles of the data recipient.</p> <p>Preserving the privacy of medical data is even more challenging because the combination of unstructured textual narratives, video, images, audio and other health-focused data modalities create unique medical profiles of individual. These profiles can be so individually identifiable that traditional privacy transformations cannot be used to create records that are privacy-preserving. Therefore sharing these data with external parties may become a lengthy process of negotiating specific use agreements. Sharing of the data among researchers within the organization where the data resides also creates privacy risks. Even when traditional identifiers are removed, the uniqueness of the resulting records may make a privacy breach easy to orchestrate or implement.</p> <p>The project explores privacy vulnerabilities in hospital discharge records, medical datasets and social science datasets that contain medical diagnoses. <em>Intellectual Merit</em>, This project characterizes the re-identification vulnerabilities in social science data, applies techniques for mitigating the risks, and develops a use-case driven framework for evaluating the utility of the transformed data. The research resulted in publications that assess how unique features impact privacy and that evaluate the utility of differentially private social science data.&nbsp; We evaluate the statistical characteristics of two high-dimensional social science datasets to better understand how unique features impact privacy.&nbsp;<em>Broader Impacts</em>, This work is the first to characterize re-identification risks in high-dimensional data that is collected in surveys designed to capture the various behaviors and experiences of groups of individuals. It informs social scientists of the privacy risks that are inherent to their data, the data collection techniques that limit re-identification, and the usefulness of data that has been anonymized using emerging obfuscation techniques.</p> <p>We apply a class of statistical de-anonymization attacks in an attempt to achieve theoretical re-identification of participants. We assume that an attacker has exact knowledge of a subset of attribute values for a particular record, and wants to link this subset of data to the actual record to discover the remaining content. We show that although 98% of the records within the dataset are unique given any three attributes, re-identification of the records may not be easily achieved. We attribute limited re-identification to the inherent similarity in the human behavior that the scientists measure. This work is the first to characterize re-identification risks in high-dimensional data that is collected in surveys designed to capture the various behaviors and experiences of groups of individuals. Approximately 33% of these records are unique based on self-reported medical related text.&nbsp; These text entries are often trivially unique due to misspellings. We feel that there is great opportunity to apply textual anonymization techniques to aggregate this textual input and reduce uniqueness for single attributes.&nbsp;&nbsp;</p> <p>We present an empirical evaluation of a differentially private (DP) behavioral science dataset. The goal of the analysis was to better understand whether DP data could be shared and within what context. Therefore, we sought to identify the data characteristics and the analytical results that are preserved even when DP noise is applied to the dataset. Our results confirm that dimensionality is a major challenge for DP algorithms, especially when the number of records in the database is sufficiently less than the number of cells covered by the query. One very interesting finding is that all algorithms produced noisy histograms that had strong results for Feature Importance. While most of our social sciences collaborators tend to favor regression-based analysis, these results indicate that there is a privacy-preserving incentive for using other data mining and machine learning techniques for data analysis, especially when coupled with DP data. Such techniques may be used by researchers during the process of collecting preliminary data and trying to identify features for their model.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/30/2017<br>      Modified by: Raquel&nbsp;L&nbsp;Hill</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The sharing of medical data has many possible benefits, which include: the creation of unified data visualizations for clinicians, the development of predictive and diagnostic support systems, reductions in institutional costs, and improvements in medical care. Medical data are often not shared with external parties because even when transformations that are intended to preserve individual privacy are applied to raw data, e.g. when data is de-identified per US HIPAA Safe Harbor rules or when data is transformed per the UK ICO?s Anonymization Guidelines, sharing of such data may still introduce privacy risks. This is because privacy does not only depend on the transformed data but also on context-specific information, such as shared demographic data of subjects, presence of fields that may be linked across other existing databases, social relationships among subjects, and profiles of the data recipient.  Preserving the privacy of medical data is even more challenging because the combination of unstructured textual narratives, video, images, audio and other health-focused data modalities create unique medical profiles of individual. These profiles can be so individually identifiable that traditional privacy transformations cannot be used to create records that are privacy-preserving. Therefore sharing these data with external parties may become a lengthy process of negotiating specific use agreements. Sharing of the data among researchers within the organization where the data resides also creates privacy risks. Even when traditional identifiers are removed, the uniqueness of the resulting records may make a privacy breach easy to orchestrate or implement.  The project explores privacy vulnerabilities in hospital discharge records, medical datasets and social science datasets that contain medical diagnoses. Intellectual Merit, This project characterizes the re-identification vulnerabilities in social science data, applies techniques for mitigating the risks, and develops a use-case driven framework for evaluating the utility of the transformed data. The research resulted in publications that assess how unique features impact privacy and that evaluate the utility of differentially private social science data.  We evaluate the statistical characteristics of two high-dimensional social science datasets to better understand how unique features impact privacy. Broader Impacts, This work is the first to characterize re-identification risks in high-dimensional data that is collected in surveys designed to capture the various behaviors and experiences of groups of individuals. It informs social scientists of the privacy risks that are inherent to their data, the data collection techniques that limit re-identification, and the usefulness of data that has been anonymized using emerging obfuscation techniques.  We apply a class of statistical de-anonymization attacks in an attempt to achieve theoretical re-identification of participants. We assume that an attacker has exact knowledge of a subset of attribute values for a particular record, and wants to link this subset of data to the actual record to discover the remaining content. We show that although 98% of the records within the dataset are unique given any three attributes, re-identification of the records may not be easily achieved. We attribute limited re-identification to the inherent similarity in the human behavior that the scientists measure. This work is the first to characterize re-identification risks in high-dimensional data that is collected in surveys designed to capture the various behaviors and experiences of groups of individuals. Approximately 33% of these records are unique based on self-reported medical related text.  These text entries are often trivially unique due to misspellings. We feel that there is great opportunity to apply textual anonymization techniques to aggregate this textual input and reduce uniqueness for single attributes.    We present an empirical evaluation of a differentially private (DP) behavioral science dataset. The goal of the analysis was to better understand whether DP data could be shared and within what context. Therefore, we sought to identify the data characteristics and the analytical results that are preserved even when DP noise is applied to the dataset. Our results confirm that dimensionality is a major challenge for DP algorithms, especially when the number of records in the database is sufficiently less than the number of cells covered by the query. One very interesting finding is that all algorithms produced noisy histograms that had strong results for Feature Importance. While most of our social sciences collaborators tend to favor regression-based analysis, these results indicate that there is a privacy-preserving incentive for using other data mining and machine learning techniques for data analysis, especially when coupled with DP data. Such techniques may be used by researchers during the process of collecting preliminary data and trying to identify features for their model.          Last Modified: 03/30/2017       Submitted by: Raquel L Hill]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
