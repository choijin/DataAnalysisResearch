<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: The File Drawer Problem</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2011</AwardEffectiveDate>
<AwardExpirationDate>02/28/2013</AwardExpirationDate>
<AwardTotalIntnAmount>10000.00</AwardTotalIntnAmount>
<AwardAmount>10000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert O'Connor</SignBlockName>
<PO_EMAI>roconnor@nsf.gov</PO_EMAI>
<PO_PHON>7032927263</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scientific progress depends on unimpeded information sharing between scientists, and from scientists to the public. Scientific problems, however, are typically ambiguously defined, involve perverse institutional incentives, and promote competition between scientists to come to conclusions before others. As a result of these features, unexpected and unwanted results are common and may not always be reported. If information is not shared, scientists may be more likely to repeat one another's mistakes, reach false conclusions, and be overconfident in their conclusions. This dissertation combines analysis and experimentation to examine the reasoning processes underlying decisions to share information, the usefulness of information shared, and the social costs of failure to properly share information. Possible solutions to these problems will be tested in a field trial.&lt;br/&gt;&lt;br/&gt;Six experiments will use lay participants in simulated science environments to examine the effects of hindsight, institutional incentives, ambiguity, and competition on their scientific reasoning and decisions about sharing information. In the first two experiments, lay participants will be given descriptions and results of experiments and must evaluate the quality of the research and whether the results should be reported. Some participants will be asked to evaluate the results in foresight, whereas others will be asked to evaluate the results in hindsight. We hypothesize that participants will conclude that hypothetical experiments are in error and should not be reported when they disconfirm the participants? prior beliefs, and that this tendency will be worse in hindsight than in foresight. In a second set of experiments, participants will address three challenges that scientists commonly face: perverse incentives, ambiguity, and competition. When reaching a conclusion can lead to a financial payoff, it is likely that participants will interpret disconfirming evidence as error and decide not to report this evidence. Ambiguity and competition are likely to enhance this perverse effect of incentives. In a final experiment, the trial history of participants from these first three studies will be shared with a new participant who will try to solve the same rule. With these experiments it is possible to examine the reasoning and potential social cost of biased information sharing.</AbstractNarration>
<MinAmdLetterDate>03/02/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/02/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1061315</AwardID>
<Investigator>
<FirstName>Baruch</FirstName>
<LastName>Fischhoff</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Baruch Fischhoff</PI_FULL_NAME>
<EmailAddress>baruch@cmu.edu</EmailAddress>
<PI_PHON>4122683246</PI_PHON>
<NSF_ID>000329372</NSF_ID>
<StartDate>03/02/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Davis</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander L Davis</PI_FULL_NAME>
<EmailAddress>ald1@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122162040</PI_PHON>
<NSF_ID>000571255</NSF_ID>
<StartDate>03/02/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>PITTSBURGH</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1321</Code>
<Text>Decision, Risk &amp; Mgmt Sci</Text>
</ProgramElement>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~10000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="text-align: center;">The File-Drawer Problem: Public Outcomes Report</p> <p style="text-align: center;">Alexander L. Davis</p> <p style="text-align: center;">Baruch Fischhoff</p> <p style="text-align: center;">Department of Engineering and Public Policy</p> <p style="text-align: center;">Carnegie Mellon University</p> <p style="text-align: center;">February 8, 2013</p> <p>The NSF Decision, Risk, and Management Sciences dissertation enhancement grant supported dissertation research that combines experimental and quantitative approaches to study scientific reasoning and communication. The dissertation was successfully defended on September 25, 2012, and two associated papers have been submitted for publication. The dissertation and papers look at the interplay between scientific reasoning, the timing of scientific judgments, and incentives for communicating scientific results. All data, materials, and statistical analyses are publicly available at Harvard's Dataverse. (1)</p> <p>The first paper finds that laypeople attribute an unexpected experimental result to error to the same degree in foresight and hindsight when given a description of a hypothetical scientific experiment. When participants felt that they could neither pinpoint the causes of the experimental result, nor make precise posterior predictions about replications of the experiment, they judged that the results should not be shared with the scientific community.</p> <p>The second paper uses the Wason 2-4-6 rule-discovery task, finding that laypeople attribute disconfirming results to error more than they do affirming results. When feedback was judged to be an error, they shared the feedback with another person completing the same task at a lower rate than results seen as accurate, even after controlling for actual error. However, when given an incentive to share only convincing results, their error identifications were not biased, nor was the data they shared deceptive.</p> <p>The dissertation advances the knowledge of decisions to share data from normative, descriptive, and prescriptive perspectives. The normative research finds that there are few, if any, cases where scientists should not share data. The descriptive research finds that laypeople do not agree, instead seeing unexpected disconfirming results as not worthy of sharing. This knowledge can promote better data sharing within DRMS and science more broadly, and provides a new understanding of the psychology of data sharing, hypothesis testing, and communication.</p> <p>The broader impacts include promoting a deeper understanding of when and why scientists choose to share data, making clear the need for institutional and educational approaches to help scientists share data effectively. This can benefit society by allowing scientists to understand why they may not decide to share data, and with proper training, how they might make better decisions about making data available to their colleagues and the public.</p> <p>&nbsp;</p> <p>1) http://hdl.handle.net/1902.1/18699 and http://hdl.handle.net/1902.1/14819</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/14/2013<br>      Modified by: Baruch&nbsp;Fischhoff</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The File-Drawer Problem: Public Outcomes Report Alexander L. Davis Baruch Fischhoff Department of Engineering and Public Policy Carnegie Mellon University February 8, 2013  The NSF Decision, Risk, and Management Sciences dissertation enhancement grant supported dissertation research that combines experimental and quantitative approaches to study scientific reasoning and communication. The dissertation was successfully defended on September 25, 2012, and two associated papers have been submitted for publication. The dissertation and papers look at the interplay between scientific reasoning, the timing of scientific judgments, and incentives for communicating scientific results. All data, materials, and statistical analyses are publicly available at Harvard's Dataverse. (1)  The first paper finds that laypeople attribute an unexpected experimental result to error to the same degree in foresight and hindsight when given a description of a hypothetical scientific experiment. When participants felt that they could neither pinpoint the causes of the experimental result, nor make precise posterior predictions about replications of the experiment, they judged that the results should not be shared with the scientific community.  The second paper uses the Wason 2-4-6 rule-discovery task, finding that laypeople attribute disconfirming results to error more than they do affirming results. When feedback was judged to be an error, they shared the feedback with another person completing the same task at a lower rate than results seen as accurate, even after controlling for actual error. However, when given an incentive to share only convincing results, their error identifications were not biased, nor was the data they shared deceptive.  The dissertation advances the knowledge of decisions to share data from normative, descriptive, and prescriptive perspectives. The normative research finds that there are few, if any, cases where scientists should not share data. The descriptive research finds that laypeople do not agree, instead seeing unexpected disconfirming results as not worthy of sharing. This knowledge can promote better data sharing within DRMS and science more broadly, and provides a new understanding of the psychology of data sharing, hypothesis testing, and communication.  The broader impacts include promoting a deeper understanding of when and why scientists choose to share data, making clear the need for institutional and educational approaches to help scientists share data effectively. This can benefit society by allowing scientists to understand why they may not decide to share data, and with proper training, how they might make better decisions about making data available to their colleagues and the public.     1) http://hdl.handle.net/1902.1/18699 and http://hdl.handle.net/1902.1/14819          Last Modified: 02/14/2013       Submitted by: Baruch Fischhoff]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
