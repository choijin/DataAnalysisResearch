<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ATD: Stochastic algorithms for countering  chemical and biological threats</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>896241.00</AwardTotalIntnAmount>
<AwardAmount>896241</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The vision for the proposed work is to capitalize on experience  in specific areas involving biological / chemical attacks and to develop as wide-ranging algorithms as possible for countermeasures to such threats. These will be based on stochastic modeling to describe (deterministic and random) features of contamination location and spread from which algorithms to detect threats may be developed. Standard parametric models will be examined for applicability, along with development of appropriate space-time random fields tailored to the specific situation. This is similar in approach to current use of Gaussian fields to model trends and fluctuations in pollution regulation studies, requiring few specific assumptions, but allowing application of general methods such as Central Limit and Extreme Value Theory. The starting point will be the PI's prior experience in assisting EPA scientists and engineers with the development of statistical designs for assuring confidence in complete decontamination of anthrax from buildings in the face of budgetary limitations which was based on very simple assumptions such as statistical independence of positions of anthrax deposits.  Better and more cost effective statistical conclusions are likely to result from the proposed models - relaxing the independence assumptions.  &lt;br/&gt;&lt;br/&gt;Clearance of contaminated buildings is typically achieved by Chlorine Dioxide fumigation - a highly effective process since the gas under appropriate pressure and duration of application can be expected to reach all areas where contaminant may be lodged. The cost of and time required for such clearance can be quite staggering (e.g. estimates of $130 million for just the Brentwood postal facility over a 26 month period, and $1 billion in all for the total of the early post 9/11 incidents). Additionally, in spite of a high expectation that fumigation achieves total clearance, systematic sampling of surfaces for residual anthrax is done for confirmation. This cannot be exhaustive in view of the high cost of sampling and laboratory analysis and the sheer number of samples required for reasonably complete coverage. Hence, determinations are made (using simple statistical assumptions) of the extent of sampling required to ensure that if all samples turn out to be negative, there is specified very high confidence that the entire area of concern is entirely contaminant free. This seems a reasonable approach to give adequate added assurance of successful clearance since if even one sample tested positive for anthrax, entire fumigation would have to be repeated. Current routines for determining necessary sampling to achieve a given clearance confidence have been developed under simple assumptions. The future work on this topic will use the more realistic statistical assumptions of stochastic modeling which are expected to lead to algorithms, which are both more accurate and cost efficient.  The above case of building decontamination is described as a starting point for much more complex investigations such as detection of the presence of contaminants being actively introduced in an HVAC system for circulation throughout a building - a topic which has received some previous attention in the literature. A related area of initial activity concerns the discrimination between toxic substances such as anthrax and harmless powders for which novel statistical (for example, wavelet) methods are already being developed in collaboration with EPA researchers. Further, the issues involved in such "Homeland examples" arise in various forms in the detection of battlefield threats (such as hidden IED's) with attendant problems of signal detection - subjects for consideration under this grant. Finally, the methods are expected to have useful "non-terrorism" applications such as to the spread of pandemics and risks of importation of anthrax bearing animal products such as meat and hides.</AbstractNarration>
<MinAmdLetterDate>09/16/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016441</AwardID>
<Investigator>
<FirstName>M. Ross</FirstName>
<LastName>Leadbetter</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>M. Ross Leadbetter</PI_FULL_NAME>
<EmailAddress>mrl@stat.unc.edu</EmailAddress>
<PI_PHON>9199621040</PI_PHON>
<NSF_ID>000253286</NSF_ID>
<StartDate>09/16/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amarjit</FirstName>
<LastName>Budhiraja</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amarjit Budhiraja</PI_FULL_NAME>
<EmailAddress>budhiraj@email.unc.edu</EmailAddress>
<PI_PHON>9199622189</PI_PHON>
<NSF_ID>000254438</NSF_ID>
<StartDate>09/16/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jan</FirstName>
<LastName>Hannig</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jan Hannig</PI_FULL_NAME>
<EmailAddress>jan.hannig@unc.edu</EmailAddress>
<PI_PHON>9199627511</PI_PHON>
<NSF_ID>000330856</NSF_ID>
<StartDate>09/16/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>CHAPEL HILL</CityName>
<StateCode>NC</StateCode>
<ZipCode>275991350</ZipCode>
<StreetAddress><![CDATA[104 AIRPORT DR STE 2200]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7552</Code>
<Text>COFFES</Text>
</ProgramElement>
<ProgramElement>
<Code>J184</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>6877</Code>
<Text>ALGORITHMS IN THREAT DETECTION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~292454</FUND_OBLG>
<FUND_OBLG>2011~299950</FUND_OBLG>
<FUND_OBLG>2012~303837</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Contributions were made in several distinct areas:&nbsp; (1) Detection of harmful substances using a statistical detection algorithm based on Laser induced plasma spectroscopy data, (2) Dynamic detection of a contaminant source in a diffusive medium based on mathematical analysis of a model for evolution of chemical concentrations, (3) Decontamination of buildings -- selection of test locations for optimal clearance, (4) Detection of mislabeled chemical substances in large chemical libraries, (5) Volatility detection in financial data, (6) Detection of changes in viral genome and information integration from heterogeneous sources. Some highlights are as follows:</p> <p>(1) When a large building, complex or area has been contaminated with a potentially harmful substance, it can be costly and require a substantial amount of time to test all unknown substances encountered within the facility to be analyzed in a laboratory by an expert.&nbsp; Portable laser-induced breakdown spectroscopy (LIBS) devices have been developed, which produce spectra of the unknown substances to aid in classification of the substance and, hence, in locating the most contaminated areas.&nbsp; Each spectrum produced should be classified as harmful or not. In the PIs paper, a new statistical technique was presented for distinguishing Bacillus anthracis spore powder from other innocuous powders using LIBS spectra.</p> <p>(2) Consider a problem setting where a harmful contaminant is introduced at a given rate at an unknown site and diffuses dynamically over time in the surrounding medium (air or water) Suppose that one is able to measure the amount of contaminant at any given site through a suitable sensor.&nbsp; The goal is to detect the source site by using as few sensor measurements as possible in the shortest possible amount of time.&nbsp; A similar problem arises when considering spread of malware over large computer networks. The PIs proposed an algorithm that is based on a suitable diffusion approximation of the underlying transport mechanism and a related optimization problem.</p> <p>(3) A Wikipedia article &ldquo;2001 anthrax attacks&rdquo; well documents the history of such attacks.&nbsp;&nbsp; These starkly indicate the importance of being able to quantify the effectiveness of decontamination procedures. The standard means of decontamination is fumigation by gases, primarily Chlorine dioxide, clearance being evaluated by placing anthrax (BA) spore strips on a rectangular lattice of points throughout the building at different time points. The number of test locations is usually just a small fraction of the number that would be required for complete certainty.&nbsp; Hence it is important to obtain the confidence in complete coverage, which is the probability of no contamination at any non-test points, when there is found to be none at any test point.</p> <p>This requires some statistical modeling of the fumigation pattern, common assumptions being that presence of contamination at different time points are statistically independent with the same distribution.&nbsp;&nbsp; Based on simple notions and any prior knowledge of contaminant pattern it can be simple to calculate the probability of no contamination anywhere when all n test points are found to be clear.&nbsp;&nbsp; This routine has been refined under the name &ldquo;VSP&rdquo; (Visual Sampling Plan) by Pacific NW National. &nbsp;The strong independence assumptions in VSP are appealing for theoretical simplicity but do tend to require more testing points to be checked to ensure clearance.&nbsp;&nbsp;&nbsp; Hence recognition of statistical dependence can actually improve clearance confidence estimation.&nbsp;&nbsp; In our grant activities we have improved upon VSO by extensively simulating more complex models with dependence and produced algorithms leading to high confidence.&nbsp; The improvement shown in our current analyses is ver...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Contributions were made in several distinct areas:  (1) Detection of harmful substances using a statistical detection algorithm based on Laser induced plasma spectroscopy data, (2) Dynamic detection of a contaminant source in a diffusive medium based on mathematical analysis of a model for evolution of chemical concentrations, (3) Decontamination of buildings -- selection of test locations for optimal clearance, (4) Detection of mislabeled chemical substances in large chemical libraries, (5) Volatility detection in financial data, (6) Detection of changes in viral genome and information integration from heterogeneous sources. Some highlights are as follows:  (1) When a large building, complex or area has been contaminated with a potentially harmful substance, it can be costly and require a substantial amount of time to test all unknown substances encountered within the facility to be analyzed in a laboratory by an expert.  Portable laser-induced breakdown spectroscopy (LIBS) devices have been developed, which produce spectra of the unknown substances to aid in classification of the substance and, hence, in locating the most contaminated areas.  Each spectrum produced should be classified as harmful or not. In the PIs paper, a new statistical technique was presented for distinguishing Bacillus anthracis spore powder from other innocuous powders using LIBS spectra.  (2) Consider a problem setting where a harmful contaminant is introduced at a given rate at an unknown site and diffuses dynamically over time in the surrounding medium (air or water) Suppose that one is able to measure the amount of contaminant at any given site through a suitable sensor.  The goal is to detect the source site by using as few sensor measurements as possible in the shortest possible amount of time.  A similar problem arises when considering spread of malware over large computer networks. The PIs proposed an algorithm that is based on a suitable diffusion approximation of the underlying transport mechanism and a related optimization problem.  (3) A Wikipedia article "2001 anthrax attacks" well documents the history of such attacks.   These starkly indicate the importance of being able to quantify the effectiveness of decontamination procedures. The standard means of decontamination is fumigation by gases, primarily Chlorine dioxide, clearance being evaluated by placing anthrax (BA) spore strips on a rectangular lattice of points throughout the building at different time points. The number of test locations is usually just a small fraction of the number that would be required for complete certainty.  Hence it is important to obtain the confidence in complete coverage, which is the probability of no contamination at any non-test points, when there is found to be none at any test point.  This requires some statistical modeling of the fumigation pattern, common assumptions being that presence of contamination at different time points are statistically independent with the same distribution.   Based on simple notions and any prior knowledge of contaminant pattern it can be simple to calculate the probability of no contamination anywhere when all n test points are found to be clear.   This routine has been refined under the name "VSP" (Visual Sampling Plan) by Pacific NW National.  The strong independence assumptions in VSP are appealing for theoretical simplicity but do tend to require more testing points to be checked to ensure clearance.    Hence recognition of statistical dependence can actually improve clearance confidence estimation.   In our grant activities we have improved upon VSO by extensively simulating more complex models with dependence and produced algorithms leading to high confidence.  The improvement shown in our current analyses is very encouraging but there are important questions regarding significant further estimation, which should be thoroughly resolved.    The weight of evidence suggests very high clearance confidence when all tes...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
