<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DC: Small: Collaborative Research: DARE: Declarative and Scalable Recovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>05/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>280000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hong Jiang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>One dominant characteristic of today's large-scale computing systems&lt;br/&gt;is the prevalence of large storage clusters.  Storage clusters at the&lt;br/&gt;scale of hundreds or thousands of commodity machines are&lt;br/&gt;increasingly being deployed. At companies like Amazon, Google, Yahoo,&lt;br/&gt;and others, thousands of nodes are managed as a single system.&lt;br/&gt;&lt;br/&gt;As large clusters have brought many benefits, they also bring a new&lt;br/&gt;challenge: a growing number and frequency of failures that must be&lt;br/&gt;managed. Bits, sectors, disks, machines, racks, and many other&lt;br/&gt;components fail.  With millions of servers and hundreds of data&lt;br/&gt;centers, there are millions of opportunities for these components to&lt;br/&gt;fail. Failing to deal with failures will directly impact the&lt;br/&gt;reliability and availability of data and jobs.&lt;br/&gt;&lt;br/&gt;Unfortunately, we still hear data-loss stories even recently. For&lt;br/&gt;example, in March 2009, Facebook lost millions of photos due to&lt;br/&gt;simultaneous disk failures that "should" rarely happen at the same&lt;br/&gt;time (but it happened); in July 2009, a large bank was fined a record&lt;br/&gt;total of 3 millions pounds after losing data on thousands of its&lt;br/&gt;customers; more recently, in October 2009, T-Mobile Sidekick, which&lt;br/&gt;uses Microsoft's cloud service, also lost its customer data.  These&lt;br/&gt;incidents have shown that existing large-scale storage systems are&lt;br/&gt;still fragile to failures.&lt;br/&gt;&lt;br/&gt;To address the challenges of large-scale recovery, the goal of this&lt;br/&gt;project is to: (1) seek the fundamental problems of recovery in&lt;br/&gt;today's scalable world of computing, (2) improve the reliability,&lt;br/&gt;performance, and scalability of existing large-scale recovery, and (3)&lt;br/&gt;explore formally grounded languages to empower rigorous specification&lt;br/&gt;of recovery properties and behaviors.  Our vision is to build systems&lt;br/&gt;that "DARE to fail": systems that deliberately fail themselves,&lt;br/&gt;exercise recovery routinely, and enable easy and correct deployment of&lt;br/&gt;new recovery policies.&lt;br/&gt;&lt;br/&gt;For more information, please visit this website:&lt;br/&gt;http://boom.cs.berkeley.edu/dare/</AbstractNarration>
<MinAmdLetterDate>09/15/2010</MinAmdLetterDate>
<MaxAmdLetterDate>11/07/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016924</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Hellerstein</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph M Hellerstein</PI_FULL_NAME>
<EmailAddress>hellerstein@cs.berkeley.edu</EmailAddress>
<PI_PHON>5106434011</PI_PHON>
<NSF_ID>000340240</NSF_ID>
<StartDate>09/15/2010</StartDate>
<EndDate>11/07/2012</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Haryadi</FirstName>
<LastName>Gunawi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Haryadi Gunawi</PI_FULL_NAME>
<EmailAddress>haryadi@cs.uchicago.edu</EmailAddress>
<PI_PHON>7737025772</PI_PHON>
<NSF_ID>000626546</NSF_ID>
<StartDate>11/07/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<CountyName>ALAMEDA</CountyName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2>1608 Fourth Street, Suite 220</StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<CountyName>ALAMEDA</CountyName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~44337</FUND_OBLG>
</Award>
</rootTag>
