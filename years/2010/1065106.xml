<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Medium: Theory and Practice of Optimal Meshing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>772857.00</AwardTotalIntnAmount>
<AwardAmount>772857</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jack S. Snoeyink</SignBlockName>
<PO_EMAI>jsnoeyin@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Meshing has been a cornerstone for simulations using the finite&lt;br/&gt;element method.  But more recently it has had applications wherever&lt;br/&gt;one needs to define a function over a domain, such as, graphics,&lt;br/&gt;computer aided design, robotics, and even machine learning. Algorithms&lt;br/&gt;will be designed to efficiently work in any fixed dimension with&lt;br/&gt;guarantees on output size and quality.&lt;br/&gt;&lt;br/&gt;At present, no theory exists to formulate and produce optimal meshes&lt;br/&gt;in the presence of small input angles even for the 2D case.  The&lt;br/&gt;research will find efficient algorithms 2D and higher dimension that&lt;br/&gt;generate optimal size Delaunay triangulations using only simplices&lt;br/&gt;with no angles approaching 180 degrees.&lt;br/&gt;&lt;br/&gt;Machine learning applications need a meshing algorithm that runs in&lt;br/&gt;polynomial time for meshing n points in log n dimensions.&lt;br/&gt;Historically, meshing algorithms return a set of space-filling&lt;br/&gt;simplices.  Even good aspect ratio simplices have too small a volume&lt;br/&gt;and return a mesh that is of super polynomial size.  Thus, new&lt;br/&gt;algorithms will be developed that handle atomic objects that are have&lt;br/&gt;much larger volume than simplices.&lt;br/&gt;&lt;br/&gt;The results from this project are eminently practical and have broad&lt;br/&gt;impact on the Sciences, Engineering, Manufacturing, and Machine&lt;br/&gt;Learning.  In particular, meshing is an enabling technology for&lt;br/&gt;designing efficient windmills and cars, and simulations of earth&lt;br/&gt;quakes and medial devices.  One goal is to incorporated techniques&lt;br/&gt;from this research into our first generation 3D code that we made&lt;br/&gt;available on the web. In addition, this material will be incorporated&lt;br/&gt;into classes taught at CMU, lectures, and papers presented at&lt;br/&gt;conferences.</AbstractNarration>
<MinAmdLetterDate>03/31/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/31/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1065106</AwardID>
<Investigator>
<FirstName>Gary</FirstName>
<LastName>Miller</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gary Miller</PI_FULL_NAME>
<EmailAddress>glmiller@cs.cmu.edu</EmailAddress>
<PI_PHON>4122682631</PI_PHON>
<NSF_ID>000462221</NSF_ID>
<StartDate>03/31/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>PITTSBURGH</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7929</Code>
<Text>COMPUTATIONAL GEOMETRY</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~772857</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary focus of the project was to design more efficient<br />algorithms for fundamental problems that arise in geometry, graph<br />theory, and linear algebra.&nbsp; Our motivation and interest for studying<br />these algorithmic problems arises partly by their application to<br />problems in the sciences, industry, medicine, and military.<br /><br />One of the applications studied in more detail by this project were<br />problems in image processing.&nbsp; We demonstrated how these new<br />algorithms can be applied to problems as medical image processing.<br />Other fast algorithms were found for the solution to numeric problems<br />arising from electro-magnetic radiation and 3D fluids problems such as<br />those involving Maxwell's equations and radar imaging.<br /><br />To efficiently solve these applications required us to study and solve<br />more fundamental questions in algorithm design. Included in our list<br />were problems in computational geometry, graph theory,<br />and linear algebra.&nbsp; The main approach undertaken to solve a problem<br />in one of these three areas was to include the tools developed in the<br />other two areas.&nbsp; A now classic algorithmic design technique is to use<br />graph theory to solve linear algebra problems and linear algebra to<br />solve graph problems in what now is known as spectral graph theory.<br />The project also developed new algorithms for problems in<br />computational geometry and numerical analysis.<br /><br />One of the simplest problems that was studied by this project and is<br />reasonably easy to describe is the problem of computing the Voronoi<br />Diagram of a set of points given in some fixed dimensional space. The<br />problem is to partition the space according to the distance to the<br />nearest input point. This will decompose the space into a polytope<br />around each input point. Unfortunately the number of corners of these<br />polytopes may be exponential in the number of points and the<br />dimension. Thus, even in three dimensions the number of corners may be<br />quadratic in the number of points.&nbsp; But quite often the number of<br />corners may only be linear in the number of points.&nbsp; We showed how to<br />use ideas from finite element mesh generation to improve the runtime<br />to compute the Voronoi diagram when the output size is small. In this<br />case we obtained a much faster algorithm then was known.<br /><br />Another very well-studied problem is finding what is known as a low<br />diameter decomposition of a graph. That is given an unweighted<br />undirected graph G with n vertices remove a small number of edges such<br />that each connected component of the remaining graph has small<br />diameter, O(log n).&nbsp; We found a very simple linear-work parallel algorithm for<br />this classic problem.&nbsp; This algorithm is now used in the fastest known<br />algorithm for solving linear systems that are symmetric and diagonally<br />dominate SDD.&nbsp; We have also used this decomposition algorithm to<br />improve the run time for parallel approximate shortest-path algorithms<br />as well as approximate graph spanners.<br /><br />The algorithm is so simple it can be explained to a general audience.<br />The input to the algorithm is an unweighted undirected graph and we<br />think of each node as having an agent or processor on each node.<br /><br />The algorithm is as follows:<br /><br />1) Each agent picks independently a value from an exponential<br />distribution, say the ith node picks Xi.<br /><br />2) The agents determine the maximum value picked, say, Xm.<br /><br />3) The ith agent comes alive at time Xm - Xi and if no one owns the<br />agent's node, it gets to start its own cluster by owning its own node, otherwise it quits.<br /><br />4) At each successive time the agent looks to see if any nodes<br />neighboring the nodes it owns are not owned and i...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary focus of the project was to design more efficient algorithms for fundamental problems that arise in geometry, graph theory, and linear algebra.  Our motivation and interest for studying these algorithmic problems arises partly by their application to problems in the sciences, industry, medicine, and military.  One of the applications studied in more detail by this project were problems in image processing.  We demonstrated how these new algorithms can be applied to problems as medical image processing. Other fast algorithms were found for the solution to numeric problems arising from electro-magnetic radiation and 3D fluids problems such as those involving Maxwell's equations and radar imaging.  To efficiently solve these applications required us to study and solve more fundamental questions in algorithm design. Included in our list were problems in computational geometry, graph theory, and linear algebra.  The main approach undertaken to solve a problem in one of these three areas was to include the tools developed in the other two areas.  A now classic algorithmic design technique is to use graph theory to solve linear algebra problems and linear algebra to solve graph problems in what now is known as spectral graph theory. The project also developed new algorithms for problems in computational geometry and numerical analysis.  One of the simplest problems that was studied by this project and is reasonably easy to describe is the problem of computing the Voronoi Diagram of a set of points given in some fixed dimensional space. The problem is to partition the space according to the distance to the nearest input point. This will decompose the space into a polytope around each input point. Unfortunately the number of corners of these polytopes may be exponential in the number of points and the dimension. Thus, even in three dimensions the number of corners may be quadratic in the number of points.  But quite often the number of corners may only be linear in the number of points.  We showed how to use ideas from finite element mesh generation to improve the runtime to compute the Voronoi diagram when the output size is small. In this case we obtained a much faster algorithm then was known.  Another very well-studied problem is finding what is known as a low diameter decomposition of a graph. That is given an unweighted undirected graph G with n vertices remove a small number of edges such that each connected component of the remaining graph has small diameter, O(log n).  We found a very simple linear-work parallel algorithm for this classic problem.  This algorithm is now used in the fastest known algorithm for solving linear systems that are symmetric and diagonally dominate SDD.  We have also used this decomposition algorithm to improve the run time for parallel approximate shortest-path algorithms as well as approximate graph spanners.  The algorithm is so simple it can be explained to a general audience. The input to the algorithm is an unweighted undirected graph and we think of each node as having an agent or processor on each node.  The algorithm is as follows:  1) Each agent picks independently a value from an exponential distribution, say the ith node picks Xi.  2) The agents determine the maximum value picked, say, Xm.  3) The ith agent comes alive at time Xm - Xi and if no one owns the agent's node, it gets to start its own cluster by owning its own node, otherwise it quits.  4) At each successive time the agent looks to see if any nodes neighboring the nodes it owns are not owned and if it finds any it gets to own them.  5) The process continues until all the nodes belong to a cluster, which will be after at most Xm time steps.  The algorithm analysis follows from basic proprieties of the exponential distribution:  The diameter bounds follows by showing that Xm cannot be too large; while bounding the number of edges between clusters follows by the memory-less property of the exponential distribution.    ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
