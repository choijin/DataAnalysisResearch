<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: System Software for Scalable Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>1.00</AwardTotalIntnAmount>
<AwardAmount>1</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Irene Qualters</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This award facilitates scientific research using the large new computational resource named Blue Waters being developed by IBM and scheduled to be deployed at the University of Illinois. &lt;br/&gt;&lt;br/&gt;As hardware complexity increases in HPC systems, it is difficult for applications to take full advantage of the available system resources and avoid potential bottlenecks. The purpose of this project is to improve the performance and productivity of key system software components on petascale platforms such as Blue Waters. The researchers from University of Chicago, University of Illinois and Argonne National Laboratores propose to study four different classes of system software: message passing libraries, parallel I/O, data visualization, and operating system.  They will use a computational chemistry application (NWChem) and a climate modeling application (CCSM) on Blue Waters to study and validate the performance and scalability of their system software components. Through rigorous experimentation, analysis, and design cycles, the researchers will dramatically improve the capabilities of not only systems being deployed in the near term, but of all systems pushing scalability limits in the near future.&lt;br/&gt;&lt;br/&gt;The benefits of this research will be available to all applications on a variety of large petascale systems in the form of revised system software.  The results will be disseminated to system vendors as well as to other researchers and students in the form of tutorials, workshop and conference presentations, and seminars.</AbstractNarration>
<MinAmdLetterDate>05/31/2011</MinAmdLetterDate>
<MaxAmdLetterDate>05/31/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1036216</AwardID>
<Investigator>
<FirstName>Ewing</FirstName>
<LastName>Lusk</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ewing Lusk</PI_FULL_NAME>
<EmailAddress>lusk@mcs.anl.gov</EmailAddress>
<PI_PHON>6302527852</PI_PHON>
<NSF_ID>000395804</NSF_ID>
<StartDate>05/31/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rajeev</FirstName>
<LastName>Thakur</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rajeev S Thakur</PI_FULL_NAME>
<EmailAddress>thakur@anl.gov</EmailAddress>
<PI_PHON>6302527847</PI_PHON>
<NSF_ID>000120835</NSF_ID>
<StartDate>05/31/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kamil</FirstName>
<LastName>Iskra</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kamil A Iskra</PI_FULL_NAME>
<EmailAddress>iskra@uchicago.edu</EmailAddress>
<PI_PHON>6302527197</PI_PHON>
<NSF_ID>000528703</NSF_ID>
<StartDate>05/31/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Pavan</FirstName>
<LastName>Balaji</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pavan Balaji</PI_FULL_NAME>
<EmailAddress>balaji@anl.gov</EmailAddress>
<PI_PHON>6302523017</PI_PHON>
<NSF_ID>000278063</NSF_ID>
<StartDate>05/31/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Ross</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Ross</PI_FULL_NAME>
<EmailAddress>rross@mcs.anl.gov</EmailAddress>
<PI_PHON>6305254588</PI_PHON>
<NSF_ID>000259978</NSF_ID>
<StartDate>05/31/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606372612</ZipCode>
<StreetAddress><![CDATA[6054 South Drexel Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7781</Code>
<Text>Leadership-Class Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7781</Code>
<Text>PETASCALE - TRACK 1</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~1</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To solve some of the most difficult problems, scientists make use of  massively parallel computers, containing hundreds of thousands to  millions of processing cores.&nbsp; The most widely used programming approach  for using these systems is the Message-Passing Interface (MPI), which  provides a way for programs to share data and to coordinate their  activities.&nbsp; This project investigated the performance and scalability  (the ability to use all of the processing cores at the same time in an  application) of the most widely used implementation of MPI, making use  of Blue Waters, one of the most powerful computers in the world.&nbsp; The  project identified improvements in a number of areas, and has resulted  in improvements to the implementation of MPI used on many parallel  computers.&nbsp; For example, an increasingly popular and effective way to  program these large-scale systems combines two different programming  models &ndash; message passing with MPI, and threads, typically with OpenMP.&nbsp;  We found that contention for thread locks, used to ensure correct  updates to shared data, interfered with the performance of the  application.&nbsp; We developed two approaches that addressed this problem,  significantly improving performance and benefitting applications.&nbsp; We  also investigated the impact on the performance of the application of  how the processes in the parallel program are assigned to the physical  resources in the parallel computer and make recommendations that can  improve application performance on large system such as Blue Waters.&nbsp;  These improvements directly benefit the application programs running on  these systems, accelerating the progress of science and engineering.</p><br> <p>            Last Modified: 08/30/2014<br>      Modified by: Rajeev&nbsp;S&nbsp;Thakur</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To solve some of the most difficult problems, scientists make use of  massively parallel computers, containing hundreds of thousands to  millions of processing cores.  The most widely used programming approach  for using these systems is the Message-Passing Interface (MPI), which  provides a way for programs to share data and to coordinate their  activities.  This project investigated the performance and scalability  (the ability to use all of the processing cores at the same time in an  application) of the most widely used implementation of MPI, making use  of Blue Waters, one of the most powerful computers in the world.  The  project identified improvements in a number of areas, and has resulted  in improvements to the implementation of MPI used on many parallel  computers.  For example, an increasingly popular and effective way to  program these large-scale systems combines two different programming  models &ndash; message passing with MPI, and threads, typically with OpenMP.   We found that contention for thread locks, used to ensure correct  updates to shared data, interfered with the performance of the  application.  We developed two approaches that addressed this problem,  significantly improving performance and benefitting applications.  We  also investigated the impact on the performance of the application of  how the processes in the parallel program are assigned to the physical  resources in the parallel computer and make recommendations that can  improve application performance on large system such as Blue Waters.   These improvements directly benefit the application programs running on  these systems, accelerating the progress of science and engineering.       Last Modified: 08/30/2014       Submitted by: Rajeev S Thakur]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
