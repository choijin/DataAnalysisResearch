<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DEVISE: Developing, Validating, and Implementing Situated Evaluation Instruments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>1261311.00</AwardTotalIntnAmount>
<AwardAmount>1404299</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert Russell</SignBlockName>
<PO_EMAI>rlrussel@nsf.gov</PO_EMAI>
<PO_PHON>7032922995</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This development project will create, test, validate, and disseminate a suite of evaluation tools for use by professionals who are developing Public Participation in Scientific Research projects. The necessary evaluation tools for what participants learn or believe after participating in citizen science projects (called Public Participation in Scientific Research or PPSR) are generally unavailable to project managers where conference participants. The project will collect examples of cognitive and affective test instruments and try them out in citizen science projects underway &lt;br/&gt;&lt;br/&gt;This project grew from discussions at a conference on Developing a Citizen Science Toolkit at Cornell in 2007 where participants noted that evaluation is the most challenging and least understood step in the process of project development. Thus to provide projects with the tools of evaluation that are relevant to the field itself and to the development of the projects on citizen science, the investigators intend to conduct a study to demonstrate how an evaluation framework can be used to assess the impact of projects by conducting evaluations and presenting them as case studies. The investigators will provide evaluation tools for project developers and will facilitate community discussion about the use of these materials. The project also will provide an evaluation of the procedures used to create the tool kit for investigators. The evaluators are expert professionals in the field of attitude measurement, cognitive measurement, informal science program creation, and citizen science management. The investigators will provide webinars for investigators planning to use the tool kit in their projects.&lt;br/&gt;&lt;br/&gt;This project is intended to strengthen the field of informal science education researchers and administrators by providing a source for acceptable measurement methods of the impact on the public of participating in a scientific research project.</AbstractNarration>
<MinAmdLetterDate>08/17/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1010744</AwardID>
<Investigator>
<FirstName>Candie</FirstName>
<LastName>Wilderman</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Candie C Wilderman</PI_FULL_NAME>
<EmailAddress>wilderma@dickinson.edu</EmailAddress>
<PI_PHON>7172451573</PI_PHON>
<NSF_ID>000210805</NSF_ID>
<StartDate>08/17/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Bonney</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard E Bonney</PI_FULL_NAME>
<EmailAddress>reb5@cornell.edu</EmailAddress>
<PI_PHON>6072542442</PI_PHON>
<NSF_ID>000201013</NSF_ID>
<StartDate>08/17/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kirsten</FirstName>
<LastName>Ellenbogen</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kirsten M Ellenbogen</PI_FULL_NAME>
<EmailAddress>kellenbogen@smm.org</EmailAddress>
<PI_PHON>6512212560</PI_PHON>
<NSF_ID>000326834</NSF_ID>
<StartDate>08/17/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148502820</ZipCode>
<StreetAddress><![CDATA[373 Pine Tree Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>7259</Code>
<Text>AISL</Text>
</ProgramElement>
<ProgramReference>
<Code>7259</Code>
<Text>INFORMAL SCIENCE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0410</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0411</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0412</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~400541</FUND_OBLG>
<FUND_OBLG>2011~435810</FUND_OBLG>
<FUND_OBLG>2012~424960</FUND_OBLG>
<FUND_OBLG>2015~142988</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main goal of DEVISE was to improve evaluation quality and capacity across the field of citizen science, that is, public participation in scientific research. Our major objectives were to&nbsp;1)<span> </span>Understand how often and in what ways evaluations of citizen science projects were taking place around the world;&nbsp;2) Develop, test, and disseminate instruments and strategies to measure learning outcomes resulting from engagement in citizen science;&nbsp;and 3)&nbsp;Provide training and support for citizen science project managers in the use of DEVISE evaluation instruments and strategies.</p> <p>We began by conducting a broad survey of the citizen science field, which confirmed that 1) few citizen science projects were conducting systematic evaluations of learning outcomes and 2) those that were doing evaluations were seldom using pre-existing, validated, or generalizable evaluation instruments in their work.</p> <p>We next developed a framework for evaluating citizen science learning outcomes. Initially based on the NSF Framework for Evaluating Impacts of Informal Science Education and the National Research Council&rsquo;s Strands of Science Learning, our framework was revised through input from leaders in the citizen science field. Our learning constructs, or major categories of potential learning outcomes, include Interest in science or the environment; Efficacy for learning and doing science or environmental action; Motivation for learning and doing science or environmental action; Knowledge of science process and the Nature of science; Skills of science Inquiry; and Environmental stewardship. A paper describing the framework and its theoretical underpinnings has been submitted to a peer-reviewed journal. &nbsp;</p> <p>We then developed, tested, and validated scales for measuring learning outcomes presented in our Framework. Development began with conducting a comprehensive review of existing scales and instruments relating to our outcomes; internal and external review for validity of the most relevant existing scales; and examination of psychometric properties of existing scales. Then, to create scales aligned with our framework, we either modified existing scales to make them relevant to citizen science or created entirely new scales. Next, we pilot tested our scales with small numbers of citizen scientists;&nbsp;refined the scales based on the pilot tests; field tested the scales with larger audiences;&nbsp;conducted statistical tests to establish reliability and validity of the scales;&nbsp;and refined the scales based on the field tests. This process led to the creation and dissemination of 9 scales, all of which are sufficiently generalizable to be used across citizen science projects. Six of the scales are also customizable to measure outcomes of specific projects. The scales include Interest in science and nature, Efficacy for learning and doing science, Efficacy for environmental action, Motivation for learning and doing science, Motivation for environmental action, Skills of science Inquiry, and General environmental stewardship. We also developed a technical brief for each scale that summarizes its intended use, psychometric properties, and tips for data analysis.</p> <p>To provide evaluation support for the citizen science field, we developed a User's Guide to Evaluation of Citizen Science projects (UG) that explains DEVISE, describes how to conduct an evaluation, and orients project managers to the DEVISE scales. The free UG has been downloaded by more than 1,250 individuals since 2014, and is available at: http://www.birds.cornell.edu/citscitoolkit/evaluation/. We also have conducted numerous webinars, workshops, and presentations to describe the UG, the scales, and how to use them. In addition, we developed a system for users to easily request and receive the DEVISE scales while allowing us to track use of the scales over time. Nearly 250 practitioners had requested the scales through the fall of 2017. Four studies (including two meta studies using multiple projects) have used the scales in new research and are planning to publish results.</p> <p>We presented our work on DEVISE at professional conferences including the Citizen Science Association, the European Citizen Science Association, the International Congress for Conservation Biology, the American Evaluation Association, and the Association of Science-Technology Centers.</p> <p>As DEVISE instruments become widely used, we will be able to compare learning outcomes across projects and determine where maximum learning occurs. This information will feed back into the citizen science field to influence issues related to recruitment, retention, and project design. Our work also has uncovered gaps in our ability to measure outcomes using standardized scales, leading to a new project funded by NSF in 2016 on embedded project assessment. &nbsp;DEVISE has also helped to develop an evaluation model for other disciplines, for example, one project (EvalFest) has been funded by NSF to create standardized instruments modeled on DEVISE for measuring learning outcomes from science festivals. In addition many projects outside of the citizen science field, both in formal and informal settings, have used the DEVISE instruments. If they prove to be useful in these settings, our project will have improved evaluation capacity far beyond the realm of citizen science.&nbsp;</p><br> <p>            Last Modified: 11/03/2017<br>      Modified by: Richard&nbsp;E&nbsp;Bonney</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main goal of DEVISE was to improve evaluation quality and capacity across the field of citizen science, that is, public participation in scientific research. Our major objectives were to 1) Understand how often and in what ways evaluations of citizen science projects were taking place around the world; 2) Develop, test, and disseminate instruments and strategies to measure learning outcomes resulting from engagement in citizen science; and 3) Provide training and support for citizen science project managers in the use of DEVISE evaluation instruments and strategies.  We began by conducting a broad survey of the citizen science field, which confirmed that 1) few citizen science projects were conducting systematic evaluations of learning outcomes and 2) those that were doing evaluations were seldom using pre-existing, validated, or generalizable evaluation instruments in their work.  We next developed a framework for evaluating citizen science learning outcomes. Initially based on the NSF Framework for Evaluating Impacts of Informal Science Education and the National Research Council?s Strands of Science Learning, our framework was revised through input from leaders in the citizen science field. Our learning constructs, or major categories of potential learning outcomes, include Interest in science or the environment; Efficacy for learning and doing science or environmental action; Motivation for learning and doing science or environmental action; Knowledge of science process and the Nature of science; Skills of science Inquiry; and Environmental stewardship. A paper describing the framework and its theoretical underpinnings has been submitted to a peer-reviewed journal.    We then developed, tested, and validated scales for measuring learning outcomes presented in our Framework. Development began with conducting a comprehensive review of existing scales and instruments relating to our outcomes; internal and external review for validity of the most relevant existing scales; and examination of psychometric properties of existing scales. Then, to create scales aligned with our framework, we either modified existing scales to make them relevant to citizen science or created entirely new scales. Next, we pilot tested our scales with small numbers of citizen scientists; refined the scales based on the pilot tests; field tested the scales with larger audiences; conducted statistical tests to establish reliability and validity of the scales; and refined the scales based on the field tests. This process led to the creation and dissemination of 9 scales, all of which are sufficiently generalizable to be used across citizen science projects. Six of the scales are also customizable to measure outcomes of specific projects. The scales include Interest in science and nature, Efficacy for learning and doing science, Efficacy for environmental action, Motivation for learning and doing science, Motivation for environmental action, Skills of science Inquiry, and General environmental stewardship. We also developed a technical brief for each scale that summarizes its intended use, psychometric properties, and tips for data analysis.  To provide evaluation support for the citizen science field, we developed a User's Guide to Evaluation of Citizen Science projects (UG) that explains DEVISE, describes how to conduct an evaluation, and orients project managers to the DEVISE scales. The free UG has been downloaded by more than 1,250 individuals since 2014, and is available at: http://www.birds.cornell.edu/citscitoolkit/evaluation/. We also have conducted numerous webinars, workshops, and presentations to describe the UG, the scales, and how to use them. In addition, we developed a system for users to easily request and receive the DEVISE scales while allowing us to track use of the scales over time. Nearly 250 practitioners had requested the scales through the fall of 2017. Four studies (including two meta studies using multiple projects) have used the scales in new research and are planning to publish results.  We presented our work on DEVISE at professional conferences including the Citizen Science Association, the European Citizen Science Association, the International Congress for Conservation Biology, the American Evaluation Association, and the Association of Science-Technology Centers.  As DEVISE instruments become widely used, we will be able to compare learning outcomes across projects and determine where maximum learning occurs. This information will feed back into the citizen science field to influence issues related to recruitment, retention, and project design. Our work also has uncovered gaps in our ability to measure outcomes using standardized scales, leading to a new project funded by NSF in 2016 on embedded project assessment.  DEVISE has also helped to develop an evaluation model for other disciplines, for example, one project (EvalFest) has been funded by NSF to create standardized instruments modeled on DEVISE for measuring learning outcomes from science festivals. In addition many projects outside of the citizen science field, both in formal and informal settings, have used the DEVISE instruments. If they prove to be useful in these settings, our project will have improved evaluation capacity far beyond the realm of citizen science.        Last Modified: 11/03/2017       Submitted by: Richard E Bonney]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
