<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS:  Medium:  Visual MIMO Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>685000.00</AwardTotalIntnAmount>
<AwardAmount>685000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Thyagarajan Nandagopal</SignBlockName>
<PO_EMAI>tnandago@nsf.gov</PO_EMAI>
<PO_PHON>7032924550</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The increasingly ubiquitous use of cameras creates an exciting novel opportunity to build camera-based optical wireless networks. Optical wireless is not only a potential low-cost alternative where it can take advantage of existing cameras and light emitting devices, but it's highly directional transmissions can present advantages over radio-frequency (RF) based wireless communications. While the optical channel differs fundamentally from the RF channel, this project recognizes that it also allows multiple spatially separated channels between an array of transmitter elements and the array of camera pixels, akin to an RF multiple-input multiple-output (MIMO) system. This inter-disciplinary project therefore brings together expertise in the areas of mobile networks, communications, and computer vision to analyze, design, and prototype a network stack for such visual MIMO communications. This stack addresses the fundamentally different visual channel and receiver constraints through innovative visual signal acquisition, tracking, interference cancellation, and modulation techniques at the physical layer as well as vision-aware link and MAC layer protocols.&lt;br/&gt;&lt;br/&gt;Visual MIMO networks can potentially support applications ranging from secure communication between cell phones, over localization of 911 callers through surveillance cameras, to interference-free car-to-car communications. The project also makes an experimental visual MIMO testbed available to the research community at large.  In addition to publications, the project takes advantage of WINLAB's biannual industry meetings to disseminate results and provides a variety of appealing educational activities involving K-12 and undergraduate students.</AbstractNarration>
<MinAmdLetterDate>04/06/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/12/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1065463</AwardID>
<Investigator>
<FirstName>Narayan</FirstName>
<LastName>Mandayam</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Narayan Mandayam</PI_FULL_NAME>
<EmailAddress>narayan@winlab.rutgers.edu</EmailAddress>
<PI_PHON>7329326857</PI_PHON>
<NSF_ID>000492962</NSF_ID>
<StartDate>04/06/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kristin</FirstName>
<LastName>Dana</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristin J Dana</PI_FULL_NAME>
<EmailAddress>kristin.dana@rutgers.edu</EmailAddress>
<PI_PHON>8484455253</PI_PHON>
<NSF_ID>000148773</NSF_ID>
<StartDate>04/06/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Marco</FirstName>
<LastName>Gruteser</LastName>
<PI_MID_INIT>O</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marco O Gruteser</PI_FULL_NAME>
<EmailAddress>gruteser@winlab.rutgers.edu</EmailAddress>
<PI_PHON>8489320993</PI_PHON>
<NSF_ID>000493624</NSF_ID>
<StartDate>04/06/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088543925</ZipCode>
<StreetAddress><![CDATA[33 Knightsbridge Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7363</Code>
<Text>RES IN NETWORKING TECH &amp; SYS</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~368689</FUND_OBLG>
<FUND_OBLG>2012~175789</FUND_OBLG>
<FUND_OBLG>2013~140522</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The objective of this project was to bring together expertise in areas of mobile networks, communications, and computer vision to analyze, design, and prototype Visual multiple-input multiple-output (MIMO) networks. This is motivated by the realization that the increasingly ubiquitous use of cameras and light emitting devices, for example, in cell phones, cars, laptops, music players, and surveillance systems, creates an exciting novel opportunity to build camera-based optical wireless communication systems and networks. While the optical channel differs fundamentally from the RF channel, it also allows multiple spatially separated channels between an array of transmitter elements and the array of camera pixels, akin to an RF MIMO system. We therefore refer to this novel class of communication systems as Visual MIMO systems and study protocols and network primitives, from the physical to the transport layer for such systems.</p> <p>Visual MIMO can also be used as an underlying concept to communicate digital information from LED and LCD display screens to cameras in mobile devices especially smartphones and tablets. With recent advances in low-cost display technology, the trend toward ubiquitous electronic displays creates a unique opportunity for camera-display messaging. Electronic displays are on computers, kiosks, smartphones, roadway signage, televisions and even cars.&nbsp;</p> <p>To gain a deeper understanding of the opportunity and limitations of visual MIMO, the project developed information capacity models for this technology. The models characterize how the capacity depends on factors such as the number of transmitter elements (e.g., light emitting diodes), the separation between transmitter and receiver, as well as the perspective of the transmitter onto the receiver. Compared to other optical technologies, visual MIMO can enable higher data rate communication over longer transmission ranges in a mobile setting.</p> <p>The project then built on these insights to design visual MIMO communication techniques and to implement them in prototypes for both LED-based and screen-based transmitters. These techniques include rate adaptation techniques, which can significantly improve throughput by adapting the transmission rate to the receiver perspective. The techniques also include radiometric calibration to improve the goodput of screen to camera communications. By modeling the photometry of the system using a camera-display transfer function, the system can compensate for nonlinearities of both the display and the camera. The project also showed how such communications can be embedded in existing imagery on displays, so that communications can occur without disrupting the viewing experience for a human observer. Such embedding significantly reduces the achievable throughput but can still allow data rates of tens of kilobits per second.</p> <p>While intensity-based optical communications are more prevalent in camera-display messaging, we also explored how the emerging time-of-flight cameras can be exploited as communications receivers. We designed a novel method that uses light phase for messaging and time-of-flight (ToF) cameras as receivers. With intensity-based methods, light signals can be degraded by reflections and ambient illumination. By comparison, communication using ToF cameras ?is more robust against challenging lighting conditions because they are able to detect changes in phase.</p> <p>Last but not least, the project also explored the use of visual MIMO techniques for localization. We introduced the use of a radio-optical beaconing approach to support augmented reality applications, making it easier to identify the objects a person is looking at. We designed a hybrid radio-optical beaconing system that leverages the high directionality characteristic of an infrared link for precise angle-of-arrival estimation and distance estimation, and a radio li...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The objective of this project was to bring together expertise in areas of mobile networks, communications, and computer vision to analyze, design, and prototype Visual multiple-input multiple-output (MIMO) networks. This is motivated by the realization that the increasingly ubiquitous use of cameras and light emitting devices, for example, in cell phones, cars, laptops, music players, and surveillance systems, creates an exciting novel opportunity to build camera-based optical wireless communication systems and networks. While the optical channel differs fundamentally from the RF channel, it also allows multiple spatially separated channels between an array of transmitter elements and the array of camera pixels, akin to an RF MIMO system. We therefore refer to this novel class of communication systems as Visual MIMO systems and study protocols and network primitives, from the physical to the transport layer for such systems.  Visual MIMO can also be used as an underlying concept to communicate digital information from LED and LCD display screens to cameras in mobile devices especially smartphones and tablets. With recent advances in low-cost display technology, the trend toward ubiquitous electronic displays creates a unique opportunity for camera-display messaging. Electronic displays are on computers, kiosks, smartphones, roadway signage, televisions and even cars.   To gain a deeper understanding of the opportunity and limitations of visual MIMO, the project developed information capacity models for this technology. The models characterize how the capacity depends on factors such as the number of transmitter elements (e.g., light emitting diodes), the separation between transmitter and receiver, as well as the perspective of the transmitter onto the receiver. Compared to other optical technologies, visual MIMO can enable higher data rate communication over longer transmission ranges in a mobile setting.  The project then built on these insights to design visual MIMO communication techniques and to implement them in prototypes for both LED-based and screen-based transmitters. These techniques include rate adaptation techniques, which can significantly improve throughput by adapting the transmission rate to the receiver perspective. The techniques also include radiometric calibration to improve the goodput of screen to camera communications. By modeling the photometry of the system using a camera-display transfer function, the system can compensate for nonlinearities of both the display and the camera. The project also showed how such communications can be embedded in existing imagery on displays, so that communications can occur without disrupting the viewing experience for a human observer. Such embedding significantly reduces the achievable throughput but can still allow data rates of tens of kilobits per second.  While intensity-based optical communications are more prevalent in camera-display messaging, we also explored how the emerging time-of-flight cameras can be exploited as communications receivers. We designed a novel method that uses light phase for messaging and time-of-flight (ToF) cameras as receivers. With intensity-based methods, light signals can be degraded by reflections and ambient illumination. By comparison, communication using ToF cameras ?is more robust against challenging lighting conditions because they are able to detect changes in phase.  Last but not least, the project also explored the use of visual MIMO techniques for localization. We introduced the use of a radio-optical beaconing approach to support augmented reality applications, making it easier to identify the objects a person is looking at. We designed a hybrid radio-optical beaconing system that leverages the high directionality characteristic of an infrared link for precise angle-of-arrival estimation and distance estimation, and a radio link for IDs. The use of a radio link also reduced the energy consumption of the infrared tags by s...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
