<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Statistical Properties of Numerical Derivatives and Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>137152.00</AwardTotalIntnAmount>
<AwardAmount>137152</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nancy Lutz</SignBlockName>
<PO_EMAI>nlutz@nsf.gov</PO_EMAI>
<PO_PHON>7032927280</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Numerical differentiation is widely used in econometrics and many other areas of quantitative economic analysis. Many functions that need to be differentiated in econometric analysis need to be estimated from the data. For example, estimating the approximate variance of an estimator often requires estimating the derivatives of the moment conditions that define the estimator. Many estimators are also obtained by finding the zeros of the first order condition of the sample objective functions.&lt;br/&gt;&lt;br/&gt;The estimated functions can be either non-differentiable or difficult to differentiate analytically.  Oftentimes the estimated functions are complex and can be challenging to compute even numerically. Empirical researchers often apply numerical differentiation methods which depend on taking a finite number of differences of the objective function at discrete points, either explicitly or implicitly through the use of software routines, to the estimated functions from the sample in order to approximate the derivative of the unknown true functions.&lt;br/&gt;&lt;br/&gt;A key tuning parameter that determines how well the numerical derivatives approximate the analytic derivatives is the step size used in the finite differencing operation. Empirical researchers often find that different step sizes can lead to very different numerical derivative estimates. While the importance of numerical derivatives has not gone unnoticed in econometrics, statistics and mathematics, the results that are available in the existing literature are very limited in scope.&lt;br/&gt;&lt;br/&gt;The goal of this project is to take an important step to provide a systematic framework for understanding the conditions on the step size in numerical differentiation that are needed to obtain the optimal quality of approximation. These conditions involve subtle tradeoffs between the complexity of the function that needs to be differentiated and the amount of information that is available in the sample of data, and the degree of smoothness of the  expectation of the function with respect to the sampling distribution. Empirical process theory provides a powerful tool for analyzing the complex of functions in the presence of randomly sampled data.&lt;br/&gt;&lt;br/&gt;This project focuses on analyzing the use of numerical derivatives in estimating the asymptotic variance of estimators and in obtaining extreme estimators through gradient based optimization routines. The PIs' first goal is to give general sufficient consistency conditions that allow for nondifferentiable and discontinuous moment functions in consistent variance estimation. The precise rate conditions for the step size in numerical differentiation that we obtain depend on the tradeoff between bias and the degree of nonsmoothness of the moment condition. These general conditions can be specialized for certain continuous models, for which choosing a smaller step size can only be beneficial in reducing the asymptotic bias. However, the asymptotic bias will be dominated by the statistical noise once it falls below a certain threshold. &lt;br/&gt;&lt;br/&gt;The second goal of this project is to analyze a class of estimators that are based on numerically differentiating a finite sample objective function, and provide conditions under which numerical derivative based optimization methods deliver consistent and asymptotic normal parameter estimates. The conditions for numerical extreme estimators require that the step size used in the numerical derivative converge to zero at specific rates when the sample size increases to infinity. The conditions required for the consistency of the asymptotic variance and for the convergence of the estimator itself can be different. The PIs seek extensive results that cover finite dimensional parametric models, infinite dimensional semiparametric models, and models that are defined by U-processes involving multiple layers of summation over the sampling data. &lt;br/&gt;&lt;br/&gt;The proposed project involves joint work with Professor Aprajit Mahajan from Stanford University.</AbstractNarration>
<MinAmdLetterDate>09/18/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/18/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1025035</AwardID>
<Investigator>
<FirstName>Denis</FirstName>
<LastName>Nekipelov</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Denis Nekipelov</PI_FULL_NAME>
<EmailAddress>denis@virginia.edu</EmailAddress>
<PI_PHON>4349247581</PI_PHON>
<NSF_ID>000556047</NSF_ID>
<StartDate>09/18/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>BERKELEY</CityName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress><![CDATA[Sponsored Projects Office]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1320</Code>
<Text>Economics</Text>
</ProgramElement>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~137152</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="margin: 0px; text-indent: 0px;">In this project we studied the  relationship between the impact of numerical approximation on the properties of statistical procedures  that rely on such an approximation.  Finite-difference approximations are  widely used in empirical work to evaluate derivatives of estimated  functions.  For instance, many standard optimization routines rely on  finite-difference formulas for gradient calculations. However, the  effect of such approximations on the statistical properties of the  resulting estimators has only been studied in a few special situations.  Empirical researchers routinely rely on finite-difference approximations  to evaluate derivatives of estimated functions. For instance, commonly  used optimization routines implicitly use finite-difference formulas for  gradient calculations which require the choice of step size parameters.  Such routines frequently require the choice of step size parameters for  finite-difference numerical gradients or similar parameters such as the  computing tolerance. The findings of our project show that first, for  unbiased inference one needs to adjust the step size or the tolerance  parameter as a function of the sample size.  Second, higher-order finite  difference formulas reduce the asymptotic bias analogous to higher  order kernels. Third, we provide weak sufficient conditions for uniform  consistency of the finite-difference approximations for gradients and  directional derivatives.  Fourth, we analyze numerical gradient-based  extremum estimators and find that the asymptotic distribution of the  resulting estimators may be a hybrid between the asymptotics of the  original extremum estimators and the asymptotics of the kernel  smoothers. In particular, the asymptotics with the fastest possible  convergence rates is characterized by the boundary-crossing  distributions for non-standard Brownian motions. Fifth, we state conditions  under which the numerical derivative estimator is consistent and  asymptotically normal. Sixth, we generalize our results to  semiparametric estimation problems. Finally, we demonstrate that our  results are useful in a range of nonstandard estimation procedures.</p><br> <p>            Last Modified: 11/20/2013<br>      Modified by: Denis&nbsp;Nekipelov</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[In this project we studied the  relationship between the impact of numerical approximation on the properties of statistical procedures  that rely on such an approximation.  Finite-difference approximations are  widely used in empirical work to evaluate derivatives of estimated  functions.  For instance, many standard optimization routines rely on  finite-difference formulas for gradient calculations. However, the  effect of such approximations on the statistical properties of the  resulting estimators has only been studied in a few special situations.  Empirical researchers routinely rely on finite-difference approximations  to evaluate derivatives of estimated functions. For instance, commonly  used optimization routines implicitly use finite-difference formulas for  gradient calculations which require the choice of step size parameters.  Such routines frequently require the choice of step size parameters for  finite-difference numerical gradients or similar parameters such as the  computing tolerance. The findings of our project show that first, for  unbiased inference one needs to adjust the step size or the tolerance  parameter as a function of the sample size.  Second, higher-order finite  difference formulas reduce the asymptotic bias analogous to higher  order kernels. Third, we provide weak sufficient conditions for uniform  consistency of the finite-difference approximations for gradients and  directional derivatives.  Fourth, we analyze numerical gradient-based  extremum estimators and find that the asymptotic distribution of the  resulting estimators may be a hybrid between the asymptotics of the  original extremum estimators and the asymptotics of the kernel  smoothers. In particular, the asymptotics with the fastest possible  convergence rates is characterized by the boundary-crossing  distributions for non-standard Brownian motions. Fifth, we state conditions  under which the numerical derivative estimator is consistent and  asymptotically normal. Sixth, we generalize our results to  semiparametric estimation problems. Finally, we demonstrate that our  results are useful in a range of nonstandard estimation procedures.       Last Modified: 11/20/2013       Submitted by: Denis Nekipelov]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
