<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CI-P Seeing speech: A community resource for analysis of multi-modal language data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2011</AwardEffectiveDate>
<AwardExpirationDate>02/28/2013</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Producing human speech requires the exquisite timing of multiple interacting modalities: movements of the lips, tongue, velum, and vocal folds are all precisely coordinated to give each sound its unique properties. Understanding how these different modalities interact is important to basic linguistic and cognitive science as well as to applied research areas, such as automatic speech recognition, speech therapy, and second language learning. This workshop will bring together experts in speech database construction, speech recognition, and articulatory research to explore the feasibility and desirability of developing software and a database to study the interaction of the speech articulators and how their coordination relates to the sounds produced. The workshop brings these three communities together to address (i) requirements of software for extracting and analyzing articulatory data in conjunction with the acoustic signal, and (ii) properties of the database to be constructed, namely: which utterances should be recorded; how to synchronize capture of the multiple modalities; markup, annotation, and storage of the data.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The goal of the project is to develop both software for multi-modal speech analysis and a database of synchronized multi-modal speech recordings. A critical first step is to assess which features of the database and software are needed to maximize the long-term value to the scientific community. This workshop brings together researchers from diverse backgrounds in human language and computer sciences to examine these issues, so that the database and software may facilitate studies of oral tract articulation across many disciplines, e.g. to understand the diversity of human language sounds, language acquisition and endangered languages, explore speech deficits, teach foreign language pronunciation or oral language to the profoundly deaf, improve speech recognition and synthesis software, understand how musicians shape sounds while playing wind instruments, etc.</AbstractNarration>
<MinAmdLetterDate>02/18/2011</MinAmdLetterDate>
<MaxAmdLetterDate>02/18/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1059266</AwardID>
<Investigator>
<FirstName>Diana</FirstName>
<LastName>Archangeli</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Diana B Archangeli</PI_FULL_NAME>
<EmailAddress>dba@email.arizona.edu</EmailAddress>
<PI_PHON>5207418100</PI_PHON>
<NSF_ID>000427375</NSF_ID>
<StartDate>02/18/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ian</FirstName>
<LastName>Fasel</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ian R Fasel</PI_FULL_NAME>
<EmailAddress>ianfasel@sista.arizona.edu</EmailAddress>
<PI_PHON>5206266000</PI_PHON>
<NSF_ID>000535309</NSF_ID>
<StartDate>02/18/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arizona</Name>
<CityName>Tucson</CityName>
<ZipCode>857194824</ZipCode>
<PhoneNumber>5206266000</PhoneNumber>
<StreetAddress>888 N Euclid Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>806345617</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARIZONA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072459266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arizona]]></Name>
<CityName>Tucson</CityName>
<StateCode>AZ</StateCode>
<ZipCode>857194824</ZipCode>
<StreetAddress><![CDATA[888 N Euclid Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Context and description of project</strong></p> <p>Human language integrates several complex systems: Sound is the most accessible of these, with measurable acoustic and articulatory properties. Understanding how sound works in language is important to basic linguistic and cognitive sciences as well as to applied research such as automatic speech recognition, language learning, correcting articulatory disorders, etc.</p> <p>However, the human&rsquo;s use of sound in language is only marginally understood because the articulation involves multiple interacting modalities. The lips, the velum, the tongue, and the larynx all conspire to give each sound its unique properties. At this point in time, while there are tools for tracking and analyzing the movements of each articulator, there is no easy means of synchronizing and integrating data from all articulators.</p> <p>The goal of this project was to determine the feasibility and desirability of developing software and a database to enable the study of the interaction of these different modalities. Our vision has two components,&nbsp; (1) <strong>UltraPraat: </strong>&nbsp;to integrate articulatory analysis with Praat (<a href="http://www.fon.hum.uva.nl/praat/">http://www.fon.hum.uva.nl/praat/</a>), an open source software widely used for acoustic analysis; (2) <strong>UltraSpeech: </strong>to design and build a database for accessing simultaneous articulatory and acoustic speech data, including the demographic data of speakers.</p> <p>The primary activity was a workshop bringing together expert language experts in articulation, speech synthesis, speech recognition, speech education and therapy. &nbsp;The goal of the workshop was to develop a plan to ensure that the infrastructure developed to fill the current gap in articulatory data and analysis software will provide as much benefit as possible to all the related fields in technology and the sciences. A secondary activity was to follow-up on recommendations from the workshop.</p> <p>The broadest impact from this project will be felt when the software and database developed and made available to other researchers. The intended users include anyone who studies oral tract articulation, e.g. to understand the diversity of human language sounds, language acquisition and endangered languages, explore speech deficits, teach foreign language pronunciation or oral language to the profoundly deaf, improve speech recognition and synthesis software, understand how musicians shape sounds while playing wind instruments, etc.</p> <p>&nbsp;</p> <p><strong>Project outcomes</strong></p> <p>The workshop brought together over 30 people for two days. It began with a discussion of the issues surrounding articulatory analysis, followed by a sketch of the vision for UltraPraat and UltraSpeech. Subsequently, the large group broke into smaller groups to focus on specific aspects of the two components of the envisioned tools. Four themes recurred in the various sub-groups.</p> <p>First, the cost of developing a dataset involving articulation is prohibitive for individual labs. However, developing the tools and corpus and making them public would allow individual labs to help populate the database with data collected in their labs, and so would enhance the overall data broadly available for research.</p> <p>Second, and relatedly, articulatory phonetics studies with more than 5-10 subjects are rare due to the diffculty of collecting and extracting data. The data extraction and analysis software addresses this point directly by greatly facilitating both extraction and analysis of articulatory data. Similarly, the strong recommendation to build an original database with 60 speakers, spread evenly across three dialects, provides a significant data resource for further studies.</p> <p>Third, a strong recommendation recurred advocating that the structure be as open as possible. As more people...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Context and description of project  Human language integrates several complex systems: Sound is the most accessible of these, with measurable acoustic and articulatory properties. Understanding how sound works in language is important to basic linguistic and cognitive sciences as well as to applied research such as automatic speech recognition, language learning, correcting articulatory disorders, etc.  However, the humanÆs use of sound in language is only marginally understood because the articulation involves multiple interacting modalities. The lips, the velum, the tongue, and the larynx all conspire to give each sound its unique properties. At this point in time, while there are tools for tracking and analyzing the movements of each articulator, there is no easy means of synchronizing and integrating data from all articulators.  The goal of this project was to determine the feasibility and desirability of developing software and a database to enable the study of the interaction of these different modalities. Our vision has two components,  (1) UltraPraat:  to integrate articulatory analysis with Praat (http://www.fon.hum.uva.nl/praat/), an open source software widely used for acoustic analysis; (2) UltraSpeech: to design and build a database for accessing simultaneous articulatory and acoustic speech data, including the demographic data of speakers.  The primary activity was a workshop bringing together expert language experts in articulation, speech synthesis, speech recognition, speech education and therapy.  The goal of the workshop was to develop a plan to ensure that the infrastructure developed to fill the current gap in articulatory data and analysis software will provide as much benefit as possible to all the related fields in technology and the sciences. A secondary activity was to follow-up on recommendations from the workshop.  The broadest impact from this project will be felt when the software and database developed and made available to other researchers. The intended users include anyone who studies oral tract articulation, e.g. to understand the diversity of human language sounds, language acquisition and endangered languages, explore speech deficits, teach foreign language pronunciation or oral language to the profoundly deaf, improve speech recognition and synthesis software, understand how musicians shape sounds while playing wind instruments, etc.     Project outcomes  The workshop brought together over 30 people for two days. It began with a discussion of the issues surrounding articulatory analysis, followed by a sketch of the vision for UltraPraat and UltraSpeech. Subsequently, the large group broke into smaller groups to focus on specific aspects of the two components of the envisioned tools. Four themes recurred in the various sub-groups.  First, the cost of developing a dataset involving articulation is prohibitive for individual labs. However, developing the tools and corpus and making them public would allow individual labs to help populate the database with data collected in their labs, and so would enhance the overall data broadly available for research.  Second, and relatedly, articulatory phonetics studies with more than 5-10 subjects are rare due to the diffculty of collecting and extracting data. The data extraction and analysis software addresses this point directly by greatly facilitating both extraction and analysis of articulatory data. Similarly, the strong recommendation to build an original database with 60 speakers, spread evenly across three dialects, provides a significant data resource for further studies.  Third, a strong recommendation recurred advocating that the structure be as open as possible. As more people access the analytic tools and the database, more research questions will be asked: it is impossible to anticipate the full potential. Coupled with this was considerable discussion of ways to encourage others to add to the database, so that the value of the toolset inc...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
