<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Better Sentiment Analysis through Forecasting</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>407164.00</AwardTotalIntnAmount>
<AwardAmount>423164</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The emerging field of sentiment analysis employs algorithmic methods to identify and summarize opinions expressed in text.  Both machine learning and ad-hoc approaches lie at the foundations of contemporary sentiment analysis systems, but progress on improving both precision and recall has been slowed by the expense and complexity of obtaining sufficiently broad, general sentiment training/validation data.&lt;br/&gt;&lt;br/&gt;Recent work has established that fundamental economic variables can successfully be forecast by applying sentiment analysis methods to news-oriented text streams.  This project turns this relation on its head, using such forecasting approaches to improve both the precision and recall of general entity-oriented sentiment analysis methods. In particular, this project provides a three-pronged research effort into entity-level sentiment analysis, focusing on improved assessment and algorithms, with applications to the social sciences and forecasting.  In particular: &lt;br/&gt;(1) Developing a complete entity-level, text and language-independent sentiment evaluation environment, both to further the development of the Lydia system and for release to the international sentiment analysis community.&lt;br/&gt;(2) Building on this environment, to develop improved sentiment-detection methods for English news, foreign language news streams, social media such as blogs and Twitter, and historical text corpora.&lt;br/&gt;(3) Finally, applying improved sentiment analysis to a variety of challenges in the social sciences.  &lt;br/&gt;&lt;br/&gt;This research promises to substantially improve both the precision and recall of sentiment detection methods, by focusing on the weakest link: rigorous yet domain-, source-, and language-independent assessment of sentiment.  Beyond improvements in natural language processing (NLP), this includes other issues in opinion mining, including article clustering and duplicate detection, entity-domain context, and combining opinions from large numbers of distinct sources.&lt;br/&gt;&lt;br/&gt;The sentiment analysis methods and data developed under this research project are expected to have a broad impact, as the results will be directly applicable in a broad range of social sciences, including sociology, economics, political science, and media and communication studies.  The techniques will serve as both an educational and scholarly resource in these fields, empowering students and researchers to conduct their own primary studies on historical trends and social forces.  Results will be disseminated to the community through the project website (http://www.textmap.org/III).</AbstractNarration>
<MinAmdLetterDate>09/08/2010</MinAmdLetterDate>
<MaxAmdLetterDate>03/16/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1017181</AwardID>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Skiena</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven Skiena</PI_FULL_NAME>
<EmailAddress>skiena@cs.sunysb.edu</EmailAddress>
<PI_PHON>5166329026</PI_PHON>
<NSF_ID>000199813</NSF_ID>
<StartDate>09/08/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Stony Brook]]></Name>
<CityName>Stony Brook</CityName>
<StateCode>NY</StateCode>
<ZipCode>117940001</ZipCode>
<StreetAddress><![CDATA[WEST 5510 FRK MEL LIB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~407164</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The major activities for this project revolved around a new approach to natural language processing and sentiment analysis which naturally generalizes to all the world's major languges. Word embeddings assign each word in a language a unique point in (say) 50 dimensional space. Two words have similar meanings/roles if they lie close to each other in space.</p> <p>Recently re-introduced techniques in unsupervised feature learning make this possible, by acquiring common features for a specific language vocabulary from unlabeled text. These features, also known as distributed words representations (embeddings), have been used by us and other groups to build a unified NLP architecture that solved multiple tasks; part of speech (POS) tagging, named entity recognition (NER), semantic role labeling and chunking.</p> <p>We have built word embeddings for one hundred of world's most frequently spoken languages (Al-Rfou, et. al. 2013), using neural networks (auto-encoders) trained on each language's Wikipedia in an unsupervised setting, and shown that they capture surprisingly subtle features of language usage like sentiment, plurality, even nation of origin (Chen et. al 2013). We have made these word embeddings freely available to the research community and employ them in our work on sentiment analysis, with well over 1,000 downloads per date. &nbsp; Further, in work presented at KDD 2014, we (Perozzi, al-Rfou, and Skiena, 2014) have developed DeepWalk, an extension of the ideas behind word embeddings to identifying features in graphs.</p> <p>We have quantitatively demonstrated the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish.</p> <p>In particular, these word embeddings point to a way to build sentiment analysis systems for all the world's languages in an elegant, consistant, non-ad hoc approach, by training on the Wikipedia edition of each language. Our work (Chen and Skiena, 2014) was reported at ACL 2014, where we presented high-quality sentiment lexicons for 136 major languages, by integrating a variety of linguistic resources into an immense knowledge graph. &nbsp; Our lexicons have a polarity agreement of 95.7% with published lexicons while achieving an overall coverage of 45.2%. &nbsp;Further, we demonstrated the performance of our lexicons in an extrinsic analysis of 2,000 distinct historical figures in Wikipedia articles from 30 languages. &nbsp;Despite cultural difference and the intended neutrality of Wikipedia, our lexicons show an average sentiment correlation of 0.28 across all language pairs.</p> <p>This paper (and the release of our lexicons) marked the successful completion of our major goal of sentiment detection systems for foreign language streams.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/10/2014<br>      Modified by: Steven&nbsp;Skiena</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The major activities for this project revolved around a new approach to natural language processing and sentiment analysis which naturally generalizes to all the world's major languges. Word embeddings assign each word in a language a unique point in (say) 50 dimensional space. Two words have similar meanings/roles if they lie close to each other in space.  Recently re-introduced techniques in unsupervised feature learning make this possible, by acquiring common features for a specific language vocabulary from unlabeled text. These features, also known as distributed words representations (embeddings), have been used by us and other groups to build a unified NLP architecture that solved multiple tasks; part of speech (POS) tagging, named entity recognition (NER), semantic role labeling and chunking.  We have built word embeddings for one hundred of world's most frequently spoken languages (Al-Rfou, et. al. 2013), using neural networks (auto-encoders) trained on each language's Wikipedia in an unsupervised setting, and shown that they capture surprisingly subtle features of language usage like sentiment, plurality, even nation of origin (Chen et. al 2013). We have made these word embeddings freely available to the research community and employ them in our work on sentiment analysis, with well over 1,000 downloads per date.   Further, in work presented at KDD 2014, we (Perozzi, al-Rfou, and Skiena, 2014) have developed DeepWalk, an extension of the ideas behind word embeddings to identifying features in graphs.  We have quantitatively demonstrated the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish.  In particular, these word embeddings point to a way to build sentiment analysis systems for all the world's languages in an elegant, consistant, non-ad hoc approach, by training on the Wikipedia edition of each language. Our work (Chen and Skiena, 2014) was reported at ACL 2014, where we presented high-quality sentiment lexicons for 136 major languages, by integrating a variety of linguistic resources into an immense knowledge graph.   Our lexicons have a polarity agreement of 95.7% with published lexicons while achieving an overall coverage of 45.2%.  Further, we demonstrated the performance of our lexicons in an extrinsic analysis of 2,000 distinct historical figures in Wikipedia articles from 30 languages.  Despite cultural difference and the intended neutrality of Wikipedia, our lexicons show an average sentiment correlation of 0.28 across all language pairs.  This paper (and the release of our lexicons) marked the successful completion of our major goal of sentiment detection systems for foreign language streams.          Last Modified: 09/10/2014       Submitted by: Steven Skiena]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
