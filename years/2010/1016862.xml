<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI:  Small:  Hierarchical Visual Scene Understanding</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>449184.00</AwardTotalIntnAmount>
<AwardAmount>449184</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Intelligent systems, both artificial and biological, must find effective ways to organize a complex visual world. The cross-disciplinary field of scene understanding is in need of a comprehensive framework in which to integrate cognitive, computational and neural approaches to the organization of knowledge. &lt;br/&gt;&lt;br/&gt;This research program aims to create a framework for organizing knowledge of visual environments that human and artificial systems encounter when navigating in the world or browsing visual databases. The aim is to determine which taxonomies are best suited for solving different visual tasks, and use computer vision algorithms to organize visual environments as humans do. For example, semantic relationships between scenes are well captured by a hierarchical tree (e.g. a basilica is a type of church, which is a type of building) but functional similarities between different environments may be best represented as clusters  (e.g. restaurants, kitchens and picnic areas clustered as places to eat; offices and internet caf√©s as places to work). &lt;br/&gt;&lt;br/&gt;Because hierarchies and taxonomies provide a way of formalizing many types of contextual information (spatial, temporal, and semantic), they can be used to enhance the performance of computer vision systems at object and scene recognition, and aid in the development of smarter image search algorithms. &lt;br/&gt;&lt;br/&gt;Besides serving as a unified benchmark for comparing different models and theories, this enterprise offers new teaching and applied tools for research and courses, which will be made available through websites and symposia.</AbstractNarration>
<MinAmdLetterDate>08/27/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016862</AwardID>
<Investigator>
<FirstName>Aude</FirstName>
<LastName>Oliva</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aude Oliva</PI_FULL_NAME>
<EmailAddress>oliva@mit.edu</EmailAddress>
<PI_PHON>6174522492</PI_PHON>
<NSF_ID>000096241</NSF_ID>
<StartDate>08/27/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~449184</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The research landscape in computer vision is poised for massive change in the next few years. With the success of new computational architectures for visual processing,&nbsp;and access to unparalleled image databases, the state of the art in computer vision is advancing rapidly. Publicly available databases have become an important resource in many fields because they provide a standard for comparing different models and theories. Within the tenure of this award, we provided two benchmarks for the field of scene understanding (the <em>SUN</em> dataset and the <em>Places</em> dataset, with more than seven millions labeled images), with additional attributes (categorical hierarchy, function, possible actions, types of objects in the scene, etc) as well as computational models of automatic scene recognition. We also released a web version of an automatic scene recognition system. Recent advancements in the arenas of image capture and storage have enabled researchers, and the public in general, to acquire an unprecedented amount of high quality images and video. Our datasets and models offer a meaningful platform for developing real-world systems of visual scene understanding. The benefit of a smart system of visual understanding could have an unprecedented impact on internet-based and wireless technologies, by enabling everyone to accomplish tasks that used to take days or minutes in a fraction of the time.</p><br> <p>            Last Modified: 12/12/2014<br>      Modified by: Aude&nbsp;Oliva</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The research landscape in computer vision is poised for massive change in the next few years. With the success of new computational architectures for visual processing, and access to unparalleled image databases, the state of the art in computer vision is advancing rapidly. Publicly available databases have become an important resource in many fields because they provide a standard for comparing different models and theories. Within the tenure of this award, we provided two benchmarks for the field of scene understanding (the SUN dataset and the Places dataset, with more than seven millions labeled images), with additional attributes (categorical hierarchy, function, possible actions, types of objects in the scene, etc) as well as computational models of automatic scene recognition. We also released a web version of an automatic scene recognition system. Recent advancements in the arenas of image capture and storage have enabled researchers, and the public in general, to acquire an unprecedented amount of high quality images and video. Our datasets and models offer a meaningful platform for developing real-world systems of visual scene understanding. The benefit of a smart system of visual understanding could have an unprecedented impact on internet-based and wireless technologies, by enabling everyone to accomplish tasks that used to take days or minutes in a fraction of the time.       Last Modified: 12/12/2014       Submitted by: Aude Oliva]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
