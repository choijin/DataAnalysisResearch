<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DC: Small: Collaborative Research: DARE: Declarative and Scalable Recovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>190000.00</AwardTotalIntnAmount>
<AwardAmount>190000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hong Jiang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>One dominant characteristic of today's large-scale computing systems&lt;br/&gt;is the prevalence of large storage clusters.  Storage clusters at the&lt;br/&gt;scale of hundreds or thousands of commodity machines are&lt;br/&gt;increasingly being deployed. At companies like Amazon, Google, Yahoo,&lt;br/&gt;and others, thousands of nodes are managed as a single system.&lt;br/&gt;&lt;br/&gt;As large clusters have brought many benefits, they also bring a new&lt;br/&gt;challenge: a growing number and frequency of failures that must be&lt;br/&gt;managed. Bits, sectors, disks, machines, racks, and many other&lt;br/&gt;components fail.  With millions of servers and hundreds of data&lt;br/&gt;centers, there are millions of opportunities for these components to&lt;br/&gt;fail. Failing to deal with failures will directly impact the&lt;br/&gt;reliability and availability of data and jobs.&lt;br/&gt;&lt;br/&gt;Unfortunately, we still hear data-loss stories even recently. For&lt;br/&gt;example, in March 2009, Facebook lost millions of photos due to&lt;br/&gt;simultaneous disk failures that "should" rarely happen at the same&lt;br/&gt;time (but it happened); in July 2009, a large bank was fined a record&lt;br/&gt;total of 3 millions pounds after losing data on thousands of its&lt;br/&gt;customers; more recently, in October 2009, T-Mobile Sidekick, which&lt;br/&gt;uses Microsoft's cloud service, also lost its customer data.  These&lt;br/&gt;incidents have shown that existing large-scale storage systems are&lt;br/&gt;still fragile to failures.&lt;br/&gt;&lt;br/&gt;To address the challenges of large-scale recovery, the goal of this&lt;br/&gt;project is to: (1) seek the fundamental problems of recovery in&lt;br/&gt;today's scalable world of computing, (2) improve the reliability,&lt;br/&gt;performance, and scalability of existing large-scale recovery, and (3)&lt;br/&gt;explore formally grounded languages to empower rigorous specification&lt;br/&gt;of recovery properties and behaviors.  Our vision is to build systems&lt;br/&gt;that "DARE to fail": systems that deliberately fail themselves,&lt;br/&gt;exercise recovery routinely, and enable easy and correct deployment of&lt;br/&gt;new recovery policies.&lt;br/&gt;&lt;br/&gt;For more information, please visit this website:&lt;br/&gt;http://boom.cs.berkeley.edu/dare/</AbstractNarration>
<MinAmdLetterDate>09/15/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1017073</AwardID>
<Investigator>
<FirstName>Andrea</FirstName>
<LastName>Arpaci-Dusseau</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrea C Arpaci-Dusseau</PI_FULL_NAME>
<EmailAddress>dusseau@cs.wisc.edu</EmailAddress>
<PI_PHON>6082656013</PI_PHON>
<NSF_ID>000275887</NSF_ID>
<StartDate>09/15/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 6401]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>161202122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>MADISON</CityName>
<StateCode>WI</StateCode>
<ZipCode>537151218</ZipCode>
<StreetAddress><![CDATA[21 North Park Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~190000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Large-scale computing and data storage systems in the cloud are an important platform for much of our society's infrastrcture. A critical factor in the availability, reliability, and performance of cloud services is how they react to failure. &nbsp;While many cloud services are able to handle a single "fail-stop" fault, how they react to other types of emerging faults are less understood. &nbsp;Other types of faults that we now must consider include when multiple components fail simultaneously, when a component misbehaves and fails silently, and when a component exhibits performance that is many times slower thanusual. &nbsp;Thus, the goals of the DARE project are to understand how existing cloud-based services react to these three new types of faults and to develop services that are more robust and scalable.</p> <p>We have analyzed a wide range of existing, widely-used cloud services (e.g., Hadoop, HDFS, ZooKeeper, Cassandra, HBase, and Dropbox) to understand how they react to these three types of failures. &nbsp;By systematically searching through the extremely large space of failures, we have identified numerous implementation and design flaws. &nbsp;For example, even in the relatively well-understood implementation of speculative execution of map-reduce jobs in Hadoop, we uncovered three problems that can lead to a collapse of the entire cluster. &nbsp;First, if all tasks in a map-reduce job are slow, then there is &ldquo;no&rdquo; stragglerthat can be identified. Second, imprecise accounting can cause a slow map node to be instead blamed on an otherwise normal reducer node. Finally, a backup task can be improperly restarted on the same problematic components due to memoryless retry.</p> <p>To fix these types of general problems, we have devised, implemented, and evaluated a number of novel solutions. &nbsp;For example, we have developed Fracture, a framework that enables an existing, complex application to be divided into individual mini-processes; each of these mini-processes can be isolated from one another, run in its own environment, and sampled, restarted, or replicated on its own. Second, we proposed an approach for robust RAID storage systems built with flash-based storage devices, Warped Mirrors; Warped Mirrors increase the liklihood that flash devices will not wear out at similar times before a faulty device can be replaced and thus, preserve the independence of device failures within a RAID array. &nbsp;Third, we implemented a robust version of a cloud-based file synchronization service, ViewBox; most importantly, ViewBox will not forward corruptions from one client machine to the cloud version or to other clients and will not propagate inconsistent file system images from a client (such as those that occur after a crash).</p> <p>We describe one of our general solutions in more detail. &nbsp;We propose that distributed systems be built with selective and lightweight versioning (SLEEVE), which can detect silent faults in select subsystems in a lightweight manner (with little space and performance overhead). For example, a developer can pick some important functionality (e.g.,file-system namespace management) and protect that functionality by developing a second lightweight implementation of thefunctionality. This approach essentially transforms a target system into an efficient two-version form.</p> <p>Using the SLEEVE approach, we hardened three pieces of HDFS functionality: namespace management, replica management, and theread/write protocol. &nbsp;Our experimental results show that while the orginal HDFS code silently misbehaves in many cases, HardFS isolates faulty behavior so that it remains within a single node. In particular, HardFS handles 90% of the fail-silent faults that result from random memory corruption and correctly detects and recovers from 100% of 78 targeted corruptions and 5 real-world bugs. &nbsp;Since errors do not propagate to p...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Large-scale computing and data storage systems in the cloud are an important platform for much of our society's infrastrcture. A critical factor in the availability, reliability, and performance of cloud services is how they react to failure.  While many cloud services are able to handle a single "fail-stop" fault, how they react to other types of emerging faults are less understood.  Other types of faults that we now must consider include when multiple components fail simultaneously, when a component misbehaves and fails silently, and when a component exhibits performance that is many times slower thanusual.  Thus, the goals of the DARE project are to understand how existing cloud-based services react to these three new types of faults and to develop services that are more robust and scalable.  We have analyzed a wide range of existing, widely-used cloud services (e.g., Hadoop, HDFS, ZooKeeper, Cassandra, HBase, and Dropbox) to understand how they react to these three types of failures.  By systematically searching through the extremely large space of failures, we have identified numerous implementation and design flaws.  For example, even in the relatively well-understood implementation of speculative execution of map-reduce jobs in Hadoop, we uncovered three problems that can lead to a collapse of the entire cluster.  First, if all tasks in a map-reduce job are slow, then there is "no" stragglerthat can be identified. Second, imprecise accounting can cause a slow map node to be instead blamed on an otherwise normal reducer node. Finally, a backup task can be improperly restarted on the same problematic components due to memoryless retry.  To fix these types of general problems, we have devised, implemented, and evaluated a number of novel solutions.  For example, we have developed Fracture, a framework that enables an existing, complex application to be divided into individual mini-processes; each of these mini-processes can be isolated from one another, run in its own environment, and sampled, restarted, or replicated on its own. Second, we proposed an approach for robust RAID storage systems built with flash-based storage devices, Warped Mirrors; Warped Mirrors increase the liklihood that flash devices will not wear out at similar times before a faulty device can be replaced and thus, preserve the independence of device failures within a RAID array.  Third, we implemented a robust version of a cloud-based file synchronization service, ViewBox; most importantly, ViewBox will not forward corruptions from one client machine to the cloud version or to other clients and will not propagate inconsistent file system images from a client (such as those that occur after a crash).  We describe one of our general solutions in more detail.  We propose that distributed systems be built with selective and lightweight versioning (SLEEVE), which can detect silent faults in select subsystems in a lightweight manner (with little space and performance overhead). For example, a developer can pick some important functionality (e.g.,file-system namespace management) and protect that functionality by developing a second lightweight implementation of thefunctionality. This approach essentially transforms a target system into an efficient two-version form.  Using the SLEEVE approach, we hardened three pieces of HDFS functionality: namespace management, replica management, and theread/write protocol.  Our experimental results show that while the orginal HDFS code silently misbehaves in many cases, HardFS isolates faulty behavior so that it remains within a single node. In particular, HardFS handles 90% of the fail-silent faults that result from random memory corruption and correctly detects and recovers from 100% of 78 targeted corruptions and 5 real-world bugs.  Since errors do not propagate to persistent storage or other nodes, previously fail-silent errors are transformed into fail-stop errors, enabling the use of standard recovery mechanisms s...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
