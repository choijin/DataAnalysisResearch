<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Collaborative Research: Programming parallel in-memory data-center applications with Piccolo</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>523290.00</AwardTotalIntnAmount>
<AwardAmount>523290</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>There is a rising demand to scale application performance by distributing&lt;br/&gt;computation across many machines in a data-center. It is difficult to write&lt;br/&gt;efficient and robust parallel programs in the data-center setting because &lt;br/&gt;programmers need to worry about reducing communication overhead while handling &lt;br/&gt;possible machine failures.  &lt;br/&gt;&lt;br/&gt;This project investigates a new data-centric parallel programming&lt;br/&gt;model, called Piccolo, that can simplify the construction of in-memory&lt;br/&gt;data-center applications such as PageRank, neural network training etc. &lt;br/&gt;&lt;br/&gt;In-memory applications can hold all their intermediate states in the aggregate&lt;br/&gt;memory of many machines and benefit from sharing these intermediate states&lt;br/&gt;between machines during computation.  Traditionally, these applications&lt;br/&gt;have been built using low-level communication-centric primitives such as MPI,&lt;br/&gt;resulting in significant programming complexity. The recently popular &lt;br/&gt;MapReduce and Dryad also do not fit well with these applications&lt;br/&gt;because their data flow programming model lacks support for shared states.&lt;br/&gt;&lt;br/&gt;Unlike data flow models, Piccolo explicitly supports the sharing of mutable,&lt;br/&gt;distributed states via a key/value table interface.  Piccolo makes sharing&lt;br/&gt;efficient by optimizing for locality of access to shared tables and&lt;br/&gt;automatically resolving write-write conflicts using user-defined accumulation&lt;br/&gt;functions.  As a result, Piccolo is easy to program for, enables applications&lt;br/&gt;that do not fit into MapReduce, and achieves good scalable performance.</AbstractNarration>
<MinAmdLetterDate>06/14/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/25/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1065169</AwardID>
<Investigator>
<FirstName>Jinyang</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jinyang Li</PI_FULL_NAME>
<EmailAddress>jinyang@cs.nyu.edu</EmailAddress>
<PI_PHON>2129983372</PI_PHON>
<NSF_ID>000105743</NSF_ID>
<StartDate>06/14/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121019</ZipCode>
<StreetAddress><![CDATA[70 WASHINGTON SQUARE S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~347722</FUND_OBLG>
<FUND_OBLG>2013~175568</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In the era of ``Big Data'', many programmers whose backgrounds are not in computer science (e.g. business, natural and social sciences) wish to process an enormous amount of data using thousands of machines in the cloud. &nbsp;However, it is extraordinarily difficult to write programs that run on many machines, even for expert programmers who are well trained in programming distributed systems. &nbsp;The goal of this project is to develop programming and storage infrastructures that can hide the complexities of distribution and simplify the task of writing distributed applications. &nbsp;</p> <p>Distributed programming frameworks make it easy to write large-scale offline batch computation, such as text analytic, machine learning algorithms etc. &nbsp;Prior to this project's work, the state-of-the-art distributed programming framework was Google's MapReduce (and its open-source clone, Hadoop). &nbsp;While MapReduce works well for large-scale text processing, it is a bad fit for large classes of important applications in machine learning, scientific computing and computational biology. &nbsp;These applications can hold all their intermediate state in memory and need to share this intermediate state between machines during computation. &nbsp;As MapReduce lacks support for distributed state, it is awkward and inefficient to write these applications in MapReduce. &nbsp;This project has developed the Piccolo programming framework for these stateful, in-memory applications. &nbsp;To better support in-memory computation, Piccolo explicitly provides the sharing of mutable, distributed state across machines via a key/value table interface. &nbsp;Piccolo makes sharing efficient by providing simple primitives that programmers can use to optimize for locality of access to shared tables and to resolve write-write conflicts using user-defined accumulation functions. &nbsp;As a result, it is easy to program in Piccolo, and it enables applications that are not handled well by MapReduce.&nbsp;</p> <p>Piccolo is designed and built for a cluster of CPU machines. &nbsp;With the recent explosion of interests in deep neural networks, there is a surging demand for distributing expensive neural network training across many GPU machines. &nbsp;We have developed the Skynet distributed training system to investigate the challenges and scalability limits of distributed neural network training. &nbsp;We have found that the simple synchronous stochastic gradient descent strategy used by Skynet performs well up to a dozen GPU machines.</p> <p>Apart from offline batch computation (such as PageRank, neural network training), another important class of applications is web applications, characterized by their online storage-intensive workloads. &nbsp;Web applications are typically backed by a database. &nbsp;Often, developers use a distributed in-memory key-value store as a caching layer to absorb the load to the backend database. &nbsp;State-of-the-art key-value systems only cache basic data items. As a result, programmers have to manually cache various derived data (such as the result of a join query). &nbsp;We have developed the Pequod system to shift the burden of maintaining the consistency of derived data onto the cache. &nbsp;Pequod calculates views on demand, incrementally updates them as required, and in many cases improves performance by reducing client communication.</p> <p><br />Another major challenge facing distributed key-value stores is how to provide fault tolerance while achieving high performance. &nbsp;In the traditional approach, data must be synchronously stored on the disks of a majority of replica nodes. &nbsp;Unfortunately, such synchronous replication increases operation latency and lowers overall throughput. &nbsp;We have demonstrated how to achieve fault-tolerant asynchronous replication with the Lazen key-value system. &nbsp;Lazen first records operations in volatile RAM ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the era of ``Big Data'', many programmers whose backgrounds are not in computer science (e.g. business, natural and social sciences) wish to process an enormous amount of data using thousands of machines in the cloud.  However, it is extraordinarily difficult to write programs that run on many machines, even for expert programmers who are well trained in programming distributed systems.  The goal of this project is to develop programming and storage infrastructures that can hide the complexities of distribution and simplify the task of writing distributed applications.    Distributed programming frameworks make it easy to write large-scale offline batch computation, such as text analytic, machine learning algorithms etc.  Prior to this project's work, the state-of-the-art distributed programming framework was Google's MapReduce (and its open-source clone, Hadoop).  While MapReduce works well for large-scale text processing, it is a bad fit for large classes of important applications in machine learning, scientific computing and computational biology.  These applications can hold all their intermediate state in memory and need to share this intermediate state between machines during computation.  As MapReduce lacks support for distributed state, it is awkward and inefficient to write these applications in MapReduce.  This project has developed the Piccolo programming framework for these stateful, in-memory applications.  To better support in-memory computation, Piccolo explicitly provides the sharing of mutable, distributed state across machines via a key/value table interface.  Piccolo makes sharing efficient by providing simple primitives that programmers can use to optimize for locality of access to shared tables and to resolve write-write conflicts using user-defined accumulation functions.  As a result, it is easy to program in Piccolo, and it enables applications that are not handled well by MapReduce.   Piccolo is designed and built for a cluster of CPU machines.  With the recent explosion of interests in deep neural networks, there is a surging demand for distributing expensive neural network training across many GPU machines.  We have developed the Skynet distributed training system to investigate the challenges and scalability limits of distributed neural network training.  We have found that the simple synchronous stochastic gradient descent strategy used by Skynet performs well up to a dozen GPU machines.  Apart from offline batch computation (such as PageRank, neural network training), another important class of applications is web applications, characterized by their online storage-intensive workloads.  Web applications are typically backed by a database.  Often, developers use a distributed in-memory key-value store as a caching layer to absorb the load to the backend database.  State-of-the-art key-value systems only cache basic data items. As a result, programmers have to manually cache various derived data (such as the result of a join query).  We have developed the Pequod system to shift the burden of maintaining the consistency of derived data onto the cache.  Pequod calculates views on demand, incrementally updates them as required, and in many cases improves performance by reducing client communication.   Another major challenge facing distributed key-value stores is how to provide fault tolerance while achieving high performance.  In the traditional approach, data must be synchronously stored on the disks of a majority of replica nodes.  Unfortunately, such synchronous replication increases operation latency and lowers overall throughput.  We have demonstrated how to achieve fault-tolerant asynchronous replication with the Lazen key-value system.  Lazen first records operations in volatile RAM while asynchronously writing to replicas' disks in the background.  A key technique of Lazen is to restore identical states to replicas even after some or all lose their memory of recent operations.  As a result...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
