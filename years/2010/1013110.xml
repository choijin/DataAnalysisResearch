<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF:  Small:  Probabilistic Considerations in the Analysis of Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>466204.00</AwardTotalIntnAmount>
<AwardAmount>466204</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balasubramanian Kalyanasundaram</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The aim of this project is to advance the understanding of various aspects of probability in relation to algorithm design and analysis. In general this means the study of randomized algorithms, the average case performance of algorithms and related, seemingly random, structures such as social networks. Randomized algorithms are important because they are often the most efficient and some times the only efficient way to solve computational problems. The study of the average case sheds light on why problems which in the worst-case seem computationally difficult or even intractable, can be routinely solved in practise, by simple algorithms. The research on random models of large real world networks is important for answering algorithmic questions about them and for understanding their evolution.&lt;br/&gt;&lt;br/&gt;The project will study several important problems from the point of view of average case analysis: (i) Cuckoo Hashing is a relatively new hashing algorithm and some of the basic questions about its performance remain unanswered, even though there has been significant progress of late. (ii) The matching problem for graphs is the quintissential polynomial time solvable problem in Combinatorial Optimiztion. Its polynomial time solution is one of the great achievements of the area. Its worst-case complexity, while polynomial still leaves room for improvement and one of the aims of the project is to settle the average case completely. (iii) The hamilton cycle problem for graphs is one of the canonical NP-hard problems. The average-case complexity was reduced to polynomial time some time ago and one of the aims of the project is to reduce this to as close to expected linear time as possible.  The project will also several other problems involving average case complexity. The methodology employed will involve the tools and techniques from the field of Random Graphs. The two main tools being concentration of measure and concentration on events that happen with probability close to one.&lt;br/&gt;&lt;br/&gt;The project will also consider the use of Rapidly Mixing Markov Chains to generate random colorings of graphs and hypergraphs. This topic has close ties to Statistical Physics and has benefited a great deal from the cross-fertilization of ideas. There are still many gaps, particularly in the case of hypergraphs, and the project aims to close them.&lt;br/&gt;&lt;br/&gt;While graph theory is at least a hundred years old, it is only in recent years that the ubiquitousness of graphs or networks has been so widely recognized. The study of Random Graphs is about fifty years old and techniques from this area are needed to study real world networks. Simply because they evolve in a seemingly random manner. The project will involve several analyses from this area.  For example, it is not known what is the component structure of a random graph, evolving under preferential attachment but subject to deletions.</AbstractNarration>
<MinAmdLetterDate>07/26/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/26/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1013110</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Frieze</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan Frieze</PI_FULL_NAME>
<EmailAddress>af1p@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122688476</PI_PHON>
<NSF_ID>000208132</NSF_ID>
<StartDate>07/26/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>PITTSBURGH</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~466204</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The modern world is replete with computational problems.Some problems are easy to solve by computer using very simple algorithms. E.g. sorting a list of names into alphabetical order. Some can be solved efficiently by sophisticated algorithms based on deep mathematical insights. E.g. Linear Programs that model the flow of goods and services in large enterprises such as oil companies.</p> <p>There are however important computational problems that seem resistant to efficient algorithms. E.g. Integer Programming, which is Linear Programming with some of the variables constrained to integers. When we say that there are no efficient algorithms, we mean that for any algorithm there are worst-case inputs that will cause a given algorithm to run for an unacceptably long time. These inputs are pathological and require ingenious design. In spite of this, many of these hard computaional problems are routinely solved in practice.</p> <p>To explain this state of affairs, we turn to the average case analysis of algorithms. By this we mean that we try to analyse the performance of an algorithm on a typical input. Hopefully this will be much much better than the worst-case performance. A large part of this project was devoted to this sort of analysis. In additon to explaining the average performance of existing algorithms, one also tries to design algorithms with good average performance.</p> <p>In addition to looking through an algorithmic lens, we have also studied the typical structure of various objects that occur as participants in computational problems. Most notably, random graphs (or networks) and hypergraphs. Random networks play an important role in the study of social and biological interactions.</p> <p>The success of the project can be measured by the large number of results of a sophisticated mathematical nature.</p><br> <p>            Last Modified: 08/10/2014<br>      Modified by: Alan&nbsp;Frieze</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The modern world is replete with computational problems.Some problems are easy to solve by computer using very simple algorithms. E.g. sorting a list of names into alphabetical order. Some can be solved efficiently by sophisticated algorithms based on deep mathematical insights. E.g. Linear Programs that model the flow of goods and services in large enterprises such as oil companies.  There are however important computational problems that seem resistant to efficient algorithms. E.g. Integer Programming, which is Linear Programming with some of the variables constrained to integers. When we say that there are no efficient algorithms, we mean that for any algorithm there are worst-case inputs that will cause a given algorithm to run for an unacceptably long time. These inputs are pathological and require ingenious design. In spite of this, many of these hard computaional problems are routinely solved in practice.  To explain this state of affairs, we turn to the average case analysis of algorithms. By this we mean that we try to analyse the performance of an algorithm on a typical input. Hopefully this will be much much better than the worst-case performance. A large part of this project was devoted to this sort of analysis. In additon to explaining the average performance of existing algorithms, one also tries to design algorithms with good average performance.  In addition to looking through an algorithmic lens, we have also studied the typical structure of various objects that occur as participants in computational problems. Most notably, random graphs (or networks) and hypergraphs. Random networks play an important role in the study of social and biological interactions.  The success of the project can be measured by the large number of results of a sophisticated mathematical nature.       Last Modified: 08/10/2014       Submitted by: Alan Frieze]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
