<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDI-Type II: Collaborative Research: Perception of Scene Layout by Machines and Visually Impaired Users</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>325741.00</AwardTotalIntnAmount>
<AwardAmount>325741</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>01060000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OIA</Abbreviation>
<LongName>Office of Integrative Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Thomas F. Russell</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The project investigates computational methods for object detection, spatial scene construction, and natural language spatial descriptions derived from real-time visual images to describe prototypical indoor spaces (e.g., rooms, offices, etc.). The primary application of this research is to provide blind or visually impaired users with spatial information about their surroundings that may otherwise be difficult to obtain from non-visual sensing. Such knowledge will assist in development of accurate cognitive models of the environment and will support better informed execution of spatial behaviors in everyday tasks. &lt;br/&gt;&lt;br/&gt;A second motivation for the work is to contribute to the improvement of spatial capacities for computers and robots. Computers and robots are similarly "blind" to images unless they have been provided some means to "see" and understand them. Currently, no robotic system is able to reliably perform high-level processing of spatial information on the basis of image sequences, e.g., to find an empty chair in a room, which not only means finding an empty chair in an image, but also localizing the chair in the room, and performing an action of reaching the chair. The guiding tenet of this research is that a better understanding of spatial knowledge acquisition from visual images and concepts of spatial awareness by humans can also be applied to reducing the ambiguity and uncertainty of information processing by autonomous systems. &lt;br/&gt;&lt;br/&gt;A central contribution of this work is to make the spatial information content of visual images available to the visually impaired, a rapidly growing demographic of our aging society. In an example scenario a blind person and her guide dog are walking to her doctor's office, an office which she has not previously visited. At the office she needs information for performing some essential tasks such as finding the check-in counter, available seating, or the bathroom. No existing accessible navigation systems are able to describe the spatial parameters of an environment and help detect and localize objects in that space. Our work will provide the underlying research and elements to realize such a system.</AbstractNarration>
<MinAmdLetterDate>09/20/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/20/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.083</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1028009</AwardID>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>09/20/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress><![CDATA[Research Services]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7751</Code>
<Text>CDI TYPE II</Text>
</ProgramElement>
<ProgramReference>
<Code>5936</Code>
<Text>GERMANY (F.R.G.)</Text>
</ProgramReference>
<ProgramReference>
<Code>5979</Code>
<Text>Europe and Eurasia</Text>
</ProgramReference>
<ProgramReference>
<Code>7721</Code>
<Text>FROM DATA TO KNOWLEDGE</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~325741</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Twelve million US residents have some form of visual impairment which challenges their daily routine and reduces their independence. One of the most common problems that visually impaired face in the daily life is to find out where they left objects around them: for example, where is the coffee cup, or what color are these shorts. The finished project tried to provide to a visually impaired person an awareness of the surrounding objects. While many applications can classify objects which occupy the whole field of view, this project introduced novel algorithms for detection of objects in clutter. Unlike other algorithms which slide a window over the image, the introduced algorithms perform an oversegmentation where every segment votes whether it belongs to an object or no, an approach resembling the biological attentional path rather than the serial operation of a sliding window over the image. The selection of segments allows the localization of the boundaries of an object allowing thus a first estimate of where such an object could be grasped. If a 3D model of the object is given then the image boundary of the hypothesized objects enables the computation of 3D position and orientation. The system could thus report to the user: " The milk container is at 30 degrees left bearing at a height of 1.2m and oriented with an angle of 30 degrees with respect to your bearing".</p><br> <p>            Last Modified: 05/19/2015<br>      Modified by: Kostas&nbsp;Daniilidis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/1028009/1028009_10047011_1432064289509_framework--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/1028009/1028009_10047011_1432064289509_framework--rgov-800width.jpg" title="3D Object Localization"><img src="/por/images/Reports/POR/2015/1028009/1028009_10047011_1432064289509_framework--rgov-66x44.jpg" alt="3D Object Localization"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A hypothesis box is detected and is  segmented into segments. Segments that make up the object are selected yielding the boundary of the object. A 3D model is fit to the boundary and its 3D position and orientation is shown from virtual views.</div> <div class="imageCredit">Menglong Zhu, University of Pennsylvania</div> <div class="imageSubmitted">Kostas&nbsp;Daniilidis</div> <div class="imageTitle">3D Object Localization</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Twelve million US residents have some form of visual impairment which challenges their daily routine and reduces their independence. One of the most common problems that visually impaired face in the daily life is to find out where they left objects around them: for example, where is the coffee cup, or what color are these shorts. The finished project tried to provide to a visually impaired person an awareness of the surrounding objects. While many applications can classify objects which occupy the whole field of view, this project introduced novel algorithms for detection of objects in clutter. Unlike other algorithms which slide a window over the image, the introduced algorithms perform an oversegmentation where every segment votes whether it belongs to an object or no, an approach resembling the biological attentional path rather than the serial operation of a sliding window over the image. The selection of segments allows the localization of the boundaries of an object allowing thus a first estimate of where such an object could be grasped. If a 3D model of the object is given then the image boundary of the hypothesized objects enables the computation of 3D position and orientation. The system could thus report to the user: " The milk container is at 30 degrees left bearing at a height of 1.2m and oriented with an angle of 30 degrees with respect to your bearing".       Last Modified: 05/19/2015       Submitted by: Kostas Daniilidis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
