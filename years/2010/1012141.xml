<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TC: Large: Collaborative Research: Practical Privacy: Metrics and Methods for Protecting Record-level and Relational Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>583169.00</AwardTotalIntnAmount>
<AwardAmount>583169</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>nan zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Safely managing the release of data containing confidential information about individuals is a problem of great societal importance. Governments, institutions, and researchers collect data whose release can have enormous benefits to society by influencing public policy or advancing scientific knowledge. But dissemination of these data can only happen if the privacy of the respondents' data is preserved or if the amount of disclosure is limited.&lt;br/&gt;&lt;br/&gt;The goal of this research project is to bridge the gap between the statistics and computer science community and between theory and practice in limiting disclosure. The research focuses on limiting statistical disclosure using synthetic data, the most advanced method from the statistics community that enables the construction of public data sets with strong statistical properties; the research incorporates formal privacy guarantees from the computer science community into this approach.  Techniques focus on household data and relational data dealing with real problems motivated by the U.S. Census Bureau and related agencies.&lt;br/&gt;&lt;br/&gt;The approach of the team is based on the development of novel techniques for boosting the utility of synthetic data generation methods with formal privacy guarantees; novel formal privacy models that formalize attackers implicitly considered in the statistics literature, and new attacker models that allow an exploration of the space between weak and strong adversaries; and novel techniques designed for data from censuses or surveys about households which have a relational structure.&lt;br/&gt;&lt;br/&gt;The research has broad impact by influencing the methodology of statistical agencies around the world. The project also develops a open-source toolkit for limiting disclosure in data publishing with formal privacy guarantees; it integrates undergraduate students into research, and it creates educational material for material for practitioners responsible for safe data handling.&lt;br/&gt;&lt;br/&gt;For further information see the project web site at the URL:&lt;br/&gt;www.cs.cornell.edu/bigreddata/privacy</AbstractNarration>
<MinAmdLetterDate>07/14/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/12/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1012141</AwardID>
<Investigator>
<FirstName>Jerome</FirstName>
<LastName>Reiter</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jerome P Reiter</PI_FULL_NAME>
<EmailAddress>jerry@stat.duke.edu</EmailAddress>
<PI_PHON>9196843030</PI_PHON>
<NSF_ID>000168419</NSF_ID>
<StartDate>07/14/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress><![CDATA[2200 W. Main St, Suite 710]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~108954</FUND_OBLG>
<FUND_OBLG>2011~113006</FUND_OBLG>
<FUND_OBLG>2012~116617</FUND_OBLG>
<FUND_OBLG>2013~244592</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The overarching goal of this project is to develop methods that help organizations share data with others without unduly compromising the confidentiality of the individuals in the data.&nbsp;&nbsp; Such widespread access has enormous benefits to society: it promotes advances in research and policy-making, it helps train students in the skills of data science, and it offers citizens ways to learn about their communities.&nbsp; However, organizations are ethically and often legally obligated to protect the confidentiality of the individuals in the data.</p> <p>To prevent unintended disclosures, organizations generally have to alter the data somehow before sharing them with others.&nbsp; Our research focused on an approach to data perturbation called synthetic data.&nbsp; In this approach, the organization releases "fake" data generated from statistical models.&nbsp; Ideally, the synthetic data reflect the most important&nbsp; relationships in the confidential data, thereby enabling meaningful data analysis, yet also have low risks of disclosures.&nbsp;</p> <p>We investigated the potential that ill-intentioned attackers could learn confidential data values by combining the information from the synthetic data with background information on the individuals in the original data.&nbsp; We learned that such attacks are unlikely to be successful in general, except for the important case when the data contain an individual who looks quite different from others, e.g., when a single billionaire is in the data file.&nbsp; The methods underlying these findings offer general approaches for assessing the disclosure risks in synthetic data.&nbsp; As a result, organizations now can better understand and quantify the risks inherent in releasing synthetic data.</p> <p>We developed methods for making synthetic data for individuals nested within households.&nbsp; The characteristics of the resulting synthetic households faithfully mimic those in the original data, e.g., family relationship structures and age compositions are similar.&nbsp; We demonstrated how this methodology can be used to generate public use, synthetic data files for the U. S. decennial census and American Community Survey.&nbsp; We developed a free software package that implements this methodology, which we made available for downloading.&nbsp; This methodology is being tested by the U. S. Census Bureau for potential use in their public use data products.</p> <p>It is inevitable that some analyses from the synthetic and original data are not as similar as we would like.&nbsp; For example, the models used to generate the synthetic data may miss certain relationships present in the original data; if so, these relationships are not present in the synthetic data.&nbsp; Unfortunately, it is difficult for users of synthetic data to determine whether or not their particular analysis is of high quality from just the synthetic data alone.&nbsp; We therefore developed a suite of methods that organizations can use to provide users feedback on the quality of statistical analyses made from synthetic data.&nbsp; We designed these methods so that queries for feedback cannot be readily manipulated by ill-intentioned attackers to learn confidential values.&nbsp; We developed a free software package that implements these verificaiton measures, which we made available for downloading.&nbsp; With these measures, users of synthetic data are now able to decide whether or not to trust their results, thereby resulting in more reliable conclusions from and increasing the usefulness of synthetic data approaches.</p> <p>The project also supported two Ph. D. students (one female and one male) in statistical science, who used the support to complete dissertations.&nbsp; As of this report, one student is now an assistant professor at Vassar College, and the other is working in industry.</p><br> <p>            Last Modified: 08/02/2016<br>      Modified by: Jerome&nbsp;P&nbsp;Reiter</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The overarching goal of this project is to develop methods that help organizations share data with others without unduly compromising the confidentiality of the individuals in the data.   Such widespread access has enormous benefits to society: it promotes advances in research and policy-making, it helps train students in the skills of data science, and it offers citizens ways to learn about their communities.  However, organizations are ethically and often legally obligated to protect the confidentiality of the individuals in the data.  To prevent unintended disclosures, organizations generally have to alter the data somehow before sharing them with others.  Our research focused on an approach to data perturbation called synthetic data.  In this approach, the organization releases "fake" data generated from statistical models.  Ideally, the synthetic data reflect the most important  relationships in the confidential data, thereby enabling meaningful data analysis, yet also have low risks of disclosures.   We investigated the potential that ill-intentioned attackers could learn confidential data values by combining the information from the synthetic data with background information on the individuals in the original data.  We learned that such attacks are unlikely to be successful in general, except for the important case when the data contain an individual who looks quite different from others, e.g., when a single billionaire is in the data file.  The methods underlying these findings offer general approaches for assessing the disclosure risks in synthetic data.  As a result, organizations now can better understand and quantify the risks inherent in releasing synthetic data.  We developed methods for making synthetic data for individuals nested within households.  The characteristics of the resulting synthetic households faithfully mimic those in the original data, e.g., family relationship structures and age compositions are similar.  We demonstrated how this methodology can be used to generate public use, synthetic data files for the U. S. decennial census and American Community Survey.  We developed a free software package that implements this methodology, which we made available for downloading.  This methodology is being tested by the U. S. Census Bureau for potential use in their public use data products.  It is inevitable that some analyses from the synthetic and original data are not as similar as we would like.  For example, the models used to generate the synthetic data may miss certain relationships present in the original data; if so, these relationships are not present in the synthetic data.  Unfortunately, it is difficult for users of synthetic data to determine whether or not their particular analysis is of high quality from just the synthetic data alone.  We therefore developed a suite of methods that organizations can use to provide users feedback on the quality of statistical analyses made from synthetic data.  We designed these methods so that queries for feedback cannot be readily manipulated by ill-intentioned attackers to learn confidential values.  We developed a free software package that implements these verificaiton measures, which we made available for downloading.  With these measures, users of synthetic data are now able to decide whether or not to trust their results, thereby resulting in more reliable conclusions from and increasing the usefulness of synthetic data approaches.  The project also supported two Ph. D. students (one female and one male) in statistical science, who used the support to complete dissertations.  As of this report, one student is now an assistant professor at Vassar College, and the other is working in industry.       Last Modified: 08/02/2016       Submitted by: Jerome P Reiter]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
