<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:  Small:  Precise Concurrency Exceptions:  Architecture Support, Semantics and System Implications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>516000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>How to popularize parallel programming is one of the Computing Research Association's Grand Research Challenges for the systems community. Being able to leverage the full potential of multi-core systems would put us back into exponential growth of usable performance, as well as lead to significant power savings. Facilitating correct and efficient execution of multithreaded programs would have a transformative effect in the IT industry, as it attacks a problem at the heart of the programmability issues in multiprocessor systems. With ubiquitous multicores and emerging parallel programs, the IT industry is now dealing with far harder reliability problems. &lt;br/&gt;&lt;br/&gt;Concurrency errors are hard to understand, are typically non-deterministic and manifest themselves way past the point of their occurrence. Moreover, they have major implications for programming language semantics. Recent work on support for concurrency debugging has made good progress, but has often focused on best-effort techniques for bug detection with probabilistic guarantees. This research takes a direct approach to the problem: making concurrency errors fail-stop by delivering an exception before the error manifests itself. In other words, the system detects that a concurrency error is about to happen and will raise an exception before the code with an error is allowed to execute.  The investigators call this mechanism concurrency exceptions. Concurrency exceptions will allow concurrency errors to be handled as conveniently as division by zero and segmentation fault.</AbstractNarration>
<MinAmdLetterDate>07/26/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016495</AwardID>
<Investigator>
<FirstName>Susan</FirstName>
<LastName>Eggers</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Susan J Eggers</PI_FULL_NAME>
<EmailAddress>eggers@cs.washington.edu</EmailAddress>
<PI_PHON>2065432118</PI_PHON>
<NSF_ID>000278740</NSF_ID>
<StartDate>07/26/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Luis</FirstName>
<LastName>Ceze</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Luis Ceze</PI_FULL_NAME>
<EmailAddress>luisceze@cs.washington.edu</EmailAddress>
<PI_PHON>2065431896</PI_PHON>
<NSF_ID>000083036</NSF_ID>
<StartDate>07/26/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~156937</FUND_OBLG>
<FUND_OBLG>2011~182904</FUND_OBLG>
<FUND_OBLG>2012~176159</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>This project aims at making parallel programming much easier by  exploring a new way of dealing with concurrency errors.&nbsp; It explores  questions with deep implications for multicore systems and how their  software is written.&nbsp; What are the canonical concurrency  errorconditions?&nbsp; How can we detect these conditions with zero  performance cost and low complexity?&nbsp; Given that concurrency bugs  involve multiple threads, which thread should get the exception?&nbsp; What  are the global state guarantees offered by the system when the exception  is delivered?&nbsp; What are possible recovery actions for concurrency  exceptions?&nbsp; And finally, what are the systems implications of  concurrency exceptions, in particular, how should the semantics of  correct thread interaction be expressed on the programming languages  level?<br /><br /></p> <p>Year four highlights were:</p> <p><strong>(1)&nbsp; Mapping Low-Level Data-Races into High-Level Data-Races</strong></p> <p>We explained why low-level data-race detectors are only useful for  low-level programs out of the box and how they can miss data races and  report false data races in high-level programs.&nbsp; To bring the benefits  of low-level data-race detection to high-level languages, we designed  low-level abstractable race detection (LARD), an extension of the  interface between low-level data-race detectors and runtime systems that  enables precise high-level data-race to be implemented using low-level  data-race detection support.&nbsp; We implemented working fully-precise  data-race exception support Java, using LARD to couple a low-level race  detector and a modified Java virtual machine.&nbsp; We evaluated the  precision of our detector and several naive low-level data-race  detection implementations for Java, showing that unmodified precise  low-level data-race detectors exhibit large numbers of missed races and  false races in practice. This was ongoing from Year 3 and now is  concluded, with an ASPLOS paper published and PhD defense done.</p> <p><strong>(2)&nbsp; FIB: Fast Instrumentation via Bias</strong></p> <p>The goal of instrumentation bias is to remove most costs of  check-access atomicity in many cases by exploiting the same obser-  vations and techniques as biased locking.&nbsp; Beyond the typical benefit of  well-applied biased locking, we can enable additional optimization  opportunities and avoid extra storage for an explicit lock.&nbsp; To improve  the efficacy of biasing instrumentation, we use adaptive information and  profiling. T his was ongoing from Year 3 and now is being wrapped up  for conference paper submission.</p> <p><strong>(3) Last Writer Slices and Communication Traps</strong></p> <p>We design efficient system support for collecting last writer slices  in executions of shared-memory concurrent programs.&nbsp; Last writer slices  are an abstraction of a program's execution that dynamically tracks  memory updates. Last writer slices provide provenance information for  values in memory that can help with debugging.&nbsp;&nbsp; We build on top of last  writer slices to develop communication traps (CTraps).&nbsp; CTraps uses  low-overhead system support for monitoring and interposing on dynamic  memory dependences corresponding to communication between threads.&nbsp;  CTraps provides an extensible framework that exposes inter-thread  communication events to CTraps applications. These can implement  analyses that monitor and react to inter-thread communication. &nbsp;<br /><br />We  fully implemented last writer slicing and CTraps.&nbsp; We also implemented a  variant of CTraps that trades off some precision for some  performance.&nbsp;&nbsp; We showed that CTraps is useful by implementing two  applications from prior work. We showed that last writer slices help  with debugging using case studies of real-world bugs.&nbsp; We evaluate the  performance and pr...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    This project aims at making parallel programming much easier by  exploring a new way of dealing with concurrency errors.  It explores  questions with deep implications for multicore systems and how their  software is written.  What are the canonical concurrency  errorconditions?  How can we detect these conditions with zero  performance cost and low complexity?  Given that concurrency bugs  involve multiple threads, which thread should get the exception?  What  are the global state guarantees offered by the system when the exception  is delivered?  What are possible recovery actions for concurrency  exceptions?  And finally, what are the systems implications of  concurrency exceptions, in particular, how should the semantics of  correct thread interaction be expressed on the programming languages  level?    Year four highlights were:  (1)  Mapping Low-Level Data-Races into High-Level Data-Races  We explained why low-level data-race detectors are only useful for  low-level programs out of the box and how they can miss data races and  report false data races in high-level programs.  To bring the benefits  of low-level data-race detection to high-level languages, we designed  low-level abstractable race detection (LARD), an extension of the  interface between low-level data-race detectors and runtime systems that  enables precise high-level data-race to be implemented using low-level  data-race detection support.  We implemented working fully-precise  data-race exception support Java, using LARD to couple a low-level race  detector and a modified Java virtual machine.  We evaluated the  precision of our detector and several naive low-level data-race  detection implementations for Java, showing that unmodified precise  low-level data-race detectors exhibit large numbers of missed races and  false races in practice. This was ongoing from Year 3 and now is  concluded, with an ASPLOS paper published and PhD defense done.  (2)  FIB: Fast Instrumentation via Bias  The goal of instrumentation bias is to remove most costs of  check-access atomicity in many cases by exploiting the same obser-  vations and techniques as biased locking.  Beyond the typical benefit of  well-applied biased locking, we can enable additional optimization  opportunities and avoid extra storage for an explicit lock.  To improve  the efficacy of biasing instrumentation, we use adaptive information and  profiling. T his was ongoing from Year 3 and now is being wrapped up  for conference paper submission.  (3) Last Writer Slices and Communication Traps  We design efficient system support for collecting last writer slices  in executions of shared-memory concurrent programs.  Last writer slices  are an abstraction of a program's execution that dynamically tracks  memory updates. Last writer slices provide provenance information for  values in memory that can help with debugging.   We build on top of last  writer slices to develop communication traps (CTraps).  CTraps uses  low-overhead system support for monitoring and interposing on dynamic  memory dependences corresponding to communication between threads.   CTraps provides an extensible framework that exposes inter-thread  communication events to CTraps applications. These can implement  analyses that monitor and react to inter-thread communication.    We  fully implemented last writer slicing and CTraps.  We also implemented a  variant of CTraps that trades off some precision for some  performance.   We showed that CTraps is useful by implementing two  applications from prior work. We showed that last writer slices help  with debugging using case studies of real-world bugs.  We evaluate the  performance and precision of our designs on a set of server programs and  standard benchmarks.  Our results show CTraps imposes overheads low  enough for use in production systems (0-15%) in many important use  cases.  This was ongoing from Year 3 and now is being prepared for  submission.   This was the last year in the g...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
