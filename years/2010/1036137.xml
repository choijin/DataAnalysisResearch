<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  System Software for Scalable Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>1.00</AwardTotalIntnAmount>
<AwardAmount>1</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Irene Qualters</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This award facilitates scientific research using the large new computational resource named Blue Waters being developed by IBM and scheduled to be deployed at the University of Illinois. &lt;br/&gt;&lt;br/&gt;As hardware complexity increases in HPC systems, it is difficult for applications to take full advantage of the available system resources and avoid potential bottlenecks. The purpose of this project is to improve the performance and productivity of key system software components on petascale platforms such as Blue Waters. The researchers from University of Chicago, University of Illinois and Argonne National Laboratores propose to study four different classes of system software: message passing libraries, parallel I/O, data visualization, and operating system.  They will use a computational chemistry application (NWChem) and a climate modeling application (CCSM) on Blue Waters to study and validate the performance and scalability of their system software components. Through rigorous experimentation, analysis, and design cycles, the researchers will dramatically improve the capabilities of not only systems being deployed in the near term, but of all systems pushing scalability limits in the near future.&lt;br/&gt;&lt;br/&gt;The benefits of this research will be available to all applications on a variety of large petascale systems in the form of revised system software.  The results will be disseminated to system vendors as well as to other researchers and students in the form of tutorials, workshop and conference presentations, and seminars.</AbstractNarration>
<MinAmdLetterDate>05/31/2011</MinAmdLetterDate>
<MaxAmdLetterDate>05/31/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1036137</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Gropp</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William D Gropp</PI_FULL_NAME>
<EmailAddress>wgropp@illinois.edu</EmailAddress>
<PI_PHON>2172446720</PI_PHON>
<NSF_ID>000266820</NSF_ID>
<StartDate>05/31/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207406</ZipCode>
<StreetAddress><![CDATA[1901 South First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7781</Code>
<Text>Leadership-Class Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7781</Code>
<Text>PETASCALE - TRACK 1</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~1</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To solve some of the most difficult problems, scientists make use of massively parallel computers, containing hundreds of thousands to millions of processing cores.&nbsp; The most widely used programming approach for using these systems is the Message-Passing Interface (MPI), which provides a way for programs to share data and to coordinate their activities.&nbsp; This project investigated the performance and scalability (the ability to use all of the processing cores at the same time in an application) of the most widely used implementation of MPI, making use of Blue Waters, one of the most powerful computers in the world.&nbsp; The project identified improvements in a number of areas, and has resulted in improvements to the implementation of MPI used on many parallel computers.&nbsp; For example, an increasingly popular and effective way to program these large-scale systems combines two different programming models &ndash; message passing with MPI, and threads, typically with OpenMP.&nbsp; We found that contention for thread locks, used to ensure correct updates to shared data, interfered with the performance of the application.&nbsp; We developed two approaches that addressed this problem, significantly improving performance and benefitting applications.&nbsp; We also investigated the impact on the performance of the application of how the processes in the parallel program are assigned to the physical resources in the parallel computer and make recommendations that can improve application performance on large system such as Blue Waters.&nbsp; These improvements directly benefit the application programs running on these systems, accelerating the progress of science and engineering.</p><br> <p>            Last Modified: 08/28/2014<br>      Modified by: William&nbsp;D&nbsp;Gropp</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To solve some of the most difficult problems, scientists make use of massively parallel computers, containing hundreds of thousands to millions of processing cores.  The most widely used programming approach for using these systems is the Message-Passing Interface (MPI), which provides a way for programs to share data and to coordinate their activities.  This project investigated the performance and scalability (the ability to use all of the processing cores at the same time in an application) of the most widely used implementation of MPI, making use of Blue Waters, one of the most powerful computers in the world.  The project identified improvements in a number of areas, and has resulted in improvements to the implementation of MPI used on many parallel computers.  For example, an increasingly popular and effective way to program these large-scale systems combines two different programming models &ndash; message passing with MPI, and threads, typically with OpenMP.  We found that contention for thread locks, used to ensure correct updates to shared data, interfered with the performance of the application.  We developed two approaches that addressed this problem, significantly improving performance and benefitting applications.  We also investigated the impact on the performance of the application of how the processes in the parallel program are assigned to the physical resources in the parallel computer and make recommendations that can improve application performance on large system such as Blue Waters.  These improvements directly benefit the application programs running on these systems, accelerating the progress of science and engineering.       Last Modified: 08/28/2014       Submitted by: William D Gropp]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
