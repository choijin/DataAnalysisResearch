<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  An Efficient Algorithm for Automated Transcription of Music, Vocalizations, and Arbitrary Sound Recordings</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>37016.00</AwardTotalIntnAmount>
<AwardAmount>37016</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Abstract&lt;br/&gt;&lt;br/&gt;This proposed EAGER project focuses on a new, robust and efficient technique to transcribe arbitrary sounds.  It also extends in a novel and transformative fashion earlier work that developed a music search engine based on identifying aesthetic similarities.  Automated transcription of musical sounds is still an open research area and one of exceptional importance.  While some limited transcription techniques are available, they lack critical abilities, such as inability to transcribe polyphonic compositions or difficulties in distinguishing sounds produced by different sources.  In addition, the frequency ranges are limited, thereby excluding a vast number of musical works.  The approach proposed in this project involves an innovative audio-to-MIDI transcription algorithm, which handles polyphonic compositions, captures harmonic, vocal and percussive instrumentation, is very efficient and works with sounds beyond human produced musical compositions, such as bird songs and sub/ultrasonic animal vocalizations.</AbstractNarration>
<MinAmdLetterDate>08/22/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/22/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1049554</AwardID>
<Investigator>
<FirstName>Bill</FirstName>
<LastName>Manaris</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bill Manaris</PI_FULL_NAME>
<EmailAddress>manarisb@cofc.edu</EmailAddress>
<PI_PHON>8439538159</PI_PHON>
<NSF_ID>000358182</NSF_ID>
<StartDate>08/22/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>College of Charleston</Name>
<CityName>CHARLESTON</CityName>
<ZipCode>294240001</ZipCode>
<PhoneNumber>8439534973</PhoneNumber>
<StreetAddress>66 GEORGE ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073723322</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLLEGE OF CHARLESTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073723322</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[College of Charleston]]></Name>
<CityName>CHARLESTON</CityName>
<StateCode>SC</StateCode>
<ZipCode>294240001</ZipCode>
<StreetAddress><![CDATA[66 GEORGE ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~37016</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Investigations into Fractals, Music, and Human Aesthetics</p> <p>The ancient Babylonians, Egyptians, and Greeks were fascinated with the technological nature of music &ndash; perhaps even more than we are today. &nbsp;For instance, Pythagoras (c. 570 &ndash; c. 495 BC) discovered that musical pitch intervals could be described by numbers. &nbsp;He and his students are credited with the discovery of mathematics, a term which they coined.&nbsp;</p> <p>Pythagoras&rsquo;s contributions helped shape the ideas of subsequent philosophers, mathematicians, and scientists, including Plato, Aristotle, and Kepler. &nbsp;One of the major Pythagorean discoveries, which helped shape music theory many centuries later, is that strings exhibit harmonic proportions &ndash; they resonate at integer ratios of their length, i.e., 1/1, 1/2, 1/3/, 1/4, 1/5, etc. (see Figure 1).</p> <p>Many centuries later, George K. Zipf, observed a similar relationship in various phenomena in nature and in human behavior, such as language and music (1949). &nbsp;He noticed that certain types of events are very frequent, whereas other types events are rare. For example, in English, short words (e.g., "a", "the") are very frequent, whereas long words (e.g., "anthropomorphologically") are quite rare. &nbsp;Zipf discovered that words in natural language follow proportions similar to those Pythagoreans observed in strings. &nbsp;If you compare how often a word appears with its statistical rank, you see an inverse relationship: successive word counts are roughly proportional to 1/1, 1/2, 1/3, 1/4, 1/5, and so on (see Figure 2). &nbsp;Zipf's main contribution was that (a) he was the first to hypothesize that there is a universal principle at play, and (b) he proposed a mathematical formula to describe it.</p> <p>Schroeder (1990) observed that the human inner ear is attuned to sounds with similar proportions. &nbsp;Given the shape of the human cochlea, such sounds stimulate acoustic nerve endings in a balanced way, thus converting the external proportions to a harmonious aesthetic experience. &nbsp;Interestingly, functional magnetic resonance imaging (fMRI) and other measurements are providing evidence of such proportions in human brain activity, including ?uctuations in the temporal lobes, brainstem, and cerebellum related to emotional memories, thoughts and meditation. (Anderson 2000; Zhang and Sejnowski 2000).</p> <p>We report the latest results from a decade-long project investigating the connection between music, these Pythagorean/Zipfian (fractal) proportions, and human aesthetics.&nbsp;</p> <p>Our driving question has been - how can we measure "beauty in music"? &nbsp;</p> <p>In this NSF project, we extended results from our decade-long investigation of the connection between music, harmonic proportions, and human aesthetics. &nbsp;Here are our results so far:</p> <p>We have found that these fractal proportions are present in musical works (see Figure 2). &nbsp;</p> <p>We have found that these fractal&nbsp;proportions fluctuate depending on composer, musical style, and popularity of musical work (2005, 2008). &nbsp;</p> <p>We have found that these fluctuations (of fractal proportions) can identify the composer of a musical piece with more than 90% accuracy (2005).</p> <p>We have found that these fluctuations&nbsp;(of fractal proportions) can identify the musical style of a piece with more than 90% accuracy (2008).</p> <p>We have found that we can predict the popularity of a musical piece with more than 90% accuracy (2007).</p> <p>We have found that we can measure similarity of musical works (2008).&nbsp;</p> <p>We have also conducted various psychological experiments, capturing emotional and physiological responses of human listeners, which demonstrate the connection between these fractal proportions and human aesthetics (2011).</p> <p>In this latest NSF project, we extended an innovative music discovery ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Investigations into Fractals, Music, and Human Aesthetics  The ancient Babylonians, Egyptians, and Greeks were fascinated with the technological nature of music &ndash; perhaps even more than we are today.  For instance, Pythagoras (c. 570 &ndash; c. 495 BC) discovered that musical pitch intervals could be described by numbers.  He and his students are credited with the discovery of mathematics, a term which they coined.   PythagorasÃ†s contributions helped shape the ideas of subsequent philosophers, mathematicians, and scientists, including Plato, Aristotle, and Kepler.  One of the major Pythagorean discoveries, which helped shape music theory many centuries later, is that strings exhibit harmonic proportions &ndash; they resonate at integer ratios of their length, i.e., 1/1, 1/2, 1/3/, 1/4, 1/5, etc. (see Figure 1).  Many centuries later, George K. Zipf, observed a similar relationship in various phenomena in nature and in human behavior, such as language and music (1949).  He noticed that certain types of events are very frequent, whereas other types events are rare. For example, in English, short words (e.g., "a", "the") are very frequent, whereas long words (e.g., "anthropomorphologically") are quite rare.  Zipf discovered that words in natural language follow proportions similar to those Pythagoreans observed in strings.  If you compare how often a word appears with its statistical rank, you see an inverse relationship: successive word counts are roughly proportional to 1/1, 1/2, 1/3, 1/4, 1/5, and so on (see Figure 2).  Zipf's main contribution was that (a) he was the first to hypothesize that there is a universal principle at play, and (b) he proposed a mathematical formula to describe it.  Schroeder (1990) observed that the human inner ear is attuned to sounds with similar proportions.  Given the shape of the human cochlea, such sounds stimulate acoustic nerve endings in a balanced way, thus converting the external proportions to a harmonious aesthetic experience.  Interestingly, functional magnetic resonance imaging (fMRI) and other measurements are providing evidence of such proportions in human brain activity, including ?uctuations in the temporal lobes, brainstem, and cerebellum related to emotional memories, thoughts and meditation. (Anderson 2000; Zhang and Sejnowski 2000).  We report the latest results from a decade-long project investigating the connection between music, these Pythagorean/Zipfian (fractal) proportions, and human aesthetics.   Our driving question has been - how can we measure "beauty in music"?    In this NSF project, we extended results from our decade-long investigation of the connection between music, harmonic proportions, and human aesthetics.  Here are our results so far:  We have found that these fractal proportions are present in musical works (see Figure 2).    We have found that these fractal proportions fluctuate depending on composer, musical style, and popularity of musical work (2005, 2008).    We have found that these fluctuations (of fractal proportions) can identify the composer of a musical piece with more than 90% accuracy (2005).  We have found that these fluctuations (of fractal proportions) can identify the musical style of a piece with more than 90% accuracy (2008).  We have found that we can predict the popularity of a musical piece with more than 90% accuracy (2007).  We have found that we can measure similarity of musical works (2008).   We have also conducted various psychological experiments, capturing emotional and physiological responses of human listeners, which demonstrate the connection between these fractal proportions and human aesthetics (2011).  In this latest NSF project, we extended an innovative music discovery engine, called Armonique. Armonique measures similarity of musical works (see Figure 3).  As such, it may be used for music search and playlist generation.  It resembles Pandora, last.fm, and iTunes Genius, except that it is fully automated (the la...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
