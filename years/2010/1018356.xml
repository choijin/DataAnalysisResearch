<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Architectural Support for New Parallel Execution Paradigms Via Agile Threads</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>468000.00</AwardTotalIntnAmount>
<AwardAmount>468000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hong Jiang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>In the multi-core processor era, microprocessors will only continue to scale in performance in the presence of abundant thread level parallelism. Achieving this goal of continuously scaling software parallelism will clearly require the exploitation of new compilation, language, and execution paradigms. One huge barrier to the viability of many proposed execution paradigms and the introduction of new paradigms is the inability of modern processor architectures to execute short threads efficiently.  Many of these new execution models can be highly effective at exposing parallelism to the hardware if they have the freedom to identify and exploit opportunities for parallelism that are 10s to 100s of instructions long.  However, current machines are not designed to execute short threads well. The goal of this research is to significantly reduce the startup cost for a new thread (or thread new to a core). This in turn reduces the break-even point that determines whether a piece of code is parallelizable or not.&lt;br/&gt;&lt;br/&gt;The term "thread migration" is used to indicate a large number of parallel execution operations, all of which involve moving stored or cached state from one core to another.  These operations include forked threads, migrated/moved threads for thermal management or load balancing, loop-parallel threads, task-level parallelism, helper threading, transactional execution, and speculative multithreading - all of these operations will be accelerated to some degree by this research. This research will attack all sources of the thread startup cost, including software (e.g., operating system) overheads, branch predictor state, cached data and instruction state, the commit latency, and the overhead of transferring the primary thread state between cores. In addition to reducing the parallel programming complexity, the broader imapcts of this research include graduate and undergraduate student training, availability of an open-souce simulation infrastructure.</AbstractNarration>
<MinAmdLetterDate>07/31/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1018356</AwardID>
<Investigator>
<FirstName>Dean</FirstName>
<LastName>Tullsen</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dean M Tullsen</PI_FULL_NAME>
<EmailAddress>tullsen@cs.ucsd.edu</EmailAddress>
<PI_PHON>8585346181</PI_PHON>
<NSF_ID>000461702</NSF_ID>
<StartDate>07/31/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~468000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As we progress into the manycore era, we become increasingly dependent &nbsp;on high levels of thread-level parallelism for performance scaling. &nbsp;This will require new programming and execution models that expose more parallelism. An important barrier to the viability of many proposed execution models is an inability to efficiently execute short&nbsp;threads, due to the overhead of copying a thread's cache state between&nbsp;cores. Several new execution models significantly decrease the mean&nbsp;core occupancy times of threads, or otherwise increase the frequency&nbsp;of thread state transfers between cores.</p> <p>This work explored both new architectures to better exploit execution paradigms that employ short threads, as well as introducing new execution paradigms that could exploit such an architecture. &nbsp;These included:&nbsp;</p> <p>1. Architecture Support for Fast Migration via Cache Working Set&nbsp;<br />Migration --&nbsp;The most significant source of lost performance when a thread migrates between cores is the loss of cache state. A significant boost in post-migration performance is possible if the cache working set can be moved, proactively, with the thread.This work accelerates thread startup performance after migration by predicting and prefetching the working set of the application into the new cache. Working set prediction as much as doubles the performanceof short-lived threads.</p> <p>2. Data Triggered Threads --&nbsp;This work introduces the concept of data-triggered threads. Unlike threads in parallel programs in conventional programming models, these threads are initiated on a change to a memory location. This enables increased parallelism and the elimination of redundant, unnecessary computation.This work shows that 78% of all loads fetch redundant data, leading to a high incidence of redundant computation. By expressing computation through data-triggered threads, that computation is executed once when the data changes, and is skipped whenever the data does not change. Significant followup to this initial work is addressed in another NSF grant.</p> <p>&nbsp;</p> <p>3. Inter-Core Prefetching --&nbsp;Multicore processors have become ubiquitous in today's systems, but exploiting the parallelism they offer remains difficult, especially for legacy application and applications with large serial components. The challenge, then, is to develop techniques that allow multiple cores to work in concert to accelerate a single thread.&nbsp;Inter-core prefetching uses one compute thread and one or more prefetching threads. The prefetching threads execute on cores that would otherwise be idle, prefetching the data that the compute thread will need. The compute thread then migrates between cores, and finds the data already waiting for it. Inter-core prefetching improves performance by an average of 31 to 63%, and speeds up some applications by as much as 2.8x. It &nbsp;reduces energy consumption between 11 and 26%.</p> <p>4. Execution Migration in a Heterogeneous-ISA Chip Multiprocessor --&nbsp;Prior research has shown that single-ISA heterogeneous chip multiprocessors have the potential for greater performance and energy efficiency than homogeneous CMPs. However, restricting the cores to a single ISA removes an important opportunity for greater heterogeneity. To take full advantage of a heterogeneous-ISA CMP, however, we must be able to migrate execution among heterogeneous cores in order to adapt to program phase changes and changing external conditions (e.g., system power state). This is non-trivial because program state is kept in an architecture-specific form; therefore, state transformation is necessary for migration. This work identifies large portions of program state whose form is not critical for performance; the compiler is modified to produce programs that keep most of their state in an architecture-neutral form so that only a small number o...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As we progress into the manycore era, we become increasingly dependent  on high levels of thread-level parallelism for performance scaling.  This will require new programming and execution models that expose more parallelism. An important barrier to the viability of many proposed execution models is an inability to efficiently execute short threads, due to the overhead of copying a thread's cache state between cores. Several new execution models significantly decrease the mean core occupancy times of threads, or otherwise increase the frequency of thread state transfers between cores.  This work explored both new architectures to better exploit execution paradigms that employ short threads, as well as introducing new execution paradigms that could exploit such an architecture.  These included:   1. Architecture Support for Fast Migration via Cache Working Set  Migration -- The most significant source of lost performance when a thread migrates between cores is the loss of cache state. A significant boost in post-migration performance is possible if the cache working set can be moved, proactively, with the thread.This work accelerates thread startup performance after migration by predicting and prefetching the working set of the application into the new cache. Working set prediction as much as doubles the performanceof short-lived threads.  2. Data Triggered Threads -- This work introduces the concept of data-triggered threads. Unlike threads in parallel programs in conventional programming models, these threads are initiated on a change to a memory location. This enables increased parallelism and the elimination of redundant, unnecessary computation.This work shows that 78% of all loads fetch redundant data, leading to a high incidence of redundant computation. By expressing computation through data-triggered threads, that computation is executed once when the data changes, and is skipped whenever the data does not change. Significant followup to this initial work is addressed in another NSF grant.     3. Inter-Core Prefetching -- Multicore processors have become ubiquitous in today's systems, but exploiting the parallelism they offer remains difficult, especially for legacy application and applications with large serial components. The challenge, then, is to develop techniques that allow multiple cores to work in concert to accelerate a single thread. Inter-core prefetching uses one compute thread and one or more prefetching threads. The prefetching threads execute on cores that would otherwise be idle, prefetching the data that the compute thread will need. The compute thread then migrates between cores, and finds the data already waiting for it. Inter-core prefetching improves performance by an average of 31 to 63%, and speeds up some applications by as much as 2.8x. It  reduces energy consumption between 11 and 26%.  4. Execution Migration in a Heterogeneous-ISA Chip Multiprocessor -- Prior research has shown that single-ISA heterogeneous chip multiprocessors have the potential for greater performance and energy efficiency than homogeneous CMPs. However, restricting the cores to a single ISA removes an important opportunity for greater heterogeneity. To take full advantage of a heterogeneous-ISA CMP, however, we must be able to migrate execution among heterogeneous cores in order to adapt to program phase changes and changing external conditions (e.g., system power state). This is non-trivial because program state is kept in an architecture-specific form; therefore, state transformation is necessary for migration. This work identifies large portions of program state whose form is not critical for performance; the compiler is modified to produce programs that keep most of their state in an architecture-neutral form so that only a small number of data items must be repositioned and no pointers need to be changed. The result is low migration cost with minimal sacrifice of non-migration performance.   5. Architecture Optimizat...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
