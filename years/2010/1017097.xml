<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Human-Driven Spatial Language for Human-Robot Interaction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>499512.00</AwardTotalIntnAmount>
<AwardAmount>499512</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>When people communicate with each other about spatially oriented tasks, they more often use qualitative spatial references (such as "behind" in the spatial description "Your eyeglasses are behind the lamp.") rather than precise quantitative terms.  Although natural for people, such qualitative references are problematic for robots that "think" in terms of mathematical expressions and numbers. Yet, providing robots with the ability to understand and communicate with these spatial references has great potential for creating a more natural interface mechanism for robot users.  This would allow users to interact with a robot much as they would with another human, and is especially critical if robots are to provide assistive capabilities in unstructured environments occupied by people.   This project will do the following: empirically capture and characterize the key components of spatial descriptions that indicate the location of a target object in a 3D immersive task embedded in an eldercare scenario; develop and refine algorithms that enable the robot to produce and comprehend descriptions containing these empirically determined key components within this scenario; and assess and validate the robot spatial language algorithm in virtual and physical environments. This project will train graduate students in an interdisciplinary setting that encompasses psychology, computer science and engineering), and will directly involve undergraduate students in the robotics work at Missouri and in the human subject experimentation work at Notre Dame.  This project will lead to a better understanding of how robots can and should be used for this class of assistive tasks in an eldercare scenario.</AbstractNarration>
<MinAmdLetterDate>07/31/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1017097</AwardID>
<Investigator>
<FirstName>Laura</FirstName>
<LastName>Carlson</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laura A Carlson</PI_FULL_NAME>
<EmailAddress>LCarlson@nd.edu</EmailAddress>
<PI_PHON>5746318052</PI_PHON>
<NSF_ID>000102527</NSF_ID>
<StartDate>07/31/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Marjorie</FirstName>
<LastName>Skubic</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marjorie Skubic</PI_FULL_NAME>
<EmailAddress>skubicm@missouri.edu</EmailAddress>
<PI_PHON>5738827766</PI_PHON>
<NSF_ID>000402198</NSF_ID>
<StartDate>07/31/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Missouri-Columbia</Name>
<CityName>COLUMBIA</CityName>
<ZipCode>652110001</ZipCode>
<PhoneNumber>5738827560</PhoneNumber>
<StreetAddress>115 Business Loop 70 W</StreetAddress>
<StreetAddress2><![CDATA[Mizzou North, Room 501]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<StateCode>MO</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MO04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153890272</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MISSOURI SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006326904</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Missouri-Columbia]]></Name>
<CityName>COLUMBIA</CityName>
<StateCode>MO</StateCode>
<ZipCode>652110001</ZipCode>
<StreetAddress><![CDATA[115 Business Loop 70 W]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MO04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~499512</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to investigate the use of natural spatial language for the purpose of directing an assistive mobile robot that could be situated in a home setting. The test case was a "find the object task" with spatial descriptions, e.g., a user telling the robot to "find the keys on the table next to the couch in the living room."&nbsp; We were particularly interested in studying whether older adults use spatial language differently than younger adults.</p> <p>We began the study with a series of human subject experiments on both younger adults and older adults, and captured their language when speaking to another person and when speaking to a robot. The results showed that older adults used different spatial language compared to younger adults. Older  adults used fewer words overall, fewer spatial units, and fewer  reference objects. Older adults referred to room labels more often.  Older adults also used fewer modifiers in their language. Older adults used different language when speaking to the robot, compared to speaking to another person including&nbsp; using a different perspective.&nbsp; Both older and younger adults used furniture items as reference objects.</p> <p>These results have implications on the design and development of a human-robot interface for elderly users, both in the language interface and the perception capabilities that would be needed to perform the task. As a result, we developed furniture recognition capabilities so that a robot could recognize a table, a couch, a bed, and so forth that might be used in the "find" operation.</p> <p>We tested speech recognition using a commercially available platform, again testing on both younger adults and older adults. The results showed that the recognition rate was higher for younger adults compared to older adults by about 10%. The recognition rate for younger male voices was higher than younger female voices. However, the recognition rate for older male voices was lower than older female voices.</p> <p>We also developed a system to process the spatial language and translate it into robot commands. And we tested the robot both in a simulated environment and in a real environment. We further tested the robustness of the method in a real environment by moving the furniture items somewhat from the original positions used for the descriptions. Even in this case, the robot was able to find the right location with a success rate of 78%.</p><br> <p>            Last Modified: 12/03/2014<br>      Modified by: Marjorie&nbsp;Skubic</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to investigate the use of natural spatial language for the purpose of directing an assistive mobile robot that could be situated in a home setting. The test case was a "find the object task" with spatial descriptions, e.g., a user telling the robot to "find the keys on the table next to the couch in the living room."  We were particularly interested in studying whether older adults use spatial language differently than younger adults.  We began the study with a series of human subject experiments on both younger adults and older adults, and captured their language when speaking to another person and when speaking to a robot. The results showed that older adults used different spatial language compared to younger adults. Older  adults used fewer words overall, fewer spatial units, and fewer  reference objects. Older adults referred to room labels more often.  Older adults also used fewer modifiers in their language. Older adults used different language when speaking to the robot, compared to speaking to another person including  using a different perspective.  Both older and younger adults used furniture items as reference objects.  These results have implications on the design and development of a human-robot interface for elderly users, both in the language interface and the perception capabilities that would be needed to perform the task. As a result, we developed furniture recognition capabilities so that a robot could recognize a table, a couch, a bed, and so forth that might be used in the "find" operation.  We tested speech recognition using a commercially available platform, again testing on both younger adults and older adults. The results showed that the recognition rate was higher for younger adults compared to older adults by about 10%. The recognition rate for younger male voices was higher than younger female voices. However, the recognition rate for older male voices was lower than older female voices.  We also developed a system to process the spatial language and translate it into robot commands. And we tested the robot both in a simulated environment and in a real environment. We further tested the robustness of the method in a real environment by moving the furniture items somewhat from the original positions used for the descriptions. Even in this case, the robot was able to find the right location with a success rate of 78%.       Last Modified: 12/03/2014       Submitted by: Marjorie Skubic]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
