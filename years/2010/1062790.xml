<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>G8 Initiative:  Collaborative Research:  ECS: Enabling Climate Simulation at Extreme Scale</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2011</AwardEffectiveDate>
<AwardExpirationDate>02/28/2014</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Daniel Katz</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This NSF award to the University of Illinois at Urbana-Champaign and the University of Tennessee at Knoxville  funds U.S. researchers participating in a project competitively selected by the G8 Research Councils Initiative on Multilateral Research through the Interdisciplinary Program on Application Software towards Exascale Computing for Global Scale Issues. This is a pilot collaboration among the U.S. National Science Foundation, the Canadian National Sciences and Engineering Research Council (NSERC), the French Agence Nationale de la Recherche (ANR), the German Deutsche Forschungsgemeinschaft (DFG), the Japan Society for the Promotion of Science (JSPS), the Russian Foundation for Basic Research (RFBR),and the United Kingdom Research Councils (RC-UK), supporting collaborative research projects selected on a competitive basis that are comprised of researchers from at least three of the partner countries.&lt;br/&gt;&lt;br/&gt;This interdisciplinary project across six countries focuses on three research topics that address limitations in numerical modeling of physics, chemistry and biology with the NCAR Community Earth System Model Version 1 (CESM1) and similar codes used by other countries. These research topics include new approaches to handle resilience, node level optimization and system level scalability. This research will enable the development of more scalable model ensembles. These will allow better evaluation of climate sensitivity and climate feedback processes, a better quantification of model uncertainty and a better understanding of the effects of natural variability.&lt;br/&gt;&lt;br/&gt;The project will provide essential knowledge toward scaling climate codes for exascale and thus reducing current uncertainties on climate evolution; it will foster interactions between computer scientists and climate scientists; will foster international collaborations in the area of climate simulations and exascale computing; and will educate a new generation of researchers that understand both the application domain of climate simulation and high-performance computing</AbstractNarration>
<MinAmdLetterDate>02/25/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1062790</AwardID>
<Investigator>
<FirstName>Marc</FirstName>
<LastName>Snir</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marc Snir</PI_FULL_NAME>
<EmailAddress>snir@illinois.edu</EmailAddress>
<PI_PHON>2172446568</PI_PHON>
<NSF_ID>000165753</NSF_ID>
<StartDate>02/25/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Donald</FirstName>
<LastName>Wuebbles</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Donald J Wuebbles</PI_FULL_NAME>
<EmailAddress>wuebbles@illinois.edu</EmailAddress>
<PI_PHON>2172441568</PI_PHON>
<NSF_ID>000298212</NSF_ID>
<StartDate>02/25/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207406</ZipCode>
<StreetAddress><![CDATA[1901 South First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>7781</Code>
<Text>Leadership-Class Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>5918</Code>
<Text>FRANCE</Text>
</ProgramReference>
<ProgramReference>
<Code>5921</Code>
<Text>JAPAN</Text>
</ProgramReference>
<ProgramReference>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>7561</Code>
<Text>CANADA</Text>
</ProgramReference>
<ProgramReference>
<Code>8060</Code>
<Text>SEES Unsolicited</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Global warming is one of the most serious problems facing humankind in the next decades. While there is no doubt that human activities are affecting climate, there still are uncertainties about the exact impact of global warming. These uncertainties are reduced by designing more accurate climate models and running a larger number of simulations that test the models against historical data and use them to predict the future evolution of climate. To do so, one will need to use the most powerful next generations supercomputers.</p> <p>We expect to see early in the next decade "exascale" supercomputers, that can execute quintillions of operations every second. Such machines will not be easy to use: They will combine hundreds of millions of processors; they will fail frequently; and power consumption will be a severe limitation. The efficient use of such machines for climate simulation will require significant changes in the climate simulation codes.</p> <p>Our project studied the performance bottlenecks in current climate codes and the mechanisms that can be used to scale these codes to future extreme scale platforms.&nbsp; Climate change is a world-wide problem. Likewise, our project was an international collaboration that brought together teams from Japan, Germany, France, Spain, Canada and US.</p> <p>The project focused on three areas:</p> <p>* Resilience: The current failure-handling approach is&nbsp; "checkpoint-restart": The application peridically dumps its state to storage; if an error is detected, then the entire computation is restarted from the last checkpoint. As errors become more frequent there is a risk that most of the machine time will be spent in checkpoints and restarts, with little forward progress. We developed mechanisms that avoid the need for global checkpoints and global restarts. We also developed annotations to indicate where errors can be tolerated in a computation and algorithms that can handle these errors.</p> <p>* Scalability: We developed performance models for key climate code modules in order to understand their performance bottlenecks. We also developed methods to reduce communication among processors in climate simulations, so as to reduce computation time and consumed energy.</p> <p>* Node performance: We developed a detailed analysis of current performance bottlenecks of climate codes at the level of individual processors, and explored the use of Graphic Processing Units (GPUs) to accelerate such codes.</p> <p>The NSF funded component of this international collaboration included much of the work on resilience and on scalability.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/08/2014<br>      Modified by: Marc&nbsp;Snir</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Global warming is one of the most serious problems facing humankind in the next decades. While there is no doubt that human activities are affecting climate, there still are uncertainties about the exact impact of global warming. These uncertainties are reduced by designing more accurate climate models and running a larger number of simulations that test the models against historical data and use them to predict the future evolution of climate. To do so, one will need to use the most powerful next generations supercomputers.  We expect to see early in the next decade "exascale" supercomputers, that can execute quintillions of operations every second. Such machines will not be easy to use: They will combine hundreds of millions of processors; they will fail frequently; and power consumption will be a severe limitation. The efficient use of such machines for climate simulation will require significant changes in the climate simulation codes.  Our project studied the performance bottlenecks in current climate codes and the mechanisms that can be used to scale these codes to future extreme scale platforms.  Climate change is a world-wide problem. Likewise, our project was an international collaboration that brought together teams from Japan, Germany, France, Spain, Canada and US.  The project focused on three areas:  * Resilience: The current failure-handling approach is  "checkpoint-restart": The application peridically dumps its state to storage; if an error is detected, then the entire computation is restarted from the last checkpoint. As errors become more frequent there is a risk that most of the machine time will be spent in checkpoints and restarts, with little forward progress. We developed mechanisms that avoid the need for global checkpoints and global restarts. We also developed annotations to indicate where errors can be tolerated in a computation and algorithms that can handle these errors.  * Scalability: We developed performance models for key climate code modules in order to understand their performance bottlenecks. We also developed methods to reduce communication among processors in climate simulations, so as to reduce computation time and consumed energy.  * Node performance: We developed a detailed analysis of current performance bottlenecks of climate codes at the level of individual processors, and explored the use of Graphic Processing Units (GPUs) to accelerate such codes.  The NSF funded component of this international collaboration included much of the work on resilience and on scalability.          Last Modified: 04/08/2014       Submitted by: Marc Snir]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
