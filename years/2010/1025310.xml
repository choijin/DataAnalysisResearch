<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Modeling Tonal Structure in Music: From Theory to Behavior and Brain Function</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Western tonal music is based on major and minor keys. When people listen to Western music over many years, they acquire an implicit understanding of how the keys are related to each other. This allows listeners to detect "wrong notes" or to perceive a sense of tension in the music. The relationships of the keys to one another, something called tonal space, can be described by their relative position on a torus (a donut-shaped geometrical space). The model developed in this proposal provides quantitative descriptions of how music moves on the torus of tonal space and how our expectations concerning that movement effect our behavioral, emotional, and neural responses.  When the model predicts that a particular note or chord is unexpected, will it be perceived as an unexpected or "wrong' note? The proposed work will also explore the timescales over which tonal expectations are structured. Is the model-predicted brain activity that follows music's movements in tonal space the same as the brain circuitry actually engaged when memories are evoked by music?&lt;br/&gt;&lt;br/&gt;The broader significance of this research lies in relating structural aspects of music to specific psychological and neural processes that underlie our ability to perceive, appreciate, and respond to music. The model that is being developed is centered on the concept of expectation. Whether the sensory input that the world provides matches our expectations is emerging as a general principle for how our brains function. By examining how expectations operate in the brain in association with music, an extremely powerful and compelling source of stimulation for humans, the investigators hope to gain more general insight into the functional organization of the human brain.</AbstractNarration>
<MinAmdLetterDate>09/19/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/19/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1025310</AwardID>
<Investigator>
<FirstName>Petr</FirstName>
<LastName>Janata</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Petr Janata</PI_FULL_NAME>
<EmailAddress>pjanata@ucdavis.edu</EmailAddress>
<PI_PHON>5307473838</PI_PHON>
<NSF_ID>000507931</NSF_ID>
<StartDate>09/19/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Davis</Name>
<CityName>Davis</CityName>
<ZipCode>956186134</ZipCode>
<PhoneNumber>5307547700</PhoneNumber>
<StreetAddress>OR/Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1850 Research Park Dr., Ste 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047120084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, DAVIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Davis]]></Name>
<CityName>Davis</CityName>
<StateCode>CA</StateCode>
<ZipCode>956186134</ZipCode>
<StreetAddress><![CDATA[OR/Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary objectives of this project were (1) to examine the ability of a computational model to explain behavioral responses to expectancy violating tonal events, e.g. "wrong notes" or "wrong chords," and (2) to make the computer programs underlying this model publically available. &nbsp;The model transforms an audio signal through several stages of processing intended to represent neural and psychological mechanisms. Because it starts with the raw audio signal, the model is faced with a more difficult task when trying to explain aspects of high-level music cognition than are models that begin at a higher stage of representing musical notes. We analyzed response time data, i.e. how long it takes a person to make a judgment about a target event that follows a context (melody or chord progression), from seven experiments that were designed to manipulate the degree to which participants' responses could be determined by the sensory information present in the contexts ("sensory processing") versus determined by the activation of musical knowledge about major and minor keys stored in long-term memory ("cognitive processing"). We used the ouput of different stages of the model to represent more "sensory" and more "cognitive" levels of processing, and combined these outputs in a statistical model in order to determine the relative contributions of each processing stage to predicting the reaction times that were observed for each of the stimuli used in the different studies (over 300 different stimuli). &nbsp;In the course of our work we discovered that we had to add a stage of processing that models the effect of "closure" in a melody or chord progression, i.e. makes the musical sequence sound complete, in order to successfully model the reaction times from all of the experiments. &nbsp;Our model performed better than all previous models that could be used to quantitatively model the relationship between the tonal properties of specific musical stimuli and behavioral responses. &nbsp;As such is provides an important tool for analyzing musical audio in terms of known neural and psychological responses to music. &nbsp;These analyses in turn allow for analyses of behavioral data and neuroimaging data so that the scientific community can better understand how we perceive music, how it is stored in long term memory, and how it interacts with other aspects of higher cognition, such as autobiographical memories.</p><br> <p>            Last Modified: 12/28/2013<br>      Modified by: Petr&nbsp;Janata</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2013/1025310/1025310_10046318_1388269458747_2projection_routes_pj--rgov-214x142.jpg" original="/por/images/Reports/POR/2013/1025310/1025310_10046318_1388269458747_2projection_routes_pj--rgov-800width.jpg" title="Processing stages of the model"><img src="/por/images/Reports/POR/2013/1025310/1025310_10046318_1388269458747_2projection_routes_pj--rgov-66x44.jpg" alt="Processing stages of the model"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Schematic diagram of the steps in a model that transforms raw musical audio from "sensory" through "cognitive" representations of tonal information.</div> <div class="imageCredit">Tom Collins</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Petr&nbsp;Janata</div> <div class="imageTitle">Processing stages of the model</div> ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary objectives of this project were (1) to examine the ability of a computational model to explain behavioral responses to expectancy violating tonal events, e.g. "wrong notes" or "wrong chords," and (2) to make the computer programs underlying this model publically available.  The model transforms an audio signal through several stages of processing intended to represent neural and psychological mechanisms. Because it starts with the raw audio signal, the model is faced with a more difficult task when trying to explain aspects of high-level music cognition than are models that begin at a higher stage of representing musical notes. We analyzed response time data, i.e. how long it takes a person to make a judgment about a target event that follows a context (melody or chord progression), from seven experiments that were designed to manipulate the degree to which participants' responses could be determined by the sensory information present in the contexts ("sensory processing") versus determined by the activation of musical knowledge about major and minor keys stored in long-term memory ("cognitive processing"). We used the ouput of different stages of the model to represent more "sensory" and more "cognitive" levels of processing, and combined these outputs in a statistical model in order to determine the relative contributions of each processing stage to predicting the reaction times that were observed for each of the stimuli used in the different studies (over 300 different stimuli).  In the course of our work we discovered that we had to add a stage of processing that models the effect of "closure" in a melody or chord progression, i.e. makes the musical sequence sound complete, in order to successfully model the reaction times from all of the experiments.  Our model performed better than all previous models that could be used to quantitatively model the relationship between the tonal properties of specific musical stimuli and behavioral responses.  As such is provides an important tool for analyzing musical audio in terms of known neural and psychological responses to music.  These analyses in turn allow for analyses of behavioral data and neuroimaging data so that the scientific community can better understand how we perceive music, how it is stored in long term memory, and how it interacts with other aspects of higher cognition, such as autobiographical memories.       Last Modified: 12/28/2013       Submitted by: Petr Janata]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
