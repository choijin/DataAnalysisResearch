<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Sparse Modeling Driven by Large-Scale Genomic Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Massive and high-dimensional data arise frequently in biology and genomics. Regularization and sparsity are critical components for modeling such data and extracting information. Broad success of sparse modeling methods, such as the Lasso, has encouraged fast development in this area. However, most existing methods were developed under the frameworks of linear models and generalized linear models. The complex structures in genomic data require further development beyond existing methods. To this end, proposed are three novel sparse modeling methods with sophisticated model structures driven by large-scale gene expression data, protein binding data and DNA sequence data. The first method, motivated by modeling the relationship between protein binding and gene expression, constructs linear regression models on the terminal nodes of a decision tree. The decision tree partitions the population into subgroups according to the predictors. Each subgroup has its own sparse linear regression model between the response and the predictors. Two types of regularization, one on the regression coefficients and the other on the size of the tree, are used to encourage sparsity. The second method concerns the construction of tight clusters for gene expression data by penalizing the difference in grouped parameters between two tight clusters and between a tight cluster and the null cluster. Block-wise coordinate descent in conjunction with majorization is developed to maximize the regularized likelihood function. The third method, motivated by the motif finding problem, aims at sequence pattern discovery. A dictionary model is used to partition a sentence into words, which represent sequence patterns, and single letters. A novel regularization through the Kullback-Leibler divergence is developed for the product-multinomial model for words, which can achieve sparsity in estimating the cell probabilities. This regularization is used to construct a sparse dictionary that contains only a small number of words. A generalized EM algorithm is proposed for parameter estimation and solution path construction.&lt;br/&gt;&lt;br/&gt;As efficient analysis of large-scale high-dimensional data is critical in many fields of science and engineering, the proposed research is of great current interest. Particularly, the proposed methods are ready for applications to front-edge research areas in genomics and molecular biology, where massive data sets have been continuously generated. To accelerate such applications, free computer packages and self-contained software are being developed for users to analyze their own data. On the other hand, this proposal contains many innovative statistical methodologies that may contribute significantly to statistics and computational sciences. Finally, the proposed research is integrated with educational activities by developing new and improving existing courses at both undergraduate and graduate levels.</AbstractNarration>
<MinAmdLetterDate>02/07/2011</MinAmdLetterDate>
<MaxAmdLetterDate>05/28/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1055286</AwardID>
<Investigator>
<FirstName>Qing</FirstName>
<LastName>Zhou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qing Zhou</PI_FULL_NAME>
<EmailAddress>zhou@stat.ucla.edu</EmailAddress>
<PI_PHON>3107947563</PI_PHON>
<NSF_ID>000005427</NSF_ID>
<StartDate>02/07/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>LOS ANGELES</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951406</ZipCode>
<StreetAddress><![CDATA[10889 Wilshire Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~65484</FUND_OBLG>
<FUND_OBLG>2012~95032</FUND_OBLG>
<FUND_OBLG>2013~90143</FUND_OBLG>
<FUND_OBLG>2014~84450</FUND_OBLG>
<FUND_OBLG>2015~64891</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Sparse modeling is an effective approach to analyzing large-scale and high-dimensional data, such as genomics data. It has been a popular topic in statistics and machine learning, with fast development in methodologies, algorithms and theory. In this project, we have done three lines of research on sparse modeling of high-dimensional and complex data, with applications in computational genomics and bioinformatics.</p> <p>First, we developed statistical learning methods based on sparse regularization for graphical models and noisy clustering. We introduced various regularization methods into structure learning of Bayesian networks, whose structure is represented by a directed acyclic graph, with applications to causal inference. Our algorithms can efficiently handle large networks with thousands of nodes from a mix of observational and intervention data. We developed theoretical results to establish the accuracy of our methods on learning sparse Bayesian networks from limited data. On the other hand, we developed a clustering algorithm via concave penalization of the distance between a pair of data points. To efficiently cluster big data, we introduced an idea of subsampling to speed up our algorithm.</p> <p>Second, we invented a novel method, called estimator augmentation, to study the distribution of regularized sparse estimators, such as the lasso and the group lasso. This line of work brings new insights into the probabilistic nature of a regularized sparse estimator. The basic idea is to augment such an estimator into a higher dimensional space in which its distribution becomes mathematically tractable so that one can find a closed-form probability density function. This allows us to use Monte Carlo methods, such as Markov chain Monte Carlo and importance sampling, to simulate from functions of a regularized estimator for making inference and quantifying uncertainty in a high-dimensional model. We demonstrated numerically its huge efficiency in calculating tail probabilities in significance tests. We also established the validity of our method in high-dimensional inference by theoretical analysis of bootstrapping the lasso and the group lasso.</p> <p>Third, we have developed domain-specific methods for decoding combinatory gene regulation and alternative splicing via statistical modeling of various genomics data. We developed a few methods to analyze combinatorial binding patterns among DNA-binding proteins in ChIP-seq data and infer their roles in regulating gene expression. Via collaborations, we also contributed methodologies for inferring differential alternative splicing from RNA-seq data.</p> <p>We have published papers, given talks and presented posters to disseminate the research results in this project to researchers in statistics, machine learning and bioinformatics. Moreover, a few software packages that implement the above methods have been developed and released for public use.</p> <p>The research of this project has been integrated into educational activities to enhance its broader impact. The PI has incorporated some of the above new methods into graduate and undergraduate courses on statistical modeling and computing. This project also provided STEM training opportunities for women and minorities. The PI has co-organized talks at UCLA on sparse modeling and related topics.</p><br> <p>            Last Modified: 08/28/2017<br>      Modified by: Qing&nbsp;Zhou</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Sparse modeling is an effective approach to analyzing large-scale and high-dimensional data, such as genomics data. It has been a popular topic in statistics and machine learning, with fast development in methodologies, algorithms and theory. In this project, we have done three lines of research on sparse modeling of high-dimensional and complex data, with applications in computational genomics and bioinformatics.  First, we developed statistical learning methods based on sparse regularization for graphical models and noisy clustering. We introduced various regularization methods into structure learning of Bayesian networks, whose structure is represented by a directed acyclic graph, with applications to causal inference. Our algorithms can efficiently handle large networks with thousands of nodes from a mix of observational and intervention data. We developed theoretical results to establish the accuracy of our methods on learning sparse Bayesian networks from limited data. On the other hand, we developed a clustering algorithm via concave penalization of the distance between a pair of data points. To efficiently cluster big data, we introduced an idea of subsampling to speed up our algorithm.  Second, we invented a novel method, called estimator augmentation, to study the distribution of regularized sparse estimators, such as the lasso and the group lasso. This line of work brings new insights into the probabilistic nature of a regularized sparse estimator. The basic idea is to augment such an estimator into a higher dimensional space in which its distribution becomes mathematically tractable so that one can find a closed-form probability density function. This allows us to use Monte Carlo methods, such as Markov chain Monte Carlo and importance sampling, to simulate from functions of a regularized estimator for making inference and quantifying uncertainty in a high-dimensional model. We demonstrated numerically its huge efficiency in calculating tail probabilities in significance tests. We also established the validity of our method in high-dimensional inference by theoretical analysis of bootstrapping the lasso and the group lasso.  Third, we have developed domain-specific methods for decoding combinatory gene regulation and alternative splicing via statistical modeling of various genomics data. We developed a few methods to analyze combinatorial binding patterns among DNA-binding proteins in ChIP-seq data and infer their roles in regulating gene expression. Via collaborations, we also contributed methodologies for inferring differential alternative splicing from RNA-seq data.  We have published papers, given talks and presented posters to disseminate the research results in this project to researchers in statistics, machine learning and bioinformatics. Moreover, a few software packages that implement the above methods have been developed and released for public use.  The research of this project has been integrated into educational activities to enhance its broader impact. The PI has incorporated some of the above new methods into graduate and undergraduate courses on statistical modeling and computing. This project also provided STEM training opportunities for women and minorities. The PI has co-organized talks at UCLA on sparse modeling and related topics.       Last Modified: 08/28/2017       Submitted by: Qing Zhou]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
