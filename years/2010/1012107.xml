<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Large: Collaborative Research: SemGrep: a System for Improving Software Reliability Through Semantic Similarity Bug Search</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2012</AwardExpirationDate>
<AwardTotalIntnAmount>131000.00</AwardTotalIntnAmount>
<AwardAmount>131000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Software bugs have been reported to take lives and cost billions of dollars annually.  Studies have shown that many bugs are "cloned" (i.e., copied-and-pasted) to many places.  Unfortunately, existing error detection tools have not provided programmers the ability to efficiently search for bug clones.  Thus, they have to resort to ad hoc manual approaches such as grepping the source tree for bug clones.&lt;br/&gt;&lt;br/&gt;This project aims to improve software reliability and integrity through automatic detection and repair of bug clones given a newly discovered vulnerability.  It will investigate a new dimension, code similarity, for detecting software bugs.  Specifically, it will investigate the feasibility of an approach that derives bug "seeds" from a new bug patch or existing static or dynamic error detection tools, searches a large code base (potentially across administrative domains) for bug clones, and automatically protects the bug clones.  This approach can detect bugs in cases where many existing techniques cannot due to code complexity:  detecting similarity between code is easier than deconstructing its meaning.&lt;br/&gt;&lt;br/&gt;If successful, this project will result in accurate tools that will help to detect and repair software vulnerabilities early.  Programmers will use these tools to detect and repair bug clones whenever applicable.  Improvements in the reliability and security of software on which business, government, and individuals depend on will positively impact society.  This project will provide a more reliable and robust computing infrastructure resilient to new threats and attacks.  Integrating the proposed research into the CS curriculum will as promote reliability and security awareness.</AbstractNarration>
<MinAmdLetterDate>06/15/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/15/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1012107</AwardID>
<Investigator>
<FirstName>Dawson</FirstName>
<LastName>Engler</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dawson R Engler</PI_FULL_NAME>
<EmailAddress>engler@csl.stanford.edu</EmailAddress>
<PI_PHON>6507230762</PI_PHON>
<NSF_ID>000485661</NSF_ID>
<StartDate>06/15/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[450 Jane Stanford Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~131000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The use of symbolic execution to find bugs and show their absence has exploded over roughly the past decade. &nbsp;The technique allows deeper analysis than other widely-used approaches such as static analysis.One of the more interesting applications has been using symbolic execution to show code is correct. &nbsp; A key problem for doing so is that typically one does not know what a piece of code intends to do. &nbsp;The contribution of our work was to show that this problem could be side-stepped in realistic settings by cross-checking functions that aimed to implement the same interface. &nbsp;On legal inputs, they must do the same thing.We demonstrated that this approach worked, that it could find many bugs,and that the techniques we devised could show their absence. &nbsp;A key result is that the programmer merely needed to provide the tool with the two routines to cross check --- the tool would fabricate the inputs as needed and use a theorem prover to check that all escapings bits were identical. &nbsp;The programmer did not have to write a specification,translate their code to a weird theorem prover language, or manually devise proof steps in order to lead a theorem-prover to sucess.</p> <p><br />We applied the technique to two mature, widely-used open source library implementations, where it:</p> <ol> <li>Found numerous interesting errors.</li> <li>&nbsp;Verified the equivalence of 300 routines (150 distinct pairs) by exhausting all their paths up to a fixed input size &nbsp;(8 bytes).</li> <li>Got high statement coverage --- the&nbsp;lowest median coverage for any experiment was 90% and the rest were 100%.</li> </ol> <p>&nbsp;</p><br> <p>            Last Modified: 10/21/2014<br>      Modified by: Dawson&nbsp;R&nbsp;Engler</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The use of symbolic execution to find bugs and show their absence has exploded over roughly the past decade.  The technique allows deeper analysis than other widely-used approaches such as static analysis.One of the more interesting applications has been using symbolic execution to show code is correct.   A key problem for doing so is that typically one does not know what a piece of code intends to do.  The contribution of our work was to show that this problem could be side-stepped in realistic settings by cross-checking functions that aimed to implement the same interface.  On legal inputs, they must do the same thing.We demonstrated that this approach worked, that it could find many bugs,and that the techniques we devised could show their absence.  A key result is that the programmer merely needed to provide the tool with the two routines to cross check --- the tool would fabricate the inputs as needed and use a theorem prover to check that all escapings bits were identical.  The programmer did not have to write a specification,translate their code to a weird theorem prover language, or manually devise proof steps in order to lead a theorem-prover to sucess.   We applied the technique to two mature, widely-used open source library implementations, where it:  Found numerous interesting errors.  Verified the equivalence of 300 routines (150 distinct pairs) by exhausting all their paths up to a fixed input size  (8 bytes). Got high statement coverage --- the lowest median coverage for any experiment was 90% and the rest were 100%.           Last Modified: 10/21/2014       Submitted by: Dawson R Engler]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
