<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: Online Control of Multisyllabic Speech Articulation Based on Auditory Feedback</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2011</AwardEffectiveDate>
<AwardExpirationDate>10/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>11948.00</AwardTotalIntnAmount>
<AwardAmount>11948</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>For normal adult talkers, articulating speech feels effortless and requires little, if any, conscious thought. However, this subjective impression belies the complexity of the neural processes underlying speech production. During speech, the articulators, including the tongue, the lips and the jaw, are moved in highly complex ways to achieve the rapid altering sequences of desired speech sounds. Our current knowledge of the brain mechanisms for the control of speech articulation remains primitive. This project aims to advance this knowledge by focusing on an important component of the speech system: auditory feedback. Auditory feedback refers to the sound of speech heard by the talker him/herself when speaking. It is has been shown to participate in correcting articulatory deviations online (with very short delays) during the production of single phonemes. However, it remains unclear how auditory feedback is used by the speech motor system to control the production of multisyllabic connected speech, the most frequently occurring type of speech in real-life communication. This is the main issue that will be addressed by this project. A computer setup based on fast digital audio processing will be used to introduce subtle perturbations of the auditory feedback of specific acoustic properties of vowel sequences when the subject utters a simple sentence.  Two types of perturbations will be used to investigate separately the spatial (frequency-related, or 'spectral') and temporal aspects of feedback-based articulatory control. The compensatory changes in the subjects' articulation will be measured indirectly by extracting changes in acoustic parameters from the audio recording of the subject's speech. In addition to characterizing the role of auditory feedback in the online control of the spatiotemporal trajectories of articulation, this project will also use functional MRI to map out the brain regions involved in this feedback control process. The knowledge gained from this project will expand our knowledge of the normal process of speech production, and will also provide clues for addressing questions in future research about the mechanisms of disorders of speech production, such as stuttering.</AbstractNarration>
<MinAmdLetterDate>04/13/2011</MinAmdLetterDate>
<MaxAmdLetterDate>04/13/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1051566</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Perkell</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph S Perkell</PI_FULL_NAME>
<EmailAddress>Perkell@mit.edu</EmailAddress>
<PI_PHON>6172533223</PI_PHON>
<NSF_ID>000265605</NSF_ID>
<StartDate>04/13/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shanqing</FirstName>
<LastName>Cai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shanqing Cai</PI_FULL_NAME>
<EmailAddress>cais@mit.edu</EmailAddress>
<PI_PHON>6172531000</PI_PHON>
<NSF_ID>000567595</NSF_ID>
<StartDate>04/13/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~11948</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project aimed to determine whether the brain utilizes real-time auditory feedback, namely the speech sounds heard by the talker himself or herself when speaking, to help control articulatory movements for multisyllabic, connected speech. This is an important question for speech science because multisyllabic speech is by far the most common type human communication, yet it has hardly been investigated and its auditory-motor underpinnings remained poorly understood.</p> <p>To address this question, we recruited a group of healthy volunteers and asked them to produce a sentence consisting of multiple words at close-to-normal speaking rates. We created a digital signal processing algorithm for perturbing the spatial and temporal parameters of auditory feedback and play altered auditory feedback to the participants in real time as they were speaking. The perturbations of the spatial parameters of auditory feedback involved changes in the formant frequencies in the speech sounds, which are determined by, and hence reflect, the positions of the articulators (e.g., the tongue and the lips). The perturbations of the temporal parameters were achieved by dynamically altering the millisecond-level timing of the formant trajectories.</p> <p>In our results, we observed that on the group level, the participants showed significant online compensations to each type of perturbations. Under the perturbations of spatial parameters, the formant frequencies in the participants&rsquo; productions changed in directions opposite to those of the perturbations, reflecting very rapid compensatory adjustment of the articulator positions. Under the perturbation of temporal parameters, the participants showed an asymmetric pattern of response: they slowed down their articulatory movements under a type of perturbation that slowed down the auditory feedback, but showed no significant changes under a type of perturbation that sped up the feedback. These compensatory responses had a latency of approximately 150 ms. &nbsp;The responses occurred despite the fact that most subjects were unaware of the existence of the perturbations, indicating automaticity of the auditory-motor interaction during speech articulation.</p> <p>To our knowledge, these findings were the first direct evidence for the involvement of auditory feedback in the online control of complex, multisyllabic articulatory patterns. They highlighted the importance of auditory feedback for the fine-tuning of both spatial and temporal parameters of speech articulation. As speech articulation is one of the most important and complex motor skills in humans, these finding also have deep implications for the principles of sequential and complex movements in general.</p> <p>In addition to the findings from healthy talkers, the funding from this project also made possible an additional study in the co-PI&rsquo;s Ph.D. dissertation that focused on the online control of speech articulation in individuals who stutter. Stuttering is disorder of speech production characterized by frequent involuntary disruption of fluency by blocks, repetitions and sound prolongations. It affects approximately 1% of the US population. Through the comparison of online compensations to the auditory-feedback perturbations from normal participants and participants who stutter, we observed that individuals who stutter made significantly weaker-than-normal compensatory adjustment in response to both the spatial and temporal perturbations. However, this magnitude of response was reduced only in early phase of the response, and reached normal magnitudes at longer post-perturbation latencies. &nbsp;This finding supported the theory of impaired, possibly slower-than-normal, auditory-motor interaction in stuttering and hence shed additional light on the etiology of this disorder.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/02/2013<br>      Modified by: Joseph&nbs...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project aimed to determine whether the brain utilizes real-time auditory feedback, namely the speech sounds heard by the talker himself or herself when speaking, to help control articulatory movements for multisyllabic, connected speech. This is an important question for speech science because multisyllabic speech is by far the most common type human communication, yet it has hardly been investigated and its auditory-motor underpinnings remained poorly understood.  To address this question, we recruited a group of healthy volunteers and asked them to produce a sentence consisting of multiple words at close-to-normal speaking rates. We created a digital signal processing algorithm for perturbing the spatial and temporal parameters of auditory feedback and play altered auditory feedback to the participants in real time as they were speaking. The perturbations of the spatial parameters of auditory feedback involved changes in the formant frequencies in the speech sounds, which are determined by, and hence reflect, the positions of the articulators (e.g., the tongue and the lips). The perturbations of the temporal parameters were achieved by dynamically altering the millisecond-level timing of the formant trajectories.  In our results, we observed that on the group level, the participants showed significant online compensations to each type of perturbations. Under the perturbations of spatial parameters, the formant frequencies in the participantsÆ productions changed in directions opposite to those of the perturbations, reflecting very rapid compensatory adjustment of the articulator positions. Under the perturbation of temporal parameters, the participants showed an asymmetric pattern of response: they slowed down their articulatory movements under a type of perturbation that slowed down the auditory feedback, but showed no significant changes under a type of perturbation that sped up the feedback. These compensatory responses had a latency of approximately 150 ms.  The responses occurred despite the fact that most subjects were unaware of the existence of the perturbations, indicating automaticity of the auditory-motor interaction during speech articulation.  To our knowledge, these findings were the first direct evidence for the involvement of auditory feedback in the online control of complex, multisyllabic articulatory patterns. They highlighted the importance of auditory feedback for the fine-tuning of both spatial and temporal parameters of speech articulation. As speech articulation is one of the most important and complex motor skills in humans, these finding also have deep implications for the principles of sequential and complex movements in general.  In addition to the findings from healthy talkers, the funding from this project also made possible an additional study in the co-PIÆs Ph.D. dissertation that focused on the online control of speech articulation in individuals who stutter. Stuttering is disorder of speech production characterized by frequent involuntary disruption of fluency by blocks, repetitions and sound prolongations. It affects approximately 1% of the US population. Through the comparison of online compensations to the auditory-feedback perturbations from normal participants and participants who stutter, we observed that individuals who stutter made significantly weaker-than-normal compensatory adjustment in response to both the spatial and temporal perturbations. However, this magnitude of response was reduced only in early phase of the response, and reached normal magnitudes at longer post-perturbation latencies.  This finding supported the theory of impaired, possibly slower-than-normal, auditory-motor interaction in stuttering and hence shed additional light on the etiology of this disorder.          Last Modified: 01/02/2013       Submitted by: Joseph S Perkell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
