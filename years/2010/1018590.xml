<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Collaborative Research: Building a Large Multilingual Semantic Network for Text Processing Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>224540.00</AwardTotalIntnAmount>
<AwardAmount>224540</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is devoted to building a large multilingual semantic network&lt;br/&gt;through the application of novel techniques for semantic analysis&lt;br/&gt;specifically targeted at the Wikipedia corpus. The driving hypothesis of&lt;br/&gt;the project is that the structure of Wikipedia can be effectively used to&lt;br/&gt;create a highly structured graph of world knowledge in which nodes&lt;br/&gt;correspond to entities and concepts described in Wikipedia, while edges&lt;br/&gt;capture ontological relations such as hypernymy and meronymy. Special&lt;br/&gt;emphasis is given to exploiting the multilingual information available in&lt;br/&gt;Wikipedia in order to improve the performance of each semantic analysis&lt;br/&gt;tool. Significant research effort is therefore aimed at developing tools&lt;br/&gt;for word sense disambiguation, reference resolution and the extraction of&lt;br/&gt;ontological relations that use multilingual reinforcement and the&lt;br/&gt;consistent structure and focused content of Wikipedia to solve these tasks&lt;br/&gt;accurately. An additional research challenge is the effective integration&lt;br/&gt;of inherently noisy evidence from multiple Wikipedia articles in order to&lt;br/&gt;increase the reliability of the overall knowledge encoded in the global&lt;br/&gt;Wikipedia graph. Computing probabilistic confidence values for every piece&lt;br/&gt;of structural information added to the network is an important step in&lt;br/&gt;this integration, and it is also meant to provide increased utility for&lt;br/&gt;downstream applications. The proposed highly structured semantic network&lt;br/&gt;complements existing semantic resources and is expected to have a broad&lt;br/&gt;impact on a wide range of natural language processing applications in need&lt;br/&gt;of large scale world knowledge.&lt;br/&gt;&lt;br/&gt;For further information, please see the project website:&lt;br/&gt;http://lit.csci.unt.edu/index.php/Mu.Se.Net</AbstractNarration>
<MinAmdLetterDate>09/13/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/13/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1018590</AwardID>
<Investigator>
<FirstName>Razvan</FirstName>
<LastName>Bunescu</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Razvan C Bunescu</PI_FULL_NAME>
<EmailAddress>rbunescu@uncc.edu</EmailAddress>
<PI_PHON>7046878444</PI_PHON>
<NSF_ID>000515843</NSF_ID>
<StartDate>09/13/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio University</Name>
<CityName>ATHENS</CityName>
<ZipCode>457012979</ZipCode>
<PhoneNumber>7405932857</PhoneNumber>
<StreetAddress>108 CUTLER HL</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>15</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH15</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041077983</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041077983</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio University]]></Name>
<CityName>ATHENS</CityName>
<StateCode>OH</StateCode>
<ZipCode>457012979</ZipCode>
<StreetAddress><![CDATA[108 CUTLER HL]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>15</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH15</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~224540</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>In this project, we built semantic analysis tools specifically targeted at the Wikipedia corpus, with the aim of building a large multilingual semantic network in which edges connect entities or concepts that are related to one another through ontological relations such as hypernymy (<em>a "book" is a "publication"</em>) or meronymy (<em>a "book" has "chapters"</em>). Each node is associated with lexicalizations in different languages, based on the multilingual information present in Wikipedia. </span></p> <p><span>The project resulted in several publications, datasets, and software systems, including:</span></p> <div><ol> <li>A taxonomic relation extraction system and a database of taxonomic relations based on Wikipedia. The system was trained on data extracted from lists and revision histories in Wikipedia, with no manual supervision. The extracted graph database contains over 2 million entity nodes and 3 million relations between pairs of entities.</li> <li><span>Supervised and semi-supervised learning approaches for multilingual word sense disambiguation and semi-supervised techniques for sense clustering. We explored the cumulative impact of features originating from multiple supporting languages on the task of word sense disambiguation, and built disambiguation systems for several languages. We also addressed the task of sense clustering in Wikipedia, using a rich feature space obtained from multilingual data, and built a system that can automatically determine if two word senses should be merged.</span></li> <li>An adaptive clustering model for coreference resolution, <span>addressing the task of clustering together nouns and pronouns that refer to the same discourse entity</span> (<em>"it" refers to a "book"</em>). The clustering model improves over the expert rules of a state-of-the-art deterministic system by using the rules as features over pairs of clusters. Statistics from a large web n-gram corpus are used to compute&nbsp;semantic compatibility features (<em>a "book" may "inspire", but a "book" cannot "eat"</em>), leading to&nbsp;improved performance for pronoun resolution.</li> </ol></div> <p><span>All the publications, datasets, and systems are publicly available at:&nbsp;</span><br /><a href="http://lit.eecs.umich.edu/research/projects/musenet" target="_blank">http://lit.eecs.umich.edu/research/projects/musenet</a></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/26/2014<br>      Modified by: Razvan&nbsp;C&nbsp;Bunescu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we built semantic analysis tools specifically targeted at the Wikipedia corpus, with the aim of building a large multilingual semantic network in which edges connect entities or concepts that are related to one another through ontological relations such as hypernymy (a "book" is a "publication") or meronymy (a "book" has "chapters"). Each node is associated with lexicalizations in different languages, based on the multilingual information present in Wikipedia.   The project resulted in several publications, datasets, and software systems, including:  A taxonomic relation extraction system and a database of taxonomic relations based on Wikipedia. The system was trained on data extracted from lists and revision histories in Wikipedia, with no manual supervision. The extracted graph database contains over 2 million entity nodes and 3 million relations between pairs of entities. Supervised and semi-supervised learning approaches for multilingual word sense disambiguation and semi-supervised techniques for sense clustering. We explored the cumulative impact of features originating from multiple supporting languages on the task of word sense disambiguation, and built disambiguation systems for several languages. We also addressed the task of sense clustering in Wikipedia, using a rich feature space obtained from multilingual data, and built a system that can automatically determine if two word senses should be merged. An adaptive clustering model for coreference resolution, addressing the task of clustering together nouns and pronouns that refer to the same discourse entity ("it" refers to a "book"). The clustering model improves over the expert rules of a state-of-the-art deterministic system by using the rules as features over pairs of clusters. Statistics from a large web n-gram corpus are used to compute semantic compatibility features (a "book" may "inspire", but a "book" cannot "eat"), leading to improved performance for pronoun resolution.   All the publications, datasets, and systems are publicly available at:  http://lit.eecs.umich.edu/research/projects/musenet             Last Modified: 11/26/2014       Submitted by: Razvan C Bunescu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
