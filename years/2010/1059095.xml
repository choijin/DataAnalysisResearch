<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHB: Medium: Quantitative Observational Practice in Family Studies: The Case of Reactivity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>766500.00</AwardTotalIntnAmount>
<AwardAmount>766500</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In contrast to the prevalent uni-modal perspective, the project focuses on quantitatively integrating information residing in multiple modalities to yield observational behavior analysis descriptions that range from global categories to time continuous behavioral abstractions and contributes novel algorithms and models for recognizing and modeling communicative and affective interaction dynamics elicited in realistic settings of couples and family therapy.  The computational challenges are multiple from automated perception of emotionally rich behaviors and cognition through models for domain specific interpretation of the sensed information, to action through combining the knowledge and expertise of humans with the information processing abilities of the machine.&lt;br/&gt;&lt;br/&gt;The research impacts a wide range of applications centered on observations of the human state and interactions, e.g., mental health applications, business customer services, negotiation tactics, law enforcement (interviews), etc. The project also provides multi-disciplinary training for undergraduate and graduate students.  The expected outcomes include benefits to psychology through novel information augmentation, in technology through improved intelligent and robust human behavior computing, and in observational practice through the introduction of transformational tools.</AbstractNarration>
<MinAmdLetterDate>08/26/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1059095</AwardID>
<Investigator>
<FirstName>Shrikanth</FirstName>
<LastName>Narayanan</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shrikanth S Narayanan</PI_FULL_NAME>
<EmailAddress>shri@sipi.usc.edu</EmailAddress>
<PI_PHON>2137406432</PI_PHON>
<NSF_ID>000377152</NSF_ID>
<StartDate>08/26/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gayla</FirstName>
<LastName>Margolin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gayla Margolin</PI_FULL_NAME>
<EmailAddress>margolin@usc.edu</EmailAddress>
<PI_PHON>2137402308</PI_PHON>
<NSF_ID>000308553</NSF_ID>
<StartDate>08/26/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Panayiotis</FirstName>
<LastName>Georgiou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Panayiotis Georgiou</PI_FULL_NAME>
<EmailAddress>georgiou@sipi.usc.edu</EmailAddress>
<PI_PHON>2137404654</PI_PHON>
<NSF_ID>000487058</NSF_ID>
<StartDate>08/26/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[University Park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~766500</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 11.0px Calibri; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 11.0px Calibri; -webkit-text-stroke: #000000; min-height: 13.0px} span.s1 {font-kerning: none} span.s2 {text-decoration: underline ; font-kerning: none} --> <p class="p1"><span class="s1">Our work focused on understanding the expression of human behavior from the multimodal channels of acoustic, visual (head motion) and lexical features, and understanding behavioral coding from humans. We focused on knowledge and data driven methods for behavioral quantification. We have employed in domain and out of domain data towards big data machine learning methods with contributions in low-resource environments, unsupervised training, and transfer learning to the data-constrained scenarios of the couple therapy domain.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">We have for example investigated using Recursive Deep Neural Network (e.g. word2vec) and Sequence to Sequence approaches to generate robust manifolds for language and then employing and adapting those in the couples therapy domain. We have shown via that that our AI Machine-Learning methods can achieve </span><span class="s2"><strong>behavioral annotation accuracy higher than the human experts themselves.</strong></span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">We have further investigated and demonstrated the ability to train large AI systems on limited amounts of data via the proposed Sparsely Connected and Disjointly Trained Deep Neural Networks framework. That has allowed us to both increase behavioral accuracy from the acoustic channel but also derive meaningful behavioral trajectories throughout the interaction.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">In general we have investigated the dynamics of human behavior throughout the couple therapy interactions through the acoustic and lexical channels. We have shown that we can both achieve significantly increased performance at the session level in identifying the behavioral code of the subject, and as a side-product we can identify the real-time (utterance level or few seconds frame) behavioral state.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">Further we investigated the evolution of the acoustic descriptors of couple language over the 26 months of interaction and showed that we can </span><span class="s2"><strong>better predict marital outcomes than human experts</strong></span><span class="s1">. The acoustic features included both session level and cross-session features and performed better than behavioral codes from human experts. Further the fusion of the two provided a slight improvement over just the acoustic features.&nbsp;</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">We further analyzed the redundancy of behavioral coding for couples therapy and provided&nbsp; improved estimation of behavior from noisy annotations, which can be critical in building robust machine estimators and assessing coder reliability. We have shown that while couples therapy data are annotated for 33 codes, even with the noise in the signal added due to the human-subjectiveness, the codes are still redundant. We created a channel-noise model for each annotator and established an improved annotation scheme, better than the baseline of just averaging annotator ratings or alternative methods from the literature.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">In the visual channel we modeled head and it's relationship to behaviors in couple therapy interactions, including through entrainment. By breaking the visual channel through head-motion tracking into horizontal and vertical elements, and by modeling each of those along both interlocutors, we showed that in interactions that the motions were more related we had more positive behaviors among the couple.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">In analyzing EDA synchrony we established a Sparse EDA Synchrony Measure (SESM) as an index of joint sparse representation of the EDA signals from the interlocutors in couple interactions. We used Simultaneous Orthogonal Matching Pursuit (SOMP) to break the signal into slow-varying trends and high-frequency signal fluctuations. We have shown that SESM can correlate highly along different emotional tasks in dyadic couple interactions.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">From a more science side we evaluated the emotional arousal of couple interactions and showed that behaviorally-based couple therapies reduce emotional arousal during couple conflict.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">In work on other data sets we investigated the expression and methods for detecting of reflections in therapist language using a range of methods such as Maximum Likelihood and Maximum Entropy language models, and Recursive Deep Neural Network lexical embedding methods. We further established methods for reliably assessing motivational interviewing fidelity using the motivational interviewing skills code.&nbsp;</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">In paralinguistic analysis we investigated the role of laughter towards client behavior change in addiction counseling. We analyzed the role of laughters during MI sessions. In our analysis with 3 separate studies we&nbsp; showed that the type of laughter (client/counselor stand alone laughter, shared laughter) can be associated with different patterns of counselor/client behaviors and depict unique relations with empathic utterances.&nbsp;</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">In two other studies we also showed that prosody and speech rate are good predictors of empathy.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">Further we improved diarization performance in our couples data (employed in above papers) through unsupervised speaker diarization using Riemannian manifold clustering. We also participated in competitions in estimating Parkinsons disease severity from diverse speech tasks and automated evaluation of non-native english pronunciation quality.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">Finally we have created a <em>prototype</em> real-time processing platform for BSP signals, that includes speech activity detection, diarization, denoising, speech recognition, behavior analysis from lexical content (transcript or ASR lattices), acoustic feature extraction and behavior analysis from acoustic features.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 11/29/2016<br>      Modified by: Panayiotis&nbsp;G&nbsp;Georgiou</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our work focused on understanding the expression of human behavior from the multimodal channels of acoustic, visual (head motion) and lexical features, and understanding behavioral coding from humans. We focused on knowledge and data driven methods for behavioral quantification. We have employed in domain and out of domain data towards big data machine learning methods with contributions in low-resource environments, unsupervised training, and transfer learning to the data-constrained scenarios of the couple therapy domain.   We have for example investigated using Recursive Deep Neural Network (e.g. word2vec) and Sequence to Sequence approaches to generate robust manifolds for language and then employing and adapting those in the couples therapy domain. We have shown via that that our AI Machine-Learning methods can achieve behavioral annotation accuracy higher than the human experts themselves.   We have further investigated and demonstrated the ability to train large AI systems on limited amounts of data via the proposed Sparsely Connected and Disjointly Trained Deep Neural Networks framework. That has allowed us to both increase behavioral accuracy from the acoustic channel but also derive meaningful behavioral trajectories throughout the interaction.   In general we have investigated the dynamics of human behavior throughout the couple therapy interactions through the acoustic and lexical channels. We have shown that we can both achieve significantly increased performance at the session level in identifying the behavioral code of the subject, and as a side-product we can identify the real-time (utterance level or few seconds frame) behavioral state.   Further we investigated the evolution of the acoustic descriptors of couple language over the 26 months of interaction and showed that we can better predict marital outcomes than human experts. The acoustic features included both session level and cross-session features and performed better than behavioral codes from human experts. Further the fusion of the two provided a slight improvement over just the acoustic features.    We further analyzed the redundancy of behavioral coding for couples therapy and provided  improved estimation of behavior from noisy annotations, which can be critical in building robust machine estimators and assessing coder reliability. We have shown that while couples therapy data are annotated for 33 codes, even with the noise in the signal added due to the human-subjectiveness, the codes are still redundant. We created a channel-noise model for each annotator and established an improved annotation scheme, better than the baseline of just averaging annotator ratings or alternative methods from the literature.   In the visual channel we modeled head and it's relationship to behaviors in couple therapy interactions, including through entrainment. By breaking the visual channel through head-motion tracking into horizontal and vertical elements, and by modeling each of those along both interlocutors, we showed that in interactions that the motions were more related we had more positive behaviors among the couple.   In analyzing EDA synchrony we established a Sparse EDA Synchrony Measure (SESM) as an index of joint sparse representation of the EDA signals from the interlocutors in couple interactions. We used Simultaneous Orthogonal Matching Pursuit (SOMP) to break the signal into slow-varying trends and high-frequency signal fluctuations. We have shown that SESM can correlate highly along different emotional tasks in dyadic couple interactions.   From a more science side we evaluated the emotional arousal of couple interactions and showed that behaviorally-based couple therapies reduce emotional arousal during couple conflict.   In work on other data sets we investigated the expression and methods for detecting of reflections in therapist language using a range of methods such as Maximum Likelihood and Maximum Entropy language models, and Recursive Deep Neural Network lexical embedding methods. We further established methods for reliably assessing motivational interviewing fidelity using the motivational interviewing skills code.    In paralinguistic analysis we investigated the role of laughter towards client behavior change in addiction counseling. We analyzed the role of laughters during MI sessions. In our analysis with 3 separate studies we  showed that the type of laughter (client/counselor stand alone laughter, shared laughter) can be associated with different patterns of counselor/client behaviors and depict unique relations with empathic utterances.    In two other studies we also showed that prosody and speech rate are good predictors of empathy.   Further we improved diarization performance in our couples data (employed in above papers) through unsupervised speaker diarization using Riemannian manifold clustering. We also participated in competitions in estimating Parkinsons disease severity from diverse speech tasks and automated evaluation of non-native english pronunciation quality.   Finally we have created a prototype real-time processing platform for BSP signals, that includes speech activity detection, diarization, denoising, speech recognition, behavior analysis from lexical content (transcript or ASR lattices), acoustic feature extraction and behavior analysis from acoustic features.          Last Modified: 11/29/2016       Submitted by: Panayiotis G Georgiou]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
