<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Large Vocabulary Gesture Recognition for Everyone: Gesture Modeling and Recognition Tools for System Builders and Users</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>513563.00</AwardTotalIntnAmount>
<AwardAmount>651563</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The PI's goal in this project is to develop new methods for automatically annotating, recognizing, and indexing large vocabularies of gestures, and to use these methods to create an integrated set of tools for sign language recognition.  Current state-of-the-art methods for recognizing large vocabularies of gestures have significant limitations that impact both system design and the user experience.  Many methods assume the existence of a near-perfect hand detector/tracker; that is a limiting assumption, which prevents deployment of these methods in complex real-world settings where such accuracy is unachievable.   In the absence of perfect hand detectors, system design may involve a large investment in manual annotation of training videos (e.g., specifying hand locations), so as to provide sufficiently clean information to training modules.   The user experience is affected by the limited accuracy and robustness of existing applications.   In this research the PI will address these issues by explicitly designing recognition and indexing methods that require neither perfect hand detectors nor extensive manual annotations, thus making it substantially easier to deploy accurate and efficient gesture recognition systems in real-world settings.  The PI will achieve these objectives through theoretical advances in the current state of the art in computer vision, pattern recognition, and database indexing.  The unifying theme in the project is the integration of low-level tracking modules that produce imperfect output, with recognition and indexing methods that are designed to take as input this imperfect output from the tracking modules.  Novel articulated tracking methods will be developed that utilize probabilistic graph models to provide fully automatic long-term tracking, while improving upon the excessive time complexity that probabilistic graph models currently incur.   New methods will be designed for extracting and exploiting information from hand appearance.  As these novel modeling and recognition methods will violate standard assumptions made by existing indexing methods, new indexing methods will be formulated which will improve the efficiency of search in large databases of dynamic gestures and static hand shapes within the proposed framework.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  Project outcomes will significantly improve the ability of sign language users around the world to search databases of sign language videos and to perform tasks such as looking up the meaning of an unknown sign or retrieving occurrences of a sign of interest in videos of continuous signing.  These search tools will have an impact in educational settings, facilitating both learning a sign language and accessing arbitrary information available in a sign language.  To these ends, the PI will make his software freely available to the public online.  He will also work with experts in American Sign Language to implement key applications using his tools, which will be made available to Deaf students.  The PI will furthermore develop a publicly available package of gesture recognition source code, applications, and datasets that will help student researchers at all levels engage in gesture recognition research.   As an additional outreach activity intended to attract young people to careers in science, the PI will co-organize summer camps that educate junior high and high school students in computer science.</AbstractNarration>
<MinAmdLetterDate>12/09/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/11/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1055062</AwardID>
<Investigator>
<FirstName>Vassilis</FirstName>
<LastName>Athitsos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vassilis Athitsos</PI_FULL_NAME>
<EmailAddress>athitsos@uta.edu</EmailAddress>
<PI_PHON>8172722105</PI_PHON>
<NSF_ID>000308836</NSF_ID>
<StartDate>12/09/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Arlington</Name>
<CityName>Arlington</CityName>
<ZipCode>760190145</ZipCode>
<PhoneNumber>8172722105</PhoneNumber>
<StreetAddress>701 S Nedderman Dr, Box 19145</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>064234610</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT ARLINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Arlington]]></Name>
<CityName>Arlington</CityName>
<StateCode>TX</StateCode>
<ZipCode>760190145</ZipCode>
<StreetAddress><![CDATA[701 S Nedderman Dr, Box 19145]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7218</Code>
<Text>RET SUPP-Res Exp for Tchr Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>7233</Code>
<Text>RES EXPERIENCE FOR TEACHERS(RET)-SUPPLEM</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~141773</FUND_OBLG>
<FUND_OBLG>2012~113360</FUND_OBLG>
<FUND_OBLG>2013~124025</FUND_OBLG>
<FUND_OBLG>2014~126779</FUND_OBLG>
<FUND_OBLG>2015~129626</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-d456d57f-836c-bf60-2193-9b35bad44d93"> <p dir="ltr"><span>The goal of the project was to develop computer vision methods for efficient and accurate recognition of a large number of gestures. The main motivating application for the project has been a system for looking up the meaning of signs in American Sign Language (ASL). Such a system would allow a user to perform a sign in front of a camera, and then the system would identify that sign and present to the user information about that sign, such as its meaning, words with similar meaning in English, and example sentences including that sign. This kind of technology can be useful both for learners of ASL who encounter an unknown sign, and for Deaf users of ASL, who are looking for the appropriate English word for a sign.</span></p> <p dir="ltr"><span>In terms of intellectual merit, we have developed novel methods for recognizing a large number of signs, and we have demonstrated the performance of those methods on a dataset containing 1,113 distinct sign classes. Some of our methods are applicable to depth videos, that can be obtained for example using the Kinect camera, and some of our methods are applicable to color videos, that can be recorded with a regular inexpensive webcam.</span></p> <p dir="ltr"><span>In addition, we have proposed theoretical methods that are applicable in more general settings than just sign language. We have designed methods for accurate gesture recognition in the presence of very few training examples for each gesture class, including the case where only a single training example per gesture class is available. We have also proposed several new methods for improving the accuracy and efficiency of searching large databases of strings and time series, that can be applied, for example, to search databases storing string representations of proteins and DNA sequences.</span></p> <p dir="ltr"><span>In terms of broader impact, our contributions to sign language recognition can be used directly to design better educational tools, such as interactive sign look-up systems, for users and learners of American Sign Language. The datasets that we have created are publicly available to researchers and educators, and will be a useful resource for sign language research in the future. The methods that we have developed are also applicable for general gesture recognition applications, and they can help improve gesture-based human-computer interfaces in diverse settings such as gaming or assistive interfaces for people with disabilities. Last but not least, the project has involved more than twenty students, both graduate and undergraduate, providing them with funding and opportunities to engage in research and obtain a solid background in computer vision.</span></p> </span></p><br> <p>            Last Modified: 07/27/2017<br>      Modified by: Vassilis&nbsp;Athitsos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The goal of the project was to develop computer vision methods for efficient and accurate recognition of a large number of gestures. The main motivating application for the project has been a system for looking up the meaning of signs in American Sign Language (ASL). Such a system would allow a user to perform a sign in front of a camera, and then the system would identify that sign and present to the user information about that sign, such as its meaning, words with similar meaning in English, and example sentences including that sign. This kind of technology can be useful both for learners of ASL who encounter an unknown sign, and for Deaf users of ASL, who are looking for the appropriate English word for a sign. In terms of intellectual merit, we have developed novel methods for recognizing a large number of signs, and we have demonstrated the performance of those methods on a dataset containing 1,113 distinct sign classes. Some of our methods are applicable to depth videos, that can be obtained for example using the Kinect camera, and some of our methods are applicable to color videos, that can be recorded with a regular inexpensive webcam. In addition, we have proposed theoretical methods that are applicable in more general settings than just sign language. We have designed methods for accurate gesture recognition in the presence of very few training examples for each gesture class, including the case where only a single training example per gesture class is available. We have also proposed several new methods for improving the accuracy and efficiency of searching large databases of strings and time series, that can be applied, for example, to search databases storing string representations of proteins and DNA sequences. In terms of broader impact, our contributions to sign language recognition can be used directly to design better educational tools, such as interactive sign look-up systems, for users and learners of American Sign Language. The datasets that we have created are publicly available to researchers and educators, and will be a useful resource for sign language research in the future. The methods that we have developed are also applicable for general gesture recognition applications, and they can help improve gesture-based human-computer interfaces in diverse settings such as gaming or assistive interfaces for people with disabilities. Last but not least, the project has involved more than twenty students, both graduate and undergraduate, providing them with funding and opportunities to engage in research and obtain a solid background in computer vision.        Last Modified: 07/27/2017       Submitted by: Vassilis Athitsos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
