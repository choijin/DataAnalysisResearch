<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Haptics for Large Scale Virtual Environments to Assess Assembly Tasks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2011</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>134000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Diwakar Gupta</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The objective of this EAGER award is to assess the performance of users in a large space virtual environment equipped with haptic interaction in order to achieve a more realistic assembly workspace simulation. To achieve this objective, a new method of using a haptic device in a large space virtual environment will be developed and tested. The approach to providing haptic interaction in a large space virtual environment is to combine a six-degree-of-freedom haptic device with a mobile powered platform. Large space virtual environments can consist of arrangements of large screen projection systems that support position tracking and stereo viewing or rooms that have large area tracking combined with head mounted displays. Placing the haptic device on a mobile platform will allow the investigators to use this haptic device in the projection screen environment as well as the large area tracked room. The mobile platform will be position tracked so that the investigators can control the relative movement of the haptic device with respect to the user. The intent is that the user would not actively move the platform around the space, but that the mobile platform would be intelligent enough to follow the user as he/she moves around in the space. &lt;br/&gt;&lt;br/&gt;The successful completion of this research has great potential to improve product design. With the expansion of force feedback to encompass the full area of an assembly workstation, virtual reality technology can be used by manufacturing engineers to prototype how humans interact with products during part assembly, long before the first part reaches the assembly station. Using CAD models and haptic devices in a large scale virtual environment, product design teams can explore the human/product/workstation interaction that affects worker ergonomics, fixture and tooling design. Other product designers can explore human/product interaction as they design safety measures and develop maintenance methods. In-depth evaluations using CAD models early in the design process can save re-work and re-design which add unnecessary costs to the final product. As a part of this award, the PIs will engage students from multiple disciplines in the research. As a result, these students will experience a meaningful multidisciplinary design project and leave with an understanding and appreciation for the knowledge that can be contributed from people who come from different disciplines.</AbstractNarration>
<MinAmdLetterDate>08/26/2010</MinAmdLetterDate>
<MaxAmdLetterDate>11/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1061458</AwardID>
<Investigator>
<FirstName>Greg</FirstName>
<LastName>Luecke</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Greg R Luecke</PI_FULL_NAME>
<EmailAddress>grluecke@iastate.edu</EmailAddress>
<PI_PHON>5152945916</PI_PHON>
<NSF_ID>000400214</NSF_ID>
<StartDate>08/26/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Judy</FirstName>
<LastName>Vance</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Judy M Vance</PI_FULL_NAME>
<EmailAddress>jmvance@iastate.edu</EmailAddress>
<PI_PHON>5152949474</PI_PHON>
<NSF_ID>000393149</NSF_ID>
<StartDate>08/26/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Iowa State University</Name>
<CityName>AMES</CityName>
<ZipCode>500112207</ZipCode>
<PhoneNumber>5152945225</PhoneNumber>
<StreetAddress>1138 Pearson</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IA04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005309844</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>IOWA STATE UNIVERSITY OF SCIENCE AND TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005309844</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Iowa State University]]></Name>
<CityName>AMES</CityName>
<StateCode>IA</StateCode>
<ZipCode>500112207</ZipCode>
<StreetAddress><![CDATA[1138 Pearson]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IA04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1464</Code>
<Text>ESD-Eng &amp; Systems Design</Text>
</ProgramElement>
<ProgramReference>
<Code>067E</Code>
<Text>DESIGN TOOLS</Text>
</ProgramReference>
<ProgramReference>
<Code>068E</Code>
<Text>DESIGN THEORY</Text>
</ProgramReference>
<ProgramReference>
<Code>073E</Code>
<Text>OPTIMIZATION &amp; DECISION MAKING</Text>
</ProgramReference>
<ProgramReference>
<Code>079E</Code>
<Text>VISUALIZATION &amp; VIRTUAL DESIGN</Text>
</ProgramReference>
<ProgramReference>
<Code>116E</Code>
<Text>RESEARCH EXP FOR UNDERGRADS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9231</Code>
<Text>SUPPL FOR UNDERGRAD RES ASSIST</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>MANU</Code>
<Text>MANUFACTURING</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~100000</FUND_OBLG>
<FUND_OBLG>2011~12000</FUND_OBLG>
<FUND_OBLG>2012~12000</FUND_OBLG>
<FUND_OBLG>2013~10000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Virtual reality describes a set of technologies that allow a person to naturally interact with digitally created three-dimensional spaces. Generally, position tracking of the person is captured through sensors worn on the body. Changes in position are interpreted in software and are used to update the computer projected images as if the person was interacting with them in the real world. This technology allows a person to move around virtual objects, look under or over virtual objects, grab and pick up virtual objects and walk through entire computer generated scenes as if they were real. Haptics is the technology that provides force feedback in the virtual environment. We experience our world through our senses of sight, touch, smell, hearing and taste. Haptic devices are used to simulate the force feedback of the real world. Adding haptics to a completely visual virtual environment can increase the realism of the experience. To date, most large scale (large working volume and large force generating) haptic devices are anchored to the ground or the table, limiting their working space to a meter or less. With increasing use of large projection screen virtual environment rooms, or CAVEs, there is a need to provide haptics within the large space of an entire room. The objective of this research was to develop and test a new method of using a haptic device in a large space virtual environment. &nbsp;The device that emerged from this research consists of a large working volume haptic device mounted on a small mobile robot. The robot has mecanum wheels which allow it to move in any direction. The controls were developed such that the robot would follow the person around in the CAVE space and provide haptic feedback at any time the user "touched" a virtual object. The commercial haptic device consisted of a multi-link robotic arm that provided force in three-dimensions and torque along three axes. The user in the virtual environment held onto the end of the robotic arm and could walk around the area within the CAVE freely, with the mobil robot following the movement. At any time, the user can reach out, press a button on the end effector of the robotic arm and "grab" a virtual object. At that time, the weight of the object would be simulated and those weight forces would be applied to the user's hand. The user could freely manipulate the virtual object by moving the robotic end effector. If the virtual object collided with any other objects in the virtual environment, the motors of the robot would be activated and the user would feel the collision force through the end effector of the robot. The real advances made in this research are the integration of the mobile robot with the haptic robotic arm and the control system that resulted in the mobile robot moving freely around the CAVE, following the user. Throughout the period of this grant, several graduate and undergraduate students participated. One visiting student from France also worked on this project. The project successfully integrated disciplinary aspects from&nbsp;robotics, controls, haptics, virtual reality and virtual environments. The students also became proficient at giving presentations of their work through the numerous demonstrations they performed for K-12 students, university students, faculty and industry. Three papers were published describing this research and a YouTube video is available.</p><br> <p>            Last Modified: 04/08/2016<br>      Modified by: Judy&nbsp;M&nbsp;Vance</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Virtual reality describes a set of technologies that allow a person to naturally interact with digitally created three-dimensional spaces. Generally, position tracking of the person is captured through sensors worn on the body. Changes in position are interpreted in software and are used to update the computer projected images as if the person was interacting with them in the real world. This technology allows a person to move around virtual objects, look under or over virtual objects, grab and pick up virtual objects and walk through entire computer generated scenes as if they were real. Haptics is the technology that provides force feedback in the virtual environment. We experience our world through our senses of sight, touch, smell, hearing and taste. Haptic devices are used to simulate the force feedback of the real world. Adding haptics to a completely visual virtual environment can increase the realism of the experience. To date, most large scale (large working volume and large force generating) haptic devices are anchored to the ground or the table, limiting their working space to a meter or less. With increasing use of large projection screen virtual environment rooms, or CAVEs, there is a need to provide haptics within the large space of an entire room. The objective of this research was to develop and test a new method of using a haptic device in a large space virtual environment.  The device that emerged from this research consists of a large working volume haptic device mounted on a small mobile robot. The robot has mecanum wheels which allow it to move in any direction. The controls were developed such that the robot would follow the person around in the CAVE space and provide haptic feedback at any time the user "touched" a virtual object. The commercial haptic device consisted of a multi-link robotic arm that provided force in three-dimensions and torque along three axes. The user in the virtual environment held onto the end of the robotic arm and could walk around the area within the CAVE freely, with the mobil robot following the movement. At any time, the user can reach out, press a button on the end effector of the robotic arm and "grab" a virtual object. At that time, the weight of the object would be simulated and those weight forces would be applied to the user's hand. The user could freely manipulate the virtual object by moving the robotic end effector. If the virtual object collided with any other objects in the virtual environment, the motors of the robot would be activated and the user would feel the collision force through the end effector of the robot. The real advances made in this research are the integration of the mobile robot with the haptic robotic arm and the control system that resulted in the mobile robot moving freely around the CAVE, following the user. Throughout the period of this grant, several graduate and undergraduate students participated. One visiting student from France also worked on this project. The project successfully integrated disciplinary aspects from robotics, controls, haptics, virtual reality and virtual environments. The students also became proficient at giving presentations of their work through the numerous demonstrations they performed for K-12 students, university students, faculty and industry. Three papers were published describing this research and a YouTube video is available.       Last Modified: 04/08/2016       Submitted by: Judy M Vance]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
