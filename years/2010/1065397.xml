<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Learning Representations of Language for Domain Adaptation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>697982.00</AwardTotalIntnAmount>
<AwardAmount>705982</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Supervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts.  A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and for the inability to generalize to previously unseen words. &lt;br/&gt;&lt;br/&gt;This project is the first to systematically investigate representation-learning as a technique for improving performance on domain adaptation.  It explores latent-variable language models ? including Factorial Hidden Markov Models, dependency parsing models, and deep architectures ? as techniques for extracting novel features from text.  The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier.  The project also explores novel procedures for training a language model, which incorporate Web-scale ngram statistics as substitutes for standard statistics used in unsupervised training.&lt;br/&gt;&lt;br/&gt;Language users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology.  By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality.  For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts.  By involving the diverse student bodies at Temple University and Philadelphia-area high schools, the project helps to broaden participation in computer science research by underrepresented groups.</AbstractNarration>
<MinAmdLetterDate>03/02/2011</MinAmdLetterDate>
<MaxAmdLetterDate>02/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1065397</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Yates</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander P Yates</PI_FULL_NAME>
<EmailAddress>yates@temple.edu</EmailAddress>
<PI_PHON>2152048869</PI_PHON>
<NSF_ID>000509321</NSF_ID>
<StartDate>03/02/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yuhong</FirstName>
<LastName>Guo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yuhong Guo</PI_FULL_NAME>
<EmailAddress>yuhong@temple.edu</EmailAddress>
<PI_PHON>2152048455</PI_PHON>
<NSF_ID>000537682</NSF_ID>
<StartDate>03/02/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Temple University</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191226003</ZipCode>
<PhoneNumber>2157077547</PhoneNumber>
<StreetAddress>1801 N. Broad Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>057123192</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>057123192</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Temple University]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191226003</ZipCode>
<StreetAddress><![CDATA[1801 N. Broad Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~166447</FUND_OBLG>
<FUND_OBLG>2012~180265</FUND_OBLG>
<FUND_OBLG>2013~177130</FUND_OBLG>
<FUND_OBLG>2014~182140</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Supervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts. A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and the inability to generalize to previously unseen words.</p> <p>This project systematically investigated representation-learning as a technique for improving performance on when adapting NLP systems to new domains. It explored latent-variable models, including factorial hidden Markov models, partial lattice Markov random fields, distributed hidden Markov models, probabilistic language adaptation models, and deep neural networks as techniques for extracting novel features from text. The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier.&nbsp;</p> <p>In our publications, we demonstrated that the new representation-learning techniques developed in this project improved the state-of-the-art in fundamental NLP tasks, such as part-of-speech tagging, dependency parsing and information extraction, when training data for the target domain is limited or absent. &nbsp;In addition to scientific publications summarizing the results, we also released a number of code bases for our new representation-learning approaches.</p> <p>Language users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology. By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality. For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/16/2016<br>      Modified by: Yuhong&nbsp;Guo</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Supervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts. A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and the inability to generalize to previously unseen words.  This project systematically investigated representation-learning as a technique for improving performance on when adapting NLP systems to new domains. It explored latent-variable models, including factorial hidden Markov models, partial lattice Markov random fields, distributed hidden Markov models, probabilistic language adaptation models, and deep neural networks as techniques for extracting novel features from text. The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier.   In our publications, we demonstrated that the new representation-learning techniques developed in this project improved the state-of-the-art in fundamental NLP tasks, such as part-of-speech tagging, dependency parsing and information extraction, when training data for the target domain is limited or absent.  In addition to scientific publications summarizing the results, we also released a number of code bases for our new representation-learning approaches.  Language users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology. By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality. For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts.           Last Modified: 06/16/2016       Submitted by: Yuhong Guo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
