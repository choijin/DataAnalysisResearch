<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Integrating Perceptual and Linguistic Information in Models of Semantic Representation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/15/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>453674.00</AwardTotalIntnAmount>
<AwardAmount>453674</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Humans learn about the meanings of words and larger discourse units from repeated experience with both linguistic and perceptual information. However, current computational models of semantic learning and representation focus only on linguistic structure. This CAREER award explores how humans use multisensory perception and linguistic experience to organize semantic memory. Dr. Michael Jones at Indiana University will use specially designed web-based experiment protocols to collect large amounts of data about the perceptual structure of word referents. For example,  a participant  presented with a target word will be required to produce a list of verbal properties to describe the word (e.g., given DOG, a participant might produce "has four legs, has fur, barks" etc.). A second participant is then provided with only the list of properties and is required to guess what word is being described. Alternatively, the information to the second participant can be a drawing of what the target word represents to the first participant, who has configured predefined object components into a 2D or 3D display. By designing these experiments as internet based, Dr. Jones will collect very large amounts of data to design and test computer models of linguistic and perceptual integration during word learning. &lt;br/&gt;&lt;br/&gt;The models resulting from this research may provide a better understanding of the learning disorders characterized by language deficiencies (e.g. many forms of autism) and age-related disorders characterized by semantic disorganization (e.g., dementias such as Alzheimer's Disease). The integration of perceptual and linguistic information could also lead to better applied algorithms for information search (e.g., Internet search engines) if the computer representation can be made to approximate the semantic representation of the human doing the searching.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>04/08/2011</MinAmdLetterDate>
<MaxAmdLetterDate>04/23/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1056744</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Jones</LastName>
<PI_MID_INIT>N</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael N Jones</PI_FULL_NAME>
<EmailAddress>jonesmn@indiana.edu</EmailAddress>
<PI_PHON>8128561490</PI_PHON>
<NSF_ID>000508515</NSF_ID>
<StartDate>04/08/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474013654</ZipCode>
<StreetAddress><![CDATA[509 E 3RD ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~84973</FUND_OBLG>
<FUND_OBLG>2012~92994</FUND_OBLG>
<FUND_OBLG>2013~185791</FUND_OBLG>
<FUND_OBLG>2015~89916</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Humans learn the meanings of words and larger discourse units from repeated experience with both linguistic and perceptual information. When children begin to learn language, much of the information they need can be bootstrapped from previous perceptual experience&mdash;our languages are structured in such a way that we don&rsquo;t frequently say that the &ldquo;ball is round&rdquo; because that is redundant information with the visual environment. Machine learning systems (such as information search platforms) are based solely on linguistic information. This is thought to be the primary reason that humans can learn linguistic patterns with much less data than machines.</p> <p>A major limitation to the development of perceptually grounded semantic models is simply a lack of structured data coding the perceptual referents of words. Under this project, which came to be known as the &ldquo;NSF Semantic Pictionary Project,&rdquo; we developed a suite of online games that were designed to crowdsource human coding of perceptual information. The massive datasets compiled were then used to develop and constrain computational models of how humans learn the meanings of words from complimentary statistical cues when both &ldquo;hearing&rdquo; language and simultaneously &ldquo;seeing&rdquo; visual properties objects that naturally occur with the language being learned. We discovered that there is a great deal more shared information between language and perception than was initially believed, and that the cognitive mechanisms humans use to integrate these information sources during learning are likely based on composite brain representations.</p> <p>These systems can be extremely useful, both for theoretical understanding of learning, and as applied algorithms for sensor-based systems to integrate sensory information with linguistic information (e.g., any smartphone). But in addition to systems that see and hear, even simple internet search depends on perceptual integration. The human doing the search is basing his/her query on a lifetime of experience with perceptual and linguistic knowledge, but the machine system doing the information retrieval knows only how concepts are related based on statistical information in language. No matter how good the machine learning algorithm, it is missing a fundamental amount of embodied information that changes the entire memory space.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/08/2017<br>      Modified by: Michael&nbsp;N&nbsp;Jones</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Humans learn the meanings of words and larger discourse units from repeated experience with both linguistic and perceptual information. When children begin to learn language, much of the information they need can be bootstrapped from previous perceptual experience&mdash;our languages are structured in such a way that we don?t frequently say that the "ball is round" because that is redundant information with the visual environment. Machine learning systems (such as information search platforms) are based solely on linguistic information. This is thought to be the primary reason that humans can learn linguistic patterns with much less data than machines.  A major limitation to the development of perceptually grounded semantic models is simply a lack of structured data coding the perceptual referents of words. Under this project, which came to be known as the "NSF Semantic Pictionary Project," we developed a suite of online games that were designed to crowdsource human coding of perceptual information. The massive datasets compiled were then used to develop and constrain computational models of how humans learn the meanings of words from complimentary statistical cues when both "hearing" language and simultaneously "seeing" visual properties objects that naturally occur with the language being learned. We discovered that there is a great deal more shared information between language and perception than was initially believed, and that the cognitive mechanisms humans use to integrate these information sources during learning are likely based on composite brain representations.  These systems can be extremely useful, both for theoretical understanding of learning, and as applied algorithms for sensor-based systems to integrate sensory information with linguistic information (e.g., any smartphone). But in addition to systems that see and hear, even simple internet search depends on perceptual integration. The human doing the search is basing his/her query on a lifetime of experience with perceptual and linguistic knowledge, but the machine system doing the information retrieval knows only how concepts are related based on statistical information in language. No matter how good the machine learning algorithm, it is missing a fundamental amount of embodied information that changes the entire memory space.          Last Modified: 08/08/2017       Submitted by: Michael N Jones]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
