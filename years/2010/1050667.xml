<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Data Connections:  Developing a Coherent Picture of Mathematics Teaching and Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>1213475.00</AwardTotalIntnAmount>
<AwardAmount>1213475</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David B. Campbell</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The purpose of this study is to develop statistical models to create a coherent model of the effects of teacher professional development in mathematics on student learning.  The project works closely with two partnerships of the NSF Math and Science Partnership (MSP) program, the Math in the Middle Institute Partnership and NebraskaMATH, and with the Lincoln Public Schools. Utilizing data already collected through these MSP partnerships, the study builds newly developed models to help these and other MSP partnerships and their evaluators interpret student and teacher data in statistically productive and meaningful ways. &lt;br/&gt;&lt;br/&gt;Specifically, the project focuses on layered value-added models to estimate teacher effects, and how those models can be adapted for use with messy data, for example when different tests are administered in different grade levels at different times of year. The project is investigating the use of Z-scores, parallel processing, and binning by quantile to address issues arising with available student achievement data. The scope of the project research includes: 1) methods to estimate student achievement trajectories over time; 2) methods for best connecting these trajectories to measures of teaching quality; 3) connecting measures of student and teacher attitudes to each other and to measures of student achievement and teaching quality; and 4) estimating the impact of MSP interventions on all of the above.&lt;br/&gt;&lt;br/&gt;MSP projects, school districts and States are grappling with how to evaluate the effects of their professional development programs for teachers on student learning. Few studies have addressed how to use value-added models to analyze achievement data that are not on a single developmental scale, and even fewer have discussed how to use information from multiple instruments in a single year that are on different scales. In addition to this work, the research findings will be disseminated widely, through presentations at national conferences, articles published in peer-reviewed journals, and a dissemination conference targeting MSP project and evaluation personnel. Utilizing value-added models increases the potential to provide evidence of high quality teacher impact in high-need schools. By helping MSP partnerships use the methods developed in this study, the project is building build their capacity to inform the nation of how their MSPs impact teaching and learning.</AbstractNarration>
<MinAmdLetterDate>06/28/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/28/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1050667</AwardID>
<Investigator>
<FirstName>Walter</FirstName>
<LastName>Stroup</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Walter Stroup</PI_FULL_NAME>
<EmailAddress>wstroup1@unl.edu</EmailAddress>
<PI_PHON>4024728290</PI_PHON>
<NSF_ID>000345229</NSF_ID>
<StartDate>06/28/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Wendy</FirstName>
<LastName>Smith</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wendy M Smith</PI_FULL_NAME>
<EmailAddress>wsmith5@unl.edu</EmailAddress>
<PI_PHON>4024727259</PI_PHON>
<NSF_ID>000531813</NSF_ID>
<StartDate>06/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Leslie</FirstName>
<LastName>Lukin</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leslie E Lukin</PI_FULL_NAME>
<EmailAddress>llukin@lps.org</EmailAddress>
<PI_PHON>4024361790</PI_PHON>
<NSF_ID>000568655</NSF_ID>
<StartDate>06/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Green</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer L Green</PI_FULL_NAME>
<EmailAddress>jg@msu.edu</EmailAddress>
<PI_PHON>5173559589</PI_PHON>
<NSF_ID>000568752</NSF_ID>
<StartDate>06/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName>Lincoln</CityName>
<StateCode>NE</StateCode>
<ZipCode>685031435</ZipCode>
<StreetAddress><![CDATA[151 Prem S. Paul Research Center]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1793</Code>
<Text>MSP-OTHER AWARDS</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0411</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~1213475</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;The primary motivation for the Data Connections Research, Evaluation and Technical Assistance (RETA) grant arose from the fact that federal agencies, notably the NSF, are making significant investments in improving the quality of STEM education. There is a need for statistical tools to characterize the impact of professional development on teacher effectiveness and student learning.</p> <p>This project focused on statistical modeling to characterize program impact, and emphasized two types of models: &nbsp;value-added models (VAMs), and hierarchical linear models (HLMs).&nbsp;</p> <p><strong><span style="text-decoration: underline;">VAMs</span></strong></p> <p>Our efforts resulted in findings along two themes:&nbsp;</p> <ol> <li>Characteristics&nbsp;data should ideally possess, and </li> <li>How to define program effects in a value-added context.</li> </ol> <p><strong>Item 1</strong>.&nbsp;With regard to ideal data characteristics for use with VAMs to estimate program impact, we intensively investigated three areas of particular importance in using VAMs to estimate program impact. These are also areas of controversy that have been prominent in national debate about VAMs in conjunction with student achievement tests. At the time the grant began, it was clear from the debate and from efforts to estimate program impact, that there was a need for more accurate information. The areas and Data Connections research findings are as follows.</p> <ul> <li><span style="text-decoration: underline;">Randomization:</span>&nbsp;Critics of VAMs claim that in order to provide accurate estimates, these models require students to be randomly assigned to teachers, whereas this never happens &ndash; for very good reasons &ndash; in practice. Our findings suggest that lack of randomization, when it occurs in ways that schools actually assign students to teachers, does not distort the accuracy of estimated program impact on student learning.</li> <li><span style="text-decoration: underline;">Ceiling&nbsp;and floor effects:</span> Even moderate ceiling effects have a catastrophic impact on VAM results, at best limiting our ability to characterize program impact; at worst, distorting estimates of program effect and leading to erroneous conclusions. While randomization is a non-issue in the absence of ceiling effects, non-randomization does exacerbate the impact of ceiling effects. Our findings suggest that that outcomes deemed essential to measuring student learning must be capable of adequately scoring all levels of achievement.</li> <li><span style="text-decoration: underline;">Relationship between the slope and the intercept:</span> In lay terms, relationship between students&rsquo; initial level (intercept) and subsequent rate of learning (slope). Failure to account for this relationship has a&nbsp;minimal impact on the bias of the&nbsp;estimates but a substantial impact on precision. The latter can severely distort estimates of program effects.&nbsp;</li> </ul> <p>In addition to research on randomization, ceiling and floor effects, and intercept-slope relationship, Data Connections researchers determined additional characteristics of VAMs. Specifically, VAMs are only as good as the data used in these models. It is important to choose student outcomes that reflect learning objectives and create data systems that provide valid and reliable information about these outcomes.&nbsp;In addition, these outcomes should be tracked over a sustained period of time. Where a student begins (the student&rsquo;s baseline) is critical: without accurate and reliable estimates of students&rsquo; baseline, VAM performance is severely compromised. This means that a great deal of planning and coordination must occur from the inception of professional development programs with regard to data collection and study design.</p> <p><strong>Item 2</strong>. With regard to how one defines program effect, foc...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The primary motivation for the Data Connections Research, Evaluation and Technical Assistance (RETA) grant arose from the fact that federal agencies, notably the NSF, are making significant investments in improving the quality of STEM education. There is a need for statistical tools to characterize the impact of professional development on teacher effectiveness and student learning.  This project focused on statistical modeling to characterize program impact, and emphasized two types of models:  value-added models (VAMs), and hierarchical linear models (HLMs).   VAMs  Our efforts resulted in findings along two themes:   Characteristics data should ideally possess, and  How to define program effects in a value-added context.   Item 1. With regard to ideal data characteristics for use with VAMs to estimate program impact, we intensively investigated three areas of particular importance in using VAMs to estimate program impact. These are also areas of controversy that have been prominent in national debate about VAMs in conjunction with student achievement tests. At the time the grant began, it was clear from the debate and from efforts to estimate program impact, that there was a need for more accurate information. The areas and Data Connections research findings are as follows.  Randomization: Critics of VAMs claim that in order to provide accurate estimates, these models require students to be randomly assigned to teachers, whereas this never happens &ndash; for very good reasons &ndash; in practice. Our findings suggest that lack of randomization, when it occurs in ways that schools actually assign students to teachers, does not distort the accuracy of estimated program impact on student learning. Ceiling and floor effects: Even moderate ceiling effects have a catastrophic impact on VAM results, at best limiting our ability to characterize program impact; at worst, distorting estimates of program effect and leading to erroneous conclusions. While randomization is a non-issue in the absence of ceiling effects, non-randomization does exacerbate the impact of ceiling effects. Our findings suggest that that outcomes deemed essential to measuring student learning must be capable of adequately scoring all levels of achievement. Relationship between the slope and the intercept: In lay terms, relationship between studentsÆ initial level (intercept) and subsequent rate of learning (slope). Failure to account for this relationship has a minimal impact on the bias of the estimates but a substantial impact on precision. The latter can severely distort estimates of program effects.    In addition to research on randomization, ceiling and floor effects, and intercept-slope relationship, Data Connections researchers determined additional characteristics of VAMs. Specifically, VAMs are only as good as the data used in these models. It is important to choose student outcomes that reflect learning objectives and create data systems that provide valid and reliable information about these outcomes. In addition, these outcomes should be tracked over a sustained period of time. Where a student begins (the studentÆs baseline) is critical: without accurate and reliable estimates of studentsÆ baseline, VAM performance is severely compromised. This means that a great deal of planning and coordination must occur from the inception of professional development programs with regard to data collection and study design.  Item 2. With regard to how one defines program effect, focus should not be exclusively on mean change over teachers. This is important. There is not a single program effect:  programs affect teachers differently, and what we really need to estimate is not a single value, but a distribution of program effects over teachers.  HLMs   While Data Connections emphasized VAMs, they were not our exclusive focus. No statistical model can be regarded as a one-size-fits-all tool. VAMs are not always the most appropriate model for determining program...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
