<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: The Exploration of Memory Hierarchy Design and Optimization for Multi-core Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>190603.00</AwardTotalIntnAmount>
<AwardAmount>190603</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recently, there has been a growing disparity between processor and memory speeds; this disparity is called the ?memory wall? problem. Memory performance has a dominating effect on overall system performance, especially for the data-dominated applications. Also, memory usually occupies half of the surface of the integrated chip of the multi-core system; the total amount of memory required is also a main contributor of the manufacturing cost and the chip size of the multi-core CPU. Memory also dictates the power consumption, since memories and buses consume large quantities of energy. The ?memory wall? problem becomes even more serious when throughput in the processor part is propelled by multi-core architectures.&lt;br/&gt;&lt;br/&gt;In this proposal, the PI is carrying out research to address the ?memory wall? problem and to develop potentially transformative approaches for memory hierarchy design and configuration in multi-core systems. Different memory architectures can lead to different solutions with different costs and with different performances. Memory size, memory configuration, and memory architecture will be optimized simultaneously for multi-core architectures in order to effectively and efficiently achieve higher performance.</AbstractNarration>
<MinAmdLetterDate>08/26/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1055290</AwardID>
<Investigator>
<FirstName>Meilin</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Meilin Liu</PI_FULL_NAME>
<EmailAddress>meilin.liu@wright.edu</EmailAddress>
<PI_PHON>9377752425</PI_PHON>
<NSF_ID>000502977</NSF_ID>
<StartDate>08/26/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Wright State University</Name>
<CityName>Dayton</CityName>
<ZipCode>454350001</ZipCode>
<PhoneNumber>9377752425</PhoneNumber>
<StreetAddress>3640 Colonel Glenn Highway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047814256</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WRIGHT STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>047814256</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Wright State University]]></Name>
<CityName>Dayton</CityName>
<StateCode>OH</StateCode>
<ZipCode>454350001</ZipCode>
<StreetAddress><![CDATA[3640 Colonel Glenn Highway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~190603</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>1.&nbsp; Proposed a new thread scheduling technique that can take advantage of data locality both among nearby warps and in the same warp to optimize the scientific applications on the GP-GPU systems. By analyzing the memory access patterns of the scientific applications, we try to find the regularities in the data access pattern of the scientific applications.&nbsp; Then we propose a new thread scheduling technique that can take advantage of data locality both among nearby warps and in the same warp to optimize the scientific applications on the GP-GPU systems. The simulation results validated our proposed thread scheduling technique and showed that significant improvements have been achieved.<br /><br />2.&nbsp; Proposed a dynamic warp scheduling algorithm considering both inter-warp and intra-warp locality to increase the global memory access throughput and L1 cache efficiency.<br /><br />3. Proposed and investigated a scheme to compute the memory cost of a basic block and a loop nest based on the data dependency distance. Use the memory cost model as the guidance for the scheduling techniques.<br /><br />4. Developed new memory allocation algorithms using&nbsp; the memory cost model. <br /><br />5.&nbsp; Developed new cache partitioning algorithm for specific hardware architectures, including multicore systems, and embedded systems.<br /><br />6. Developed a new graduate level course, CS4370/6370, "Parallel Programming for manycore GPUs". The graduate studens and undergraduate student who took the class will learn about new many-core GPU architecture, CUDA programming model, memory hierarchy design, parallel programming concepts, and compiling techniques to improve parallelism.&nbsp; Students who took the class will also learn how to develop scalable parallel programs to achieve high performance on GPUs.<br /><br />7. Teaching proposal &ldquo; Building a CUDA Teaching Center Laboratory at Wright State University,&rdquo; is currently awarded by Nvidia with matching funds ($9000) and equipment donation ($3940.18).</p><br> <p>            Last Modified: 08/31/2013<br>      Modified by: Meilin&nbsp;Liu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ 1.  Proposed a new thread scheduling technique that can take advantage of data locality both among nearby warps and in the same warp to optimize the scientific applications on the GP-GPU systems. By analyzing the memory access patterns of the scientific applications, we try to find the regularities in the data access pattern of the scientific applications.  Then we propose a new thread scheduling technique that can take advantage of data locality both among nearby warps and in the same warp to optimize the scientific applications on the GP-GPU systems. The simulation results validated our proposed thread scheduling technique and showed that significant improvements have been achieved.  2.  Proposed a dynamic warp scheduling algorithm considering both inter-warp and intra-warp locality to increase the global memory access throughput and L1 cache efficiency.  3. Proposed and investigated a scheme to compute the memory cost of a basic block and a loop nest based on the data dependency distance. Use the memory cost model as the guidance for the scheduling techniques.  4. Developed new memory allocation algorithms using  the memory cost model.   5.  Developed new cache partitioning algorithm for specific hardware architectures, including multicore systems, and embedded systems.  6. Developed a new graduate level course, CS4370/6370, "Parallel Programming for manycore GPUs". The graduate studens and undergraduate student who took the class will learn about new many-core GPU architecture, CUDA programming model, memory hierarchy design, parallel programming concepts, and compiling techniques to improve parallelism.  Students who took the class will also learn how to develop scalable parallel programs to achieve high performance on GPUs.  7. Teaching proposal " Building a CUDA Teaching Center Laboratory at Wright State University," is currently awarded by Nvidia with matching funds ($9000) and equipment donation ($3940.18).       Last Modified: 08/31/2013       Submitted by: Meilin Liu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
