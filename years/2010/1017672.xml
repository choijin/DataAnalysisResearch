<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Towards Portable Navigational Devices for the Visually Impaired</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>320389.00</AwardTotalIntnAmount>
<AwardAmount>320389</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gregory Chirikjian</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The objective of this project is to devise computer vision methods that enable a Portable Blind Navigational Device (PBND) to guide a visually impaired person in unstructured environments. The main research question of this project is to answer if the approach of employing a single perception sensor can solve blind navigation problem, including localization of the PBND and object recognition. A distinctive feature of this work is that it addresses blind navigation problem by simultaneously processing the visual and range information of a 3D imaging sensor. &lt;br/&gt;&lt;br/&gt;The project consists of four related research endeavors. First, it investigates techniques for accurate and precise pose estimation of the PBND in a GPS-denied environment.  Second, it develops an effective 3D data segmentation method to allow scene recognition for wayfinding. Third, it applies the pose estimation method to register 3D range data and devises methods to reduce registration error. Four, it addresses real-time implementation of the methods in the PBND with limited computing power.    &lt;br/&gt;&lt;br/&gt;The research will result in new algorithms that can improve the lives of the visually impaired in the near term. They will also enable the autonomy of small robots that have wider applications in military situational awareness, firefighting, and search and rescue. The discoveries will revolutionize small robot autonomy and impact the robotics research community as a whole. Broader impacts also include training of undergraduate and graduate students, and educating the public on robotics through workshops and robot exhibits in science museums and technology showcases.</AbstractNarration>
<MinAmdLetterDate>08/08/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1017672</AwardID>
<Investigator>
<FirstName>Cang</FirstName>
<LastName>Ye</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cang Ye</PI_FULL_NAME>
<EmailAddress>cye@vcu.edu</EmailAddress>
<PI_PHON>8048280346</PI_PHON>
<NSF_ID>000318516</NSF_ID>
<StartDate>08/08/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arkansas Little Rock</Name>
<CityName>Little Rock</CityName>
<ZipCode>722041000</ZipCode>
<PhoneNumber>5015698474</PhoneNumber>
<StreetAddress>2801 South University</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arkansas</StateName>
<StateCode>AR</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AR02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036725083</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARKANSAS SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>055600001</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arkansas Little Rock]]></Name>
<CityName>Little Rock</CityName>
<StateCode>AR</StateCode>
<ZipCode>722041000</ZipCode>
<StreetAddress><![CDATA[2801 South University]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arkansas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AR02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~320389</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project outcomes are the development of new computer vision methods that can be used by a Portable Navigation Aid (PNA) to navigate a blind traveler in indoor environments. The methods include: (1) An new egomotion estimation method, called Visual Range Odometry (VRO), that can accurately determine the PNA&rsquo;s pose (position and orientation) change between two time steps by simultaneously processing the intensity and range data from a 3D camera&frac34;SwissRanger SR4000; (2) A method that integrates the pose changes estimated by the VRO over time into the PNA&rsquo;s pose in the world coordinate system and reduce the integration error; (3) A 3D data segmentation method that can reliably segment the camera&rsquo;s range data into planar segments; and (4) An object recognition method to group the extracted planar segments into structures (e.g., stairways, doorways, etc.) and detect the structures as the waypoints for navigation. A PNA prototype, called smart cane, was built to test the functions of the pose estimation and object detection methods. The smart cane uses a speech interface for human-device interaction, i.e., for the blind traveler to indicate the destination or the object to be located and for the smart cane to notify the traveler the desired motion and the location of the detected object. Seven sighted human subjects were recruited to test the smart cane. They were blind-folded when performing the experiments. In the first test, the smart cane used the pose estimation result and the floor plan of the EIT/ETAS building to locate the subject in the building and guided the subject to the destination using a series of audio commands. The destination was a room number specified by the subject using the speech interface. The experimental results show that the smart cane successfully guided the human subject to the doorway of the room in each of the test cases. In the second test, a human subject walked in the ETAS building with the smart cane. The cane spoke to the subjects about the objects (doorway, stairway, hallway, and flat ground) it detected along the way. The experimental results demonstrate a 90% success rate in object detection. It is noted that a single rage data frame was used for object detection in this test. The success rate can be improved if multiple frames of range data are registered and used for object detection.</p> <p>The methods developed through this project have resulted in near term applications that may positively impact the visually impaired community. They also have potential in improving the autonomy of small robots that have a myriad of applications in military surveillance, law enforcement, and search and rescue, and therefore have broader scientific impacts to the community of robotics research. Other broader impacts of the project include: (i) improvement of the research infrastructure of the PI&rsquo;s university, (ii) training of graduate and undergraduate students for their future careers in science and engineering disciplines, and (iii) broaden the participation of underrepresented groups in science and engineering. The project has involved&nbsp;five graduate students (two female students) and provided them rigorous scientific training in robotics, real-time operating systems, computer vision, and object oriented software design. Two Ph.D. students have graduated and were hired by US companies as research engineers. The research has produced a course project for the Real-time Embedded Systems class. Through the course project, students outside the research project also built their skills in real-time embedded software design. The PI also used the computer vision methods in his Mobile Robotics class. The PI actively participated in the EIT college&rsquo;s Summer Undergraduate Program for Research &amp; Entrepreneurship (SUPER) and Summer Research Academy (SRA). During the project period, he has hosted two undergrad...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project outcomes are the development of new computer vision methods that can be used by a Portable Navigation Aid (PNA) to navigate a blind traveler in indoor environments. The methods include: (1) An new egomotion estimation method, called Visual Range Odometry (VRO), that can accurately determine the PNAÆs pose (position and orientation) change between two time steps by simultaneously processing the intensity and range data from a 3D camera&frac34;SwissRanger SR4000; (2) A method that integrates the pose changes estimated by the VRO over time into the PNAÆs pose in the world coordinate system and reduce the integration error; (3) A 3D data segmentation method that can reliably segment the cameraÆs range data into planar segments; and (4) An object recognition method to group the extracted planar segments into structures (e.g., stairways, doorways, etc.) and detect the structures as the waypoints for navigation. A PNA prototype, called smart cane, was built to test the functions of the pose estimation and object detection methods. The smart cane uses a speech interface for human-device interaction, i.e., for the blind traveler to indicate the destination or the object to be located and for the smart cane to notify the traveler the desired motion and the location of the detected object. Seven sighted human subjects were recruited to test the smart cane. They were blind-folded when performing the experiments. In the first test, the smart cane used the pose estimation result and the floor plan of the EIT/ETAS building to locate the subject in the building and guided the subject to the destination using a series of audio commands. The destination was a room number specified by the subject using the speech interface. The experimental results show that the smart cane successfully guided the human subject to the doorway of the room in each of the test cases. In the second test, a human subject walked in the ETAS building with the smart cane. The cane spoke to the subjects about the objects (doorway, stairway, hallway, and flat ground) it detected along the way. The experimental results demonstrate a 90% success rate in object detection. It is noted that a single rage data frame was used for object detection in this test. The success rate can be improved if multiple frames of range data are registered and used for object detection.  The methods developed through this project have resulted in near term applications that may positively impact the visually impaired community. They also have potential in improving the autonomy of small robots that have a myriad of applications in military surveillance, law enforcement, and search and rescue, and therefore have broader scientific impacts to the community of robotics research. Other broader impacts of the project include: (i) improvement of the research infrastructure of the PIÆs university, (ii) training of graduate and undergraduate students for their future careers in science and engineering disciplines, and (iii) broaden the participation of underrepresented groups in science and engineering. The project has involved five graduate students (two female students) and provided them rigorous scientific training in robotics, real-time operating systems, computer vision, and object oriented software design. Two Ph.D. students have graduated and were hired by US companies as research engineers. The research has produced a course project for the Real-time Embedded Systems class. Through the course project, students outside the research project also built their skills in real-time embedded software design. The PI also used the computer vision methods in his Mobile Robotics class. The PI actively participated in the EIT collegeÆs Summer Undergraduate Program for Research &amp; Entrepreneurship (SUPER) and Summer Research Academy (SRA). During the project period, he has hosted two undergraduate students for their SUPER projects and five high school students for their SRA projects. The PI als...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
