<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Automated Analysis of Constructed Response Concept Inventories to Reveal Student Thinking: Forging a National Network for Innovative Assessment Methods</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>382601.00</AwardTotalIntnAmount>
<AwardAmount>458575</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Myles Boylan</SignBlockName>
<PO_EMAI>mboylan@nsf.gov</PO_EMAI>
<PO_PHON>7032928670</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Numerous reports on the effectiveness of U.S. higher education in the Science, Technology, Engineering and Mathematics (STEM) disciplines call for increased emphasis on conceptual learning, rather than rote memorization. Suitable assessments (tests) of conceptual learning (often referred to as concept inventories or diagnostic question clusters), however, are few and are constrained by the ability to score the outcomes of the tests in a cost-effective manner. Multiple-choice assessments (selected responses) are more widespread in higher education, especially at medium to large institutions where class sizes are large, and where automated scoring provides the essential cost-effectiveness. Conversely, written response assessments (constructed responses), which are widely held to be superior at revealing actual student thinking, are quite rare in practice given the time and effort required for manual scoring. This project leverages the latest computerized tools and statistical techniques to make constructed response assessments more broadly available. Computer automation allows the use of these more insightful conceptual questions and tests with much larger numbers of students, thereby providing an enhanced understanding of students' conceptual learning. Project personnel work with developers of conceptual testing instruments to create constructed response versions of the tests coupled with the necessary computerized scoring tools with the eventual goal of providing computer-automated evaluation of conceptual thinking. The project is a collaboration among three major public universities.</AbstractNarration>
<MinAmdLetterDate>09/06/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/11/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1022653</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Merrill</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John E Merrill</PI_FULL_NAME>
<EmailAddress>merrill3@msu.edu</EmailAddress>
<PI_PHON>5174321316</PI_PHON>
<NSF_ID>000433249</NSF_ID>
<StartDate>09/06/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Urban-Lurain</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Urban-Lurain</PI_FULL_NAME>
<EmailAddress>urban@msu.edu</EmailAddress>
<PI_PHON>5174322108</PI_PHON>
<NSF_ID>000320484</NSF_ID>
<StartDate>09/06/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Julie</FirstName>
<LastName>Libarkin</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Julie C Libarkin</PI_FULL_NAME>
<EmailAddress>libarkin@msu.edu</EmailAddress>
<PI_PHON>5173558369</PI_PHON>
<NSF_ID>000476945</NSF_ID>
<StartDate>09/06/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tammy</FirstName>
<LastName>Long</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tammy M Long</PI_FULL_NAME>
<EmailAddress>longta@msu.edu</EmailAddress>
<PI_PHON>5174322706</PI_PHON>
<NSF_ID>000327982</NSF_ID>
<StartDate>09/06/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488242600</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>7492</Code>
<Text>CCLI-Type 2 (Expansion)</Text>
</ProgramElement>
<ProgramElement>
<Code>7511</Code>
<Text>TUES-Type 2 Project</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0410</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0411</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~382601</FUND_OBLG>
<FUND_OBLG>2011~75974</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Critical to determining what and how students are learning about science are the assessments that faculty use.&nbsp; &ldquo;What is tested becomes what is important.&rdquo;&nbsp;&nbsp; Often, large, introductory science courses concentrate on disconnected scientific &ldquo;facts&rdquo; that students then memorize in order to pass multiple-choice tests.&nbsp; Yet, science is about constructing explanations.&nbsp; Scientists do not take multiple choice tests.&nbsp; Rather, they write and propose models and theories to explain the natural world.&nbsp;</p> <p>The Automated Analysis of Constructed Response (AACR) research group explores computerized techniques for analyzing student writing in these large introductory&nbsp; courses so that faculty can ask students to engage in the more authentic task of writing and constructing explanations, rather than relying on multiple choice tests.&nbsp; While computers remain unable to &ldquo;understand&rdquo; free-form writing, they do have the ability to rapidly and accurately identify words and phrases in text. Computers exceed the potential of human evaluators in terms of their ability to rapidly process large volumes of text-based responses into their essential terms, phrases, and concepts quickly, accurately and consistently.</p> <p>The AACR group developed questions that address challenging concepts in biology and biochemistry and the computer techniques to analyze responses and predict how expert humans would score the responses.&nbsp; The agreement between computer predictions and expert human scorers is as good as or better than the agreement between expert human scorers. &nbsp;&nbsp;</p> <p>The<strong> </strong>AACR group has created, tested and implemented 120 AACR questions about evolution, biomolecules, genetics, metabolism, and thermodynamics. &nbsp;The project has produced 9 journal articles, 43 conference papers, 5 invited talks, 17 posters, 21 YouTube videos, and a project website (<a href="http://www.msu.edu/~aacr">www.msu.edu/~aacr</a>).</p> <p>With additional NSF funding (1323162, 1347740) the AACR team is expanding the national network to include 6 collaborating universities.&nbsp; Faculty at those institutions are using the AACR questions in classes and receiving reports that identify students&rsquo; scientific ideas and misconceptions about crucial, foundational concepts in biology.&nbsp; The faculty are developing instruction to address student learning challenges revealed by those questions.&nbsp;</p><br> <p>            Last Modified: 11/04/2014<br>      Modified by: Mark&nbsp;Urban-Lurain</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Critical to determining what and how students are learning about science are the assessments that faculty use.  "What is tested becomes what is important."   Often, large, introductory science courses concentrate on disconnected scientific "facts" that students then memorize in order to pass multiple-choice tests.  Yet, science is about constructing explanations.  Scientists do not take multiple choice tests.  Rather, they write and propose models and theories to explain the natural world.   The Automated Analysis of Constructed Response (AACR) research group explores computerized techniques for analyzing student writing in these large introductory  courses so that faculty can ask students to engage in the more authentic task of writing and constructing explanations, rather than relying on multiple choice tests.  While computers remain unable to "understand" free-form writing, they do have the ability to rapidly and accurately identify words and phrases in text. Computers exceed the potential of human evaluators in terms of their ability to rapidly process large volumes of text-based responses into their essential terms, phrases, and concepts quickly, accurately and consistently.  The AACR group developed questions that address challenging concepts in biology and biochemistry and the computer techniques to analyze responses and predict how expert humans would score the responses.  The agreement between computer predictions and expert human scorers is as good as or better than the agreement between expert human scorers.     The AACR group has created, tested and implemented 120 AACR questions about evolution, biomolecules, genetics, metabolism, and thermodynamics.  The project has produced 9 journal articles, 43 conference papers, 5 invited talks, 17 posters, 21 YouTube videos, and a project website (www.msu.edu/~aacr).  With additional NSF funding (1323162, 1347740) the AACR team is expanding the national network to include 6 collaborating universities.  Faculty at those institutions are using the AACR questions in classes and receiving reports that identify studentsÆ scientific ideas and misconceptions about crucial, foundational concepts in biology.  The faculty are developing instruction to address student learning challenges revealed by those questions.        Last Modified: 11/04/2014       Submitted by: Mark Urban-Lurain]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
