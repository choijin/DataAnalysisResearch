<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>G&amp;V: Medium: Collaborative Research: Contact-Based Human Motion Acquisition and Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>318570.00</AwardTotalIntnAmount>
<AwardAmount>318570</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>To date, motion capture technologies suffer from three major limitations. First, the hardware devices are restrictive, cumbersome, and expensive. Second, most techniques only record the kinematic information of the movement, rather than underlying dynamic properties or control mechanisms. Third, the current technique fails to capture the interaction between the subject and the environment. Without the information of contacts, reconstructing motion that consists of complex contact phenomena is nearly impossible. This project develops a new motion acquisition and reconstruction technique that solves all three problems aforementioned. The new technique combines the force sensors and a single video camera to reconstruct full-body poses, joint torques, and contact forces in an unconstrained setting. In contrast to expensive lab equipment, the proposed system consists of a pair of low-cost, non-intrusive force-sensing shoes and a single consumer-level video camera that can be used to acquire motions difficult to capture in the lab. This acquisition technology enables new design of motion controllers by leveraging a large amount of real-world contact data. The research also develops new data representations and novel algorithms for intelligent and efficient motion planning and evaluates the developed motion controllers by simulating a human figure performing challenging balanced activities in a novel and unpredicted environment. &lt;br/&gt;&lt;br/&gt;The project is tightly integrated with education components in both Georgia Tech and Texas A&amp;M. The research of this project lends itself well to solve important real-world problems for computer graphics. The results from this project would impact research in video gaming, sports training, remote health care, biped robots, and virtual characters, etc.</AbstractNarration>
<MinAmdLetterDate>03/25/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/28/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1065384</AwardID>
<Investigator>
<FirstName>Jinxiang</FirstName>
<LastName>Chai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jinxiang Chai</PI_FULL_NAME>
<EmailAddress>jchai@cs.tamu.edu</EmailAddress>
<PI_PHON>9798453510</PI_PHON>
<NSF_ID>000081361</NSF_ID>
<StartDate>03/25/2011</StartDate>
<EndDate>03/28/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dilma</FirstName>
<LastName>Da Silva</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dilma Da Silva</PI_FULL_NAME>
<EmailAddress>dilma@tamu.edu</EmailAddress>
<PI_PHON>9794587617</PI_PHON>
<NSF_ID>000695299</NSF_ID>
<StartDate>03/28/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M Engineering Experiment Station</Name>
<CityName>College Station</CityName>
<ZipCode>778454645</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy S</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>847205572</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A&amp;M ENGINEERING EXPERIMENT STATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&amp;M Engineering Experiment Station]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778454645</ZipCode>
<StreetAddress><![CDATA[400 Harvey Mitchell Pkwy S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~148908</FUND_OBLG>
<FUND_OBLG>2012~96516</FUND_OBLG>
<FUND_OBLG>2013~73146</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To date, motion capture technologies suffer from three major limitations. First, the hardware devices are restrictive, cumbersome, and expensive. Second, most techniques only record the kinematic information of the movement, rather than underlying dynamic properties or control mechanisms. Third, the current technique fails to capture the interaction between the subject and the environment. Without the information of contacts, reconstructing motion that consists of complex contact phenomena is nearly impossible. The main goal of this project has been to investigate a new motion acquisition and reconstruction technique that solves all three problems aforementioned. Another goal of this project is to develop new data representations and novel algorithms for intelligent and efficient motion generation. The main outcomes of this project were published in six SIGGRAPH/TOG papers and are summarized below.&nbsp;</p> <p>1). We presented a new method for full-body motion capture that uses input data captured by three depth cameras and a pair of pressure sensing shoes. Our system is low-cost, non-intrusive and fully automatic, and can accurately reconstruct both full-body kinematics and dynamics data. We first introduced a novel tracking process that automatically reconstructs 3D skeletal poses using input data captured by three Kinect cameras and wearable pressure sensors. We formulated the problem in an optimization framework and incrementally update 3D skeletal poses with observed depth data and pressure data via iterative linear solvers. The system is highly accurate because we integrate depth data from multiple depth cameras, foot pressure data, detailed full-body geometry, and environmental contact constraints into a unified framework. In addition, we developed an efficient physics-based motion reconstruction algorithm for solving internal joint torques and contact forces in the quadratic programming framework. During reconstruction, we leveraged Newtonian physics, friction cone constraints, contact pressure information, and 3D kinematic poses obtained from the kinematic tracking process to reconstruct full-body dynamics data. We demonstrated the power of our approach by capturing a wide range of human movements and achieve state-of-heart accuracy in our comparison against alternative systems.</p> <p>2).&nbsp;We presented a new method for acquiring physically realistic hand manipulation data from multiple video streams. The key idea of our approach is to introduce a composite motion control to simultaneously model hand articulation, object movement, and subtle interaction between the hand and object.&nbsp;We demonstrated the effectiveness of our approach by capturing a wide range of high-fidelity dexterous manipulation data. We showed the power of our recovered motion controllers by adapting the captured motion data to new objects with different properties.</p> <p>3).&nbsp;We presented a fast, automatic method for accurately capturing full body motion data using a single depth camera. At the core of our system lies a real-time registration process that accurately reconstructs 3D human poses from single monocular depth images, even in the case of significant occlusions.&nbsp;We demonstrated the power of our approach by capturing a wide range of human movements in real time and achieve state-of-the-art accuracy in our comparison against alternative systems such as Kinect.</p> <p>4). We showed how statistical motion priors can be combined seamlessly with physical constraints for human motion modeling and generation. The key idea of the approach is to learn a nonlinear probabilistic force field function from prerecorded motion data with Gaussian processes and combine it with physical constraints in a probabilistic framework. Also, we showed how to effectively utilize the new model to generate a wide range of natural looking motions that achieve the goals specified by the users.&nbsp;</p> <p>5). We introduced the first data-driven technique for solving full-body inverse dynamics problem. Comparisons against existing methods show our system achieves the state-of-the-art accuracy in full-body inverse dynamics. We also showed how to use our new data-driven inverse dynamics algorithm for improving the efficiency of human motion editing and human motion control.</p> <p>6). We introduced a robust physics-based motion control system for real-time synthesis of human grasping. Our solution leverages prerecorded motion data and physics-based simulation for human grasping. We demonstrated the power of our approach by generating physics-based motion control for grasping objects with different properties such as shapes, weights, spatial orientations, and frictions.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/30/2017<br>      Modified by: Dilma&nbsp;Da Silva</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To date, motion capture technologies suffer from three major limitations. First, the hardware devices are restrictive, cumbersome, and expensive. Second, most techniques only record the kinematic information of the movement, rather than underlying dynamic properties or control mechanisms. Third, the current technique fails to capture the interaction between the subject and the environment. Without the information of contacts, reconstructing motion that consists of complex contact phenomena is nearly impossible. The main goal of this project has been to investigate a new motion acquisition and reconstruction technique that solves all three problems aforementioned. Another goal of this project is to develop new data representations and novel algorithms for intelligent and efficient motion generation. The main outcomes of this project were published in six SIGGRAPH/TOG papers and are summarized below.   1). We presented a new method for full-body motion capture that uses input data captured by three depth cameras and a pair of pressure sensing shoes. Our system is low-cost, non-intrusive and fully automatic, and can accurately reconstruct both full-body kinematics and dynamics data. We first introduced a novel tracking process that automatically reconstructs 3D skeletal poses using input data captured by three Kinect cameras and wearable pressure sensors. We formulated the problem in an optimization framework and incrementally update 3D skeletal poses with observed depth data and pressure data via iterative linear solvers. The system is highly accurate because we integrate depth data from multiple depth cameras, foot pressure data, detailed full-body geometry, and environmental contact constraints into a unified framework. In addition, we developed an efficient physics-based motion reconstruction algorithm for solving internal joint torques and contact forces in the quadratic programming framework. During reconstruction, we leveraged Newtonian physics, friction cone constraints, contact pressure information, and 3D kinematic poses obtained from the kinematic tracking process to reconstruct full-body dynamics data. We demonstrated the power of our approach by capturing a wide range of human movements and achieve state-of-heart accuracy in our comparison against alternative systems.  2). We presented a new method for acquiring physically realistic hand manipulation data from multiple video streams. The key idea of our approach is to introduce a composite motion control to simultaneously model hand articulation, object movement, and subtle interaction between the hand and object. We demonstrated the effectiveness of our approach by capturing a wide range of high-fidelity dexterous manipulation data. We showed the power of our recovered motion controllers by adapting the captured motion data to new objects with different properties.  3). We presented a fast, automatic method for accurately capturing full body motion data using a single depth camera. At the core of our system lies a real-time registration process that accurately reconstructs 3D human poses from single monocular depth images, even in the case of significant occlusions. We demonstrated the power of our approach by capturing a wide range of human movements in real time and achieve state-of-the-art accuracy in our comparison against alternative systems such as Kinect.  4). We showed how statistical motion priors can be combined seamlessly with physical constraints for human motion modeling and generation. The key idea of the approach is to learn a nonlinear probabilistic force field function from prerecorded motion data with Gaussian processes and combine it with physical constraints in a probabilistic framework. Also, we showed how to effectively utilize the new model to generate a wide range of natural looking motions that achieve the goals specified by the users.   5). We introduced the first data-driven technique for solving full-body inverse dynamics problem. Comparisons against existing methods show our system achieves the state-of-the-art accuracy in full-body inverse dynamics. We also showed how to use our new data-driven inverse dynamics algorithm for improving the efficiency of human motion editing and human motion control.  6). We introduced a robust physics-based motion control system for real-time synthesis of human grasping. Our solution leverages prerecorded motion data and physics-based simulation for human grasping. We demonstrated the power of our approach by generating physics-based motion control for grasping objects with different properties such as shapes, weights, spatial orientations, and frictions.          Last Modified: 06/30/2017       Submitted by: Dilma Da Silva]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
