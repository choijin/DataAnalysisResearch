<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III:Small:Transactional Data Stores in the Cloud</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>499336.00</AwardTotalIntnAmount>
<AwardAmount>515336</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>nan zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Cloud computing has emerged as a powerful paradigm for hosting Internet scale applications in large computing infrastructures due to their desirable features of unlimited resources and infinite scalability, pay-per-use model requiring no up-front investment, elasticity of resources, and fault-tolerance.  Since one of the primary goals of the cloud is to host data-intensive applications, large-scale data management is a crucial component. Traditional relational databases have been extremely successful but lack scalability, elasticity, fault-tolerance, and self-management features that are required in cloud settings. This has led to the emergence of a new storage model referred to as the key-value store model. Although key-value stores have the desired features, they provide minimal consistency and reduced functionality due to their single-key access guarantees thus placing unprecedented burden on application developers. This project explores two alternative scalable data stores designs in the cloud -- ElasTraS: an elastic transactional data store targeted towards enterprise applications requiring a relational storage model; and G-Store: a transactional multi-key value store targeted for applications which favor the data model of key-value stores, but require consistent and scalable access beyond single keys. This project brings forth many novel research solutions for designing and implementing scalable data management systems, and acts as a building block for developing commodity solutions dealing with the growing scale of the Internet. The project enables both graduate and undergraduate students to be trained in the design and development of software and solutions for large-scale distributed systems. The project URL is available at: http://www.cs.ucsb.edu/~dsl/?q=cloud-transactions.</AbstractNarration>
<MinAmdLetterDate>08/01/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1018637</AwardID>
<Investigator>
<FirstName>Divyakant</FirstName>
<LastName>Agrawal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Divyakant Agrawal</PI_FULL_NAME>
<EmailAddress>agrawal@cs.ucsb.edu</EmailAddress>
<PI_PHON>8058934385</PI_PHON>
<NSF_ID>000176871</NSF_ID>
<StartDate>08/01/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amr</FirstName>
<LastName>El Abbadi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amr El Abbadi</PI_FULL_NAME>
<EmailAddress>amr@cs.ucsb.edu</EmailAddress>
<PI_PHON>8058934239</PI_PHON>
<NSF_ID>000188334</NSF_ID>
<StartDate>08/01/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Barbara</Name>
<CityName>Santa Barbara</CityName>
<ZipCode>931062050</ZipCode>
<PhoneNumber>8058934188</PhoneNumber>
<StreetAddress>Office of Research</StreetAddress>
<StreetAddress2><![CDATA[Rm 3227 Cheadle Hall]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA24</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>094878394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA BARBARA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Barbara]]></Name>
<CityName>Santa Barbara</CityName>
<StateCode>CA</StateCode>
<ZipCode>931062050</ZipCode>
<StreetAddress><![CDATA[Office of Research]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA24</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~159137</FUND_OBLG>
<FUND_OBLG>2011~356199</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="tinyMCEContent"> <p>At the conception of this project -- the  conventional wisdom espoused by the Internet giants such as Google,  Yahoo, and Amazon was to abandon the classical notions of  "data consistency" to build scalable data systems in the cloud. This was&nbsp;  one of the first research projects to anticipate the challenges  associated with the development of such systems.</p> <p>Since then, the same companies who were talking about new radical  notions of data consistency -- have come to the realization that as soon  as the need arises to store and manipulate data with certain guarantees -- stronger notions of consistency are needed.&nbsp;</p> <p>The architectures developed during this research project -- are  becoming the pervasive paradigm for the next generation architectures.  The papers reporting these systems are highly cited (source: Google  Scholar).</p> <p>One of the main, long term impactful results is the fundamental lower  bound result that provides a bound on transaction latency in terms of  the round trip time communication between data centers.&nbsp; The lower bound  is especially interesting as it is stated in terms of the sum of the  transaction latencies in different data centers.&nbsp; This result opens the  way for designers to trade off latency delays in different data  centers depending on application demands, so for example, it is possible  for the latencies at one data center to be zero, while the other data  centers has to absorb the entire delay specified by the bound.&nbsp; This result has the potential to have significant impact on the design of large scale global data management systems in cloud  based data centers.</p> <p>During the course of this grant, several significant projects were developed.&nbsp; We summarize some highlights of these systems below:</p> <p><span style="text-decoration: underline;"><strong>Helios</strong></span></p> <p>Based on the lower bound on latencies, we   developed a global scale commit protocol that manages geo-replicated data, called Helios, that achieves low commit   latencies.&nbsp; Efficient geo-replication is essential for the practical usage of global scale data managment, which is need for high transaction performance as well as fault-tolerenace in the face of catastrophic failures, such as earthquacks, hurricans, etc.</p> <p><strong><span style="text-decoration: underline;">MaaT</span></strong></p> <p>&nbsp;Maat is a novel re-design of optimistic concurrency control (OCC)  that provides high performance for data management in a data center. MaaT has demonstrated high throughput while ensuring low transaction abort rates.</p> <p><span style="text-decoration: underline;"><strong>Chariots</strong></span></p> <p>Web-based applications face unprecedented workloads demanding the  processing of a large number of events reaching to the millions per  second. That is why developers are increasingly relying on scalable  cloud platforms to implement cloud applications. Chariots exposes a  shared log to be used by cloud applications, like bookkeeping, recovery, and debugging.&nbsp; Chariots is a  novel distributed deterministic shared log system that scales beyond  the limitations of a single machine and overcomes the bottleneck of a  single-point of contention.&nbsp;  As a cloud platform, Chariots offers fault-tolerance, persistence, and  high-availability, transparently.</p> <br /> <p><span style="text-decoration: underline;"><strong>Squall<br /></strong></span></p> <p>For data-intensive applications with many concurrent users, modern  distributed main memory database management systems (DBMS) provide the  necessary scale-out support beyond what is possible with single-node  systems. These DBMSs typically partition the database into disjoint  subsets and use a single-threaded transaction manager per partition that  executes transactions one-at-a-time in serial order.&nbsp; Squall supports  fine-grained repartitioning of databases in the presence of distributed  transactions,thus providing high throughput, which is critical for successful cloud based data management systems.</p> <p><span style="text-decoration: underline;"><strong>DB-Risk</strong></span></p> <p>Geo-replication is the process of maintaining copies of data at  geographically dispersed datacenters for better availability and  fault-tolerance. The distinguishing characteristic of geo-replication is  the large wide-area latency between datacenters that varies widely  depending on the location of the datacenters. Thus, choosing which  datacenters to deploy a cloud application has a direct impact on the  observable response time. DB-Risk is an optimization framework that  automatically derives a geo-replication placement plan with the  objective of minimizing latency. DB-Risk is a demonstration  that highlights the geo-replication placement optimizations through a  game.</p> <p><span style="text-decoration: underline;"><strong>Ogre</strong></span></p> <p>Data replication across multiple datacenters (geo-replication)  improves availability and fault-tolerance. Analytics and maintenance  tasks are important for cloud applications. Ogre enables analytical tasks  to proceed without any cross-datacenter communication leading to low  latency and high throughput.<br />What distinguishes Ogre from traditionalapproaches&nbsp; is that it is designed for  geo-replicated data. Thus, it avoids design patterns that are  inefficient for geo-replication like extensive communication between  replicas and ordered log shipping.</p> <p><span style="text-decoration: underline;"><strong>Typhon</strong></span></p> <p>Variety of data has lead to the fall of the 'One size fits all'  paradigm in databases. Applications now store data in multiple  representations, rather than just storing the data in a relational  database.&nbsp; <em>Typhon is a</em> multiple-representation data processing framework, which defines  consistency semantics across multiple representations of the data.&nbsp; Our implementation  of Typhon demonstrate performance close to a solution&nbsp;providing no  consistency guarantees across representations, and&nbsp; outperforms a  baseline locking protocol providing consistency guarantees across  multiple data representations. <span style="font-family: Times;"><br /></span></p> </div><br> <p>            Last Modified: 10/31/2016<br>      Modified by: Divyakant&nbsp;Agrawal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  At the conception of this project -- the  conventional wisdom espoused by the Internet giants such as Google,  Yahoo, and Amazon was to abandon the classical notions of  "data consistency" to build scalable data systems in the cloud. This was   one of the first research projects to anticipate the challenges  associated with the development of such systems.  Since then, the same companies who were talking about new radical  notions of data consistency -- have come to the realization that as soon  as the need arises to store and manipulate data with certain guarantees -- stronger notions of consistency are needed.   The architectures developed during this research project -- are  becoming the pervasive paradigm for the next generation architectures.  The papers reporting these systems are highly cited (source: Google  Scholar).  One of the main, long term impactful results is the fundamental lower  bound result that provides a bound on transaction latency in terms of  the round trip time communication between data centers.  The lower bound  is especially interesting as it is stated in terms of the sum of the  transaction latencies in different data centers.  This result opens the  way for designers to trade off latency delays in different data  centers depending on application demands, so for example, it is possible  for the latencies at one data center to be zero, while the other data  centers has to absorb the entire delay specified by the bound.  This result has the potential to have significant impact on the design of large scale global data management systems in cloud  based data centers.  During the course of this grant, several significant projects were developed.  We summarize some highlights of these systems below:  Helios  Based on the lower bound on latencies, we   developed a global scale commit protocol that manages geo-replicated data, called Helios, that achieves low commit   latencies.  Efficient geo-replication is essential for the practical usage of global scale data managment, which is need for high transaction performance as well as fault-tolerenace in the face of catastrophic failures, such as earthquacks, hurricans, etc.  MaaT   Maat is a novel re-design of optimistic concurrency control (OCC)  that provides high performance for data management in a data center. MaaT has demonstrated high throughput while ensuring low transaction abort rates.  Chariots  Web-based applications face unprecedented workloads demanding the  processing of a large number of events reaching to the millions per  second. That is why developers are increasingly relying on scalable  cloud platforms to implement cloud applications. Chariots exposes a  shared log to be used by cloud applications, like bookkeeping, recovery, and debugging.  Chariots is a  novel distributed deterministic shared log system that scales beyond  the limitations of a single machine and overcomes the bottleneck of a  single-point of contention.   As a cloud platform, Chariots offers fault-tolerance, persistence, and  high-availability, transparently.    Squall   For data-intensive applications with many concurrent users, modern  distributed main memory database management systems (DBMS) provide the  necessary scale-out support beyond what is possible with single-node  systems. These DBMSs typically partition the database into disjoint  subsets and use a single-threaded transaction manager per partition that  executes transactions one-at-a-time in serial order.  Squall supports  fine-grained repartitioning of databases in the presence of distributed  transactions,thus providing high throughput, which is critical for successful cloud based data management systems.  DB-Risk  Geo-replication is the process of maintaining copies of data at  geographically dispersed datacenters for better availability and  fault-tolerance. The distinguishing characteristic of geo-replication is  the large wide-area latency between datacenters that varies widely  depending on the location of the datacenters. Thus, choosing which  datacenters to deploy a cloud application has a direct impact on the  observable response time. DB-Risk is an optimization framework that  automatically derives a geo-replication placement plan with the  objective of minimizing latency. DB-Risk is a demonstration  that highlights the geo-replication placement optimizations through a  game.  Ogre  Data replication across multiple datacenters (geo-replication)  improves availability and fault-tolerance. Analytics and maintenance  tasks are important for cloud applications. Ogre enables analytical tasks  to proceed without any cross-datacenter communication leading to low  latency and high throughput. What distinguishes Ogre from traditionalapproaches  is that it is designed for  geo-replicated data. Thus, it avoids design patterns that are  inefficient for geo-replication like extensive communication between  replicas and ordered log shipping.  Typhon  Variety of data has lead to the fall of the 'One size fits all'  paradigm in databases. Applications now store data in multiple  representations, rather than just storing the data in a relational  database.  Typhon is a multiple-representation data processing framework, which defines  consistency semantics across multiple representations of the data.  Our implementation  of Typhon demonstrate performance close to a solution providing no  consistency guarantees across representations, and  outperforms a  baseline locking protocol providing consistency guarantees across  multiple data representations.          Last Modified: 10/31/2016       Submitted by: Divyakant Agrawal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
