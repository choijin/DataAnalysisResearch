<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Mining a Year of Speech</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>99899.00</AwardTotalIntnAmount>
<AwardAmount>99899</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Technologies for storing and processing vast amounts of text are mature and well-defined. In contrast, technologies for browsing or mining content from large collections of non-textual material, especially audio and video, are less well developed. Large sale data mining on text has helped transform the relevant disciplines; the disciplines dealing with spoken language will reap similar benefits from accessible, searchable, large corpora.&lt;br/&gt;&lt;br/&gt;This project explores the difficult problem of providing rich, intelligent data mining capabilities for a substantial collection of spoken audio data in American and British English.  It applies and extends state-of-the-art techniques to offer sophisticated, rapid and flexible access to a richly annotated corpus of a year of speech (about 9,000 hours, 100 million words, or 2 terabytes), derived from the Linguistic Data Consortium, the British National Corpus, and other existing resources. This is ten times more data than has previously been used by researchers in fields such as phonetics, linguistics, and psychology, and 100 to 1,000 times  the amounts that are used in common practice.&lt;br/&gt;&lt;br/&gt;Speech-to-text alignment and search tools will open a new universe of data to researchers in many fields, from linguistics and phonetics to anthropology, speech communication, oral history, and media studies. Audio-video usage on the internet is large and growing at an extraordinary rate, offering increasingly large amounts of an increasingly large range of material. Reliable automatic annotation, indexing and search of this material will allow researchers to examine the distribution of both form and content across time, space, and social structure.</AbstractNarration>
<MinAmdLetterDate>08/08/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1048900</AwardID>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Liberman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Liberman</PI_FULL_NAME>
<EmailAddress>myl@unagi.cis.upenn.edu</EmailAddress>
<PI_PHON>2155735490</PI_PHON>
<NSF_ID>000118365</NSF_ID>
<StartDate>08/08/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Cieri</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher M Cieri</PI_FULL_NAME>
<EmailAddress>ccieri@ldc.upenn.edu</EmailAddress>
<PI_PHON>2155735489</PI_PHON>
<NSF_ID>000208085</NSF_ID>
<StartDate>08/08/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jiahong</FirstName>
<LastName>Yuan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jiahong Yuan</PI_FULL_NAME>
<EmailAddress>jiahong@ling.upenn.edu</EmailAddress>
<PI_PHON>2157463136</PI_PHON>
<NSF_ID>000148891</NSF_ID>
<StartDate>08/08/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress><![CDATA[Research Services]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~99899</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main aim of this project was to demonstrate the applicability of data-mining techniques to the scientific investigation of collections of thousands of hours of transcribed speech. A secondary goal was to complete the digitization and phonetic alignment of the spoken portion of the British National Corpus, in collaboration with Oxford University and the British Library.</p> <p>Despite the advances we enjoy today in information retrieval, we still lack technologies that search spoken audio (and video) with the same ease as text. One solution to this problem is to provide time-aligned transcripts of audio recordings. Such transcripts include time-stamps for each part of the audio, from speaker turns and phrases to words, syllables, consonants, and vowels. Suitably configured systems allow a user to search the transcript and then use the time-stamps to retrieve the corresponding piece of the recording.</p> <p>But in addition to search and retrieval, such datasets become open to data-mining research in new ways. Scholar and scientists who are studying grammar and language change, who are looking for diagnostic or therapeutic measures in clinical contexts, or who are seeking to build better language technology, all benefit from access to these very large collections of automatically-analyzed information about spoken-language performance.&nbsp;</p> <p>The traditional way to create time-aligned phonetic transcripts is to use interactive tools that allow a human expert to annotate the audio corresponding to the associated text. This process is very labor-intensive if a detailed transcript is needed. taking more than 100 hours of effort for every hour of audio. To address this problem, we begin with technique called &ldquo;forced alignment&rdquo;, which uses speech-recognition technology to indicate exactly where in the audio each word from the transcript appears, at the same time providing detailed information about the timing of the various syllables and segments in the word, the exact vowel qualities used, and so on.</p> <p>The main outcome from this project is the demonstration that our time alignment techniques produce accurate results for more than 10,000 hours of speech in English, Chinese and Spanish. To illusrate the utility of the results for speeh data-mining, we used the resulting audio and time-aligned transcripts to study variation in the pronunciation of consonants, vowels, and pitch contours, as documented in several papers published in refereed conference proceedings, and one in a refereed journal. In collaboration with colleagues at Oxford University and the British Library, we contributed to the alignment of the spoken portion of the &ldquo;British National Corpus&rdquo;, which is now available to the public at: <a href="http://www.phon.ox.ac.uk/AudioBNC">http://www.phon.ox.ac.uk/AudioBNC</a>. A sampler is also available here: <a href="http://www.phon.ox.ac.uk/SpokenBNC">http://www.phon.ox.ac.uk/SpokenBNC</a>. In a related project, we also worked with colleagues at oyez.org to word-align the complete set of Supreme Court Oral Arguments (available back to the 1950s), which is now available to the public on the oyez.org web site, and soon will be published for research use by LDC.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/16/2013<br>      Modified by: Mark&nbsp;Liberman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main aim of this project was to demonstrate the applicability of data-mining techniques to the scientific investigation of collections of thousands of hours of transcribed speech. A secondary goal was to complete the digitization and phonetic alignment of the spoken portion of the British National Corpus, in collaboration with Oxford University and the British Library.  Despite the advances we enjoy today in information retrieval, we still lack technologies that search spoken audio (and video) with the same ease as text. One solution to this problem is to provide time-aligned transcripts of audio recordings. Such transcripts include time-stamps for each part of the audio, from speaker turns and phrases to words, syllables, consonants, and vowels. Suitably configured systems allow a user to search the transcript and then use the time-stamps to retrieve the corresponding piece of the recording.  But in addition to search and retrieval, such datasets become open to data-mining research in new ways. Scholar and scientists who are studying grammar and language change, who are looking for diagnostic or therapeutic measures in clinical contexts, or who are seeking to build better language technology, all benefit from access to these very large collections of automatically-analyzed information about spoken-language performance.   The traditional way to create time-aligned phonetic transcripts is to use interactive tools that allow a human expert to annotate the audio corresponding to the associated text. This process is very labor-intensive if a detailed transcript is needed. taking more than 100 hours of effort for every hour of audio. To address this problem, we begin with technique called "forced alignment", which uses speech-recognition technology to indicate exactly where in the audio each word from the transcript appears, at the same time providing detailed information about the timing of the various syllables and segments in the word, the exact vowel qualities used, and so on.  The main outcome from this project is the demonstration that our time alignment techniques produce accurate results for more than 10,000 hours of speech in English, Chinese and Spanish. To illusrate the utility of the results for speeh data-mining, we used the resulting audio and time-aligned transcripts to study variation in the pronunciation of consonants, vowels, and pitch contours, as documented in several papers published in refereed conference proceedings, and one in a refereed journal. In collaboration with colleagues at Oxford University and the British Library, we contributed to the alignment of the spoken portion of the "British National Corpus", which is now available to the public at: http://www.phon.ox.ac.uk/AudioBNC. A sampler is also available here: http://www.phon.ox.ac.uk/SpokenBNC. In a related project, we also worked with colleagues at oyez.org to word-align the complete set of Supreme Court Oral Arguments (available back to the 1950s), which is now available to the public on the oyez.org web site, and soon will be published for research use by LDC.          Last Modified: 04/16/2013       Submitted by: Mark Liberman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
