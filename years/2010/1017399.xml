<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:  Small:  Software Fundamentals for Manycore Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>499825.00</AwardTotalIntnAmount>
<AwardAmount>504625</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The modern computer has two processors, the "central processing unit"(CPU) and the "graphics processing unit" (GPU). Historically, the CPU has been used for general-purpose computing and the GPU for graphics, but as GPUs have become more flexible and programmable, they are increasingly used for computationally-intense general-purpose tasks. However, programming a GPU is difficult, because GPU programmers must divide their programs into many parallel parts. What makes this process easier is using common parallel building blocks developed by expert programmers.&lt;br/&gt;&lt;br/&gt;The research group, together with colleagues from NVIDIA, is building a library of these parallel primitives. In this work, the PIs are concentrating on three major tasks: the addition of new primitives to this library; techniques for optimizing the primitives that we are including; and an extension of the library to supporting many GPUs in a system.</AbstractNarration>
<MinAmdLetterDate>07/27/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/14/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1017399</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Owens</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John D Owens</PI_FULL_NAME>
<EmailAddress>jowens@ece.ucdavis.edu</EmailAddress>
<PI_PHON>5307544289</PI_PHON>
<NSF_ID>000377403</NSF_ID>
<StartDate>07/27/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Davis</Name>
<CityName>Davis</CityName>
<ZipCode>956186134</ZipCode>
<PhoneNumber>5307547700</PhoneNumber>
<StreetAddress>OR/Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1850 Research Park Dr., Ste 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047120084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, DAVIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Davis]]></Name>
<CityName>Davis</CityName>
<StateCode>CA</StateCode>
<ZipCode>956186134</ZipCode>
<StreetAddress><![CDATA[OR/Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~499825</FUND_OBLG>
<FUND_OBLG>2012~4800</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our work focuses on techniques for efficient parallel computing. Over its history, the dominant form of computing has been serial computing, where a computer gets a series of instructions and executes them in sequence, one after another. Recently, the progress in building faster serial computers has significantly slowed; computers and programs today can run faster not by running serial programs faster but instead by running pieces of their programs simultaneously ("in parallel").</p> <p><br />This move to parallel computing comes with numerous challenges. Our research group primarily focuses on a particular parallel processor called the graphics processing unit, or GPU, which has historically been used to render images quickly, but can also be used to process a wide range of computing problems faster and more efficiently than serial computers. Under this project, we developed new data structures and algorithms that are suitable for parallel computing; explored new directions in optimizing programs for parallel architectures; and extended our techniques to work across multiple GPUs on larger machines.</p> <p><br />The most important broad impact of this work has been the training of the graduate students who move on to industry (e.g., Google, NVIDIA Research) and academia (Pinar Muyan-Ozcelik is my first graduate to join a university as an assistant professor) and lead our future scientific discovery. This project provided &nbsp;funding for nearly a dozen graduate students at some point in their graduate careers. Another broad impact is the benefits of the advancements we outline below, which allow more efficient, higher-performance computing, allowing more powerful supercomputers and faster/cooler laptops, tablets, and phones.</p> <p><br />These research advancements constitute our contributions in terms of intellectual merit, which have been published in top venues across the field of computing, and which have often resulted in open-source software release so others can easily build on them. Among the important advances we made are:</p> <p>&nbsp;</p> <ul> <li>Numerous new algorithms and data structures for parallel computing. These are the building blocks of parallel computing; others can build new applications using the primitives we have designed and built. We have extended our work on a class of numerical algorithms called tridiagonal systems; (with Sandia National Labs) explored a variety of approaches to high-dimensional sampling; pioneered new models and implementations for exploiting task parallelism on GPUs; characterized and developed methodologies and implementations for a novel GPU programming model called persistent threads; (with NC State) explored backtracking computation; built new techniques for parallel (merge) sorting; developed techniques for multitasking on GPUs for embedded applications; implemented the first lossless compression scheme for GPUs; and improved that scheme with the development of suffix array implementations for GPUs. One of the overarching projects, developed under NSF support, that incorporated numerous advances from this area of study was our "Piko" project for prototyping and building complex, user-defined pipelines for computer graphics applications.</li> </ul> <p>&nbsp;</p> <ul> <li>Optimizations: We developed a methodology for analyzing and characterizing GPU performance bottlenecks; built a register-packing strategy for optimizing communication patterns on GPUs; and showed how our tridiagonal systems solver could be autotuned for top performance.</li> </ul> <p>&nbsp;</p> <ul> <li>Multi-GPU computing: Our MapReduce implementation had two firsts in the GPU world: multiple GPUs and out-of-core processing. And with a collaborator from Argonne, we explored next-generation message-passing interfaces and requirements for future GPU-based multi-node systems.</li> </ul> <p><br />Over the course of this grant, the topic of graph a...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our work focuses on techniques for efficient parallel computing. Over its history, the dominant form of computing has been serial computing, where a computer gets a series of instructions and executes them in sequence, one after another. Recently, the progress in building faster serial computers has significantly slowed; computers and programs today can run faster not by running serial programs faster but instead by running pieces of their programs simultaneously ("in parallel").   This move to parallel computing comes with numerous challenges. Our research group primarily focuses on a particular parallel processor called the graphics processing unit, or GPU, which has historically been used to render images quickly, but can also be used to process a wide range of computing problems faster and more efficiently than serial computers. Under this project, we developed new data structures and algorithms that are suitable for parallel computing; explored new directions in optimizing programs for parallel architectures; and extended our techniques to work across multiple GPUs on larger machines.   The most important broad impact of this work has been the training of the graduate students who move on to industry (e.g., Google, NVIDIA Research) and academia (Pinar Muyan-Ozcelik is my first graduate to join a university as an assistant professor) and lead our future scientific discovery. This project provided  funding for nearly a dozen graduate students at some point in their graduate careers. Another broad impact is the benefits of the advancements we outline below, which allow more efficient, higher-performance computing, allowing more powerful supercomputers and faster/cooler laptops, tablets, and phones.   These research advancements constitute our contributions in terms of intellectual merit, which have been published in top venues across the field of computing, and which have often resulted in open-source software release so others can easily build on them. Among the important advances we made are:     Numerous new algorithms and data structures for parallel computing. These are the building blocks of parallel computing; others can build new applications using the primitives we have designed and built. We have extended our work on a class of numerical algorithms called tridiagonal systems; (with Sandia National Labs) explored a variety of approaches to high-dimensional sampling; pioneered new models and implementations for exploiting task parallelism on GPUs; characterized and developed methodologies and implementations for a novel GPU programming model called persistent threads; (with NC State) explored backtracking computation; built new techniques for parallel (merge) sorting; developed techniques for multitasking on GPUs for embedded applications; implemented the first lossless compression scheme for GPUs; and improved that scheme with the development of suffix array implementations for GPUs. One of the overarching projects, developed under NSF support, that incorporated numerous advances from this area of study was our "Piko" project for prototyping and building complex, user-defined pipelines for computer graphics applications.      Optimizations: We developed a methodology for analyzing and characterizing GPU performance bottlenecks; built a register-packing strategy for optimizing communication patterns on GPUs; and showed how our tridiagonal systems solver could be autotuned for top performance.      Multi-GPU computing: Our MapReduce implementation had two firsts in the GPU world: multiple GPUs and out-of-core processing. And with a collaborator from Argonne, we explored next-generation message-passing interfaces and requirements for future GPU-based multi-node systems.    Over the course of this grant, the topic of graph analytics on large graphs ("big data") became increasingly important (and popular). Large graphs are useful in both science and industry; one popular application is among social networking companies,...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
