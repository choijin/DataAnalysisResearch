<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Integrating Paradigms for Approximate Stochastic Planning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>450508.00</AwardTotalIntnAmount>
<AwardAmount>466508</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Todd Leen</SignBlockName>
<PO_EMAI>tleen@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A fundamental challenge for Artificial Intelligence is sequential decision making under uncertainty, a task where automated algorithms lag far behind human-level intelligence. The primary reason for the disparity is curse of dimensionality - the number of states is exponential in the problem features. Recent advances that restrict decision-theoretic computation to a reachable subset of state space have scaled to moderately-sized problems, but proven ineffective in scaling to real problems. On the other hand, probabilistic planners based on deterministic planning might scale up, but with a massive loss in solution quality.&lt;br/&gt;&lt;br/&gt;This project is investigating several methods to scale probabilistic planning to real-sized problems. We combine decision-theoretic analysis, basis function approximation and the classical AI planning techniques, to develop a series of highly scalable planners. A common theme in our techniques is the use of deterministic plans to automatically obtain domain abstractions in the form of 'good' or 'bad' properties, or intermediate subgoals. The project introduces and exploits a principled collaboration between decision theory and classical planning techniques, thus retaining the benefits of both - high quality as well as high performance. Experiments show that our new planner solves difficult planning competition problems using orders of magnitude less memory outputting high quality policies.&lt;br/&gt;&lt;br/&gt;Our research also proposes effective solutions to long-standing problems of generating a set of basis functions and computing a hierarchical problem decomposition. Both basis function approximation and hierarchical decomposition are popular in existing literature for speeding up planning, but they are not fully automated - a human is required to specify the basis functions and the hierarchy. We provide novel, domain-independent solutions that remove this additional human effort.  &lt;br/&gt;&lt;br/&gt;Our research addresses several long standing challenges in AI, like scaling stochastic planning, and automatically generating basis functions and subgoal hierarchies. We expect to produce state-of-the-art planners that will be effective in large and complex real world scenarios, e.g., planetary exploration, military operations planning, and robotic decision making.</AbstractNarration>
<MinAmdLetterDate>08/16/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016465</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Weld</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel S Weld</PI_FULL_NAME>
<EmailAddress>weld@cs.washington.edu</EmailAddress>
<PI_PHON>2065439196</PI_PHON>
<NSF_ID>000099760</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mausam</FirstName>
<LastName>Mausam</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mausam Mausam</PI_FULL_NAME>
<EmailAddress>mausam@cs.washington.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000080843</NSF_ID>
<StartDate>08/16/2010</StartDate>
<EndDate>09/09/2013</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~450508</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Automated planning &ndash; devising a course of action to best accomplish an agent&rsquo;s goals and objectives &ndash; is a major area of study for Artificial Intelligence researchers. Planning is especially difficult in domains where uncertainty is rife and actions do not always have their intended effect. In this case a mathematical framework, called&nbsp;Markov Decision Processes (MDPs), is often adopted because it can describe a wide variety of planning scenarios ranging from military operations planning to controlling a Mars rover. However, today&rsquo;s MDP solution techniques scale poorly, limiting their practical applicability. Furthermore, theorists have shown that efforts to devise algorithms that solve MDPs exactly (eg find the guaranteed best possible policy) are doomed to be impractically slow for large problems. Therefore, under this grant, we investigated approximate strategies to find methods with that run quickly enough to be practical yet produced excellent (if not optimal) plans. Specifically, we devised algorithms that automatically discover and exploit the hidden structure of factored MDPs. Doing so helps solve MDPs faster and with less memory than state-of-the-art techniques.<br />Our algorithms discover two complementary state abstractions &ndash;&nbsp;basis functions&nbsp;and&nbsp;nogoods.&nbsp;A basis function is a conjunction of literals; if the conjunction holds true in a state, this guarantees the existence of at least one trajectory to the goal. Conversely, a nogood is a conjunction whose presence implies the&nbsp;non-existence&nbsp;of any such trajectory, meaning the state is a dead end. We compute basis functions by regressing goal descriptions through a determinized version of the MDP (i.e., one in which the effects of actions are predictable). Nogoods are constructed with a novel machine-learning algorithm that uses basis functions as training data.<br />Our state abstractions can be leveraged in several ways. We describe three diverse approaches &mdash; GOTH, a heuristic function for use in heuristic search algorithms such as RTDP; RETRASE, an MDP solver that performs modified Bellman backups on basis functions instead of states; and SIXTHSENSE, a method to quickly detect dead- end states. In essence, our work integrates ideas from deterministic planning and basis function-based approximation, leading to methods that outperform previous approaches by a wide margin.</p><br> <p>            Last Modified: 09/18/2014<br>      Modified by: Daniel&nbsp;S&nbsp;Weld</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Automated planning &ndash; devising a course of action to best accomplish an agentÆs goals and objectives &ndash; is a major area of study for Artificial Intelligence researchers. Planning is especially difficult in domains where uncertainty is rife and actions do not always have their intended effect. In this case a mathematical framework, called Markov Decision Processes (MDPs), is often adopted because it can describe a wide variety of planning scenarios ranging from military operations planning to controlling a Mars rover. However, todayÆs MDP solution techniques scale poorly, limiting their practical applicability. Furthermore, theorists have shown that efforts to devise algorithms that solve MDPs exactly (eg find the guaranteed best possible policy) are doomed to be impractically slow for large problems. Therefore, under this grant, we investigated approximate strategies to find methods with that run quickly enough to be practical yet produced excellent (if not optimal) plans. Specifically, we devised algorithms that automatically discover and exploit the hidden structure of factored MDPs. Doing so helps solve MDPs faster and with less memory than state-of-the-art techniques. Our algorithms discover two complementary state abstractions &ndash; basis functions and nogoods. A basis function is a conjunction of literals; if the conjunction holds true in a state, this guarantees the existence of at least one trajectory to the goal. Conversely, a nogood is a conjunction whose presence implies the non-existence of any such trajectory, meaning the state is a dead end. We compute basis functions by regressing goal descriptions through a determinized version of the MDP (i.e., one in which the effects of actions are predictable). Nogoods are constructed with a novel machine-learning algorithm that uses basis functions as training data. Our state abstractions can be leveraged in several ways. We describe three diverse approaches &mdash; GOTH, a heuristic function for use in heuristic search algorithms such as RTDP; RETRASE, an MDP solver that performs modified Bellman backups on basis functions instead of states; and SIXTHSENSE, a method to quickly detect dead- end states. In essence, our work integrates ideas from deterministic planning and basis function-based approximation, leading to methods that outperform previous approaches by a wide margin.       Last Modified: 09/18/2014       Submitted by: Daniel S Weld]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
