<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Kolmogorov's Algorithm Statistics for Dynamics in High Frequency Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2010</AwardEffectiveDate>
<AwardExpirationDate>12/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>348765.00</AwardTotalIntnAmount>
<AwardAmount>348765</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>When applying likelihood theory for analyzing high-frequency time series data, scientists and analysts simultaneously face both computational complexity due to the accommodation of several million observations,  and informational complexity, due to the involvement of manifold and diverse dynamic mechanisms.  The investigator proposes to establish Kolmogorov's algorithmic statistics as the unified foundation to bridge the gaps caused by these computational and informational complexities, and to make it possible for systematic and effective discovery of characteristic dynamics.  The proposal focuses on its development by resolving critical issues including: What are the models of data's individuality and typicality, why are they crucial, and how can they be applied by scientists and analysts for discovery and detection?  A new vehicle for this development is the Hierarchical Factor Segmentation (HFS) algorithm.  This completely distinct approach is undertaken to transform an observed time series into various counting processes corresponding to different events of interest, and then to apply the coding schemes to achieve lossy data compression as a way to find the governing state-space trajectory.  This is accomplished without estimating the point processes' time-varying intensity functions, nor relying on any unrealistic prior knowledge about the number of changes, nor assumptions about the regime-generating mechanisms.  Using the computed state-space trajectory, the investigator is able to modify or replace currently prevailing statistical thinking ? such as likelihood theory ? and existing popular methodologies ? such as those based on statistical correlation and association ? by using the connectivity and concurrence of the decoded states.  These real-world applications in finance, biology and national security will realistically illuminate the great merit and potential of this new statistical thinking and computing for discovering real dynamics that are of great interest in the sciences and in society. &lt;br/&gt;&lt;br/&gt;Currently, there are many situations in which data are being sampled and recorded on a time scale of milliseconds, or even nanoseconds.  These high-frequency data are found not only in the sciences, but also in economics, finance and national security.  However, due to its enormous length and complexity, these data types cannot be handled well using existing statistical methodologies.  In fact, prevailing statistical thinking is inadequate for resolving issues underlying these kinds of data.  Brand-new statistical thinking is urgently needed to bridge the gap between computing and conception in order to produce coherent and real mechanisms for data analysis.  The investigator proposes the algorithmic statistics as the new foundation for scientists and analysts to focus on extracting key characteristics, such as individuality and typicality, within high-frequency data.  An algorithmic statistic is a computer algorithm that takes a multidimensional time series consisting of millions of time points as the input, and efficiently computes realistic and sufficient analytic results as the output. Accordingly, the classic concepts, such as correlation and association, would be modified or replaced based on the connectivity and concurrence of decoded significant regimes.  In particular, resultant models of individuality and typicality are tremendously useful and important for regulating and detecting purposes.  This proposal also targets the detection of any abnormality or extremism, for example, in trading or in physiological and behavioral processes, embedded within long and noisy high-frequency data.</AbstractNarration>
<MinAmdLetterDate>07/14/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/14/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1007219</AwardID>
<Investigator>
<FirstName>Fushing</FirstName>
<LastName>Hsieh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Fushing Hsieh</PI_FULL_NAME>
<EmailAddress>fhsieh@ucdavis.edu</EmailAddress>
<PI_PHON>5305542898</PI_PHON>
<NSF_ID>000337667</NSF_ID>
<StartDate>07/14/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Davis</Name>
<CityName>Davis</CityName>
<ZipCode>956186134</ZipCode>
<PhoneNumber>5307547700</PhoneNumber>
<StreetAddress>OR/Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1850 Research Park Dr., Ste 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047120084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, DAVIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Davis]]></Name>
<CityName>Davis</CityName>
<StateCode>CA</StateCode>
<ZipCode>956186134</ZipCode>
<StreetAddress><![CDATA[OR/Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>7721</Code>
<Text>FROM DATA TO KNOWLEDGE</Text>
</ProgramReference>
<ProgramReference>
<Code>7752</Code>
<Text>CDI NON SOLICITED RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~348765</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Project Outcome Report of NSF 1007219</strong></p> <p>Physicist and Nobel Prize winner, Murray Gell-Mann, was quoted in Coveney &amp; Highfield&rsquo;s 1994 book &ldquo;Frontiers of Complexity-the search for order in a chaotic world&rdquo; (p-8) as saying:</p> <p><em>We must get away from the idea that serious work is restricted to &ldquo;beating to death a well-defined problem in a narrow discipline, while broadly integrative thinking is relegated to cocktail parties.&rdquo;</em></p> <p>This vision is more pertinent now than ever before. As Information Technology advances, researchers are able to create many new and revisit even more old systems in human society and in nature.&nbsp; These new and old systems receive tremendous amounts of new research attentions because of brand new technologies of collecting large amounts of data. Data of various formats, high dimensional point cloud or high frequency time series, or networks of many types and sizes, is collected and explored to shed new lights of authentic understanding into systems of interest. &nbsp;</p> <p>Why integrative thinking is desperately needed now is because all these data formats contain complex dependence structures among and between sampled subjects and measured features. Such structural dependences consist of two closely coupled systemic characteristics: deterministic structures and inherent randomness. This concept is one well-known in statistical physics, particularly in the field of Complex System. This concept prescribes scientific endeavors in complex systems as formulating and discovering patterns of deterministic structures and mechanisms of inherent randomness.</p> <p>In contrast, these two systemic characteristics clearly are in defiance of majority of statistical principles, concepts, modeling and bootstrapping approaches that are based on unrealistic independent and stationary assumptions. Therefore statistical investigations are commonly conducted by ignoring systemic sensitivity. This is why statistical results for complex system are likely unrealistic or even illusory.&nbsp;</p> <p>The primary achievement in this NSF grant is that I build an integrative foundation for discovering systemic characteristics by combining principles from three perspectives of: 1) Statistical physics: I quantify the deterministic structure through the lowest energy macrostate with respect to a suitably chosen Hamiltonian; 2) Computational geometry: I develop a new computing paradigm, called Data Mechanics, by building two highly coupled Ultrametric trees on row and column axes of any data matrix to bring out its multiscale block patterns as the optimal resolution for the macrostate; 3) Information theory: I take the mutliscale block patterns as minimum sufficient statistics of Kolmogorov&rsquo;s algorithmic statistics, and construct algorithms for capturing mechanisms for inherent randomness within all involving blocks.</p> <p>The computed deterministic structures and inherent randomness is then called a coupling geometry on a data matrix.&nbsp; Such a coupling geometry indeed embraces a matrix version of Kolmogorov complexity. Kolmogorov&rsquo;s two-part coding scheme then becomes the integrative principle for mimicking or bootstrapping a matrix by retaining its systemic characteristics. That is why such a coupling geometry plays the new foundation role for studying a complex system at a single phase, which is approximated by a network or high dimensional data cloud or time series.</p> <p>For the study of an entire system, a partial coupling geometry computation is developed to compute the causal and predictive associations linking two adjacent phases. In such a fashion, an evolution of phases can be sequentially linked and studied to manifest systemic understanding.</p> <p>Network bootstrapping ensembles as technical devices are derived to bear with various scales of structural information, and t...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Project Outcome Report of NSF 1007219  Physicist and Nobel Prize winner, Murray Gell-Mann, was quoted in Coveney &amp; HighfieldÆs 1994 book "Frontiers of Complexity-the search for order in a chaotic world" (p-8) as saying:  We must get away from the idea that serious work is restricted to "beating to death a well-defined problem in a narrow discipline, while broadly integrative thinking is relegated to cocktail parties."  This vision is more pertinent now than ever before. As Information Technology advances, researchers are able to create many new and revisit even more old systems in human society and in nature.  These new and old systems receive tremendous amounts of new research attentions because of brand new technologies of collecting large amounts of data. Data of various formats, high dimensional point cloud or high frequency time series, or networks of many types and sizes, is collected and explored to shed new lights of authentic understanding into systems of interest.    Why integrative thinking is desperately needed now is because all these data formats contain complex dependence structures among and between sampled subjects and measured features. Such structural dependences consist of two closely coupled systemic characteristics: deterministic structures and inherent randomness. This concept is one well-known in statistical physics, particularly in the field of Complex System. This concept prescribes scientific endeavors in complex systems as formulating and discovering patterns of deterministic structures and mechanisms of inherent randomness.  In contrast, these two systemic characteristics clearly are in defiance of majority of statistical principles, concepts, modeling and bootstrapping approaches that are based on unrealistic independent and stationary assumptions. Therefore statistical investigations are commonly conducted by ignoring systemic sensitivity. This is why statistical results for complex system are likely unrealistic or even illusory.   The primary achievement in this NSF grant is that I build an integrative foundation for discovering systemic characteristics by combining principles from three perspectives of: 1) Statistical physics: I quantify the deterministic structure through the lowest energy macrostate with respect to a suitably chosen Hamiltonian; 2) Computational geometry: I develop a new computing paradigm, called Data Mechanics, by building two highly coupled Ultrametric trees on row and column axes of any data matrix to bring out its multiscale block patterns as the optimal resolution for the macrostate; 3) Information theory: I take the mutliscale block patterns as minimum sufficient statistics of KolmogorovÆs algorithmic statistics, and construct algorithms for capturing mechanisms for inherent randomness within all involving blocks.  The computed deterministic structures and inherent randomness is then called a coupling geometry on a data matrix.  Such a coupling geometry indeed embraces a matrix version of Kolmogorov complexity. KolmogorovÆs two-part coding scheme then becomes the integrative principle for mimicking or bootstrapping a matrix by retaining its systemic characteristics. That is why such a coupling geometry plays the new foundation role for studying a complex system at a single phase, which is approximated by a network or high dimensional data cloud or time series.  For the study of an entire system, a partial coupling geometry computation is developed to compute the causal and predictive associations linking two adjacent phases. In such a fashion, an evolution of phases can be sequentially linked and studied to manifest systemic understanding.  Network bootstrapping ensembles as technical devices are derived to bear with various scales of structural information, and then give rise to an energy distribution profile. This energy distribution profile becomes the base for inferring and testing hypothesis regarding systemic structure. Then a mutual energy derived from part...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
