<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Computational Behavioral Science: Modeling, Analysis, and Visualization of Social and Communicative Behavior</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>1499518.00</AwardTotalIntnAmount>
<AwardAmount>1531518</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computational Behavioral Science: Modeling, Analysis, and Visualization of Social and &lt;br/&gt;Communicative Behavior&lt;br/&gt;Lead PI/Institution: James M. Rehg, Georgia Institute of Technology&lt;br/&gt;This Expedition will develop novel computational methods for measuring and analyzing the behavior of children and adults during face-to-face social interactions. Social behavior plays a key role in the acquisition of social and communicative skills during childhood. Children with developmental disorders, such as autism, face great challenges in acquiring these skills, resulting in substantial lifetime risks. Current best practices for evaluating behavior and assessing risk are based on direct observation by highly-trained specialists, and cannot be easily scaled to the large number of individuals who need evaluation and treatment. For example, autism affects 1 in 110 children in the U.S., with a lifetime cost of care of $3.2 million per person. By developing methods to automatically collect fine-grained behavioral data, this project will enable large-scale objective screening and more effective delivery and assessment of therapy. Going beyond the treatment of disorders, this technology will make it possible to automatically measure behavior over long periods of time for large numbers of individuals in a wide range of settings. Many disciplines, such as education, advertising, and customer relations, could benefit from a quantitative, data-drive approach to behavioral analysis. &lt;br/&gt;Human behavior is inherently multi-modal, and individuals use eye gaze, hand gestures, facial expressions, body posture, and tone of voice along with speech to convey engagement and regulate social interactions.  This project will develop multiple sensing technologies, including vision, speech, and wearable sensors, to obtain a comprehensive, integrated portrait of expressed behavior. Cameras and microphones provide an inexpensive, noninvasive means for measuring eye, face, and body movements along with speech and nonspeech utterances. Wearable sensors can measure physiological variables such as heart-rate and skin conductivity, which contain important cues about levels of internal stress and arousal that are linked to expressed behavior. This project is developing unique capabilities for synchronizing multiple sensor streams, correlating these streams to measure behavioral variables such as affect and attention, and modeling extended interactions between two or more individuals. In addition, novel behavior visualization methods are being developed to enable real-time decision support for interventions and the effective use of repositories of behavioral data. Methods are also under development for reflecting the capture and analysis process to users of the technology.&lt;br/&gt;The long-term goal of this project is the creation of a new scientific discipline of computational behavioral science, which draws equally from computer science and psychology in order to transform the study of human behavior. A comprehensive education plan supports this goal through the creation of an interdisciplinary summer school for young researchers and the development of new courses in computational behavior. Outreach activities include significant and on-going collaborations with major autism research centers in Atlanta, Boston, Pittsburgh, Urbana-Champaign, and Los Angeles.</AbstractNarration>
<MinAmdLetterDate>08/19/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1029549</AwardID>
<Investigator>
<FirstName>Takeo</FirstName>
<LastName>Kanade</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Takeo Kanade</PI_FULL_NAME>
<EmailAddress>kanade@cs.cmu.edu</EmailAddress>
<PI_PHON>4122683016</PI_PHON>
<NSF_ID>000210182</NSF_ID>
<StartDate>08/19/2010</StartDate>
<EndDate>07/31/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anind</FirstName>
<LastName>Dey</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anind K Dey</PI_FULL_NAME>
<EmailAddress>anind@uw.edu</EmailAddress>
<PI_PHON>2066859937</PI_PHON>
<NSF_ID>000260331</NSF_ID>
<StartDate>08/19/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yaser</FirstName>
<LastName>Sheikh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yaser Sheikh</PI_FULL_NAME>
<EmailAddress>yaser@cs.cmu.edu</EmailAddress>
<PI_PHON>4122681138</PI_PHON>
<NSF_ID>000502497</NSF_ID>
<StartDate>07/31/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>PITTSBURGH</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7723</Code>
<Text>EXPERIMENTAL EXPEDITIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7969</Code>
<Text>FY 2010 Funding for PTR</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~898860</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<FUND_OBLG>2012~600658</FUND_OBLG>
<FUND_OBLG>2014~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There are many available sources of behavioral data from humans: interactions online, interactions with everyday objects, interactions with computing devices etc. There are also a wide variety of techniques for sensing and collecting this data including the use of cameras, microphones, fixed and mobile sensors. These devices collect data in a range of modalities, with time as an index into the human behavioral data. Our ability to reason about and understand human behaviors is critical for understanding how people communicate, how they perform tasks, and how they achieve goals.&nbsp;</p> <p>In this project, we have developed a number of tools for collecting a large variety of behavioral data through the instrumentation of everyday objects, instrumentation of computing technologies, and instrumentation of physical spaces. The tools we have created also supports the ability to visualize the complex data, to support reasoning about the data, and the ability to anlayze the behavioral data in an interactive manner. These tools allow us to understand human behavior, reason about it, extract human routines from observed behaviors, and to leverage the results to identify techniques for changing behaviors of interest.&nbsp;</p> <p>Intellectual merit: we have developed new algorithms, definitions and tools for better understand human behaviors, primarily for social and communicative behavior, but also applicable to a wide range of behaviors.</p> <p>Broader impact: the algorithsm, definitions and tools will be useful to other scientists looking to understand and reason about human behavior. In addition the findings we achieved through deep domain analyses will be of interest to people associated with those domains.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/31/2017<br>      Modified by: Anind&nbsp;K&nbsp;Dey</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There are many available sources of behavioral data from humans: interactions online, interactions with everyday objects, interactions with computing devices etc. There are also a wide variety of techniques for sensing and collecting this data including the use of cameras, microphones, fixed and mobile sensors. These devices collect data in a range of modalities, with time as an index into the human behavioral data. Our ability to reason about and understand human behaviors is critical for understanding how people communicate, how they perform tasks, and how they achieve goals.   In this project, we have developed a number of tools for collecting a large variety of behavioral data through the instrumentation of everyday objects, instrumentation of computing technologies, and instrumentation of physical spaces. The tools we have created also supports the ability to visualize the complex data, to support reasoning about the data, and the ability to anlayze the behavioral data in an interactive manner. These tools allow us to understand human behavior, reason about it, extract human routines from observed behaviors, and to leverage the results to identify techniques for changing behaviors of interest.   Intellectual merit: we have developed new algorithms, definitions and tools for better understand human behaviors, primarily for social and communicative behavior, but also applicable to a wide range of behaviors.  Broader impact: the algorithsm, definitions and tools will be useful to other scientists looking to understand and reason about human behavior. In addition the findings we achieved through deep domain analyses will be of interest to people associated with those domains.             Last Modified: 01/31/2017       Submitted by: Anind K Dey]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
