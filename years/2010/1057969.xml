<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Multisensory Perceptual Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>03/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>280598.00</AwardTotalIntnAmount>
<AwardAmount>336725</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Lawrence Gottlob</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Ladan Shams, University of California at Los Angeles&lt;br/&gt;Aaron Seitz, University of California at Riverside&lt;br/&gt;&lt;br/&gt;COLLABORATIVE RESEARCH: MULTISENSORY PERCEPTUAL LEARNING&lt;br/&gt;&lt;br/&gt;ABSTRACT&lt;br/&gt;&lt;br/&gt;In daily life, we frequently experience correlated sensations across our different sensory modalities. For example, as we climb up the stairs, we receive sensations to our auditory, visual, tactile, and vestibular systems that are all related to the experience of stair-climbing. These types of multisensory experiences are a key aspect of how we interact with, and learn about, the world around us. Through our experience with the world, our sensory abilities undergo refinements that allow us to optimize our performance in the tasks that we perform. These sensory refinements involve fine-tuning of processing in each of our sensory modalities, but equally importantly, in how we merge information across modalities. While much research has focused on how learning can take place within each individual sensory system, the learning of how information is combined across the senses has been largely neglected. The PIs will conduct a series of experiments in which they can track visual, auditory, and auditory-visual multisensory learning in parallel, and discriminate among different theories of multisensory processing and learning. Behavioral and neuroimaging methods will be combined to shed light on the roles that different brain areas, and the interactions between brain areas, play in the process of multisensory learning. Altogether these studies will provide fundamental insights into how our sensory systems work together and refine their interactions to best operate in the tasks that we perform.&lt;br/&gt;&lt;br/&gt;This project will be the first systematic investigation of multisensory perceptual learning. It will also be the first study of changes in interaction between brain areas that may occur as a result of sensory learning. Altogether, this study promises to provide foundational knowledge regarding the brain mechanisms involved in multisensory learning as well as the mechanisms of learning in general. Understanding multisensory learning can contribute to the development of more effective strategies for learning. These strategies can be utilized to enhance learning for typically-developed children and adults, as well as to facilitate learning and communication for individuals with deprivation in one sense (e.g., individuals with low-vision or low-hearing, patients with cochlear implants or undergoing macular degeneration or cataract surgeries). They can also contribute to devising remedial programs for dyslexia, which appears to involve deficits in combining information across the senses.</AbstractNarration>
<MinAmdLetterDate>03/31/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/20/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1057969</AwardID>
<Investigator>
<FirstName>Ladan</FirstName>
<LastName>Shams</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ladan Shams</PI_FULL_NAME>
<EmailAddress>ladan@psych.ucla.edu</EmailAddress>
<PI_PHON>3102063630</PI_PHON>
<NSF_ID>000471539</NSF_ID>
<StartDate>03/31/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>LOS ANGELES</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951406</ZipCode>
<StreetAddress><![CDATA[10889 Wilshire Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1397</Code>
<Text>Cross-Directorate  Activities</Text>
</ProgramElement>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>CL10</Code>
<Text>CLB-Career Life Balance</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~198848</FUND_OBLG>
<FUND_OBLG>2012~81750</FUND_OBLG>
<FUND_OBLG>2013~18929</FUND_OBLG>
<FUND_OBLG>2016~37198</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Most of human&rsquo;s daily experience involves processing of information in multiple sensory modalities, including vision and hearing. In this project we investigated how processing of information in multiple modalities influences perception, memory and learning in human brain.</p> <p>&nbsp;</p> <p>Using sophisticated behavioral, neuroscientific, and computational techniques we made discoveries about interaction between the senses in the process of perceiving, learning and remembering. Some findings were highly surprising and counter-intuitive. For example, we discovered that in a difficult visual task (that involves processing sequence of visual stimuli), while training individuals using visual stimuli failes to produce improvement in performance, training them using auditory stimuli (sequence of beeps) results in substantial improvement in the visual performance. This suggests that depending on the nature of the task, the brain outsources the processing of visual information to auditory parts of the brain, and therefore, training using acoustic information is superior to training using visual information.</p> <p>&nbsp;</p> <p>Other studies showed that translating visual information to sound can benefit performance in challenging visual tasks. And associating visual objects with acoustic signals (such as melodies) can benefit memory of the visual objects.</p> <p>&nbsp;</p> <p>We also developed and refined computational models for analyzing behavioral data, and developed a toolbox that other researchers can use to analyze and interpret their experimental data rigorously. This toolbox is now made available to public, and can be used for educational or research purposes.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/30/2018<br>      Modified by: Ladan&nbsp;Shams</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Most of human?s daily experience involves processing of information in multiple sensory modalities, including vision and hearing. In this project we investigated how processing of information in multiple modalities influences perception, memory and learning in human brain.     Using sophisticated behavioral, neuroscientific, and computational techniques we made discoveries about interaction between the senses in the process of perceiving, learning and remembering. Some findings were highly surprising and counter-intuitive. For example, we discovered that in a difficult visual task (that involves processing sequence of visual stimuli), while training individuals using visual stimuli failes to produce improvement in performance, training them using auditory stimuli (sequence of beeps) results in substantial improvement in the visual performance. This suggests that depending on the nature of the task, the brain outsources the processing of visual information to auditory parts of the brain, and therefore, training using acoustic information is superior to training using visual information.     Other studies showed that translating visual information to sound can benefit performance in challenging visual tasks. And associating visual objects with acoustic signals (such as melodies) can benefit memory of the visual objects.     We also developed and refined computational models for analyzing behavioral data, and developed a toolbox that other researchers can use to analyze and interpret their experimental data rigorously. This toolbox is now made available to public, and can be used for educational or research purposes.          Last Modified: 07/30/2018       Submitted by: Ladan Shams]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
