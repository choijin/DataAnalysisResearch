<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Theory and Practice of Bayesian Motion Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2011</AwardEffectiveDate>
<AwardExpirationDate>02/28/2017</AwardExpirationDate>
<AwardTotalIntnAmount>403328.00</AwardTotalIntnAmount>
<AwardAmount>403328</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops the theory and algorithms for the next generation of statistical motion models and their applications in Bayesian motion synthesis. Thus far, one of the most effective ways to model human movement is to construct statistical motion models from prerecorded motion data. While the promise of learning from motion data is unlimited, current statistical motion modeling techniques suffer from four major limitations. Firstly, they lack scalability and ability to model large and heterogeneous datasets. Secondly, they do not capture environmental contact information embedded in prerecorded motion data. Thirdly, they are mainly focused on modeling spatial-temporal patterns within a small temporal window rather than the global motion structures of human actions and thus face great risk of destroying global motion structures in motion generalization. Lastly and most importantly, they do not consider dynamics that cause the motion. This project investigates a new generation of statistical motion models that address these four challenges. The project also develops new Bayesian motion synthesis algorithms that leverage the proposed generative models in graphics and vision applications. In addition, the research produces new animation modeling systems for novices, new performance interfaces for full-body motion control, and new technologies for video-based motion capture. In the project, the PI makes special efforts to recruit students from under-represented groups, to integrate the research into existing and new courses, and to use a high school competition as a channel for attracting more young students to pursue careers in computer science.</AbstractNarration>
<MinAmdLetterDate>03/21/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1055046</AwardID>
<Investigator>
<FirstName>Jinxiang</FirstName>
<LastName>Chai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jinxiang Chai</PI_FULL_NAME>
<EmailAddress>jchai@cs.tamu.edu</EmailAddress>
<PI_PHON>9798453510</PI_PHON>
<NSF_ID>000081361</NSF_ID>
<StartDate>03/21/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M Engineering Experiment Station</Name>
<CityName>College Station</CityName>
<ZipCode>778454645</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy S</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>847205572</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A&amp;M ENGINEERING EXPERIMENT STATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&amp;M Engineering Experiment Station]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778454645</ZipCode>
<StreetAddress><![CDATA[400 Harvey Mitchell Pkwy S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~59694</FUND_OBLG>
<FUND_OBLG>2012~68453</FUND_OBLG>
<FUND_OBLG>2013~96110</FUND_OBLG>
<FUND_OBLG>2014~90124</FUND_OBLG>
<FUND_OBLG>2015~88947</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main goal of this project has been to investigate a new generation of statistical motion models for Bayesian motion&nbsp;synthesis. Our research outcome will signigcantly advance the frontier of data-driven modeling by&nbsp;developing statistical models that are not only contact-aware but also suitable for large and heterogeneous datasets. In addition, we aim to develop new generative models that break the barriers between&nbsp;data-driven models and physical models. Such models will not only improve the generalization ability of statistical motion models but also enable us to generate physically realistic motions&nbsp;that react to external disturbances or changes in physical quantities of human bodies and interaction&nbsp;environments. Another goal of this project is to develop new motion generation algorithms that leverage&nbsp;the proposed generative models in graphics applications. The main outcomes of this project were published in eight SIGGRAPH/TOG papers and are summarized below.&nbsp;</p> <p>1). We showed how statistical motion priors can be combined seamlessly with physical constraints for human motion modeling and generation. The key idea of the approach is to learn a nonlinear probabilistic force field function from prerecorded motion data with Gaussian processes and combine it with physical constraints in a probabilistic framework. In addition, we showed how to effectively utilize the new model to generate a wide range of natural looking motions that achieve the goals specified by the users.&nbsp;</p> <p>2). &nbsp;We introduced a new generative statistical model that allows for human motion analysis and synthesis at both semantic and kinematic levels. Our key idea is to decouple complex variations of human movements into finite structural variations and continuous style variations and encode them with a concatenation of morphable functional models. We demonstrated the power and effectiveness of our models by exploring a wide variety of applications, ranging from automatic motion segmentation, recognition, and annotation, and online/offline motion synthesis at both kinematics and behavior levels to semantic motion editing.</p> <p>3). We introduced a novel solution for realtime generation of stylistic human motion that automatically transforms unlabeled, heterogeneous motion data into new styles. The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models (MAR) to capture the complex relationships between styles of motion. In addition, we introduced an efficient local regression model to predict the timings of synthesized poses in the output style.</p> <p>4).We introduced the first data-driven technique for solving full-body inverse dynamics problem. Comparisons against existing methods show our system achieves the state-of-the-art accuracy in full-body inverse dynamics. We also showed how to use our new data-driven inverse dynamics algorithm for improving efficiency of human motion editing and human motion control.</p> <p>5). We introduced a robust physics-based motion control system for realtime synthesis of human grasping. Our solution leverages prerecorded motion data and physics-based simulation for human grasping. We demonstrated the power of our approach by generating physics-based motion control for grasping objects with different properties such as shapes, weights, spatial orientations, and frictions.</p> <p>6).&nbsp;We presented a new method for acquiring physically realistic hand manipulation data from multiple video streams. The key idea of our approach is to introduce a composite motion control to simultaneously model hand articulation, object movement, and subtle interaction between the hand and object.&nbsp;We demonstrated the effectiveness of our approach by capturing a wide range of high-fidelity dexterous manipulation data. We showed the power of our recovered motion controllers by adapting the captured motion data to new objects with different properties.</p> <p>7).&nbsp;We presented a new method for full-body motion capture that uses input data captured by three depth cameras and a pair of pressuresensing shoes.&nbsp;In addition, we developed an efficient physics-based motion reconstruction algorithm for solving internal joint torques and contact forces in the quadratic programming framework.&nbsp;We demonstrated the power of our approach by capturing a wide range of human movements and achieve state-of-theart accuracy in our comparison against alternative systems.</p> <p>8). We presented a fast, automatic method for accurately capturing fullbody motion data using a single depth camera. At the core of our system lies a realtime registration process that accurately reconstructs 3D human poses from single monocular depth images, even in the case of significant occlusions.&nbsp;We demonstrated the power of our approach by capturing a wide range of human movements in real time and achieve state-ofthe-art accuracy in our comparison against alternative systems such as Kinect.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/30/2017<br>      Modified by: Jinxiang&nbsp;Chai</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498837853716_paper1--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498837853716_paper1--rgov-800width.jpg" title="Physically-Valid Statistical Models for Human Motion Generation"><img src="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498837853716_paper1--rgov-66x44.jpg" alt="Physically-Valid Statistical Models for Human Motion Generation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We introduced a new statistical  motion model that seamlessly  combines statistical motion priors with physical constraints for human motion modelingand generation. The paper is published in ACM Transactions on Graphics 2011 and presented in SIGGRAPH 2011</div> <div class="imageCredit">ACM Transactions on Graphics</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jinxiang&nbsp;Chai</div> <div class="imageTitle">Physically-Valid Statistical Models for Human Motion Generation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838237748_paper2--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838237748_paper2--rgov-800width.jpg" title="Motion Graphs++: a Compact Generative Model for Semantic Motion Analysis and Synthesis"><img src="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838237748_paper2--rgov-66x44.jpg" alt="Motion Graphs++: a Compact Generative Model for Semantic Motion Analysis and Synthesis"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We introduced a new generative statistical model (Motion graphs ++) that allowsfor human motion analysis and synthesis at both semantic andkinematic levels. This paper was published in ACM Transactions on Graphics 2012 (SIGGRAPH ASIA 2012).</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jinxiang&nbsp;Chai</div> <div class="imageTitle">Motion Graphs++: a Compact Generative Model for Semantic Motion Analysis and Synthesis</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838553831_paper3--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838553831_paper3--rgov-800width.jpg" title="Accurate Realtime Full-body Motion Capture Using a Single Depth Camera"><img src="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838553831_paper3--rgov-66x44.jpg" alt="Accurate Realtime Full-body Motion Capture Using a Single Depth Camera"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We presented a fast, automatic method for accurately capturing fullbody motion data using a single depth camera. This paper was published in ACM Transactions on Graphics (SIGGRAPH ASIA 2012)</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jinxiang&nbsp;Chai</div> <div class="imageTitle">Accurate Realtime Full-body Motion Capture Using a Single Depth Camera</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838704079_paper4--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838704079_paper4--rgov-800width.jpg" title="Video-based Hand Manipulation Capture Through Composite Motion Control"><img src="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838704079_paper4--rgov-66x44.jpg" alt="Video-based Hand Manipulation Capture Through Composite Motion Control"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We introduced a new method for acquiring physically realistichand manipulation data from multiple video streams. The paper was published in ACM Transactions on Graphics (SIGGRAPH 2013)</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jinxiang&nbsp;Chai</div> <div class="imageTitle">Video-based Hand Manipulation Capture Through Composite Motion Control</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838834434_paper5--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838834434_paper5--rgov-800width.jpg" title="Realtime Style Transfer for Unlabeled Heterogeneous Human Motion"><img src="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498838834434_paper5--rgov-66x44.jpg" alt="Realtime Style Transfer for Unlabeled Heterogeneous Human Motion"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We introduced a novel solution for realtime generation ofstylistic human motion that automatically transforms unlabeled,heterogeneous motion data into new styles. The paper was published in ACM Transactions on Graphics (SIGGRAPH 2016).</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jinxiang&nbsp;Chai</div> <div class="imageTitle">Realtime Style Transfer for Unlabeled Heterogeneous Human Motion</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498839076519_paper7--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498839076519_paper7--rgov-800width.jpg" title="Leveraging Depth Cameras and Wearable Pressure Sensors for Full-body Kinematics and Dynamics Capture"><img src="/por/images/Reports/POR/2017/1055046/1055046_10079939_1498839076519_paper7--rgov-66x44.jpg" alt="Leveraging Depth Cameras and Wearable Pressure Sensors for Full-body Kinematics and Dynamics Capture"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We inintroduced a new method for full-body motion capture that usesinput data captured by three depth cameras and a pair of pressure sensingshoes. The paper was published in ACM Transactions on Graphics (SIGGRAPH ASIA 2014)</div> <div class="imageCredit">ACM</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jinxiang&nbsp;Chai</div> <div class="imageTitle">Leveraging Depth Cameras and Wearable Pressure Sensors for Full-body Kinematics and Dynamics Capture</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main goal of this project has been to investigate a new generation of statistical motion models for Bayesian motion synthesis. Our research outcome will signigcantly advance the frontier of data-driven modeling by developing statistical models that are not only contact-aware but also suitable for large and heterogeneous datasets. In addition, we aim to develop new generative models that break the barriers between data-driven models and physical models. Such models will not only improve the generalization ability of statistical motion models but also enable us to generate physically realistic motions that react to external disturbances or changes in physical quantities of human bodies and interaction environments. Another goal of this project is to develop new motion generation algorithms that leverage the proposed generative models in graphics applications. The main outcomes of this project were published in eight SIGGRAPH/TOG papers and are summarized below.   1). We showed how statistical motion priors can be combined seamlessly with physical constraints for human motion modeling and generation. The key idea of the approach is to learn a nonlinear probabilistic force field function from prerecorded motion data with Gaussian processes and combine it with physical constraints in a probabilistic framework. In addition, we showed how to effectively utilize the new model to generate a wide range of natural looking motions that achieve the goals specified by the users.   2).  We introduced a new generative statistical model that allows for human motion analysis and synthesis at both semantic and kinematic levels. Our key idea is to decouple complex variations of human movements into finite structural variations and continuous style variations and encode them with a concatenation of morphable functional models. We demonstrated the power and effectiveness of our models by exploring a wide variety of applications, ranging from automatic motion segmentation, recognition, and annotation, and online/offline motion synthesis at both kinematics and behavior levels to semantic motion editing.  3). We introduced a novel solution for realtime generation of stylistic human motion that automatically transforms unlabeled, heterogeneous motion data into new styles. The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models (MAR) to capture the complex relationships between styles of motion. In addition, we introduced an efficient local regression model to predict the timings of synthesized poses in the output style.  4).We introduced the first data-driven technique for solving full-body inverse dynamics problem. Comparisons against existing methods show our system achieves the state-of-the-art accuracy in full-body inverse dynamics. We also showed how to use our new data-driven inverse dynamics algorithm for improving efficiency of human motion editing and human motion control.  5). We introduced a robust physics-based motion control system for realtime synthesis of human grasping. Our solution leverages prerecorded motion data and physics-based simulation for human grasping. We demonstrated the power of our approach by generating physics-based motion control for grasping objects with different properties such as shapes, weights, spatial orientations, and frictions.  6). We presented a new method for acquiring physically realistic hand manipulation data from multiple video streams. The key idea of our approach is to introduce a composite motion control to simultaneously model hand articulation, object movement, and subtle interaction between the hand and object. We demonstrated the effectiveness of our approach by capturing a wide range of high-fidelity dexterous manipulation data. We showed the power of our recovered motion controllers by adapting the captured motion data to new objects with different properties.  7). We presented a new method for full-body motion capture that uses input data captured by three depth cameras and a pair of pressuresensing shoes. In addition, we developed an efficient physics-based motion reconstruction algorithm for solving internal joint torques and contact forces in the quadratic programming framework. We demonstrated the power of our approach by capturing a wide range of human movements and achieve state-of-theart accuracy in our comparison against alternative systems.  8). We presented a fast, automatic method for accurately capturing fullbody motion data using a single depth camera. At the core of our system lies a realtime registration process that accurately reconstructs 3D human poses from single monocular depth images, even in the case of significant occlusions. We demonstrated the power of our approach by capturing a wide range of human movements in real time and achieve state-ofthe-art accuracy in our comparison against alternative systems such as Kinect.          Last Modified: 06/30/2017       Submitted by: Jinxiang Chai]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
