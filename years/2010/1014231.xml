<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I: Wearable Augmented Perception for Environmental Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>12/31/2010</AwardExpirationDate>
<AwardTotalIntnAmount>149939.00</AwardTotalIntnAmount>
<AwardAmount>149939</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This Small Business Innovation Research (SBIR) Phase I project focuses on reducing the footprint of machine vision and human interface solutions in support of a wearable apparatus that will improve environmental awareness of the visually impaired. Computer vision has provided significant capabilities in the robotics domain including object tracking, facial recognition, environmental localization, and hazard detection.  In the past, computer vision sensor/software systems have been slow, bulky, and power-hungry.  Recent advances in imaging hardware and embedded processing now provide an opportunity to shrink vision systems, including stereo vision and other complex operations, to a size that would allow them to be embedded in a wearable apparatus, similar to wraparound sunglasses. Using an auditory signal to feedback environmental information to the wearer, this device will provide valuable, and previously unimaginable, visual sensing capabilities to the visually impaired.  These include: 1) determining distance traveled even in GPS-denied environments; 2) detecting and classifying obstacles, drop-offs, overhangs, and other nearby hazards; and 3) detecting the presence and relative location of nearby people. &lt;br/&gt;&lt;br/&gt;The broader impact/commercial potential of this project will be a significant breakthrough in the compact combination of computer vision and human interface technologies.  The technology developed in this proposal has considerable impact for the visually impaired and strong commercial potential.  The needs of the visually impaired are not being met by existing technology.  The proposed technology will  increase the independence of the visually impaired and improve their quality of life, especially with respect to social interaction. The technology, produced initially to help the visually impaired, has the potential for a much broader scientific and commercial impact. Commercial potential for this product includes robotics, military ground forces, augmented reality, and surveillance.  The augmented reality market has particular broad impact beyond the visually impaired.  First responders such as fire fighters and police officers can receive additional information via a computer vision prosthetic that enhances their existing perception.  Additionally, there exists a strong demand by human interface researchers for this technology in a commercially available device.</AbstractNarration>
<MinAmdLetterDate>05/25/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/25/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1014231</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Huber</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric Huber</PI_FULL_NAME>
<EmailAddress>huber@traclabs.com</EmailAddress>
<PI_PHON>2814617884</PI_PHON>
<NSF_ID>000553462</NSF_ID>
<StartDate>05/25/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>TRACLabs Inc.</Name>
<CityName>San Antonio</CityName>
<ZipCode>782164727</ZipCode>
<PhoneNumber>2814617884</PhoneNumber>
<StreetAddress>100 N.E. Loop 510</StreetAddress>
<StreetAddress2><![CDATA[Suite 520]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193786014</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRACLABS INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>156753402</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[TRACLabs Inc.]]></Name>
<CityName>San Antonio</CityName>
<StateCode>TX</StateCode>
<ZipCode>782164727</ZipCode>
<StreetAddress><![CDATA[100 N.E. Loop 510]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9139</Code>
<Text>INFORMATION INFRASTRUCTURE &amp; TECH APPL</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~149939</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As processors, cameras, and displays become smaller and more energy efficient, people increasingly rely on mobile computers for organization, entertainment, and communication in their daily lives.These technologies also have the ability to change the daily lives for populations with low-vision, hearing loss, or cognitive disabilities.To that end, TRACLabs is developing the <em>WeaRable Augmented&nbsp;&nbsp;PercePtion for Environmental Recognition</em> system (WRAPPER for short). &nbsp;WRAPPER is a revolutionary computer vision system, embedded into the common wraparound sunglasses form factor, that provides timely information about the surrounding environment to individuals who have impaired vision.&nbsp;</p> <p>WRAPPER uses a collection of AI, machine learning, and computer vision algorithms for detection, tracking, and reasoning, and WRAPPER presents this information to the user. &nbsp;This information is provided via rich auditory profiles (for those with severely impaired vision) or via small, embedded LCD screens (for those with partial vision). In our preliminary work, we identified six distinct modes that should exist in the WRAPPER product:</p> <ol> <li><em>Local navigation assistance</em> relies on algorithms for&nbsp;&nbsp;detecting, tracking, and reasoning about obstacles, free corridors,&nbsp;&nbsp;doorways, intersections, drop-offs, and overhangs.</li> <li><em>Large-scale navigation assistance</em> provides distance&nbsp;&nbsp;traveled, localization, and beacon-following in a mapped environment&nbsp;&nbsp;(e.g., leading the user directly to Macy's in the mall).</li> <li><em>Social awareness</em> employs reliable person detection,&nbsp;&nbsp;tracking, and facial recognition. &nbsp;Gesture interpretation and&nbsp;&nbsp;analysis of social interactions can also be utilized here.</li> <li><em>General object recognition</em> uses 3D matching techniques for&nbsp;&nbsp;quickly determining generic items of interest to the user (e.g.,&nbsp;&nbsp;park benches, tables).</li> <li><em>Specific object identification</em> will also be integrated&nbsp;&nbsp;into the system. &nbsp;Items of particular interest to the user can be&nbsp;&nbsp;quickly identified (e.g., discriminating between a $1 and a $5&nbsp;&nbsp;bill or recognizing the user's dog at the dog park.)</li> <li><em>Generic vision processing</em> such as text-to-speech,&nbsp;&nbsp;magnification of images, color enhancement, or other 2D filters that&nbsp;&nbsp;help low-vision users will be&nbsp;included.</li> </ol> <p>&nbsp;</p> <p>In our preliminary work, we identified key technical challenges that we expect to encounter when designing the final WRAPPER system to provide the six functions listed above. &nbsp;The high-level challenges are:</p> <ul> <li>Miniaturization</li> <li>Efficient, real-time computation</li> <li>Intelligent perception</li> <li>Interfaces for low-vision users</li> <li>Scientifically grounded user studies</li> </ul> <p>These are not independent challenges. &nbsp;Miniaturized hardware and power sources dictate an upper bound on available memory and computation.This in turn affects how reliable real-time software algorithms are.The richness of perception that can be gleaned from the sensors affects the fidelity of information that can be conveyed to the user. In future work, we will seek to strike a balance; keeping hardware to a practical size while ensure that the user interface is sufficiently reactive and precise to be of high utility to the low-vision user.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/27/2011<br>      Modified by: Eric&nbsp;Huber</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As processors, cameras, and displays become smaller and more energy efficient, people increasingly rely on mobile computers for organization, entertainment, and communication in their daily lives.These technologies also have the ability to change the daily lives for populations with low-vision, hearing loss, or cognitive disabilities.To that end, TRACLabs is developing the WeaRable Augmented  PercePtion for Environmental Recognition system (WRAPPER for short).  WRAPPER is a revolutionary computer vision system, embedded into the common wraparound sunglasses form factor, that provides timely information about the surrounding environment to individuals who have impaired vision.   WRAPPER uses a collection of AI, machine learning, and computer vision algorithms for detection, tracking, and reasoning, and WRAPPER presents this information to the user.  This information is provided via rich auditory profiles (for those with severely impaired vision) or via small, embedded LCD screens (for those with partial vision). In our preliminary work, we identified six distinct modes that should exist in the WRAPPER product:  Local navigation assistance relies on algorithms for  detecting, tracking, and reasoning about obstacles, free corridors,  doorways, intersections, drop-offs, and overhangs. Large-scale navigation assistance provides distance  traveled, localization, and beacon-following in a mapped environment  (e.g., leading the user directly to Macy's in the mall). Social awareness employs reliable person detection,  tracking, and facial recognition.  Gesture interpretation and  analysis of social interactions can also be utilized here. General object recognition uses 3D matching techniques for  quickly determining generic items of interest to the user (e.g.,  park benches, tables). Specific object identification will also be integrated  into the system.  Items of particular interest to the user can be  quickly identified (e.g., discriminating between a $1 and a $5  bill or recognizing the user's dog at the dog park.) Generic vision processing such as text-to-speech,  magnification of images, color enhancement, or other 2D filters that  help low-vision users will be included.      In our preliminary work, we identified key technical challenges that we expect to encounter when designing the final WRAPPER system to provide the six functions listed above.  The high-level challenges are:  Miniaturization Efficient, real-time computation Intelligent perception Interfaces for low-vision users Scientifically grounded user studies   These are not independent challenges.  Miniaturized hardware and power sources dictate an upper bound on available memory and computation.This in turn affects how reliable real-time software algorithms are.The richness of perception that can be gleaned from the sensors affects the fidelity of information that can be conveyed to the user. In future work, we will seek to strike a balance; keeping hardware to a practical size while ensure that the user interface is sufficiently reactive and precise to be of high utility to the low-vision user.             Last Modified: 05/27/2011       Submitted by: Eric Huber]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
