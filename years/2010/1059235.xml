<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: CI-ADDO-EN: Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>66630.00</AwardTotalIntnAmount>
<AwardAmount>98630</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.&lt;br/&gt; &lt;br/&gt;The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. &lt;br/&gt;&lt;br/&gt;The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL.</AbstractNarration>
<MinAmdLetterDate>07/27/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/24/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1059235</AwardID>
<Investigator>
<FirstName>Vassilis</FirstName>
<LastName>Athitsos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vassilis Athitsos</PI_FULL_NAME>
<EmailAddress>athitsos@uta.edu</EmailAddress>
<PI_PHON>8172722105</PI_PHON>
<NSF_ID>000308836</NSF_ID>
<StartDate>07/27/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Arlington</Name>
<CityName>Arlington</CityName>
<ZipCode>760190145</ZipCode>
<PhoneNumber>8172722105</PhoneNumber>
<StreetAddress>701 S Nedderman Dr, Box 19145</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>064234610</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT ARLINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Arlington]]></Name>
<CityName>Arlington</CityName>
<StateCode>TX</StateCode>
<ZipCode>760190145</ZipCode>
<StreetAddress><![CDATA[701 S Nedderman Dr, Box 19145]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~66630</FUND_OBLG>
<FUND_OBLG>2012~16000</FUND_OBLG>
<FUND_OBLG>2013~8000</FUND_OBLG>
<FUND_OBLG>2014~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>INTELLECTUAL MERIT:</p> <p><br />Sign language recognition using computer vision methods is an important but difficult problem. Solving this problem would allow researchers to create real world systems that help in various ways users and learners of a sign language. For example, we could have applications that &nbsp;translate between a sign language and a spoken language, or applications that identify student mistakes in signing and help students perform signs correctly.</p> <p>One major bottleneck that slows down progress in this field is the lack of large amounts of training data. The algorithms that we design for detecting body parts in video, and for recognizing signs based on the shape and motion of these body parts, need large amounts of training examples. Such training examples are needed in order to learn the differences among thousands of different signs, and the various ways in which different signers can perform the same sign.</p> <p>The main goal of this project has been to provide a large dataset of sign language videos to researchers around the world, and to provide interfaces that make it easy for researchers to identify and download the parts of the dataset that they are interested in. This has been a collaborative project, with the participation of Boston University, Rutgers University, Gallaudet University, and UT Arlington (UTA). At UTA we have contributed to this project in two ways:</p> <p>First, we have manually annotated thousands of videos of signs, by marking the location of the hands at each frame of each video. These annotations are a useful resource for researchers developing new methods for detecting hands and other body parts in videos, as well as for researchers developing methods for recognizing gestures and signs. Our project website ( http://vlm1.uta.edu/~athitsos/asl_lexicon/ ) offers annotations for about 60,000 video frames, and these annotations are available to download for anyone interested in using them.</p> <p>We have also developed methods that allow users to look up information about a sign of interest by providing as input a video of that sign. As an example application of such methods, a student that encounters an unknown sign, can perform that sign in front of a camera, and the system can provide information about the meaning of that sign. Our methods, while still far from being as accurate as humans for this task, have significantly improved in accuracy during the period of this project.</p> <p>BROADER IMPACTS:</p> <p>The annotations we have produced help computer vision researchers develop new and better methods for sign language recognition. Such methods, when mature enough, will provide the Deaf community with useful applications. One such application is translation to English, so as to facilitate communication with people not using a sign language. Another such application could be a search engine for sign language videos, allowing users to find sign language videos of interest on the web, in the same way that users of English find web content of interest by typing in a few words on a search engine.</p> <p>The project has also supported participation in research for two doctoral students and two undergraduate students. These students obtained hands-on experience in developing computer vision systems and designing new computer vision methods. Furthermore, the data and annotations that we have produced has been used for class projects in computer science classes at UTA. The data is now available to students and instructors around the world, for similar usage in coursework and student research projects.<br /><br /></p><br> <p>            Last Modified: 11/04/2015<br>      Modified by: Vassilis&nbsp;Athitsos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ INTELLECTUAL MERIT:   Sign language recognition using computer vision methods is an important but difficult problem. Solving this problem would allow researchers to create real world systems that help in various ways users and learners of a sign language. For example, we could have applications that  translate between a sign language and a spoken language, or applications that identify student mistakes in signing and help students perform signs correctly.  One major bottleneck that slows down progress in this field is the lack of large amounts of training data. The algorithms that we design for detecting body parts in video, and for recognizing signs based on the shape and motion of these body parts, need large amounts of training examples. Such training examples are needed in order to learn the differences among thousands of different signs, and the various ways in which different signers can perform the same sign.  The main goal of this project has been to provide a large dataset of sign language videos to researchers around the world, and to provide interfaces that make it easy for researchers to identify and download the parts of the dataset that they are interested in. This has been a collaborative project, with the participation of Boston University, Rutgers University, Gallaudet University, and UT Arlington (UTA). At UTA we have contributed to this project in two ways:  First, we have manually annotated thousands of videos of signs, by marking the location of the hands at each frame of each video. These annotations are a useful resource for researchers developing new methods for detecting hands and other body parts in videos, as well as for researchers developing methods for recognizing gestures and signs. Our project website ( http://vlm1.uta.edu/~athitsos/asl_lexicon/ ) offers annotations for about 60,000 video frames, and these annotations are available to download for anyone interested in using them.  We have also developed methods that allow users to look up information about a sign of interest by providing as input a video of that sign. As an example application of such methods, a student that encounters an unknown sign, can perform that sign in front of a camera, and the system can provide information about the meaning of that sign. Our methods, while still far from being as accurate as humans for this task, have significantly improved in accuracy during the period of this project.  BROADER IMPACTS:  The annotations we have produced help computer vision researchers develop new and better methods for sign language recognition. Such methods, when mature enough, will provide the Deaf community with useful applications. One such application is translation to English, so as to facilitate communication with people not using a sign language. Another such application could be a search engine for sign language videos, allowing users to find sign language videos of interest on the web, in the same way that users of English find web content of interest by typing in a few words on a search engine.  The project has also supported participation in research for two doctoral students and two undergraduate students. These students obtained hands-on experience in developing computer vision systems and designing new computer vision methods. Furthermore, the data and annotations that we have produced has been used for class projects in computer science classes at UTA. The data is now available to students and instructors around the world, for similar usage in coursework and student research projects.         Last Modified: 11/04/2015       Submitted by: Vassilis Athitsos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
