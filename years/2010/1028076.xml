<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDI-Type II: Computational Tools for Behavioral Analysis, Diagnosis, and Intervention of at Risk Children</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>1498897.00</AwardTotalIntnAmount>
<AwardAmount>1578897</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04010000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SMA</Abbreviation>
<LongName>SBE Off Of Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Fahmida Chowdhury</SignBlockName>
<PO_EMAI>fchowdhu@nsf.gov</PO_EMAI>
<PO_PHON>7032924672</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will develop algorithms to assist with the early diagnosis of children who are at risk of developing behavioral disorders.  Previous research has indicated that two critical areas of behavioral investigation for use in identifying at-risk children have been abnormalities in motor activities and emotional range displays, especially of the face. Motor abnormalities are based on the observation that motor control involves the circuits of the brain associated with dopamine; these are also implicated in behavioral disorders. Many different disorders share the observation of disruption in the emotional range regulation, so  facial expressions are included in the study.&lt;br/&gt;&lt;br/&gt;To date, assessments of motor and emotional range have been done by the experts  who view and rate videos of an individual. However, these expert, subjective ratings limit the analysis of behavioral conditions to only a narrow range of behaviors, work only for small populations of individual subjects, and are both costly and  dependent on the observer's particular expertise.  In order to enable wider population screening, automation is required. Innovative ways of capturing and quantifying the expertise of experts will be accompanied by metrics for assessing the evolution of the behavior. In addition, new computational tools will support evaluation of the effectiveness of interventions.&lt;br/&gt;&lt;br/&gt;The broader impacts of the proposed work will involve improved mental health levels across the populations by providing a systematic approach for enhancing early detection, prevention, or mitigation of behavioral disorders and likely reduce the long-term costs of missed or late diagnosis.  The research results will be blended with the educational process through inclusion of project themes in the curricula at the Institute of Technology, the Medical School, and the College of Education and Human Development at the University of Minnesota and the creation of a program with annual workshops, tutorials, web pages and a wiki on knowledge discovery and behavioral analysis.  The team will develop an interactive exhibit for children at the Bakken Museum, and create of new instructional material for student teachers at the Institute of Child Development and similar institutions. Development of a central web repository will insure that the algorithms and the data will be readily available for appropriate research.</AbstractNarration>
<MinAmdLetterDate>09/02/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1028076</AwardID>
<Investigator>
<FirstName>Nikolaos</FirstName>
<LastName>Papanikolopoulos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nikolaos Papanikolopoulos</PI_FULL_NAME>
<EmailAddress>npapas@cs.umn.edu</EmailAddress>
<PI_PHON>6126250163</PI_PHON>
<NSF_ID>000205563</NSF_ID>
<StartDate>09/02/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Guillermo</FirstName>
<LastName>Sapiro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guillermo Sapiro</PI_FULL_NAME>
<EmailAddress>guillermo.sapiro@duke.edu</EmailAddress>
<PI_PHON>9196813675</PI_PHON>
<NSF_ID>000107316</NSF_ID>
<StartDate>09/02/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kelvin</FirstName>
<LastName>Lim</LastName>
<PI_MID_INIT>O</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kelvin O Lim</PI_FULL_NAME>
<EmailAddress>kolim@umn.edu</EmailAddress>
<PI_PHON>6126255000</PI_PHON>
<NSF_ID>000229279</NSF_ID>
<StartDate>09/02/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Arindam</FirstName>
<LastName>Banerjee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arindam Banerjee</PI_FULL_NAME>
<EmailAddress>arindamb@illinois.edu</EmailAddress>
<PI_PHON>2173333426</PI_PHON>
<NSF_ID>000491325</NSF_ID>
<StartDate>11/22/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 OAK ST SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1139</Code>
<Text>RSCH EXPER FOR UNDERGRAD SITES</Text>
</ProgramElement>
<ProgramElement>
<Code>1331</Code>
<Text>Sociology</Text>
</ProgramElement>
<ProgramElement>
<Code>1397</Code>
<Text>Cross-Directorate  Activities</Text>
</ProgramElement>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7721</Code>
<Text>FROM DATA TO KNOWLEDGE</Text>
</ProgramReference>
<ProgramReference>
<Code>7752</Code>
<Text>CDI NON SOLICITED RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9250</Code>
<Text>REU SITE-Res Exp for Ugrd Site</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~1498897</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<FUND_OBLG>2012~16000</FUND_OBLG>
<FUND_OBLG>2013~16000</FUND_OBLG>
<FUND_OBLG>2014~16000</FUND_OBLG>
<FUND_OBLG>2015~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project&nbsp; developed algorithms to assist with the early diagnosis of children who are at risk of developing behavioral disorders. Previous research has indicated that two critical areas of behavioral investigation for use in identifying at-risk children have been abnormalities in motor activities and emotional range displays, especially of the face. Motor abnormalities are based on the observation that motor control involves the circuits of the brain associated with dopamine; these are also implicated in behavioral disorders. Many different disorders share the observation of disruption in the emotional range regulation, so facial expressions were included in the study.<br /><br />To date, assessments of motor and emotional range have been done by the experts who view and rate videos of an individual. However, these expert, subjective ratings limit the analysis of behavioral conditions to only a narrow range of behaviors, work only for small populations of individual subjects, and are both costly and dependent on the observer's particular expertise. In order to enable wider population screening, automation is required. Innovative ways of capturing and quantifying the expertise of experts are accompanied by metrics for assessing the evolution of the behavior. In addition, new computational tools support evaluation of the effectiveness of interventions.</p> <p>&nbsp;</p> <p>This study also tried to automate the detection and monitoring of &nbsp;motor stereotypies. In psychiatric literature, the term motor stereotypies typically&nbsp;describes a class of actions performed by both normally&nbsp;developing and at-risk children which are: purposeless, repetitive,&nbsp;and suppressible. While these behaviors occur in both&nbsp;normal and at-risk populations they are still of interest to the&nbsp;psychiatric community, as better understanding is essential&nbsp;for early diagnosis and treatment.&nbsp;</p> <p>Studies have shown that &nbsp;non-continuous methods have&nbsp;been known to vary in accuracy over continuous observation. These direct observational methods are meant to&nbsp;understand how often stereotypic episodes occur, how much&nbsp;time they occupy, and what evokes them in the afflicted.&nbsp;Applying computer vision as an observational aid can assist&nbsp;in answering these questions. In order to answer these&nbsp;questions using computer vision, we developed methods that do: identification of interest&nbsp;points, characterization of motion features, and an approach&nbsp;for the analysis of motion features.</p> <p>Inspired by static image analysis, in video there has been&nbsp;an effort to reduce the amount of information required for&nbsp;analysis down to interesting or salient features. For videos,&nbsp;this raw information is often considered to be a spatiotemporal&nbsp;volume constructed by sequentially stacking images&nbsp;from a video across time. One popular approach to identify&nbsp;salient features, &nbsp;finds space-time interest points using a&nbsp;3D version of the Harris corner detector at different scales&nbsp;along a spatio-temporal volume.</p> <p>After determining a reduced set of interesting space-time&nbsp;points in a video sequence, different methods have been&nbsp;employed to describe the appearance and motion present at&nbsp;or around these interesting points. &nbsp;Our work&nbsp;explores approaches for classifying&nbsp;different motor stereotypies as they appear in video&nbsp;sequences. A view-invariant feature is used which is&nbsp;amenable for use in conjunction with multi-view tracking&nbsp;systems. As a comparison, a state-of-the-art feature description&nbsp;intended for single views is also introduced.</p> <p>Two methods for classifying the presented feature descriptions&nbsp;for video are compared. &nbsp;Experiments were performed on a dataset that consists of&nbsp;videos recorded in a pre-school classroom in order to test&nbsp;the performance of the various methods.&nbsp;The videos were recorded with IRB approval at the Shirley&nbsp;G. Moore Laboratory School. Since the occurrence of the behaviors&nbsp;intended for classification can be rare even in a clinical&nbsp;setting, a &ldquo;Simon Says&rdquo;-like mimicry game was employed to &nbsp;elicit behaviors in normal children that approximate motor&nbsp;stereotypies. The game consists of a leader who performs a&nbsp;particular action and child participants who are instructed to&nbsp;mimic the actions performed by the leader. Using different visual features, we tried to evaluate their effectiveness. &nbsp;</p> <p>We also worked on low-cost screening for autism. In spite of recent advances in the genetics&nbsp;and neuroscience of early childhood mental health, behavioral observation is&nbsp;still the gold standard in screening, diagnosis, and outcome assessment.&nbsp;Unfortunately, clinical observation is often subjective, needs signicant&nbsp;rater training, does not capture data from participants in their natural environment, and is not scalable for use in large populations or for&nbsp;longitudinal monitoring. To address these challenges, we developed and&nbsp;tested a self-contained app designed to measure toddlers' social communication behaviors in a primary care, school, or home setting. 16-30&nbsp;month old children with and without autism participated in this study.&nbsp;Toddlers watched the developmentally-appropriate visual stimuli on an iPad&nbsp;in a pediatric clinic and in our lab while the iPad camera simultaneously&nbsp;recorded video of the child's behaviors. Automated computer vision&nbsp;algorithms coded emotions and social referencing to quantify autism risk&nbsp;behaviors. We validated our automatic computer coding by comparing the&nbsp;computer-generated analysis of facial expression and social referencing to&nbsp;human coding of these behaviors.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/29/2016<br>      Modified by: Nikolaos&nbsp;Papanikolopoulos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project  developed algorithms to assist with the early diagnosis of children who are at risk of developing behavioral disorders. Previous research has indicated that two critical areas of behavioral investigation for use in identifying at-risk children have been abnormalities in motor activities and emotional range displays, especially of the face. Motor abnormalities are based on the observation that motor control involves the circuits of the brain associated with dopamine; these are also implicated in behavioral disorders. Many different disorders share the observation of disruption in the emotional range regulation, so facial expressions were included in the study.  To date, assessments of motor and emotional range have been done by the experts who view and rate videos of an individual. However, these expert, subjective ratings limit the analysis of behavioral conditions to only a narrow range of behaviors, work only for small populations of individual subjects, and are both costly and dependent on the observer's particular expertise. In order to enable wider population screening, automation is required. Innovative ways of capturing and quantifying the expertise of experts are accompanied by metrics for assessing the evolution of the behavior. In addition, new computational tools support evaluation of the effectiveness of interventions.     This study also tried to automate the detection and monitoring of  motor stereotypies. In psychiatric literature, the term motor stereotypies typically describes a class of actions performed by both normally developing and at-risk children which are: purposeless, repetitive, and suppressible. While these behaviors occur in both normal and at-risk populations they are still of interest to the psychiatric community, as better understanding is essential for early diagnosis and treatment.   Studies have shown that  non-continuous methods have been known to vary in accuracy over continuous observation. These direct observational methods are meant to understand how often stereotypic episodes occur, how much time they occupy, and what evokes them in the afflicted. Applying computer vision as an observational aid can assist in answering these questions. In order to answer these questions using computer vision, we developed methods that do: identification of interest points, characterization of motion features, and an approach for the analysis of motion features.  Inspired by static image analysis, in video there has been an effort to reduce the amount of information required for analysis down to interesting or salient features. For videos, this raw information is often considered to be a spatiotemporal volume constructed by sequentially stacking images from a video across time. One popular approach to identify salient features,  finds space-time interest points using a 3D version of the Harris corner detector at different scales along a spatio-temporal volume.  After determining a reduced set of interesting space-time points in a video sequence, different methods have been employed to describe the appearance and motion present at or around these interesting points.  Our work explores approaches for classifying different motor stereotypies as they appear in video sequences. A view-invariant feature is used which is amenable for use in conjunction with multi-view tracking systems. As a comparison, a state-of-the-art feature description intended for single views is also introduced.  Two methods for classifying the presented feature descriptions for video are compared.  Experiments were performed on a dataset that consists of videos recorded in a pre-school classroom in order to test the performance of the various methods. The videos were recorded with IRB approval at the Shirley G. Moore Laboratory School. Since the occurrence of the behaviors intended for classification can be rare even in a clinical setting, a "Simon Says"-like mimicry game was employed to  elicit behaviors in normal children that approximate motor stereotypies. The game consists of a leader who performs a particular action and child participants who are instructed to mimic the actions performed by the leader. Using different visual features, we tried to evaluate their effectiveness.    We also worked on low-cost screening for autism. In spite of recent advances in the genetics and neuroscience of early childhood mental health, behavioral observation is still the gold standard in screening, diagnosis, and outcome assessment. Unfortunately, clinical observation is often subjective, needs signicant rater training, does not capture data from participants in their natural environment, and is not scalable for use in large populations or for longitudinal monitoring. To address these challenges, we developed and tested a self-contained app designed to measure toddlers' social communication behaviors in a primary care, school, or home setting. 16-30 month old children with and without autism participated in this study. Toddlers watched the developmentally-appropriate visual stimuli on an iPad in a pediatric clinic and in our lab while the iPad camera simultaneously recorded video of the child's behaviors. Automated computer vision algorithms coded emotions and social referencing to quantify autism risk behaviors. We validated our automatic computer coding by comparing the computer-generated analysis of facial expression and social referencing to human coding of these behaviors.           Last Modified: 11/29/2016       Submitted by: Nikolaos Papanikolopoulos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>