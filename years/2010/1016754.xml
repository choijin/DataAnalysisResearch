<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Active Learning of Language Models for Information Extraction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>183736.00</AwardTotalIntnAmount>
<AwardAmount>183736</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project studies methods for extracting accurate knowledge bases from the &lt;br/&gt;Web.  Fully-automated Web information extraction techniques are massively &lt;br/&gt;scalable, but have accuracy and coverage limitations.  This proposal &lt;br/&gt;investigates how to improve automated extraction techniques by introducing &lt;br/&gt;carefully-selected human guidance.  The proposed system continually extracts &lt;br/&gt;knowledge from the Web, along the way dynamically synthesizing and issuing &lt;br/&gt;queries to humans to increase the accuracy of the system's knowledge base and &lt;br/&gt;extractors.&lt;br/&gt;&lt;br/&gt;The approach extends the PI's previous work utilizing statistical language &lt;br/&gt;models (SLMs) for information extraction.  Novel SLMs are investigated for &lt;br/&gt;unifying the extraction of relational data expressed in Web tables with &lt;br/&gt;extraction from free text.  New active learning techniques utilize the models &lt;br/&gt;to identify "high-leverage" queries -- requesting, for example, textual &lt;br/&gt;extraction patterns that when retrieved from the Web yield thousands of novel &lt;br/&gt;extractions.  The queries investigated are mostly amenable to non-experts, &lt;br/&gt;meaning that much of the human input can be acquired at scale via online &lt;br/&gt;mass-collaboration.&lt;br/&gt;&lt;br/&gt;The broader impact of this project lies in the potential for accurate Web &lt;br/&gt;extraction to radically improve Web search, allowing users to answer &lt;br/&gt;complicated questions by synthesizing information across multiple Web pages.  &lt;br/&gt;In domains like medicine and biology, mining extracted knowledge bases could &lt;br/&gt;lead to important discoveries and novel therapies.&lt;br/&gt;&lt;br/&gt;Further information may be found at the project web page:&lt;br/&gt;http://wail.eecs.northwestern.edu/projects/activelms/index.html</AbstractNarration>
<MinAmdLetterDate>08/01/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016754</AwardID>
<Investigator>
<FirstName>Douglas</FirstName>
<LastName>Downey</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Douglas C Downey</PI_FULL_NAME>
<EmailAddress>dougd@allenai.org</EmailAddress>
<PI_PHON>8474913710</PI_PHON>
<NSF_ID>000534948</NSF_ID>
<StartDate>08/01/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~183736</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project studied methods for automatically extracting knowledge bases from the World Wide Web.&nbsp; The goal behind our work is to transform the Web&rsquo;s vast human-readable content into machine-understandable knowledge.&nbsp; This capability would enable transformative technologies, such as new search engines that answer complex questions by synthesizing information scattered across the Web.</p> <p>We focused on three primary research questions:</p> <ul> <li>How can we&nbsp;<strong>integrate</strong>&nbsp;knowledge extracted from both Web text and Web tables?</li> <li>How can&nbsp;<strong>statistical language models</strong>&nbsp;trained over large text corpora help improve extraction accuracy?</li> <li>How can an extraction system actively solicit&nbsp;<strong>well-selected human input</strong>&nbsp;to improve the extraction process?</li> </ul> <p>The project led to the invention of new knowledge extraction techniques, primarily aimed at Wikipedia&rsquo;s text and data tables.&nbsp; A fundamental knowledge extraction challenge involves automatically identifying relationships between concepts. &nbsp;We developed state-of-the-art methods for estimating the degree of semantic relatedness (SR) between two Wikipedia concepts, along with new methods for explaining the relationships to Web users in natural language.&nbsp; These methods leveraged machine learning techniques to mine Wikipedia&rsquo;s text, hyperlinks, and categories for semantic information. &nbsp;We also developed new techniques for extracting data from Wikipedia tables and automatically joining together different tables that contain related information.</p> <p>We also developed new methods for scaling up statistical language models (SLMs) for information extraction.&nbsp; &ldquo;Latent-variable&rdquo; SLMs have been shown to improve extraction systems, but the memory required to train the models forms a bottleneck.&nbsp; We developed a new method for overcoming the memory bottleneck, based on intelligently partitioning the corpus across a parallel computing cluster.&nbsp; Our experiments showed that the partitioning method decreases the memory footprint of model training by half for large data sets.</p> <p>The broader impacts of our work included student training, public prototype applications, and the release of data sets and code to the research community.&nbsp; Multiple PhD, MS, and undergraduate students participated in our research and co-authored publications.&nbsp; We also delivered a public prototype demonstrating our table extraction research, called &ldquo;WikiTables.&rdquo;&nbsp; An additional public prototype of the &ldquo;Atlasify&rdquo; system, which uses our semantic relatedness research to create interactive visualizations query concepts (e.g. &ldquo;nuclear power&rdquo;) on familiar reference systems (e.g. the World Map or periodic table), is under development.&nbsp; We disseminated our work to the research community in the form of multiple papers at major conferences and workshops, and we released other resources (including a codebase for our SLM training technique, new datasets for SR and table extraction, and a scalable public API for computing SR).&nbsp; The papers, prototypes, and other research products are publicly available.&nbsp; For further information, please consult the project Web site: <a href="http://websail.cs.northwestern.edu/activelms/">http://websail.cs.northwestern.edu/activelms/</a></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/17/2013<br>      Modified by: Douglas&nbsp;C&nbsp;Downey</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="g...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project studied methods for automatically extracting knowledge bases from the World Wide Web.  The goal behind our work is to transform the WebÆs vast human-readable content into machine-understandable knowledge.  This capability would enable transformative technologies, such as new search engines that answer complex questions by synthesizing information scattered across the Web.  We focused on three primary research questions:  How can we integrate knowledge extracted from both Web text and Web tables? How can statistical language models trained over large text corpora help improve extraction accuracy? How can an extraction system actively solicit well-selected human input to improve the extraction process?   The project led to the invention of new knowledge extraction techniques, primarily aimed at WikipediaÆs text and data tables.  A fundamental knowledge extraction challenge involves automatically identifying relationships between concepts.  We developed state-of-the-art methods for estimating the degree of semantic relatedness (SR) between two Wikipedia concepts, along with new methods for explaining the relationships to Web users in natural language.  These methods leveraged machine learning techniques to mine WikipediaÆs text, hyperlinks, and categories for semantic information.  We also developed new techniques for extracting data from Wikipedia tables and automatically joining together different tables that contain related information.  We also developed new methods for scaling up statistical language models (SLMs) for information extraction.  "Latent-variable" SLMs have been shown to improve extraction systems, but the memory required to train the models forms a bottleneck.  We developed a new method for overcoming the memory bottleneck, based on intelligently partitioning the corpus across a parallel computing cluster.  Our experiments showed that the partitioning method decreases the memory footprint of model training by half for large data sets.  The broader impacts of our work included student training, public prototype applications, and the release of data sets and code to the research community.  Multiple PhD, MS, and undergraduate students participated in our research and co-authored publications.  We also delivered a public prototype demonstrating our table extraction research, called "WikiTables."  An additional public prototype of the "Atlasify" system, which uses our semantic relatedness research to create interactive visualizations query concepts (e.g. "nuclear power") on familiar reference systems (e.g. the World Map or periodic table), is under development.  We disseminated our work to the research community in the form of multiple papers at major conferences and workshops, and we released other resources (including a codebase for our SLM training technique, new datasets for SR and table extraction, and a scalable public API for computing SR).  The papers, prototypes, and other research products are publicly available.  For further information, please consult the project Web site: http://websail.cs.northwestern.edu/activelms/          Last Modified: 10/17/2013       Submitted by: Douglas C Downey]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
