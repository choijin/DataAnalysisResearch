<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Technology Audit and Insertion Service for TeraGrid</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>7763246.00</AwardTotalIntnAmount>
<AwardAmount>8054145</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Edward Walker</SignBlockName>
<PO_EMAI>edwalker@nsf.gov</PO_EMAI>
<PO_PHON>7032924863</PO_PHON>
</ProgramOfficer>
<AbstractNarration>1025159&lt;br/&gt;Furlani&lt;br/&gt;This five year award is to provide a technology audit service for the eXtremeDigital (XD) program, the follow on to the successful NSF TeraGrid program. The technology audit service is designed to; continually test the user environment and capabilities provided by XD to ensure delivery of the highest possible quality of service, to provide internal quality assurance and quality control for XD, measuring quantitative and qualitative metrics of quality of service and to periodically report to the coordinating body for the XD. The award provides the objective metrics of XD quality of service that will be reviewed and revised by the XD in consultation with NSF as the technology evolves.  The technology audit service will have user-level access to all XD computational, storage and visualization services and will use these for testing advanced software in partnership with the relevant service providers.</AbstractNarration>
<MinAmdLetterDate>06/23/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/04/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1025159</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Furlani</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas R Furlani</PI_FULL_NAME>
<EmailAddress>furlani@buffalo.edu</EmailAddress>
<PI_PHON>7168818939</PI_PHON>
<NSF_ID>000146269</NSF_ID>
<StartDate>06/23/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Vipin</FirstName>
<LastName>Chaudhary</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vipin Chaudhary</PI_FULL_NAME>
<EmailAddress>vipin@case.edu</EmailAddress>
<PI_PHON>2163680171</PI_PHON>
<NSF_ID>000378463</NSF_ID>
<StartDate>06/23/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gregor</FirstName>
<LastName>von Laszewski</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregor von Laszewski</PI_FULL_NAME>
<EmailAddress>gvonlasz@indiana.edu</EmailAddress>
<PI_PHON>8128561311</PI_PHON>
<NSF_ID>000275379</NSF_ID>
<StartDate>06/23/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Jones</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew D Jones</PI_FULL_NAME>
<EmailAddress>jonesm@ccr.buffalo.edu</EmailAddress>
<PI_PHON>7168818958</PI_PHON>
<NSF_ID>000170376</NSF_ID>
<StartDate>06/23/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Green</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark L Green</PI_FULL_NAME>
<EmailAddress>mlgreen@txcorp.com</EmailAddress>
<PI_PHON>7166343834</PI_PHON>
<NSF_ID>000072385</NSF_ID>
<StartDate>06/23/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Buffalo</Name>
<CityName>Buffalo</CityName>
<ZipCode>142282567</ZipCode>
<PhoneNumber>7166452634</PhoneNumber>
<StreetAddress>520 Lee Entrance</StreetAddress>
<StreetAddress2><![CDATA[Suite 211]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY26</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>038633251</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Buffalo]]></Name>
<CityName>Buffalo</CityName>
<StateCode>NY</StateCode>
<ZipCode>142282567</ZipCode>
<StreetAddress><![CDATA[520 Lee Entrance]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY26</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>7476</Code>
<Text>XD-Extreme Digital</Text>
</ProgramElement>
<ProgramReference>
<Code>7476</Code>
<Text>ETF</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~1549127</FUND_OBLG>
<FUND_OBLG>2011~6228419</FUND_OBLG>
<FUND_OBLG>2012~14000</FUND_OBLG>
<FUND_OBLG>2013~14000</FUND_OBLG>
<FUND_OBLG>2014~248599</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>High performance computing (HPC) systems, more commonly known as supercomputers, play a pivotal rule in society today, including the U.S. economy.&nbsp; They are essential tools in a diverse range of areas including finance, oil and gas exploration, pharmaceutical drug design, medical and basic research, computer animation, aeronautics, and automotive design to name a few.&nbsp; Today&rsquo;s supercomputers are a complex combination of computer hardware (servers, network switches, storage) and software, and it is important that system support personnel have at their disposal tools to ensure that this complex infrastructure is running with optimal efficiency as well as the ability to proactively identify underperforming hardware and software.&nbsp; In addition, most HPC systems are overloaded, with many jobs queued waiting to run, and accordingly system support personnel desire the capability to monitor and analyze all end-user jobs to determine how efficiently they are running and what resources they are consuming (computer memory, processing, storage, networking, etc.) in order to optimize the number of computations run as well as plan for future needs.&nbsp;</p> <p>Given the important role that high performance computers play in research and the economy, it is somewhat surprising that, prior to this project, no open source tools were available that provided for the comprehensive management of HPC systems. With this deficiency in mind, XDMoD was developed to provide a comprehensive management framework for the NSF&rsquo;s high performance computing systems that are managed through the XSEDE program.&nbsp; In addition, the closely related Open XDMoD, an open source tool, provides similar functionality for HPC systems in general including government, industrial and academic HPC centers as well as Blue Waters &ndash; the largest supercomputer in the NSF portfolio. &nbsp;XDMoD for XSEDE and Open XDMoD were designed to meet the following objectives:</p> <p>(1)&nbsp;&nbsp;&nbsp; provide the end-user community with a tool to optimize their use of HPC resources,</p> <p>(2)&nbsp;&nbsp;&nbsp; provide operational staff with the ability to monitor, diagnose, and tune system performance as well as measure the performance of all applications running on the HPC systems they manage,</p> <p>(3)&nbsp;&nbsp;&nbsp; provide software developers with the ability to easily obtain detailed analysis of application performance to aid in optimizing code performance,</p> <p>(4)&nbsp;&nbsp;&nbsp; provide stakeholders with a diagnostic tool to facilitate HPC planning and analysis, and</p> <p>(5)&nbsp;&nbsp;&nbsp; provide metrics to help measure return on investment.</p> <p>XDMoD&nbsp; provides a rich set of features accessible through an intuitive graphical interface, which is tailored to the role of the user, from scientists and engineers running computations to HPC facility and funding agency managers. Metrics provided by XDMoD include comprehensive statistics on: number and type of computational jobs run, resources (computation, memory, disk, network, etc.) consumed, job wait and wall time, scientific impact, and quality of service. &nbsp;The web interface is intuitive, allowing one to chart various metrics and interactively drill down to access additional related information.</p> <p>The XDMoD framework is also designed to help ensure that the HPC infrastructure is delivering a high quality of service to its end-users by continuously monitoring system performance and reliability through the deployment of a series of programs specifically designed to monitor overall system performance. System managers are therefore able to proactively monitor the HPC infrastructure as opposed to having to rely on users to report failures or underperforming hardware and software.</p> <p>An important capability of XDMoD is centered around monitoring the performance of all user jobs running on a given HPC resour...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ High performance computing (HPC) systems, more commonly known as supercomputers, play a pivotal rule in society today, including the U.S. economy.  They are essential tools in a diverse range of areas including finance, oil and gas exploration, pharmaceutical drug design, medical and basic research, computer animation, aeronautics, and automotive design to name a few.  TodayÆs supercomputers are a complex combination of computer hardware (servers, network switches, storage) and software, and it is important that system support personnel have at their disposal tools to ensure that this complex infrastructure is running with optimal efficiency as well as the ability to proactively identify underperforming hardware and software.  In addition, most HPC systems are overloaded, with many jobs queued waiting to run, and accordingly system support personnel desire the capability to monitor and analyze all end-user jobs to determine how efficiently they are running and what resources they are consuming (computer memory, processing, storage, networking, etc.) in order to optimize the number of computations run as well as plan for future needs.   Given the important role that high performance computers play in research and the economy, it is somewhat surprising that, prior to this project, no open source tools were available that provided for the comprehensive management of HPC systems. With this deficiency in mind, XDMoD was developed to provide a comprehensive management framework for the NSFÆs high performance computing systems that are managed through the XSEDE program.  In addition, the closely related Open XDMoD, an open source tool, provides similar functionality for HPC systems in general including government, industrial and academic HPC centers as well as Blue Waters &ndash; the largest supercomputer in the NSF portfolio.  XDMoD for XSEDE and Open XDMoD were designed to meet the following objectives:  (1)    provide the end-user community with a tool to optimize their use of HPC resources,  (2)    provide operational staff with the ability to monitor, diagnose, and tune system performance as well as measure the performance of all applications running on the HPC systems they manage,  (3)    provide software developers with the ability to easily obtain detailed analysis of application performance to aid in optimizing code performance,  (4)    provide stakeholders with a diagnostic tool to facilitate HPC planning and analysis, and  (5)    provide metrics to help measure return on investment.  XDMoD  provides a rich set of features accessible through an intuitive graphical interface, which is tailored to the role of the user, from scientists and engineers running computations to HPC facility and funding agency managers. Metrics provided by XDMoD include comprehensive statistics on: number and type of computational jobs run, resources (computation, memory, disk, network, etc.) consumed, job wait and wall time, scientific impact, and quality of service.  The web interface is intuitive, allowing one to chart various metrics and interactively drill down to access additional related information.  The XDMoD framework is also designed to help ensure that the HPC infrastructure is delivering a high quality of service to its end-users by continuously monitoring system performance and reliability through the deployment of a series of programs specifically designed to monitor overall system performance. System managers are therefore able to proactively monitor the HPC infrastructure as opposed to having to rely on users to report failures or underperforming hardware and software.  An important capability of XDMoD is centered around monitoring the performance of all user jobs running on a given HPC resource with the goal of automatically identifying poorly performing jobs. Using the XDMoD Job Viewer, a utility developed by this program that provides detailed performance information for each job, system support personnel can work with each end-...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
