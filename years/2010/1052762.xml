<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Modeling and Recognizing Collective Activities</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>80000.00</AwardTotalIntnAmount>
<AwardAmount>80000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project explores a novel principled framework for learning generic models of collective activities. Examples of collective activities are: people talking; a group of zebras escaping from a lion. Such models are used, in turn, for detecting, classifying, and segmenting activities as well as indentifying activities that differ from the collective behavior from videos sequences. Research developed in this project is distinctly different from previous research on action classification wherein activities are analyzed by considering individuals in isolation. Furthermore, unlike many current contributions, the aim is to work under unrestrictive conditions such as dynamic cluttered background, moving, monocular and un-calibrated cameras.&lt;br/&gt;&lt;br/&gt;Key intellectual contributions of this project are: i) a learning scheme based on Random Forest that is able to adaptively characterize the coherent behavior of individuals, thus enabling discriminative classification of collective activities. This learning scheme is also relevant to other visual recognition tasks using context (e.g., scene and object recognition); ii) a methodology based on Relational Dependency Networks for segmenting different collective activities and discovering anomalous ones. &lt;br/&gt;&lt;br/&gt;This project can provides critical building blocks toward addressing high level visual problems such as modeling the interaction between humans/animals and objects,  constructing an ontology of human/animal activities, modeling complex human/animal behaviors. This research has a potential to play a transformative role in strategic areas such as robotics and navigation. It also provides a crucial tool for analyzing and studying typical spatial-temporal collective behaviors in biology (insects, animals) or biomedicine (cells).</AbstractNarration>
<MinAmdLetterDate>08/06/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/06/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1052762</AwardID>
<Investigator>
<FirstName>Silvio</FirstName>
<LastName>Savarese</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Silvio Savarese</PI_FULL_NAME>
<EmailAddress>ssilvio@stanford.edu</EmailAddress>
<PI_PHON>6504970360</PI_PHON>
<NSF_ID>000489619</NSF_ID>
<StartDate>08/06/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Regents of the University of Michigan - Ann Arbor]]></Name>
<CityName>Ann Arbor</CityName>
<StateCode>MI</StateCode>
<ZipCode>481091274</ZipCode>
<StreetAddress><![CDATA[3003 South State St. Room 1062]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~80000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main purpose of this project was to provide a novel principled framework for learning generic models of collective activities from videos sequences. Examples of collective activities are: people talking; a group of zebras escaping from a lion. Such models were used, in turn, for detecting, classifying, and segmenting activities as well as identifying activities that differ from the collective behavior. Research that we developed in this project is distinctly different from previous research on action classification wherein activities are analyzed by considering individuals <em>in isolation</em>. Furthermore, unlike many current contributions, the aim was to work under unrestrictive conditions such as dynamic cluttered background, moving, monocular and un-calibrated cameras. A key intellectual contributions of this project was a learning scheme based on Random Forest that is able to adaptively characterize the coherent behavior of individuals, thus enabling discriminative classification of collective activities. This learning scheme is also relevant to other visual recognition tasks (e.g., scene and object recognition) that can take advantage of contextual cues. Another key contribution was the collection of a dataset of collective activities. Such a dataset comprises videos of various classes of collective activities and was used to experimentally demonstrate our theoretical findings. Both dataset and the software that implements our framework is available at authors&rsquo; website.</p> <p>This project provides a platform for addressing high level visual problems such as modeling the interaction between humans/animals and objects,&nbsp; constructing an ontology of human/animal activities, modeling complex human/animal behaviors. This research has the potential to play a transformative role in strategic areas such as robotics and navigation. It provides a crucial tool for analyzing and studying typical spatial-temporal collective behaviors in biology (insects, animals) or biomedicine (cells).</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/23/2013<br>      Modified by: Silvio&nbsp;Savarese</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main purpose of this project was to provide a novel principled framework for learning generic models of collective activities from videos sequences. Examples of collective activities are: people talking; a group of zebras escaping from a lion. Such models were used, in turn, for detecting, classifying, and segmenting activities as well as identifying activities that differ from the collective behavior. Research that we developed in this project is distinctly different from previous research on action classification wherein activities are analyzed by considering individuals in isolation. Furthermore, unlike many current contributions, the aim was to work under unrestrictive conditions such as dynamic cluttered background, moving, monocular and un-calibrated cameras. A key intellectual contributions of this project was a learning scheme based on Random Forest that is able to adaptively characterize the coherent behavior of individuals, thus enabling discriminative classification of collective activities. This learning scheme is also relevant to other visual recognition tasks (e.g., scene and object recognition) that can take advantage of contextual cues. Another key contribution was the collection of a dataset of collective activities. Such a dataset comprises videos of various classes of collective activities and was used to experimentally demonstrate our theoretical findings. Both dataset and the software that implements our framework is available at authorsÃ† website.  This project provides a platform for addressing high level visual problems such as modeling the interaction between humans/animals and objects,  constructing an ontology of human/animal activities, modeling complex human/animal behaviors. This research has the potential to play a transformative role in strategic areas such as robotics and navigation. It provides a crucial tool for analyzing and studying typical spatial-temporal collective behaviors in biology (insects, animals) or biomedicine (cells).                Last Modified: 05/23/2013       Submitted by: Silvio Savarese]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
