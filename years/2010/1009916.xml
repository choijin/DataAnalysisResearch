<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III:  Large:  Collaborative Research: Web Archive Cooperative</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>2350507.00</AwardTotalIntnAmount>
<AwardAmount>2350507</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Web Science is an emerging discipline that studies the Web: how human activity is shaped by Web interactions, how the Web can benefit society, and how Web technologies can be improved. Central to Web Science is access to data that records the history of the Web, as well as data that records human activity (e.g., posed queries, tagged pages, Twitter updates). It is currently very difficult for academic researchers to obtain such Web data because it is hard to locate, it is fragmented across diverse sites, and is recorded using inconsistent formats and strategies. This project will build a Web Archive Cooperative (WAC) that will integrate existing archives (repositories of Web data), making it feasible to access large volumes of data in a simplified fashion. The WAC will be a virtual service, providing search facilities and access mechanisms to existing resources. These resources will not just be Web pages, but all types of available Web information, such as query logs, tag annotations, blogs, profiles and Twitter updates. Furthermore, resources will also include the software tools for building and managing Web archives.&lt;br/&gt;&lt;br/&gt;The project will explore three goals for a resource discovery service: (1) the manual or automated discovery of entire existing Web related archives; (2) the selection among known archives of the ones that support a specific research question; and (3) the identification of individual resources from within the selected archives. Tools for characterizing discovered archives, especially for the case where the archive does not provide rich descriptive metadata, will also be developed. Characterization of an archive includes elements such as an estimate of the archive's coverage, particulars of the crawling parameters, like dates/frequencies, crawl duration, depth, per-site ceiling on the number of collected pages, content statistics, and link structure.  Mechanisms for integrating diverse archives will be developed, and the mechanisms will be applied to site reconstruction (from various archives) and archive views (a logical fusion of resources from multiple sources). Since integration issues are so challenging, an experimental testbed will be set up with small but diverse resources. The testbed will contain several crawls of the same target sites, each obtained with different crawlers and using different parameters. The testbed will also contain related resources.  Storage trading schemes will be developed, allowing members to trade local backup space for remote space. A Web archive replication tool will be developed based on existing notions for self-preserving objects. Alternatives for replica synchronization will be studied.&lt;br/&gt;&lt;br/&gt;Workshops to bring together key Web Science researchers will be organized to discuss available resources and impediments to sharing. These workshops will drive research and identify needed tools and protocols. With small groups of participants, challenge problems will be established, e.g., combining a set of Web archives. Reports of these results at future workshops can incentivize others to participate in the WAC. In addition, an Advisory Board of industrial, government, and academic experts has been set up to guide the project.  A Summer Institute for Web Science graduate students will be held. At this Institute, students will learn to use the latest tools and will learn from each other's experiences in dealing with Web data. In addition, a one-day workshop will be developed, to be offered at Web Science conferences (WWW, SIGIR, etc.) to educate participants about WAC resources. An undergraduate Web Sciences track for computer science majors will be set up, taking advantage of WAC resources. The project will have impact in two ways.  First, it will provide tools and services that facilitate access to Web resources. Any researcher, from a computer scientist studying efficient Web search, to a social scientist studying how human beliefs are changing today, to a historian studying how the early Web evolved, to a biologist understanding how disease spreads, will benefit from the work.  Second, the project motivates students and young researchers to stay in academia. Currently top talent is flowing to industry because only they have comprehensive Web data, and it is so hard to do significant Web Science at universities.  The WAC can provide an alternative, attracting more researchers and teachers to this important area.</AbstractNarration>
<MinAmdLetterDate>08/04/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1009916</AwardID>
<Investigator>
<FirstName>Hector</FirstName>
<LastName>Garcia-Molina</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hector Garcia-Molina</PI_FULL_NAME>
<EmailAddress>hector@cs.stanford.edu</EmailAddress>
<PI_PHON>6507230685</PI_PHON>
<NSF_ID>000335305</NSF_ID>
<StartDate>08/04/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[450 Jane Stanford Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~2350507</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-901fce0f-ce1d-3f81-bb89-34cbf064d229">&nbsp;</span></p> <p>One of the highlights of this project was its ability to consolidate<br />and unify data of how participants of massively open online courses<br />(MOOCs) interact with today's online course offerings. MOOCs are<br />public courses that universities like Stanford offer for free. The<br />classes are taught by professors and lecturers, and around 3M learners<br />worldwide have enrolled in Stanford's offerings alone over the past<br />years. Courses routinely have enrollments of 20,000 learners or more.<br /><br />The entire higher education community, however, is new to MOOCs. So<br />most of the underlying technologies are simple one-to-one translations<br />of lecture classes to the online medium. We would all like these<br />courses to take much better advantage of the Web medium. We also need<br />to understand better the pedagogy that optimizes online learner<br />experiences.<br /><br />Towards such understanding one of the activities of this project<br />consolidated data that captures how learners throughout the world<br />interact with Stanford's online learning material. The resulting<br />archive contains 1 billion learner actions, such as starting or<br />rewinding instructional videos, or submitting assignments. Beyond this<br />technical component, the work also included efforts to establish best<br />practices for sharing the data with other researchers without<br />violating learners' trust or the law.<br /><br />The data is now used to explore how best to help online learners stick<br />to a class, rather than drop away when the learning gets tough. For<br />example, the project developed a system that recognizes when a course<br />forum post is evidence of the poster's confusion about a topic. The<br />system then finds pointers into segments of the course's instructional<br />videos at a granularity of seconds. The hope is that in spite of large<br />enrollment numbers, learners will find the resources they need to<br />persevere.<br /><br />Figures 1-4 show examples of how we can used these data to visualize<br />how tens of thousands of learners are working with the course<br />material. Figures 1-3 all show quiz results. Each bubble represents<br />one quiz. The (hard-to-see) numbers on the bubbles are average grades<br />in percents that learners earned on that bubble's quiz. The slider on<br />the right may be used to dial in what is to be considered a<br />satisfactory grade. Red bubbles show the resulting quizzes in which<br />the average learner would lie below that threshold. Green bubbles show<br />the quizzes in which the average grade was above the dialed-in level.<br />The check boxes on the right allow us to select any of the courses<br />that Stanford offered.<br /><br />Figure 1 shows quizzes from 20 courses all in one image. The slider is<br />set to 70%. All bubbles together represent performance by an estimated<br />80,000 learners. In the live interactive chart we can find out which<br />quiz each bubble represents. We can thereby easily see which quizzes<br />cause the most problems.<br /><br />Figures 2-3 show quizzes of just a single statistics course in which<br />27,800 learners enrolled. The figures show the difference between<br />setting the 'difficult-quiz threshold' to 70% vs. 90%.<br /><br />While Figures 1-3 are examples of overviews, Figure 4 shows the<br />opposite. That Figure shows how we can dive into second-resolution<br />behavior detail. The Figure shows the number of views for every second<br />of one instructional video. The y-axis shows the number of views,<br />counting all learners, including repeated views by learners after<br />rewinding. The x-axis shows the number of seconds into the video.&nbsp; The<br />'calm' line records the absolute number of views. The wiggly line<br />shows changes in how many views succe...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    One of the highlights of this project was its ability to consolidate and unify data of how participants of massively open online courses (MOOCs) interact with today's online course offerings. MOOCs are public courses that universities like Stanford offer for free. The classes are taught by professors and lecturers, and around 3M learners worldwide have enrolled in Stanford's offerings alone over the past years. Courses routinely have enrollments of 20,000 learners or more.  The entire higher education community, however, is new to MOOCs. So most of the underlying technologies are simple one-to-one translations of lecture classes to the online medium. We would all like these courses to take much better advantage of the Web medium. We also need to understand better the pedagogy that optimizes online learner experiences.  Towards such understanding one of the activities of this project consolidated data that captures how learners throughout the world interact with Stanford's online learning material. The resulting archive contains 1 billion learner actions, such as starting or rewinding instructional videos, or submitting assignments. Beyond this technical component, the work also included efforts to establish best practices for sharing the data with other researchers without violating learners' trust or the law.  The data is now used to explore how best to help online learners stick to a class, rather than drop away when the learning gets tough. For example, the project developed a system that recognizes when a course forum post is evidence of the poster's confusion about a topic. The system then finds pointers into segments of the course's instructional videos at a granularity of seconds. The hope is that in spite of large enrollment numbers, learners will find the resources they need to persevere.  Figures 1-4 show examples of how we can used these data to visualize how tens of thousands of learners are working with the course material. Figures 1-3 all show quiz results. Each bubble represents one quiz. The (hard-to-see) numbers on the bubbles are average grades in percents that learners earned on that bubble's quiz. The slider on the right may be used to dial in what is to be considered a satisfactory grade. Red bubbles show the resulting quizzes in which the average learner would lie below that threshold. Green bubbles show the quizzes in which the average grade was above the dialed-in level. The check boxes on the right allow us to select any of the courses that Stanford offered.  Figure 1 shows quizzes from 20 courses all in one image. The slider is set to 70%. All bubbles together represent performance by an estimated 80,000 learners. In the live interactive chart we can find out which quiz each bubble represents. We can thereby easily see which quizzes cause the most problems.  Figures 2-3 show quizzes of just a single statistics course in which 27,800 learners enrolled. The figures show the difference between setting the 'difficult-quiz threshold' to 70% vs. 90%.  While Figures 1-3 are examples of overviews, Figure 4 shows the opposite. That Figure shows how we can dive into second-resolution behavior detail. The Figure shows the number of views for every second of one instructional video. The y-axis shows the number of views, counting all learners, including repeated views by learners after rewinding. The x-axis shows the number of seconds into the video.  The 'calm' line records the absolute number of views. The wiggly line shows changes in how many views successive seconds received.  We can use such charts to find areas in each video to which many learners repeatedly returned. Such multiply viewed video segments may be a signal that the instructor was discussing difficult material. But the repeat viewings could instead be a sign that the video poses a problem at the respective segments, and that the instructor may need to clarify a point, or re-record the segment.       Last Modified: 09/17/2015       Submitted b...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
