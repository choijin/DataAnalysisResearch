<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase II:  Units-based numeric data extraction with knowledge of scientific context</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
<AwardExpirationDate>01/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>900112</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Glenn H. Larsen</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This Small Business Innovation Research (SBIR) Phase II project aims to establish that a units-based approach to retrieving quantitative data from scientific and technical documents is a powerful alternative to keyword and document based search models. Keyword approaches to data extraction and contextualization are limited due to poor semantic contextualization and because quantities are often written in a wide variety of numeric and unit formats. The proposed approach to reliable numeric data extraction begins with quantity-intelligent indexing that recognizes many numeric formats and converts quantities to standardized base-unit tokens, to significantly enhance search recall over keyword approaches. The resulting number-unit pairs will anchor the index to enable efficient scientific exploratory search with high semantic precision, but without overly relying on sophisticated imposed semantic ontologies. Research will focus on a proprietary search-time data scoring algorithm that utilizes context-sensitive numeric spectra, to score otherwise ambiguous results based on probabilistic methods. This approach is expected to improve both precision and recall of contextual numeric data extraction. In turn, the resulting search engine will enable instant visualization and analysis of collective technology landscapes and trends, which will guide researchers in any area of technology represented by the indexed documents.&lt;br/&gt;&lt;br/&gt;The broader impact of this project will be to enable reliable and efficient extraction of numeric data from diverse sources such as scientific literature and patent databases. These unstructured document sets contain a wealth of latent quantitative data which, if properly extracted and aggregated, can enable powerful modes of data exploration. The unit-based index and data-scoring algorithm are customized for an exploratory search model that will allow non-expert users to rapidly aggregate thousands of relevant data points, with simple keyword inputs and without laboriously opening and parsing individual documents. Researchers and students may thus explore data sets that were previously inaccessible, or known only to experts in a field. This will also contribute to knowledge discovery within large unstructured databases, since patterns and correlations between seemingly disparate variables can be immediately visualized. The platform will provide the capability to efficiently generate technology landscapes, anticipate emerging trends, and recognize competitive technical outliers. If successful, this will be valuable for high-tech industrial innovation including for engineers involved in R&amp;D as well as business development executives and intellectual asset managers who focus on asset allocation, new technology ventures, prior art and patent infringement within a technical parameter space.</AbstractNarration>
<MinAmdLetterDate>08/11/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/12/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1026493</AwardID>
<Investigator>
<FirstName>Ari</FirstName>
<LastName>Tuchman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ari Tuchman</PI_FULL_NAME>
<EmailAddress>ari.tuchman@gmail.com</EmailAddress>
<PI_PHON>6508044179</PI_PHON>
<NSF_ID>000519893</NSF_ID>
<StartDate>08/11/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Quantifind Inc.</Name>
<CityName>Palo Alto</CityName>
<ZipCode>943061716</ZipCode>
<PhoneNumber>6508044179</PhoneNumber>
<StreetAddress>2470 el camino real #214</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832307362</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>QUANTIFIND INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Quantifind Inc.]]></Name>
<CityName>Palo Alto</CityName>
<StateCode>CA</StateCode>
<ZipCode>943061716</ZipCode>
<StreetAddress><![CDATA[2470 el camino real #214]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5373</Code>
<Text>SBIR Phase II</Text>
</ProgramElement>
<ProgramReference>
<Code>165E</Code>
<Text>SBIR Phase IIB</Text>
</ProgramReference>
<ProgramReference>
<Code>5373</Code>
<Text>SMALL BUSINESS PHASE II</Text>
</ProgramReference>
<ProgramReference>
<Code>6850</Code>
<Text>DIGITAL SOCIETY&amp;TECHNOLOGIES</Text>
</ProgramReference>
<ProgramReference>
<Code>9139</Code>
<Text>INFORMATION INFRASTRUCTURE &amp; TECH APPL</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~400112</FUND_OBLG>
<FUND_OBLG>2012~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our SBIR initiative, &ldquo;Units Based Numeric Data Extraction with Knowledge of Scientific Context&ldquo; was focused on extracting signals from an unstructured database when little external contextualization is available.&nbsp;&nbsp; Dictionary-heavy approaches, such as search engines over medical records, are pre-programmed with causal relationships such as medicines and their side effects, and therefore it is not difficult to extract these relationships when they appear again in new documents.&nbsp; However, often the relationships between topics are unknown to the user, or the documents discuss fields where causal relationships have not yet been determined or are rapidly changing.&nbsp; In these circumstances, when effectively the user does not know what they are looking for, there must still be a methodology for extracting what is important.&nbsp; This was the focus of our efforts, and we discovered techniques to uncover these unknown relationships.</p> <p>&nbsp;</p> <p>This exercise in extracting signal without expertise in the field requires a proxy for defining importance in a particular application.&nbsp; We invented techniques for using time series data to be that proxy. &nbsp;We analyzed social media chatter, for which existing technologies had focused on two primary methods of discovering importance.&nbsp; The first, most prevalent method, relies on simple entity counting, and selects those phrases that appeared the most in the corpus.&nbsp; This approach often populates a word cloud.&nbsp; Another approach, often called sentiment analysis, involves analyzing the emotional tone of phrases by comparing against a list of emotional phrases, and elevating those entities that are highly associated with an extreme of emotion.&nbsp; Although this approach has significant natural language challenges, we have found that it does not consistently provide signals that correlate (even in-sample) with underlying business metrics.</p> <p>&nbsp;</p> <p>During the course of this effort, we developed a novel approach of prioritizing signal by training over a time series of structured data to be used as an intelligent filter for extracting those language patterns that were most highly correlated with the training metric.&nbsp; This structured, time series data, served as the Bayesian prior for contextualizing which conversation patterns were most likely important.&nbsp; &nbsp;Our approach required a flexible architecture that was capable of leveraging the real-time nature of unstructured feedback, capable of scaling to large datasets, and compatible with compute intensive algorithms.&nbsp; We also developed a platform and methodology for scoring the accuracy of our extracted signals.</p> <p>&nbsp;</p> <p>Much of the work accomplished during this SBIR was centered on social media chatter, including Facebook and Twitter, and was trained over financial transaction data, such as sales or churn.&nbsp; However, our technology of training over unstructured data by extracting correlations in language clusters with a structured time series is a generic approach with applications ranging from healthcare to defense/intelligence applications.</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/05/2015<br>      Modified by: Ari&nbsp;Tuchman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our SBIR initiative, "Units Based Numeric Data Extraction with Knowledge of Scientific Context" was focused on extracting signals from an unstructured database when little external contextualization is available.   Dictionary-heavy approaches, such as search engines over medical records, are pre-programmed with causal relationships such as medicines and their side effects, and therefore it is not difficult to extract these relationships when they appear again in new documents.  However, often the relationships between topics are unknown to the user, or the documents discuss fields where causal relationships have not yet been determined or are rapidly changing.  In these circumstances, when effectively the user does not know what they are looking for, there must still be a methodology for extracting what is important.  This was the focus of our efforts, and we discovered techniques to uncover these unknown relationships.     This exercise in extracting signal without expertise in the field requires a proxy for defining importance in a particular application.  We invented techniques for using time series data to be that proxy.  We analyzed social media chatter, for which existing technologies had focused on two primary methods of discovering importance.  The first, most prevalent method, relies on simple entity counting, and selects those phrases that appeared the most in the corpus.  This approach often populates a word cloud.  Another approach, often called sentiment analysis, involves analyzing the emotional tone of phrases by comparing against a list of emotional phrases, and elevating those entities that are highly associated with an extreme of emotion.  Although this approach has significant natural language challenges, we have found that it does not consistently provide signals that correlate (even in-sample) with underlying business metrics.     During the course of this effort, we developed a novel approach of prioritizing signal by training over a time series of structured data to be used as an intelligent filter for extracting those language patterns that were most highly correlated with the training metric.  This structured, time series data, served as the Bayesian prior for contextualizing which conversation patterns were most likely important.   Our approach required a flexible architecture that was capable of leveraging the real-time nature of unstructured feedback, capable of scaling to large datasets, and compatible with compute intensive algorithms.  We also developed a platform and methodology for scoring the accuracy of our extracted signals.     Much of the work accomplished during this SBIR was centered on social media chatter, including Facebook and Twitter, and was trained over financial transaction data, such as sales or churn.  However, our technology of training over unstructured data by extracting correlations in language clusters with a structured time series is a generic approach with applications ranging from healthcare to defense/intelligence applications.          Last Modified: 02/05/2015       Submitted by: Ari Tuchman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
