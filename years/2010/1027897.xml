<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDI-Type II: Collaborative Research: Perception of Scene Layout by Machines and Visually Impaired Users</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>249507.00</AwardTotalIntnAmount>
<AwardAmount>249507</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>01060000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OIA</Abbreviation>
<LongName>Office of Integrative Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Stephen Meacham</SignBlockName>
<PO_EMAI>smeacham@nsf.gov</PO_EMAI>
<PO_PHON>7032927599</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project investigates computational methods for object detection, spatial scene construction, and natural language spatial descriptions derived from real-time visual images to describe prototypical indoor spaces (e.g., rooms, offices, etc.). The primary application of this research is to provide blind or visually impaired users with spatial information about their surroundings that may otherwise be difficult to obtain from non-visual sensing. Such knowledge will assist in development of accurate cognitive models of the environment and will support better informed execution of spatial behaviors in everyday tasks. &lt;br/&gt;&lt;br/&gt;A second motivation for the work is to contribute to the improvement of spatial capacities for computers and robots. Computers and robots are similarly "blind" to images unless they have been provided some means to "see" and understand them. Currently, no robotic system is able to reliably perform high-level processing of spatial information on the basis of image sequences, e.g., to find an empty chair in a room, which not only means finding an empty chair in an image, but also localizing the chair in the room, and performing an action of reaching the chair. The guiding tenet of this research is that a better understanding of spatial knowledge acquisition from visual images and concepts of spatial awareness by humans can also be applied to reducing the ambiguity and uncertainty of information processing by autonomous systems. &lt;br/&gt;&lt;br/&gt;A central contribution of this work is to make the spatial information content of visual images available to the visually impaired, a rapidly growing demographic of our aging society. In an example scenario a blind person and her guide dog are walking to her doctor's office, an office which she has not previously visited. At the office she needs information for performing some essential tasks such as finding the check-in counter, available seating, or the bathroom. No existing accessible navigation systems are able to describe the spatial parameters of an environment and help detect and localize objects in that space. Our work will provide the underlying research and elements to realize such a system.</AbstractNarration>
<MinAmdLetterDate>09/20/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/20/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.083</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1027897</AwardID>
<Investigator>
<FirstName>Longin Jan</FirstName>
<LastName>Latecki</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Longin Jan Latecki</PI_FULL_NAME>
<EmailAddress>latecki@temple.edu</EmailAddress>
<PI_PHON>2152045781</PI_PHON>
<NSF_ID>000227657</NSF_ID>
<StartDate>09/20/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Temple University</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191226003</ZipCode>
<PhoneNumber>2157077547</PhoneNumber>
<StreetAddress>1801 N. Broad Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>057123192</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>057123192</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Temple University]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191226003</ZipCode>
<StreetAddress><![CDATA[1801 N. Broad Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7751</Code>
<Text>CDI TYPE II</Text>
</ProgramElement>
<ProgramReference>
<Code>5936</Code>
<Text>GERMANY (F.R.G.)</Text>
</ProgramReference>
<ProgramReference>
<Code>5979</Code>
<Text>Europe and Eurasia</Text>
</ProgramReference>
<ProgramReference>
<Code>7721</Code>
<Text>FROM DATA TO KNOWLEDGE</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~249507</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our work focused on detection and recognition of instances of known object classes in a given image. Furthermore, we have also worked on localizing the detected instances in the depicted scene. The input to the developed software is a single RGB depth (RGB-D) image. The output is image regions assigned labels of recognized object classes. Since the image pixels correspond directly to 3D points, the location of the recognized object instances is also known in the given 3D scene. Hence in our work we have addressed the problem of semantic segmentation of indoor scenes from RGB-D images. We introduced a novel image region labeling method which augments Conditional Random Field (CRF) formulation with hard mutual exclusion (mutex) constraints. This way our approach utilizes rich and accurate 3D geometric structure coming from RGB-D sensors in a principled manner. The final labeling result must satisfy all mutex constraints, which allows us to eliminate configurations that violate common sense physics laws like placing a floor above a night stand. Three classes of mutex constraints have been proposed: global object co-occurrence constraint, relative height relationship constraint, and local support relationship constraint.</p> <p>Our research on this project has benefited from the recent development of the RGB-D sensors like the Kinect sensor. Its data significantly improves the quality of the 3D information and consequently of the top view mosaics, since it yields a depth map in addition to the RGB image. The 3D depth information obtained from Kinect is of significantly better quality than depth maps obtained from stereo cameras. This offers a unique opportunity for providing to a blind user depth perception of a significantly improved quality.</p> <p>We have also developed an autonomous navigation system and implemented it on a mobile robot. It is based on classic perception/action cycle. Perception starts with taking a new image, transforming it to a world model, which in our framework is ground plane projection (GPP), and performing a path planning to the target object in GPP. The key advantage of using GPP is the fact that the intrinsic target appearance variation and extrinsic noise is far less likely to appear in GPP than in a regular side-view image. Moreover, it is a very simple task to determine free space in GPP without any appearance learning even from a moving camera. Hence GPP is very different from the top-view image obtained from a ceiling mounted camera. We perform both object detection and tracking in GPP. Two kinds of GPP images are utilized: gray GPP, which represents the maximal height of 3D points projecting to each pixel, and binary GPP, which is obtained by thresholding the gray GPP. For detection, a simple connected component labeling is used to detect footprints of targets in binary GPP. For tracking, a novel Pixel Level Association strategy is proposed to link the same target in consecutive frames in gray GPP. It utilizes optical flow in gray GPP, which to our best knowledge has never been done before. Then we &ldquo;back project" the detected and tracked objects in GPP to original, side view (RGB-D) images. Hence we are able to detect and track objects in the side-view (RGB-D) images. Our system is able to robustly detect and track multiple moving targets in real time. The detection process does not rely on any target model, which means we do not need any training process. Moreover, tracking does not require any manual initialization, since entering objects are robustly detected. We also use the novel framework for robot navigation by tracking. The performance gain in comparison to state-of-the-art trackers is most significant in the presence of occlusion and background clutter.</p> <p>The project provided training for PhD students by offering them research and publications opportunities. Also several undergraduate students have gained researc...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our work focused on detection and recognition of instances of known object classes in a given image. Furthermore, we have also worked on localizing the detected instances in the depicted scene. The input to the developed software is a single RGB depth (RGB-D) image. The output is image regions assigned labels of recognized object classes. Since the image pixels correspond directly to 3D points, the location of the recognized object instances is also known in the given 3D scene. Hence in our work we have addressed the problem of semantic segmentation of indoor scenes from RGB-D images. We introduced a novel image region labeling method which augments Conditional Random Field (CRF) formulation with hard mutual exclusion (mutex) constraints. This way our approach utilizes rich and accurate 3D geometric structure coming from RGB-D sensors in a principled manner. The final labeling result must satisfy all mutex constraints, which allows us to eliminate configurations that violate common sense physics laws like placing a floor above a night stand. Three classes of mutex constraints have been proposed: global object co-occurrence constraint, relative height relationship constraint, and local support relationship constraint.  Our research on this project has benefited from the recent development of the RGB-D sensors like the Kinect sensor. Its data significantly improves the quality of the 3D information and consequently of the top view mosaics, since it yields a depth map in addition to the RGB image. The 3D depth information obtained from Kinect is of significantly better quality than depth maps obtained from stereo cameras. This offers a unique opportunity for providing to a blind user depth perception of a significantly improved quality.  We have also developed an autonomous navigation system and implemented it on a mobile robot. It is based on classic perception/action cycle. Perception starts with taking a new image, transforming it to a world model, which in our framework is ground plane projection (GPP), and performing a path planning to the target object in GPP. The key advantage of using GPP is the fact that the intrinsic target appearance variation and extrinsic noise is far less likely to appear in GPP than in a regular side-view image. Moreover, it is a very simple task to determine free space in GPP without any appearance learning even from a moving camera. Hence GPP is very different from the top-view image obtained from a ceiling mounted camera. We perform both object detection and tracking in GPP. Two kinds of GPP images are utilized: gray GPP, which represents the maximal height of 3D points projecting to each pixel, and binary GPP, which is obtained by thresholding the gray GPP. For detection, a simple connected component labeling is used to detect footprints of targets in binary GPP. For tracking, a novel Pixel Level Association strategy is proposed to link the same target in consecutive frames in gray GPP. It utilizes optical flow in gray GPP, which to our best knowledge has never been done before. Then we "back project" the detected and tracked objects in GPP to original, side view (RGB-D) images. Hence we are able to detect and track objects in the side-view (RGB-D) images. Our system is able to robustly detect and track multiple moving targets in real time. The detection process does not rely on any target model, which means we do not need any training process. Moreover, tracking does not require any manual initialization, since entering objects are robustly detected. We also use the novel framework for robot navigation by tracking. The performance gain in comparison to state-of-the-art trackers is most significant in the presence of occlusion and background clutter.  The project provided training for PhD students by offering them research and publications opportunities. Also several undergraduate students have gained research and work experience in connection with this project including minority students. The...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
