<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Spontaneous 4D-Facial Expression Corpus for Automated Facial Image Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>65784.00</AwardTotalIntnAmount>
<AwardAmount>65784</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Currently, few publically available, annotated databases exist. Those that do are limited to 2D static images or video of posed facial behavior. Further development is stymied by lack of adequate training data. Because posed and un-posed (aka ?spontaneous?) facial expressions differ along several dimensions including complexity, well annotated video of un-posed facial behavior is needed.  Moreover, because the face is a three-dimensional deformable object, 2D video is insufficient. A 3D video archive is needed.&lt;br/&gt;&lt;br/&gt;This project develops a 3D video corpus of spontaneous facial and vocal expression in a diverse group of young adults. Well-validated emotion inductions elicit expressions of emotion and paralinguistic communication. Sequence-level ground truth is obtained via participant self-report. Frame-level ground-truth is obtained via facial action unit coding using the Facial Action Coding System. The project promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.&lt;br/&gt;&lt;br/&gt;The project promotes research on next-generation affective computing with applications in security, law-enforcement, biomedicine, behavior science, entertainment and education. The multimodal 3D video database and its metadata are for the research community for new algorithm development, assessment, comparison, and evaluation.</AbstractNarration>
<MinAmdLetterDate>08/24/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/24/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1051103</AwardID>
<Investigator>
<FirstName>Lijun</FirstName>
<LastName>Yin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lijun Yin</PI_FULL_NAME>
<EmailAddress>lijun@cs.binghamton.edu</EmailAddress>
<PI_PHON>6077775484</PI_PHON>
<NSF_ID>000163329</NSF_ID>
<StartDate>08/24/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Binghamton</Name>
<CityName>BINGHAMTON</CityName>
<ZipCode>139026000</ZipCode>
<PhoneNumber>6077776136</PhoneNumber>
<StreetAddress>4400 VESTAL PKWY E</StreetAddress>
<StreetAddress2><![CDATA[PO Box 6000]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>22</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY22</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>090189965</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Binghamton]]></Name>
<CityName>BINGHAMTON</CityName>
<StateCode>NY</StateCode>
<ZipCode>139026000</ZipCode>
<StreetAddress><![CDATA[4400 VESTAL PKWY E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>22</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY22</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~65784</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span style="text-decoration: underline;">Project Outcomes Report: </span></p> <p>Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Further development is stymied by lack of adequate annotated training data. Because posed and un-posed (aka &ldquo;spontaneous&rdquo;) facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed.&nbsp; Moreover, because the face is a three-dimensional deformable object, 2D video is insufficient. A 3D video archive is needed.</p> <p>&nbsp;</p> <p>Thanks to support from NSF grants IIS-1051103 (PI: Dr. Lijun Yin of Binghamton University) and IIS-1051169 (PI: Dr. Jeff Cohn of University of Pittsburgh), a collaborative, inter-institutional research team from Binghamton University and University of Pittsburgh has developed a new 4D video (3D + time) &nbsp;database of spontaneous facial expression (Zhang, Yin, Cohn, Canavan, Reale, Horowitz, &amp; Liu, 2013). Participants were 23 women and 18 men from diverse ancestries that include Asian, African-American, Hispanic/Latino, and Euro-American.&nbsp; Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication.</p> <p>&nbsp;</p> <p>The database includes multiple types of metadata to maximize its information value and usefulness. Sequence-level ground truth was obtained via participant self-report. Frame-level annotation of facial actions was obtained using manual FACS coding (Facial Action Coding System) (Ekman, Friesen, &amp; Hager, 2002) Sixty-six fiduciary facial landmarks were hand labeled in approximately 5% of video frames.&nbsp; The landmarks then were used to train person-specific active appearance models (Baker, Gross, &amp; Matthews, 2004). Facial features were tracked in both 2D and 3D domains using both person-specific (AAM) and generic (constrained local models, or CLM) (Lucey, Wang, Cox, Sridharan, &amp; Cohn, 2009) approaches.</p> <p>&nbsp;</p> <p>To provide benchmarks for automatic facial action unit detection, the project team developed a unique space-time feature &ndash; referred to as a Nebular feature &ndash; to represent facial actions and detect expressions. Unlike traditional methods that use two dimensional images or posed 3D facial models, the Nebula feature (Reale, Zhang, &amp; Yin, 2013) uses high-resolution 3D motion models to detect subtle shape and appearance deformations on a 3D surface. This method can recognize facial expressions not only by individual models, but also by their dynamic actions over time. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.</p> <p>&nbsp;</p> <p>This 4D spontaneous facial expression database is the first of its kind for spontaneous facial expression research. It will be released to the research community in April 2013 at the IEEE International Conference on Automatic Face and Gesture Recognition for use in algorithm development and testing.&nbsp; Based on our past experience, we anticipate that it will serve as a valuable and well-utilized benchmark for research and development in biometrics, security, biomedicine, psychology, affective computing, computer graphics, and related fields.</p> <p>&nbsp;</p> <p><span style="text-decoration: underline;">References:</span></p> <p>Baker, S., Gross, R., &amp; Matthews, I. (2004). Lucas-Kanade 20 years on: A unifying framework: Part 4. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 26</em>, 810-815.</p> <p>&nbsp;</p> <p>Ekman, P., Friesen, W....]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Project Outcomes Report:   Facial expression is central to human experience. Its efficient and valid measurement is a challenge that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Further development is stymied by lack of adequate annotated training data. Because posed and un-posed (aka "spontaneous") facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed.  Moreover, because the face is a three-dimensional deformable object, 2D video is insufficient. A 3D video archive is needed.     Thanks to support from NSF grants IIS-1051103 (PI: Dr. Lijun Yin of Binghamton University) and IIS-1051169 (PI: Dr. Jeff Cohn of University of Pittsburgh), a collaborative, inter-institutional research team from Binghamton University and University of Pittsburgh has developed a new 4D video (3D + time)  database of spontaneous facial expression (Zhang, Yin, Cohn, Canavan, Reale, Horowitz, &amp; Liu, 2013). Participants were 23 women and 18 men from diverse ancestries that include Asian, African-American, Hispanic/Latino, and Euro-American.  Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication.     The database includes multiple types of metadata to maximize its information value and usefulness. Sequence-level ground truth was obtained via participant self-report. Frame-level annotation of facial actions was obtained using manual FACS coding (Facial Action Coding System) (Ekman, Friesen, &amp; Hager, 2002) Sixty-six fiduciary facial landmarks were hand labeled in approximately 5% of video frames.  The landmarks then were used to train person-specific active appearance models (Baker, Gross, &amp; Matthews, 2004). Facial features were tracked in both 2D and 3D domains using both person-specific (AAM) and generic (constrained local models, or CLM) (Lucey, Wang, Cox, Sridharan, &amp; Cohn, 2009) approaches.     To provide benchmarks for automatic facial action unit detection, the project team developed a unique space-time feature &ndash; referred to as a Nebular feature &ndash; to represent facial actions and detect expressions. Unlike traditional methods that use two dimensional images or posed 3D facial models, the Nebula feature (Reale, Zhang, &amp; Yin, 2013) uses high-resolution 3D motion models to detect subtle shape and appearance deformations on a 3D surface. This method can recognize facial expressions not only by individual models, but also by their dynamic actions over time. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.     This 4D spontaneous facial expression database is the first of its kind for spontaneous facial expression research. It will be released to the research community in April 2013 at the IEEE International Conference on Automatic Face and Gesture Recognition for use in algorithm development and testing.  Based on our past experience, we anticipate that it will serve as a valuable and well-utilized benchmark for research and development in biometrics, security, biomedicine, psychology, affective computing, computer graphics, and related fields.     References:  Baker, S., Gross, R., &amp; Matthews, I. (2004). Lucas-Kanade 20 years on: A unifying framework: Part 4. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26, 810-815.     Ekman, P., Friesen, W. V., &amp; Hager, J. C. (2002). Facial action coding system: Research Nexus, Network Research Information, Salt Lake City, UT.     Lucey, S., Wang, Y., Cox, M., Sridharan, S., &amp; Cohn, J. F. (2009). Efficient constrained local model fitting for non-rigid face alignment. Image and Vision Computing Journal, 27(12), 18...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
