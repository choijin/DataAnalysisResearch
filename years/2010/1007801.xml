<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Statistical Methods for High Dimensional Discrete Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2010</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Very high dimensional count and binary data are now common in many fields including machine learning, imaging and marketing.  In high-throughput biology, ultra-high thoughput sequencing technologies which produce count and categorical data are displacing microarrays and other "omics" measurement devices. The output  of these measurement devices are counts per gene or other biological subunit for tens of thousands of responses per sample, or presence/absence for features such as single nucleotide polymorphisms (SNPs), for possibly millions of responses per sample.  Similar data can be derived on for features on satellite images, medical scans, monitoring devices and other very high dimensional measurement devices.  The investigator will extend highly multivariate and multiple testing methods developed for continuous (primarily normally distributed) data to discrete data.   New methods will be developed in four areas: A)  analyses for differences in distribution for discrete data that can accommodate complex experimental designs using generalized linear mixed models with overdispersion and Bayesian or empirical Bayes shrinkage.  B)  methods for supervised clustering of samples and variables in the discrete data setting taking into account the error structure of the discrete predictors.  C) classical and sufficient dimensions reduction methods such as canonical correlation and sliced inverse regression for discrete data.  D) extension of concepts and methods in multiple testing, such as false discovery rate estimation to the discrete setting in which the p-values from independent or weakly dependent tests may have different null distributions using conditional mixture modeling.  The methods will be tested on genomics and imaging data.&lt;br/&gt;&lt;br/&gt;Very highly multivariate data are now the norm in fields as diverse as cell biology, marketing, medical and satellite imaging, meteorology, epidemiology,&lt;br/&gt; fraud detection and cancer research.  These data may include thousands or millions of measurements on each item in the sample.  For example, genotyping &lt;br/&gt;services provide individuals with information on hundreds of thousands of genetic variants in their cells and retailer databases may have information on &lt;br/&gt;the sales of tens of thousands of items for each store in the chain. Many of these data come in the form of counts (such as number of items of each type &lt;br/&gt;in inventory, number of mRNA molecules encoding a particular protein) or in the form of categories (such as on/off, present/absent, or genotype AA, aa &lt;br/&gt;or Aa).  Methodology for highly multivariate continuous measurements such as blood pressure and temperature are well-developed but do not apply directly &lt;br/&gt;to count and categorical data.  The investigator will develop statistical methodology and software to improve analysis and summary of count and categorical data.  Four main areas of research are proposed:  A) statistical models and tests to determine if the variables are associated with differences among groups; B) statistical methods for prediction or classification of group membership; C) methods to summarize the data with a much smaller set of derived variables which preserve the predictive power of the full data and D) multiple comparisons methods to estimate the error rates.   For example, in a study of the genes associated with metastatic versus non-metastatic cancer, the methods could be used to determine which genes express differently in tumors which did or did not advance to metastasis, select a smaller set of genes which could be used as a diagnostic tool and then provide convenient  summaries which can readily be interpreted by clinicians.  In a study of stresses on a machine part, the pixels of scans of the part before and during  the application of the stresses could be used to determine precise locations at which the part might fail and differences among features of the scan  between parts which fail at low versus high stress.  In studies in which a large number of models are fitted or tests conducted, it is necessary to tolerate a small percentage of errors.  Concepts and methods in multiple testing which have been developed for continuous data will be extended to assist in estimating and controlling the number of false conclusions with count and categorical data.</AbstractNarration>
<MinAmdLetterDate>05/21/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/18/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1007801</AwardID>
<Investigator>
<FirstName>Naomi</FirstName>
<LastName>Altman</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Naomi S Altman</PI_FULL_NAME>
<EmailAddress>nsa1@psu.edu</EmailAddress>
<PI_PHON>8148653791</PI_PHON>
<NSF_ID>000446723</NSF_ID>
<StartDate>05/21/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Pennsylvania State Univ University Park</Name>
<CityName>University Park</CityName>
<ZipCode>168021503</ZipCode>
<PhoneNumber>8148651372</PhoneNumber>
<StreetAddress>201 Old Main</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003403953</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PENNSYLVANIA STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003403953</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Pennsylvania State Univ University Park]]></Name>
<CityName>University Park</CityName>
<StateCode>PA</StateCode>
<ZipCode>168021503</ZipCode>
<StreetAddress><![CDATA[201 Old Main]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~48768</FUND_OBLG>
<FUND_OBLG>2011~74489</FUND_OBLG>
<FUND_OBLG>2012~76743</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed statistical methods that can be applied to large sets of count or categorical data in which many features are measured.&nbsp; The challenge is to detect signals - that is nonrandom patterns - in the presence of the natural variability of the data.</p> <p>Count and categorical data arise in many settings.&nbsp; For example, in ecological studies there may be many species of interest and we might count the number of individuals of each species in many sampling locations, or categorize them by a characteristic such as healthy or unhealthy. We consider the species to be features, the locations to be samples and the actual counts or classification to be the measurements.&nbsp; For macro-scale studies, the features might be a few dozen insect species and the samples might be fields.&nbsp; For micro-scale studies, the features might be hundreds or thousands of single-cell organisms and the samples might be soil samples from various locations, or even fecal samples from human subjects.&nbsp; In high throughput biology studies, the features might be tens or hundreds of thousands of species of molecule such as different proteins or messenger RNAs and the samples might be tissues such as biopsy samples.&nbsp; Categories might be gene variants.</p> <p>As the number of features measured in our data grow from a few dozen to hundreds of thousands, we expect the information content to grow as well.&nbsp; However, problems of differentiating between spurious chance patterns in the data and real signals also increase.&nbsp; Assuming that the percentage of features with true signal stays constant as we increase the number of features measured, we can be over-whelmed by false detections even when the "per feature" false detection rate is quite small.&nbsp; Typically if we attempt to control the number of false detections by increasing stringency of determining if a signal is present, then we miss more of the weak signals, increasing the false non-detection rate.</p> <p>Two types of signal were the particular focus of this project: features that exhibit differences in between known subsets of samples (such as different ecosystems or cancerous versus normal tissues) and patterns (in either the features or the samples) which summarize the data by accounting for much of the variability in the measurements.&nbsp; For the former problem, the project investigated the efficacy of a proposed improvement in statistical testing and considered methods for estimating the false detection and non-detection rates applicable to any test. &nbsp;For the latter problem, the project developed a statistical framework for some currently available pattern detection methods that will lead to improvements in methodology and computational efficiency, as well as the development of new methods.</p> <p>The statistical methods developed in this project were applied to high throughput genomics data that quantify genes activity by counting the numbers of molecules of specific types present in the samples.&nbsp; &nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/03/2016<br>      Modified by: Naomi&nbsp;S&nbsp;Altman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed statistical methods that can be applied to large sets of count or categorical data in which many features are measured.  The challenge is to detect signals - that is nonrandom patterns - in the presence of the natural variability of the data.  Count and categorical data arise in many settings.  For example, in ecological studies there may be many species of interest and we might count the number of individuals of each species in many sampling locations, or categorize them by a characteristic such as healthy or unhealthy. We consider the species to be features, the locations to be samples and the actual counts or classification to be the measurements.  For macro-scale studies, the features might be a few dozen insect species and the samples might be fields.  For micro-scale studies, the features might be hundreds or thousands of single-cell organisms and the samples might be soil samples from various locations, or even fecal samples from human subjects.  In high throughput biology studies, the features might be tens or hundreds of thousands of species of molecule such as different proteins or messenger RNAs and the samples might be tissues such as biopsy samples.  Categories might be gene variants.  As the number of features measured in our data grow from a few dozen to hundreds of thousands, we expect the information content to grow as well.  However, problems of differentiating between spurious chance patterns in the data and real signals also increase.  Assuming that the percentage of features with true signal stays constant as we increase the number of features measured, we can be over-whelmed by false detections even when the "per feature" false detection rate is quite small.  Typically if we attempt to control the number of false detections by increasing stringency of determining if a signal is present, then we miss more of the weak signals, increasing the false non-detection rate.  Two types of signal were the particular focus of this project: features that exhibit differences in between known subsets of samples (such as different ecosystems or cancerous versus normal tissues) and patterns (in either the features or the samples) which summarize the data by accounting for much of the variability in the measurements.  For the former problem, the project investigated the efficacy of a proposed improvement in statistical testing and considered methods for estimating the false detection and non-detection rates applicable to any test.  For the latter problem, the project developed a statistical framework for some currently available pattern detection methods that will lead to improvements in methodology and computational efficiency, as well as the development of new methods.  The statistical methods developed in this project were applied to high throughput genomics data that quantify genes activity by counting the numbers of molecules of specific types present in the samples.             Last Modified: 01/03/2016       Submitted by: Naomi S Altman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
