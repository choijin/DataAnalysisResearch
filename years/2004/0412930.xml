<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Optimizing Classification Models to Application-Specific Performance Metrics</AwardTitle>
<AwardEffectiveDate>08/15/2004</AwardEffectiveDate>
<AwardExpirationDate>07/31/2008</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>276000</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Many different criteria can be used to train and evaluate classifiers. Different criteria are appropriate in different settings, and learning methods that perform well on one criterion may not perform well on other. If a user must use a specific learning algorithm or model class, but needs to optimize to a performance measure for which that model class is not designed, they cannot do so. For example, neural nets are easy to train for continuous measures such as squared error and cross-entropy, but are difficult to train for discontinuous measures such as accuracy, Lift, and ROC area. SVMs are designed to optimize accuracy, but not squared error or cross-entropy. Decision trees typically optimize information-theoretic measures or accuracy, but are not designed to maximize ROC area or to minimize squared error. Moreover, for some performance metrics such as Lift we do not yet have any effective learning procedures. &lt;br/&gt;&lt;br/&gt;We are developing general-purpose cross-optimization methods for training learning algorithms to any performance measure. More specifically, we are developing meta-algorithms for optimizing the performance of different types of classifiers to metrics other than the one for which they were designed. We are using two meta-learning methods to accomplish this. The first is an ensemble learning method can optimize the performance of an ensemble of base-level classifiers to the user's criterion. The second method is a model adaption procedure that starts with a model optimized to one metric, and then iteratively transforms it into a model that is near-optimal with respect to a different user specified criterion. Both methods are designed to be compatible with most existing supervised learning methods. &lt;br/&gt;&lt;br/&gt;Our work has the potential to substantially improve classifiers by dealing up-front with the performance requirements of real-world applications. The work will have broad impact by giving machine learning users the flexibility to apply the performance metric that best fits their scientific, governmental or commercial needs. Our plans for outreach include distribution of software to aid classifier evaluation on multiple metrics, distributing software for ensemble selection, creating a multiple-metric competition at a conference such as KDD, organizing workshops on multi-metric learning, and building educational modules that demonstrate the importance of performance metrics in application-specific classifier design. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/16/2004</MinAmdLetterDate>
<MaxAmdLetterDate>03/15/2006</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0412930</AwardID>
<Investigator>
<FirstName>Rich</FirstName>
<LastName>Caruana</LastName>
<EmailAddress>caruana@cs.cornell.edu</EmailAddress>
<StartDate>08/16/2004</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thorsten</FirstName>
<LastName>Joachims</LastName>
<EmailAddress>tj@cs.cornell.edu</EmailAddress>
<StartDate>08/16/2004</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>6856</Code>
<Text>ARTIFICIAL INTELL &amp; COGNIT SCI</Text>
</ProgramElement>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
