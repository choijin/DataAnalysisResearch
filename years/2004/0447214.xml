<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Discriminative Syntactic Language Modeling: Automatic Feature Selection and Efficient Annotation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2005</AwardEffectiveDate>
<AwardExpirationDate>03/31/2011</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>527550</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The focus of this proposal is on the effective use of parser-derived and tagger-derived features within discriminative approaches to language modeling for automatic speech recognition.  Discriminative&lt;br/&gt;language modeling approaches provide a tremendous amount of flexibility in defining features, but the size of the potential parser-derived feature space requires efficient feature annotation and  selection algorithms.  The project has four specific aims.  The first aim is to develop a set of efficient, general, and scalable syntactic feature selection algorithms for use with various kinds of annotation  and several parameter estimation techniques.  The second aim is to develop general tree and grammar transformation algorithms designed to preserve selected feature annotations yet lead to faster parsing or  even tagging approximations to parsing.  The third aim is to evaluate a broad range of feature selection and grammar transformation approaches on a large vocabulary continuous speech recognition (LVCSR) task, namely Switchboard.  The final aim is to design and package the algorithms to straightforwardly support future research into other applications, such as machine translation (MT); and into other languages, such as Chinese and Arabic.  The algorithms developed as a part of this project are expected to contribute to improvements in LVCSR accuracy and applications that rely upon this technology.  The algorithms are being packaged into a publicly available software library, enabling researchers working in many application areas including LVCSR and MT and various languages to investigate best practices in syntactic language modeling for their specific task, without having to hand-select and evaluate feature sets.</AbstractNarration>
<MinAmdLetterDate>04/11/2005</MinAmdLetterDate>
<MaxAmdLetterDate>05/18/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0447214</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Roark</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian E Roark</PI_FULL_NAME>
<EmailAddress>roarkbr@gmail.com</EmailAddress>
<PI_PHON>5037481752</PI_PHON>
<NSF_ID>000434316</NSF_ID>
<StartDate>04/11/2005</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon Health &amp; Science University</Name>
<CityName>Portland</CityName>
<ZipCode>972393098</ZipCode>
<PhoneNumber>5034947784</PhoneNumber>
<StreetAddress>3181 S W Sam Jackson Park Rd</StreetAddress>
<StreetAddress2><![CDATA[Mail Code L106OPAM]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>096997515</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OREGON HEALTH &amp; SCIENCE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>096997515</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Oregon Health &amp; Science University, West Campus]]></Name>
<CityName>Beaverton</CityName>
<StateCode>OR</StateCode>
<ZipCode>970068921</ZipCode>
<StreetAddress><![CDATA[20000 N.W. Walker Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>7274</Code>
<Text>HUMAN LANGUAGE &amp; COMMUNICATION</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0105</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2005~100000</FUND_OBLG>
<FUND_OBLG>2006~100000</FUND_OBLG>
<FUND_OBLG>2007~100000</FUND_OBLG>
<FUND_OBLG>2008~105800</FUND_OBLG>
<FUND_OBLG>2009~113750</FUND_OBLG>
<FUND_OBLG>2010~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The focus of this project was on methods to improve statistical modeling for applications such as speech recognition, machine translation and optical character recognition.&nbsp; Such applications produce word sequences -- sentences or fragments of sentences -- corresponding to their input speech, text or images in a target language such as English.&nbsp; For example, a speech recognizer takes a spoken utterance and produces its best guess of the words that were spoken in that utterance.&nbsp; Statistical language models assign scores to these sentences that indicate whether they are "good" sentences or not, where "goodness" roughly means acceptable examples of the language being modeled.&nbsp; In such a manner, all else being equal, the application will output sequences of words that are a better a priori fit to the target language being modeled.&nbsp; Thus the speech recognition system would prefer "their dog" to "they're dog" even though they are acoustically identical.<br /><br />Language models typically simply look at whether each word in a given sequence often goes together with its neighboring words in some large corpus of observed sequences in the language.&nbsp; However, language is a very productive phenomenon, i.e., any given sequence of words -- valid or not -- will contain unobserved subsequences.&nbsp; More information beyond word collocations can be of utility, such as syntactic or morphological structure. Such information, however, unlike word collocations, is not explicit in the word sequence; rather, it is hidden structure that must be annotated by some kind of structural inference algorithm, e.g., syntactic parsing.&nbsp; This project was investigating methods of annotating and exploiting syntactic information to improve language models used in applications like speech recognition.&nbsp; Syntactic parsing is computationally expensive, particularly if a large set of word sequences must be collectively parsed in order to provide language model scores to each of them.&nbsp; For this reason, much of the work in this project involved exploring new methods for fast syntactic parsing, and many important innovations were achieved in that area.<br /><br />Over the five years of the project, we have published nearly 20 papers in leading journals and academic conferences on the topic of efficient annotation of syntactic structure and the use of that structure to support natural language processing applications.&nbsp; Among the accomplishments of the project, we:<br /><br />- Established the utility of syntactic and morphological features in discriminative language models for English and other languages such as Turkish.&nbsp; We found that features derived from part-of-speech tags can yield significant improvements to English speech recognition; and those derived from morphological annotations help in agglutinative languages such as Turkish.<br /><br />- Created new methods for using fast finite-state annotation algorithms to speed up the slower context-free parsing algorithms, and often making them more accurate in addition to faster.&nbsp;&nbsp; Novel cascaded system architectures (e.g., pipeline iteration), system combination approaches, and specialized finite-state classifiers yielded, in aggregate, orders of magnitude speedups of context-free parsing.&nbsp; Of particular note were methods to guarantee improved worst-case complexity bounds on parsing via novel finite-state annotations.<br /><br />- Created new methods for inducing stochastic context-free grammars that have beneficial properties, such as being very compact and efficient to parse with, while still recovering syntactic structure with high accuracy.&nbsp; We found that methods from statistical hypothesis testing could be used to distinguish between syntactic configurations that were unobserved due to sparse data ("sampling zeros") and those that were unobserved due to legitimate syntactic c...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The focus of this project was on methods to improve statistical modeling for applications such as speech recognition, machine translation and optical character recognition.  Such applications produce word sequences -- sentences or fragments of sentences -- corresponding to their input speech, text or images in a target language such as English.  For example, a speech recognizer takes a spoken utterance and produces its best guess of the words that were spoken in that utterance.  Statistical language models assign scores to these sentences that indicate whether they are "good" sentences or not, where "goodness" roughly means acceptable examples of the language being modeled.  In such a manner, all else being equal, the application will output sequences of words that are a better a priori fit to the target language being modeled.  Thus the speech recognition system would prefer "their dog" to "they're dog" even though they are acoustically identical.  Language models typically simply look at whether each word in a given sequence often goes together with its neighboring words in some large corpus of observed sequences in the language.  However, language is a very productive phenomenon, i.e., any given sequence of words -- valid or not -- will contain unobserved subsequences.  More information beyond word collocations can be of utility, such as syntactic or morphological structure. Such information, however, unlike word collocations, is not explicit in the word sequence; rather, it is hidden structure that must be annotated by some kind of structural inference algorithm, e.g., syntactic parsing.  This project was investigating methods of annotating and exploiting syntactic information to improve language models used in applications like speech recognition.  Syntactic parsing is computationally expensive, particularly if a large set of word sequences must be collectively parsed in order to provide language model scores to each of them.  For this reason, much of the work in this project involved exploring new methods for fast syntactic parsing, and many important innovations were achieved in that area.  Over the five years of the project, we have published nearly 20 papers in leading journals and academic conferences on the topic of efficient annotation of syntactic structure and the use of that structure to support natural language processing applications.  Among the accomplishments of the project, we:  - Established the utility of syntactic and morphological features in discriminative language models for English and other languages such as Turkish.  We found that features derived from part-of-speech tags can yield significant improvements to English speech recognition; and those derived from morphological annotations help in agglutinative languages such as Turkish.  - Created new methods for using fast finite-state annotation algorithms to speed up the slower context-free parsing algorithms, and often making them more accurate in addition to faster.   Novel cascaded system architectures (e.g., pipeline iteration), system combination approaches, and specialized finite-state classifiers yielded, in aggregate, orders of magnitude speedups of context-free parsing.  Of particular note were methods to guarantee improved worst-case complexity bounds on parsing via novel finite-state annotations.  - Created new methods for inducing stochastic context-free grammars that have beneficial properties, such as being very compact and efficient to parse with, while still recovering syntactic structure with high accuracy.  We found that methods from statistical hypothesis testing could be used to distinguish between syntactic configurations that were unobserved due to sparse data ("sampling zeros") and those that were unobserved due to legitimate syntactic constraints ("structural zeros").  These methods can be used to impose constraints on grammars of very high utility, yielding compact but accurate grammars.  - Applied these syntactic models to...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
