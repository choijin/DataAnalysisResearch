<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TRANSFORM: flexible voice synthesis through articulatory voice transformation</AwardTitle>
<AwardEffectiveDate>05/15/2005</AwardEffectiveDate>
<AwardExpirationDate>04/30/2009</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>327142</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Many people have always wanted machines to talk to them, but most have strong preferences for particular voices.  Current techniques in speech synthesis can build voices that sound very close to the original speaker, capturing the style, manner and articulation of the source voice.  However such systems require many hours of carefully recorded speech and expert tuning to reach an acceptable level of quality.  An exciting new alternative method for building synthetic voices is voice transformation.  This method uses an existing recorded database and converts it to a target voice using as little as 10-20 sentences.  This technique offers the potential to make speech synthesizers talk in whatever voice desired, with significantly less effort required than previous techniques.&lt;br/&gt;&lt;br/&gt;This project offers a new direction in voice transformation.  Current transformation techniques concentrate on a spectral mapping of the voice, i.e., converting the properties of the speech signal.  Instead we use the underlying positions of the vocal tract articulators (i.e., the position of the teeth, tongue, lips, velum), which give rise to the spectral output of the voice.  Using new statistical modeling techniques we can successfully predict the positions of a speaker's articulators from the speech signal.  Then in the virtual vocal tract domain map between speakers and regenerate the speech for the target voice.&lt;br/&gt;&lt;br/&gt;This work enables the easy construction of new synthetic voices allowing personalization of speech output.  It increases our knowledge of the speech generation process and characterizes what make a voice personal.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>05/10/2005</MinAmdLetterDate>
<MaxAmdLetterDate>04/15/2008</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0414675</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Black</LastName>
<EmailAddress>awb@cs.cmu.edu</EmailAddress>
<StartDate>05/10/2005</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>7274</Code>
<Text>HUMAN LANGUAGE &amp; COMMUNICATION</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
