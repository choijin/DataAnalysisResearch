<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: Empirical Studies on Multi-stream Integration and the Embodied Understanding of Events</AwardTitle>
<AwardEffectiveDate>09/01/2005</AwardEffectiveDate>
<AwardExpirationDate>08/31/2006</AwardExpirationDate>
<AwardTotalIntnAmount>12000.00</AwardTotalIntnAmount>
<AwardAmount>12000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040500</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher T. Kello</SignBlockName>
</ProgramOfficer>
<AbstractNarration>When we talk to one another, we don't just talk.  We gesture, point, look at objects, and touch them; and our speech often refers to objects in our environment.  This is so prevalent that we gesture when we're on the phone, and we describe things differently depending on whether or not the people we're talking to can see us.  If speech is not an isolated cognitive activity, but depends on integrating information from other processes, then how does this integration take place?  How do speakers and listeners integrate language, as it is being performed, with other streams of information from the visual environment?&lt;br/&gt;&lt;br/&gt;Work on gesture analysis and situated cognition has recently provided a good deal of evidence to address this question.  Some of this evidence has come from work by Dr. Sweetser and Ms. Narayan on fine-grained linguistic analyses of utterances and their accompanying bodily movements.  With support from NSF, Ms. Narayan will continue this line of research to investigate the role of timing in the integration of speech and non-speech processes.  The experiments promise to bring together work in linguistics and psychology to bear on the question of how we think when we talk.  The results may lead to more effective methods of communication in teaching, and may lead to insights into the mechanisms of sign language.  The experiments being funded are part of Ms. Narayan's dissertation work.</AbstractNarration>
<MinAmdLetterDate>08/26/2005</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2005</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0450957</AwardID>
<Investigator>
<FirstName>Eve</FirstName>
<LastName>Sweetser</LastName>
<EmailAddress>sweetser@cogsci.berkeley.edu</EmailAddress>
<StartDate>08/26/2005</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7252</Code>
<Text>PERCEPTION, ACTION &amp; COGNITION</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
