<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:   I/UCRC:    Safety Security Rescue Research Center (SSR-RC)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2004</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>532750</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Lawrence A. Hornak</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This multi-university Industry/University Cooperative Research Center for Safety, Security and Rescue Research located at the University of South Florida and the University of Minnesota will bring together industry, academe, and public sector users together to provide integrative robotics and artificial intelligence solutions in robotics for activities conducted by the police, FBI, FEMA, firefighting, transportation safety, and emergency response to mass casuality-related activities.  The need for safety, security, and rescue technologies has accelerated in the aftermath of 9/11 and a new research community is forming, as witnessed by the first IEEE Workshop on Safety, Security and Rescue Robotics in February 2003.  &lt;br/&gt;&lt;br/&gt;The Center will be built upon the knowledge and expertise of multi-disciplinary researchers in computer science, engineering, industrial organization, psychology, public health, and marine sciences at the University of South Florida (the lead institution) and the University of Minnesota.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/12/2004</MinAmdLetterDate>
<MaxAmdLetterDate>06/12/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0443945</AwardID>
<Investigator>
<FirstName>Maria</FirstName>
<LastName>Gini</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maria L Gini</PI_FULL_NAME>
<EmailAddress>gini@cs.umn.edu</EmailAddress>
<PI_PHON>6126255582</PI_PHON>
<NSF_ID>000468121</NSF_ID>
<StartDate>08/12/2004</StartDate>
<EndDate>07/31/2007</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nikolaos</FirstName>
<LastName>Papanikolopoulos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nikolaos Papanikolopoulos</PI_FULL_NAME>
<EmailAddress>npapas@cs.umn.edu</EmailAddress>
<PI_PHON>6126250163</PI_PHON>
<NSF_ID>000205563</NSF_ID>
<StartDate>06/12/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nikolaos</FirstName>
<LastName>Papanikolopoulos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nikolaos Papanikolopoulos</PI_FULL_NAME>
<EmailAddress>npapas@cs.umn.edu</EmailAddress>
<PI_PHON>6126250163</PI_PHON>
<NSF_ID>000205563</NSF_ID>
<StartDate>08/12/2004</StartDate>
<EndDate>07/20/2007</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Voyles</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard M Voyles</PI_FULL_NAME>
<EmailAddress>rvoyles@purdue.edu</EmailAddress>
<PI_PHON>7654943733</PI_PHON>
<NSF_ID>000293618</NSF_ID>
<StartDate>08/12/2004</StartDate>
<EndDate>07/20/2007</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stergios</FirstName>
<LastName>Roumeliotis</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stergios I Roumeliotis</PI_FULL_NAME>
<EmailAddress>stergios@cs.umn.edu</EmailAddress>
<PI_PHON>6126267507</PI_PHON>
<NSF_ID>000487045</NSF_ID>
<StartDate>08/12/2004</StartDate>
<EndDate>06/12/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tasoulla</FirstName>
<LastName>Hadjiyanni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tasoulla Hadjiyanni</PI_FULL_NAME>
<EmailAddress>thadjiya@umn.edu</EmailAddress>
<PI_PHON>6126261245</PI_PHON>
<NSF_ID>000282991</NSF_ID>
<StartDate>10/29/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 OAK ST SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0400000</Code>
<Name>Industry University - Co-op</Name>
</FoaInformation>
<ProgramElement>
<Code>2890</Code>
<Text>CISE Research Resources</Text>
</ProgramElement>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1049</Code>
<Text>INDUSTRY-UNIV COOPERATIVE RSCH PROJECTS</Text>
</ProgramReference>
<ProgramReference>
<Code>115E</Code>
<Text>RESEARCH EXP FOR TEACHERS</Text>
</ProgramReference>
<ProgramReference>
<Code>116E</Code>
<Text>RESEARCH EXP FOR UNDERGRADS</Text>
</ProgramReference>
<ProgramReference>
<Code>122E</Code>
<Text>CENTERS: OTHER INDUSTRY-UNIV</Text>
</ProgramReference>
<ProgramReference>
<Code>5761</Code>
<Text>INDUSTRY/UNIV COOP RES CENTERS</Text>
</ProgramReference>
<ProgramReference>
<Code>7218</Code>
<Text>RET SUPP-Res Exp for Tchr Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>8039</Code>
<Text>Information, Communication &amp; Computing</Text>
</ProgramReference>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0104</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0105</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2004~50000</FUND_OBLG>
<FUND_OBLG>2005~50000</FUND_OBLG>
<FUND_OBLG>2006~50000</FUND_OBLG>
<FUND_OBLG>2007~60000</FUND_OBLG>
<FUND_OBLG>2008~145000</FUND_OBLG>
<FUND_OBLG>2009~169750</FUND_OBLG>
<FUND_OBLG>2012~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>One of the projects at the Center studied the contents and use of shared mental models for mixed teams of humans, robots, and agents in the context of urban search and rescue. The study was supported by an agent based simulator that was used for modeling complex systems with hundreds of agents in a large city and to study their behaviors under different circumstances. Agents were modeled as autonomous entities that interacted with other agents while moving around the city to search for victims and to stop fires from spreading. Humans&nbsp; interacted with the agents in making decisions and carrying them out. We&nbsp; focused on the use of shared team mental models in those mixed human-robot-agent teams, in situations where information was uncertain, and communication was limited and unreliable. Specific outcomes were:<br />1. development of &nbsp;shared mental models for mixed teams of humans, robots, and agents;</p> <p>2. study of the effects of different levels of shared mental models on team performance;</p> <p>3. measurement of &nbsp;the performance of the shared team mental models in complex urban search and rescue environments with uncertain knowledge and limited communication.</p> <p>&nbsp;</p> <p>Another outcome focused on incremental learning of people appearances for tracking across cameras. Modern surveillance systems have come a long way in the last decade having being transformed into automata providing security warnings about activities recorded in video streams. However, it is unrealistic to believe that these systems can be fully automatic given the unacceptable high rates of false alarms under which they operate. It is also often the case that security warnings generated by the various cameras in a surveillance network are viewed in isolation. Therefore, interpretation of activities resulting from analysis of video sequences characterized by limited temporal and spatial scope is an impossible task not only for computer/camera systems but for humans as well. This work put forth a new paradigm of video interpretation and information linking amongst multiple cameras. It assumed availability of information extracted by low level processing video streams, it continued with supervised learning for data organization and management across time and space and it finally ended with visualization of activity pertinent to human initiated queries. Specifically, we introduced a technique for dynamic clustering of people appearances by taking advantage of availability of data extracted from low level vision processes such as tracking and labeled data provided by the user who escalates security hypotheses. We also used supervised clustering to explore the manifold of people appearances in an incremental (or dynamic) fashion.<br /><br />Another outcome was about dictionary learning for robust background subtraction.&nbsp; Background subtraction is a&nbsp;fundamental task in many computer vision applications, such as robotics and automated surveillance systems. The performance of high-level vision tasks such as object detection and tracking is dependent on effective foreground detection techniques. In this work, we have developed a novel background modeling algorithm that represents the background as a linear combination of dictionary atoms and the foreground as a sparse error, when one uses the respective set of dictionary atoms as basis elements to linearly approximate/ reconstruct a new image. The&nbsp;dictionary atoms represent variations of the background model and they are learned from a batch of randomly sampled training frames. The sparse foreground is estimated during both training and performance phases, and this is formulated as a Lasso problem, while the dictionary update step in the training phase is motivated from the K-SVD algorithm. Our method works well in the presence of foreground in the training frames, and also gives the foreground masks for the ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ One of the projects at the Center studied the contents and use of shared mental models for mixed teams of humans, robots, and agents in the context of urban search and rescue. The study was supported by an agent based simulator that was used for modeling complex systems with hundreds of agents in a large city and to study their behaviors under different circumstances. Agents were modeled as autonomous entities that interacted with other agents while moving around the city to search for victims and to stop fires from spreading. Humans  interacted with the agents in making decisions and carrying them out. We  focused on the use of shared team mental models in those mixed human-robot-agent teams, in situations where information was uncertain, and communication was limited and unreliable. Specific outcomes were: 1. development of  shared mental models for mixed teams of humans, robots, and agents;  2. study of the effects of different levels of shared mental models on team performance;  3. measurement of  the performance of the shared team mental models in complex urban search and rescue environments with uncertain knowledge and limited communication.     Another outcome focused on incremental learning of people appearances for tracking across cameras. Modern surveillance systems have come a long way in the last decade having being transformed into automata providing security warnings about activities recorded in video streams. However, it is unrealistic to believe that these systems can be fully automatic given the unacceptable high rates of false alarms under which they operate. It is also often the case that security warnings generated by the various cameras in a surveillance network are viewed in isolation. Therefore, interpretation of activities resulting from analysis of video sequences characterized by limited temporal and spatial scope is an impossible task not only for computer/camera systems but for humans as well. This work put forth a new paradigm of video interpretation and information linking amongst multiple cameras. It assumed availability of information extracted by low level processing video streams, it continued with supervised learning for data organization and management across time and space and it finally ended with visualization of activity pertinent to human initiated queries. Specifically, we introduced a technique for dynamic clustering of people appearances by taking advantage of availability of data extracted from low level vision processes such as tracking and labeled data provided by the user who escalates security hypotheses. We also used supervised clustering to explore the manifold of people appearances in an incremental (or dynamic) fashion.  Another outcome was about dictionary learning for robust background subtraction.  Background subtraction is a fundamental task in many computer vision applications, such as robotics and automated surveillance systems. The performance of high-level vision tasks such as object detection and tracking is dependent on effective foreground detection techniques. In this work, we have developed a novel background modeling algorithm that represents the background as a linear combination of dictionary atoms and the foreground as a sparse error, when one uses the respective set of dictionary atoms as basis elements to linearly approximate/ reconstruct a new image. The dictionary atoms represent variations of the background model and they are learned from a batch of randomly sampled training frames. The sparse foreground is estimated during both training and performance phases, and this is formulated as a Lasso problem, while the dictionary update step in the training phase is motivated from the K-SVD algorithm. Our method works well in the presence of foreground in the training frames, and also gives the foreground masks for the training frames as a by-product of the batch training phase. Experimental validation was provided on standard datasets with ground truth info...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
