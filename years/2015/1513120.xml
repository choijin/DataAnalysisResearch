<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>II-New: Infrastructure for Energy-Aware High Performance Computing (HPC) and Data Analytics on Heterogeneous Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>898685.00</AwardTotalIntnAmount>
<AwardAmount>898685</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sankar Basu</SignBlockName>
<PO_EMAI>sabasu@nsf.gov</PO_EMAI>
<PO_PHON>7032927843</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project builds a comprehensive research infrastructure to meet the needs of a large research team at the Ohio State University allowing experimental research in a number of computer science areas including high performance computing, data analytics, storage, and virtualization. The project will lead to significant advances in many computer science areas, and its impact will be enhanced through active dissemination of software from the investigators. The project will contribute substantially to human resource development, education in computer science increasing diversity in related areas.  &lt;br/&gt;&lt;br/&gt;More specifically, researchers will acquire and deploy an infrastructure that includes three types of accelerators, conventional as well as energy-efficient nodes, large main memory including Solid State Drive (SSD) on a subset of nodes, and hardware for fine-grained power measurements. The requested resources will allow experimentation with a number of popular or emerging cluster configurations that address needs of a variety of compute-intensive and/or data-intensive workloads.  Such an internally hosted and reconfigurable cluster will also allow power measurements, voltage margin reduction experiments, and failure detection and recovery studies in the presence of physical failures - none of which is typically feasible at national supercomputing and cloud infrastructures. The infrastructure is motivated by the multiple paradigm shifts that high performance computing is presently undergoing. They include increasing use of accelerators or coprocessors, increased criticality of energy and resilience beyond performance, synergy with data analytic applications, and popularity of massive main memory or SSD technologies. These trends provide opportunities and challenges for a variety of scientific, medical, and enterprise applications to be explored in this project.</AbstractNarration>
<MinAmdLetterDate>06/22/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/22/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1513120</AwardID>
<Investigator>
<FirstName>Ponnuswamy</FirstName>
<LastName>Sadayappan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ponnuswamy Sadayappan</PI_FULL_NAME>
<EmailAddress>saday@cs.utah.edu</EmailAddress>
<PI_PHON>6142164213</PI_PHON>
<NSF_ID>000182536</NSF_ID>
<StartDate>06/22/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Xiaodong</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaodong Zhang</PI_FULL_NAME>
<EmailAddress>zhang@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142922770</PI_PHON>
<NSF_ID>000416471</NSF_ID>
<StartDate>06/22/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dhabaleswar K Panda</PI_FULL_NAME>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142925199</PI_PHON>
<NSF_ID>000487085</NSF_ID>
<StartDate>06/22/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gagan</FirstName>
<LastName>Agrawal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gagan Agrawal</PI_FULL_NAME>
<EmailAddress>gagrawal@augusta.edu</EmailAddress>
<PI_PHON>7067210506</PI_PHON>
<NSF_ID>000230838</NSF_ID>
<StartDate>06/22/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Umit</FirstName>
<LastName>Catalyurek</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Umit V Catalyurek</PI_FULL_NAME>
<EmailAddress>umit@gatech.edu</EmailAddress>
<PI_PHON>4048942592</PI_PHON>
<NSF_ID>000227926</NSF_ID>
<StartDate>06/22/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName/>
<StateCode>OH</StateCode>
<ZipCode>432101277</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~898685</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the project is to&nbsp; acquire and deploy an infrastructure that includes three different types of accelerators, conventional as well as energy-efficient nodes, large main memory and SSDs (on a subset of nodes), and hardware for fine-grained power measurements. The main objectives were to carry out state-of-the-art systems research.</p> <p>The cluster was acquired and installed in two phases involving state-of-the-art Intel processors, NVIDIA GPUs, High-performance InfiniBand networks and fast SSDs.</p> <p>The infrastructure has allowed around 15 faculty members and their students (around 90) to experiment with a number of popular or emerging cluster configurations that address needs of a variety of compute-intensive and/or data-intensive workloads. This has resulted into carrying out state-of-the-art research in multiple areas: architecture, programming models, compilers, storage and I/O, big data, deep learning, data mining, graph analytics and fault-tolerance. These research directions have led to a large number of journal, refereed conference and referred workshop publications. Around 15 of these publications have been recognized as Best Paper Awards/Finalists. Several of these projects (including MVAPICH, HiBD, HiDL, Tensor contraction, SEP-Graph, Direct Load, HYPHA, and Catfish) have led to open-source software releases. These software releases are widely used in clusters, supercomputers and datacenters worldwide.</p> <p>The infrastructure has helped significantly to train around 90 students (PhDs, Masters, and Undergraduates) and 10 post-docs in multiple areas related to systems research. Some of these students and post-docs have also been trained in interdisciplinary research.</p> <p>The infrastructure has also been used in classroom teaching by the faculty members. The emphasis has been to carry out graduate-level class projects using the state-of-the-art processors, GPUs, networking and storage. Around 200 students from different classes have benefited from such teaching.</p><br> <p>            Last Modified: 11/03/2019<br>      Modified by: Dhabaleswar&nbsp;K&nbsp;Panda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the project is to  acquire and deploy an infrastructure that includes three different types of accelerators, conventional as well as energy-efficient nodes, large main memory and SSDs (on a subset of nodes), and hardware for fine-grained power measurements. The main objectives were to carry out state-of-the-art systems research.  The cluster was acquired and installed in two phases involving state-of-the-art Intel processors, NVIDIA GPUs, High-performance InfiniBand networks and fast SSDs.  The infrastructure has allowed around 15 faculty members and their students (around 90) to experiment with a number of popular or emerging cluster configurations that address needs of a variety of compute-intensive and/or data-intensive workloads. This has resulted into carrying out state-of-the-art research in multiple areas: architecture, programming models, compilers, storage and I/O, big data, deep learning, data mining, graph analytics and fault-tolerance. These research directions have led to a large number of journal, refereed conference and referred workshop publications. Around 15 of these publications have been recognized as Best Paper Awards/Finalists. Several of these projects (including MVAPICH, HiBD, HiDL, Tensor contraction, SEP-Graph, Direct Load, HYPHA, and Catfish) have led to open-source software releases. These software releases are widely used in clusters, supercomputers and datacenters worldwide.  The infrastructure has helped significantly to train around 90 students (PhDs, Masters, and Undergraduates) and 10 post-docs in multiple areas related to systems research. Some of these students and post-docs have also been trained in interdisciplinary research.  The infrastructure has also been used in classroom teaching by the faculty members. The emphasis has been to carry out graduate-level class projects using the state-of-the-art processors, GPUs, networking and storage. Around 200 students from different classes have benefited from such teaching.       Last Modified: 11/03/2019       Submitted by: Dhabaleswar K Panda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
