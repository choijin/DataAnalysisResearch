<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>The scope of signed phonological generalizations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>297414.00</AwardTotalIntnAmount>
<AwardAmount>297414</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tyler Kendall</SignBlockName>
<PO_EMAI>tkendall@nsf.gov</PO_EMAI>
<PO_PHON>7032922434</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Every language includes a phonological system that expresses words as patterns of meaningless elements. Moreover, people readily generalize these patterns to novel forms that they have never heard before. For example, upon hearing the forms "bagogo" and "malulu", people rapidly extract the ABB pattern and they extend it to "wufifi". These results from spoken languages are significant because they suggest that human brains encode linguistic phonological patterns by means of abstract rules. Phonological patterns however, are not limited to spoken languages. Like their spoken counterparts, every sign language exhibits (manual) phonological patterns, and signers extend those patterns to novel forms. Accordingly, one can ask whether rules form part of all languages, or of spoken systems alone.  Similarities (and differences) in the design of signed and spoken languages are significant because they allow us to determine whether some features of natural language are core to the human biological capacity for language, irrespective of the specific modality (speech vs. manual gestures) in which it is transmitted. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to gauge the role of rules in sign language phonology. To this end, this investigation will examine the scope of generalizations afforded by sign language phonology to determine whether these generalizations are best captured by algebraic or non-algebraic (statistical and visual/phonetic) mechanisms. As a case study, Dr. Berent will investigate the reduplication rule in American Sign Language. Among the issues to be addressed are: whether signers freely generalize the reduplication pattern across the board--irrespective of the similarity of novel items to familiar signed elements; and whether reduplication relies on linguistic knowledge or visual repetition. For example, experiments will examine whether reduplication depends on various linguistically-relevant variables such as the lexical status of the signs; and whether it is dependant on participants' linguistic experience with ASL. The project also includes a set of Near Infrared Spectroscopy (NIRS) studies to explore sensitivity to reduplication in neonates. Results will shed light on the basis of the human capacity for language. By exploring the structure of typical phonological systems, the findings from this project are relevant to our understanding of the host of clinical disorders linked to phonology, ranging from specific language impairment to dyslexia. This investigation of signed phonological systems may also help promote reading gains among deaf individuals by informing research-based educational practices.</AbstractNarration>
<MinAmdLetterDate>06/08/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/08/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1528411</AwardID>
<Investigator>
<FirstName>Iris</FirstName>
<LastName>Berent</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Iris Berent</PI_FULL_NAME>
<EmailAddress>i.berent@neu.edu</EmailAddress>
<PI_PHON>6173734033</PI_PHON>
<NSF_ID>000648194</NSF_ID>
<StartDate>06/08/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northeastern University</Name>
<CityName>BOSTON</CityName>
<ZipCode>021155005</ZipCode>
<PhoneNumber>6173733004</PhoneNumber>
<StreetAddress>360 HUNTINGTON AVE</StreetAddress>
<StreetAddress2><![CDATA[177-500]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001423631</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHEASTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001423631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northeastern University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>021155005</ZipCode>
<StreetAddress><![CDATA[360 Huntington Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~297414</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Every human language forms words by combining meaningless elements, either speech sounds (in spoken language) or manual gestures (for sign languages). These elements, moreover, pattern together in specific ways. For example, in English, we ban <em>drinking</em> and driving, not<em> rdinking</em> and <em>rdiving</em>. How human brains encode such patterns, however, is a matter of debate.</p> <p>Many laypeople believe that English speakers don't rdive because such patterns are impossible for humans to articulate. But other languages actually allow similar patterns&nbsp; (Russian: rzhan, "zealous"; Polish: rte&#796;c&#769; "mercury"). Linguists thus suggest a different explanation: certain patterns are disliked because they violate abstract tacit rules of language.</p> <p>This project sought to adjudicate between these possibilities using an unexpected source: sign language. Like spoken language, sign languages convey meaning by patterning meaningless elements. In fact, some of these patterns resemble those of spoken languages. For example, many spoken words exhibit syllable doubling (e.g., <em>papa</em>); sign languages do the same. Here, we asked whether doubling relies on an abstract rule (XX, where X is any syllable). Results suggest that such abstract rules might indeed by encoded by human brains.</p> <p>First, Deaf signers are sensitive to the structure of signs in their native language (American Sign Language, ASL) even if these signs are entirely novel: they systematically prefer XX signs (two identical syllables, akin to papa) to XY signs (two different syllables, akin to pada). This suggests that they extract the abstract XX pattern, rather than rely only on rote memory.</p> <p>A second piece of evidence in support of abstract rules in signs is presented by the behavior of speakers who have had no previous experience with a sign language. Many laypeople believe that sign languages are patent to nonsigners. This intuition is false, as sign languages are not pantomimes. Yet some of the <em>abstract structure</em> of a sign language can be extracted even by naive speakers who have no command of a sign language.&nbsp;</p> <p>As noted, sign languages exhibit the pattern of doubling, much like papa and mama, in English. Now, if English speakers encoded this pattern by an abstract rule (e.g., XX, where X is any syllable), and if they can further identify a syllable in sign languages (which they can), then it is conceivable that doubling rules from their <strong>spoken language </strong>would apply to signs. Consequently, naive English speakers, for instance, might spontaneously extract doubling in signs (e.g., they might prefer XX to XY signs). And since the specific linguistic constraints on doubling differ across languages (e.g., English generally avoids doubling, Hebrew enforces it), then the responses of naive speakers to <strong>signs</strong> might depend on the structure of their <strong>spoken</strong> language. This is exactly what we found. Not only did we find that speakers of various languages were sensitive to doubling, but the specific pattern of responses differed systematically for speakers of English, Hebrew, Mandarin, and Malayalam, in line with the distinct structure of these spoken languages. These results show for the first time that naive speakers can spontaneously extract some of the abstract grammatical structure of signs.&nbsp;</p> <p>Human brains, it seems can extract linguistic structure from different language modalities--both speech and signs. The final component of this project specifically tested this possibility. Here, we asked whether the brains of six-month old infants, from hearing families, extract the doubling structure of signs. We addressed this question using Near Infrared Spectroscopy--a technique that traces the absorption of infrared light by the brain. Results suggest that infants do extract the doubling structure of signs, and their responses to signs engage the same brain network that encodes doubling in speech, and distinct from the one engaged by nonlinguistic visual stimuli.</p> <p>These results have several broad implications. First, they suggest that humans--both speakers and signers--encode linguistic patterns by relying on powerful abstract rules. Second, some rules spontaneously transfer from one language modality (e.g., English) to another (ASL, using signs). Finally, young infants are language ready--they can deploy their brain language network to extract linguistic patterns in any modality of natural language--speech or sign.</p> <p>In addition to their scientific significance, these conclusions also address several translational matters that are of broad societal interest. First, these results unveil the linguistic communication of Deaf signers--a minority group that is still underrepresented in current linguistic research. Moreover, our conclusions potentially speak to a fierce ongoing controversy on the role of sign language in Deaf education. Our findings that linguistic principles from one language modality transfer to another might suggest that education in ASL not only strengthen signers' command of their native language but it could potentially benefit the subsequent acquisition of spoken language, like English.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/02/2020<br>      Modified by: Iris&nbsp;Berent</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Every human language forms words by combining meaningless elements, either speech sounds (in spoken language) or manual gestures (for sign languages). These elements, moreover, pattern together in specific ways. For example, in English, we ban drinking and driving, not rdinking and rdiving. How human brains encode such patterns, however, is a matter of debate.  Many laypeople believe that English speakers don't rdive because such patterns are impossible for humans to articulate. But other languages actually allow similar patterns  (Russian: rzhan, "zealous"; Polish: rte&#796;c&#769; "mercury"). Linguists thus suggest a different explanation: certain patterns are disliked because they violate abstract tacit rules of language.  This project sought to adjudicate between these possibilities using an unexpected source: sign language. Like spoken language, sign languages convey meaning by patterning meaningless elements. In fact, some of these patterns resemble those of spoken languages. For example, many spoken words exhibit syllable doubling (e.g., papa); sign languages do the same. Here, we asked whether doubling relies on an abstract rule (XX, where X is any syllable). Results suggest that such abstract rules might indeed by encoded by human brains.  First, Deaf signers are sensitive to the structure of signs in their native language (American Sign Language, ASL) even if these signs are entirely novel: they systematically prefer XX signs (two identical syllables, akin to papa) to XY signs (two different syllables, akin to pada). This suggests that they extract the abstract XX pattern, rather than rely only on rote memory.  A second piece of evidence in support of abstract rules in signs is presented by the behavior of speakers who have had no previous experience with a sign language. Many laypeople believe that sign languages are patent to nonsigners. This intuition is false, as sign languages are not pantomimes. Yet some of the abstract structure of a sign language can be extracted even by naive speakers who have no command of a sign language.   As noted, sign languages exhibit the pattern of doubling, much like papa and mama, in English. Now, if English speakers encoded this pattern by an abstract rule (e.g., XX, where X is any syllable), and if they can further identify a syllable in sign languages (which they can), then it is conceivable that doubling rules from their spoken language would apply to signs. Consequently, naive English speakers, for instance, might spontaneously extract doubling in signs (e.g., they might prefer XX to XY signs). And since the specific linguistic constraints on doubling differ across languages (e.g., English generally avoids doubling, Hebrew enforces it), then the responses of naive speakers to signs might depend on the structure of their spoken language. This is exactly what we found. Not only did we find that speakers of various languages were sensitive to doubling, but the specific pattern of responses differed systematically for speakers of English, Hebrew, Mandarin, and Malayalam, in line with the distinct structure of these spoken languages. These results show for the first time that naive speakers can spontaneously extract some of the abstract grammatical structure of signs.   Human brains, it seems can extract linguistic structure from different language modalities--both speech and signs. The final component of this project specifically tested this possibility. Here, we asked whether the brains of six-month old infants, from hearing families, extract the doubling structure of signs. We addressed this question using Near Infrared Spectroscopy--a technique that traces the absorption of infrared light by the brain. Results suggest that infants do extract the doubling structure of signs, and their responses to signs engage the same brain network that encodes doubling in speech, and distinct from the one engaged by nonlinguistic visual stimuli.  These results have several broad implications. First, they suggest that humans--both speakers and signers--encode linguistic patterns by relying on powerful abstract rules. Second, some rules spontaneously transfer from one language modality (e.g., English) to another (ASL, using signs). Finally, young infants are language ready--they can deploy their brain language network to extract linguistic patterns in any modality of natural language--speech or sign.  In addition to their scientific significance, these conclusions also address several translational matters that are of broad societal interest. First, these results unveil the linguistic communication of Deaf signers--a minority group that is still underrepresented in current linguistic research. Moreover, our conclusions potentially speak to a fierce ongoing controversy on the role of sign language in Deaf education. Our findings that linguistic principles from one language modality transfer to another might suggest that education in ASL not only strengthen signers' command of their native language but it could potentially benefit the subsequent acquisition of spoken language, like English.          Last Modified: 03/02/2020       Submitted by: Iris Berent]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
