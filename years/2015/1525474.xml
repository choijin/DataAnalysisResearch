<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CNS:CSR Collaborative Research: Leveraging Intra-chip/Inter-chip Silicon-Photonic Networks for Designing Next-Generation Accelerators</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>249828.00</AwardTotalIntnAmount>
<AwardAmount>249828</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A little over a decade ago, GPUs were fixed-function processors built around a pipeline, dedicated to rendering 3-D graphics. In the past decade, as the potential for GPUs to provide massive compute parallelism became apparent, the software community developed new programming environments (CUDA and OpenCL) to leverage these massively parallel devices. Today, the leading graphics vendors tailor their GPU designs for general-purpose high-performance computing by providing higher compute throughput. While GPUs are able to provide high computational throughput by launching a large number of threads, memory bandwidth and system power continue to be limiting constraints for a number of applications.  &lt;br/&gt;&lt;br/&gt;This project proposes to explore the use of silicon-photonic link technology to design the intra-chip and inter-chip networks in future GPU/APU systems with the goal of addressing the memory bandwidth constraint. In particular, we will investigate the potential of the silicon-photonic networks to efficiently support the memory hierarchy of a heterogeneous design, which integrates a CPU and a GPU (i.e., an Accelerated Processing Unit (APU)) that share the same memory address space. To this end, using state-of-the-art cycle-based simulators, we will evaluate a range of memory hierarchies and intra-chip/inter-chip silicon-photonic network architectures while running a demanding set of workloads that require high memory bandwidth. Additionally, this work will explore the limits and opportunities to leverage the high-bandwidth density and low latency of the silicon-photonic networks to architect the next generations APU devices that can provide better compute throughput. It is anticipated that the results of this research will clearly demonstrate the advantages of using silicon-photonic intra-chip/inter-chip networks in the memory hierarchy of GPU and APU devices. To catalyze and sustain research in this area of APUs and silicon-photonic networks, an open-source simulator that supports intra-chip and inter-chip silicon-photonic networks will be made available to the wider accelerator/networking research communities.&lt;br/&gt;&lt;br/&gt;At a broader level, this work bridges the gap between the heterogeneous systems community and the on-chip/off-chip networks community and opens multiple opportunities for education and outreach.</AbstractNarration>
<MinAmdLetterDate>08/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1525474</AwardID>
<Investigator>
<FirstName>Ajay</FirstName>
<LastName>Joshi</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ajay J Joshi</PI_FULL_NAME>
<EmailAddress>joshi@bu.edu</EmailAddress>
<PI_PHON>6173534840</PI_PHON>
<NSF_ID>000554674</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress><![CDATA[881 Commonwealth Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~249828</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-559367df-7fff-e863-6ae6-ab0ab5e75770"><span>Machine Learning (ML) has pervaded pretty much every aspect of our day-to-day lives. ML is commonly used in a growing number of fields including, but not limited to, weather forecasting, e-commerce, healthcare and security. One of the biggest challenges with these ML-based applications is the development of ML models based on training with mountains of data. Graphics Processing Unit (GPU) systems are commonly used today for training of ML models in pretty much all ML-based applications. Unfortunately, this training of ML models takes several days, sometime several weeks. The high-level goal of this project was to develop novel system architectures for single-GPU systems and multi-GPU systems to improve their overall system performance, and in turn, speed up the process of training ML workloads. We focused on developing novel electrical and silicon-photonic NoC architectures for improving the performance of single-GPU systems, developing a unified memory hierarchy for improving the performance of discrete multi-GPU systems, evaluating the bottlenecks in today&rsquo;s multi-GPU systems when running workloads involving the training of ML models, developing a highly adaptable multi-GPU simulator for modeling and evaluation of multi-GPU systems, developing a novel page migration mechanism for improving the performance of multi-GPU systems, and developing a novel cache coherency mechanism for reducing the programmer effort. All these new network and memory architectures that were developed as part of this project directly contributed to the overall improvement of the single-GPU and multi-GPU systems.&nbsp; A by-product of our simulator effort included the Akita simulation framework.  Our research was disseminated to the broader community through seven peer-reviewed publications and tutorials on our simulator. In terms of project personnel, six PhD students worked on this project.  Each of the students had the opportunity to present their work at various conferences. These students also got an opportunity to engage in industry internships/coops during their PhD programs, which enabled them to be better prepared when starting their respective future careers in industry and academia.  Two out of these six PhD students have graduated and are currently working in the computing industry. The remaining four PhD students are expected to graduate in 2020, and are expected to either take an academic job or an industry job. As part of this project we also collaborated with researchers from Spain, South Korea and AMD.</span></span></p><br> <p>            Last Modified: 12/23/2019<br>      Modified by: Ajay&nbsp;J&nbsp;Joshi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Machine Learning (ML) has pervaded pretty much every aspect of our day-to-day lives. ML is commonly used in a growing number of fields including, but not limited to, weather forecasting, e-commerce, healthcare and security. One of the biggest challenges with these ML-based applications is the development of ML models based on training with mountains of data. Graphics Processing Unit (GPU) systems are commonly used today for training of ML models in pretty much all ML-based applications. Unfortunately, this training of ML models takes several days, sometime several weeks. The high-level goal of this project was to develop novel system architectures for single-GPU systems and multi-GPU systems to improve their overall system performance, and in turn, speed up the process of training ML workloads. We focused on developing novel electrical and silicon-photonic NoC architectures for improving the performance of single-GPU systems, developing a unified memory hierarchy for improving the performance of discrete multi-GPU systems, evaluating the bottlenecks in todayâ€™s multi-GPU systems when running workloads involving the training of ML models, developing a highly adaptable multi-GPU simulator for modeling and evaluation of multi-GPU systems, developing a novel page migration mechanism for improving the performance of multi-GPU systems, and developing a novel cache coherency mechanism for reducing the programmer effort. All these new network and memory architectures that were developed as part of this project directly contributed to the overall improvement of the single-GPU and multi-GPU systems.  A by-product of our simulator effort included the Akita simulation framework.  Our research was disseminated to the broader community through seven peer-reviewed publications and tutorials on our simulator. In terms of project personnel, six PhD students worked on this project.  Each of the students had the opportunity to present their work at various conferences. These students also got an opportunity to engage in industry internships/coops during their PhD programs, which enabled them to be better prepared when starting their respective future careers in industry and academia.  Two out of these six PhD students have graduated and are currently working in the computing industry. The remaining four PhD students are expected to graduate in 2020, and are expected to either take an academic job or an industry job. As part of this project we also collaborated with researchers from Spain, South Korea and AMD.       Last Modified: 12/23/2019       Submitted by: Ajay J Joshi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
