<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: RobotSLANG: Simultaneous Localization, Mapping, and Language Acquisition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>649999.00</AwardTotalIntnAmount>
<AwardAmount>649999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Humans and robots alike have a critical need to navigate through new environments to carry out everyday tasks.  A parent and child may be touring a college campus; a robot may be searching for survivors after a building has collapsed.  In this collaboration by faculty at two institutions, the PIs envision human and robotic partners sharing common perceptual-linguistic experiences and cooperating in mundane tasks like janitorial work and home care as well as in critical tasks like emergency response or search-and-rescue.  But while mapping and navigation are now commonplace for mobile robots, when considering human-robot collaboration for even simple tasks one is confronted by a critical barrier: robots and people do not share a common language.  Human language is rich in linguistic elements for describing our spatial environment, the objects and places within it, and navigable paths through it (e.g., "go down the hallway and enter the third door on the right.").  Robots, on the other hand, inhabit a metric world of occupied and unoccupied discretized grid cells, wherein most objects are devoid of meaning (semantics).  The PIs' goal in this project is to overcome this limitation by conjoining the well understood problem of simultaneous localization and mapping (SLAM) with that of language acquisition, in order to enable robots to learn to communicate with people in English about navigation tasks.  The PIs will spur interest in this novel research area within the scientific community by means of an Amazing Race challenge problem modeled after the reality television show of the same name, which will place robots and human-robot teams in unknown environments and charge them with completing a specific task as quickly as possible.  Other outreach activities will include visits to K-12 schools with demonstrations.  &lt;br/&gt;&lt;br/&gt;This work will focus on simultaneous localization, mapping, and language acquisition, a field of inquiry that remains untouched.  The crucial principles are that semantics are formulated as a cost function, which in turn specifies a joint distribution over many variables including those capturing sensory input, language, the environment map, and robot motor control.  The cost function and joint distribution support standard inference of many forms, such as command following.  More importantly, they support multidirectional inference over multiple variable sets jointly, such as simultaneous mapping and language interpretation.  Within this innovative multivariate optimization-based framework, the PIs plan a thorough experimental regimen including both synthetic and real-world datasets of challenging environments, grounding the semantics of natural language in spatial maps of the realistic visual world and robot motor control, while navigating along particular paths or to arrive at particular destinations in (possibly novel) environments that are mapped not only in a geometric sense but also with linguistic underpinning to these particular paths and destinations.  The language approach is compositional and uses spatially-grounded representations of nouns (objects/places) and prepositions (relations between them).  These spatially-grounded representations will be modeled in the context of mapping.  Furthermore, the PIs will consider realistic environments and adapt visual models thereof according to the joint model.  The PIs are aware of no other work that jointly models mapping, vision, and language acquisition.</AbstractNarration>
<MinAmdLetterDate>08/06/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/06/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1522904</AwardID>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Corso</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jason J Corso</PI_FULL_NAME>
<EmailAddress>jcorso@stevens.edu</EmailAddress>
<PI_PHON>7162083671</PI_PHON>
<NSF_ID>000187228</NSF_ID>
<StartDate>08/06/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName/>
<StateCode>MI</StateCode>
<ZipCode>481091274</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~649999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Humans and robots alike have a critical need to navigate through new environments to carry out everyday tasks. A parent and child may be touring a college campus; a robot may be searching for survivors after a building has collapsed. In this collaboration by faculty at the University of Michigan and Purdue University, the PIs have envisioned human and robotic partners sharing common perceptual-linguistic experiences and cooperating in mundane tasks like janitorial work and home care as well as in critical tasks like emergency response or search-and-rescue. But while mapping and navigation are now commonplace for mobile robots, when considering human-robot collaboration for even simple tasks one is confronted by a critical barrier: robots and people do not share a common language. Human language is rich in linguistic elements for describing our spatial environment, the objects and places within it, and navigable paths through it (e.g., "go down the hallway and enter the third door on the right."). Robots, on the other hand, inhabit a metric world of occupied and unoccupied discretized grid cells, wherein most objects are devoid of meaning (semantics).</p> <p>&nbsp;<br />The PIs' goal in this project was to study means for overcoming this limitation by conjoining the well understood problem of simultaneous localization and mapping (SLAM) with that of language acquisition, in order to enable robots to learn to communicate with people in English about navigation tasks.&nbsp; This project advanced the understanding of how to jointly support robot operation in known and unknown environments along with language acquisition and usage in two key ways.&nbsp;&nbsp;</p> <p><br />First, the project made major strides in how robot systems can use mathematical and computational representations jointly over visual perception, spatial data, and linguistic data.&nbsp; These representations were driven by advances in multi-modal deep learning that contributed to the state of the art in their ability to link phenomena across modalities.&nbsp; One particular contribution of note was an innovative representation that was able to "ground" linguistic terms directly in imagery without the need for full supervision.&nbsp; In other words, the method is able to discover the link between the language and the imagery without a human needing to explicitly point it out, which is error-prone, costly and burdensome.</p> <p><br />Second, the project contributed physical robotic systems and a new open benchmark problem (https://umrobotslang.github.io/) to support the research in this project as well as to catalyze the research community around the important problem of multimodel robot learning.&nbsp; The physical infrastructure built to support the project included a full ten-by-ten tabletop configurable robot environment along with three mobile robot platforms that were used to drive through this environment.&nbsp; To support the physical infrastructure, the project team built an advanced web-based software system that was used by human operators to compete in challenges designed to support creation of rich robot navigation and language acquisition datasets.&nbsp; The project team released dozens of such data in the new RobotSLANG benchmark.</p> <p>Although but a small one in a large problem area, this project succeeded in advancing research on joint robot navigation and localization joint with language acquisition, which is a critical need for the future of human-robot cooperation.</p><br> <p>            Last Modified: 12/14/2020<br>      Modified by: Jason&nbsp;J&nbsp;Corso</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Humans and robots alike have a critical need to navigate through new environments to carry out everyday tasks. A parent and child may be touring a college campus; a robot may be searching for survivors after a building has collapsed. In this collaboration by faculty at the University of Michigan and Purdue University, the PIs have envisioned human and robotic partners sharing common perceptual-linguistic experiences and cooperating in mundane tasks like janitorial work and home care as well as in critical tasks like emergency response or search-and-rescue. But while mapping and navigation are now commonplace for mobile robots, when considering human-robot collaboration for even simple tasks one is confronted by a critical barrier: robots and people do not share a common language. Human language is rich in linguistic elements for describing our spatial environment, the objects and places within it, and navigable paths through it (e.g., "go down the hallway and enter the third door on the right."). Robots, on the other hand, inhabit a metric world of occupied and unoccupied discretized grid cells, wherein most objects are devoid of meaning (semantics).    The PIs' goal in this project was to study means for overcoming this limitation by conjoining the well understood problem of simultaneous localization and mapping (SLAM) with that of language acquisition, in order to enable robots to learn to communicate with people in English about navigation tasks.  This project advanced the understanding of how to jointly support robot operation in known and unknown environments along with language acquisition and usage in two key ways.     First, the project made major strides in how robot systems can use mathematical and computational representations jointly over visual perception, spatial data, and linguistic data.  These representations were driven by advances in multi-modal deep learning that contributed to the state of the art in their ability to link phenomena across modalities.  One particular contribution of note was an innovative representation that was able to "ground" linguistic terms directly in imagery without the need for full supervision.  In other words, the method is able to discover the link between the language and the imagery without a human needing to explicitly point it out, which is error-prone, costly and burdensome.   Second, the project contributed physical robotic systems and a new open benchmark problem (https://umrobotslang.github.io/) to support the research in this project as well as to catalyze the research community around the important problem of multimodel robot learning.  The physical infrastructure built to support the project included a full ten-by-ten tabletop configurable robot environment along with three mobile robot platforms that were used to drive through this environment.  To support the physical infrastructure, the project team built an advanced web-based software system that was used by human operators to compete in challenges designed to support creation of rich robot navigation and language acquisition datasets.  The project team released dozens of such data in the new RobotSLANG benchmark.  Although but a small one in a large problem area, this project succeeded in advancing research on joint robot navigation and localization joint with language acquisition, which is a critical need for the future of human-robot cooperation.       Last Modified: 12/14/2020       Submitted by: Jason J Corso]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
