<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Contributions of Endangered Language Data for Advances in Technology-enhanced Speech Annotation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>227770.00</AwardTotalIntnAmount>
<AwardAmount>252837</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>D.  Langendoen</SignBlockName>
<PO_EMAI>dlangend@nsf.gov</PO_EMAI>
<PO_PHON>7032925088</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Linguists have increased efforts to collect authentic speech materials from endangered and little-studied languages to discover linguistic diversity. However, the challenge of transcribing these speech into written form to facilitate analysis is daunting. This is because of both the sheer quantity of digitally collected speech that needs to be transcribed and the difficulty of unpacking the sounds of spoken speech. &lt;br/&gt;&lt;br/&gt;Linguist Andreas Kathol and computer scientist Vikramjit Mitra of SRI international and linguist Jonathan D. Amith of Gettysburg College will team up to create software that can substantially reduce the language transcription bottleneck. Using as a test case Yoloxochitl Mixtec, an endangered language from the state of Guerrero, Mexico, the team will develop a software tool that will use previously transcribed Yoloxochitl Mixtec speech data to both train a new generation of native speakers in practical orthography and to develop automatic speech recognition software. The output of the recognition software will be used as preliminary transcription that native speakers will correct, as necessary, to create additional high-quality training data. This recursive method will create corpus of transcribed speech large enough so that software will be able to complete automatic transcription of newly collected speech materials. &lt;br/&gt;&lt;br/&gt;The project will include the training of undergraduate and graduate students in software development and the analysis of the Yoloxochitl Mixtec sound system. The project will also train native speakers as documenters in an interactive fashion that systematically introduces them to the transcription conventions of their language. This software tool will help in establishing literacy in Yoloxochitl Mixtec among a broader base of speakers. &lt;br/&gt;&lt;br/&gt;The results of this project will be available at the Archive of Indigenous Languages of Latin America (University of Texas, Austin), Kaipuleohone (University of Hawai'i Digital Language Archive), and at the Linguistic Data Consortium (University of Pennsylvania).</AbstractNarration>
<MinAmdLetterDate>06/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/15/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1500595</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Amith</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan D Amith</PI_FULL_NAME>
<EmailAddress>jamith@gettysburg.edu</EmailAddress>
<PI_PHON>7173376795</PI_PHON>
<NSF_ID>000197819</NSF_ID>
<StartDate>06/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Gettysburg College</Name>
<CityName>Gettysburg</CityName>
<ZipCode>173251483</ZipCode>
<PhoneNumber>7173376505</PhoneNumber>
<StreetAddress>North Washington Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>071445134</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GETTYSBURG COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071445134</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Gettysburg College]]></Name>
<CityName>Gettsyburg</CityName>
<StateCode>PA</StateCode>
<ZipCode>173251483</ZipCode>
<StreetAddress><![CDATA[300 N. Washington Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7298</Code>
<Text>COLLABORATIVE RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7719</Code>
<Text>DEL</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~235770</FUND_OBLG>
<FUND_OBLG>2019~17067</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The documentation of endangered languages---languages on the verge of disappearing within a few generations at most---has become an increasing priority for researchers (principally linguists and anthropologists) and native speaker communities (that wish to maintain and revitalize their language and culture). Estimates vary of the number of extant languages, the rate of language disappearance and cultural loss, and even the meaning of cultural loss and language death (versus change and shift, as when an Indigenous language is increasingly impacted by colonial languages and cultures). Yet despite any nuances it appears certain that the majority of approximately 6,000-7,000 languages presently spoken will effectively disappear within this century. With this, the diversity of linguistic expression is diminished and the vast reservoir of human knowledge is impoverished.&nbsp;</p> <p>Yet our ability to document endangered languages and cultures is inhibited by a "transcription bottleneck": hundreds of hours of high quality recordings can be produced in a short amount of time but take months to process. For example, in one week in Yoloxochitl, state of Guerrero, Mexico, Jonathan Amith, a project co-PI recorded 30 hours of conversation between a dozen native speakers. An excellent native speaker linguist, Rey Castillo Garc?a, would require close to a year to produce an archival quality transcription and translation of this material (Image 1: ELAN transcription, analysis, and translation). To address this issue researchers have looked to automatic speech recognition (ASR), the computer-generated production of a written transcription of an audio recording. This project has explored the possibility of achieving, with ASR, a high level of accuracy in the transcription of an endangered tonal language from west-central Mexico: Yoloxochitl Mixtec. This language was chosen because of its complex and challenging tonal system: It has 4 level tones (from low, marked by "1", to high, marked by "4"). As an example, ku1mi4 (low-high) is the word for the number '4'. One sequence of segments, , is associated with 30 distinct words represented by 25 distinct tonal patterns (e.g., na1ma4 'soap', na1ma42 'my soap', na1ma3 'to get changed', ,na1ma32 'I will get changed', 'na14ma32 'I will not get changed').</p> <p>The project's primary goal has been to address the "transcription bottleneck". This was accomplished in part by training, under the direction of co-PI Andreas Kathol, an HMM&nbsp;Kaldi&nbsp;chain model for YM speech recognition. The first attempts to automate the transcription process resulted in a word error rate (WER) of 30.1%. Sometimes a single character in a word would be mistaken, at other times an entire word not in the gold-standard reference was introduced by the computer. In still other cases, the computer failed to transcribe a word that was in the reference. All three scenarios (Substitution, Insertion, Deletion) count as a word error. By project end the WER had been reduced to 19%.</p> <p>A secondary goal has been to (1) TRAIN native speakers to write their language through a progression of increasingly challenging transcription tasks; (2) TEST native speakers on their success in learning as they progressed through increasingly difficult lessons; and (3) Allow users to ANNOTATE, or correct, a transcription proposed by the ASR algorithm. To accomplish all these tasks a TTA (Training, Testing, Annotation) tool was created at SRI and installed on a Chromebook (see attached images 2 to 5). The Training and Testing was evaluated with the participation of a native speaker, Esteban Guadalupe Sierra, who finished all lessons and then began to transcribe directly from audio. He has reached a level that is 97% in accord with the transcription of Rey Castillo Garcia, the expert native speaker linguist.</p> <p>A final goal was to implement ASR (along with the TTA tool) on Chromebook computers to be used in Yoloxochitl. Amith along with Castillo and Guadalupe would work with native speakers to record narratives (either directly into the computer or recorded independently and transferred to the computer). The speech signal would then be processed locally through the ASR system on the Chromebook and the transcription would be loaded into the TAA tool to be used in the Annotation mode. Then the original speaker would correct the computer generated transcription as best as possible. While there is little possibility that the speaker would properly mark tone, nasalization, and glottal stops, it is hoped that s/he would eliminate words that the ASR program mistakenly inserted or add words that the ASR program failed to transcribe. The transcription so corrected by the narrator would then be given to Castillo to be finalized.</p> <p>Whether or not the WER that this project has produced stands the test of time given that ASR technology is constantly being improved, this project has shown the utility of an integrated approach of Training and Testing native speakers, recording and processing ASR locally, and then having the original speaker take the first step in reviewing his or her computer-generated transcription.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/30/2020<br>      Modified by: Jonathan&nbsp;D&nbsp;Amith</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603988205049_01_ELAN-example--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603988205049_01_ELAN-example--rgov-800width.jpg" title="Transcription/translation in the ELAN program"><img src="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603988205049_01_ELAN-example--rgov-66x44.jpg" alt="Transcription/translation in the ELAN program"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Screenshot of the ELAN program interface. Image shows four tiers: (1) Surface transcription; (2) Underlying (parsed) transcription; (3) English translation; (4) Spanish translation. A small section of the audio file is highlighted for playback to fine-tune transcription. Time-code is shown.</div> <div class="imageCredit">Jonathan D. Amith</div> <div class="imageSubmitted">Jonathan&nbsp;D&nbsp;Amith</div> <div class="imageTitle">Transcription/translation in the ELAN program</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603988745603_02_Lesson-sequence_YM-learning_cr--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603988745603_02_Lesson-sequence_YM-learning_cr--rgov-800width.jpg" title="Lesson table of contents: First seven lessons"><img src="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603988745603_02_Lesson-sequence_YM-learning_cr--rgov-66x44.jpg" alt="Lesson table of contents: First seven lessons"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Screenshot showing first 7 lessons in the TAA (Training, Annotation, Analysis) tool. In these lessons the tones are always identical on the first and second mora ("syllable"). That is, they are either 1.1, 3.3 or 4.4 (from low 1 to high 4).  The difference is the structure of the syllable.</div> <div class="imageCredit">Jonathan D. Amith</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jonathan&nbsp;D&nbsp;Amith</div> <div class="imageTitle">Lesson table of contents: First seven lessons</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603989850112_03_Lesson-on-words-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603989850112_03_Lesson-on-words-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-800width.jpg" title="Teaching through repetition: Lesson 2b"><img src="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603989850112_03_Lesson-on-words-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-66x44.jpg" alt="Teaching through repetition: Lesson 2b"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Screenshot of teaching interface for lesson 2b. Student replays all words that have identical tones on the first and second mora ("syllable") for words in which there is an intermediate (and intervocalic) glottal stop (represented by an asterisk in the orthography used).</div> <div class="imageCredit">Jonathan D. Amith</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jonathan&nbsp;D&nbsp;Amith</div> <div class="imageTitle">Teaching through repetition: Lesson 2b</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603990467350_04_Test-of-word-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603990467350_04_Test-of-word-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-800width.jpg" title="Testing: Student is asked to transcribe a sound that they play (lesson 1d)"><img src="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603990467350_04_Test-of-word-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-66x44.jpg" alt="Testing: Student is asked to transcribe a sound that they play (lesson 1d)"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Student plays back words that meet criteria for the lesson and fills in the transcription. The student transcriptions are evaluated for correctness and s/he is given the correct form if different. Students must successfully complete tests at one level before proceeding to the next.</div> <div class="imageCredit">Jonathan D. Amith</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jonathan&nbsp;D&nbsp;Amith</div> <div class="imageTitle">Testing: Student is asked to transcribe a sound that they play (lesson 1d)</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603990794451_05_Answer-of-word-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603990794451_05_Answer-of-word-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-800width.jpg" title="Evaluation: Correct transcription displayed for the student (Lesson 1d)"><img src="/por/images/Reports/POR/2020/1500595/1500595_10370817_1603990794451_05_Answer-of-word-with-intermediate-glottal-stop-identical-level-tones_cr--rgov-66x44.jpg" alt="Evaluation: Correct transcription displayed for the student (Lesson 1d)"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Student is shown the correct form, automatically if the student inputs three incorrect transcriptions (very possible in later lessons). The student may request to see the correct form but in this case s/he will not receive credit for a correct transcription and the test must be repeated.</div> <div class="imageCredit">Jonathan D. Amith</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jonathan&nbsp;D&nbsp;Amith</div> <div class="imageTitle">Evaluation: Correct transcription displayed for the student (Lesson 1d)</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The documentation of endangered languages---languages on the verge of disappearing within a few generations at most---has become an increasing priority for researchers (principally linguists and anthropologists) and native speaker communities (that wish to maintain and revitalize their language and culture). Estimates vary of the number of extant languages, the rate of language disappearance and cultural loss, and even the meaning of cultural loss and language death (versus change and shift, as when an Indigenous language is increasingly impacted by colonial languages and cultures). Yet despite any nuances it appears certain that the majority of approximately 6,000-7,000 languages presently spoken will effectively disappear within this century. With this, the diversity of linguistic expression is diminished and the vast reservoir of human knowledge is impoverished.   Yet our ability to document endangered languages and cultures is inhibited by a "transcription bottleneck": hundreds of hours of high quality recordings can be produced in a short amount of time but take months to process. For example, in one week in Yoloxochitl, state of Guerrero, Mexico, Jonathan Amith, a project co-PI recorded 30 hours of conversation between a dozen native speakers. An excellent native speaker linguist, Rey Castillo Garc?a, would require close to a year to produce an archival quality transcription and translation of this material (Image 1: ELAN transcription, analysis, and translation). To address this issue researchers have looked to automatic speech recognition (ASR), the computer-generated production of a written transcription of an audio recording. This project has explored the possibility of achieving, with ASR, a high level of accuracy in the transcription of an endangered tonal language from west-central Mexico: Yoloxochitl Mixtec. This language was chosen because of its complex and challenging tonal system: It has 4 level tones (from low, marked by "1", to high, marked by "4"). As an example, ku1mi4 (low-high) is the word for the number '4'. One sequence of segments, , is associated with 30 distinct words represented by 25 distinct tonal patterns (e.g., na1ma4 'soap', na1ma42 'my soap', na1ma3 'to get changed', ,na1ma32 'I will get changed', 'na14ma32 'I will not get changed').  The project's primary goal has been to address the "transcription bottleneck". This was accomplished in part by training, under the direction of co-PI Andreas Kathol, an HMM Kaldi chain model for YM speech recognition. The first attempts to automate the transcription process resulted in a word error rate (WER) of 30.1%. Sometimes a single character in a word would be mistaken, at other times an entire word not in the gold-standard reference was introduced by the computer. In still other cases, the computer failed to transcribe a word that was in the reference. All three scenarios (Substitution, Insertion, Deletion) count as a word error. By project end the WER had been reduced to 19%.  A secondary goal has been to (1) TRAIN native speakers to write their language through a progression of increasingly challenging transcription tasks; (2) TEST native speakers on their success in learning as they progressed through increasingly difficult lessons; and (3) Allow users to ANNOTATE, or correct, a transcription proposed by the ASR algorithm. To accomplish all these tasks a TTA (Training, Testing, Annotation) tool was created at SRI and installed on a Chromebook (see attached images 2 to 5). The Training and Testing was evaluated with the participation of a native speaker, Esteban Guadalupe Sierra, who finished all lessons and then began to transcribe directly from audio. He has reached a level that is 97% in accord with the transcription of Rey Castillo Garcia, the expert native speaker linguist.  A final goal was to implement ASR (along with the TTA tool) on Chromebook computers to be used in Yoloxochitl. Amith along with Castillo and Guadalupe would work with native speakers to record narratives (either directly into the computer or recorded independently and transferred to the computer). The speech signal would then be processed locally through the ASR system on the Chromebook and the transcription would be loaded into the TAA tool to be used in the Annotation mode. Then the original speaker would correct the computer generated transcription as best as possible. While there is little possibility that the speaker would properly mark tone, nasalization, and glottal stops, it is hoped that s/he would eliminate words that the ASR program mistakenly inserted or add words that the ASR program failed to transcribe. The transcription so corrected by the narrator would then be given to Castillo to be finalized.  Whether or not the WER that this project has produced stands the test of time given that ASR technology is constantly being improved, this project has shown the utility of an integrated approach of Training and Testing native speakers, recording and processing ASR locally, and then having the original speaker take the first step in reviewing his or her computer-generated transcription.           Last Modified: 10/30/2020       Submitted by: Jonathan D Amith]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
