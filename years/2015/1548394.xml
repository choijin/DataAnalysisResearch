<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER/Collaborative Research: A New Science of Visual Experience</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>195845.00</AwardTotalIntnAmount>
<AwardAmount>195845</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Georgia-Ann Klutke</SignBlockName>
<PO_EMAI>gaklutke@nsf.gov</PO_EMAI>
<PO_PHON>7032922443</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The essence of human experience is interacting with the natural and man-made environments through the five human senses, and through vision in particular. The objective of this EArly-concept Grant for Exploratory Research (EAGER) project is to build analytical foundations for a new science of visual experience that will bridge basic approaches from cognitive science and systems engineering. Specifically, the research will build mathematical and computational models of a human navigating a three dimensional space such as a factory, museum, or retail store.  The idea is to gain insights into the limits on observability and controllability in human-technology systems, and to improve the user's situational awareness.  If the research is successful, researchers will be able to describe situations in terms of possibilities for action and access to information.  Such quantitative tools will allow engineers to design environments to achieve outcomes such as increased focus, improved safety, better wayfinding, and improved experience. In time, it might be possible to engineer interactive environments that adapt to the personal attributes and identities of the humans that inhabit them. The results of the research have the potential to be used by many disciplines such as engineering, business, architecture, psychology, cognition, and human factors.&lt;br/&gt;&lt;br/&gt;The multidisciplinary research team consisting of academics (engineering, psychology, and computer science) and two industry personnel suggests moving visual experience of a three dimensional environment from the realm of intuition and experience to analytical science. The specific focus of this grant will be on developing a general set of analytical models that emerge from the analysis of a variety of context-specific human-environment interactions (e.g., nurse in CCU, shopper in a retail store). With a solid grounding in the basic psychology of Perception-Action, the analytical models will integrate three-dimensional spatial relationships with the human eye's field of vision and the physical attributes of a human. Most significantly, the research will consider complex dynamics resulting from human movement in the space, with all the attendant changes in visual angles, and the appearance and disappearance of visual obstacles. Human performance will be empirically examined in a Virtual Environment in order to validate the analytical metrics of visual experience.</AbstractNarration>
<MinAmdLetterDate>09/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/13/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1548394</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Flach</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John M Flach</PI_FULL_NAME>
<EmailAddress>john.flach@wright.edu</EmailAddress>
<PI_PHON>9377752396</PI_PHON>
<NSF_ID>000273823</NSF_ID>
<StartDate>09/11/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jennie</FirstName>
<LastName>Gallimore</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennie J Gallimore</PI_FULL_NAME>
<EmailAddress>Jennie.Gallimore@wright.edu</EmailAddress>
<PI_PHON>9377754901</PI_PHON>
<NSF_ID>000454492</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate>09/11/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Wischgoll</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Wischgoll</PI_FULL_NAME>
<EmailAddress>thomas.wischgoll@wright.edu</EmailAddress>
<PI_PHON>9377755057</PI_PHON>
<NSF_ID>000488037</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Pratik</FirstName>
<LastName>Parikh</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pratik J Parikh</PI_FULL_NAME>
<EmailAddress>pratik.parikh@louisville.edu</EmailAddress>
<PI_PHON>5402304420</PI_PHON>
<NSF_ID>000543996</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Munch</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James Munch</PI_FULL_NAME>
<EmailAddress>james.munch@wright.edu</EmailAddress>
<PI_PHON>9377752425</PI_PHON>
<NSF_ID>000692968</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate>09/11/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Wright State University</Name>
<CityName>Dayton</CityName>
<ZipCode>454350001</ZipCode>
<PhoneNumber>9377752425</PhoneNumber>
<StreetAddress>3640 Colonel Glenn Highway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047814256</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WRIGHT STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>047814256</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Wright State University]]></Name>
<CityName>Dayton</CityName>
<StateCode>OH</StateCode>
<ZipCode>454350001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7633</Code>
<Text>EFRI Research Projects</Text>
</ProgramElement>
<ProgramReference>
<Code>030E</Code>
<Text>CONTROL SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>031E</Code>
<Text>MECHATRONICS</Text>
</ProgramReference>
<ProgramReference>
<Code>032E</Code>
<Text>SENSORS AND ACTUATORS</Text>
</ProgramReference>
<ProgramReference>
<Code>033E</Code>
<Text>Smart and responsive structures</Text>
</ProgramReference>
<ProgramReference>
<Code>034E</Code>
<Text>Dynamical systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8025</Code>
<Text>Advanced Materials Processing</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~195845</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This is a collaborative EAGER research project with Professor Kevin Gue at University of Louisville (UofL), KY. The objective of the project was to build analytical and numerical foundations for a new &lsquo;science of visual experience&rsquo; that quantifies what humans can see in a 3D environment such as a retail store, hospital, library, or museum. The ability to model environments in this way enables spatial designers to know how their designs will affect the people inhabiting those spaces. For example, retailers will have a better understanding of which products can be seen from where (and for how long) as shoppers roam the aisles, and hospital facility designers will know where to place nursing stations so that nurses can easily maintain eye-contact with patients to proactively intervene to avoid falls and injuries.</p> <p>While the team at WSU focused on two key measures of visual experience, exposure (what can be seen) and intensity (for how long), the team at UofL focused on legibility (what can be read or interpreted?).</p> <p>The team at WSU built quantitative models, first considering a 2D approximation of the real environments and then extending to 3D. The models were analytical when possible, and numerical otherwise, to estimate the exposure and intensity of a layout as humans walk along a prespecified pathway. For this project, we considered a small section of a retail store to test these models. We incorporate the complex dynamics that evolves when the human field of view interacts with the racks in these models. We also considered three forms of obstruction by rack n; self (obstructing its own locations), preceding (obstructing locations on n-1 rack), and succeeding (obstructing locations on the n+1 rack). Our extensions considered a range of values for horizontal and vertical field of view, along with depth of vision, pointed out in the human factors literature.</p> <p>To evaluate the promising designs, we developed a Virtual Environment (VE) of a store in collaboration with our Computer Science and Psychology Co-Is. The VE was designed in a fully immersive, 27-panel, 3-walled setup, with a head tracker. We tested several promising designs across 40+ participants, which involved orienting racks at an angle and also modifying the curvature of racks (to emulate recent curved racks we have seen at nearby stores). The VE study confirmed the findings of the quantitative models. That is, acute orientations align well with the human field of view and provide very high exposure and intensity under the assumption that the shopper is able to recognize the brand by color, symbol, or shape. Further, in case of bidirectional travel, supplementary (obtuse) angles exist that achieve the same performance as acute orientations. We also noted that curving racks to a certain extent can aid in increasing exposure.</p> <p>Comparing the findings from WSU and UofL studies, one practical insight for retail environments from our research is that if products have a high degree of brand recognition (by color or logo; e.g., Tide detergent or Cheerios cereal) or can be identified only by shape (jeans or sweaters on a shelf), then racks should be oriented acutely (and slightly curved) to maximize exposure or intensity. If products require the shopper to read a label or otherwise interpret some form of messaging, then racks should be oriented obtusely to maximize legibility. Layouts with straight racks orientated orthogonally are limited on both these measures.</p> <p>We extended these models and findings to introduce two design problems in retail. The <em>Retail Rack Layout Problem (RRLP)</em> focuses on determining the number of columns and orientation of racks in each column to maximize exposure directly. On the other hand, the <em>Rack Orientation and Curvature Problem (ROCP)</em>, focuses on maximizing impulse revenue (a function of exposure) by determining the best rack orientation and curvature. The impact of such layouts on floor space and aspect ratio were also accounted. Optimization models for both RRLP and ROCP were introduced and two simulation-optimization based approaches were proposed. Each approach embedded a simulator (to estimate exposure for each candidate solution) within a particle swarm optimization based metaheuristic. RRLP results suggested that an increase of exposure ranging from 213&ndash;226% (small head turns) and 17&ndash;18% (large head turns) over 90&deg;-rack layouts can be achieved with angled-racks. ROCP results indicated that high-acute and straight-to-medium-curved racks tend to increase marginal impulse profit by 70-233% over commonly seen rack layouts, and that these findings are dependent upon shopper volume, cost of floor space, travel direction, and aspect ratio is also evaluated.</p> <p class="Default">In summary, this research generated a set of quantitative tools to aid a designer in understanding the implications of design choices on a human&rsquo;s visual experience. In the long term, incorporating additional behavioral components and visual adaptation by humans through changes in their scan patterns would help capture interdependencies between a human&rsquo;s action-perception and the physical environment.</p><br> <p>            Last Modified: 09/06/2018<br>      Modified by: Pratik&nbsp;J&nbsp;Parikh</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This is a collaborative EAGER research project with Professor Kevin Gue at University of Louisville (UofL), KY. The objective of the project was to build analytical and numerical foundations for a new ?science of visual experience? that quantifies what humans can see in a 3D environment such as a retail store, hospital, library, or museum. The ability to model environments in this way enables spatial designers to know how their designs will affect the people inhabiting those spaces. For example, retailers will have a better understanding of which products can be seen from where (and for how long) as shoppers roam the aisles, and hospital facility designers will know where to place nursing stations so that nurses can easily maintain eye-contact with patients to proactively intervene to avoid falls and injuries.  While the team at WSU focused on two key measures of visual experience, exposure (what can be seen) and intensity (for how long), the team at UofL focused on legibility (what can be read or interpreted?).  The team at WSU built quantitative models, first considering a 2D approximation of the real environments and then extending to 3D. The models were analytical when possible, and numerical otherwise, to estimate the exposure and intensity of a layout as humans walk along a prespecified pathway. For this project, we considered a small section of a retail store to test these models. We incorporate the complex dynamics that evolves when the human field of view interacts with the racks in these models. We also considered three forms of obstruction by rack n; self (obstructing its own locations), preceding (obstructing locations on n-1 rack), and succeeding (obstructing locations on the n+1 rack). Our extensions considered a range of values for horizontal and vertical field of view, along with depth of vision, pointed out in the human factors literature.  To evaluate the promising designs, we developed a Virtual Environment (VE) of a store in collaboration with our Computer Science and Psychology Co-Is. The VE was designed in a fully immersive, 27-panel, 3-walled setup, with a head tracker. We tested several promising designs across 40+ participants, which involved orienting racks at an angle and also modifying the curvature of racks (to emulate recent curved racks we have seen at nearby stores). The VE study confirmed the findings of the quantitative models. That is, acute orientations align well with the human field of view and provide very high exposure and intensity under the assumption that the shopper is able to recognize the brand by color, symbol, or shape. Further, in case of bidirectional travel, supplementary (obtuse) angles exist that achieve the same performance as acute orientations. We also noted that curving racks to a certain extent can aid in increasing exposure.  Comparing the findings from WSU and UofL studies, one practical insight for retail environments from our research is that if products have a high degree of brand recognition (by color or logo; e.g., Tide detergent or Cheerios cereal) or can be identified only by shape (jeans or sweaters on a shelf), then racks should be oriented acutely (and slightly curved) to maximize exposure or intensity. If products require the shopper to read a label or otherwise interpret some form of messaging, then racks should be oriented obtusely to maximize legibility. Layouts with straight racks orientated orthogonally are limited on both these measures.  We extended these models and findings to introduce two design problems in retail. The Retail Rack Layout Problem (RRLP) focuses on determining the number of columns and orientation of racks in each column to maximize exposure directly. On the other hand, the Rack Orientation and Curvature Problem (ROCP), focuses on maximizing impulse revenue (a function of exposure) by determining the best rack orientation and curvature. The impact of such layouts on floor space and aspect ratio were also accounted. Optimization models for both RRLP and ROCP were introduced and two simulation-optimization based approaches were proposed. Each approach embedded a simulator (to estimate exposure for each candidate solution) within a particle swarm optimization based metaheuristic. RRLP results suggested that an increase of exposure ranging from 213&ndash;226% (small head turns) and 17&ndash;18% (large head turns) over 90&deg;-rack layouts can be achieved with angled-racks. ROCP results indicated that high-acute and straight-to-medium-curved racks tend to increase marginal impulse profit by 70-233% over commonly seen rack layouts, and that these findings are dependent upon shopper volume, cost of floor space, travel direction, and aspect ratio is also evaluated. In summary, this research generated a set of quantitative tools to aid a designer in understanding the implications of design choices on a human?s visual experience. In the long term, incorporating additional behavioral components and visual adaptation by humans through changes in their scan patterns would help capture interdependencies between a human?s action-perception and the physical environment.       Last Modified: 09/06/2018       Submitted by: Pratik J Parikh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
