<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Scaling Up Machine Learning with Virtual Memory</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>184904.00</AwardTotalIntnAmount>
<AwardAmount>184904</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Aidong Zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Large datasets in terabytes or petabytes are increasingly common, calling for new kinds of scalable machine learning approaches. While state-of-the-art techniques often use complex designs, specialized methods to store and work with large datasets, this project proposes a minimalist approach that forgoes such complexities, by leveraging the fundamental virtual memory capability found on all modern operating systems, to load into the virtual memory space the large datasets that are otherwise too large to fit in the computer's main memory. This main idea will allow developers to easily work with large datasets as if they were in-memory data, enabling them to create machine learning software that is significantly easier to develop and maintain, yet faster and more scalable. Developers will achieve higher work efficiency and make fewer programming errors; companies will reduce operating costs; and researchers will innovate methodology without getting bogged down by implementation details and scalability concerns. The proposed ideas could make a far-reaching impact on industry and academia, in science, education, and technology, as they face increasing challenges in applying machine learning on large datasets. The proposed ideas will also help train the next generation of scientists and engineers by allowing students to learn to work with large datasets in significantly simpler ways. As virtual memory is universally available on modern devices and operating systems, the proposed ideas will also work on mobile, low-power devices, enabling them to perform computation at unprecedented scales and speed.&lt;br/&gt;&lt;br/&gt;This project investigates a fundamental, radical way to scale up machine learning algorithms based on virtual memory, one that may be easier to code and maintain, but currently under-utilized in by both single-machine and multi-machine distributed approaches. This research aims to develop deep understanding of this radical idea, its benefits and limitations, and to what extent these results apply in various settings, with respect to datasets, memory sizes, page sizes (e.g., from the default 4KB to the jumbo 2MB pages that enable terabyes of virtual memory space), and architectures (e.g., testing on distributed shared memory file systems like Lustre that support paging and virtual memory over large computer clusters). The researchers will build on their preliminary work on graph algorithms that already demonstrates significant speed-up over state-of-the-art approaches; they will extend their approach to a wide range of machine learning and data mining algorithms. They will also develop mathematical models and systematic approaches to profile and predict algorithm performance and energy usage based on extensive evaluation across platforms, datasets, and languages. &lt;br/&gt;&lt;br/&gt;For further information, see the project web site at: http://poloclub.gatech.edu/mmap/.</AbstractNarration>
<MinAmdLetterDate>09/14/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/14/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1551614</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Vuduc</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard W Vuduc</PI_FULL_NAME>
<EmailAddress>richie@cc.gatech.edu</EmailAddress>
<PI_PHON>5103017014</PI_PHON>
<NSF_ID>000080331</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Duen Horng</FirstName>
<LastName>Chau</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Duen Horng Chau</PI_FULL_NAME>
<EmailAddress>polo@gatech.edu</EmailAddress>
<PI_PHON>4048944819</PI_PHON>
<NSF_ID>000628477</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~184904</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project's goal is to develop deep understanding of a fundamental, radical way to scale up machine learning algorithms based on virtual memory, one that may be easier to code and maintain, but currently under-utilized in by both single-machine and multi-machine distributed approaches.\</p> <p><br /><strong>IMPACT:&nbsp;</strong></p> <p>1) The findings from our work have led to the important new knowledge that (a) the ubiquitous virtual memory capabilities commonly found on all modern operating systems can be a simple yet powerful way to scale up graph algorithms and general machine learning algorithms; and (b) virtual memory based techniques is a feasible computational middle ground between single-machine fully-in-core computation and parallel distributed computation.</p> <p>2) Our research motivates researchers to rethink the benefits of the more sophisticated approaches, in light of our results demonstrating that simpler approaches could significantly outperform them.&nbsp;</p> <p>3) A variety of disciplines benefits from our research. For example, it is very common for material scientists to work with large image datasets (scans of material slices) but their existing computational tools (e.g., Matlab) often do not scale to such datasets. Our approach can also allow researchers and practitioners to work with graph datasets that are orders of magnitude larger than ever before. Our approach can scale up backend computation for interactive graph visualization tools for exploring graph datasets. (Current graph visualization tools are limited in scalability, requiring the graph to be fully fit in RAM.)</p> <p>4) Research results have been disseminated via lectures to over 700 undergraduate and graduate students since Fall&rsquo;16 in the "Data and Visual Analytics" course at Georgia Tech (http://poloclub.gatech.edu/cse6242). Our research will reach a dramatically greater student population when the course's online version is launched in Spring'18, as an required course of the Online Master of Science in Analytics (OMSA) program that will enroll thousands of students across the globe. Students directly applied the results in an homework assignment to run graph mining algorithms on their own commodity computers/laptops that scales to a 69 million edge graph -- before students learned about our research and potential of virtual memory, most students did not believe that was possible. They get to try it first hand, and learned to appreciate that virtual memory based techniques could be a feasible computational middle ground between single-machine fully-in-core computation and parallel distributed computation.</p> <p>5) We open-sourced significant portion of our code developed on GitHub (https://github.com/M-Flash), such as that created as part of our MFlash PKDD&rsquo;16 publication.&nbsp;<br /><br /><br /><strong>MERIT:&nbsp;</strong></p> <p>1) We developed an innovative Bimodal Block Processing (BBP) technique that boosts graph computation by minimizing the I/O cost. We proved that memory mapping based techniques can be a simple, yet powerful and scalable way to scale up graph algorithms with up to 12 billion edges, achieving the highest speed among single-machine graph computation approaches.</p> <p>2) We showed that memory mapping techniques&rsquo; benefits generalize from graph algorithms to general machine learning algorithms, like logistic regression and k-means, when data fits in or exceeds RAM. Our method enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p> <p>3) Memory mapping techniques are easy to understand and to use, and can be integrated into undergraduate curriculum, as we have done so, for the Data and Visual Analytics course at Georgia Tech.<br /><br /><br />The work resulted in over 10 publications in top venues (like PKDD, IEEE VIS, SIGMOD), and formed the foundations of two phd dissertations of (1) Dr. Acar Tamersoy, now a research scientist at Symantec; and (2) Dr. Robert Pienta.</p><br> <p>            Last Modified: 01/02/2018<br>      Modified by: Duen Horng&nbsp;Chau</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project's goal is to develop deep understanding of a fundamental, radical way to scale up machine learning algorithms based on virtual memory, one that may be easier to code and maintain, but currently under-utilized in by both single-machine and multi-machine distributed approaches.\   IMPACT:   1) The findings from our work have led to the important new knowledge that (a) the ubiquitous virtual memory capabilities commonly found on all modern operating systems can be a simple yet powerful way to scale up graph algorithms and general machine learning algorithms; and (b) virtual memory based techniques is a feasible computational middle ground between single-machine fully-in-core computation and parallel distributed computation.  2) Our research motivates researchers to rethink the benefits of the more sophisticated approaches, in light of our results demonstrating that simpler approaches could significantly outperform them.   3) A variety of disciplines benefits from our research. For example, it is very common for material scientists to work with large image datasets (scans of material slices) but their existing computational tools (e.g., Matlab) often do not scale to such datasets. Our approach can also allow researchers and practitioners to work with graph datasets that are orders of magnitude larger than ever before. Our approach can scale up backend computation for interactive graph visualization tools for exploring graph datasets. (Current graph visualization tools are limited in scalability, requiring the graph to be fully fit in RAM.)  4) Research results have been disseminated via lectures to over 700 undergraduate and graduate students since Fall?16 in the "Data and Visual Analytics" course at Georgia Tech (http://poloclub.gatech.edu/cse6242). Our research will reach a dramatically greater student population when the course's online version is launched in Spring'18, as an required course of the Online Master of Science in Analytics (OMSA) program that will enroll thousands of students across the globe. Students directly applied the results in an homework assignment to run graph mining algorithms on their own commodity computers/laptops that scales to a 69 million edge graph -- before students learned about our research and potential of virtual memory, most students did not believe that was possible. They get to try it first hand, and learned to appreciate that virtual memory based techniques could be a feasible computational middle ground between single-machine fully-in-core computation and parallel distributed computation.  5) We open-sourced significant portion of our code developed on GitHub (https://github.com/M-Flash), such as that created as part of our MFlash PKDD?16 publication.    MERIT:   1) We developed an innovative Bimodal Block Processing (BBP) technique that boosts graph computation by minimizing the I/O cost. We proved that memory mapping based techniques can be a simple, yet powerful and scalable way to scale up graph algorithms with up to 12 billion edges, achieving the highest speed among single-machine graph computation approaches.  2) We showed that memory mapping techniques? benefits generalize from graph algorithms to general machine learning algorithms, like logistic regression and k-means, when data fits in or exceeds RAM. Our method enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.  3) Memory mapping techniques are easy to understand and to use, and can be integrated into undergraduate curriculum, as we have done so, for the Data and Visual Analytics course at Georgia Tech.   The work resulted in over 10 publications in top venues (like PKDD, IEEE VIS, SIGMOD), and formed the foundations of two phd dissertations of (1) Dr. Acar Tamersoy, now a research scientist at Symantec; and (2) Dr. Robert Pienta.       Last Modified: 01/02/2018       Submitted by: Duen Horng Chau]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
