<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: FP: Collaborative Research: Advancing autovectorization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>338006.00</AwardTotalIntnAmount>
<AwardAmount>338006</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Title: XPS:FULL:FP:Collaborative Research:Advancing autovectorization&lt;br/&gt;&lt;br/&gt;The goal of this project is to advance the state of the art in autovectorization. This is a technique applied by compilers to automatically transform computer programs so that they can take advantage of the vector devices found in most processors. Today, most compilers have autovectorization capabilities, but their effectiveness is limited. The intellectual merit of this project lies in its potential to advance an important and beautiful core area of computer science, compiler technology, by creating new techniques and extending our understanding of programming patterns, program analysis, and transformation techniques. Beyond computer science, the project's broader significance and importance is that its results aim at increasing the fraction of code segments that, without human intervention, make use of vector devices. The effect of this increase is the acceleration of computer programs and the reduction of the energy that they consume.  Faster programs are of great importance in all application areas, but are particularly important in science and engineering where computing speed is an enabler of discoveries and better designs.  &lt;br/&gt;&lt;br/&gt;The research strategy is to develop and evaluate a prototype autovectorizer based on the exploration of the space of equivalent versions of a program guided by an intelligent search engine. The space of equivalent versions is obtained with a source-to-source restructurer. A repository of codelets is planned in order to train the search engine so that it becomes capable of guiding the selection in the space of possibilities in order to identify a highly efficient version of the code.</AbstractNarration>
<MinAmdLetterDate>07/20/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1533926</AwardID>
<Investigator>
<FirstName>Alexandru</FirstName>
<LastName>Nicolau</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexandru Nicolau</PI_FULL_NAME>
<EmailAddress>anicolau@uci.edu</EmailAddress>
<PI_PHON>9498244079</PI_PHON>
<NSF_ID>000108570</NSF_ID>
<StartDate>07/20/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Veidenbaum</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander V Veidenbaum</PI_FULL_NAME>
<EmailAddress>alexv@ics.uci.edu</EmailAddress>
<PI_PHON>9498246188</PI_PHON>
<NSF_ID>000181882</NSF_ID>
<StartDate>07/20/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926970001</ZipCode>
<StreetAddress><![CDATA[3056 Bren Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~338006</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <div class="gmail_signature" dir="ltr">This project &nbsp;endeavored to pinpoint and solve the problems that hindered automatic&nbsp;<em>vectorization</em>&nbsp;for the last 40+ years. Vectorization is a very efficient technique for parallel execution of certain types of programs and is &nbsp;performed by a compiler.&nbsp;Prior work has&nbsp;shown that, in practice, vectorization yielded only modest performance improvements because only &nbsp;about 30% of a program was being vectorized, on average. It was also not possible to determine how much more vectorization might be achievable.&nbsp;</div> <div class="gmail_signature" dir="ltr">This is a very important problem, because significantly&nbsp;improved vectorization yields not only a much faster execution of programs but significant energy savings as well. The latter is another critical problem in computer design. Performance &nbsp;and energy constraints are critical bottlenecks in many important applications&nbsp;- from weather prediction to development of new drugs and materials, climate change, and advances in every scientific and engineering field, They also limit consumer products such as cell phones, TV's, etc. in what they can do - just think of artificial intelligence tasks that we want run directly on our devices.<br /><br />In this project we were able to establish that relative failure to vectorize well is due not to the lack of parallelization/vectorization techniques (which are plentiful following 50+ years of prior research), but rather because of the inability of existing tools to predict well which part of the code can be parallelized/vectorized and which precise program &nbsp;transformations (out of the myriad available) would result in the most significant performance/efficiency increase.&nbsp;We then&nbsp;determined the transformations that are most useful in vectorizing real applications and established a large open-source, extensible, repository of program kernels - &nbsp;annotated with thorough data on their performance, the transformations applied to best vectorize them, and speedups achieved on a modern computer system. This was then&nbsp;used to develop machine learning techniques (ML) that can predict with high accuracy what transformations may be usefully applied to a given code to optimize its performance.&nbsp; In addition, this repository can&nbsp;serve as a valuable resource for future generations of researchers as well as a baseline to benchmark any future work in the area against. Based on this foundation, we then developed a meta-compiler, that draws on the best available state-of-the-art vectorizing and parallelizing compilers, to produce code that is guaranteed to be as good as that produced by the&nbsp;best tools available at a given time, and in practice usually produces code superior (more efficient in speed and/or energy consumption) than any of the state-of-the-art&nbsp;compilers&nbsp;available at this time, with significant&nbsp;speedups obtained on full programs ranging upwards of 28 times as fast (!) when compared with the&nbsp;best compiler available.&nbsp;</div> <p>&nbsp;</p><br> <p>            Last Modified: 01/08/2021<br>      Modified by: Alexander&nbsp;V&nbsp;Veidenbaum</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   This project  endeavored to pinpoint and solve the problems that hindered automatic vectorization for the last 40+ years. Vectorization is a very efficient technique for parallel execution of certain types of programs and is  performed by a compiler. Prior work has shown that, in practice, vectorization yielded only modest performance improvements because only  about 30% of a program was being vectorized, on average. It was also not possible to determine how much more vectorization might be achievable.  This is a very important problem, because significantly improved vectorization yields not only a much faster execution of programs but significant energy savings as well. The latter is another critical problem in computer design. Performance  and energy constraints are critical bottlenecks in many important applications - from weather prediction to development of new drugs and materials, climate change, and advances in every scientific and engineering field, They also limit consumer products such as cell phones, TV's, etc. in what they can do - just think of artificial intelligence tasks that we want run directly on our devices.  In this project we were able to establish that relative failure to vectorize well is due not to the lack of parallelization/vectorization techniques (which are plentiful following 50+ years of prior research), but rather because of the inability of existing tools to predict well which part of the code can be parallelized/vectorized and which precise program  transformations (out of the myriad available) would result in the most significant performance/efficiency increase. We then determined the transformations that are most useful in vectorizing real applications and established a large open-source, extensible, repository of program kernels -  annotated with thorough data on their performance, the transformations applied to best vectorize them, and speedups achieved on a modern computer system. This was then used to develop machine learning techniques (ML) that can predict with high accuracy what transformations may be usefully applied to a given code to optimize its performance.  In addition, this repository can serve as a valuable resource for future generations of researchers as well as a baseline to benchmark any future work in the area against. Based on this foundation, we then developed a meta-compiler, that draws on the best available state-of-the-art vectorizing and parallelizing compilers, to produce code that is guaranteed to be as good as that produced by the best tools available at a given time, and in practice usually produces code superior (more efficient in speed and/or energy consumption) than any of the state-of-the-art compilers available at this time, with significant speedups obtained on full programs ranging upwards of 28 times as fast (!) when compared with the best compiler available.           Last Modified: 01/08/2021       Submitted by: Alexander V Veidenbaum]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
