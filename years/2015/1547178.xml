<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>INSPIRE: The RAVE Revolution for Children with Minimal Language Experience During Sensitive Periods of Brain and Language Development</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1000000.00</AwardTotalIntnAmount>
<AwardAmount>1150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This Integrated NSF Support Promoting Interdisciplinary Research and Education (INSPIRE) project "The RAVE Revolution for Children with Minimal Language Experience During Sensitive Periods of Brain and Language Development" is jointly funded by  Robust Intelligence and Cyber-Human Systems Programs of the Computer and Information Science and Engineering Directorate;  the Developmental and Learning Sciences Program, and the Science of Learning Centers Program of the Directorate for Social, Behavioral &amp; Economic Sciences; the Office of International Science and Engineering; and the Office of Integrated Activities. The interdisciplinary team expands the boundaries of traditionally separate disciplines by uniting synergistically to explore a transformative learning tool to reduce the devastating impact of minimal language experience on children. Children's dramatically reduced language experience has been shown to have a deleterious impact on learning language, reading, and achieving normal cognitive functions across life,a problem facing many children in the nation (e.g., children from low SES backgrounds; late-exposed bilingual children). Deaf babies are at particular risk, as beyond minimal language experience, many receive no accessible language experience in early life. The Robot AVatar thermal-Enhanced learning tool, or RAVE, is placed near a baby's high-chair and makes available multiple core components of sign language, with speech options, in first-time socially interactive ways, and, crucially, during critical periods of human brain and behavior development. Exploration of this revolutionary learning tool involves scientists spanning four disciplines (Developmental Cognitive Neuroscience, Virtual Human Science, Robotics, and Applied Psychophysiology/Biomedical Engineering), provides science answers about how all babies discover core components of language, resolves previously insoluble problems in the four sciences, and is propelled by the shared objective to enhance early learning gains for populations that would otherwise be at a lifelong disadvantage.&lt;br/&gt;&lt;br/&gt;Of particular novelty, fNIRS brain imaging provides first-time discovery of babies' sensitivities to multiple rhythmic components underlying language vital to healthy language learning. This forms the basis for creating rhythmic patterns at the core of nursery rhymes and conversational samples in sign language (through Motion Capture) and speech. Thermal IR imaging+eye tracking identifies when deaf babies who cannot yet produce language are in a peaked arousal state and ready to learn. This triggers the robot+virtual human to start, cease, or solicit interaction based on the baby's social engagement. When the baby's gaze locks with the robot, the robot initiates gaze direction to virtual humans, initiating joint attention and socially-contingent conversation in human artificial agent interaction. This triggers an interfaced screen where virtual humans provide rhythmic nursery rhymes and conversations in sign language, with speech options. Together, the RAVE team explores a new aid to children with minimal or no language input; provides the nation with a competitive science and technological edge; and, trains under-represented groups in STEM, students in interdisciplinary science, and young deaf scientists in the advancement of science with transformative translational significance for all society.</AbstractNarration>
<MinAmdLetterDate>08/25/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1547178</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Traum</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David R Traum</PI_FULL_NAME>
<EmailAddress>traum@ict.usc.edu</EmailAddress>
<PI_PHON>3105745729</PI_PHON>
<NSF_ID>000173016</NSF_ID>
<StartDate>08/25/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Laura-Ann</FirstName>
<LastName>Petitto</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laura-Ann Petitto</PI_FULL_NAME>
<EmailAddress>Laura-Ann.Petitto@Gallaudet.Edu</EmailAddress>
<PI_PHON>2026515866</PI_PHON>
<NSF_ID>000119199</NSF_ID>
<StartDate>08/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Gallaudet University</Name>
<CityName>Washington</CityName>
<ZipCode>200023660</ZipCode>
<PhoneNumber>2026515497</PhoneNumber>
<StreetAddress>800 Florida Avenue, NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003259439</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GALLAUDET UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003259439</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Gallaudet University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200023695</ZipCode>
<StreetAddress><![CDATA[800 Florida Ave. NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>004Y</Code>
<Text>Science of Learning</Text>
</ProgramElement>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>1698</Code>
<Text>DS -Developmental Sciences</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7704</Code>
<Text>Science of Learning Activities</Text>
</ProgramElement>
<ProgramElement>
<Code>8078</Code>
<Text>INSPIRE</Text>
</ProgramElement>
<ProgramReference>
<Code>5920</Code>
<Text>ITALY</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8653</Code>
<Text>INSPIRE Track-1 Creative</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~1000000</FUND_OBLG>
<FUND_OBLG>2017~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We created an innovative language learning tool and research platform, called the RAVE (Robot,Avatar, thermal&nbsp;Enhanced language learning tool) through new discoveries that expanded the boundaries of traditionally separate sciences. The RAVE prototype makes available multiple components of human language in socially interactive and contingent conversational ways to young infants during critical periods of human learning and brain development. We especially target periods in early child development whereupon minimal language input (or, lack of sufficient language input), and other states of extreme <em>language deprivation</em>, can have devastating deleterious impact on language, reading, and academic success spanning the lifetime. We bring together science from multiple disciplines to explore the potential for technologies such as functional near infrared spectroscopy neuroimaging (fNIRS; measures neural activity underlying human cognition), thermal infrared imaging (measures human emotions), robotics, and avatars to positively impact children's language learning. Our broader impact objectives are ultimately to allow early learning gains for populations of deaf and hearing children who would otherwise be at a lifelong disadvantage, and also, more immediately, to advance involvement of underrepresented groups in STEM, propel interdisciplinary student scholarship, and train young deaf scientists in the advancement of scientific knowledge with transformative translational significance for all society.</p> <p>Accomplishments: We have successfully built a novel prototype of an Artificial Intelligence Machine consisting of two Artificial Agents (robot+avatar) that engages babies ages 6 to 12 months to promote early language learning, especially deaf and hearing infants with minimal or no early-life language exposure. Our RAVE prototype also involves advancement of new experimental discoveries about the growing brain, language, and cognition in early child development, and new technology integration that, in turn, renders a powerful research platform for future scientific and technological exploration and expansion.</p> <p>How RAVE Works? Our RAVE prototype is placed near an infant's highchair, feeding seat, or crib (see Figure 1). (i) Thermal IR imaging+eye tracking triggers the robot+avatar to start, cease, or solicit interaction based on the infant's emotional/social engagement. (ii) When an infant's socially-interactive gaze locks with the robot, the robot shifts gaze to the avatar screen. (iii) Simultaneously, this triggers an interfaced screen where an avatar provides nursery rhymes and simple visual conversation in American Sign Language (ASL). Together, this permits a first-time simulation of joint attention and socially contingent conversation in human artificial agent interaction. (iv) Cognitive Neuroscience research uses fNIRS neuroimaging in three ways: to (a) crack-the-code for the rhythmic temporal movement frequencies underlying all infants' sensitivity to different parts of the structure of human language (and when) crucial to early phonological development and later reading success. This, in turn, is used for building both linguistic movements with precision into the avatar's ASL nursery rhymes and the robot's perception of the infant's movements (is the infant signing, or is the infant just moving its hands?); (b) reveal whether infants' perception of RAVE engages classic linguistic brain sites (LIFC, STG, etc.) vs nonlinguistic brain sites, thus yielding powerful answers as to whether RAVE is engaging the correct neural tissue necessary to kick-start language acquisition in infants who cannot yet produce language; and, (c) determine whether infants' productions to RAVE are linguistic (e.g., producing signs, protosigns, or sign babbling) or nonlinguistic (producing general excitatory hand gestures found in all infants). (v) Thermal IR imaging reveals those linguistic frequencies to which infants have peaked emotional, arousal-attentional interest, and, thus, reveals when a human infant is Ready to Learn, even before they can produce language. Together, fNIRS and thermal IR imaging discoveries guide (vi) motion capture in building visual sign language samples and simple sentences for the avatar with just those precise rhythmic temporal frequencies unique to language structure.&nbsp;</p> <p>Does RAVE Work? Our experiments revealed the surprising (see Figures 2-4 of babies in Experimental set up): RAVE evidenced the potential to impart language learning in very young hearing and deaf babies ages 6-12 months. All babies preferred the avatar on the TV screen more than the physically embodied robot. Experimental results demonstrated that this preference was due to the babies' sensitivity to specific rhythmic temporal patterns underlying phonetic-syllabic structure in all human language (signed or spoken). Like a lock and key, &nbsp;we intentionally built these rhythmic temporal language patterns into the avatar's ASL signing based on brain discoveries about the language patterns to which all infant brains are maximally sensitive (and at what maturational ages). Remarkably, all babies produced linguistic responses to the avatar's linguistic productions, even hearing babies with no sign exposure&nbsp;</p> <p>Broad Impact:&nbsp; A RAVE language learning tool can break the communication barrier that impacts many deaf and hearing children with&nbsp;<em>language deprivation</em>&nbsp;in the United States. By providing sign language experiences to deaf and hearing babies, RAVE may lessen the&nbsp;devastating deleterious impact that minimal or no language input in infancy can have on language and reading spanning the lifetime.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>This document has been edited with the <a href="https://html-cleaner.com/" target="_blank">instant web content beautifier tool</a>&nbsp;which can be found at htmleditor.tools - give it a try.</p><br> <p>            Last Modified: 07/26/2020<br>      Modified by: Laura-Ann&nbsp;Petitto</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549667556698_PetittoFigure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549667556698_PetittoFigure2--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549667556698_PetittoFigure2--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Composite image showing the multiparty interactions (and, thus, the multiple social conversational roles) that this AI system made possible among the Avatar, Robot, and infant.</div> <div class="imageCredit">Laura-Ann Petitto@Gallaudet University</div> <div class="imageSubmitted">Laura-Ann&nbsp;Petitto</div> <div class="imageTitle">Figure 2</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668155984_PetittoFigure3--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668155984_PetittoFigure3--rgov-800width.jpg" title="Figure 3"><img src="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668155984_PetittoFigure3--rgov-66x44.jpg" alt="Figure 3"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Shows one baby+system interactional sequence. Avatar began an ASL Nursery Rhyme. Robot turns to Avatar, nods to her. Baby then copies the Avatar by spontaneously producing signs/proto-signs. Surprisingly, this hearing boy, age 12 months, 1 day, has had no exposure to American Sign Language.</div> <div class="imageCredit">Laura-Ann Petitto@Gallaudet University</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Laura-Ann&nbsp;Petitto</div> <div class="imageTitle">Figure 3</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668269961_PetittoFigure4--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668269961_PetittoFigure4--rgov-800width.jpg" title="Figure 4"><img src="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668269961_PetittoFigure4--rgov-66x44.jpg" alt="Figure 4"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Composite image of a baby girl, age 11 months, with early exposure to American Sign Language, producing a spontaneous waving-hand social gesture (�Hi�) towards the Avatar (top left).</div> <div class="imageCredit">Laura-Ann Petitto@Gallaudet University</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Laura-Ann&nbsp;Petitto</div> <div class="imageTitle">Figure 4</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668562866_PetittoFigure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668562866_PetittoFigure1--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2019/1547178/1547178_10391318_1549668562866_PetittoFigure1--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Components of the Artificial Intelligence RAVE Language Learning Tool for young Deaf and Hearing Infants</div> <div class="imageCredit">Laura-Ann Petitto@Gallaudet University</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Laura-Ann&nbsp;Petitto</div> <div class="imageTitle">Figure 1</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We created an innovative language learning tool and research platform, called the RAVE (Robot,Avatar, thermal Enhanced language learning tool) through new discoveries that expanded the boundaries of traditionally separate sciences. The RAVE prototype makes available multiple components of human language in socially interactive and contingent conversational ways to young infants during critical periods of human learning and brain development. We especially target periods in early child development whereupon minimal language input (or, lack of sufficient language input), and other states of extreme language deprivation, can have devastating deleterious impact on language, reading, and academic success spanning the lifetime. We bring together science from multiple disciplines to explore the potential for technologies such as functional near infrared spectroscopy neuroimaging (fNIRS; measures neural activity underlying human cognition), thermal infrared imaging (measures human emotions), robotics, and avatars to positively impact children's language learning. Our broader impact objectives are ultimately to allow early learning gains for populations of deaf and hearing children who would otherwise be at a lifelong disadvantage, and also, more immediately, to advance involvement of underrepresented groups in STEM, propel interdisciplinary student scholarship, and train young deaf scientists in the advancement of scientific knowledge with transformative translational significance for all society.  Accomplishments: We have successfully built a novel prototype of an Artificial Intelligence Machine consisting of two Artificial Agents (robot+avatar) that engages babies ages 6 to 12 months to promote early language learning, especially deaf and hearing infants with minimal or no early-life language exposure. Our RAVE prototype also involves advancement of new experimental discoveries about the growing brain, language, and cognition in early child development, and new technology integration that, in turn, renders a powerful research platform for future scientific and technological exploration and expansion.  How RAVE Works? Our RAVE prototype is placed near an infant's highchair, feeding seat, or crib (see Figure 1). (i) Thermal IR imaging+eye tracking triggers the robot+avatar to start, cease, or solicit interaction based on the infant's emotional/social engagement. (ii) When an infant's socially-interactive gaze locks with the robot, the robot shifts gaze to the avatar screen. (iii) Simultaneously, this triggers an interfaced screen where an avatar provides nursery rhymes and simple visual conversation in American Sign Language (ASL). Together, this permits a first-time simulation of joint attention and socially contingent conversation in human artificial agent interaction. (iv) Cognitive Neuroscience research uses fNIRS neuroimaging in three ways: to (a) crack-the-code for the rhythmic temporal movement frequencies underlying all infants' sensitivity to different parts of the structure of human language (and when) crucial to early phonological development and later reading success. This, in turn, is used for building both linguistic movements with precision into the avatar's ASL nursery rhymes and the robot's perception of the infant's movements (is the infant signing, or is the infant just moving its hands?); (b) reveal whether infants' perception of RAVE engages classic linguistic brain sites (LIFC, STG, etc.) vs nonlinguistic brain sites, thus yielding powerful answers as to whether RAVE is engaging the correct neural tissue necessary to kick-start language acquisition in infants who cannot yet produce language; and, (c) determine whether infants' productions to RAVE are linguistic (e.g., producing signs, protosigns, or sign babbling) or nonlinguistic (producing general excitatory hand gestures found in all infants). (v) Thermal IR imaging reveals those linguistic frequencies to which infants have peaked emotional, arousal-attentional interest, and, thus, reveals when a human infant is Ready to Learn, even before they can produce language. Together, fNIRS and thermal IR imaging discoveries guide (vi) motion capture in building visual sign language samples and simple sentences for the avatar with just those precise rhythmic temporal frequencies unique to language structure.   Does RAVE Work? Our experiments revealed the surprising (see Figures 2-4 of babies in Experimental set up): RAVE evidenced the potential to impart language learning in very young hearing and deaf babies ages 6-12 months. All babies preferred the avatar on the TV screen more than the physically embodied robot. Experimental results demonstrated that this preference was due to the babies' sensitivity to specific rhythmic temporal patterns underlying phonetic-syllabic structure in all human language (signed or spoken). Like a lock and key,  we intentionally built these rhythmic temporal language patterns into the avatar's ASL signing based on brain discoveries about the language patterns to which all infant brains are maximally sensitive (and at what maturational ages). Remarkably, all babies produced linguistic responses to the avatar's linguistic productions, even hearing babies with no sign exposure   Broad Impact:  A RAVE language learning tool can break the communication barrier that impacts many deaf and hearing children with language deprivation in the United States. By providing sign language experiences to deaf and hearing babies, RAVE may lessen the devastating deleterious impact that minimal or no language input in infancy can have on language and reading spanning the lifetime.              This document has been edited with the instant web content beautifier tool which can be found at htmleditor.tools - give it a try.       Last Modified: 07/26/2020       Submitted by: Laura-Ann Petitto]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
