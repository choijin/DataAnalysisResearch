<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Large-Scale Structured Sparse Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>499862.00</AwardTotalIntnAmount>
<AwardAmount>499862</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei Ding</SignBlockName>
<PO_EMAI>weiding@nsf.gov</PO_EMAI>
<PO_PHON>7032928017</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recent technological revolutions have lead to dramatically growing scale, diversity, and complexity of data.  Modern data analysis is facing new challenges in handling this complexity.  Although complex, the underlying representations of many real-world data are often sparse.  This sparseness often exhibits intrinsic structure, e.g., spatial or temporal smoothness, graphs, trees, and groups. Finding effective sparse representations is fundamentally important for scientific discovery; the a-priori structure information may significantly improve the sparse learning model.  This project is developing algorithms and tools (including open source software) to enable knowledge discovery from massive high-dimensional and complex data, as well as a new curriculum that incorporates the proposed research into the classroom.&lt;br/&gt;&lt;br/&gt;Most sparse learning algorithms are based on the L1 norm due to its sparsity-inducing property and strong theoretical guarantees, but this does not capture structure.  This project is advancing structured sparse learning by (1) analyzing the so-called proximal operators associated with various feature structures, which explains how and why they can induce the desired structured sparsity; (2) developing efficient algorithms for computing the proximal operators, which plays a key building block role in the proposed optimization algorithms; (3) developing a structured sparse learning framework, which includes various sparse learning models and algorithms developed in this project.</AbstractNarration>
<MinAmdLetterDate>04/22/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/17/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1539991</AwardID>
<Investigator>
<FirstName>Jieping</FirstName>
<LastName>Ye</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jieping Ye</PI_FULL_NAME>
<EmailAddress>jpye@umich.edu</EmailAddress>
<PI_PHON>7346155510</PI_PHON>
<NSF_ID>000488391</NSF_ID>
<StartDate>04/22/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName/>
<StateCode>MI</StateCode>
<ZipCode>481091274</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~333360</FUND_OBLG>
<FUND_OBLG>2015~166502</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>The underlying representations of many real-world data are often sparse. Therefore,&nbsp;finding sparse representations is fundamentally important for scientific discovery.&nbsp;<span>Traditional sparse learning models based on the &#8467;1 norm do not take the existing feature&nbsp;structure into consideration. However, in many applications, the features exhibit certain intrinsic&nbsp;structures, e.g., spatial or temporal smoothness, graphs, trees, and groups. The<span>&nbsp;structured sparse learning (SSL) models involve a nonsmooth optimization problem,&nbsp;which is challenging to solve especially for large-scale problems encountered in many practical&nbsp;applications. The goal of this project is to develop effective structured sparse learning models that take the existing feature structure into account and develop efficient algorithms for the proposed models.&nbsp;</span></span></span></p> <p><span><span><span>The SSL models are difficult to solve due to the high complexity in feature structures. In this project, we have provided novel theoretical insights into the so-called proximal operator and developed highly efficient and scalable algorithms for various SSL models. In addition, we have developed novel sparse screening rules which significantly speed up SSL algorithms. Specifically, we have developed novel Dual Polytope Projections (DPP) rules for Lasso, which was shown to gain two orders of magnitude speedup, and extended the DPP methods to SSL models, including group Lasso, sparse group Lasso, and tree Lasso.&nbsp;</span></span></span></p> <p><span><span><span><span>Results related to the proposed research have been published at top conference and journal papers.&nbsp;</span></span></span></span>We have also developed a software package (DPC), which includes the implementation of many sparse screening algorithms developed in this project.</p> <p>&nbsp;</p> <p><span><span><span><br /></span></span></span></p><br> <p>            Last Modified: 11/18/2019<br>      Modified by: Jieping&nbsp;Ye</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The underlying representations of many real-world data are often sparse. Therefore, finding sparse representations is fundamentally important for scientific discovery. Traditional sparse learning models based on the &#8467;1 norm do not take the existing feature structure into consideration. However, in many applications, the features exhibit certain intrinsic structures, e.g., spatial or temporal smoothness, graphs, trees, and groups. The structured sparse learning (SSL) models involve a nonsmooth optimization problem, which is challenging to solve especially for large-scale problems encountered in many practical applications. The goal of this project is to develop effective structured sparse learning models that take the existing feature structure into account and develop efficient algorithms for the proposed models.   The SSL models are difficult to solve due to the high complexity in feature structures. In this project, we have provided novel theoretical insights into the so-called proximal operator and developed highly efficient and scalable algorithms for various SSL models. In addition, we have developed novel sparse screening rules which significantly speed up SSL algorithms. Specifically, we have developed novel Dual Polytope Projections (DPP) rules for Lasso, which was shown to gain two orders of magnitude speedup, and extended the DPP methods to SSL models, including group Lasso, sparse group Lasso, and tree Lasso.   Results related to the proposed research have been published at top conference and journal papers. We have also developed a software package (DPC), which includes the implementation of many sparse screening algorithms developed in this project.             Last Modified: 11/18/2019       Submitted by: Jieping Ye]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
