<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS:FULL:DSD: Collaborative Research: FPGA Cloud Platform for Deep Learning, Applications in Computer Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>574044.00</AwardTotalIntnAmount>
<AwardAmount>574044</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>We stand on the verge of dramatic advances in deep learning applications, which will soon enable practicality and widespread adoption of computer vision based recognition in scientific inquiry, commercial applications, and everyday life.  Grand challenge problems are within our reach; we will soon be able to build automated systems that recognize nearly everything we see, systems that can recognize the tens of thousands of basic-level categories that psychologists posit humans can recognize, systems that continuously learn from photos, video, and web content in order to create more complete and accurate visual models of the world.  However, while it is clear that the computational capabilities for deep learning are within reach, it is equally clear that the required computational power cannot come from general-purpose processors.  To succeed, we will need to build specialized domain-specific computing systems based on hardware accelerators that are capable of exploiting the extreme fine-grained parallelism inherent in deep-learning workloads.  This project leverages parallelization and reconfigurable hardware to create an automated system that distributes computer vision algorithms onto a large number of field-programmable gate arrays (FPGA Cloud). &lt;br/&gt;&lt;br/&gt;This project builds on recent advances in domain-specific hardware generation tools in order to bring the potential parallelism and performance per watt advantages of FPGAs to large-scale computer vision problems.   By developing a platform to run deep learning algorithms on large clouds of FPGAs, this proposal explicitly addresses scaling algorithms beyond what a single chip can process.  This involves addressing a wide range of challenging problems in algorithm analysis, building domain-specific hardware generators, communication for scaling algorithms across multiple FPGAs, and extensive validation of generating hardware for state-of-the-art deep learning approaches applied to computer vision problems.  This project advances tools for designing domain-specific FPGA implementations of algorithms, taking a step toward making more efficient computing with greater parallelism more widely available.  In particular, for computer vision, there will be significant benefits from a product of multiple improvements: higher parallelism, lower gate requirement by moving to fixed point when possible, and better performance per watt leading to higher computation density in servers.  Together, these have the potential to significantly increase the extent to which computer vision can be a part of our daily lives, making computers better able to understand the context of our world.</AbstractNarration>
<MinAmdLetterDate>08/13/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1533739</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Milder</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter Milder</PI_FULL_NAME>
<EmailAddress>peter.milder@stonybrook.edu</EmailAddress>
<PI_PHON>6316328407</PI_PHON>
<NSF_ID>000629904</NSF_ID>
<StartDate>08/13/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Ferdman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Ferdman</PI_FULL_NAME>
<EmailAddress>mferdman@cs.stonybrook.edu</EmailAddress>
<PI_PHON>6316328449</PI_PHON>
<NSF_ID>000634656</NSF_ID>
<StartDate>08/13/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Stony Brook]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>117944400</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~574044</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Deep neural networks have revolutionized many fields including computer vision, natural language processing, fraud detection, bioinformatics, and many other domains with a wide range of commercial and scientific applications. Convolutional neural networks (CNNs), primarily used to solve computer vision challenges, are at the vanguard of this transformation. However, CNNs exhibit immense computational costs, necessitating hardware acceleration.</p> <p>The goal of this project was to develop technologies for leveraging field-programmable gate arrays (FPGAs) for cloud-scale computation. FPGAs exhibit efficiency advantages over CPUs and GPUs; efficient use of FPGAs can greatly improve the practical capabilities of computer vision.&nbsp;</p> <p>In the course of this project, we made several key innovations in implementing CNN accelerators, which enable higher efficiency (and therefore higher performance). We evaluated our ideas analytically and through FPGA prototyping, and we developed associated design optimization methods that would allow an automated tool to perform the necessary design exploration to parameterize the architectures that we developed. &nbsp;This work has resulted in papers published and presented at major conferences, including ISCA, MICRO, FCCM, and FPL. The intellectual contributions of this project include:</p> <p>- Fused layer convolutional neural network evaluation. &nbsp;We developed a new method for greatly reducing the memory bandwidth required for CNN accelerators. This method works by optimizing the flow of data across CNN layers. &nbsp;Rather than shuffling data on and off chip after each network stage, computation is reordered such that multiple stages can be fused, eliminating intermediate data transfer. Our results show that, on a portion of VGGNet-E, we can reduce the data transfer by 95%, from 77MB down to 3.6MB, at the cost of only 362KB of on-chip storage. &nbsp;This work was published at MICRO'16 and was granted a US patent.</p> <p>- Multi-CLP accelerator architecture. &nbsp;We developed a new CNN accelerator paradigm and an accompanying automated design methodology that partitions the available FPGA resources into multiple processors, each of which is tailored for a different subset of the CNN convolutional layers. Using the same FPGA resources as a single large processor, multiple smaller specialized processors increase computational efficiency and lead to a higher overall throughput. Our design methodology achieved 3.8x higher throughput than the state-of-the-art approach on evaluating the popular AlexNet CNN on a Xilinx Virtex-7 FPGA. Initial versions of this technique were published in FCCM'16 and a generalized version was published in ISCA'17.</p> <p>- Escher CNN accelerator design. &nbsp;We developed a new accelerator design with a flexible data buffering scheme that ensures a balance between the input and weight transfer bandwidth, significantly reducing overall bandwidth requirements. For example, compared to the state-of-the-art CNN accelerator designs targeting a Virtex-7 690T FPGA, our design reduces the accelerator's peak bandwidth requirements by 2.4&times; across both fully-connected and convolutional layers on fixed-point AlexNet, and reduces convolutional layer bandwidth by up to 10.5&times; on fixed-point GoogLeNet. &nbsp;This work was published at FCCM'17.</p> <p>- Medusa memory interconnect. &nbsp;We developed a new memory interconnect specifically suited for Multi-CLP / many-narrow-port accelerators in systems with wide memory buses, such as FPGAs and systems with HBM memories. &nbsp;The Medusa interconnect uses significantly less resources than a traditional interconnect, while allowing it to run at much higher clock rates. &nbsp;This work was published at FPL'18.</p> <p>- Argus system. &nbsp;We wrote an overview paper for the IEEE Micro special issue on neural network accelerators that describes the end-to-end framework for CNN accelerators and all of the components developed and prototyped through this project.</p> <p>- Argus web-based generator. &nbsp;We created and made publicly and freely available a web site connected to our Argus hardware generator framework that can leverage our technologies to automatically produce optimized CNN designs for a given target network and FPGA. &nbsp;This interactive webpage allows users to log in, select their parameters, and download the resulting hardware design along with the necessary driver and software code to use that hardware.</p> <p>In addition to the technical impacts described above, other broader impacts of this project include the creation of a new graduate course on hardware accelerators for deep learning, and the training of PhD students, masters students, and undergraduates.<br /><br /><br /></p><br> <p>            Last Modified: 11/16/2020<br>      Modified by: Michael&nbsp;Ferdman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Deep neural networks have revolutionized many fields including computer vision, natural language processing, fraud detection, bioinformatics, and many other domains with a wide range of commercial and scientific applications. Convolutional neural networks (CNNs), primarily used to solve computer vision challenges, are at the vanguard of this transformation. However, CNNs exhibit immense computational costs, necessitating hardware acceleration.  The goal of this project was to develop technologies for leveraging field-programmable gate arrays (FPGAs) for cloud-scale computation. FPGAs exhibit efficiency advantages over CPUs and GPUs; efficient use of FPGAs can greatly improve the practical capabilities of computer vision.   In the course of this project, we made several key innovations in implementing CNN accelerators, which enable higher efficiency (and therefore higher performance). We evaluated our ideas analytically and through FPGA prototyping, and we developed associated design optimization methods that would allow an automated tool to perform the necessary design exploration to parameterize the architectures that we developed.  This work has resulted in papers published and presented at major conferences, including ISCA, MICRO, FCCM, and FPL. The intellectual contributions of this project include:  - Fused layer convolutional neural network evaluation.  We developed a new method for greatly reducing the memory bandwidth required for CNN accelerators. This method works by optimizing the flow of data across CNN layers.  Rather than shuffling data on and off chip after each network stage, computation is reordered such that multiple stages can be fused, eliminating intermediate data transfer. Our results show that, on a portion of VGGNet-E, we can reduce the data transfer by 95%, from 77MB down to 3.6MB, at the cost of only 362KB of on-chip storage.  This work was published at MICRO'16 and was granted a US patent.  - Multi-CLP accelerator architecture.  We developed a new CNN accelerator paradigm and an accompanying automated design methodology that partitions the available FPGA resources into multiple processors, each of which is tailored for a different subset of the CNN convolutional layers. Using the same FPGA resources as a single large processor, multiple smaller specialized processors increase computational efficiency and lead to a higher overall throughput. Our design methodology achieved 3.8x higher throughput than the state-of-the-art approach on evaluating the popular AlexNet CNN on a Xilinx Virtex-7 FPGA. Initial versions of this technique were published in FCCM'16 and a generalized version was published in ISCA'17.  - Escher CNN accelerator design.  We developed a new accelerator design with a flexible data buffering scheme that ensures a balance between the input and weight transfer bandwidth, significantly reducing overall bandwidth requirements. For example, compared to the state-of-the-art CNN accelerator designs targeting a Virtex-7 690T FPGA, our design reduces the accelerator's peak bandwidth requirements by 2.4&times; across both fully-connected and convolutional layers on fixed-point AlexNet, and reduces convolutional layer bandwidth by up to 10.5&times; on fixed-point GoogLeNet.  This work was published at FCCM'17.  - Medusa memory interconnect.  We developed a new memory interconnect specifically suited for Multi-CLP / many-narrow-port accelerators in systems with wide memory buses, such as FPGAs and systems with HBM memories.  The Medusa interconnect uses significantly less resources than a traditional interconnect, while allowing it to run at much higher clock rates.  This work was published at FPL'18.  - Argus system.  We wrote an overview paper for the IEEE Micro special issue on neural network accelerators that describes the end-to-end framework for CNN accelerators and all of the components developed and prototyped through this project.  - Argus web-based generator.  We created and made publicly and freely available a web site connected to our Argus hardware generator framework that can leverage our technologies to automatically produce optimized CNN designs for a given target network and FPGA.  This interactive webpage allows users to log in, select their parameters, and download the resulting hardware design along with the necessary driver and software code to use that hardware.  In addition to the technical impacts described above, other broader impacts of this project include the creation of a new graduate course on hardware accelerators for deep learning, and the training of PhD students, masters students, and undergraduates.          Last Modified: 11/16/2020       Submitted by: Michael Ferdman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
