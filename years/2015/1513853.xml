<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Assessing Speaker and Teacher Effectiveness through Gestural Analysis, EEG Recordings, and Eye Tracking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>899796.00</AwardTotalIntnAmount>
<AwardAmount>899796</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project helps speakers and teachers to measure and improve their impact on their audiences.  It uses visual observations of body, head, and hand gestures of the communicator, plus recordings of brain activity and eye movements of the audience.  Together, these determine which sections of a presentation elicit the most audience engagement. The project is developing new methods to capture and calibrate electroencephalogram and eye-tracking data from listeners and from students. It is determining new ways to relate this subject information to what a speaker or teacher can be seen to be doing while developing an argument or reviewing a concept.  The project produces analyses of when and how the communicator is most effective. This system is being ported to the Columbia Video Network distance education facility, for their use in improving the online delivery of Columbia University Master's level technical courses. This project continues a research effort that has involved women, minorities, disabled students, and undergrads.&lt;br/&gt;&lt;br/&gt;This research investigates the degree to which certain speaker gestures can convey significant information that are correlated to audience engagement, in speeches and in classroom lectures.  The project develops and validates a catalog of gestural attributes derived from pose and movements of body, head, and hand, and automatically extracts these attributes from videos.  It demonstrates correlations between gesture attributes and an objective method of measuring audience engagement: electroencephalography (EEG).  The project leverages a multi-disciplinary approach, with neural engineers and computer/media scientists collaborating to build a system that identifies and tracks physiological measures of engagement, and relates these to features in the video as well as information content.  It records subjects' high-density EEG, and tracks their eyes and pupillary responses while they are watching video lectures.  It uses machine learning, specifically novel methods which expand upon canonical correlation analysis, to relate inter- and intra-subject correlations, between the physiological changes and the gestural features derived from the video by using enhanced computer vision techniques.  These measures are further integrated with pupillary measures, which have been shown to correlate with arousal, as well as with gaze measures, which are indicative of attention.  The project is producing an analysis of body, head, and hand gestures useful in persuasion and in education, and a catalog of their influence on engagement and speaker effectiveness.</AbstractNarration>
<MinAmdLetterDate>08/28/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/28/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1513853</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Kender</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John R Kender</PI_FULL_NAME>
<EmailAddress>kender@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397115</PI_PHON>
<NSF_ID>000285996</NSF_ID>
<StartDate>08/28/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Sajda</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Sajda</PI_FULL_NAME>
<EmailAddress>ps629@columbia.edu</EmailAddress>
<PI_PHON>2128546851</PI_PHON>
<NSF_ID>000153174</NSF_ID>
<StartDate>08/28/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~899796</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project investigated how teachers and speakers can best engage and educate their audiences by the use of appropriate hand gestures and voice intonations.</p> <p><br />We wrote several instructional videos whose content was carefully synchronized with a series of post-test questions measuring the amount of learning.&nbsp; We used computer vision and machine learning techniques to extract speaker actions and audience eye movements, and correlated these to each other and to their test scores.&nbsp; About 60 people viewed these three videos under differing circumstances, with and without hearing speaker voice, with and without seeing speaker movements, resulting in a catalog of speaker head gestures and hand gestures, speaker intonations, subject eye fixations, and subject pupil diameter.</p> <p><br />We built a number of systems to extract and visualize specific speaker upper body positions and motions (shoulder, elbow, wrist), speaker gestural types ("deictic, iconic, metaphoric, beat"), speaker head positions (looking at camera, slides, hands), speaker word categories (concrete objects, abstract concepts, descriptive adjectives, adverbs of motions), speaker intonations (invitations, directions, change of concept), audience eye movements (looking at speaker, text, or illustrations), audience pupil diameter, and test question cognitive load (concrete recall, abstract inference).&nbsp; We found that we could predict many of these speaker and audience activities both from their immediately prior occurrences within a speaker or audience modality, but also across them.&nbsp; For example, we found that audience members who looked where the speaker was looking tended to have better test scores.&nbsp; We displayed these relationships using a number of techniques, such as heat maps, state transition graphs, and t-SNE scatter plots.</p> <p><br />Among our more significant findings were that head gestures simply amplified hand gestures; speakers point with their entire upper body; metaphoric gestures are best used for the description of abstract concepts and used with more precise intonations; different speaker voice properties will direct audience gaze at the speaker, the text, or to illustrations; pupil diameter, a sign of audience engagement, is predictable, especially for subjects who scored highly; audience engagement is better correlated to inference rather than recall questions; some subjects are more voice-oriented than text-oriented; and, in general, visual gestures often are distractive rather than helpful, and voice, text, and figure are the most suggestive of engagement and higher scores.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/13/2020<br>      Modified by: John&nbsp;R&nbsp;Kender</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583977398119_jk_integrated_visualization--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583977398119_jk_integrated_visualization--rgov-800width.jpg" title="Integrated research display"><img src="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583977398119_jk_integrated_visualization--rgov-66x44.jpg" alt="Integrated research display"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Speaker, text, and figures are displayed with eye fixations and pupil diameters superimposed.  Head and hand positions are illustrated in skeleton fashion</div> <div class="imageCredit">Columbia University</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">John&nbsp;R&nbsp;Kender</div> <div class="imageTitle">Integrated research display</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583977571446_ps_pd_prediction--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583977571446_ps_pd_prediction--rgov-800width.jpg" title="Pupil diameter prediction"><img src="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583977571446_ps_pd_prediction--rgov-66x44.jpg" alt="Pupil diameter prediction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Data flow of machine learning algorithm to predict pupil diameter (a proxy for engagement) based on prior location of eye fixation.</div> <div class="imageCredit">Columbia University</div> <div class="imageSubmitted">John&nbsp;R&nbsp;Kender</div> <div class="imageTitle">Pupil diameter prediction</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583978370899_sl_all_features--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583978370899_sl_all_features--rgov-800width.jpg" title="Input/output variables"><img src="/por/images/Reports/POR/2020/1513853/1513853_10393244_1583978370899_sl_all_features--rgov-66x44.jpg" alt="Input/output variables"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Analysis of variables involved in the creation of stimuli and the recording of responses of 60 subjects viewing 3 instructional videos.</div> <div class="imageCredit">Columbia University</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">John&nbsp;R&nbsp;Kender</div> <div class="imageTitle">Input/output variables</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project investigated how teachers and speakers can best engage and educate their audiences by the use of appropriate hand gestures and voice intonations.   We wrote several instructional videos whose content was carefully synchronized with a series of post-test questions measuring the amount of learning.  We used computer vision and machine learning techniques to extract speaker actions and audience eye movements, and correlated these to each other and to their test scores.  About 60 people viewed these three videos under differing circumstances, with and without hearing speaker voice, with and without seeing speaker movements, resulting in a catalog of speaker head gestures and hand gestures, speaker intonations, subject eye fixations, and subject pupil diameter.   We built a number of systems to extract and visualize specific speaker upper body positions and motions (shoulder, elbow, wrist), speaker gestural types ("deictic, iconic, metaphoric, beat"), speaker head positions (looking at camera, slides, hands), speaker word categories (concrete objects, abstract concepts, descriptive adjectives, adverbs of motions), speaker intonations (invitations, directions, change of concept), audience eye movements (looking at speaker, text, or illustrations), audience pupil diameter, and test question cognitive load (concrete recall, abstract inference).  We found that we could predict many of these speaker and audience activities both from their immediately prior occurrences within a speaker or audience modality, but also across them.  For example, we found that audience members who looked where the speaker was looking tended to have better test scores.  We displayed these relationships using a number of techniques, such as heat maps, state transition graphs, and t-SNE scatter plots.   Among our more significant findings were that head gestures simply amplified hand gestures; speakers point with their entire upper body; metaphoric gestures are best used for the description of abstract concepts and used with more precise intonations; different speaker voice properties will direct audience gaze at the speaker, the text, or to illustrations; pupil diameter, a sign of audience engagement, is predictable, especially for subjects who scored highly; audience engagement is better correlated to inference rather than recall questions; some subjects are more voice-oriented than text-oriented; and, in general, visual gestures often are distractive rather than helpful, and voice, text, and figure are the most suggestive of engagement and higher scores.          Last Modified: 03/13/2020       Submitted by: John R Kender]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
