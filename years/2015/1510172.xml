<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>High-dimensional M-estimation: Understanding risk, improving performance and assessing resampling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>10/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>394178.00</AwardTotalIntnAmount>
<AwardAmount>394178</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The nature of datasets that scientists in academia or industry are currently working with is changing at a very rapid pace. The data is more complex, larger and higher-dimensional than it has ever been before. A lot of the methods used in practice are based on the idea that they are somehow optimal, in terms of measurement accuracy or prediction of future outcomes, at least for certain models of data generating mechanism. A most widely applied and time-honored principle of data analysis is the use of so-called "maximum likelihood methods". It has recently been discovered by the PI and collaborators that in a setting often encountered with modern and large datasets, these maximum likelihood methods are suboptimal and can be improved upon. This is true even for an extremely basic and widely used technique (e.g.,linear regression). One of the aims of the project is to understand if the same phenomena occur for other methods that are widely used in machine/statistical learning practice and in turn develop better tools for data scientists and data analysts. Currently, accuracy assessment for these estimators are often performed through data driven procedures (such as the bootstrap). Another aim of the project is to understand if the corresponding accuracy assessment are misleading for datasets with many predictors. If that is the case, the PI is planning to work on methods to correct the existing procedures so they yield trustworthy accuracy assessments. &lt;br/&gt;&lt;br/&gt;High-dimensional statistics offers a profound challenge to classical statistics, both on the applied and the theoretical end. A broad class of methods used in practice is based on solving nontrivial optimization problems to estimate parameters of interest. This yields a so-called M-estimator. When the dimension of this estimator is small compared to the number of observations the practitioner has, standard empirical process techniques can be applied to understand the statistical properties of those estimators. In the setting the PI considers, these techniques fail and new techniques need to be developed. The PI plans on using a mix of tools inspired from random matrix theory, convex analysis and concentration of measure results to study those estimators. The development of new optimal methods is expected - based on using tools from convex analysis. Another exciting research line is that the techniques developed by the PI should allow us to study resampling methods in high-dimension (such as the bootstrap). Those are widely used to assess statistical significance from the observed dataset, without having to appeal to theoretical arguments. While the low-dimensional theory is well-established and relatively easy, and suggests that these numerical methods should work well, the high-dimensional case has yet to be understood. The PI plans on studying these problems thoroughly and propose practically relevant solutions if these widely used-in-practice methods are shown to provide statistically misleading accuracy assessments.</AbstractNarration>
<MinAmdLetterDate>07/24/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/30/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1510172</AwardID>
<Investigator>
<FirstName>Noureddine</FirstName>
<LastName>El Karoui</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Noureddine El Karoui</PI_FULL_NAME>
<EmailAddress>nkaroui@stat.berkeley.edu</EmailAddress>
<PI_PHON>5106423332</PI_PHON>
<NSF_ID>000488593</NSF_ID>
<StartDate>07/24/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>947203860</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~106688</FUND_OBLG>
<FUND_OBLG>2016~140693</FUND_OBLG>
<FUND_OBLG>2017~146797</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main focus of this grant was to understand how widely used statistical methods behave in certain "big data settings", both theoretically and numerically. <br /><br />Suppose someone wants to predict the selling price of a house. They can collect measurements such as square footage, number of bedrooms, lot size etc... Based on this collection of data for several houses, standard statistical methods can be used to give a prediction for the price of a house coming on the market (with new measurements) and an estimate of uncertainty about this prediction and various important characteristics of a generic prediction.<br /><br />Traditionally, statistical theory has been developed in the framework where the number of observations (here houses) in the dataset is much higher than the number of measurements per observations. We call this the traditional framework. However, in practice nowadays that is not always the case. One reason is that it is increasingly easy to collect many measurements per observations of interest. Another reason is that we may not have access to a very large dataset of observations: for instance the housing market changes, so we may want to keep only a limited number of recent house selling prices in a certain ZIP code. One central question investigated in this grant was: how reliable are these statistical methods when the number of observations in the dataset is proportional to the number of measurements per observations? This is called the high-dimensional framework.<br /><br />We developed new mathematical and theoretical tools to answer this question, in certain theoretical settings. The short answer is that classical uncertainty assessment methods and results become unreliable in the high-dimensional framework, whereas they are reliable in the traditional framework. <br /><br />Another approach for uncertainty assessment relies less on theory and more on computer simulations. This numerical approach is nowadays often considered preferable as it relies less on mathematical specifics of a model and can handle easily very intricate setups and methods. We showed that unfortunately a widely used class of numerical techniques for uncertainty assessment also become unreliable in the high-dimensional setup we considered. Moreover, the problems these methods exhibit are somewhat unpredictable, so it is hard to find a generic fix. We proposed nonetheless some solutions in certain setups we understand well. <br /><br />An area of application where similar methods and statistical tools are used are internet auctions. Using related mathematical tools, we developed theoretical results for new bidder-friendly strategies in certain optimized auction formats. We also developed numerical approaches to this problem, which are more flexible in practice. This is important because billions of auctions are run on the internet everyday.<br /><br />One current concern with related statistical methods is whether they are fair with respect to certain protected characteristics. Work with other researchers interested in these fairness questions yielded new ideas and statistical/machine learning methods to try to improve fairness of these algorithms and mitigate algorithmic bias.</p><br> <p>            Last Modified: 02/29/2020<br>      Modified by: Noureddine&nbsp;El Karoui</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main focus of this grant was to understand how widely used statistical methods behave in certain "big data settings", both theoretically and numerically.   Suppose someone wants to predict the selling price of a house. They can collect measurements such as square footage, number of bedrooms, lot size etc... Based on this collection of data for several houses, standard statistical methods can be used to give a prediction for the price of a house coming on the market (with new measurements) and an estimate of uncertainty about this prediction and various important characteristics of a generic prediction.  Traditionally, statistical theory has been developed in the framework where the number of observations (here houses) in the dataset is much higher than the number of measurements per observations. We call this the traditional framework. However, in practice nowadays that is not always the case. One reason is that it is increasingly easy to collect many measurements per observations of interest. Another reason is that we may not have access to a very large dataset of observations: for instance the housing market changes, so we may want to keep only a limited number of recent house selling prices in a certain ZIP code. One central question investigated in this grant was: how reliable are these statistical methods when the number of observations in the dataset is proportional to the number of measurements per observations? This is called the high-dimensional framework.  We developed new mathematical and theoretical tools to answer this question, in certain theoretical settings. The short answer is that classical uncertainty assessment methods and results become unreliable in the high-dimensional framework, whereas they are reliable in the traditional framework.   Another approach for uncertainty assessment relies less on theory and more on computer simulations. This numerical approach is nowadays often considered preferable as it relies less on mathematical specifics of a model and can handle easily very intricate setups and methods. We showed that unfortunately a widely used class of numerical techniques for uncertainty assessment also become unreliable in the high-dimensional setup we considered. Moreover, the problems these methods exhibit are somewhat unpredictable, so it is hard to find a generic fix. We proposed nonetheless some solutions in certain setups we understand well.   An area of application where similar methods and statistical tools are used are internet auctions. Using related mathematical tools, we developed theoretical results for new bidder-friendly strategies in certain optimized auction formats. We also developed numerical approaches to this problem, which are more flexible in practice. This is important because billions of auctions are run on the internet everyday.  One current concern with related statistical methods is whether they are fair with respect to certain protected characteristics. Work with other researchers interested in these fairness questions yielded new ideas and statistical/machine learning methods to try to improve fairness of these algorithms and mitigate algorithmic bias.       Last Modified: 02/29/2020       Submitted by: Noureddine El Karoui]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
