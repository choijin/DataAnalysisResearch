<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Personalized Benchmarks for High Performance Computing Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>191000.00</AwardTotalIntnAmount>
<AwardAmount>191000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Edward Walker</SignBlockName>
<PO_EMAI>edwalker@nsf.gov</PO_EMAI>
<PO_PHON>7032924863</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As high-performance computing applications target ever-larger problems, data input and output (I/O) takes up more and more run time. Users, software developers, and platform administrators often find it difficult to understand what an application's I/O code is doing, why it is slow, how it might be improved, or how well it would perform on a different platform. I/O benchmarks help address this problem, but they are expensive to produce and thus are not available for most applications. This project is providing user-friendly personalized I/O benchmarks for all applications, by leveraging existing lightweight I/O profilers that already monitor the behavior of applications on high-performance computing platforms. The resulting personalized benchmarks will help researchers, developers, and purchasers in evaluating potential new storage system architectures, evaluating existing or new versions of storage systems and I/O libraries, planning for purchases, comparing performance of application clusters or workloads across platforms, and improving the performance of parallel I/O libraries and applications. The analytics and benchmark generation software, and example benchmarks, will be publicly released.&lt;br/&gt; &lt;br/&gt;This project uses two methods to construct personalized I/O benchmarks. First, the project is making existing applications self-benchmarking across all of their runs, by providing analytics and visualization facilities to convey to stakeholders the information already automatically captured by lightweight I/O profilers such as Darshan during each run. Second, the project is creating platform-customized benchmark suites that represent the mix of application-level workloads observed on a given platform. To accomplish this, the project is clustering observed production jobs based on their I/O behavior and using both new and existing I/O kernel generation techniques to generate a compact benchmark for each cluster. The resulting benchmark suite will advance the state of the art by serving as a proxy for real-world, platform-specific production I/O workloads, and by providing previously unavailable insight into how prevalent those workloads are at a given facility.</AbstractNarration>
<MinAmdLetterDate>08/14/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1535112</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Ross</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Ross</PI_FULL_NAME>
<EmailAddress>rross@mcs.anl.gov</EmailAddress>
<PI_PHON>6305254588</PI_PHON>
<NSF_ID>000259978</NSF_ID>
<StartDate>08/14/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Philip</FirstName>
<LastName>Carns</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Philip H Carns</PI_FULL_NAME>
<EmailAddress>carns@mcs.anl.gov</EmailAddress>
<PI_PHON>6306851268</PI_PHON>
<NSF_ID>000692401</NSF_ID>
<StartDate>08/14/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>7684</Code>
<Text>STRATEGIC TECHNOLOGIES FOR CI</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~191000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-a442c729-0455-a8b0-c6d4-cc0a621d28f2" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">This research seeks to improve scientific computing productivity by enabling more effective assessment of data access optimizations and procurement options for large-scale data-intensive computing platforms. &nbsp;More specifically, the two-year goal of this project was to develop methods to provide personalized supercomputing I/O benchmarks and analytics that represent the entire range of production workloads. &nbsp;To do this we instrumented a broad sampling of jobs at production HPC facilities, automated analysis of that instrumentation, and created customized benchmarks that reflect the aggregate workload of the system.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">The first component of this work is embodied in a software tool called Darshan that enables lightweight, transparent characterization of large-scale scientific computing applications [1]. &nbsp;Version 3.0 of Darshan includes enhanced instrumentation [2] developed in direct response to questions posed by a comprehensive study of I/O workloads on three major computing facilities [3]. This software is now deployed in production at the Argonne Leadership Computing Facility, the National Energy Research Scientific Computing Center, and the National Center for Supercomputing Applications.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">The second component of this work is embodied in a software tool called CODES that enables modeling of large scale storage and networking systems [4]. &nbsp;CODES includes a workload generator component that can ingest instrumentation data to recreate workloads for subsequent study. &nbsp;As part of this project, we enhanced the CODES workload generator to support the newest version of the Darshan I/O characterization tool [5].</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">These software tools can be used in tandem to create personalized benchmarks that recreate the I/O workload of arbitrary production applications, thereby enabling users, administrators, and researchers to more easily study the impact of optimizations and platform changes in isolation without the complexity of executing individual applications. &nbsp;This technology also enables the rapid construction of benchmarks for scientific applications that are otherwise not well-represented in the computer science community. &nbsp;Unlike previous I/O trace replay technologies, it does not require the user to execute costly special-purpose tracing or profiling runs to gather information. Instead, it generates a representative benchmark directly from data captured by Darshan during normal production runs.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">The attached figure illustrates an example of the application of this technology, in which a workload replay (i.e., personalized benchmark) is compared against the original application that it represents. &nbsp;The application in this example is a 576 process production climate simulation that accessed 660 GiB of data. &nbsp;The corresponding replay achieves close agreement to the original workload for experimental evaluation purposes without access to the original application.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">[1] </span><a style="text-decoration: none;" href="http://www.mcs.anl.gov/research/projects/darshan/"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; vertical-align: baseline;">http://www.mcs.anl.gov/research/projects/darshan/</span></a></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">[2] Shane Snyder, Philip Carns, Kevin Harms, Robert Ross, Glenn K. Lockwood, Nicholas J. Wright. Modular HPC I/O Characterization with Darshan. In </span><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;">Proceedings of 5th Workshop on Extreme-scale Programming Tools (ESPT 2016), </span><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">2016.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">[3] H. Luu, M. Winslett, W. Gropp R. Ross, P. Carns, K. Harms Prabhat, S. Byna, and Y. Yao. &nbsp;A Multiplatform Study of I/O Behavior on Petascale Supercomputers. &nbsp;24th International ACM Symposium on High-Performance Parallel and Distributed Computing (HPDC 2015), 2015.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">[4] </span><a style="text-decoration: none;" href="http://www.mcs.anl.gov/projects/codes/"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; vertical-align: baseline;">http://www.mcs.anl.gov/projects/codes/</span></a></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">[5] </span><a style="text-decoration: none;" href="https://xgitlab.cels.anl.gov/codes/codes/tree/darshan-3x-support"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; vertical-align: baseline;">https://xgitlab.cels.anl.gov/codes/codes/tree/darshan-3x-support</span></a></p><br> <p>            Last Modified: 11/28/2017<br>      Modified by: Philip&nbsp;H&nbsp;Carns</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1535112/1535112_10387982_1511901439373_online-figure--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1535112/1535112_10387982_1511901439373_online-figure--rgov-800width.jpg" title="Example of personalized benchmark technology"><img src="/por/images/Reports/POR/2017/1535112/1535112_10387982_1511901439373_online-figure--rgov-66x44.jpg" alt="Example of personalized benchmark technology"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The attached figure illustrates an example of the application of this technology, in which a workload replay (i.e., personalized benchmark) is compared against the original scientific application that it represents.</div> <div class="imageCredit">Philip Carns and Robert Ross</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Philip&nbsp;H&nbsp;Carns</div> <div class="imageTitle">Example of personalized benchmark technology</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[This research seeks to improve scientific computing productivity by enabling more effective assessment of data access optimizations and procurement options for large-scale data-intensive computing platforms.  More specifically, the two-year goal of this project was to develop methods to provide personalized supercomputing I/O benchmarks and analytics that represent the entire range of production workloads.  To do this we instrumented a broad sampling of jobs at production HPC facilities, automated analysis of that instrumentation, and created customized benchmarks that reflect the aggregate workload of the system.    The first component of this work is embodied in a software tool called Darshan that enables lightweight, transparent characterization of large-scale scientific computing applications [1].  Version 3.0 of Darshan includes enhanced instrumentation [2] developed in direct response to questions posed by a comprehensive study of I/O workloads on three major computing facilities [3]. This software is now deployed in production at the Argonne Leadership Computing Facility, the National Energy Research Scientific Computing Center, and the National Center for Supercomputing Applications.    The second component of this work is embodied in a software tool called CODES that enables modeling of large scale storage and networking systems [4].  CODES includes a workload generator component that can ingest instrumentation data to recreate workloads for subsequent study.  As part of this project, we enhanced the CODES workload generator to support the newest version of the Darshan I/O characterization tool [5].    These software tools can be used in tandem to create personalized benchmarks that recreate the I/O workload of arbitrary production applications, thereby enabling users, administrators, and researchers to more easily study the impact of optimizations and platform changes in isolation without the complexity of executing individual applications.  This technology also enables the rapid construction of benchmarks for scientific applications that are otherwise not well-represented in the computer science community.  Unlike previous I/O trace replay technologies, it does not require the user to execute costly special-purpose tracing or profiling runs to gather information. Instead, it generates a representative benchmark directly from data captured by Darshan during normal production runs.    The attached figure illustrates an example of the application of this technology, in which a workload replay (i.e., personalized benchmark) is compared against the original application that it represents.  The application in this example is a 576 process production climate simulation that accessed 660 GiB of data.  The corresponding replay achieves close agreement to the original workload for experimental evaluation purposes without access to the original application.    [1] http://www.mcs.anl.gov/research/projects/darshan/    [2] Shane Snyder, Philip Carns, Kevin Harms, Robert Ross, Glenn K. Lockwood, Nicholas J. Wright. Modular HPC I/O Characterization with Darshan. In Proceedings of 5th Workshop on Extreme-scale Programming Tools (ESPT 2016), 2016.    [3] H. Luu, M. Winslett, W. Gropp R. Ross, P. Carns, K. Harms Prabhat, S. Byna, and Y. Yao.  A Multiplatform Study of I/O Behavior on Petascale Supercomputers.  24th International ACM Symposium on High-Performance Parallel and Distributed Computing (HPDC 2015), 2015.    [4] http://www.mcs.anl.gov/projects/codes/    [5] https://xgitlab.cels.anl.gov/codes/codes/tree/darshan-3x-support       Last Modified: 11/28/2017       Submitted by: Philip H Carns]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
