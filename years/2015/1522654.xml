<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Acceleration Techniques for Lower-Order Algorithms in Nonlinear Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>177771.00</AwardTotalIntnAmount>
<AwardAmount>177771</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project focuses on developing efficient innovative acceleration techniques and their underlying theories for the algorithms in nonlinear optimization. The acceleration techniques and algorithms developed in this project will have broad impact in many areas of computational science, including imaging/signal processing, optimal control, computer vision, petroleum engineering, topology optimization, and electronic structure computations. The algorithms developed in this research will be made publicly available on the web and will be applied in solving various computational problems. In addition, the student involved in this project will have excellent opportunities to participate in interdisciplinary research.&lt;br/&gt;&lt;br/&gt;The research will include developing subspace techniques for nonlinear conjugate gradient method and accelerated nonlinear conjugate gradient methods with theoretically guaranteed optimal global complexity. A framework of inexact alternating direction method of multipliers (ADMM) will also be developed, in which multiple steps are allowed to solve the subproblem to an adaptive accuracy, while still maintaining global convergence even when the problem has more than two blocks. The project will also study acceleration strategies for gradient based stochastic optimization. In particular, adaptive strategies for choosing sample points and extracting quasi-Newton information based on the obtained stochastic information will be explored. In addition, a novel dual active set approach will be developed for solving smooth large-scale nonlinear optimization. For example, in projection on polyhedra, an algorithm can be developed to approximately identify the active linear constraints, while an asymptotically faster algorithm can be used to compute a high accuracy solution.</AbstractNarration>
<MinAmdLetterDate>07/28/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1522654</AwardID>
<Investigator>
<FirstName>Hongchao</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hongchao Zhang</PI_FULL_NAME>
<EmailAddress>hozhang@math.lsu.edu</EmailAddress>
<PI_PHON>2255781982</PI_PHON>
<NSF_ID>000518125</NSF_ID>
<StartDate>07/28/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Louisiana State University</Name>
<CityName>Baton Rouge</CityName>
<ZipCode>708032701</ZipCode>
<PhoneNumber>2255782760</PhoneNumber>
<StreetAddress>202 Himes Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Louisiana</StateName>
<StateCode>LA</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>LA06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>075050765</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LOUISIANA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>940050792</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Louisiana State University & Agricultural and Mechanical College]]></Name>
<CityName>Baton Rouge</CityName>
<StateCode>LA</StateCode>
<ZipCode>708032701</ZipCode>
<StreetAddress><![CDATA[202 Himes Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Louisiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>LA06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~177771</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has developed a series of innovative fi rst-order and/or zero-order nonlinear optimization algorithms, the associated acceleration techniques and their underlying theories. In particular, we have studied the acceleration techniques for nonlinear conjugate gradient methods. This includes choosing proper subspaces to monitor convergence and to generate a better search direction when slow convergence is detected. Based on these techniques,&nbsp; we propose a framework of accelerated nonlinear conjugate gradient methods with theoretically guaranteed optimal global complexity for minimizing both convex and nonconvex objective functions. We also propose a framework for developing inexact alternating direction method of multipliers (ADMM). In this framework, each subproblem is solved inexactly to an adaptive accuracy based on the summation of the subproblem stepsizes and an error relative to the whole problem KKT error. Hence, the overall convergence speed of ADMM will be greatly accelerated. We have also studied fi rst-order and zero-order stochastic methods for constrained (possibly nonconvex) stochastic composite optimization. Randomized stochastic projected gradient methods using mini-batch samples on stochastic gradients and stochastic function values have been developed. We have shown optimal complexities of the developed stochastic algorithms. Finally,&nbsp; we have proposed a super fast dual active set algorithm for doing projection on a polyhedral. And based on this algorithm, a dual active set algorithm, called PASA, is developed for solving nonlinear optimization with polyhedral constraints. PASA has been shown to have asymptotically superlinear convergence when proper algorithms have been implemented. <br /><br />The algorithms developed in the project will also have very broad applications in science, engineering and industry, including imaging/signal processing, optimal control, computer vision, petroleum engineering, topology optimization, electronic structure computations and&nbsp; nite element methods for scienti c computing.</p><br> <p>            Last Modified: 10/09/2019<br>      Modified by: Hongchao&nbsp;Zhang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has developed a series of innovative fi rst-order and/or zero-order nonlinear optimization algorithms, the associated acceleration techniques and their underlying theories. In particular, we have studied the acceleration techniques for nonlinear conjugate gradient methods. This includes choosing proper subspaces to monitor convergence and to generate a better search direction when slow convergence is detected. Based on these techniques,  we propose a framework of accelerated nonlinear conjugate gradient methods with theoretically guaranteed optimal global complexity for minimizing both convex and nonconvex objective functions. We also propose a framework for developing inexact alternating direction method of multipliers (ADMM). In this framework, each subproblem is solved inexactly to an adaptive accuracy based on the summation of the subproblem stepsizes and an error relative to the whole problem KKT error. Hence, the overall convergence speed of ADMM will be greatly accelerated. We have also studied fi rst-order and zero-order stochastic methods for constrained (possibly nonconvex) stochastic composite optimization. Randomized stochastic projected gradient methods using mini-batch samples on stochastic gradients and stochastic function values have been developed. We have shown optimal complexities of the developed stochastic algorithms. Finally,  we have proposed a super fast dual active set algorithm for doing projection on a polyhedral. And based on this algorithm, a dual active set algorithm, called PASA, is developed for solving nonlinear optimization with polyhedral constraints. PASA has been shown to have asymptotically superlinear convergence when proper algorithms have been implemented.   The algorithms developed in the project will also have very broad applications in science, engineering and industry, including imaging/signal processing, optimal control, computer vision, petroleum engineering, topology optimization, electronic structure computations and  nite element methods for scienti c computing.       Last Modified: 10/09/2019       Submitted by: Hongchao Zhang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
