<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Collaborative Research: Communicating While Computing: Mobile Fog Computing Over Wireless Heterogeneous Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>249848.00</AwardTotalIntnAmount>
<AwardAmount>249848</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>An increasing number of applications, including surveillance, medical monitoring, automatic translations and gaming, rely on the capability of mobile wireless devices to carry out computation-intensive tasks in a timely manner. This requirement conflicts with the expectation that mobile devices should run on a battery without needing frequent recharging. A promising solution to this challenge is mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider by means of wireless transmission. However, the energy and latency entailed by wireless transmission may offset the potential gains of mobile cloud computing. This project proposes to tackle the outlined problem via the development of effective, low-complexity, scalable and flexible offloading strategies that operate over a mobile fog computing architecture, in which small-cell base stations are endowed with computing capabilities to offer proximate wireless access and computing. The insights gained from the successful completion of this project will be beneficial for a gamut of other exciting problem domains that require large-scale optimization, including big data mining, signal processing, machine learning, and smart grid. The research agenda is complemented by a multidisciplinary educational plan that targets both undergraduate and graduate students via hands-on learning and experimentation activities. Industrial collaboration is also envisaged through internship and co-op opportunities. &lt;br/&gt;&lt;br/&gt;The inter-layer optimization of the computation and communication resources in a mobile fog computing network yields unstructured nonconvex mixed-integer problems, which are unexplored and challenging, and whose formulation depends on whether the mobile applications are splittable, i.e., divisible into subtasks that can be individually offloaded, or not. Since the problems at hand do not lend themselves to the application of existing iterative optimization techniques, such as Difference-of-Convex programming, a class of scalable and flexible solution methods with controllable convergence, complexity and overhead is introduced based on a novel successive convex approximation framework. In the case of splittable applications, the analytical and algorithmic framework is augmented by the application of message passing strategies that leverage the call graph representation of the mobile applications.</AbstractNarration>
<MinAmdLetterDate>08/11/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/11/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1525629</AwardID>
<Investigator>
<FirstName>Osvaldo</FirstName>
<LastName>Simeone</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Osvaldo Simeone</PI_FULL_NAME>
<EmailAddress>osvaldo.simeone@njit.edu</EmailAddress>
<PI_PHON>9735965275</PI_PHON>
<NSF_ID>000502056</NSF_ID>
<StartDate>08/11/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New Jersey Institute of Technology</Name>
<CityName>Newark</CityName>
<ZipCode>071021982</ZipCode>
<PhoneNumber>9735965275</PhoneNumber>
<StreetAddress>University Heights</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>075162990</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW JERSEY INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>075162990</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New Jersey Institute of Technology]]></Name>
<CityName/>
<StateCode>NJ</StateCode>
<ZipCode>071021982</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7935</Code>
<Text>COMM &amp; INFORMATION THEORY</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~249848</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>While the number of computation-intensive applications that users expect to run on mobile devices&ndash;including video processing, object recognition, gaming, automatic translation and medical monitoring&ndash;continues to grow, the devices' computing capabilities are ultimately limited by the battery lifetime. Barring breakthroughs in battery technology, the only potential solution to this challenge appears to be mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider, such as for speech recognition with Google Voice Search and Apple Siri. However, accessing the cloud through a wireless network entails the energy and latency required for uplink and downlink transmissions, as well as the delay caused by routing on the backhaul network, hence potentially offsetting the gains of mobile cloud computing.</p> <p>This project proposed to tackle the above problem by introducing the<em> mobile fog computing architecture</em>, in which the small-cell base stations of a cellular system are endowed with computing capabilities to offer <em>proximate</em> <em>wireless access and computing</em>.</p> <p>The key objective of the project was to develop effective, low-complexity, scalable and flexible offloading strategies based on the <em>inter-layer optimization of computation and communication resources</em>, with the aim of ensuring Quality of Experience (QoE) constraints in terms of minimal mobile energy expenditure and latency. The main outcomes are summarized as follows.</p> <p class="vert-spacing"><strong>Main Outcome 1.</strong> Consider a mobile cloud computing system for a scenario with multiple users transmitting over a shared wireless medium across multiple cells (see Fig. 1). The problem requires the management of interference for both the uplink, through which users offload the data needed for computation in the cloud, and for the downlink, through which the outcome of the cloud computation are fed back to the users. It also calls for the allocation of backhaul resources for communication between wireless access points and cloud, and of computing resources at the cloudlet/cloud servers. Furthermore, the optimization should include user selection mechanisms whereby offloading users are guaranteed an energy consumption that is smaller than the amount required for local computing at the device. The design of a mobile cloud computing system was proposed that accounts for multi-antenna uplink and downlink transmissions, with or without cooperation on the downlink, along with backhaul and computational resource allocation and user selection. As an example, consider Fig. 2: upon optimization, when the backhaul capacity constraint is stringent, edge of&#64258;oading is more energy ef&#64257;cient as compared to cloud of&#64258;oading, e.g., by about 44% for the parameters selected here. Furthermore, larger values of cloud capacity can compensate for limited backhaul resources, making cloud computing preferable to edge computing.</p> <p class="vert-spacing"><strong>Main Outcome 2.</strong> A novel mobile cloud computing scenario was proposed in which the "cloudlet" processor that provides offloading opportunities to mobile devices is mounted on unmanned aerial vehicles (UAVs) to enhance coverage (see Fig. 3). The joint optimization of the number of input bits transmitted in the uplink by the mobile to the UAV, the number of input bits processed by the cloudlet at the UAV, and the number of output bits returned by the cloudlet to the mobile in the downlink in each slot was carried out under maximum latency constraints. As an example, Fig. 3 compares the average total energy consumptions for mobile execution with the mobile energy needed for offloading using orthogonal and non-orthogonal access as a function of the deadline <em>T</em>. We observe that the proposed scheme requires an average total MUs&rsquo; energy consumption that is 14.5%-32.7% smaller than non-optimized systems with equal bit allocation and constant-velocity cloudlet trajectory.</p> <p><strong>Main Outcome 3.</strong> The uplink of a cloud radio access network architecture is studied in  which decoding at the cloud takes place via network function  virtualization on commercial off-the-shelf servers (see Fig. 5). In order to mitigate  the impact of straggling decoders in this platform, a novel coding  strategy is proposed, whereby the cloud re-encodes the received frames  via a linear code before distributing them to the decoding processors. As an example, consider Fig. 6. Leveraging multiple servers in parallel for decoding by simply decoding each frame at a different server is seen to yield significant gains in terms of the trade-off between latency and Frame Unavailability Probability (FUP). The FUP is the probability that at a given time all the transmitted frames are available correctly decoded at the cloud server. In particular, the parallel scheme is observed to be preferred for lower latencies. This is due to the shorter blocklength of the frame to be decoded at each server, which entails a smaller average decoding latency. However, the error floor of the parallel scheme is large due to the higher error probability for short blocklengths. In this case, the proposed forms of coding at the cloud are seen to be beneficial. Coding-based strategies can be designed based on the analytical bounds derived in the project, which describe the trade-off between decoding latency and reliability.</p><br> <p>            Last Modified: 09/06/2019<br>      Modified by: Osvaldo&nbsp;Simeone</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786255657_NSF1--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786255657_NSF1--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786255657_NSF1--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Cloud and edge mobile computing in multi-cell cellular systems with limited backhaul (Outcome 1).</div> <div class="imageCredit">IEEE</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Osvaldo&nbsp;Simeone</div> <div class="imageTitle">Figure 1</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786302689_NSF2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786302689_NSF2--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786302689_NSF2--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Mobile sum-energy versus the backhaul capacity for cloud, edge and hybrid offloading strategies (Outcome 1).</div> <div class="imageCredit">IEEE</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Osvaldo&nbsp;Simeone</div> <div class="imageTitle">Figure 2</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786797500_NSF3--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786797500_NSF3--rgov-800width.jpg" title="Figure 3"><img src="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786797500_NSF3--rgov-66x44.jpg" alt="Figure 3"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Cloud mobile computing with an UAV-mounted cloudlet (Outcome 2).</div> <div class="imageCredit">IEEE</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Osvaldo&nbsp;Simeone</div> <div class="imageTitle">Figure 3</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786862811_NSF4--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786862811_NSF4--rgov-800width.jpg" title="Figure 4"><img src="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567786862811_NSF4--rgov-66x44.jpg" alt="Figure 4"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Average total energy consumptions for mobile execution and offloading using orthogonal and non-orthogonal access as a function of the deadline T (Outcome 2).</div> <div class="imageCredit">IEEE</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Osvaldo&nbsp;Simeone</div> <div class="imageTitle">Figure 4</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567787275953_NSF5--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567787275953_NSF5--rgov-800width.jpg" title="Figure 5"><img src="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567787275953_NSF5--rgov-66x44.jpg" alt="Figure 5"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Cloud-based decoding: the packets are received by the Remote Radio Head (RRH) and forwarded to the cloud. Server 0 in the cloud re-encodes the received packet with a linear code in order to enhance the robustness against potentially straggling Servers 1,..,N (Outcome 3).</div> <div class="imageCredit">IEEE</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Osvaldo&nbsp;Simeone</div> <div class="imageTitle">Figure 5</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567787317048_NSF6--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567787317048_NSF6--rgov-800width.jpg" title="Figure 6"><img src="/por/images/Reports/POR/2019/1525629/1525629_10386388_1567787317048_NSF6--rgov-66x44.jpg" alt="Figure 6"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Decoding latency versus Frame Unavailability Probability  (Outcome 3).</div> <div class="imageCredit">IEEE</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Osvaldo&nbsp;Simeone</div> <div class="imageTitle">Figure 6</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ While the number of computation-intensive applications that users expect to run on mobile devices&ndash;including video processing, object recognition, gaming, automatic translation and medical monitoring&ndash;continues to grow, the devices' computing capabilities are ultimately limited by the battery lifetime. Barring breakthroughs in battery technology, the only potential solution to this challenge appears to be mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider, such as for speech recognition with Google Voice Search and Apple Siri. However, accessing the cloud through a wireless network entails the energy and latency required for uplink and downlink transmissions, as well as the delay caused by routing on the backhaul network, hence potentially offsetting the gains of mobile cloud computing.  This project proposed to tackle the above problem by introducing the mobile fog computing architecture, in which the small-cell base stations of a cellular system are endowed with computing capabilities to offer proximate wireless access and computing.  The key objective of the project was to develop effective, low-complexity, scalable and flexible offloading strategies based on the inter-layer optimization of computation and communication resources, with the aim of ensuring Quality of Experience (QoE) constraints in terms of minimal mobile energy expenditure and latency. The main outcomes are summarized as follows. Main Outcome 1. Consider a mobile cloud computing system for a scenario with multiple users transmitting over a shared wireless medium across multiple cells (see Fig. 1). The problem requires the management of interference for both the uplink, through which users offload the data needed for computation in the cloud, and for the downlink, through which the outcome of the cloud computation are fed back to the users. It also calls for the allocation of backhaul resources for communication between wireless access points and cloud, and of computing resources at the cloudlet/cloud servers. Furthermore, the optimization should include user selection mechanisms whereby offloading users are guaranteed an energy consumption that is smaller than the amount required for local computing at the device. The design of a mobile cloud computing system was proposed that accounts for multi-antenna uplink and downlink transmissions, with or without cooperation on the downlink, along with backhaul and computational resource allocation and user selection. As an example, consider Fig. 2: upon optimization, when the backhaul capacity constraint is stringent, edge of&#64258;oading is more energy ef&#64257;cient as compared to cloud of&#64258;oading, e.g., by about 44% for the parameters selected here. Furthermore, larger values of cloud capacity can compensate for limited backhaul resources, making cloud computing preferable to edge computing. Main Outcome 2. A novel mobile cloud computing scenario was proposed in which the "cloudlet" processor that provides offloading opportunities to mobile devices is mounted on unmanned aerial vehicles (UAVs) to enhance coverage (see Fig. 3). The joint optimization of the number of input bits transmitted in the uplink by the mobile to the UAV, the number of input bits processed by the cloudlet at the UAV, and the number of output bits returned by the cloudlet to the mobile in the downlink in each slot was carried out under maximum latency constraints. As an example, Fig. 3 compares the average total energy consumptions for mobile execution with the mobile energy needed for offloading using orthogonal and non-orthogonal access as a function of the deadline T. We observe that the proposed scheme requires an average total MUs? energy consumption that is 14.5%-32.7% smaller than non-optimized systems with equal bit allocation and constant-velocity cloudlet trajectory.  Main Outcome 3. The uplink of a cloud radio access network architecture is studied in  which decoding at the cloud takes place via network function  virtualization on commercial off-the-shelf servers (see Fig. 5). In order to mitigate  the impact of straggling decoders in this platform, a novel coding  strategy is proposed, whereby the cloud re-encodes the received frames  via a linear code before distributing them to the decoding processors. As an example, consider Fig. 6. Leveraging multiple servers in parallel for decoding by simply decoding each frame at a different server is seen to yield significant gains in terms of the trade-off between latency and Frame Unavailability Probability (FUP). The FUP is the probability that at a given time all the transmitted frames are available correctly decoded at the cloud server. In particular, the parallel scheme is observed to be preferred for lower latencies. This is due to the shorter blocklength of the frame to be decoded at each server, which entails a smaller average decoding latency. However, the error floor of the parallel scheme is large due to the higher error probability for short blocklengths. In this case, the proposed forms of coding at the cloud are seen to be beneficial. Coding-based strategies can be designed based on the analytical bounds derived in the project, which describe the trade-off between decoding latency and reliability.       Last Modified: 09/06/2019       Submitted by: Osvaldo Simeone]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
