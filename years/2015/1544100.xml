<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  The Role of Instructor and Peer Feedback in Improving the Cognitive, Interpersonal, and Intrapersonal Competencies of Student Writers in STEM Courses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>107715.00</AwardTotalIntnAmount>
<AwardAmount>107715</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Connie Della-Piana</SignBlockName>
<PO_EMAI>cdellapi@nsf.gov</PO_EMAI>
<PO_PHON>7032925309</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field.&lt;br/&gt;&lt;br/&gt;This project will have critical significance for Science, Technology, Engineering, and Mathematics (STEM) educators by increasing writing and collaboration skills in students, areas of importance to economics, science, and national security. This study focuses on teacher and peer interactions and writing quality and improvement in the context of undergraduate STEM courses. Specifically, the project will map the development of three competency domains (cognitive, interpersonal and intrapersonal) by researching the effects of teacher and peer response on writing improvement and knowledge adaptation in STEM courses. The project utilizes a web-based assessment tool called My Reviewers (MyR). The tool will be piloted by STEM faculty in college-level Introductory Biology or Chemistry on the campuses of University of South Florida (USF), North Carolina State University (NCSU), Dartmouth, Massachusetts Institute of Technology (MIT), and University of Pennsylvania (UPenn). Research domains include both academic performance and inter/intra-personal competencies. Project deliverables will provide new tools and procedures to assist in the assessment of students' knowledge, skills, and attitudes for project and program evaluation.&lt;br/&gt;&lt;br/&gt;Approximately 10,000 students enrolled in STEM courses at USF, NCSU, Dartmouth, MIT, and UPenn will upload their course-based writing to My Reviewers, an assessment tool, and use the tool to conduct peer reviews and team projects.  This information is supplemented by surveys of demographics and dispositions along with click patterns within the toolset. Researchers will subsequently analyze this wealth of data using predictive modeling of student writing ability and improvement, including text-based methods to identify useful features of comments, papers, peer reviews, student evaluations of other peers? reviews, and instructor and student meta-reflections. Outcome goals are to (1) demonstrate ways the assessment community can use real-time assessment tools to create valid measures of writing development; (2) provide quantitative evidence regarding the likely effects of particular commenting and scoring patterns on cohorts of students; (3) offer a domain map to help STEM educators better understand student success in the STEM curriculum; and (4) inform STEM faculty regarding the efficacy of peer review.</AbstractNarration>
<MinAmdLetterDate>09/16/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/16/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1544100</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Anson</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher M Anson</PI_FULL_NAME>
<EmailAddress>chris_anson@ncsu.edu</EmailAddress>
<PI_PHON>9195132577</PI_PHON>
<NSF_ID>000111644</NSF_ID>
<StartDate>09/16/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>North Carolina State University</Name>
<CityName>Raleigh</CityName>
<ZipCode>276957514</ZipCode>
<PhoneNumber>9195152444</PhoneNumber>
<StreetAddress>2601 Wolf Village Way</StreetAddress>
<StreetAddress2><![CDATA[Admin. III, STE 240]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042092122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTH CAROLINA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[North Carolina State University]]></Name>
<CityName>Raleigh</CityName>
<StateCode>NC</StateCode>
<ZipCode>276958105</ZipCode>
<StreetAddress><![CDATA[Tompkins Hall 131G, Box 8105]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>009Z</Code>
<Text>PRIME - Promoting Research and Innovatio</Text>
</ProgramReference>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~107715</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary purpose of this grant was to study student peer-review practices in STEM disciplines using a digital peer-review system called MyReviewers (MyR), housed at the University of South Florida. The suite of tools in MyR is designed to assist students and instructors in the provision of response to writing in progress, thus facilitating students' written competencies, critical thinking, and collaborative skills.</p> <p>Co-PIs at five institutions (the University of South Florida, Dartmouth College, the Massachussetts Institute of Technology, the University of Pennsylvania, and North Carolina State University) recruited faculty in STEM courses to participate by using MyR in the context of their writing assignments. Although common goals were established for all the institutions, it proved impossible to systematize the data collection and analysis process across the diverse curriculums; as a result, each institution established specific foci, based on the general goals of the project.</p> <p>In addition to the process of implementation and teacher development?itself one of the goals of the project--the focus at NC State University sharpened to the question of whether fundamental writing concepts learned in required first-year composition courses would ?transfer? effectively into the writing students do in STEM disciplines. This question is important for determining the extent to which instructors in STEM courses can rely on what is learned in foundational courses or must continue to focus explicitly on writing development in their own courses, which would have implications for faculty development and curricular redesign. Because large corpora of peer-review data from first-year writing courses (approximately 50,000 peer reviews) were available to us, we could examine those for the presence of key terms and compare the results to a similar examination of the terms used in the STEM peer reviews.</p> <p>To research this question, a corpus of key writing terms was established through a non-probability survey of approximately 500 expert composition instructors and researchers. Respondents were asked to provide up to ten terms associated with high-quality response to writing in progress, as well as ten terms they would associate with novice (student) response in peer-review settings. The results were statistically analyzed to create two test corpora.</p> <p>The survey results where then applied to corpora of student peer reviews in first-year composition courses and STEM courses to examine whether the concepts associated with high-quality response were present in students? peer reviews, and if so, whether they transferred from foundational courses to STEM courses. Also examined were the presence of high-quality terms in the responses of instructors of first-year writing courses, which amounted to approximately the same numbers as those of the peer reviews.</p> <p>The following are the central conclusions of this project:</p> <ul> <li>Survey results show significant differences in the terms instructors associate with high-quality response to writing and those they associate with student response. High-quality terms reflect fundamental understandings of writing, response, and revision (including terms considered to be threshold concepts within the field), including rhetorical terms such as "audience," process-related terms such as "revision," and socially-determined textual terms such as "genre."</li> <li>Instructors in foundational writing courses use a significant number of high-quality terms (reflecting important rhetorical and writing-related principles) in their response.</li> <li>Students in foundational writing courses do not use high-quality terms to the same degree as instructors, but the results show that they are beginning to adopt high-quality terms rather than primarily using terms survey respondents associate with novice response.</li> <li>Students in STEM courses appear to transfer some high-quality writing-related concepts from foundational courses to STEM courses, but unevenly?possibly due to the specific adaptations of the concepts to the STEM environment and possibly due to students' partial learning, or forgetting, of concepts from first-year composition courses.</li> <li>In comparisons of courses in which instructors provided specific, guiding questions as part of a peer review rubric (that is, built into the MyR peer review digital system), students? peer reviews exhibited stronger adoption of high-quality concepts than courses in which instructors did not intervene in the same way.</li> <li>Digital peer review systems such as MyR do appear to facilitate students? transfer of key concepts, as measured lexically, from foundational to STEM courses when appropriate instructor interventions are made in the peer-review system.</li> <li>Further lexical analysis using topic comparisons revealed four tendencies among STEM peer reviewers, two of them reflecting weaker use of writing concepts and two reflecting stronger use. These differences need to be further researched against writing quality indices to determine whether more specific interventions are needed in STEM contexts.</li> </ul> <p>&nbsp;</p><br> <p>            Last Modified: 11/04/2018<br>      Modified by: Christopher&nbsp;M&nbsp;Anson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary purpose of this grant was to study student peer-review practices in STEM disciplines using a digital peer-review system called MyReviewers (MyR), housed at the University of South Florida. The suite of tools in MyR is designed to assist students and instructors in the provision of response to writing in progress, thus facilitating students' written competencies, critical thinking, and collaborative skills.  Co-PIs at five institutions (the University of South Florida, Dartmouth College, the Massachussetts Institute of Technology, the University of Pennsylvania, and North Carolina State University) recruited faculty in STEM courses to participate by using MyR in the context of their writing assignments. Although common goals were established for all the institutions, it proved impossible to systematize the data collection and analysis process across the diverse curriculums; as a result, each institution established specific foci, based on the general goals of the project.  In addition to the process of implementation and teacher development?itself one of the goals of the project--the focus at NC State University sharpened to the question of whether fundamental writing concepts learned in required first-year composition courses would ?transfer? effectively into the writing students do in STEM disciplines. This question is important for determining the extent to which instructors in STEM courses can rely on what is learned in foundational courses or must continue to focus explicitly on writing development in their own courses, which would have implications for faculty development and curricular redesign. Because large corpora of peer-review data from first-year writing courses (approximately 50,000 peer reviews) were available to us, we could examine those for the presence of key terms and compare the results to a similar examination of the terms used in the STEM peer reviews.  To research this question, a corpus of key writing terms was established through a non-probability survey of approximately 500 expert composition instructors and researchers. Respondents were asked to provide up to ten terms associated with high-quality response to writing in progress, as well as ten terms they would associate with novice (student) response in peer-review settings. The results were statistically analyzed to create two test corpora.  The survey results where then applied to corpora of student peer reviews in first-year composition courses and STEM courses to examine whether the concepts associated with high-quality response were present in students? peer reviews, and if so, whether they transferred from foundational courses to STEM courses. Also examined were the presence of high-quality terms in the responses of instructors of first-year writing courses, which amounted to approximately the same numbers as those of the peer reviews.  The following are the central conclusions of this project:  Survey results show significant differences in the terms instructors associate with high-quality response to writing and those they associate with student response. High-quality terms reflect fundamental understandings of writing, response, and revision (including terms considered to be threshold concepts within the field), including rhetorical terms such as "audience," process-related terms such as "revision," and socially-determined textual terms such as "genre." Instructors in foundational writing courses use a significant number of high-quality terms (reflecting important rhetorical and writing-related principles) in their response. Students in foundational writing courses do not use high-quality terms to the same degree as instructors, but the results show that they are beginning to adopt high-quality terms rather than primarily using terms survey respondents associate with novice response. Students in STEM courses appear to transfer some high-quality writing-related concepts from foundational courses to STEM courses, but unevenly?possibly due to the specific adaptations of the concepts to the STEM environment and possibly due to students' partial learning, or forgetting, of concepts from first-year composition courses. In comparisons of courses in which instructors provided specific, guiding questions as part of a peer review rubric (that is, built into the MyR peer review digital system), students? peer reviews exhibited stronger adoption of high-quality concepts than courses in which instructors did not intervene in the same way. Digital peer review systems such as MyR do appear to facilitate students? transfer of key concepts, as measured lexically, from foundational to STEM courses when appropriate instructor interventions are made in the peer-review system. Further lexical analysis using topic comparisons revealed four tendencies among STEM peer reviewers, two of them reflecting weaker use of writing concepts and two reflecting stronger use. These differences need to be further researched against writing quality indices to determine whether more specific interventions are needed in STEM contexts.           Last Modified: 11/04/2018       Submitted by: Christopher M Anson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
