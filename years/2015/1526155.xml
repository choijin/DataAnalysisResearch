<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Advancing the Human Work of Data Analytics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Data analytics, the automatic discovery of patterns in large datasets, is an integral part of contemporary digital practice. Owing to their large scale, broad scope, and unprecedented granularity, such data are manually intractable and, thus, data mining algorithms do work that humans cannot. Still, data analytics necessitates human labor to make it work, for example deciding what data to collect, pre-processing the data to make them algorithm-ready, and making sense of the results. This research addresses related technical and societal challenges by identifying, tracking, and analyzing the multiple forms of human labor involved in the practice of data analytics, and by using this analysis to develop new methods for data analytics research and training. By articulating work practices that previously have been taught largely through apprenticeship, this work expands the reach of data analytics beyond those with direct connections to existing researchers. It increases transparency and accountability of data analysis by making clear how data analysis results are developed and by developing techniques to better communicate results. It supports a better fit between data analysis and domain contexts and demonstrates good practices for integrating social and technical research. The key question this project will answer is how people and machines can work together more effectively to make sense of large-scale data.&lt;br/&gt;&lt;br/&gt;Through a collaboration between sociologists of technology and data scientists, this research will identify and address invisible labor at three stages in the analytics process: (1) Conceptualization: How is a problem conceptualized and translated into a machine-solvable data analytic problem? (2) Pre-processing: How are data collected, cleaned, and made algorithm-ready? (3) Post-processing: How are the results of data analysis contextualized, represented, and made sense of, both individually and publically? This research answers these questions by analyzing the uptake of data analytics in the digital humanities. This is a useful site for surfacing questions of human labor because data analytics is a powerful potential tool for the humanities, but does not map directly onto traditional research methods in this field. Thus, mapping problems onto data analytics and translating the results of data analytics into meaningful arguments for the target domain requires more explicit articulation than is the case in more "data-native" disciplines. This research develops implications for the practice of data analysis in 4 areas: (1) designing new curricula for training in data analysis; (2) developing software systems and research methods that better address and support human labor; (3) exploring new ways to make the process of data analysis transparent, accountable, and communicable; and (4) creating a nuanced sociological understanding of the practice of data analysis.</AbstractNarration>
<MinAmdLetterDate>08/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1526155</AwardID>
<Investigator>
<FirstName>Phoebe</FirstName>
<LastName>Sengers</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Phoebe Sengers</PI_FULL_NAME>
<EmailAddress>sengers@cs.cornell.edu</EmailAddress>
<PI_PHON>6072556554</PI_PHON>
<NSF_ID>000237223</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Mimno</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Mimno</PI_FULL_NAME>
<EmailAddress>dm655@cornell.edu</EmailAddress>
<PI_PHON>6072555014</PI_PHON>
<NSF_ID>000667456</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148537501</ZipCode>
<StreetAddress><![CDATA[Gates Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~156061</FUND_OBLG>
<FUND_OBLG>2016~185318</FUND_OBLG>
<FUND_OBLG>2017~158621</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Data science is now a central feature of our everyday and professional lives. While often understood as a purely technical practice, data scientists are aware that data science also requires a great deal of human work&mdash;work that often remains unaccounted for in data science learning and research, and which is therefore pursued without empirically validated guidelines. This project analyzed the human work involved in academic and corporate data science practices. It used this analysis to identify the many forms of human and organizational work involved in data science practices and to theorize the implications of such forms of work for the wider social and ethical implications of data science systems. The project then used these insights to shift development work on machine learning tools towards parts of the data science workflow that have been understudied or even ignored in standard research.</p> <p><strong>Intellectual Merit: </strong>Through examination of data science learning and professionalization practices, the project made visible the necessary role of human work in data science. It made revealed the impact of empirical contingencies, prior knowledge, and practical issues on data science results. Through examination of corporate data science practices, the project made visible the heterogeneous nature of applied data science work. Data science systems are as much designed by business personnel, project managers, and product managers, as they are by data scientists. These varied stakeholders work together in practice to envision, build, and test data science results and systems. These findings, in turn, helped to shed light on human and organizational dimensions of data science work. For example, it revealed the normative implications of the practical work of problem formulation &ndash; that is, how translating high-level goals into tractable computational problems leads to specific forms of bias and fairness in data science systems. It also revealed the collaborative practices through which data science practitioners negotiate and establish trust in data, models, and numbers, finding that trust in data science is as much a collaborative as a technical accomplishment.</p> <p>Observing how data science is actually practiced in both academic and industrial settings brought attention to gaps and blind spots in standard machine learning protocols. Research typically focuses on core algorithms, evaluated on standard problems and standard test metrics. In practice, the project found that most of the work of applied data science is in mapping academic or business needs to specific machine learning tasks and curating data sets to meet the needs of algorithms, all with little background experience. Tacit assumptions about the meaning of key terms (e.g., &ldquo;document&rdquo;) that were clear to machine learning researchers were subject to substantial confusion and misinterpretation by practitioners. To address this, the project developed a number of new techniques for common text-data curation methods, focusing on transformations that are commonly applied but rarely systematically studied, such as stemming (i.e., treating words with the same base such as &ldquo;walking,&rdquo; &ldquo;walks&rdquo;, and &ldquo;walked&rdquo; as though they are the same word). In one particular case, the project showed that data curation methods can effectively eliminate the need for complicated models, allowing users to more consciously shape the results of standard algorithms through more transparent, observable text transformations.</p> <p><strong>Broader Impact: </strong>In the academic context, the project&rsquo;s findings led to specific teaching and training aids for undergraduate and graduate researchers and teachers. These aids help students to understand how computation is used in practice and to re-orient their focus to consider the context of real-world applied data science work. In the corporate context, the project&rsquo;s findings informed organizational and technical practices in corporate data science organizations. This enabled new ways to document forms of human work (e.g., making discretionary choices and decisions a required part of project code documentation). It also created greater visibility for the work of translating business goals into computational problems. This led, for example, to requiring a specific chunk of project time to be dedicated to discussing the pros and cons of choosing one problem formulation over another. These findings have also resulted in case studies and general recommendations for machine learning researchers.</p><br> <p>            Last Modified: 02/14/2020<br>      Modified by: Phoebe&nbsp;Sengers</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Data science is now a central feature of our everyday and professional lives. While often understood as a purely technical practice, data scientists are aware that data science also requires a great deal of human work&mdash;work that often remains unaccounted for in data science learning and research, and which is therefore pursued without empirically validated guidelines. This project analyzed the human work involved in academic and corporate data science practices. It used this analysis to identify the many forms of human and organizational work involved in data science practices and to theorize the implications of such forms of work for the wider social and ethical implications of data science systems. The project then used these insights to shift development work on machine learning tools towards parts of the data science workflow that have been understudied or even ignored in standard research.  Intellectual Merit: Through examination of data science learning and professionalization practices, the project made visible the necessary role of human work in data science. It made revealed the impact of empirical contingencies, prior knowledge, and practical issues on data science results. Through examination of corporate data science practices, the project made visible the heterogeneous nature of applied data science work. Data science systems are as much designed by business personnel, project managers, and product managers, as they are by data scientists. These varied stakeholders work together in practice to envision, build, and test data science results and systems. These findings, in turn, helped to shed light on human and organizational dimensions of data science work. For example, it revealed the normative implications of the practical work of problem formulation &ndash; that is, how translating high-level goals into tractable computational problems leads to specific forms of bias and fairness in data science systems. It also revealed the collaborative practices through which data science practitioners negotiate and establish trust in data, models, and numbers, finding that trust in data science is as much a collaborative as a technical accomplishment.  Observing how data science is actually practiced in both academic and industrial settings brought attention to gaps and blind spots in standard machine learning protocols. Research typically focuses on core algorithms, evaluated on standard problems and standard test metrics. In practice, the project found that most of the work of applied data science is in mapping academic or business needs to specific machine learning tasks and curating data sets to meet the needs of algorithms, all with little background experience. Tacit assumptions about the meaning of key terms (e.g., "document") that were clear to machine learning researchers were subject to substantial confusion and misinterpretation by practitioners. To address this, the project developed a number of new techniques for common text-data curation methods, focusing on transformations that are commonly applied but rarely systematically studied, such as stemming (i.e., treating words with the same base such as "walking," "walks", and "walked" as though they are the same word). In one particular case, the project showed that data curation methods can effectively eliminate the need for complicated models, allowing users to more consciously shape the results of standard algorithms through more transparent, observable text transformations.  Broader Impact: In the academic context, the project’s findings led to specific teaching and training aids for undergraduate and graduate researchers and teachers. These aids help students to understand how computation is used in practice and to re-orient their focus to consider the context of real-world applied data science work. In the corporate context, the project’s findings informed organizational and technical practices in corporate data science organizations. This enabled new ways to document forms of human work (e.g., making discretionary choices and decisions a required part of project code documentation). It also created greater visibility for the work of translating business goals into computational problems. This led, for example, to requiring a specific chunk of project time to be dedicated to discussing the pros and cons of choosing one problem formulation over another. These findings have also resulted in case studies and general recommendations for machine learning researchers.       Last Modified: 02/14/2020       Submitted by: Phoebe Sengers]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
