<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER-NEON: Image-Based Ecological Information System (IBEIS) for Animal Sighting Data for NEON</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>105999.00</AwardTotalIntnAmount>
<AwardAmount>105999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>08040000</Code>
<Directorate>
<Abbreviation>BIO</Abbreviation>
<LongName>Direct For Biological Sciences</LongName>
</Directorate>
<Division>
<Abbreviation>EF</Abbreviation>
<LongName>Emerging Frontiers</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michelle Elekonich</SignBlockName>
<PO_EMAI>melekoni@nsf.gov</PO_EMAI>
<PO_PHON>7032927202</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The National Ecological Observatory Network (NEON) is coming online and will provide atmospheric and ecological data locally, regionally and continent wide. At the same time, images are rapidly becoming the most abundant, widely available, and cheapest source of information about the natural world, especially about animals. This project will extend NEON's data, scientific, and citizen science capacity with image-based animal sighting data to scalably collect, manage, and analyze data for individually identifiable wildlife using the Image-Based Ecological Information System (IBEIS) prototype recently developed under another NSF award. Combined with other ecological data, the image data offer the promise of addressing big questions about animal ecology, behavior, and conservation - who? where? when? what? and why? - at high resolution and at fine-grained scale, across landscapes and ecosystems, from an individual animal to regional and global systems. As part of this project, undergraduate and graduate students from ecology and computer science at four institutions will produce and test the application interface, and will develop a suite of companion applications and training tools to allow greater involvement of citizen scientists.&lt;br/&gt;&lt;br/&gt;These tools will allow NEON to connect its database to data derived from large volumes of animal photographic images. Although this is primarily a proof of concept proposal focused on connecting whale shark images to NEONs atmospheric data, it will provide the means to be able to apply IBEIS algorithms and databases on images of distinctly marked North American species such as tortoises, monarch butterflies, salamanders, spotted skunk, bobcat, lynx, and humpback whales, thereby connecting these to NEON?s other data streams related to organisms, land use, hydrology and biogeochemistry. The proposed suite of tools includes: 1. an infrastructure and a mechanism for collecting images from scientists, automated remote cameras, citizen scientists and other sources; 2. a data management system for storing, accessing and manipulating images and derived data; 3. computer vision techniques for extracting information from the images about the identity of species and individual animals, as well as techniques for combining that information with other relevant data to derive information about ecological units such as animals, populations, species, and habitats; 4. a software application-program interface integrating the image and derived data with and within NEON; 5. a framework for engaging citizen scientists in data collection, derived science, and interaction with nature. Previous funding from NSF allowed building and testing of an IBEIS prototype.  This project will focus on the detection and identification methods for the identifiable US species, on integrating the system with NEON, and on scaling the system to many thousands of daily images from a variety of sources.</AbstractNarration>
<MinAmdLetterDate>08/07/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.074</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1550880</AwardID>
<Investigator>
<FirstName>Charles</FirstName>
<LastName>Stewart</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Charles V Stewart</PI_FULL_NAME>
<EmailAddress>stewart@cs.rpi.edu</EmailAddress>
<PI_PHON>5182766731</PI_PHON>
<NSF_ID>000307751</NSF_ID>
<StartDate>08/07/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rensselaer Polytechnic Institute</Name>
<CityName>Troy</CityName>
<ZipCode>121803522</ZipCode>
<PhoneNumber>5182766000</PhoneNumber>
<StreetAddress>110 8TH ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002430742</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RENSSELAER POLYTECHNIC INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002430742</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rensselaer Polytechnic Institute]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>121803522</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7959</Code>
<Text>MacroSysBIO &amp; NEON-Enabled Sci</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7959</Code>
<Text>MACROSYSTEM BIOLOGY</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~105999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-46141426-d0b9-6340-c462-7177c43aff81"> </span></p> <p><span id="docs-internal-guid-0e219ac6-d0bb-1796-b103-e01109d58d5d"> </span></p> <p dir="ltr"><span>According to a July 2017 study in the Proceedings of the National Academy of Sciences, a &ldquo;sixth mass extinction&rdquo; is underway, a trend signalled by widespread vertebrate losses that &ldquo;will have negative cascading consequences on ecosystem functioning and services vital to sustaining civilization.&rdquo; &nbsp;This study represents a growing awareness in the wildlife research community that more rapid assessment, response, and review are needed to understand and counter this decline. Unfortunately, t</span><span>he collection and management of wildlife data remains a largely ad hoc and academic exercise focused on moving small data sets into local, custom population studies for &ldquo;one-off&rdquo; analyses without long-term data curation or collaboration across borders and regions. Arriving at a critical mass of data for population analysis can take years (especially for rare or endangered species). Long required observation periods and manual data processing (e.g., matching photos &ldquo;by eye&rdquo;) can create multi-year lags between study initialization and scientific results, as well as create conclusions too coarse or slow for effective and optimizable conservation action. This limits the scope, scale, repeatability, continuity, and return-on-investment of the studies as they face the limits of their home-grown tools and IT capabilities. </span><span>Wildlife researchers lack a common yet customizable platform for collaboration and often don&rsquo;t have the technical experience to take advantage of advanced computing tools from the fields computer vision, machine learning and artificial intelligence.</span></p> <p dir="ltr"><span>This NSF project took an important step toward solving this problem by continuing the development of the prototype of the Wildbook system that produces animal population analyses based on the ability to automatically identify individual animals and follow them across many images. &nbsp;These images, taken at different times and locations, may be </span><span>collected by scientists, field technicians, and the general public, or they may be </span><span>gathered through social media. Wildbook is actually a combination of two projects, including the original Wildbook and the Image-Based Ecological Information System (IBEIS), developed in part using previous NSF funding. &nbsp;Wildbook is the name that will be used in the future.</span></p> <p dir="ltr"><span>Funding from NSF through NEON allowed the team at Rensselaer Polytechnic Institute, working with collaborators at Princeton, the University of Illinois-Chicago, and the non-profit WildMe, to make several advances towards completing the prototype Wildbook system and towards beginning to integrate it with NEON. &nbsp;First, we developed methods for running the Wildbook computer vision and machine learning algorithms and accessing the associated image data through a web site, making it easier for scientists to work in collaboration to contribute and analyze data from many different sites around the country.  Second, we completed the development of automatic tools based on machine learning algorithms based on neural networks to find animals in images, and to determine the species of each animal. &nbsp;Third, we created a new computer algorithm that automatically identifies individual dolphins based on pictures of their dorsal fins and identifies individual humpback whales based on pictures of their flukes. This method will soon be able to identify African elephants from pictures of their ears.</span></p> <p dir="ltr"><span>Finally, with some additional funding from the World Wildlife Fund, we have now created Wildbooks for sea turtles, manta rays, and Iberian lynx. &nbsp;Each Wildbook includes the ability for scientists and conservation managers to upload pictures, to run the algorithms to find and identify the animals in the pictures, and to analyze the results to learn about population size, population health, social structures, and the home ranges of individual animals.</span></p> <p dir="ltr"><span>The demonstrated success of the prototype Wildbook system offers great hope for the development of a future Wildbook product that can be used by scientists to analyze the behavior of animals over time and space, at scales ranging from individuals, to groups, to herds, and even to entire populations. &nbsp;The resulting data will allow conservation managers to make timely, data-driven plans and decisions in their efforts to combat the effects of the sixth great mass extinction.</span></p> <p>&nbsp;</p> <p dir="ltr">&nbsp;</p><br> <p>            Last Modified: 06/05/2018<br>      Modified by: Charles&nbsp;V&nbsp;Stewart</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      According to a July 2017 study in the Proceedings of the National Academy of Sciences, a "sixth mass extinction" is underway, a trend signalled by widespread vertebrate losses that "will have negative cascading consequences on ecosystem functioning and services vital to sustaining civilization."  This study represents a growing awareness in the wildlife research community that more rapid assessment, response, and review are needed to understand and counter this decline. Unfortunately, the collection and management of wildlife data remains a largely ad hoc and academic exercise focused on moving small data sets into local, custom population studies for "one-off" analyses without long-term data curation or collaboration across borders and regions. Arriving at a critical mass of data for population analysis can take years (especially for rare or endangered species). Long required observation periods and manual data processing (e.g., matching photos "by eye") can create multi-year lags between study initialization and scientific results, as well as create conclusions too coarse or slow for effective and optimizable conservation action. This limits the scope, scale, repeatability, continuity, and return-on-investment of the studies as they face the limits of their home-grown tools and IT capabilities. Wildlife researchers lack a common yet customizable platform for collaboration and often don?t have the technical experience to take advantage of advanced computing tools from the fields computer vision, machine learning and artificial intelligence. This NSF project took an important step toward solving this problem by continuing the development of the prototype of the Wildbook system that produces animal population analyses based on the ability to automatically identify individual animals and follow them across many images.  These images, taken at different times and locations, may be collected by scientists, field technicians, and the general public, or they may be gathered through social media. Wildbook is actually a combination of two projects, including the original Wildbook and the Image-Based Ecological Information System (IBEIS), developed in part using previous NSF funding.  Wildbook is the name that will be used in the future. Funding from NSF through NEON allowed the team at Rensselaer Polytechnic Institute, working with collaborators at Princeton, the University of Illinois-Chicago, and the non-profit WildMe, to make several advances towards completing the prototype Wildbook system and towards beginning to integrate it with NEON.  First, we developed methods for running the Wildbook computer vision and machine learning algorithms and accessing the associated image data through a web site, making it easier for scientists to work in collaboration to contribute and analyze data from many different sites around the country.  Second, we completed the development of automatic tools based on machine learning algorithms based on neural networks to find animals in images, and to determine the species of each animal.  Third, we created a new computer algorithm that automatically identifies individual dolphins based on pictures of their dorsal fins and identifies individual humpback whales based on pictures of their flukes. This method will soon be able to identify African elephants from pictures of their ears. Finally, with some additional funding from the World Wildlife Fund, we have now created Wildbooks for sea turtles, manta rays, and Iberian lynx.  Each Wildbook includes the ability for scientists and conservation managers to upload pictures, to run the algorithms to find and identify the animals in the pictures, and to analyze the results to learn about population size, population health, social structures, and the home ranges of individual animals. The demonstrated success of the prototype Wildbook system offers great hope for the development of a future Wildbook product that can be used by scientists to analyze the behavior of animals over time and space, at scales ranging from individuals, to groups, to herds, and even to entire populations.  The resulting data will allow conservation managers to make timely, data-driven plans and decisions in their efforts to combat the effects of the sixth great mass extinction.            Last Modified: 06/05/2018       Submitted by: Charles V Stewart]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
