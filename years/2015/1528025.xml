<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Deriving and Exploiting Shape Semantics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>3D representations are the most faithful digital encoding of physical objects, allowing us to store and manipulate all kinds of information about the object, both high-level (e.g., affordances and functionality) and low-level (e.g., appearance and material).   Furthermore, they do so in a way that is more complete than 2D images or entirely symbolic representations such as text-based knowledge graphs.   Yet associating semantic information with 3D models is not easy, because data that directly links 3D models to their function and use, their semantic parts and attributes, is not widely available.  Given the emergence of 3D shape repositories on the Web supporting a variety of applications (such as 3D printing), plus the availability of affordable 3D scanners and their incorporation in computers and mobile devices, the time is ripe to enrich these repositories with semantic information that will vastly increase their accessibility and usefulness beyond the specialized applications for which they were originally developed.  The PI's goal in this research is to do just that, by developing mathematical and algorithmic techniques which extract, encode, and exploit the semantics of 3D models; these in turn will be combined in a novel search engine for 3D models that will exploit semantic shape attributes in a unified way and will allow much broader access to and use of 3D repositories, thereby ultimately supporting many commercial applications while also proving useful to other research communities.&lt;br/&gt;&lt;br/&gt;Toward these ends, the PI will acquire semantic information about shapes by a combination of geometric analysis and user annotation.  The geometric analysis is not only of shapes in isolation, but instead a joint analysis of collections of related shapes.   The aim is to build networks among 3D models that can transfer information between them.  Through these networks, using novel mathematical techniques, the PI will enable understanding of the shared structure as well as the variability of shapes in a collection.   Common parts or structures in shapes invariably have semantic significance, and the role of a shape in a collection (its relationships to its partner and peer shapes) often defines semantic attributes of the shape.   Since user annotations are expensive to obtain, the plan is to develop tools that exploit the above-mentioned networks so that only a modest number of annotations need to be obtained by crowd-sourcing queries.   However, user annotations will be sparse and noisy, so the PI will also develop techniques for cleaning them up, as well as for propagating and aggregating them.  Understanding how to integrate semantic structure reflected in the geometry of shapes with semantic structure reflected in language is one of the deep problems the PI will tackle in this research.</AbstractNarration>
<MinAmdLetterDate>08/20/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/09/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1528025</AwardID>
<Investigator>
<FirstName>Leonidas</FirstName>
<LastName>Guibas</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leonidas J Guibas</PI_FULL_NAME>
<EmailAddress>guibas@cs.stanford.edu</EmailAddress>
<PI_PHON>4157230350</PI_PHON>
<NSF_ID>000467730</NSF_ID>
<StartDate>08/20/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[450 Serra Mall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~164941</FUND_OBLG>
<FUND_OBLG>2016~199455</FUND_OBLG>
<FUND_OBLG>2017~135604</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this effort has been to help make 3D data &ldquo;universally accessible and useful&rdquo; (paraphrasing Google&rsquo;s motto) -- with a focus on the 3D geometry of everyday objects. The contributions have been along three axes: data sets, shape analysis, and shape synthesis.</p> <p>On the data set side, the project has produced detailed semantic annotations of 3D data that facilitate the training of machine learning algorithms in computer vision (3D understanding) and computer graphics (3D content creation). These annotations were added to the ShapeNet repository, a collaborative effort supported by NSF through this and other grants. ShapeNet has indexed roughly 3 million models of common objects which are classified into approximately 3K categories (WordNet synsets) and has to-date been used by thousands of groups in the vision and graphics research community. In particular, work under this grant created the PartNet, repository, a consistent, large-scale subset of ShapeNet annotated with fine-grained, instance-level, and hierarchical 3D part information. Fine-grain here means small parts, such as annotations of each key of a keyboard or of the casters of a swivel office chair. The dataset covers over half a million part instances in tens of thousands of 3D models covering 25 object categories.</p> <p>On the shape analysis front, work under this grant led the design of deep architectures capable of processing unorganized point cloud data &ndash; which is a common output of 3D sensor pipelines. Such data does not have the regular grid structure exploited by the highly successful convolutional architectures in 2D computer vision.&nbsp; The PointNet and PointNet++ architectures developed in part under this project were highly effective and widely adopted in industry. The lightweight nature of the processing required has also made them suitable for small form-factor devices such as smartphones. Motivated by this impetus, deep learning on point clouds has now taken off in the research community, with many tens of papers in each major vision conference. The focus of the contributions under this project has been on object classification (what kind of object is it?) and segmentation (what are the object parts) &ndash; and a number of the proposed techniques for segmenting objects into parts, segmenting objects into geometric primitives (boxes, spheres, cylinders, etc.), or segmenting scenes into objects, define the curent state of the art.</p> <p>On the shape synthesis side of the work, building on the shape datasets and the object understanding discussed above, the project led to multiple tools that facilitate 3D content creation. An especially important contribution was deep architectures that can encode and decode detailed part hierarchies and thus generate or modify shape structure as well as geometry. These tools are useful in a variety of tasks, ranging from making semantically meaningful shape edits, to offering part recommendations that aid a designer in creating a desired shape, to automated tools that can replace noisy objects in 3D scans with adapted clean and high-quality 3D models. Finally, these tools can facilitate the large scale synthesis of 3D data (objects or scenes) that can then provide free ground truth annotations for training machine learning algorithms.</p> <p>The project also investigated the interaction of geometry and human language in describing shape differences and developed tools that allow the use of natural language in shape search.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/09/2020<br>      Modified by: Leonidas&nbsp;J&nbsp;Guibas</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this effort has been to help make 3D data "universally accessible and useful" (paraphrasing Google’s motto) -- with a focus on the 3D geometry of everyday objects. The contributions have been along three axes: data sets, shape analysis, and shape synthesis.  On the data set side, the project has produced detailed semantic annotations of 3D data that facilitate the training of machine learning algorithms in computer vision (3D understanding) and computer graphics (3D content creation). These annotations were added to the ShapeNet repository, a collaborative effort supported by NSF through this and other grants. ShapeNet has indexed roughly 3 million models of common objects which are classified into approximately 3K categories (WordNet synsets) and has to-date been used by thousands of groups in the vision and graphics research community. In particular, work under this grant created the PartNet, repository, a consistent, large-scale subset of ShapeNet annotated with fine-grained, instance-level, and hierarchical 3D part information. Fine-grain here means small parts, such as annotations of each key of a keyboard or of the casters of a swivel office chair. The dataset covers over half a million part instances in tens of thousands of 3D models covering 25 object categories.  On the shape analysis front, work under this grant led the design of deep architectures capable of processing unorganized point cloud data &ndash; which is a common output of 3D sensor pipelines. Such data does not have the regular grid structure exploited by the highly successful convolutional architectures in 2D computer vision.  The PointNet and PointNet++ architectures developed in part under this project were highly effective and widely adopted in industry. The lightweight nature of the processing required has also made them suitable for small form-factor devices such as smartphones. Motivated by this impetus, deep learning on point clouds has now taken off in the research community, with many tens of papers in each major vision conference. The focus of the contributions under this project has been on object classification (what kind of object is it?) and segmentation (what are the object parts) &ndash; and a number of the proposed techniques for segmenting objects into parts, segmenting objects into geometric primitives (boxes, spheres, cylinders, etc.), or segmenting scenes into objects, define the curent state of the art.  On the shape synthesis side of the work, building on the shape datasets and the object understanding discussed above, the project led to multiple tools that facilitate 3D content creation. An especially important contribution was deep architectures that can encode and decode detailed part hierarchies and thus generate or modify shape structure as well as geometry. These tools are useful in a variety of tasks, ranging from making semantically meaningful shape edits, to offering part recommendations that aid a designer in creating a desired shape, to automated tools that can replace noisy objects in 3D scans with adapted clean and high-quality 3D models. Finally, these tools can facilitate the large scale synthesis of 3D data (objects or scenes) that can then provide free ground truth annotations for training machine learning algorithms.  The project also investigated the interaction of geometry and human language in describing shape differences and developed tools that allow the use of natural language in shape search.          Last Modified: 10/09/2020       Submitted by: Leonidas J Guibas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
