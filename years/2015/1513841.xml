<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Medium: Collaborative Research: Augmented Reality for Multiple People, Perspectives, Platforms, and Tasks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>397714.00</AwardTotalIntnAmount>
<AwardAmount>397714</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This is a project to understand and improve how augmented reality can facilitate task performance by superimposing information in users' visual fields to help identify objects and features, direct actions, and provide spatial-temporal overviews and other perspectives. Augmented reality is a rapidly developing information display technology, using either head-mounted displays or handhelds incorporating cameras, to add information to the visual field of the user. The project will develop and test the best ways to facilitate performance with multiple people, perspectives, platforms, and tasks to convey the what, where, when, and how of action. The research will be conducted in the laboratory and in the field, including the historic El Barrio section of East Harlem.  Project software is expected to significantly improve the ability of residents and visitors, alone and in groups, to learn about, plan visits to, and explore the neighborhood.  In their personal and professional lives, people need to perform an enormous range of complex tasks, including navigation, maintenance, and assembly. Technology can make these tasks easier, and the use of augmented reality to interactively assist users by overlaying crucial missing information directly on a user's view is especially promising.&lt;br/&gt;&lt;br/&gt;Each of the tasks studied in this research entails an organized sequence of actions with respect to features or objects in the world. The features or objects may be hard to find or even be occluded. The actions may be complex. Work is increasingly collaborative, requiring coordination with others who have different perspectives. Each action can depend on previous and subsequent actions. The research will integrate work in cognitive science and computer science to develop principled approaches for using the transformative technology of augmented reality to assist people in navigation, maintenance, assembly, and related tasks. It will expand our abilities in understanding how people represent, transform, and communicate space and the actions in it; in designing instructions; in developing systems to assist users in assembly, maintenance, and navigation; and in designing stationary, mobile, and wearable user interfaces that use graphics, multimedia, and augmented reality, both indoors and outdoors. The scientific results will enlighten the study of the communication of objects, actions, and data, instructional design, and user interface design. The techniques and systems developed will inform the design of future systems that can aid the general public for educational and recreational ends, as well as systems that can assist people with auditory, visual, or physical impairments. Navigation, maintenance, and assembly are representative of many important daily tasks. Software created by the project will be adaptable to different situations and displays, and made available to the public as open source. The project will train students, and will be carried out in part through collaborative interdisciplinary projects in courses in computer science and cognitive science.</AbstractNarration>
<MinAmdLetterDate>08/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/22/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1513841</AwardID>
<Investigator>
<FirstName>Barbara</FirstName>
<LastName>Tversky</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Barbara Tversky</PI_FULL_NAME>
<EmailAddress>btversky@stanford.edu</EmailAddress>
<PI_PHON>2126786669</PI_PHON>
<NSF_ID>000072811</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Teachers College, Columbia University</Name>
<CityName>New York</CityName>
<ZipCode>100276625</ZipCode>
<PhoneNumber>2126783000</PhoneNumber>
<StreetAddress>525 West 120th Street</StreetAddress>
<StreetAddress2><![CDATA[Box 151]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>071050983</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEACHERS COLLEGE, COLUMBIA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071050983</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Teachers College, Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276696</ZipCode>
<StreetAddress><![CDATA[525 W. 120th]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>9199</Code>
<Text>Unallocated Program Costs</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~104141</FUND_OBLG>
<FUND_OBLG>2016~293573</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-d6d5cf3d-7fff-bdc9-9e6e-96705a4612bf"> </span></p> <p>Tools that augment what we are seeing and doing are increasingly evident on the job, in museums, in schools, on the streets, and in homes and stores. These tools can enrich our experience by adding information. They can also guide our behavior, making whatever tasks we are doing easier and more fluent. We have selected for our collaborative research project tasks that are common both in everyday life and in work: understanding and navigating an environment, and assembling, maintaining, or repairing a device. Some of our research has been devoted to understanding the ways people perform those tasks unaided, carefully observing how gesture, language, diagrams, movements of the eyes, hands, and head, and action sequences work together to accomplish the tasks. The larger part of our work has been to design, develop, and test augmented reality research tools to make tasks more fluent and accurate.</p> <p><em>Augmented reality</em> (AR) combines virtual graphics and other media with our perception of the physical world, using devices ranging from smartphones to head-worn displays that will someday become commonplace. Our tool building has focused on visual perception, augmented through head-worn displays. The research tools that we have created are designed to help users to understand, formulate, and direct actions, facilitating performance involving multiple people, multiple perspectives, and multiple platforms. Our emphasis has been on conveying the what, where, when, and how of action, helping users plan and perform a range of tasks individually and collaboratively, whether the users are in the same location or different locations.</p> <p>Significant outcomes of this project have included ones that address collaborative AR for remote task assistance, AR to assist in manually orienting 3D objects, improved map navigation, an open-source software framework for developing collaborative AR user interfaces, and collaborative AR for visualizing urban data, each described briefly below. Our work has also given insight into the ways that actions themselves communicate and explain, as well as gestures and visualizations. These under-researched aspects of communication play critical roles in collaborative work.</p> <p><strong>Collaborative AR for remote task assistance.</strong> We developed, evaluated, and demonstrated live at <em>SIGGRAPH 2017</em>, new approaches to allow a local maintenance technician in AR to be advised by a remote expert in AR or virtual reality (VR). Unlike commercial AR systems, the expert can create and precisely manipulate virtual copies of physical objects in the local environment, to refer to parts of those physical objects and to indicate complex actions on them.</p> <p><strong>AR to assist in manually orienting 3D objects.</strong> We developed and evaluated four 3D AR visualizations for guiding a user in rotating a physical object to a desired orientation, a crucial task in many assembly and repair scenarios. Our formal user study showed that a novel visualization technique we developed made it possible for users to perform rotation tasks more efficiently than the best known current visualizations.</p> <p><strong>Improved map navigation.</strong> We developed and evaluated approaches, presented at <em>CHI 2016</em> and <em>2018</em>, that significantly improve user navigation in interactive maps. One uses a personalized set of points of interest to help users better understand the distance and direction to a currently unseen target location. Another allows users to directly manipulate locations themselves to perform tasks better than current commercial software.&nbsp;</p> <p><strong>An open-source software framework for developing collaborative AR user interfaces.</strong> We developed and released for free public access Mercury Messaging (<a href="https://github.com/ColumbiaCGUI/MercuryMessaging">https://github.com/ColumbiaCGUI/MercuryMessaging</a>), an open-source software framework. Mercury Messaging is implemented in Unity 3D and allows programmers to create AR (and VR) user interfaces that support rich interconnections among their components on a single computer or across multiple networked computers with significantly less effort than other approaches. We used Mercury Messaging in most of the research software implemented for this grant.</p> <p><strong>Collaborative AR for exploring urban data.</strong> We created, and demonstrated live at <em>SIGGRAPH 2018,</em> a novel AR research system that allows multiple users to collaboratively interact with live data from social media (e,g., Twitter and Yelp) or municipal databases (e.g., <a href="https://opendata.cityofnewyork.us">https://opendata.cityofnewyork.us</a>)&nbsp; in context of an immersive scale model of an urban environment.</p> <p>Broader impacts of this collaborative research project have included training eight PhD students (seven of whom graduated during the grant period) and over a dozen MS and undergrad students (two of whom have formed AR startups after participating in grant research: <a href="https://www.echoar.xyz/">https://www.echoar.xyz/</a> and <a href="https://www.ravn.com/">https://www.ravn.com/</a>), giving numerous presentations of our work to the general public and to researchers in other domains, and experimentally applying our research to other domains, including education, design, rehabilitation, and medicine.</p> <p>&nbsp;</p> <p dir="ltr">&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/08/2020<br>      Modified by: Barbara&nbsp;Tversky</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Tools that augment what we are seeing and doing are increasingly evident on the job, in museums, in schools, on the streets, and in homes and stores. These tools can enrich our experience by adding information. They can also guide our behavior, making whatever tasks we are doing easier and more fluent. We have selected for our collaborative research project tasks that are common both in everyday life and in work: understanding and navigating an environment, and assembling, maintaining, or repairing a device. Some of our research has been devoted to understanding the ways people perform those tasks unaided, carefully observing how gesture, language, diagrams, movements of the eyes, hands, and head, and action sequences work together to accomplish the tasks. The larger part of our work has been to design, develop, and test augmented reality research tools to make tasks more fluent and accurate.  Augmented reality (AR) combines virtual graphics and other media with our perception of the physical world, using devices ranging from smartphones to head-worn displays that will someday become commonplace. Our tool building has focused on visual perception, augmented through head-worn displays. The research tools that we have created are designed to help users to understand, formulate, and direct actions, facilitating performance involving multiple people, multiple perspectives, and multiple platforms. Our emphasis has been on conveying the what, where, when, and how of action, helping users plan and perform a range of tasks individually and collaboratively, whether the users are in the same location or different locations.  Significant outcomes of this project have included ones that address collaborative AR for remote task assistance, AR to assist in manually orienting 3D objects, improved map navigation, an open-source software framework for developing collaborative AR user interfaces, and collaborative AR for visualizing urban data, each described briefly below. Our work has also given insight into the ways that actions themselves communicate and explain, as well as gestures and visualizations. These under-researched aspects of communication play critical roles in collaborative work.  Collaborative AR for remote task assistance. We developed, evaluated, and demonstrated live at SIGGRAPH 2017, new approaches to allow a local maintenance technician in AR to be advised by a remote expert in AR or virtual reality (VR). Unlike commercial AR systems, the expert can create and precisely manipulate virtual copies of physical objects in the local environment, to refer to parts of those physical objects and to indicate complex actions on them.  AR to assist in manually orienting 3D objects. We developed and evaluated four 3D AR visualizations for guiding a user in rotating a physical object to a desired orientation, a crucial task in many assembly and repair scenarios. Our formal user study showed that a novel visualization technique we developed made it possible for users to perform rotation tasks more efficiently than the best known current visualizations.  Improved map navigation. We developed and evaluated approaches, presented at CHI 2016 and 2018, that significantly improve user navigation in interactive maps. One uses a personalized set of points of interest to help users better understand the distance and direction to a currently unseen target location. Another allows users to directly manipulate locations themselves to perform tasks better than current commercial software.   An open-source software framework for developing collaborative AR user interfaces. We developed and released for free public access Mercury Messaging (https://github.com/ColumbiaCGUI/MercuryMessaging), an open-source software framework. Mercury Messaging is implemented in Unity 3D and allows programmers to create AR (and VR) user interfaces that support rich interconnections among their components on a single computer or across multiple networked computers with significantly less effort than other approaches. We used Mercury Messaging in most of the research software implemented for this grant.  Collaborative AR for exploring urban data. We created, and demonstrated live at SIGGRAPH 2018, a novel AR research system that allows multiple users to collaboratively interact with live data from social media (e,g., Twitter and Yelp) or municipal databases (e.g., https://opendata.cityofnewyork.us)  in context of an immersive scale model of an urban environment.  Broader impacts of this collaborative research project have included training eight PhD students (seven of whom graduated during the grant period) and over a dozen MS and undergrad students (two of whom have formed AR startups after participating in grant research: https://www.echoar.xyz/ and https://www.ravn.com/), giving numerous presentations of our work to the general public and to researchers in other domains, and experimentally applying our research to other domains, including education, design, rehabilitation, and medicine.               Last Modified: 01/08/2020       Submitted by: Barbara Tversky]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
