<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Rich and Scalable Optimization for Modern Bayesian Nonparametric Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>486255.00</AwardTotalIntnAmount>
<AwardAmount>486255</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Large-scale data analysis has become an indispensable tool throughout academia and industry.  When the amount of data is very large, one often faces a tradeoff between the richness, flexibility, and potential predictive power of the models, and the computational requirements.  While recent advances in statistics and machine learning provide us with a rich set of models and tools, many of these cannot be applied in contemporary applications due to the sheer volume of data available for analysis.  In particular, Bayesian nonparametric models are a rich class of models which have largely been restricted to small-scale setting.  This class of methods, in contrast to standard parametric statistical models, does not fix the complexity of the model, instead allowing the data to determine how complex the resulting models are.  While these models appear well-suited for large-scale data analysis, current methods for inference in Bayesian nonparametric models have not been shown to work at scale. In terms of broader impacts, applications of machine learning continue to emerge in fields such as medicine, engineering, and the humanities; this research has the potential to impact a number of problems in these fields.  Further, the PI's research has applications in the field of computer vision, which itself has emerging impacts in autonomous driving and eldercare.  The project also includes an effort to further integrate coursework in computer science with coursework in statistics, aiming to continue to bridge the gap between the fields.  Finally, the research will introduce undergraduate and high school students to research, and will also yield new software for nonparametric problems that can be applied by practitioners outside the machine learning field.&lt;br/&gt;&lt;br/&gt;This CAREER project explores scalable optimization methods aimed at making Bayesian nonparametric models applicable in large-scale settings.  The research in this project is focused on three general themes:&lt;br/&gt;&lt;br/&gt;1) Small-variance asymptotics for scalable nonparametric modeling.  This part of the project aims to develop new and scalable algorithms for nonparametric problems that can be applied to problems such as topic modeling, image segmentation, and image feature learning.  The goals include extending the technique of small-variance asymptotics to new Bayesian nonparametric models, improving the theoretical underpinnings of the asymptotic techniques, and developing large-scale software for several problems.&lt;br/&gt;&lt;br/&gt;2) New variational inference techniques for nonparametric problems.  This part of the project focuses on extending variational inference methods to new settings.  In particular, the PI focuses on applying variational inference to gamma process models, and will develop variational inference methods for the emerging class of exponential-family completely random measures.&lt;br/&gt;&lt;br/&gt;3) New applications for large-scale Bayesian nonparametrics.  This part of the project uses the results in the previous two parts to explore applications of Bayesian nonparametric models that were previously unattainable.  Applications include large-scale image modeling, social network analysis, and large-scale document analysis.</AbstractNarration>
<MinAmdLetterDate>09/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>02/11/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1559558</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Kulis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian Kulis</PI_FULL_NAME>
<EmailAddress>bkulis@bu.edu</EmailAddress>
<PI_PHON>6173535909</PI_PHON>
<NSF_ID>000601190</NSF_ID>
<StartDate>09/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress><![CDATA[881 Commonwealth Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~94320</FUND_OBLG>
<FUND_OBLG>2016~93357</FUND_OBLG>
<FUND_OBLG>2017~96359</FUND_OBLG>
<FUND_OBLG>2018~99484</FUND_OBLG>
<FUND_OBLG>2019~102735</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This CAREER project explored a framework for the development of scalable machine learning algorithms inspired by Bayesian nonparametric models.&nbsp; Most traditional machine learning methods optimize over a fixed set of parameters, but nonparametric methods, and Bayesian nonparametric (BNP) methods in particular, allow the number of parameters of the model to grow with the data.&nbsp; In turn, this allows the data itself to determine how complex the resulting model is.&nbsp; BNP models can be applied when the number of features in the model is allowed to grow unboundedly or the number of clusters/topics in the data is not fixed upfront.&nbsp; Unfortunately, these models require complex algorithms (e.g., MCMC or variational inference) that typically do not scale to modern large data sets.&nbsp;</p> <p>In this project, we developed a number of scalable nonparametric algorithms, mainly focused on those derived from BNP models.&nbsp; A particular focus was on the technique of small-variance asymptotics, which is inspired by the small-variance connection between mixtures of Gaussians and k-means.&nbsp; In our work, we extended this connection to a number of BNP models, including models for clustering data over time, infinite-state hidden Markov models, topic models, and nonparametric clustering problems.&nbsp; We further explored theoretical underpinnings of these methods. We also explored scalable nonparametric algorithms based on variational inference and for metric learning problems.&nbsp; We released code for our methods and applied our developed algorithms to several domains, most notably for computer vision tasks.&nbsp;</p><br> <p>            Last Modified: 10/29/2020<br>      Modified by: Brian&nbsp;Kulis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This CAREER project explored a framework for the development of scalable machine learning algorithms inspired by Bayesian nonparametric models.  Most traditional machine learning methods optimize over a fixed set of parameters, but nonparametric methods, and Bayesian nonparametric (BNP) methods in particular, allow the number of parameters of the model to grow with the data.  In turn, this allows the data itself to determine how complex the resulting model is.  BNP models can be applied when the number of features in the model is allowed to grow unboundedly or the number of clusters/topics in the data is not fixed upfront.  Unfortunately, these models require complex algorithms (e.g., MCMC or variational inference) that typically do not scale to modern large data sets.   In this project, we developed a number of scalable nonparametric algorithms, mainly focused on those derived from BNP models.  A particular focus was on the technique of small-variance asymptotics, which is inspired by the small-variance connection between mixtures of Gaussians and k-means.  In our work, we extended this connection to a number of BNP models, including models for clustering data over time, infinite-state hidden Markov models, topic models, and nonparametric clustering problems.  We further explored theoretical underpinnings of these methods. We also explored scalable nonparametric algorithms based on variational inference and for metric learning problems.  We released code for our methods and applied our developed algorithms to several domains, most notably for computer vision tasks.        Last Modified: 10/29/2020       Submitted by: Brian Kulis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
