<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF:  Small:  Linear Algebra++ and applications to machine learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2015</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>466000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many areas of unsupervised learning (i.e, learning with data that has not been labeled by humans) currently rely on heuristic algorithms that lack provable guarantees on solution quality or running time. In fact the underlying problems - as currently formulated - are often known to be computationally intractable (NP-hard, to use a technical term). This proposal identifies a big set of these problems that can be seen as twists - involving constraints such as nonnegativity and sparsity - on classical linear algebra problems like solving linear systems, rank computation, and eigenvalues/eigenvectors. The PI has proposed calling this set of problems collectively as Linear Algebra++. The project will seek to develop  algorithms with provable guarantees for these problems. The methodology will be to make suitable assumptions about the structure of realistic inputs.  The algorithms will be applied to problems in unsupervised learning, in areas such as topic modeling, natural language processing, semantic embeddings, sparse principle components analysis (PCA), deep nets, etc.&lt;br/&gt;&lt;br/&gt;The work will lead to new and more efficient algorithms for big data processing that will come with provable guarantees of quality. It will bring new rigorous approaches to machine learning. (Some recent work of the PI shows that the new rigorous approaches can be quite practical.) It will advance the state of art in theoretical computer science by expanding its range and its standard toolkit of algorithms. It will contribute fundamentally new primitives to classical linear algebra and applied mathematics.&lt;br/&gt;&lt;br/&gt;The project will train a new breed of graduate students who will be fluent both in theoretical algorithms and machine learning. The PI has a track record in this kind of work and training during the past few years and will continue this including working with undergrads and Research Experiences for Undergraduates (REU) students.  Any new algorithms discovered as part of this project will be released as open source code. The PI also plans a series of other outreach activities in the next few years including (a) A workshop. (b) A special semester or year at the Simons Institute in 2016-17 on provable bounds in machine learning which he will coorganize. (c) A new book on graduate algorithms based upon his new grad course, which tries to re-orient algorithms training for today's computer science problems. (d) A series of talks aimed at broad audiences, of which he gives several each year.&lt;br/&gt;&lt;br/&gt;The techniques will build upon recent progress by the PI and others on problems such as nonnegative matrix factorization, sparse coding, alternating minimization etc. They involve average case analysis, classical linear algebra, convex optimization, numerical analysis, etc., as well involve completely new ideas. They could have a transformative effect on machine learning and algorithms.</AbstractNarration>
<MinAmdLetterDate>06/11/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/25/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1527371</AwardID>
<Investigator>
<FirstName>Sanjeev</FirstName>
<LastName>Arora</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sanjeev Arora</PI_FULL_NAME>
<EmailAddress>arora@cs.princeton.edu</EmailAddress>
<PI_PHON>6092583869</PI_PHON>
<NSF_ID>000101873</NSF_ID>
<StartDate>06/11/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[87 Prospect Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~450000</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project focused on developing understanding NP-hard extensions of linear algebraic procedures, which the PI&nbsp;refered&nbsp;to as "Linear Algebra++" in the original proposal. These involve adding twists such as sparsity,&nbsp;nonnegativity&nbsp;and nonlinearities to linear algebra, and are interesting in a host of areas. The goal was to understand their role in machine learning, &nbsp;especially natural language processing, semantic&nbsp;embeddings, sparse PCA, deep nets etc.</p> <p>&nbsp;</p> <p>Research successes: (a) New theory for word and sentence embeddings which explained properties of existing embeddings such as GloVe and word2vec, and also led to a string of new and simpler sentence embeddings, with publications in top venues of Natural Language processing. Some of the simpler sentence embeddings proved useful in neuroscience investigations involving analysis of fMRI of human subjects who are watching a movie. (b) An analysis of why real-life deep nets don&rsquo;t overfit despite having orders of magnitude more variables than number of training data points. This rests upon a new empirical discovery, that trained nets are highly resistant to injection of a lot of noise at its nodes, and a provable connection of this noise resistance to compressibility.&nbsp; (c) A discovery of how to solve matrix completion (a well-studied problem in recommender systems, traditionally solved with a convex program) via a deep linear net.&nbsp; This method is better at recovery than the traditional convex program and provides new insights into gradient descent optimization for deep learning.</p> <p>&nbsp;</p> <p>Broader Impacts: By providing rigorous theory, the project helped provide new foundational insight into machine learning methods and deep learning. The work on word and sentence embeddings was widely noted in natural language processing and also led to new analysis methods for neuroscience (reported in a joint paper with neuroscience colleagues).</p> <p>&nbsp;</p> <p>Many grad students, postdocs and undergrads were trained in the group during the duration of this grant. Three grad students are now faculty at Stanford and CMU (2x), one postdoc is faculty at U. Wisconsin, and another at Tel Aviv Univ. Four undergrads have gone on to top grad programs.</p> <p>&nbsp;</p> <p>The PI created a new blog, Of the Convex Path (offconvex.org) where he and others post expository articles about new research in theoretical machine learning, and which attracts about 6000 views per month. He was invited to present a tutorial at ICML 2018 on theory of deep learning, and also a plenary talk on machine learning at International Congress of Mathematicians 2018. He also gave about half a dozen plenary or invited talks on his work at various venues each year.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/22/2019<br>      Modified by: Sanjeev&nbsp;Arora</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project focused on developing understanding NP-hard extensions of linear algebraic procedures, which the PI refered to as "Linear Algebra++" in the original proposal. These involve adding twists such as sparsity, nonnegativity and nonlinearities to linear algebra, and are interesting in a host of areas. The goal was to understand their role in machine learning,  especially natural language processing, semantic embeddings, sparse PCA, deep nets etc.     Research successes: (a) New theory for word and sentence embeddings which explained properties of existing embeddings such as GloVe and word2vec, and also led to a string of new and simpler sentence embeddings, with publications in top venues of Natural Language processing. Some of the simpler sentence embeddings proved useful in neuroscience investigations involving analysis of fMRI of human subjects who are watching a movie. (b) An analysis of why real-life deep nets don?t overfit despite having orders of magnitude more variables than number of training data points. This rests upon a new empirical discovery, that trained nets are highly resistant to injection of a lot of noise at its nodes, and a provable connection of this noise resistance to compressibility.  (c) A discovery of how to solve matrix completion (a well-studied problem in recommender systems, traditionally solved with a convex program) via a deep linear net.  This method is better at recovery than the traditional convex program and provides new insights into gradient descent optimization for deep learning.     Broader Impacts: By providing rigorous theory, the project helped provide new foundational insight into machine learning methods and deep learning. The work on word and sentence embeddings was widely noted in natural language processing and also led to new analysis methods for neuroscience (reported in a joint paper with neuroscience colleagues).     Many grad students, postdocs and undergrads were trained in the group during the duration of this grant. Three grad students are now faculty at Stanford and CMU (2x), one postdoc is faculty at U. Wisconsin, and another at Tel Aviv Univ. Four undergrads have gone on to top grad programs.     The PI created a new blog, Of the Convex Path (offconvex.org) where he and others post expository articles about new research in theoretical machine learning, and which attracts about 6000 views per month. He was invited to present a tutorial at ICML 2018 on theory of deep learning, and also a plenary talk on machine learning at International Congress of Mathematicians 2018. He also gave about half a dozen plenary or invited talks on his work at various venues each year.          Last Modified: 07/22/2019       Submitted by: Sanjeev Arora]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
