<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CCF AF:EAGER:ASSESSING PRACTICALITY OF A NEW FRAMEWORK FOR SOLVING CONIC OPTIMIZATION PROBLEMS BY FIRST-ORDER METHODS</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>02/28/2018</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jack S. Snoeyink</SignBlockName>
<PO_EMAI>jsnoeyin@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A key component of Analytics, the scientific process of transforming data into insight for better decisions, is Optimization, which produces the best solution satisfying given constraints -- the solution that maximizes a chosen "objective" function.  &lt;br/&gt;&lt;br/&gt;The objective and constraints together form "an optimization model." The difficulty of solving a model depends on the types of objective and constraint functions, and on the number of variables in the functions.  Since huge, complicated, real-world models can have many variables, the amount of computer memory needed becomes the bottleneck for solutions by classical algorithms.  Modern algorithms avoid this bottleneck by  calculating fewer structures that have to be stored in memory. For example, some modern algorithms evaluate only first derivatives of functions, whereas older algorithms also stored second derivatives in memory.  These modern algorithms are known as "first-order methods" (meaning, roughly, "algorithms using only first derivatives").  &lt;br/&gt;&lt;br/&gt;First-order methods can handle complicated objective functions, but it has been unknown how to handle complicated constraint functions.  The focus of the project is a new framework that allows many optimization models with complicated constraints to be easily transformed into equivalent models with only simple constraints, so that existing first-order methods can be applied. The goal of the project is to thoroughly test whether, using the new framework, important huge models that were previously unsolvable can now be solved routinely.  If so, entities relying on Analytics could benefit, in that their huge models involving complicated constraints might actually become solvable by existing first-order methods. &lt;br/&gt;&lt;br/&gt;The new framework transforms any convex, conic optimization problem into an equivalent optimization problem whose only constraints are linear equations, one more equation than for the original problem.  Virtually any subgradient method can be applied to the equivalent problem. Moreover, for a wide class of conic optimization problems (hyperbolic programs), the objective function for the equivalent problem can be "smoothed," thus allowing for application of accelerated gradient methods.  The goal of the project is to thoroughly test practicality of the new approach in applying first-order methods to solve large, general, conic optimization problems.&lt;br/&gt; &lt;br/&gt;PhD students will test this framework as part of their careers formation, in consultation with optimization experts from both academia and industry.  The result should be better analytics in business, government, healthcare and education for making decisions based on data.</AbstractNarration>
<MinAmdLetterDate>08/25/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1552518</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Renegar</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>James M Renegar</PI_FULL_NAME>
<EmailAddress>renegar@orie.cornell.edu</EmailAddress>
<PI_PHON>6072559142</PI_PHON>
<NSF_ID>000362038</NSF_ID>
<StartDate>08/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148533801</ZipCode>
<StreetAddress><![CDATA[136 Hoy Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><br />The setting of interest for the project was general convex optimization, where the goal is to compute a point which minimizes a given convex function (the "objective function") subject to the point lying within a given closed, convex set (the "feasible region"). The foremost algorithms designed for this high level of generality are subgradient methods, first introduced in the early 1960's in the Soviet Union (and unknown to researchers in the West until years later). Since then, subgradient methods have been refined for a wide variety of application areas, especially when the optimization problem to be solved has objective function which is not differentiable -- instead, it suffices for the function to be Lipschitz continuous.</p> <p><br />While subgradient methods are, in theory, applicable to solving convex optimization problems generally, at issue is that they typically rely on a computational operation that prohibits their use in practice when solving large-scale problems, unless the feasible region has simple structure. Let us briefly explain. Analogous to steepest descent, a subgradient method is an iterative algorithm, which at each iteration makes a step from the current iterate to the next iterate, where the step is a negative multiple of a subgradient at the current iterate (if the function is differentiable at the current iterate, this is precisely a steepest-descent step). While making a subgradient step is an operation that usually scales to problems with a vast number of variables, a subsequent operation is the issue: Specifically, if the next iterate happens to lie outside the feasible region, then the iterate is orthogonally projected onto the feasible region in order that the following subgradient step is taken from a feasible point. Unless the feasible region has simple structure, however, orthogonally projecting onto the feasible region is itself a costly optimization problem when there are many variables. Thus, traditional subgradient methods are less useful than their generality might suggest.</p> <p><br />The project focused on fully developing an idea of the PI, whereby a general convex optimization problem is transformed to an equivalent optimization problem with only one additional variable, and for which the feasible region is the solution set to an affine space (translate of a subspace). Here, orthogonally projecting onto the feasible region is readily accomplished by solving a system of linear equations. The PI had previously developed the idea in a few restricted settings, but the project funding allowed for the theory to be developed with complete generality, resulting in papers by the PI and a PhD student (Benjamin Grimmer) which are appearing in premiere journals and which also can be found on arXiv. (Additionally, an informative open-source Julia notebook by Grimmer can be found at GitHub (search for "Radial-Subgradient-Method").)</p> <p>For a broad range of important optimization problems, the transformation to an equivalent problem is computationally tractable, leading to the possibility of solving an optimization problem by applying a subgradient method to the equivalent problem. However, while difficult orthogonal projections are entirely avoided, it remained the fact that existing subgradient methods are often slow algorithms, requiring many iterations to obtain a good approximation to an optimal solution. A second focus of the project has thus been to develop new subgradient methods which are far superior in performance when applied to minimizing convex functions possessing special, but still quite general, structure.&nbsp;</p> <p><br />A new subgradient method was developed which is essentially a parallel algorithm but also can be run sequentially. The new method has provably superior performance when the function to be minimized has particular, but still quite general, structure. For example, when the new method is applied to minimize a convex, piecewise-linear function, the method essentially attains linear convergence, the first subgradient method to do so without having to rely on any information about the function that usually would be unavailable in practice.</p> <p><br />Towards the end of the project, it was realized that the key ideas behind the new subgradient method can be extended to many first-order methods, including accelerated gradient methods. For a first-order method, the resulting parallel algorithm attains provably faster convergence rates for important, and quite general, classes of functions. The consequent paper can be found on arXiv, and is under review for journal publication.</p> <p><br />Project funding has ended, but the research continues to expand.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/04/2018<br>      Modified by: James&nbsp;M&nbsp;Renegar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The setting of interest for the project was general convex optimization, where the goal is to compute a point which minimizes a given convex function (the "objective function") subject to the point lying within a given closed, convex set (the "feasible region"). The foremost algorithms designed for this high level of generality are subgradient methods, first introduced in the early 1960's in the Soviet Union (and unknown to researchers in the West until years later). Since then, subgradient methods have been refined for a wide variety of application areas, especially when the optimization problem to be solved has objective function which is not differentiable -- instead, it suffices for the function to be Lipschitz continuous.   While subgradient methods are, in theory, applicable to solving convex optimization problems generally, at issue is that they typically rely on a computational operation that prohibits their use in practice when solving large-scale problems, unless the feasible region has simple structure. Let us briefly explain. Analogous to steepest descent, a subgradient method is an iterative algorithm, which at each iteration makes a step from the current iterate to the next iterate, where the step is a negative multiple of a subgradient at the current iterate (if the function is differentiable at the current iterate, this is precisely a steepest-descent step). While making a subgradient step is an operation that usually scales to problems with a vast number of variables, a subsequent operation is the issue: Specifically, if the next iterate happens to lie outside the feasible region, then the iterate is orthogonally projected onto the feasible region in order that the following subgradient step is taken from a feasible point. Unless the feasible region has simple structure, however, orthogonally projecting onto the feasible region is itself a costly optimization problem when there are many variables. Thus, traditional subgradient methods are less useful than their generality might suggest.   The project focused on fully developing an idea of the PI, whereby a general convex optimization problem is transformed to an equivalent optimization problem with only one additional variable, and for which the feasible region is the solution set to an affine space (translate of a subspace). Here, orthogonally projecting onto the feasible region is readily accomplished by solving a system of linear equations. The PI had previously developed the idea in a few restricted settings, but the project funding allowed for the theory to be developed with complete generality, resulting in papers by the PI and a PhD student (Benjamin Grimmer) which are appearing in premiere journals and which also can be found on arXiv. (Additionally, an informative open-source Julia notebook by Grimmer can be found at GitHub (search for "Radial-Subgradient-Method").)  For a broad range of important optimization problems, the transformation to an equivalent problem is computationally tractable, leading to the possibility of solving an optimization problem by applying a subgradient method to the equivalent problem. However, while difficult orthogonal projections are entirely avoided, it remained the fact that existing subgradient methods are often slow algorithms, requiring many iterations to obtain a good approximation to an optimal solution. A second focus of the project has thus been to develop new subgradient methods which are far superior in performance when applied to minimizing convex functions possessing special, but still quite general, structure.    A new subgradient method was developed which is essentially a parallel algorithm but also can be run sequentially. The new method has provably superior performance when the function to be minimized has particular, but still quite general, structure. For example, when the new method is applied to minimize a convex, piecewise-linear function, the method essentially attains linear convergence, the first subgradient method to do so without having to rely on any information about the function that usually would be unavailable in practice.   Towards the end of the project, it was realized that the key ideas behind the new subgradient method can be extended to many first-order methods, including accelerated gradient methods. For a first-order method, the resulting parallel algorithm attains provably faster convergence rates for important, and quite general, classes of functions. The consequent paper can be found on arXiv, and is under review for journal publication.   Project funding has ended, but the research continues to expand.             Last Modified: 05/04/2018       Submitted by: James M Renegar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
