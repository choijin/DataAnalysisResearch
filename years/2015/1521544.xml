<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Novel Computational and Statistical Approaches to Prediction and Estimation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Terabytes of data are collected by companies and individuals every day. These data possess no value unless one can efficiently process them and use them to make decisions. The scale and the streaming nature of data pose both computational and statistical challenges. The objective of this research project is to develop novel approaches to making online, real-time decisions when data are constantly evolving and highly structured. In particular, this project focuses on online prediction problems involving multiple users in dynamic networks. The project also aims to tackle the privacy issues arising in such multi-user scenarios.&lt;br/&gt;&lt;br/&gt;In recent years, it was shown that a majority of online machine learning algorithms can be viewed as solutions to approximate dynamic programming (ADP) problems that incorporate one additional datum per step. Along with directly addressing the computational concerns, the ADP framework also provides guaranteed performance on prediction problems. This project is to use and extend the ADP framework to develop prediction algorithms that simultaneously address the issues of computation, robustness, non-stationarity, privacy, and multiplicity of users.</AbstractNarration>
<MinAmdLetterDate>07/21/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1521544</AwardID>
<Investigator>
<FirstName>Karthik</FirstName>
<LastName>Sridharan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karthik Sridharan</PI_FULL_NAME>
<EmailAddress>sridharan@cs.cornell.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000678997</NSF_ID>
<StartDate>07/21/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148537501</ZipCode>
<StreetAddress><![CDATA[Gates Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~160000</FUND_OBLG>
<FUND_OBLG>2017~15000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1"><span class="s1">Real world Machine Learning (ML) applications require methods that continually update themselves, are robust to model misspecification, and that can handle active interactions with multiple users. The aim of this project was to develop a rigorous theory for machine learning that tackles these issues and has an intrinsic computational or algorithmic focus. Specifically the PI's proposed to develop and extend the approximate dynamic programming technique to complex large-scale structured learning problems.&nbsp;</span></p> <p class="p1"><span class="s1"><br /></span></p> <p class="p1"><span class="s1"><strong>Technical Outcomes:</strong>&nbsp;</span></p> <ol class="ol1"> <li class="li2"><strong>&nbsp;</strong><span class="s1"><strong>&nbsp;Adaptive Online Learning and the Connection to Martingales: </strong></span>A key reason for the chasm between theory and practice in ML is that theoretically derived algorithms are conservative and aim at providing best performance in the worst case. However real world instances are typically more benign and hence heuristically engineered methods take advantage of this to provide better empirical performance. To bridge this gap between theory and practice, we need to develop a strong theory for adaptive learning that while retaining worst case guarantees, can also provably adapt to more benign instances.<span>&nbsp;</span>In [1], using minimax analysis, the PIs characterized if a given adaptive performance guarantee is achievable or not. In this same work, extending the approximate dynamic programming tool to handle the adaptive case, the PIs provided a algorithm design principles for adaptive learning. The minimax analysis of adaptive online learning yielded a surprising connection to the topic of probabilistic inequalities involving martingales. Specifically, in joint work in [2], the PIs were able to show an equivalence between deterministic regret bounds in online learning and high-probability tail bounds for the supremum of a collection of martingales.<span>&nbsp;</span></li> <li class="li2"><strong>&nbsp;</strong><span class="s1"><strong>General Algorithm Design Principle for Online Learning:</strong>&nbsp;The connection between online learning and probabilistic inequalities tied in neatly with the approximate dynamic programming principle and hence helped develop new online learning algorithms. In [3] and [4] using and extending a set of tools from martingale theory first introduced by Donald Burkholder, the PIs introduced a new method, called the Burkholder method to derive new efficient and adaptive online learning algorithms. This work provided us with an end-to-end universal procedure to derive online learning algorithms that can automatically adapt to instance hardness.</span></li> <li><span class="s1">&nbsp;</span><span class="s1"><strong>Learning Under Partial Information Feedback Model:</strong>&nbsp;Many practical scenarios where ML algorithms are deployed operate in the so called partial information regime. For instance, when ML algorithms are used to display Ads to users, once the interaction with the user is complete, the learning algorithm only gets the feedback of whether or not the user clicked on any of the displayed Ads. The algorithms is not given information about whether the user would have clicked on a different Ad, had it been displayed.&nbsp; Having developed a unifying tool of approximate dynamic programming for design of online learning algorithms in earlier works, our first thought at dealing with partial information setting was to extend these relaxations to the partial information setting. However, this task turned out to be much more delicate than we first anticipated. Finally, in our work in [5], we extended Approximate Dynamic Prohramming to handle contextual bandit problems.&nbsp; This work was the first to obtain ``oracle" efficient contextual bandit algorithms for the adversarial costs and contexts drawn from a fixed distribution setting.</span></li> </ol> <p class="p4"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1"><strong>Academic Outcomes:<span>&nbsp;</span></strong> The tools and results established in this project have now been included in the graduate course of the PI on theoretical machine learning at Cornell University. Some of the high level issues and problems illuminated by the project, like social issues like fairness and privacy arising in large multiuser interactive systems that this research project brought out have been included in discussions in the PIs undergraduate course on Machine Learning for Data Science.&nbsp;</span></p> <p class="p4"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1"><strong>Broader Impact:</strong> The research outcomes of this project have had broader impact beyond the scope of this project as the tools developed have subsequently been used not only by the PIs but others to tackle various machine learning problems. Follow up work of the publications mentioned have made some significant strides in some of the open problems highlighted by the PIs. The project has also highlighted a surprising connection between concepts in probability theory and machine learning and this connection has had the impact of bringing together concepts and researcher in the two fields. The<span>&nbsp;</span><br /></span></p> <p class="p5"><span class="s2"><br /></span></p> <p class="p6"><span class="s1">[1] D. Foster, A. Rakhlin, and K. Sridharan.&nbsp;"Adaptive Online Learning,"&nbsp;Neural Information Processing Systems ,&nbsp;2015.<br /><br />[2] A. Rakhlin and K. Sridharan.&nbsp;"On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities,"&nbsp;&nbsp;<em>Conference on Learning Theory (COLT)</em>,&nbsp;2017.<br /><br />[3] D. Foster, A. Rakhlin, and K. Sridharan.&nbsp;"ZigZag: A new approach to adaptive online learning,"&nbsp;<em>Conference on Learning Theory (COLT)</em>,&nbsp;2017.<br /><br />[4] D. Foster, A. Rakhlin, and K. Sridharan. "Online Learning: Sufficient Statistics and the Burkholder Method", <em>Conference on Learning Theory (COLT)</em>,&nbsp;2018.<br /><br />[5] A. Rakhlin and K. Sridharan.&nbsp;"BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits,"&nbsp;<em>International Conference on Machine Learning (ICML)</em>,&nbsp;2016.<br /></span></p> <p>&nbsp;</p><br> <p>            Last Modified: 08/22/2020<br>      Modified by: Karthik&nbsp;Sridharan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Real world Machine Learning (ML) applications require methods that continually update themselves, are robust to model misspecification, and that can handle active interactions with multiple users. The aim of this project was to develop a rigorous theory for machine learning that tackles these issues and has an intrinsic computational or algorithmic focus. Specifically the PI's proposed to develop and extend the approximate dynamic programming technique to complex large-scale structured learning problems.    Technical Outcomes:     Adaptive Online Learning and the Connection to Martingales: A key reason for the chasm between theory and practice in ML is that theoretically derived algorithms are conservative and aim at providing best performance in the worst case. However real world instances are typically more benign and hence heuristically engineered methods take advantage of this to provide better empirical performance. To bridge this gap between theory and practice, we need to develop a strong theory for adaptive learning that while retaining worst case guarantees, can also provably adapt to more benign instances. In [1], using minimax analysis, the PIs characterized if a given adaptive performance guarantee is achievable or not. In this same work, extending the approximate dynamic programming tool to handle the adaptive case, the PIs provided a algorithm design principles for adaptive learning. The minimax analysis of adaptive online learning yielded a surprising connection to the topic of probabilistic inequalities involving martingales. Specifically, in joint work in [2], the PIs were able to show an equivalence between deterministic regret bounds in online learning and high-probability tail bounds for the supremum of a collection of martingales.   General Algorithm Design Principle for Online Learning: The connection between online learning and probabilistic inequalities tied in neatly with the approximate dynamic programming principle and hence helped develop new online learning algorithms. In [3] and [4] using and extending a set of tools from martingale theory first introduced by Donald Burkholder, the PIs introduced a new method, called the Burkholder method to derive new efficient and adaptive online learning algorithms. This work provided us with an end-to-end universal procedure to derive online learning algorithms that can automatically adapt to instance hardness.  Learning Under Partial Information Feedback Model: Many practical scenarios where ML algorithms are deployed operate in the so called partial information regime. For instance, when ML algorithms are used to display Ads to users, once the interaction with the user is complete, the learning algorithm only gets the feedback of whether or not the user clicked on any of the displayed Ads. The algorithms is not given information about whether the user would have clicked on a different Ad, had it been displayed.  Having developed a unifying tool of approximate dynamic programming for design of online learning algorithms in earlier works, our first thought at dealing with partial information setting was to extend these relaxations to the partial information setting. However, this task turned out to be much more delicate than we first anticipated. Finally, in our work in [5], we extended Approximate Dynamic Prohramming to handle contextual bandit problems.  This work was the first to obtain ``oracle" efficient contextual bandit algorithms for the adversarial costs and contexts drawn from a fixed distribution setting.    Academic Outcomes:  The tools and results established in this project have now been included in the graduate course of the PI on theoretical machine learning at Cornell University. Some of the high level issues and problems illuminated by the project, like social issues like fairness and privacy arising in large multiuser interactive systems that this research project brought out have been included in discussions in the PIs undergraduate course on Machine Learning for Data Science.    Broader Impact: The research outcomes of this project have had broader impact beyond the scope of this project as the tools developed have subsequently been used not only by the PIs but others to tackle various machine learning problems. Follow up work of the publications mentioned have made some significant strides in some of the open problems highlighted by the PIs. The project has also highlighted a surprising connection between concepts in probability theory and machine learning and this connection has had the impact of bringing together concepts and researcher in the two fields. The     [1] D. Foster, A. Rakhlin, and K. Sridharan. "Adaptive Online Learning," Neural Information Processing Systems , 2015.  [2] A. Rakhlin and K. Sridharan. "On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities,"  Conference on Learning Theory (COLT), 2017.  [3] D. Foster, A. Rakhlin, and K. Sridharan. "ZigZag: A new approach to adaptive online learning," Conference on Learning Theory (COLT), 2017.  [4] D. Foster, A. Rakhlin, and K. Sridharan. "Online Learning: Sufficient Statistics and the Burkholder Method", Conference on Learning Theory (COLT), 2018.  [5] A. Rakhlin and K. Sridharan. "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits," International Conference on Machine Learning (ICML), 2016.           Last Modified: 08/22/2020       Submitted by: Karthik Sridharan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
