<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small:Testing in the Presence of Continuous Change</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>425000.00</AwardTotalIntnAmount>
<AwardAmount>425000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many organizations that develop software are utilizing continuous integration&lt;br/&gt;processes, in which engineers merge code frequently with the mainline codebase,&lt;br/&gt;and continuously regression test it.  The pace at which evolution occurs in&lt;br/&gt;such environments, and the numbers of tests that must be run, can be staggering.&lt;br/&gt;To ensure the quality of software in these situations, testing processes need&lt;br/&gt;to adapt quickly to changes in available resources, testing request frequency,&lt;br/&gt;and product release cycles.  Tests must be chosen and scheduled appropriately,&lt;br/&gt;tests must be analyzed for robustness, and testing processes must be&lt;br/&gt;appropriately monitored.  The proposed work addresses these needs, and by&lt;br/&gt;focusing on software dependability, the work promises to benefit both&lt;br/&gt;producers (i.e., software developers) and consumers (i.e., the public at large)&lt;br/&gt;of software.&lt;br/&gt;&lt;br/&gt;This work will create new techniques for scheduling test cases during&lt;br/&gt;continuous testing based on their historical effectiveness, the relationships&lt;br/&gt;between tests and changes, and their potential for execution; these techniques&lt;br/&gt;will also be able to adapt to release practices and environmental changes, and&lt;br/&gt;will be supported by analysis techniques that are efficient enough for use in&lt;br/&gt;CI testing situations.  The work will involve studying the effect of problems&lt;br/&gt;related to test design, and developing lightweight static and dynamic&lt;br/&gt;techniques for analyzing test cases and test suites to determine whether they&lt;br/&gt;may be problematic. The work will lead to the creation of new approaches for&lt;br/&gt;test monitoring, and the definition of new metrics that capture test and test&lt;br/&gt;suite attributes and states for the assessment of testing results in continuous&lt;br/&gt;testing processes.  Finally, the work will result in the creation of&lt;br/&gt;infrastructure to support rigorous empirical assessments of the techniques&lt;br/&gt;developed.</AbstractNarration>
<MinAmdLetterDate>07/20/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1526652</AwardID>
<Investigator>
<FirstName>Gregg</FirstName>
<LastName>Rothermel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregg Rothermel</PI_FULL_NAME>
<EmailAddress>gerother@ncsu.edu</EmailAddress>
<PI_PHON>4024296639</PI_PHON>
<NSF_ID>000092147</NSF_ID>
<StartDate>07/20/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sebastian</FirstName>
<LastName>Elbaum</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sebastian G Elbaum</PI_FULL_NAME>
<EmailAddress>selbaum@virginia.edu</EmailAddress>
<PI_PHON>4342435213</PI_PHON>
<NSF_ID>000412723</NSF_ID>
<StartDate>07/20/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName>Lincoln</CityName>
<StateCode>NE</StateCode>
<ZipCode>685880150</ZipCode>
<StreetAddress><![CDATA[256 Avery Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~425000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The work we have performed has provided a greater understanding of the challenges associated with testing in continuous integration environments, and produced highly novel approaches and practices for improving the cost-effectiveness of testing techniques operating in such environments, which are far more ubiquitous in practice than in research.</p> <p>This project resulted in several significant findings and outcomes; we summarize these next.</p> <p>In continuous integration environments, test cases, test suites and commits are commonly queued for execution. Test results obtained during queuing time can be continuously used to significantly improve the decisions made regarding whether and when to execute a queued test.</p> <p>Prioritization at the level of test suites is no longer sustainable or useful when operating under large and frequently updated code bases. &nbsp;Instead, approaches that operate at the commit level result in substantially greater gains by enabling larger changes in the testing schedule.</p> <p>Continuous integration testing requires new metrics to account for the richer test state space that occurs in continuous integration environments. That test state space is no longer binary and includes states such as "queued", "scheduled", "under execution", "failed to build", and "passed/failed". Test executions can also be viewed at several different levels of granularity such as the test, suite, build, and configuration levels.</p> <p>Keeping code changes small and integrating code frequently is often a wise approach for developers; however, the continuous integration server may be more efficient operating on different scales (from single commits to clusters of related commits) at different times and under different contexts to avoid redundant operations.</p> <p>Lightweight static analyses can be cost-effective for pinpointing patterns associated with faults in tests such as those associated with execution order dependencies, time coordination, and hard-coded paths. Such faults in tests can cause incorrect or flaky test behavior, or lead to unnecessary inefficiencies.</p> <p>Test suite type does affect the effectiveness of selection techniques, but it can do so differently across different types of test dependence tracking techniques, so different selection techniques may be required across different types of test suites.</p> <p>The collection and curation of continuous integration testing datasets, such as those ones produced through this project, will be crucial to assist in the evolution of test design and execution techniques.</p> <p>Overall, the body of knowledge generated by this work should allow software development organizations - especially those that operate at large scale and speed - to utilize and maintain tests more cost-effectively and reduce the incidence of faults released into production software. This also positively affects (even if indirectly) all segments of society that depend on software.</p> <p>In addition, the work under this proposal resulted in techniques to reduce users&rsquo; effort when performing program synthesis, and to automatically assess the quality of recommender systems beyond the traditional metrics which offer only coarse, one-dimensional views of system performance.&nbsp; Last, this work led to the identification of important misalignments between the techniques and the tools built by the research community for use in automated test case generation and program analysis, and the remedies that address these misalignments.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/08/2018<br>      Modified by: Sebastian&nbsp;G&nbsp;Elbaum</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The work we have performed has provided a greater understanding of the challenges associated with testing in continuous integration environments, and produced highly novel approaches and practices for improving the cost-effectiveness of testing techniques operating in such environments, which are far more ubiquitous in practice than in research.  This project resulted in several significant findings and outcomes; we summarize these next.  In continuous integration environments, test cases, test suites and commits are commonly queued for execution. Test results obtained during queuing time can be continuously used to significantly improve the decisions made regarding whether and when to execute a queued test.  Prioritization at the level of test suites is no longer sustainable or useful when operating under large and frequently updated code bases.  Instead, approaches that operate at the commit level result in substantially greater gains by enabling larger changes in the testing schedule.  Continuous integration testing requires new metrics to account for the richer test state space that occurs in continuous integration environments. That test state space is no longer binary and includes states such as "queued", "scheduled", "under execution", "failed to build", and "passed/failed". Test executions can also be viewed at several different levels of granularity such as the test, suite, build, and configuration levels.  Keeping code changes small and integrating code frequently is often a wise approach for developers; however, the continuous integration server may be more efficient operating on different scales (from single commits to clusters of related commits) at different times and under different contexts to avoid redundant operations.  Lightweight static analyses can be cost-effective for pinpointing patterns associated with faults in tests such as those associated with execution order dependencies, time coordination, and hard-coded paths. Such faults in tests can cause incorrect or flaky test behavior, or lead to unnecessary inefficiencies.  Test suite type does affect the effectiveness of selection techniques, but it can do so differently across different types of test dependence tracking techniques, so different selection techniques may be required across different types of test suites.  The collection and curation of continuous integration testing datasets, such as those ones produced through this project, will be crucial to assist in the evolution of test design and execution techniques.  Overall, the body of knowledge generated by this work should allow software development organizations - especially those that operate at large scale and speed - to utilize and maintain tests more cost-effectively and reduce the incidence of faults released into production software. This also positively affects (even if indirectly) all segments of society that depend on software.  In addition, the work under this proposal resulted in techniques to reduce users? effort when performing program synthesis, and to automatically assess the quality of recommender systems beyond the traditional metrics which offer only coarse, one-dimensional views of system performance.  Last, this work led to the identification of important misalignments between the techniques and the tools built by the research community for use in automated test case generation and program analysis, and the remedies that address these misalignments.           Last Modified: 08/08/2018       Submitted by: Sebastian G Elbaum]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
