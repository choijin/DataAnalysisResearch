<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Novel microLIDAR Design and Sensing Algorithms for Flapping-Wing Micro-Aerial Vehicles</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>406477.00</AwardTotalIntnAmount>
<AwardAmount>414477</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project makes it possible for a tiny robotic bee to sense its distance to any nearby object. Such depth sensing for a small robot insect pushes the limits of sensor and algorithm design in terms size, weight, computing, and power. The key idea is joint design; every part of the robotic insect is optimized together, from wing design and optics to intelligent algorithms and efficient computation. This is possible by inter-disciplinary work across scientists and engineers from diverse backgrounds. The lessons learned through this project can be applied to transform other applications that involve small devices including medical sensors and endoscope imaging, smart homes and the internet of things, agricultural and industrial monitoring systems, and mobile vision for search and rescue.&lt;br/&gt;&lt;br/&gt;Lidar sensing has enabled large robotic cars to navigate complex environments. This proposal introduces designs for "micro-lidar" that can be used on insect-scale aerial robots. Making micro-lidar work on small platforms involves four intertwined research thrusts. The first thrust uses MEMS mirrors and wide-angle optics to sense and modulate the laser pulses. The second thrust is adapting signal processing algorithms to estimate range data at this scale. The third thrust is developing novel perception and navigation algorithms to map the indoor environments using a micro-aerial vehicle. The fourth thrust is to improve robotic insect flight to allow novel manipulations that require knowledge of the surrounding range map. The utility of these sensors will be demonstrated on the robobee for novel maneuvers and building topo-feature maps of indoor environments.</AbstractNarration>
<MinAmdLetterDate>06/08/2015</MinAmdLetterDate>
<MaxAmdLetterDate>03/01/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1514154</AwardID>
<Investigator>
<FirstName>Huikai</FirstName>
<LastName>Xie</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Huikai Xie</PI_FULL_NAME>
<EmailAddress>hkxie@ece.ufl.edu</EmailAddress>
<PI_PHON>3528460441</PI_PHON>
<NSF_ID>000095277</NSF_ID>
<StartDate>06/08/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sanjeev</FirstName>
<LastName>Koppal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sanjeev Koppal</PI_FULL_NAME>
<EmailAddress>sjkoppal@ece.ufl.edu</EmailAddress>
<PI_PHON>3523923516</PI_PHON>
<NSF_ID>000668802</NSF_ID>
<StartDate>06/08/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName/>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~133581</FUND_OBLG>
<FUND_OBLG>2016~134964</FUND_OBLG>
<FUND_OBLG>2017~137932</FUND_OBLG>
<FUND_OBLG>2018~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project is about small depth sensors. A depth sensor gets the 3D  locations of objects in a scene, and making such sensor small would  impact robotics and mobile devices. These intelligent systems would be  able to reason about how far away objects are around them, and this  would beneficially impact applications. In this project we focus on a type of depth sensor called a LIDAR sensor.</p> <p>Our key idea is to use a  concept from regular, color imaging, called zoom. In a normal camera,  zoom allows the imaging of objects that are far away. Optical zoom  lenses are usually large, which is why you see giant lenses on those  taking images of sportspeople in arenas.</p> <p>We can get the same effect from using a tiny mirror, called a micro electro mechanical (MEMS) mirror, to zoom onto objects of interest. We have built both novel MEMS mirrors and accompanying algorithms to do this. In the figure, we show a scene in the lab, and demonstrate that our LIDAR sensor can zoom into the object of interest, in this case the flower.</p> <p>A small sensor really needs zoom, because the number of measurements that are possible on a low-power platfrom, and the compute power available to process these, are all limited resources. Depth sensing zoom can place limited measurements onto objects of interest, avoiding sensing uninteresting parts of the scene and saving power and time.</p><br> <p>            Last Modified: 07/19/2019<br>      Modified by: Sanjeev&nbsp;Koppal</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1514154/1514154_10367991_1563551542018_LidarZoom--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1514154/1514154_10367991_1563551542018_LidarZoom--rgov-800width.jpg" title="LIDAR Zoom"><img src="/por/images/Reports/POR/2019/1514154/1514154_10367991_1563551542018_LidarZoom--rgov-66x44.jpg" alt="LIDAR Zoom"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We create a LIDAR sensor and accompanying algorithms that allow zooming into objects of interest.</div> <div class="imageCredit">Sanjeev Koppal</div> <div class="imageSubmitted">Sanjeev&nbsp;Koppal</div> <div class="imageTitle">LIDAR Zoom</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project is about small depth sensors. A depth sensor gets the 3D  locations of objects in a scene, and making such sensor small would  impact robotics and mobile devices. These intelligent systems would be  able to reason about how far away objects are around them, and this  would beneficially impact applications. In this project we focus on a type of depth sensor called a LIDAR sensor.  Our key idea is to use a  concept from regular, color imaging, called zoom. In a normal camera,  zoom allows the imaging of objects that are far away. Optical zoom  lenses are usually large, which is why you see giant lenses on those  taking images of sportspeople in arenas.  We can get the same effect from using a tiny mirror, called a micro electro mechanical (MEMS) mirror, to zoom onto objects of interest. We have built both novel MEMS mirrors and accompanying algorithms to do this. In the figure, we show a scene in the lab, and demonstrate that our LIDAR sensor can zoom into the object of interest, in this case the flower.  A small sensor really needs zoom, because the number of measurements that are possible on a low-power platfrom, and the compute power available to process these, are all limited resources. Depth sensing zoom can place limited measurements onto objects of interest, avoiding sensing uninteresting parts of the scene and saving power and time.       Last Modified: 07/19/2019       Submitted by: Sanjeev Koppal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
