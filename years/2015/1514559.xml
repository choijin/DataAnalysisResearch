<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Smoothing Methods in Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>186999.00</AwardTotalIntnAmount>
<AwardAmount>186999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Pedro Embid</SignBlockName>
<PO_EMAI>pembid@nsf.gov</PO_EMAI>
<PO_PHON>7032924859</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project concerns mathematical optimization, a field that has experienced explosive growth of over the past thirty years due to its wide applicability in science, engineering, business, and medicine. Contributing factors include the advent of the internet, advances in computational power and computing architectures, the availability of very large data sets, as well as advances in science, engineering, communication, and business. These developments have created a fertile ground for the emergence of new applications and data acquisition modalities, in addition to new methods for data management, interpretation, and modeling. These are the driving forces behind big data and machine learning research. In addition, there is a greater urgency in many disciplines for addressing questions concerning design, efficiency, risk, and inference, as well as model selection, system identification, and error and uncertainty quantification. This research project aims to develop new methods in non-smooth optimization and to study the practical impact of these methods.  The research will emphasize underlying optimization tools, including model development, the design of numerical solution procedures, and the assessment and quantification of model validity, sensitivity, robustness, and uncertainty.&lt;br/&gt;&lt;br/&gt;This project concerns the methods and theory associated with the use of smoothing techniques in optimization. In order to extract solutions having pre-specified properties, objective functions in modern optimization problems are often non-differentiable, with the non-differentiability being a key descriptive component. In addition, non-differentiability is present in an essential way when the problem is constrained. Problems possessing non-differentiability appear across a broad spectrum of applications. These include robust statistical modeling, regularization formulations to encode prior information, system identification, sparsity optimization, matrix completion, semi-definite programming, and any problem possessing constraints. In addition, many modern problems are very large scale.  Hence, there is a focus on the development of fast optimization algorithms for large-scale applications in the presence of non-smooth/non-convex objectives. Smoothing methods are designed to approximate these non-smooth problems, with the goal of rapidly obtaining good approximate solutions. This project is devoted to the development and understanding of new and emerging smoothing methods for non-smooth optimization, to providing a mathematical foundation for these methods, and to studying the practical impact of these methods on a range of applications. Primary objectives include (1) developing a framework for convergence analysis, (2) extending results for convex problems to non-convex problems, (3) providing a calculus for smoothing techniques, (4) developing error bounds using duality theory, (5) continued development of the level set method for optimization, and (6) consideration of parametrized optimal value functions.</AbstractNarration>
<MinAmdLetterDate>09/03/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1514559</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Burke</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James V Burke</PI_FULL_NAME>
<EmailAddress>jvburke@uw.edu</EmailAddress>
<PI_PHON>2065436183</PI_PHON>
<NSF_ID>000311455</NSF_ID>
<StartDate>09/03/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981954350</ZipCode>
<StreetAddress><![CDATA[Box 354350 Padelford Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1266</Code>
<Text>APPLIED MATHEMATICS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~186999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">Progress in data-science, machine learning, signal processing as well as problems of system identification, stability and sensitivity analysis in engineering design depends heavily on the theory, techniques, and numerical methods for the nonsmooth optimization.&nbsp; In this research, the PI and his co--authors have made fundamental contributions to several areas of nonsmooth optimization where non-differentiability arises as an essential descriptive component. Below are brief descriptions of these contributions.</p> <p class="p1">Foundations of gauge and perspective duality: Gauge functions generalize the notion of metric to incorporate specified geometric features. Gauges occur in many modern large-scale optimization problems and are used to extract desired solution properties. Gauge duals were proposed by Robert Freund in 1987, however, thier scope remained unexplored, particularly the connection to sensitivity theory, a hallmark of Lagrangian duality, was completely unknown. Our work filled in these gaps and goes further by introducing ``perspective duality'' expanding the applicable problems classes.</p> <p class="p1">Level set methods for convex optimization: Constraint complexity determines the difficulty of many large-scale optimization problems. In this research we show how to reduce this complexity by exchanging the objective with a constraint to arrive at a problem with low iteration complexity. The new problems are parametrized by a level set parameter yielding a convex optimal value function. The original problem is then solved by appling our new Newton method to a scalar nonsmooth convex equation. The overall algorithm has iteration complexity only slightly greater than the parametrized problems.</p> <p class="p1">Generalized Kalman smoothing: Kalman smoothing has been a bedrock tool for state estimation with uncertain measurments since its development in the 1960's. However, it depends on Gaussian noise assumptions. Our work is a continuation of our groundbreaking extensions of Kalman's methodology to state constrained problems with piecewise linear-quadratic&nbsp;log-concave densities.&nbsp;&nbsp;</p> <p class="p1">Generalized system identification with stable splines: Linear system identification uses statistical methods to build linear models of dynamical systems from measured data. We extend linear system identification to a wide class of nonsmooth stable spline estimators, where regularization functionals and data misfits can be selected from a rich set of piecewise linear-quadratic (PLQ) penalties. The&nbsp; approach allows the inclusion of linear inequality constraints on the output response.</p> <p class="p1">Variational analysis of convexly generated spectral max functions: The spectral abscissa is the largest real part of an eigenvalue of a matrix and the spectral radius is the largest modulus; both examples of convexly generated spectral max functions. These mappings arise in the control and stabilization of dynamical systems and are non-Lipschitzian. In this work we characterized their regular subdifferential and show subdifferentially regular in the sense of Clarke if and only if all active eigenvalues are nonderogatory. These results yield new variational results for both the spectral abscissa and radius.&nbsp;</p> <p class="p1">Variational properties of matrix functions via the generalized matrix-fractional function: We introduce generalized matrix-fractional (GMF) functions as a means to solve a broad range of matrix optimization problems. GMF functions yield deep insight into the variational structure of many non-smooth objectives such as&nbsp; the Ky-Fan norms, weighted sums of norms, matrix gauge functionals,&nbsp; and variational gram functions. This research provides the theoretical foundation for the variational properties of the GMF functions and those functions that can be represented as the partial infimal convolution with GMF functions. In particular we establish the smoothness of GMF functions on the interior of their domains thus paving the way for a new smoothing approach to a wide range of nonsmooth regularizers in matrix optimization.</p> <p class="p1">Epi-convergence properties of smoothing by infimal convolutions: In this work we study parametrized families of infimal convolution smoothing functions that epi-converge to an underlying nonsmooth mapping. We establish the gradient consistency of these smooth approximations. In the fully convex case, gradient consistency is a consequence of Attouch's Theorem which establishes the graph convergence of the gradient mapping.&nbsp; In the convex-composite case, we also establish the graph convergence of the gradient mapping under a standard regularity hypothesis thus extending Attouch's Theorem to convex-composite functions.&nbsp;</p> <p class="p1">Subdifferentiation and smoothing of nonsmooth integral functions: The subdifferential calculus for the expectation of nonsmooth integrands is a critical challenge for stochastic optimization. It is known that, for Clarke regular integrands, the Clarke subdifferential equals the expectation of their Clarke subdifferential, but, this is generally false for non-regular integrands. In this work we develop the mathematical foundations for approximating Clarke subgradients for the expectation of random integrands by smoothing methods. The results are applied to a class of measurable composite max integrands containing the non-regular integrands that appear in stochastic complementarity and statistical learning. We give precise estimates for the distance of the gradient of the smoothed expectation functional to the Clarke subdifferential of the approximated expectation functional.</p> <p class="p1">This research involved 6 graduate students resulting in 3 Ph.D.s. In addition, the research produced 9 papers whose results were presented at numerous seminars and conferences.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/29/2020<br>      Modified by: James&nbsp;V&nbsp;Burke</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Progress in data-science, machine learning, signal processing as well as problems of system identification, stability and sensitivity analysis in engineering design depends heavily on the theory, techniques, and numerical methods for the nonsmooth optimization.  In this research, the PI and his co--authors have made fundamental contributions to several areas of nonsmooth optimization where non-differentiability arises as an essential descriptive component. Below are brief descriptions of these contributions. Foundations of gauge and perspective duality: Gauge functions generalize the notion of metric to incorporate specified geometric features. Gauges occur in many modern large-scale optimization problems and are used to extract desired solution properties. Gauge duals were proposed by Robert Freund in 1987, however, thier scope remained unexplored, particularly the connection to sensitivity theory, a hallmark of Lagrangian duality, was completely unknown. Our work filled in these gaps and goes further by introducing ``perspective duality'' expanding the applicable problems classes. Level set methods for convex optimization: Constraint complexity determines the difficulty of many large-scale optimization problems. In this research we show how to reduce this complexity by exchanging the objective with a constraint to arrive at a problem with low iteration complexity. The new problems are parametrized by a level set parameter yielding a convex optimal value function. The original problem is then solved by appling our new Newton method to a scalar nonsmooth convex equation. The overall algorithm has iteration complexity only slightly greater than the parametrized problems. Generalized Kalman smoothing: Kalman smoothing has been a bedrock tool for state estimation with uncertain measurments since its development in the 1960's. However, it depends on Gaussian noise assumptions. Our work is a continuation of our groundbreaking extensions of Kalman's methodology to state constrained problems with piecewise linear-quadratic log-concave densities.   Generalized system identification with stable splines: Linear system identification uses statistical methods to build linear models of dynamical systems from measured data. We extend linear system identification to a wide class of nonsmooth stable spline estimators, where regularization functionals and data misfits can be selected from a rich set of piecewise linear-quadratic (PLQ) penalties. The  approach allows the inclusion of linear inequality constraints on the output response. Variational analysis of convexly generated spectral max functions: The spectral abscissa is the largest real part of an eigenvalue of a matrix and the spectral radius is the largest modulus; both examples of convexly generated spectral max functions. These mappings arise in the control and stabilization of dynamical systems and are non-Lipschitzian. In this work we characterized their regular subdifferential and show subdifferentially regular in the sense of Clarke if and only if all active eigenvalues are nonderogatory. These results yield new variational results for both the spectral abscissa and radius.  Variational properties of matrix functions via the generalized matrix-fractional function: We introduce generalized matrix-fractional (GMF) functions as a means to solve a broad range of matrix optimization problems. GMF functions yield deep insight into the variational structure of many non-smooth objectives such as  the Ky-Fan norms, weighted sums of norms, matrix gauge functionals,  and variational gram functions. This research provides the theoretical foundation for the variational properties of the GMF functions and those functions that can be represented as the partial infimal convolution with GMF functions. In particular we establish the smoothness of GMF functions on the interior of their domains thus paving the way for a new smoothing approach to a wide range of nonsmooth regularizers in matrix optimization. Epi-convergence properties of smoothing by infimal convolutions: In this work we study parametrized families of infimal convolution smoothing functions that epi-converge to an underlying nonsmooth mapping. We establish the gradient consistency of these smooth approximations. In the fully convex case, gradient consistency is a consequence of Attouch's Theorem which establishes the graph convergence of the gradient mapping.  In the convex-composite case, we also establish the graph convergence of the gradient mapping under a standard regularity hypothesis thus extending Attouch's Theorem to convex-composite functions.  Subdifferentiation and smoothing of nonsmooth integral functions: The subdifferential calculus for the expectation of nonsmooth integrands is a critical challenge for stochastic optimization. It is known that, for Clarke regular integrands, the Clarke subdifferential equals the expectation of their Clarke subdifferential, but, this is generally false for non-regular integrands. In this work we develop the mathematical foundations for approximating Clarke subgradients for the expectation of random integrands by smoothing methods. The results are applied to a class of measurable composite max integrands containing the non-regular integrands that appear in stochastic complementarity and statistical learning. We give precise estimates for the distance of the gradient of the smoothed expectation functional to the Clarke subdifferential of the approximated expectation functional. This research involved 6 graduate students resulting in 3 Ph.D.s. In addition, the research produced 9 papers whose results were presented at numerous seminars and conferences.          Last Modified: 12/29/2020       Submitted by: James V Burke]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
