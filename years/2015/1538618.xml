<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>VEC: Medium: The 3D Memex for Virtual Teleportation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>616000.00</AwardTotalIntnAmount>
<AwardAmount>739000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Although virtual and augmented reality headsets are finally on the brink of mass consumer availability, very little is known about how to capture the world with the fidelity needed to enable the most exciting applications of VR and AR: designing a home remodeling plan by adding fixtures or removing walls with a flick of a finger, teleporting to Hawaii to "walk through" a vacation rental, or having a conversation with a close relative on their couch, as if you're literally in the same room. Achieving such teleportation scenarios will require 3D scene modeling and rendering that go beyond the current state of the art. The primary factor holding back progress on the 3D front is lack of data.  There are billions of photos available online, and this has revolutionized image understanding.  Since publicly available 3D data is comparably scarce, this project takes a data-driven approach to the research problem of virtual teleportation.  Its centerpiece is the creation of a 3D Memex:  a vast, evolving, freely available, online repository of 3D geometry, segmentations, and semantic labels.  The 3D Memex will capture a broad range of household objects and scenes.  Beyond geometry and labels, the 3D Memex will describe articulations and degrees of freedom.&lt;br/&gt;&lt;br/&gt;This research will leverage this new database, producing 3D-Memex-aware scanning and modeling algorithms to bring the state of the art closer to high quality immersive VR experiences.  Memex-aware reconstruction algorithms will infer complete scene models, extrapolating far beyond what the camera sees, by matching to similar scenes in the 3D Memex and completing hidden geometry and texture.  Instead of reconstructing static point clouds, it will build complete models and kinematic degrees of freedom (i.e., doors will open and close, chairs move, etc.)  These breakthrough capabilities will enable important teleportation scenarios to be prototyped in the project. The teleportation capabilities will enable new and better applications for all, including real estate, tourism, and better interpersonal communication through live telepresence.  All data, along with visualization and labeling tools, will be released freely online for the broader community, along with newly developed, next-generation 3D scanning tools.  Finally, the project will advance educational objectives by engaging undergraduates directly in the ongoing research, and through a capstone course for undergraduates and graduates that will help seed the 3D Memex and expose the students to the rapidly growing technologies and algorithms for virtual reality.</AbstractNarration>
<MinAmdLetterDate>09/14/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1538618</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Curless</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian L Curless</PI_FULL_NAME>
<EmailAddress>curless@cs.washington.edu</EmailAddress>
<PI_PHON>2066853796</PI_PHON>
<NSF_ID>000417360</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Seitz</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven M Seitz</PI_FULL_NAME>
<EmailAddress>seitz@cs.washington.edu</EmailAddress>
<PI_PHON>2066169431</PI_PHON>
<NSF_ID>000195084</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dieter</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dieter Fox</PI_FULL_NAME>
<EmailAddress>fox@cs.washington.edu</EmailAddress>
<PI_PHON>2066852517</PI_PHON>
<NSF_ID>000210667</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Irena</FirstName>
<LastName>Kemelmacher-Shlizerman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Irena Kemelmacher-Shlizerman</PI_FULL_NAME>
<EmailAddress>kemelmi@cs.washington.edu</EmailAddress>
<PI_PHON>2066160621</PI_PHON>
<NSF_ID>000649417</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>002Z</Code>
<Text>Intel/NSF VEC Partnership</Text>
</ProgramReference>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~211289</FUND_OBLG>
<FUND_OBLG>2016~404711</FUND_OBLG>
<FUND_OBLG>2018~123000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-4088a930-7fff-4d98-d1a2-c11a557eb1e3"> </span></p> <p dir="ltr"><span>In this NSF/Intel funded effort, we have advanced the state of the art in reconstructing the shape and appearance of people and places to create a wide variety of novel experiences.&nbsp; The advent of new 3D sensing technology, much of it now used in gaming consoles and even smartphones, the explosion of databases of 3D object models and materials, and advances in deep learning have fueled our progress.&nbsp; We have developed new methods for reconstructing shape, materials, and lighting at a wide variety of scales, from individual objects to entire rooms in a home, enabling high quality virtual visualization of these objects and spaces, including making changes to spaces in augmented reality.&nbsp; We have created new ways of reconstructing people, ranging from complex hair geometry from videos to whole, clothed bodies from a single image, allowing us to bring a person to life from just one photo.  Finally, we have introduced new methods for reconstructing sporting events, such as soccer and basketball, in 3D, from a single video, by leveraging data readily extracted from computer games.&nbsp; These sports reconstructions enable viewers with AR headsets to watch a game play out on a tabletop in front them.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>These research advances lay the groundwork for changing the way we interact with photos, virtually explore places and spaces -- including refurnishing and remodeling them -- and for how we will experience sporting events in the future.&nbsp; These new interactions and experiences are particularly compelling in augmented and virtual reality.  As part of our effort, we have educated graduate students and postdocs through research projects, but we have also engaged undergraduates in research and moreover have launched new courses and programs, notably a Virtual Reality Capstone course.&nbsp; In this course, undergraduates learn about VR from the ground up, hear from leaders around the country about current progress in VR, and develop new applications ranging from games to experiences for education and social good.  Finally, the research and educational efforts supported by the grant helped to launch a new industry-sponsored center, the UW Reality Lab, focused on advancing the state of the art in virtual and augmented reality and educating the next generation of researchers, developers, and creators for this field.</span></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/20/2019<br>      Modified by: Brian&nbsp;L&nbsp;Curless</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   In this NSF/Intel funded effort, we have advanced the state of the art in reconstructing the shape and appearance of people and places to create a wide variety of novel experiences.  The advent of new 3D sensing technology, much of it now used in gaming consoles and even smartphones, the explosion of databases of 3D object models and materials, and advances in deep learning have fueled our progress.  We have developed new methods for reconstructing shape, materials, and lighting at a wide variety of scales, from individual objects to entire rooms in a home, enabling high quality virtual visualization of these objects and spaces, including making changes to spaces in augmented reality.  We have created new ways of reconstructing people, ranging from complex hair geometry from videos to whole, clothed bodies from a single image, allowing us to bring a person to life from just one photo.  Finally, we have introduced new methods for reconstructing sporting events, such as soccer and basketball, in 3D, from a single video, by leveraging data readily extracted from computer games.  These sports reconstructions enable viewers with AR headsets to watch a game play out on a tabletop in front them.    These research advances lay the groundwork for changing the way we interact with photos, virtually explore places and spaces -- including refurnishing and remodeling them -- and for how we will experience sporting events in the future.  These new interactions and experiences are particularly compelling in augmented and virtual reality.  As part of our effort, we have educated graduate students and postdocs through research projects, but we have also engaged undergraduates in research and moreover have launched new courses and programs, notably a Virtual Reality Capstone course.  In this course, undergraduates learn about VR from the ground up, hear from leaders around the country about current progress in VR, and develop new applications ranging from games to experiences for education and social good.  Finally, the research and educational efforts supported by the grant helped to launch a new industry-sponsored center, the UW Reality Lab, focused on advancing the state of the art in virtual and augmented reality and educating the next generation of researchers, developers, and creators for this field.                Last Modified: 12/20/2019       Submitted by: Brian L Curless]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
