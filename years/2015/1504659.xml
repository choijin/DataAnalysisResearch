<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Applying Multidimensional Item Response Theory Models to Generate an Interconnected Bank of Items for Earth System Science</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>225584.00</AwardTotalIntnAmount>
<AwardAmount>225584</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Elizabeth Rom</SignBlockName>
<PO_EMAI>elrom@nsf.gov</PO_EMAI>
<PO_PHON>7032927709</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will investigate the potential for applying high quality test development strategies to assessment of learning in Earth System Science courses taught at the undergraduate level. Assessing student learning at the collegiate level is difficult, a difficulty that stems from the diversity of U.S. colleges and universities and the diversity of curriculum taught within those institutions. The project team will seek to provide college faculty with an assessment resource that will allow them to evaluate learning in their courses through use of a common bank of questions. These questions will be designed to allow meaningful assessment across different institutional settings, from large research institutions to tribal colleges. In addition, different populations of students will be included in the study, from non-足science majors through advanced undergraduates. This work builds upon prior development of a set of geoscience-足focused questions, some of which were developed in collaboration with tribal college colleagues. This project will provide: 1) a bank of test questions that faculty from diverse institutions can use to evaluate student learning, providing a mechanism for truly understanding if and when learning is occurring; 2) a model for assessment instrument development and analysis that can be followed by colleagues within other science and engineering fields; and 3) training for faculty, particularly tribal college faculty, for development and use of assessment questions that are meaningful within their own institutional and course contexts.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will apply high quality psychometric standards to the current quantitative assessment practice utilized in higher education Earth System Science. This work will apply cutting-足edge Multidimensional Item Response Theory (MIRT) techniques to item and scale validation as well as scoring of multiple dimensions of Earth System Science understanding. Data will be collected from freshmen, students enrolled in non-足majors courses, geoscience majors enrolled in schools nationally, and students enrolled in tribal colleges. This broad data collection will allow for accurate estimation of ability measures and use of Differential Item Functioning (DIF) to evaluate potential bias. Providing mechanisms for inter-correlation of concept inventory items is vital to guarantee that measurement of learning outcomes is meaningful across educational contexts and can be effectively used to inform higher education policy and practice.</AbstractNarration>
<MinAmdLetterDate>03/25/2015</MinAmdLetterDate>
<MaxAmdLetterDate>03/25/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1504659</AwardID>
<Investigator>
<FirstName>Julie</FirstName>
<LastName>Libarkin</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Julie C Libarkin</PI_FULL_NAME>
<EmailAddress>libarkin@msu.edu</EmailAddress>
<PI_PHON>5173558369</PI_PHON>
<NSF_ID>000476945</NSF_ID>
<StartDate>03/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ryan</FirstName>
<LastName>Bowles</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ryan Bowles</PI_FULL_NAME>
<EmailAddress>bowlesr@msu.edu</EmailAddress>
<PI_PHON>5173555040</PI_PHON>
<NSF_ID>000666414</NSF_ID>
<StartDate>03/25/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488241000</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>1998</Code>
<Text>IUSE</Text>
</ProgramElement>
<ProgramReference>
<Code>8209</Code>
<Text>Improv Undergrad STEM Ed(IUSE)</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~225584</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project focused on developing an assessment that could be used to evaluate conceptual understanding of Earth&rsquo;s systems. The scope of the assessment was determined through analysis of the Next Generation Science Standards as well as four documents outlining literacy principles for the general public. Based on this analysis, five broad categories for assessment development were identified: Atmosphere, Biosphere, Hydrosphere, Deep Earth, and Earth&rsquo;s Surface.</p> <p>We generated a set of assessment items based on prior work. We then collected two data sets, one from non-science major college students and a second from individuals with some coursework in earth system science, from undergraduates through professional scientists. These data allowed us to determine the value of the generated assessment items for measuring a wide range of abilities across different types of people.</p> <p>Rasch analysis of the data set (n=1830 individuals) indicated that 37 of the generated items were good measures of the targeted topics and for the wide range of abilities in our samples. In fact, the test items covered the full range of ability, allowing differentiation of low and high ability performers. This means that the subset of items can be used in studies investigating learning within entry-level through advanced Earth system science courses. In addition, we combined items related to the atmosphere with items created by colleagues at other institutions and developed a new assessment. Items from this new assessment were also evaluated using Rasch analysis and resulted in a similar measurement of a wide range of abilities.</p> <p>In general, this work indicates that careful item design and use of Rasch approaches to evaluate item performance and validity is important for development of assessments in science. This work provides a strong case example that other researchers can follow in developing new measurements designed to evaluate student learning.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/09/2018<br>      Modified by: Julie&nbsp;C&nbsp;Libarkin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project focused on developing an assessment that could be used to evaluate conceptual understanding of Earth?s systems. The scope of the assessment was determined through analysis of the Next Generation Science Standards as well as four documents outlining literacy principles for the general public. Based on this analysis, five broad categories for assessment development were identified: Atmosphere, Biosphere, Hydrosphere, Deep Earth, and Earth?s Surface.  We generated a set of assessment items based on prior work. We then collected two data sets, one from non-science major college students and a second from individuals with some coursework in earth system science, from undergraduates through professional scientists. These data allowed us to determine the value of the generated assessment items for measuring a wide range of abilities across different types of people.  Rasch analysis of the data set (n=1830 individuals) indicated that 37 of the generated items were good measures of the targeted topics and for the wide range of abilities in our samples. In fact, the test items covered the full range of ability, allowing differentiation of low and high ability performers. This means that the subset of items can be used in studies investigating learning within entry-level through advanced Earth system science courses. In addition, we combined items related to the atmosphere with items created by colleagues at other institutions and developed a new assessment. Items from this new assessment were also evaluated using Rasch analysis and resulted in a similar measurement of a wide range of abilities.  In general, this work indicates that careful item design and use of Rasch approaches to evaluate item performance and validity is important for development of assessments in science. This work provides a strong case example that other researchers can follow in developing new measurements designed to evaluate student learning.          Last Modified: 11/09/2018       Submitted by: Julie C Libarkin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
