<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Large-Scale Distributed Learning of Noisy Labels for Images and Video</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>233995.00</AwardTotalIntnAmount>
<AwardAmount>233995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops algorithms for learning from images and video with noisy labels. The overwhelming amounts of images and video freely available online present unprecedented challenges for machine learning and computer vision research communities. They also bring tremendous opportunities and great potentials for addressing human-machine semantic gaps in image understanding and for revolutionizing our ways to index, retrieve, and interact with images and video. Inaccurate labels and mislabeled data are common problems for image and video datasets. Noisy labels would cause problems with the existing learning algorithms. This project can have broad impacts on other big data problem. The project is integrated with education by training students, ensuring broad participation of underrepresented groups, and outreaching general public.&lt;br/&gt;&lt;br/&gt;This research exploring distributed learning methods for large-scale images and video with noisy labels. The PI investigates the learning problem of loss functions with both smooth and non-smooth regularization terms, and accordingly develops new distributed learning algorithms that are capable of leveraging the abundance of images that are too large to fit into a single machine. The research has an immense potential in image and video analysis, and computer vision applications. Specifically, this research emphasizes both algorithmic and theoretic aspects by (1) developing distributed learning based approaches for optimization and learning of noisy labels; and (2) investigating issues such as guaranteed convergence, convergence rate, and scalability. This work provides new methods that are widely applicable to many economically, medically and scientifically important large-scale datasets for novel discoveries across many domains.</AbstractNarration>
<MinAmdLetterDate>08/27/2015</MinAmdLetterDate>
<MaxAmdLetterDate>03/28/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1554264</AwardID>
<Investigator>
<FirstName>Xue-Wen</FirstName>
<LastName>Chen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xue-Wen Chen</PI_FULL_NAME>
<EmailAddress>xwchen@wayne.edu</EmailAddress>
<PI_PHON>3135772477</PI_PHON>
<NSF_ID>000275410</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate>03/28/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Zichun</FirstName>
<LastName>Zhong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zichun Zhong</PI_FULL_NAME>
<EmailAddress>zichunzhong@wayne.edu</EmailAddress>
<PI_PHON>3135779530</PI_PHON>
<NSF_ID>000703881</NSF_ID>
<StartDate>03/28/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Wayne State University</Name>
<CityName>Detroit</CityName>
<ZipCode>482023622</ZipCode>
<PhoneNumber>3135772424</PhoneNumber>
<StreetAddress>5057 Woodward</StreetAddress>
<StreetAddress2><![CDATA[6th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001962224</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WAYNE STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001962224</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Wayne State University]]></Name>
<CityName/>
<StateCode>MI</StateCode>
<ZipCode>482023622</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~233995</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>***General intellectual merit and broader of impacts:</strong></p> <p>With the rapid advent of new technologies such as social media and mobile sensors, the number of digital images and video has been growing exponentially. Currently, images and video available in the Web comes with sheer sizes where most of them may not be labeled correctly because of the labor-expensive and time-consuming process or simply because of the difficulties involved in labeling data. Consequently, it is increasingly critical to leverage the abundance of image and video data, which is inherently a Big Data problem, while alleviating the effect of incorrect labels. The proposed methods have provided novel ways to deal with noisy labels while taking advantage of large volume of unlabeled data. There are many related applications that involve large-scale and noisy data. The proposed method will have impact on various applications such as image annotation and video processing.</p> <p>&nbsp;</p> <p><strong>***Summary of outcomes of publications:</strong></p> <p>Several research results are published in conference proceedings and journals (e.g., ICDM 2016, Pattern Recognition Letters 2017) and presented in professional conferences, Ph.D. dissertations, or are currently under consideration in conferences/journals (e.g., ACCV 2018, AAAI 2019, CILS, etc.).</p> <p>&nbsp;</p> <p><strong>Ph.D. dissertations:</strong></p> <p><strong><em>Zeyad Hailat: &ldquo;Deep Learning Methods for Visual Object Recognition&rdquo;.</em></strong></p> <p>First, we present a new Hybrid Residual Network Method that exploits the power of both supervised and unsupervised deep learning methods into a single deep supervised learning model. Second, we propose a deep semi-supervised learning method, which originates from its nature in employing a limited number of labeled training examples in conjunction with sufficiently large unlabeled examples to create a classification model. Finally, we introduce a new teacher/student semi-supervised deep learning method.</p> <p>&nbsp;</p> <p><strong><em>Elaheh Rashedi: &ldquo;Learning Convolutional Neural Network for Face Verification&rdquo;.</em></strong></p> <p>In the proposed system, we present a Detection-Verification-Tracking method which accomplishes the long-term face tracking task through the collaboration of face detection, face verification, and (short-term) face tracking. Secondly, we present a successful automatic video collection approach to generate a large scale video training dataset. Lastly, we introduce a stream-based ConvNet architecture for video face verification task.</p> <p>&nbsp;</p> <p><strong>Ph.D. dissertation prospectus report:</strong></p> <p><strong><em>Elaheh Barati: &ldquo;Attention-based Models for Deep Reinforcement Learning&rdquo;.</em></strong></p> <p>In this work, we design attention-based models for challenging tasks including navigation, autonomous driving, and video captioning. In these tasks, deep reinforcement learning algorithms facilitate training of their sophisticated models, and the attention mechanism serves different purposes. Our model can attend over different views of the environment provided by different available cameras to decide about best actions.</p> <p>&nbsp;</p> <p><strong>Journal and conference papers:</strong></p> <p><strong><em>Elaheh Barati, Xue-wen Chen, &ldquo;Attention-based Deep Reinforcement Learning for Multi-view Environments,&rdquo; submitted in ACCV 2018.&nbsp;</em></strong></p> <p>In this work, we present a novel attention-based deep reinforcement learning method in a multi-view environment in which each view can provide various representative information about the environment. We evaluate the performance of our method on TORCS racing car simulator and three other complex 3D environments with obstacles.</p> <p>&nbsp;</p> <p><strong><em>Elaheh Barati, Xue-wen Chen, &ldquo;Deep Adversarial Network for Event-based Video Captioning,&rdquo; submitted in AAAI 2019.</em></strong></p> <p>In this work, we target at captioning a long video through generating sentences for its temporal regions by accounting for the correlation and overlapping in the video. We employ an attention mechanism in the scheduler module to attend over events correlated to a temporal region to obtain a visual representation of that region.</p> <p>&nbsp;</p> <p><strong><em>Zeyu Li, Seongho Kim, Sikai Zhong, Zichun Zhong, Ikuko Kato, Xiang Zhang, &ldquo;Coherent Point Drift Peak Alignment Algorithms Using Distance and Similarity Measures for Two-Dimensional Mass Spectrometry Data,&rdquo; submitted in Chemometrics and Intelligent Laboratory Systems (CILS).</em></strong></p> <p>Almost all existing algorithms have been focused on a local alignment and so are suffering from low accuracy especially when aligning dense biologicaldata with many peaks. We have developed four global peak alignment algorithms using coherent point drift point matching algorithms for mass spectrometry data.</p> <p>&nbsp;</p> <p><strong><em>Melih Aslan, Zeyad Hailat, Tarik K. Alafif, and Xuewen Chen, &ldquo;Multi-channel multimodel feature learning for face recognition,&rdquo; Pattern Recognition Letters 85 (2017): 79-83.</em></strong></p> <p>This work aims to study how multiple face regions/channels and multiple models (e.g., hand-crafted and unsupervised learning methods) answer to the face recognition problem. The proposed method also uses the advantage of K-means clustering and histogram of gradients to boost the recognition rates.</p> <p>&nbsp;</p> <p><strong><em>Ishan Jindal, Matthew Nokleby, and Xue-wen Chen, &ldquo;Learning Deep Networks from Noisy Labels with Dropout Regularization,&rdquo; IEEE 16th International conference on Data Mining, 2016.</em></strong></p> <p>Large datasets often have unreliable labels and classi?ers trained on mislabeled datasets often exhibit poor performance. We present a simple, effective technique for accounting for label noise when training deep neural networks. We augment a standard deep network with a softmax layer that models the label noise statistics. Then, we train the deep network and noise model jointly via end-to-end stochastic gradient descent on different datasets.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/07/2018<br>      Modified by: Zichun&nbsp;Zhong</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ ***General intellectual merit and broader of impacts:  With the rapid advent of new technologies such as social media and mobile sensors, the number of digital images and video has been growing exponentially. Currently, images and video available in the Web comes with sheer sizes where most of them may not be labeled correctly because of the labor-expensive and time-consuming process or simply because of the difficulties involved in labeling data. Consequently, it is increasingly critical to leverage the abundance of image and video data, which is inherently a Big Data problem, while alleviating the effect of incorrect labels. The proposed methods have provided novel ways to deal with noisy labels while taking advantage of large volume of unlabeled data. There are many related applications that involve large-scale and noisy data. The proposed method will have impact on various applications such as image annotation and video processing.     ***Summary of outcomes of publications:  Several research results are published in conference proceedings and journals (e.g., ICDM 2016, Pattern Recognition Letters 2017) and presented in professional conferences, Ph.D. dissertations, or are currently under consideration in conferences/journals (e.g., ACCV 2018, AAAI 2019, CILS, etc.).     Ph.D. dissertations:  Zeyad Hailat: "Deep Learning Methods for Visual Object Recognition".  First, we present a new Hybrid Residual Network Method that exploits the power of both supervised and unsupervised deep learning methods into a single deep supervised learning model. Second, we propose a deep semi-supervised learning method, which originates from its nature in employing a limited number of labeled training examples in conjunction with sufficiently large unlabeled examples to create a classification model. Finally, we introduce a new teacher/student semi-supervised deep learning method.     Elaheh Rashedi: "Learning Convolutional Neural Network for Face Verification".  In the proposed system, we present a Detection-Verification-Tracking method which accomplishes the long-term face tracking task through the collaboration of face detection, face verification, and (short-term) face tracking. Secondly, we present a successful automatic video collection approach to generate a large scale video training dataset. Lastly, we introduce a stream-based ConvNet architecture for video face verification task.     Ph.D. dissertation prospectus report:  Elaheh Barati: "Attention-based Models for Deep Reinforcement Learning".  In this work, we design attention-based models for challenging tasks including navigation, autonomous driving, and video captioning. In these tasks, deep reinforcement learning algorithms facilitate training of their sophisticated models, and the attention mechanism serves different purposes. Our model can attend over different views of the environment provided by different available cameras to decide about best actions.     Journal and conference papers:  Elaheh Barati, Xue-wen Chen, "Attention-based Deep Reinforcement Learning for Multi-view Environments," submitted in ACCV 2018.   In this work, we present a novel attention-based deep reinforcement learning method in a multi-view environment in which each view can provide various representative information about the environment. We evaluate the performance of our method on TORCS racing car simulator and three other complex 3D environments with obstacles.     Elaheh Barati, Xue-wen Chen, "Deep Adversarial Network for Event-based Video Captioning," submitted in AAAI 2019.  In this work, we target at captioning a long video through generating sentences for its temporal regions by accounting for the correlation and overlapping in the video. We employ an attention mechanism in the scheduler module to attend over events correlated to a temporal region to obtain a visual representation of that region.     Zeyu Li, Seongho Kim, Sikai Zhong, Zichun Zhong, Ikuko Kato, Xiang Zhang, "Coherent Point Drift Peak Alignment Algorithms Using Distance and Similarity Measures for Two-Dimensional Mass Spectrometry Data," submitted in Chemometrics and Intelligent Laboratory Systems (CILS).  Almost all existing algorithms have been focused on a local alignment and so are suffering from low accuracy especially when aligning dense biologicaldata with many peaks. We have developed four global peak alignment algorithms using coherent point drift point matching algorithms for mass spectrometry data.     Melih Aslan, Zeyad Hailat, Tarik K. Alafif, and Xuewen Chen, "Multi-channel multimodel feature learning for face recognition," Pattern Recognition Letters 85 (2017): 79-83.  This work aims to study how multiple face regions/channels and multiple models (e.g., hand-crafted and unsupervised learning methods) answer to the face recognition problem. The proposed method also uses the advantage of K-means clustering and histogram of gradients to boost the recognition rates.     Ishan Jindal, Matthew Nokleby, and Xue-wen Chen, "Learning Deep Networks from Noisy Labels with Dropout Regularization," IEEE 16th International conference on Data Mining, 2016.  Large datasets often have unreliable labels and classi?ers trained on mislabeled datasets often exhibit poor performance. We present a simple, effective technique for accounting for label noise when training deep neural networks. We augment a standard deep network with a softmax layer that models the label noise statistics. Then, we train the deep network and noise model jointly via end-to-end stochastic gradient descent on different datasets.          Last Modified: 10/07/2018       Submitted by: Zichun Zhong]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
