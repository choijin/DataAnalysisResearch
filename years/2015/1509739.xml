<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Asymptotics and concentration in spectral estimation for large matrices</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>289425.00</AwardTotalIntnAmount>
<AwardAmount>289425</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Estimation of large matrices and their spectral characteristics is crucial in a variety of problems of science and engineering that deal with large high-dimensional data sets. Spectral methods are of the utmost importance in kernel machine learning, manifold learning, functional data analysis, community detection in large networks, quantum statistics and quantum information, and many other applications. The purpose of the project is to develop new mathematical tools needed in analysis of high-dimensional and infinite-dimensional matrix estimation problems that could potentially lead to more powerful methods of statistical inference for complex, high-dimensional data. The project also includes a number of activities with an impact on graduate education and on research collaborations between statistics, computer science and other areas.&lt;br/&gt;&lt;br/&gt;The focus of the project is on concentration properties and asymptotics of spectral characteristics (eigenvalues, eigenvectors, spectral projectors) of several important classes of random matrices and operators playing crucial role in high dimensional and infinite dimensional statistical inference and in machine learning. They include sample covariance operators, kernel matrices in machine learning, empirical heat kernels and Laplacians for manifold data, matrices involved in spectral clustering problems on graphs, matrix estimators in trace regression problems (such as matrix completion and quantum state tomography). The main goal is to study the problems where there exists an operator norm consistent estimator of the target matrix (operator), but it converges at a slow rate and to develop a broad range of concentration bounds and asymptotic results for specific functionals of the underlying random matrix (operator) such as its eigenvalues, bilinear forms of its spectral projection operators, norms of deviations of empirical spectral projection operators from their true counterparts. The solution of these problems relies on further development of the methods of high-dimensional probability, such as concentration inequalities, generic chaining bounds for Gaussian, empirical and related classes of stochastic processes, non-asymptotic bounds for random matrices.</AbstractNarration>
<MinAmdLetterDate>06/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1509739</AwardID>
<Investigator>
<FirstName>Vladimir</FirstName>
<LastName>Koltchinskii</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vladimir Koltchinskii</PI_FULL_NAME>
<EmailAddress>vlad@math.gatech.edu</EmailAddress>
<PI_PHON>4048942718</PI_PHON>
<NSF_ID>000118109</NSF_ID>
<StartDate>06/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~289425</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Among the main objects of interest in high-dimensional statistics are covariance matrices that represent the structure of dependencies between the components of complex, high-dimensional data sets. This project dealt with a circle of challenging problems related to estimation of variuos low dimensional features of covariance and related classes of matrices, in particular, their spectral characterstics that provide a way to identify the underlying structure of large matrices. The main outcome of the project is the development of new methods of estimation of such features based on recent achievements in high-dimensional probability and analysis. An important part of this research was related to novel approaches to bias reduction in the problem of estimation of smooth functions of high-dimensional parameters of statistical model. The methods of bias reduction developed in the project go beyond its initial scope (estimation of spectral characteristics of covariance and related matrices) and are applicable to much broader classes of problems in high-dimensional statistics. These methods yield estimators of low dimensional features of high-dimensional parameters with nearly optimal error rates. The results of the project are likely to find a number of applications in many areas of contemporary data science where it is of importance to extract low dimensional structures from high-dimensional data. The project provided a number of opportunities for graduate education and professional development, including new graduate level courses on high-dimensional statistics taken by the students in mathematics, statistics, electrical engineering and computer science. It resulted in two PhD dissertations in the areas related to the project. The results of the project have been disseminated to communities of interest through a number of publications in the leading journals, conference and seminar talks, series of lectures on the results of this research.&nbsp;</p><br> <p>            Last Modified: 09/13/2019<br>      Modified by: Vladimir&nbsp;Koltchinskii</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Among the main objects of interest in high-dimensional statistics are covariance matrices that represent the structure of dependencies between the components of complex, high-dimensional data sets. This project dealt with a circle of challenging problems related to estimation of variuos low dimensional features of covariance and related classes of matrices, in particular, their spectral characterstics that provide a way to identify the underlying structure of large matrices. The main outcome of the project is the development of new methods of estimation of such features based on recent achievements in high-dimensional probability and analysis. An important part of this research was related to novel approaches to bias reduction in the problem of estimation of smooth functions of high-dimensional parameters of statistical model. The methods of bias reduction developed in the project go beyond its initial scope (estimation of spectral characteristics of covariance and related matrices) and are applicable to much broader classes of problems in high-dimensional statistics. These methods yield estimators of low dimensional features of high-dimensional parameters with nearly optimal error rates. The results of the project are likely to find a number of applications in many areas of contemporary data science where it is of importance to extract low dimensional structures from high-dimensional data. The project provided a number of opportunities for graduate education and professional development, including new graduate level courses on high-dimensional statistics taken by the students in mathematics, statistics, electrical engineering and computer science. It resulted in two PhD dissertations in the areas related to the project. The results of the project have been disseminated to communities of interest through a number of publications in the leading journals, conference and seminar talks, series of lectures on the results of this research.        Last Modified: 09/13/2019       Submitted by: Vladimir Koltchinskii]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
