<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: New Foundations for Next-Generation  Reliable Throughput Architecture Design</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/01/2014</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>411484.00</AwardTotalIntnAmount>
<AwardAmount>411484</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With the demand on improving performance and energy-efficiency, novel technologies including non-volatile memory (e.g., spin-transfer torque RAM (STT-RAM)), 3D integration technology (3D), and near-threshold voltage computing (NTC) have been increasingly deployed in the state-of-the-art throughput processors. Since the novel technologies are not designed for dependable computing, the reliability challenges, which have been a crucial issue in conventional throughput architecture design, become the major obstacle for integrating them into next-generation throughput processors. There is a pressing need for the investigation of innovative techniques that are able to take advantage of throughput processors' unique features for characterizing and improving the reliability of the next-generation new-technology based throughput architecture design. &lt;br/&gt;&lt;br/&gt;The paramount reliability challenges in throughput processors include particle strikes induced soft errors, hard errors driven by aging effects, and manufacturing process variations. The principle investigator is building new foundations for vulnerability characterization and prediction, error detection, and fault tolerance against those dominant reliability challenges in throughput processors integrated with novel technologies. The project objectives include: (1) modeling and analyzing the vulnerability of novel-technology (e.g., STT-RAM, NTC, and 3D) enabled throughput processors in the presence of soft error, aging effects, and process variations; (2) fast and accurate predictive model to forecast the vulnerability phase behavior of throughput processors under new technologies; (3) developing the light-weight error detection mechanisms; and (4) exploring the opportunities and challenges introduced by the novel technologies to cost-effectively tolerate various types of errors in next-generation throughput architecture design. The proposed research will significantly promote the capability of architecting reliable throughput processors in future technologies beyond CMOS, making it possible to fulfill the Moore's Law without suffering the negative effects caused by various fault mechanisms. Moreover, this project will realize the desire of applying throughput processors into a wide range of computing scale from mobile computing to cloud computing, and increasing the deployment of throughput processors in support of supercomputing in science and engineering (e.g., finance, medical, biology, petroleum, aerospace, and geology). This project will also contribute to society through engaging high-school and undergraduate students from minority-serving institutions into research, expanding the computer engineering curriculum with reliability modeling and optimization techniques on throughput processors, attracting women and under-represented groups into graduate education, and disseminating research infrastructure for education and training of US IT workforce.</AbstractNarration>
<MinAmdLetterDate>05/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/20/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1537085</AwardID>
<Investigator>
<FirstName>Xin</FirstName>
<LastName>Fu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xin Fu</PI_FULL_NAME>
<EmailAddress>xfu8@central.uh.edu</EmailAddress>
<PI_PHON>7137436104</PI_PHON>
<NSF_ID>000583715</NSF_ID>
<StartDate>05/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036837920</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042916627</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>772042015</ZipCode>
<StreetAddress><![CDATA[4800 Calhoun Rd.  316 E. Cullen]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~142890</FUND_OBLG>
<FUND_OBLG>2016~85882</FUND_OBLG>
<FUND_OBLG>2017~182712</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The objective of this research is to construct new foundations for reliability characterization and prediction, error detection and fault tolerance in next-generation throughput processors integrated with emerging technologies (e.g., non-volatile memory (NVM), near-threshold voltage (NTV)). We have made the following three achievements:</p> <p>(1)&nbsp;&nbsp;&nbsp;&nbsp; Combating the Hardware Variability:</p> <p>(a)&nbsp;&nbsp;&nbsp;&nbsp; As technology keeps scaling down, hardware variability, such as process variations (PV) and negative bias temperature instability (NBTI), emerges as a growing challenge in GPGPUs (General-Purpose Graphics Processing Units). PV induces significant delay variations statically, while NBTI dynamically slows down the GPGPUs. The sizable register file in GPGPUs is very sensitive to the hardware variability, and becomes one of the major units in determining the core frequency. We proposed a set of techniques that mitigate both the PV and NBTI impacts on GPGPUs register file. In order to mitigate the susceptibility to PV, we first developed a mechanism that classifies registers into fast and slow categories in the highly-banked register architecture to maximize the frequency improvement. We then leveraged the unique features in GPGPU applications to tolerate the extra access delay to the slow registers. Moreover, we proposed to dynamically balance the utilization across registers to further tolerate the NBTI degradation.</p> <p>(b)&nbsp;&nbsp;&nbsp;&nbsp; We observed several interesting memory access patterns at both intra-CTA (cooperative thread array) and inter-CTA levels: most L2 cache blocks are private to threads belonging to the same CTA, and are accessed only once or twice; L2 cache blocks that are shared to threads across CTAs are usually accessed by certain threads. We leveraged those observations and proposed a set of CTA allocations and memory scheduling policies to hide the extra delay caused by the slower streaming multiprocessors.</p> <p>&nbsp;</p> <p>(2)&nbsp;&nbsp;&nbsp;&nbsp; Reliability Enhancement in Processors Integrated with NVM</p> <p>(a)&nbsp;&nbsp;&nbsp; Register file becomes the major contributor to the overall soft-error rate of GPUs, and also the power-hungry structure in GPGPUs. Spin-transfer torque RAM (STT-RAM) offers several benefits such as extremely low leakage power, and immunity to soft error attacks. We thus proposed LESS, which LEverages reSistive Memory to effectively mitigate both the registers Soft-error vulnerability and energy consumption. As its disadvantage, STT-RAM experiences significantly slower write latency than SRAM. We built the hybrid STT-RAM and SRAM based register file in LESS, and explored the unique characteristics of GPGPU applications to hide the long write latency to STT-RAM and obtain the win-win gains: achieving the near-full soft-error protection for the register file, and meanwhile substantially reducing the power consumption without hurting the performance.</p> <p>(b)&nbsp;&nbsp; With the capability of performing neural computations at the location of data, phase change memory (PCM) has been used as the synaptic weight element for large-scale neural networks (NNs). In the NN training stage, the weight (synapse) update is one of the core procedures and need to be done fast and accurate. However, there are two major challenges in PCM-based synapses that affect the speed and accuracy of the weight update, which are the variation, and the asymmetry of the PCM conductance response that cause the value of PCM cells being &ldquo;frozen&rdquo; at the maximum value and cannot accurately represent the weight value. To combat the variation challenge, one solution is cell duplication that uses multi-cells to represent one weight but duplicating every cell is undesirable, we proposed selective cell duplication and weight pruning to tolerate PCM variations and maximize the network accuracy. To solve the asymmetric problem, occasional reset (OR) is applied frequently which stalls the training process. We proposed dynamic OR scheduling to minimize the OR trigger frequency and improve the training speed while still maintaining the accuracy.</p> <p>&nbsp;</p> <p>(3)&nbsp;&nbsp;&nbsp;&nbsp; Reliability Enhancement at NTV</p> <p>(a)&nbsp;&nbsp;&nbsp;&nbsp; Applying NTV is an effective approach to significantly reduce the GPU energy consumption. As the largest on-chip storage structure, the register file becomes the reliability hotspot that prevents NTV due to the PV effects. To address this issue, we first modeled and analyzed the process variation impact on the GPU register file at different voltage levels. We made a key observation that the time GPU registers contain useless data (i.e., dead time) is long, which provides unique opportunities to enhance the register reliability. We then proposed GR-Guard, which leverages the long register dead time to enable reliable operations from unreliable register file at low voltages.</p> <p>(b)&nbsp;&nbsp;&nbsp;&nbsp; We proposed a neuron-level voltage scaling framework to model the impact of near-threshold voltages on NNs from output accuracy, power, and performance perspectives, and furthermore select the appropriate voltage to balance these three perspectives. We first analyzed the error propagation characteristics in NNs at both inter- and intra- network layers to precisely model the impact of voltage scaling on the final output accuracy at neuron-level. We then combined a voltage clustering method and the multi-objective optimization to identify the optimal voltage islands and apply the same voltage to neurons with similar fault tolerance capability.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/28/2020<br>      Modified by: Xin&nbsp;Fu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The objective of this research is to construct new foundations for reliability characterization and prediction, error detection and fault tolerance in next-generation throughput processors integrated with emerging technologies (e.g., non-volatile memory (NVM), near-threshold voltage (NTV)). We have made the following three achievements:  (1)     Combating the Hardware Variability:  (a)     As technology keeps scaling down, hardware variability, such as process variations (PV) and negative bias temperature instability (NBTI), emerges as a growing challenge in GPGPUs (General-Purpose Graphics Processing Units). PV induces significant delay variations statically, while NBTI dynamically slows down the GPGPUs. The sizable register file in GPGPUs is very sensitive to the hardware variability, and becomes one of the major units in determining the core frequency. We proposed a set of techniques that mitigate both the PV and NBTI impacts on GPGPUs register file. In order to mitigate the susceptibility to PV, we first developed a mechanism that classifies registers into fast and slow categories in the highly-banked register architecture to maximize the frequency improvement. We then leveraged the unique features in GPGPU applications to tolerate the extra access delay to the slow registers. Moreover, we proposed to dynamically balance the utilization across registers to further tolerate the NBTI degradation.  (b)     We observed several interesting memory access patterns at both intra-CTA (cooperative thread array) and inter-CTA levels: most L2 cache blocks are private to threads belonging to the same CTA, and are accessed only once or twice; L2 cache blocks that are shared to threads across CTAs are usually accessed by certain threads. We leveraged those observations and proposed a set of CTA allocations and memory scheduling policies to hide the extra delay caused by the slower streaming multiprocessors.     (2)     Reliability Enhancement in Processors Integrated with NVM  (a)    Register file becomes the major contributor to the overall soft-error rate of GPUs, and also the power-hungry structure in GPGPUs. Spin-transfer torque RAM (STT-RAM) offers several benefits such as extremely low leakage power, and immunity to soft error attacks. We thus proposed LESS, which LEverages reSistive Memory to effectively mitigate both the registers Soft-error vulnerability and energy consumption. As its disadvantage, STT-RAM experiences significantly slower write latency than SRAM. We built the hybrid STT-RAM and SRAM based register file in LESS, and explored the unique characteristics of GPGPU applications to hide the long write latency to STT-RAM and obtain the win-win gains: achieving the near-full soft-error protection for the register file, and meanwhile substantially reducing the power consumption without hurting the performance.  (b)   With the capability of performing neural computations at the location of data, phase change memory (PCM) has been used as the synaptic weight element for large-scale neural networks (NNs). In the NN training stage, the weight (synapse) update is one of the core procedures and need to be done fast and accurate. However, there are two major challenges in PCM-based synapses that affect the speed and accuracy of the weight update, which are the variation, and the asymmetry of the PCM conductance response that cause the value of PCM cells being "frozen" at the maximum value and cannot accurately represent the weight value. To combat the variation challenge, one solution is cell duplication that uses multi-cells to represent one weight but duplicating every cell is undesirable, we proposed selective cell duplication and weight pruning to tolerate PCM variations and maximize the network accuracy. To solve the asymmetric problem, occasional reset (OR) is applied frequently which stalls the training process. We proposed dynamic OR scheduling to minimize the OR trigger frequency and improve the training speed while still maintaining the accuracy.     (3)     Reliability Enhancement at NTV  (a)     Applying NTV is an effective approach to significantly reduce the GPU energy consumption. As the largest on-chip storage structure, the register file becomes the reliability hotspot that prevents NTV due to the PV effects. To address this issue, we first modeled and analyzed the process variation impact on the GPU register file at different voltage levels. We made a key observation that the time GPU registers contain useless data (i.e., dead time) is long, which provides unique opportunities to enhance the register reliability. We then proposed GR-Guard, which leverages the long register dead time to enable reliable operations from unreliable register file at low voltages.  (b)     We proposed a neuron-level voltage scaling framework to model the impact of near-threshold voltages on NNs from output accuracy, power, and performance perspectives, and furthermore select the appropriate voltage to balance these three perspectives. We first analyzed the error propagation characteristics in NNs at both inter- and intra- network layers to precisely model the impact of voltage scaling on the final output accuracy at neuron-level. We then combined a voltage clustering method and the multi-objective optimization to identify the optimal voltage islands and apply the same voltage to neurons with similar fault tolerance capability.          Last Modified: 04/28/2020       Submitted by: Xin Fu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
