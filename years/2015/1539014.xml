<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>VEC: Small: Collaborative Research: Scene Understanding from RGB-D Images</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>135000.00</AwardTotalIntnAmount>
<AwardAmount>135000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project exploits the benefits of RGB-D (color and depth) image collections with extra depth information to significantly advance the state-of-the-art in visual scene understanding, and makes computer vision techniques become usable in practical applications. Recent advance in affordable depth sensors has made depth acquisition significantly easier for ordinary users. These depth cameras are becoming very common in digital devices and help automatic scene understanding. The research team develops technologies to take advantage of depth information. Besides the published research results, the research team plans to distribute source code and benchmark data sets that could benefit researchers in a variety of disciplines. This project is integrated with educational programs, such as interdisciplinary workshops and courses at the graduate, undergraduate, and professional levels and diversity enhancement programs that promote opportunities for disadvantaged groups. The research team is closely collaborating with the industrial partner (Intel), involving interns and technology transfer in real products. The project is also applying the developed algorithms to the assistive technology for the blind and visually impaired.&lt;br/&gt;&lt;br/&gt;This research develops algorithms required to perform real-time segmentation, labeling, and recognition of RGB-D images, videos, and 3D scans of indoor environments. Specifically, the PIs develop methods to: (1) acquire large labeled RGB-D datasets for training and evaluation, (2) study algorithms to recognize objects and estimate detailed 3D knowledge about the scene, (3) exploit the object-to-object contextual relationships in 3D, and (4) demonstrate applications to benefit the general public, including household robotics and assistive technologies for the blind.</AbstractNarration>
<MinAmdLetterDate>08/27/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/26/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1539014</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Funkhouser</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas A Funkhouser</PI_FULL_NAME>
<EmailAddress>funk@cs.princeton.edu</EmailAddress>
<PI_PHON>6092581748</PI_PHON>
<NSF_ID>000092134</NSF_ID>
<StartDate>05/26/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Funkhouser</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas A Funkhouser</PI_FULL_NAME>
<EmailAddress>funk@cs.princeton.edu</EmailAddress>
<PI_PHON>6092581748</PI_PHON>
<NSF_ID>000092134</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate>05/26/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jianxiong</FirstName>
<LastName>Xiao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jianxiong Xiao</PI_FULL_NAME>
<EmailAddress>xj@princeton.edu</EmailAddress>
<PI_PHON>6092583090</PI_PHON>
<NSF_ID>000654589</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate>05/26/2016</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[87 Prospect Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>002Z</Code>
<Text>Intel/NSF VEC Partnership</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~135000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main goal of the project was to study how to exploit the depth information in RGB-D data to boost the performance of general-purpose visual scene understanding.&nbsp; Specifically, we investigated: 1) how to acquire large RGB-D datasets with labels, 2) how to design or learn RGB-D specific features using 3D representation, 3) how to reason about scenes in 3D, and 4) how to demonstrate the power of RGB-D recognition for applications in robotics.</p> <p>Over the course of the project, we addressed these goals with several collaborative research projects.&nbsp; First, we developed benchmark datasets for RGB-D scene understanding.&nbsp; Second, we developed algorithms for estimating missing depths using models trained on color images, estimating camera poses by aligning semantic and structural features, learning shape descriptors with self-supervion from prior surface reconstructions, completing partially observed scenes based on semantic segmentation, and parsing scenes based on learned contextual priors, in addition to several other challenging problems.&nbsp; We have used these algorithms in robotics applications that require navigation through complex scenes and&nbsp;manipulating objects in complex cluttered environments.&nbsp; &nbsp;We have disseminated all the research through peer-reviewed publications, in addition to publicly available datasets, benchmarks, and code.</p> <p>The educational goals of the project have been addressed through teaching and mentoring at several levels.&nbsp; &nbsp;The PIs have taught undergraduate courseson computer vision, graduate courses on 3D deep learning, and advanced graduate seminars on 3D scene understanding.&nbsp; The project has provided mentoring for dozens of undergraduate and graduate students, including several that received Ph.Ds and went on to faculty positions at top schools.</p> <p>Overall, the project has contributed to a significant advance in the technologies required to understand 3D scenes using RGB-D sensors.</p><br> <p>            Last Modified: 12/07/2019<br>      Modified by: Thomas&nbsp;A&nbsp;Funkhouser</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main goal of the project was to study how to exploit the depth information in RGB-D data to boost the performance of general-purpose visual scene understanding.  Specifically, we investigated: 1) how to acquire large RGB-D datasets with labels, 2) how to design or learn RGB-D specific features using 3D representation, 3) how to reason about scenes in 3D, and 4) how to demonstrate the power of RGB-D recognition for applications in robotics.  Over the course of the project, we addressed these goals with several collaborative research projects.  First, we developed benchmark datasets for RGB-D scene understanding.  Second, we developed algorithms for estimating missing depths using models trained on color images, estimating camera poses by aligning semantic and structural features, learning shape descriptors with self-supervion from prior surface reconstructions, completing partially observed scenes based on semantic segmentation, and parsing scenes based on learned contextual priors, in addition to several other challenging problems.  We have used these algorithms in robotics applications that require navigation through complex scenes and manipulating objects in complex cluttered environments.   We have disseminated all the research through peer-reviewed publications, in addition to publicly available datasets, benchmarks, and code.  The educational goals of the project have been addressed through teaching and mentoring at several levels.   The PIs have taught undergraduate courseson computer vision, graduate courses on 3D deep learning, and advanced graduate seminars on 3D scene understanding.  The project has provided mentoring for dozens of undergraduate and graduate students, including several that received Ph.Ds and went on to faculty positions at top schools.  Overall, the project has contributed to a significant advance in the technologies required to understand 3D scenes using RGB-D sensors.       Last Modified: 12/07/2019       Submitted by: Thomas A Funkhouser]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
