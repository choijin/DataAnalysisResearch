<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Diagnosing Performance and Correctness Errors in Parallel Applications at Large Scales</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>466000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Dependability has become a critically necessary property for many of the computer systems that surround us or work behind the scenes supporting our personal and professional lives. We have made great strides in our ability to design, implement, and operate dependable systems. However, dependability solutions are increasingly being stressed due to rapid increases in the scale of the computing systems. Computer applications used in areas such as computational genomics, data mining, and prediction of natural phenomena tackle extremely complex problems which generate vast amounts of sensory data; thus, the inputs to these applications is tremendous.  Computing is rapidly becoming more dependent on parallelism - where many calculations are carried out simultaneously. This means increasing core counts for servers, more servers and racks for data centers, and a dramatic increase in the number of computing cores that these applications must run. The traditional dependability solutions just are not working.&lt;br/&gt; &lt;br/&gt;When an application does not complete or completes with incorrect results, the developer must identify the offending parallel task and then the portion of the code in that task that caused the error. This is hard enough for parallel applications at small to moderate sizes. These issues get exacerbated at large scales. Dealing with tens of processes is within reach of mere mortal developers, a few hundreds of processes is within reach of heroic developers, but on machines of petascale and beyond, this requires sophisticated support.&lt;br/&gt;&lt;br/&gt;This project will create design principles for debugging tools that can operate at large scales of data and process count and a practical instantiation of these principles in a system called LANCET. The methodology will be based on the insight that the numbers of equivalence classes of processes in an application do not grow even as the number of processes grows.  Analysis will mostly deal with equivalence classes. Resilience runtime will have elements that operate on individual processes in a completely distributed manner. Where non-local knowledge is needed, the techniques will operate in a sampling mode. Finally, the project will develop solutions for data-dependent errors that have resisted convincing widely applicable solutions, i.e., errors of the kind that manifest themselves for specific input datasets or specific input parameter combinations.</AbstractNarration>
<MinAmdLetterDate>08/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/03/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1527262</AwardID>
<Investigator>
<FirstName>Saurabh</FirstName>
<LastName>Bagchi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Saurabh Bagchi</PI_FULL_NAME>
<EmailAddress>sbagchi@purdue.edu</EmailAddress>
<PI_PHON>7654941741</PI_PHON>
<NSF_ID>000309372</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072035</ZipCode>
<StreetAddress><![CDATA[465 Northwestern Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~450000</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Dependability has become a critically necessary property for many of the computer systems that surround us or work behind the scenes to support our personal and professional lives. We have made great strides in our ability to design, implement, and operate dependable systems. However, dependability solutions are increasingly being stressed due to rapid increases in the scale of the computing systems. As applications in domains such as computational genomics, data mining, and prediction of natural phenomena are let loose on ever-more-complex problems and with ever-increasing amounts of sensory data, the scale of the inputs to these applications has shot up. Also, as the pursuit of parallelism has led to increasing core counts for servers, and increasing numbers of servers and racks for data centers, the number of computing cores that these applications must run with has also dramatically risen. The traditional dependability solutions were found to be inadequate in this setting.</p> <p>When such a large-scale application does not complete or completes with incorrect results, the developer must identify the offending portion of the application that caused the error. Our project created techniques and tools to provide debugging support and resultant guarantees of correct results and compliant performance for such challenging applications, when run at unprecedented scales (of data or number of processing elements). In the project, we created design principles for debugging tools that can operate at large scales of data and process count and a practical instantiation of these principles for applications in computational genomics, streaming analytics, and digital factories.</p> <p><strong>Intellectual Merit: </strong></p> <p>Our methodology brought forth and leveraged three insights ? the number of equivalence classes of processes in an application does not grow even as the number of processes grows and scalable analysis should deal with equivalence classes, rather than individual processes. Our resilience runtime has elements that operate on individual processes in a completely distributed manner. For parts that need non-local knowledge, the techniques operate in a sampling mode. Finally, our solutions show a way to handle data-dependent errors, <em>i.e.</em>, errors of the kind that manifest themselves for specific input datasets or specific input parameter combinations.</p> <p>A key challenge to successfully tuning approximations is finding the optimal configuration, which may change across and within the streaming content, such as input videos, because it is content-dependent. Searching through the entire search space for every frame in the video stream is infeasible, while tuning the pipeline offline, on a set of training videos, yields suboptimal results. We developed VIDEOCHEF, a system for approximate optimization of video pipelines. VIDEOCHEF finds the optimal configurations of approximate filters at runtime (balancing the speedup with the error), by leveraging the previously proposed concept of canary inputs?using small inputs to tune the accuracy of the computations and transferring the approximate configurations to full inputs.</p> <p>An important part of software testing for scale-dependent bugs is generation of worst-case test inputs, which exercise a program under extreme loads. For such a task, symbolic execution is a useful tool with its capability to reason about all possible execution paths of a program, including the one with the worst case behavior. However, symbolic execution suffers from the path explosion problem and frequent calls to a constraint solver, which make it impractical to be used at a large scale. To address the issue, we developed XSTRESSOR that is able to generate test inputs that can run specific loops in a program with the worst-case complexity in a large scale. XSTRESSOR synthetically generates the path condition for the large-scale, worst-case execution from a predictive model that is built from a set of small scale tests. XSTRESSOR avoids the scaling problem of prior techniques by limiting full-blown symbolic execution and run-time calls to constraint solver to small scale tests only. XSTRESSOR improved upon the state-of-the-art, namely, WISE and SPF-WCA.</p> <p><strong>Broader Impacts: </strong></p> <p>The project released open source versions of the following software packages:</p> <p>Approximation with reliability guarantees: Opprox and VideoChef.</p> <p>Generating test inputs that can trigger scalability bugs: XStressor and PySE.</p> <p>In terms of the application domain, we showed how our approach can help in developing scalable computational genomics applications, through a domain-specific language called Sarvavid and demonstrated its applications to the important applications in the domain (such as, re-sequencing and genomic assembly). This can have broad impacts in uncovering the genomic basis for our health and well-being with the emerging promise of being able to take actions based on such knowledge. We also applied our approach to GE?s digital factory domain showing how we can uncover bottlenecks in a digital manufacturing pipeline by applying pattern mining on runtime data at each machine in the pipeline.</p><br> <p>            Last Modified: 02/09/2020<br>      Modified by: Saurabh&nbsp;Bagchi</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270413961_opprox--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270413961_opprox--rgov-800width.jpg" title="Opprox: Approximate computing for scaling applications"><img src="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270413961_opprox--rgov-66x44.jpg" alt="Opprox: Approximate computing for scaling applications"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Approximate computing for scaling applications to large numbers of processes or large amounts of data, while maintaining probabilistic guarantees of correctness and performance</div> <div class="imageCredit">Saurabh Bagchi</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">Opprox: Approximate computing for scaling applications</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270489116_videochef--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270489116_videochef--rgov-800width.jpg" title="VideoChef: Content-dependent approximation for streaming analytics"><img src="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270489116_videochef--rgov-66x44.jpg" alt="VideoChef: Content-dependent approximation for streaming analytics"></a> <div class="imageCaptionContainer"> <div class="imageCaption">VideoChef shows how to do content-dependent approximation, while making dynamic changes as the stream characteristics or resource availability changes. It maintains probabilistic guarantees of correctness and performance.</div> <div class="imageCredit">Saurabh Bagchi</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">VideoChef: Content-dependent approximation for streaming analytics</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270568878_xstressor--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270568878_xstressor--rgov-800width.jpg" title="XStressor: Scalable method to trigger scalability bugs"><img src="/por/images/Reports/POR/2020/1527262/1527262_10389350_1581270568878_xstressor--rgov-66x44.jpg" alt="XStressor: Scalable method to trigger scalability bugs"></a> <div class="imageCaptionContainer"> <div class="imageCaption">XStressor shows how to trigger scale-dependent bugs without running into scalability bottlenecks of existing techniques, that are based on symbolic analysis</div> <div class="imageCredit">Saurabh Bagchi</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">XStressor: Scalable method to trigger scalability bugs</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Dependability has become a critically necessary property for many of the computer systems that surround us or work behind the scenes to support our personal and professional lives. We have made great strides in our ability to design, implement, and operate dependable systems. However, dependability solutions are increasingly being stressed due to rapid increases in the scale of the computing systems. As applications in domains such as computational genomics, data mining, and prediction of natural phenomena are let loose on ever-more-complex problems and with ever-increasing amounts of sensory data, the scale of the inputs to these applications has shot up. Also, as the pursuit of parallelism has led to increasing core counts for servers, and increasing numbers of servers and racks for data centers, the number of computing cores that these applications must run with has also dramatically risen. The traditional dependability solutions were found to be inadequate in this setting.  When such a large-scale application does not complete or completes with incorrect results, the developer must identify the offending portion of the application that caused the error. Our project created techniques and tools to provide debugging support and resultant guarantees of correct results and compliant performance for such challenging applications, when run at unprecedented scales (of data or number of processing elements). In the project, we created design principles for debugging tools that can operate at large scales of data and process count and a practical instantiation of these principles for applications in computational genomics, streaming analytics, and digital factories.  Intellectual Merit:   Our methodology brought forth and leveraged three insights ? the number of equivalence classes of processes in an application does not grow even as the number of processes grows and scalable analysis should deal with equivalence classes, rather than individual processes. Our resilience runtime has elements that operate on individual processes in a completely distributed manner. For parts that need non-local knowledge, the techniques operate in a sampling mode. Finally, our solutions show a way to handle data-dependent errors, i.e., errors of the kind that manifest themselves for specific input datasets or specific input parameter combinations.  A key challenge to successfully tuning approximations is finding the optimal configuration, which may change across and within the streaming content, such as input videos, because it is content-dependent. Searching through the entire search space for every frame in the video stream is infeasible, while tuning the pipeline offline, on a set of training videos, yields suboptimal results. We developed VIDEOCHEF, a system for approximate optimization of video pipelines. VIDEOCHEF finds the optimal configurations of approximate filters at runtime (balancing the speedup with the error), by leveraging the previously proposed concept of canary inputs?using small inputs to tune the accuracy of the computations and transferring the approximate configurations to full inputs.  An important part of software testing for scale-dependent bugs is generation of worst-case test inputs, which exercise a program under extreme loads. For such a task, symbolic execution is a useful tool with its capability to reason about all possible execution paths of a program, including the one with the worst case behavior. However, symbolic execution suffers from the path explosion problem and frequent calls to a constraint solver, which make it impractical to be used at a large scale. To address the issue, we developed XSTRESSOR that is able to generate test inputs that can run specific loops in a program with the worst-case complexity in a large scale. XSTRESSOR synthetically generates the path condition for the large-scale, worst-case execution from a predictive model that is built from a set of small scale tests. XSTRESSOR avoids the scaling problem of prior techniques by limiting full-blown symbolic execution and run-time calls to constraint solver to small scale tests only. XSTRESSOR improved upon the state-of-the-art, namely, WISE and SPF-WCA.  Broader Impacts:   The project released open source versions of the following software packages:  Approximation with reliability guarantees: Opprox and VideoChef.  Generating test inputs that can trigger scalability bugs: XStressor and PySE.  In terms of the application domain, we showed how our approach can help in developing scalable computational genomics applications, through a domain-specific language called Sarvavid and demonstrated its applications to the important applications in the domain (such as, re-sequencing and genomic assembly). This can have broad impacts in uncovering the genomic basis for our health and well-being with the emerging promise of being able to take actions based on such knowledge. We also applied our approach to GE?s digital factory domain showing how we can uncover bottlenecks in a digital manufacturing pipeline by applying pattern mining on runtime data at each machine in the pipeline.       Last Modified: 02/09/2020       Submitted by: Saurabh Bagchi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
