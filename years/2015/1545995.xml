<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: F: New Algorithms of Online Machine Learning for Big Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>712401.00</AwardTotalIntnAmount>
<AwardAmount>712401</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei Ding</SignBlockName>
<PO_EMAI>weiding@nsf.gov</PO_EMAI>
<PO_PHON>7032928017</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is developing innovative, theoretically rigorous algorithms to learn from continuously arriving (streaming) data.  Specific challenges addressed are class imbalance (one of the concepts to be learned is very rare, as in disease detection), cost constraints on both obtaining features (e.g., computationally expensive image processing), and cost constraints on obtaining class labels (e.g., human annotation.)  The algorithms developed in this project make it possible to effectively address big data challenges in streaming data due to increased complexities in various aspects such as heavily imbalanced data distributions, ultrahigh dimensional features, a large number of labels, highly complex constraints, etc.  The project will also contribute to training future professionals in big data analytics, including participation in the University of Iowa's undergraduate summer research program and high school student training program.&lt;br/&gt;&lt;br/&gt;Most work devoted to online learning algorithms and their analysis were developed with the goal of minimizing a symmetric measure (e.g., the classification error) and without considering practical constraints arising in big data.  This project addresses imbalanced data by developing online learning algorithms for minimizing asymmetric measures including F-score, area under the ROC curve, and area under precision and recall curve.  Convex or non-convex surrogate loss functions that well-approximate these asymmetric measures are constructed and minimized in an online fashion. The project also develops online algorithms under three types of constraints arising in big data context namely constraints on computing costs, on query costs, and complex inequality constraints, by exploring techniques in randomized algorithms, active learning and convex optimization.  The developed algorithms are being evaluated in real applications including biomedical semantic indexing, social media mining, and image annotation.</AbstractNarration>
<MinAmdLetterDate>09/08/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/08/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1545995</AwardID>
<Investigator>
<FirstName>Padmini</FirstName>
<LastName>Srinivasan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Padmini Srinivasan</PI_FULL_NAME>
<EmailAddress>padmini-srinivasan@uiowa.edu</EmailAddress>
<PI_PHON>3196210118</PI_PHON>
<NSF_ID>000186619</NSF_ID>
<StartDate>09/08/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tianbao</FirstName>
<LastName>Yang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tianbao Yang</PI_FULL_NAME>
<EmailAddress>tianbao-yang@uiowa.edu</EmailAddress>
<PI_PHON>3193532541</PI_PHON>
<NSF_ID>000678293</NSF_ID>
<StartDate>09/08/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Iowa</Name>
<CityName>IOWA CITY</CityName>
<ZipCode>522421320</ZipCode>
<PhoneNumber>3193352123</PhoneNumber>
<StreetAddress>2 GILMORE HALL</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>062761671</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF IOWA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>062761671</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Iowa]]></Name>
<CityName>Iowa</CityName>
<StateCode>IA</StateCode>
<ZipCode>522421320</ZipCode>
<StreetAddress><![CDATA[2 Gilmore Hal]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~712401</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project studies online optimization and stochastic optimization algorithms for solving various machine learning problems, in particular for optimizing non-decomposable losses in machine learning, with a focus on accelerating the convergence rate by leveraging the objective and data&rsquo;s properties. Most existing stochastic algorithms are mostly developed for optimizing decomposable losses such as averaged loss. In addition, their convergence rates are either slow and they require strong assumptions of the objective function. The proposed algorithms can address these issues. We have achieved the following outcomes:</p> <ol> <li>We developed new online optimization algorithms for AUC and F-measure maximization, which enjoy faster convergence rate. </li> <li>We developed new stochastic optimization algorithms to adapt to the error bound conditions and properties of data to enjoy faster convergence. </li> <li>We developed new online optimization algorithms for minimizing the dynamic regret to tackle dynamic environment. </li> <li>We developed new stochastic min-max optimization algorithms for optimizing non-decomposable losses, which have applications in learning with imbalanced data and outliers.</li> <li>We develop new stochastic algorithms for tackling learning problems with complex constraints (e.g. Neyman-Pearson classification, distance metric learning). &nbsp;</li> </ol> <p>This project also supports six graduate students&rsquo; research and one post-doc researcher, including three graduate students&rsquo; thesis research. Three PhD students have graduated under the support of this grant. They have started their career in the industrial companies in the United States.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/13/2019<br>      Modified by: Tianbao&nbsp;Yang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project studies online optimization and stochastic optimization algorithms for solving various machine learning problems, in particular for optimizing non-decomposable losses in machine learning, with a focus on accelerating the convergence rate by leveraging the objective and data’s properties. Most existing stochastic algorithms are mostly developed for optimizing decomposable losses such as averaged loss. In addition, their convergence rates are either slow and they require strong assumptions of the objective function. The proposed algorithms can address these issues. We have achieved the following outcomes:  We developed new online optimization algorithms for AUC and F-measure maximization, which enjoy faster convergence rate.  We developed new stochastic optimization algorithms to adapt to the error bound conditions and properties of data to enjoy faster convergence.  We developed new online optimization algorithms for minimizing the dynamic regret to tackle dynamic environment.  We developed new stochastic min-max optimization algorithms for optimizing non-decomposable losses, which have applications in learning with imbalanced data and outliers. We develop new stochastic algorithms for tackling learning problems with complex constraints (e.g. Neyman-Pearson classification, distance metric learning).     This project also supports six graduate students’ research and one post-doc researcher, including three graduate students’ thesis research. Three PhD students have graduated under the support of this grant. They have started their career in the industrial companies in the United States.           Last Modified: 11/13/2019       Submitted by: Tianbao Yang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
