<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR:Medium:Collaborative Research: SparseKaffe: high-performance, auto-tuned, energy-aware algorithms for sparse direct methods on modern heterogeneous architectures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>399993.00</AwardTotalIntnAmount>
<AwardAmount>399993</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The use of sparse direct methods in computational science is ubiquitous. Direct methods can be used to find solutions to many numerical algebra applications, including sparse linear systems, sparse linear least squares, and eigenvalue problems; consequently they form the backbone of a broad spectrum of large scale applications.  In the widely used and actively growing University of Florida Sparse Matrix Collection, there are problems from structural engineering, computational fluid dynamics (CFD), computer graphics/vision, robotics/kinematics, theoretical and quantum chemistry, power networks, social networks, document networks, among others.&lt;br/&gt; &lt;br/&gt;The SparseKaffe project team will develop algorithms and software for high-performance parallel sparse direct methods with irregular and hierarchical structure that can exploit clusters of Hybrid Multicore Processors to achieve orders of magnitude gains in computational performance, while also paying careful attention to the energy requirements.  This requires the development of novel and innovative algorithms for scheduling, energy minimization, and memory management; development of novel user-guided autotuning algorithms that exploit different hardware characteristics; and designing a common infrastructure for creating auto-tuned software.&lt;br/&gt; &lt;br/&gt;The use of sparse direct methods is extensive, with many of the relevant science and engineering application areas being pushed to run at ever higher scales.  The team expects SparseKaffe solvers to be able deliver not only high performance to the applications that use them, but also the energy efficiency that they will increasingly demand. The team will also create a course, and a corresponding set of course modules, to teach students how to develop algorithms and software that deliver orders of magnitude gains in performance on clusters of hybrid multicore processors.</AbstractNarration>
<MinAmdLetterDate>08/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/01/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1514286</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Dongarra</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack J Dongarra</PI_FULL_NAME>
<EmailAddress>dongarra@icl.utk.edu</EmailAddress>
<PI_PHON>8659748295</PI_PHON>
<NSF_ID>000299281</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName/>
<StateCode>TN</StateCode>
<ZipCode>379960003</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~262091</FUND_OBLG>
<FUND_OBLG>2017~137902</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-96abf774-7fff-fa51-d5d6-8324cc70a776"> <p dir="ltr"><span>The Sparse direct methods via Run-time Scheduling and Execution of Kernels with Auto-tunable and Frequency-scaling Features for Energy-aware computing on heterogeneous architectures (SparseKaffe) project&mdash;a collaboration between the University of Tennessee, Knoxville, the University of Florida, and Texas A&amp;M University&mdash;created fast and efficient batched and sparse-direct methods for platforms with multi-core processors with one or more accelerators (e.g., GPUs).</span></p> <p dir="ltr"><span>Batched linear algebra techniques have a significant impact on the scientific computing software stack. Vendor math libraries now include some batched BLAS routines, and SparseKaffe&rsquo;s sparse and dense solvers have already been incorporated by many researchers. For example, SuiteSparse and MAGMA&mdash;both of which are part of the SparseKaffe effort&mdash;appear on NVIDIA&rsquo;s short list of GPU-accelerated libraries: </span><a href="https://developer.nvidia.com/gpu-accelerated-libraries"><span>https://developer.nvidia.com/gpu-accelerated-libraries</span></a><span>.</span></p> <p dir="ltr"><span>Moreover, many scientific applications require the solution of a large number of relatively small, independent linear systems in parallel. Examples include astrophysics; quantum chemistry; metabolic networks; computational fluid dynamics and the resulting partial differential equations through direct and multifrontal solvers; high-order finite element method schemes for hydrodynamics; direct-iterative preconditioned solvers; and image and signal processing. All of these areas will benefit from the batched scheduling and linear algebra developed in the SparseKaffe project, and the efforts to make batched BLAS a standard will impact many disciplines, including the ones mentioned above.</span></p> </span></p><br> <p>            Last Modified: 12/03/2019<br>      Modified by: Jack&nbsp;J&nbsp;Dongarra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The Sparse direct methods via Run-time Scheduling and Execution of Kernels with Auto-tunable and Frequency-scaling Features for Energy-aware computing on heterogeneous architectures (SparseKaffe) project&mdash;a collaboration between the University of Tennessee, Knoxville, the University of Florida, and Texas A&amp;M University&mdash;created fast and efficient batched and sparse-direct methods for platforms with multi-core processors with one or more accelerators (e.g., GPUs). Batched linear algebra techniques have a significant impact on the scientific computing software stack. Vendor math libraries now include some batched BLAS routines, and SparseKaffe’s sparse and dense solvers have already been incorporated by many researchers. For example, SuiteSparse and MAGMA&mdash;both of which are part of the SparseKaffe effort&mdash;appear on NVIDIA’s short list of GPU-accelerated libraries: https://developer.nvidia.com/gpu-accelerated-libraries. Moreover, many scientific applications require the solution of a large number of relatively small, independent linear systems in parallel. Examples include astrophysics; quantum chemistry; metabolic networks; computational fluid dynamics and the resulting partial differential equations through direct and multifrontal solvers; high-order finite element method schemes for hydrodynamics; direct-iterative preconditioned solvers; and image and signal processing. All of these areas will benefit from the batched scheduling and linear algebra developed in the SparseKaffe project, and the efforts to make batched BLAS a standard will impact many disciplines, including the ones mentioned above.        Last Modified: 12/03/2019       Submitted by: Jack J Dongarra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
