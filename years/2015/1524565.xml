<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Comp Cog:  Collaborative Research on the Development of Visual Object Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>313582.00</AwardTotalIntnAmount>
<AwardAmount>313582</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Vishton</SignBlockName>
<PO_EMAI>pvishton@nsf.gov</PO_EMAI>
<PO_PHON>7032928132</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Human visual object recognition is fast and robust.  People can recognize a large number of visual objects in complex scenes, from varied views, and in less than optimal circumstances.  This ability underlies many advanced human skills, including tool use, reading, and navigation.  Artificial intelligence devices do not yet approach the level of skill of everyday human object recognition. This project will address one gap in current knowledge, an understanding of the visual experiences that allow skilled object recognition to develop, by capturing and analyzing the visual experiences of 1- to 2-year-old toddlers.  This is a key period for understanding human visual object recognition because it is the time when toddlers learn a large number of object categories, when they learn the names for those objects, and when they instrumentally act on and use objects as tools.  Two-year-old children, unlike computer vision systems, rapidly learn to recognize many visual objects.  This project seeks to understand how the training experiences (everyday object viewing) of toddlers may be optimal for building robust visual object recognition. The project aims to (1) understand the visual and statistical regularities in 1- to 2-year-old children's experiences of common objects (e.g., cups, chairs, trucks, dogs) and (2) determine whether a training regimen like that experienced by human toddlers supports visual object recognition by state-of-the art machine vision. &lt;br/&gt;&lt;br/&gt;Considerable progress in understanding adult vision has been made by studying the visual statistics of "natural scenes." However, there is concern about possible artifacts in these scenes because they typically photographs taken by adults and thus potentially biased by the already developed mature visual system that holds the camera and frames the pictures. Also, photographed scenes differ systematically from the scenes sampled by people as they move about and act in the world.  Accordingly, there is increased interest in egocentric views collected from body-worn cameras, the method used in the present work.  Toddlers will wear lightweight head cameras as they go about their daily activities, allowing the investigators to capture the objects the toddlers see and the perspectives and contexts in which they see them.  The research will analyze the frequency, views, visual properties, and range of seen objects for the first 100 object names normatively learned by young children, providing a first description of the early learning environment for human visual object recognition.  These toddler-perspective scenes  will be used as inputs to machine learning models to better understand how the visual information in the scenes supports and constrains the development of visual object recognition. Machine-learning experiments will determine which properties and statistical regularities are most critical for learning to recognize common object categories in multiple scene contexts.  Data collected will be shared through Databrary, an open data library for developmental science.</AbstractNarration>
<MinAmdLetterDate>07/21/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/02/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1524565</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Rehg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James Rehg</PI_FULL_NAME>
<EmailAddress>rehg@cc.gatech.edu</EmailAddress>
<PI_PHON>4048949105</PI_PHON>
<NSF_ID>000257071</NSF_ID>
<StartDate>07/21/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Fuxin</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Fuxin Li</PI_FULL_NAME>
<EmailAddress>lif@eecs.oregonstate.edu</EmailAddress>
<PI_PHON>4049061899</PI_PHON>
<NSF_ID>000637562</NSF_ID>
<StartDate>07/21/2015</StartDate>
<EndDate>07/14/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Maithilee</FirstName>
<LastName>Kunda</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maithilee Kunda</PI_FULL_NAME>
<EmailAddress>mkunda@vanderbilt.edu</EmailAddress>
<PI_PHON>6158758469</PI_PHON>
<NSF_ID>000649232</NSF_ID>
<StartDate>07/21/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1698</Code>
<Text>DS -Developmental Sciences</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1698</Code>
<Text>DS-Developmental Sciences</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>7298</Code>
<Text>COLLABORATIVE RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~107033</FUND_OBLG>
<FUND_OBLG>2016~206549</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Deep learning technology has revolutionized Artificial Intelligence by providing powerful new capabilities for learning from large amounts of labeled training data. At the same time, a substantial gap exists between machine learners and human learners, as the latter have the ability to learn new concepts rapidly from a small number of training examples with noisy labels. The aim of this project was to help close the gap between machine and human learning by developing a computational learning approach inspired by infant's ability to learn to recognize objects through play. Fig 1 shows a rendering of toy objects that we use to simulate a child's view of objects during play. Children's learning is incremental, in that they learn about objects one at a time. Modern deep learners do poorly in incremental learning, as illustrated in Fig. 2 (all accuracies decline over time). The incremental learning algorithms in Fig. 2 were designed to retain information about the objects they have learned in the past and avoid forgetting previous concepts. Unfortunately they are unable to approach the accuracy of an algorithm which gets to see all of the objects at once (batch learning), illustrated by the cross in the top right of the figure. We have discovered that allowing allowing incremental deep learning methods revisit objects repeatedly over time substantially improves their learning ability, as illustrated in Fig. 3. The same algorithms that did poorly in Fig. 2 converge to high levels of accuracy in Fig. 3, approaching the batch performance (black cross). These experiments demonstrate that repetition, which is a natural part of our everyday visual experiences, is a powerful construct in incremental learning. We were the first to quantify the benefit of repetition in incremental object learning and our experiments point to new avenues for research.</p><br> <p>            Last Modified: 05/04/2020<br>      Modified by: James&nbsp;Rehg</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637798322_Fig4b--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637798322_Fig4b--rgov-800width.jpg" title="Success of Incremental Learning with Repetition"><img src="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637798322_Fig4b--rgov-66x44.jpg" alt="Success of Incremental Learning with Repetition"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Allowing learners to be repeatedly exposed to objects multiple times, even while learning sequentially and incrementally, ameliorates the effects of catastrophic forgetting.</div> <div class="imageCredit">Stefan Stojanov, Anh Ngoc Thai, Samarth Mishra, Nikhil Dhanda, Ahmad Humayun, and James M. Rehg</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">James&nbsp;Rehg</div> <div class="imageTitle">Success of Incremental Learning with Repetition</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637625282_Fig4a--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637625282_Fig4a--rgov-800width.jpg" title="Failure of Incremental Learning"><img src="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637625282_Fig4a--rgov-66x44.jpg" alt="Failure of Incremental Learning"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This graph shows the declining accuracy over time of deep learning algorithms that are designed to remember what they have learned when exposed to objects sequentially.</div> <div class="imageCredit">Stefan Stojanov, Anh Ngoc Thai, Samarth Mishra, Nikhil Dhanda, Ahmad Humayun, and and James M. Rehg</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">James&nbsp;Rehg</div> <div class="imageTitle">Failure of Incremental Learning</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637125317_fig2--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637125317_fig2--rgov-800width.jpg" title="3D Model Toys in CRIB"><img src="/por/images/Reports/POR/2020/1524565/1524565_10378537_1588637125317_fig2--rgov-66x44.jpg" alt="3D Model Toys in CRIB"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Computer graphics rendering of 3D toy object models, to illustrate the diversity of our set of objects, which is used for object recognition experiments.</div> <div class="imageCredit">Stefan Stojanov</div> <div class="imageSubmitted">James&nbsp;Rehg</div> <div class="imageTitle">3D Model Toys in CRIB</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Deep learning technology has revolutionized Artificial Intelligence by providing powerful new capabilities for learning from large amounts of labeled training data. At the same time, a substantial gap exists between machine learners and human learners, as the latter have the ability to learn new concepts rapidly from a small number of training examples with noisy labels. The aim of this project was to help close the gap between machine and human learning by developing a computational learning approach inspired by infant's ability to learn to recognize objects through play. Fig 1 shows a rendering of toy objects that we use to simulate a child's view of objects during play. Children's learning is incremental, in that they learn about objects one at a time. Modern deep learners do poorly in incremental learning, as illustrated in Fig. 2 (all accuracies decline over time). The incremental learning algorithms in Fig. 2 were designed to retain information about the objects they have learned in the past and avoid forgetting previous concepts. Unfortunately they are unable to approach the accuracy of an algorithm which gets to see all of the objects at once (batch learning), illustrated by the cross in the top right of the figure. We have discovered that allowing allowing incremental deep learning methods revisit objects repeatedly over time substantially improves their learning ability, as illustrated in Fig. 3. The same algorithms that did poorly in Fig. 2 converge to high levels of accuracy in Fig. 3, approaching the batch performance (black cross). These experiments demonstrate that repetition, which is a natural part of our everyday visual experiences, is a powerful construct in incremental learning. We were the first to quantify the benefit of repetition in incremental object learning and our experiments point to new avenues for research.       Last Modified: 05/04/2020       Submitted by: James Rehg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
