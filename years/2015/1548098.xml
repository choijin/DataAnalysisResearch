<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Exploration of Validity Evidence Gaps in Science Educational Achievement Testing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>292980.00</AwardTotalIntnAmount>
<AwardAmount>292980</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field. Three types of proposals will be supported by the program: Exploratory Projects that include proof-of-concept and feasibility studies; more extensive Full-Scale Projects; and workshops and conferences. &lt;br/&gt;&lt;br/&gt;The PIs propose to study the validity of measures of science achievement. They will assess validity gaps (when they exist) and build a data base for critical dialogue about such gaps. Two major questions will be addressed by the proposed project. The first, what are the nature, extent and quality of validity evidence gaps for science education achievement testing in research and evaluation reports found in science education? The second, what are the research and policy implications related to potential sources of influence on the gaps. The work will generate tools to support capacity and infrastructure of the evaluation field (specifically, a data base with validity evidence for science education researches and evaluators of science education interventions).</AbstractNarration>
<MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1548098</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Gardner</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael K Gardner</PI_FULL_NAME>
<EmailAddress>mike.gardner@utah.edu</EmailAddress>
<PI_PHON>8015506162</PI_PHON>
<NSF_ID>000196807</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>Salt Lake City</CityName>
<StateCode>UT</StateCode>
<ZipCode>841128914</ZipCode>
<StreetAddress><![CDATA[1721 Campus Center Dr., SAEC 322]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>009Z</Code>
<Text>PRIME - Promoting Research and Innovatio</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~292980</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Science achievement measures are an essential part of determining which interventions improve science instruction.&nbsp; However, for these measures to be useful benchmarks, they must be valid: that is, there must be support for the intended interpretations of test scores for particular uses.&nbsp; This project reviewed the evidence adduced in support of the validity of science achievement measures found in 95 published articles found in the <em>Journal of Research in Science Teaching</em>.&nbsp; These articles primarily involved interventions designed to improve science instruction.&nbsp; The articles were coded for the presence or absence of 23 individual aspects of validity that came from 6 broader categories of validity.&nbsp; Three raters performed the coding, and intercoder reliability was quite high (all kappas for individual items were greater than 0.88).&nbsp; Results showed that aspects of validity (derived from the <em>Standards for Educational and Psychological Testing</em>) coded for were found at quite different levels across the articles reviewed.&nbsp; The category of &ldquo;internal structure: single or multiple homogeneous dimensions present in the assessment&rdquo; was found in 100% of the articles, and the categories &ldquo;content/construct: representativeness to a specified domain&rdquo; and &ldquo;content/construct: relevance to interpretation of test scores&rdquo; were found in 99%.&nbsp; On the other hand, the categories &ldquo;relationship to external variables: discriminant validity evidence,&rdquo; &ldquo;consequences of testing: teacher,&rdquo; and &ldquo;consequences of testing: organization&rdquo; were not present in any of the articles.&nbsp; In general, validity evidence related to content/construct aspects of validity was the most likely to be present in this corpus of articles, while evidence related to the consequences of testing was least likely to present.</p> <p>The articles themselves varied widely in terms of the number of pieces of validity evidence presented: the article with the most pieces of evidence had 60.9% of the aspects coded for present, while the articles with the least pieces of evidence had 17.4% of the aspects coded for present.&nbsp; While it might be tempting to use this percentage as an index of the validity quality of each article, we would caution against such an interpretation.&nbsp; Each article has a different content and context, and only some of the aspects coded for would be relevant with each context.</p> <p>Our results overlap with results by Cizek, Rosenberg, and Koons (2008, for test reviewed from the Buros Mental Measurement Yearbook) and Sussman and Wilson (2016, for math and science interventions funded by IES).&nbsp; However, there were differences as well related to types of material each set of authors reviewed.&nbsp; Cizek et al. found low rates of evidence related for consequences of testing and think aloud protocols; we found low rates for consequences of testing, but relatively moderate rates for think aloud protocols.&nbsp; This likely was due to the interests of researchers in our sample, and their relatively small sample sizes that allowed videotaping of student performance.&nbsp; Soundness of the validity argument was found to be low in the present project as well as Sussman and Wilson&rsquo;s study.&nbsp; Evidence on content/construct aspects of validity were found at high levels in all three studies.</p> <p>The broader implications of the current project are found in ways that we might close the gap between the suggestions made in the <em>Standards</em>, and how validity evidence is accrued in actual practice. &nbsp;It is fairly clear that the current commercial test developers are not routinely adhering to the <em>Standards</em>, broadly speaking, when providing validity evidence to test users.&nbsp; Aspects such as content/construct evidence of validity are usually presented.&nbsp; For commercial tests, convergent and predictive validity evidence is also often presented.&nbsp; But, we must keep in mind that tests today are often modified by school districts or governmental agencies.&nbsp; The final product of these ventures is often not validated, even though the original assessment may have been.&nbsp; It is essential that all those involved in the testing process &ndash; test developers, sponsors, test takers, test interpreters, trainers of users, reviewers, and researchers &ndash; take responsibility for the testing process.&nbsp; This calls for all parties to have agency, human agency, in the process.&nbsp; Tests need to go beyond what the individual can do on the test, to what the individual might do with appropriate scaffolding.&nbsp; Clearly, all participants in the testing process need appropriate education in what validity is and how it can be accessed in particular contexts and for particular uses of test scores.&nbsp; Finally, all stakeholders need to have a voice in the process of test development.&nbsp; In today&rsquo;s complex world of testing and assessment, validity evidence can no longer be the sole domain of large test development companies and school districts: it needs to be distributed among all stakeholders, especially those closest to students being assessed (e.g., teachers, local administrators, and parents).</p><br> <p>            Last Modified: 09/04/2018<br>      Modified by: Michael&nbsp;K&nbsp;Gardner</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Science achievement measures are an essential part of determining which interventions improve science instruction.  However, for these measures to be useful benchmarks, they must be valid: that is, there must be support for the intended interpretations of test scores for particular uses.  This project reviewed the evidence adduced in support of the validity of science achievement measures found in 95 published articles found in the Journal of Research in Science Teaching.  These articles primarily involved interventions designed to improve science instruction.  The articles were coded for the presence or absence of 23 individual aspects of validity that came from 6 broader categories of validity.  Three raters performed the coding, and intercoder reliability was quite high (all kappas for individual items were greater than 0.88).  Results showed that aspects of validity (derived from the Standards for Educational and Psychological Testing) coded for were found at quite different levels across the articles reviewed.  The category of "internal structure: single or multiple homogeneous dimensions present in the assessment" was found in 100% of the articles, and the categories "content/construct: representativeness to a specified domain" and "content/construct: relevance to interpretation of test scores" were found in 99%.  On the other hand, the categories "relationship to external variables: discriminant validity evidence," "consequences of testing: teacher," and "consequences of testing: organization" were not present in any of the articles.  In general, validity evidence related to content/construct aspects of validity was the most likely to be present in this corpus of articles, while evidence related to the consequences of testing was least likely to present.  The articles themselves varied widely in terms of the number of pieces of validity evidence presented: the article with the most pieces of evidence had 60.9% of the aspects coded for present, while the articles with the least pieces of evidence had 17.4% of the aspects coded for present.  While it might be tempting to use this percentage as an index of the validity quality of each article, we would caution against such an interpretation.  Each article has a different content and context, and only some of the aspects coded for would be relevant with each context.  Our results overlap with results by Cizek, Rosenberg, and Koons (2008, for test reviewed from the Buros Mental Measurement Yearbook) and Sussman and Wilson (2016, for math and science interventions funded by IES).  However, there were differences as well related to types of material each set of authors reviewed.  Cizek et al. found low rates of evidence related for consequences of testing and think aloud protocols; we found low rates for consequences of testing, but relatively moderate rates for think aloud protocols.  This likely was due to the interests of researchers in our sample, and their relatively small sample sizes that allowed videotaping of student performance.  Soundness of the validity argument was found to be low in the present project as well as Sussman and Wilson?s study.  Evidence on content/construct aspects of validity were found at high levels in all three studies.  The broader implications of the current project are found in ways that we might close the gap between the suggestions made in the Standards, and how validity evidence is accrued in actual practice.  It is fairly clear that the current commercial test developers are not routinely adhering to the Standards, broadly speaking, when providing validity evidence to test users.  Aspects such as content/construct evidence of validity are usually presented.  For commercial tests, convergent and predictive validity evidence is also often presented.  But, we must keep in mind that tests today are often modified by school districts or governmental agencies.  The final product of these ventures is often not validated, even though the original assessment may have been.  It is essential that all those involved in the testing process &ndash; test developers, sponsors, test takers, test interpreters, trainers of users, reviewers, and researchers &ndash; take responsibility for the testing process.  This calls for all parties to have agency, human agency, in the process.  Tests need to go beyond what the individual can do on the test, to what the individual might do with appropriate scaffolding.  Clearly, all participants in the testing process need appropriate education in what validity is and how it can be accessed in particular contexts and for particular uses of test scores.  Finally, all stakeholders need to have a voice in the process of test development.  In today?s complex world of testing and assessment, validity evidence can no longer be the sole domain of large test development companies and school districts: it needs to be distributed among all stakeholders, especially those closest to students being assessed (e.g., teachers, local administrators, and parents).       Last Modified: 09/04/2018       Submitted by: Michael K Gardner]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
