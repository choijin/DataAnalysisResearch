<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: CC: Large: A High-Performance Data Center Operating System</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>900000.00</AwardTotalIntnAmount>
<AwardAmount>900000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Today, many popular cloud data center applications spend a huge fraction of their time running operating system code.  Examples include the backend services implementing Facebook, Google, Amazon, and other popular websites.  These applications spend much of their time moving, processing, and storing data. In a traditional operating system, however, the operating system kernel mediates all network and storage access, as a means to provide application, data, and network security within the data center.  Thus, the code path for a network or storage request to traverse from application code through the kernel to the device hardware (and back again) is many times longer than the minimum required.&lt;br/&gt;&lt;br/&gt;This work streamlines the performance of these applications without compromising security by changing the roles of the operating systems kernel, application runtime library, and device hardware. Instead, the traditional role of the kernel is split in two.  Applications have direct access to virtualized I/O devices, allowing most I/O operations to skip the kernel entirely.  The kernel operates primarily in the control plane, establishing and limiting data plane connections in accordance with the operating system security policy.  &lt;br/&gt;&lt;br/&gt;The work has the potential for dramatic improvements in application and server performance, as well as data center energy consumption and protocol flexibility.  Network and storage intensive data center applications are used by literally billions of people around the globe on a daily basis.  By reducing the overhead of network and storage these applications, the hardware needed to support existing services can be reduced, making it cheaper for new services to be developed.</AbstractNarration>
<MinAmdLetterDate>07/20/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1518702</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Anderson</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas E Anderson</PI_FULL_NAME>
<EmailAddress>tom@cs.washington.edu</EmailAddress>
<PI_PHON>2065439348</PI_PHON>
<NSF_ID>000196821</NSF_ID>
<StartDate>07/20/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Arvind</FirstName>
<LastName>Krishnamurthy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arvind Krishnamurthy</PI_FULL_NAME>
<EmailAddress>arvind@cs.washington.edu</EmailAddress>
<PI_PHON>2066160957</PI_PHON>
<NSF_ID>000488256</NSF_ID>
<StartDate>07/20/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Xi</FirstName>
<LastName>Wang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xi Wang</PI_FULL_NAME>
<EmailAddress>xi@cs.washington.edu</EmailAddress>
<PI_PHON>2065435708</PI_PHON>
<NSF_ID>000684160</NSF_ID>
<StartDate>07/20/2015</StartDate>
<EndDate>07/14/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~588445</FUND_OBLG>
<FUND_OBLG>2017~311555</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Today, many popular data center applications spend a huge fraction of their time running operating system code to perform network and file I/O. In a traditional system, the kernel mediates access to device hardware to enforce process isolation as well as network and disk security. Typically, the kernel also provides higher-level functionality, such as a TCP/IP network protocol stack and sophisticated file and storage management. Thus, the code path for a network or disk request to traverse from application code through the kernel to the device hardware (and back again) is many times longer than the minimum required.</p> <p>&nbsp;</p> <p>We proposed a new approach to the co-design of the operating system kernel, application runtime library, and device hardware, with a goal of streamlining the performance of I/O-bound server applications. We believe our approach has the potential for dramatic improvements in application and server performance, as well as data center energy consumption and protocol flexibility.</p> <p>&nbsp;</p> <p>We proposed to split the traditional role of the kernel in two. In our model, applications have direct access to virtualized I/O devices, allowing most I/O operations to skip the kernel entirely. The kernel operates primarily in the control plane, establishing and limiting data plane connections in accordance with the operating system security policy. A key enabler is emerging I/O device hardware trends. Our aim was to understand the implications of this new paradigm for application, operating system, and hardware design. What data plane interface is needed to achieve maximal user-level performance and flexibility (e.g., transmit/receive queues, RDMA, SCSI)?&nbsp; What control plane interface is needed to allow the operating system kernel to enforce security policy (e.g., to prevent a rogue application from flooding the network)?&nbsp; How do we implement congestion control in the data center, when applications have direct access to the network?&nbsp; How should the operating system kernel be re-designed to setup and manage data paths?&nbsp; How should we specialize application runtime libraries once the network stack and file system can become application-specific?&nbsp; What is the impact of on-chip network and disk interfaces for steering incoming I/O to the appropriate core?&nbsp; To our knowledge, none of them were well-addressed in the research literature.</p> <p>&nbsp;</p> <p>We also did research this year with undergraduates as part of the Arrakis project (broadly all of them were working on aspects of designing and prototyping a way to extend Arrakis to support container virtualization). The other part of the experiment was to see whether it would be possible to scale research experiences to more undergraduates.</p> <p>&nbsp;</p> <p>I/O-intensive data center applications are used by literally billions of people around the globe on a daily basis. By reducing the overhead of I/O, we can dramatically reduce the cost of provisioning existing public services, like Wikipedia, as well as make it much cheaper for new public services to be developed.&nbsp; As importantly, if the research approach is successful, we will need to change how we teach operating systems to both undergraduate and graduate students. We plan to develop lecture notes, slides, and assignments to better prepare students for the changes sweeping through data center operating systems.</p> <p>&nbsp;</p> <p>We published a paper regarding TAS in Eurosys 2019 and Slim in NSDI 2019. We have open sourced the TAS/FlexNIC code, and researchers at University of Wisconsin, Madison, are already building on top of this code in order to engineer other high-performance datacenter applications.&nbsp; Related papers were also published at NSDI&rsquo;17 and NSDI&rsquo;18.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/05/2019<br>      Modified by: Arvind&nbsp;Krishnamurthy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today, many popular data center applications spend a huge fraction of their time running operating system code to perform network and file I/O. In a traditional system, the kernel mediates access to device hardware to enforce process isolation as well as network and disk security. Typically, the kernel also provides higher-level functionality, such as a TCP/IP network protocol stack and sophisticated file and storage management. Thus, the code path for a network or disk request to traverse from application code through the kernel to the device hardware (and back again) is many times longer than the minimum required.     We proposed a new approach to the co-design of the operating system kernel, application runtime library, and device hardware, with a goal of streamlining the performance of I/O-bound server applications. We believe our approach has the potential for dramatic improvements in application and server performance, as well as data center energy consumption and protocol flexibility.     We proposed to split the traditional role of the kernel in two. In our model, applications have direct access to virtualized I/O devices, allowing most I/O operations to skip the kernel entirely. The kernel operates primarily in the control plane, establishing and limiting data plane connections in accordance with the operating system security policy. A key enabler is emerging I/O device hardware trends. Our aim was to understand the implications of this new paradigm for application, operating system, and hardware design. What data plane interface is needed to achieve maximal user-level performance and flexibility (e.g., transmit/receive queues, RDMA, SCSI)?  What control plane interface is needed to allow the operating system kernel to enforce security policy (e.g., to prevent a rogue application from flooding the network)?  How do we implement congestion control in the data center, when applications have direct access to the network?  How should the operating system kernel be re-designed to setup and manage data paths?  How should we specialize application runtime libraries once the network stack and file system can become application-specific?  What is the impact of on-chip network and disk interfaces for steering incoming I/O to the appropriate core?  To our knowledge, none of them were well-addressed in the research literature.     We also did research this year with undergraduates as part of the Arrakis project (broadly all of them were working on aspects of designing and prototyping a way to extend Arrakis to support container virtualization). The other part of the experiment was to see whether it would be possible to scale research experiences to more undergraduates.     I/O-intensive data center applications are used by literally billions of people around the globe on a daily basis. By reducing the overhead of I/O, we can dramatically reduce the cost of provisioning existing public services, like Wikipedia, as well as make it much cheaper for new public services to be developed.  As importantly, if the research approach is successful, we will need to change how we teach operating systems to both undergraduate and graduate students. We plan to develop lecture notes, slides, and assignments to better prepare students for the changes sweeping through data center operating systems.     We published a paper regarding TAS in Eurosys 2019 and Slim in NSDI 2019. We have open sourced the TAS/FlexNIC code, and researchers at University of Wisconsin, Madison, are already building on top of this code in order to engineer other high-performance datacenter applications.  Related papers were also published at NSDI?17 and NSDI?18.          Last Modified: 03/05/2019       Submitted by: Arvind Krishnamurthy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
