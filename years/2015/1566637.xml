<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: SHF: Investigation of Effective On-chip Network Designs for GPUs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2016</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>174978.00</AwardTotalIntnAmount>
<AwardAmount>174978</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Graphics Processing Units (GPUs) have been proliferating at an extraordinary speed in the past decade. Continuing innovations in related technologies allow today?s GPUs to play critical roles in numerous disciplines and sectors as well as many emerging fields that might not otherwise be possible. Examples include processing ambient video inputs in automobiles for enhanced safety and intelligent driving; powering graphics-based medical processing applications in mobile devices for ubiquitous biometric monitoring and personalized healthcare; supporting virtual reality headsets for transformative and immersive new experiences in education, training, and entertainment; and providing energy-efficient parallel computing in HPC systems and data-centers to facilitate a myriad of scientific, economic, and social computing applications. Such promising developments are enabled by the massively parallel computing capacity of GPU architectures, which can integrate thousands of processing cores on a single chip. To continue meeting growing performance expectations, on-chip interconnect architectures must be developed to provide fast and efficient communications among the vast number of processing cores in GPUs.&lt;br/&gt;&lt;br/&gt;This research investigates cross-cutting approaches and techniques to improve the effectiveness of on-chip networks (or NoCs) in GPU systems. The objective is to fully explore the challenges and develop framework useful for GPU NoC designs that will meet the performance, energy, and resource efficiency targets of current and future GPU systems. Among some of the specific aspects investigated are the bottlenecks of NoCs in the GPU context, alternative methods of enabling scale-up, sensitivity of NoCs to various types of GPU applications, and the impact of NoCs on GPU system-level trade-offs. This research also investigates opportunities in coordinated design among NoC components as well as co-optimizations between NoCs and other GPU subsystems. The objective is to enable on-chip networks to operate more consistently and efficiently for the overall benefit of GPU systems by factoring in multiple components and key application characteristics. Beyond its specific technical contributions to fundamental advancements in computing, this research has broader potential impact to society through its activities on research education and outreach that aim to broaden participation for people from diverse background, including groups underrepresented in engineering at various education levels.</AbstractNarration>
<MinAmdLetterDate>02/16/2016</MinAmdLetterDate>
<MaxAmdLetterDate>02/16/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1566637</AwardID>
<Investigator>
<FirstName>Lizhong</FirstName>
<LastName>Chen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lizhong Chen</PI_FULL_NAME>
<EmailAddress>chenliz@oregonstate.edu</EmailAddress>
<PI_PHON>5417373317</PI_PHON>
<NSF_ID>000689417</NSF_ID>
<StartDate>02/16/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon State University</Name>
<CityName>Corvallis</CityName>
<ZipCode>973318507</ZipCode>
<PhoneNumber>5417374933</PhoneNumber>
<StreetAddress>OREGON STATE UNIVERSITY</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>053599908</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OREGON STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053599908</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Oregon State University]]></Name>
<CityName/>
<StateCode>OR</StateCode>
<ZipCode>973318507</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~174978</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The overall goal of the research project has been to develop cross-cutting approaches and techniques to improve the effectiveness of on-chip networks (a.k.a., NoCs) in GPU systems where thousands of processing cores are integrated on a single chip. It is critical to support on-chip communications among the vast number of processing cores and provide fast and efficient data transfer to/from memory to feed concurrent computations. To that end, this research has conducted investigation to increase fundamental understanding of various key aspects of on-chip networks in GPUs; explore the opportunities in utilizing coordinated designs among NoC components to address the potential bottlenecks; investigate innovative approaches and techniques that co-optimize NoCs with other GPU subsystems to achieve higher overall efficiency and better consistency; evaluate and assess the benefits of the proposed techniques under a wide range of configurations and application workloads; and, furthermore, provide full understanding of the trade-offs not only within the on-chip network but also across the GPU system taking into account performance, energy, and resource efficiency.</p> <p>There are a number of specific outcomes of this research, three of which are the following. One significant outcome is to characterize GPU NoC traffic behaviors by employing machine learning methods (DNN and T-SNE algorithms to be specific) to identify eight major spatial traffic patterns and several temporal traffic patterns that are manifested across various workloads. This is the first work to reveal these interesting data communication patterns in GPUs. Another specific outcome is to address the performance bottleneck in GPU NoCs by providing fast and high throughput injection from both supply and consumption aspects of the injection. In the supply aspect, the network interface is augmented with a split injection queue structure and widened interconnects, allowing data to reach the injection points at the raw injection rate of memory controllers. In the consumption aspect, ports in crossbar switches are allocated to injection ports in a more balanced way to help quickly transfer the injected data packets out of the &ldquo;hot regions&rdquo; around memory controllers. Multi-level packet prioritization and silicon interposers are also utilized to further alleviate the bottleneck. Yet another specific outcome is to explore the opportunities of co-optimizing NoCs with GPU cache subsystems. A dynamically linked MSHR (DL-MSHR) architecture was proposed, where MSHR entries are dynamically formed from a pool of available slots. DL-MSHR can self-adapt to primary-miss-predominant applications by forming more entries with fewer slots, and self-adapt to secondary-miss-predominant applications by having fewer entries but more slots per entry. There are several other specific outcomes and, ultimately, the research carried out in this project has established systematic and comprehensive, empirically-based methods, soundly grounded by theoretical support, to allow the investigation of promising new approaches and techniques to increase the effectiveness of GPU NoC architectures.</p> <p>Four Ph.D. students and three Master students received training on, and contributed significantly to, the research carried out on this project. Over nine publications, including two Master theses, and publicly available simulation infrastructure are additional outcomes of this research. Findings of this research have been incorporated into curriculum and course content at the PI&rsquo;s home institution, e.g., in &ldquo;Computer Architecture&rdquo; courses at undergraduate and graduate levels, and special topic courses &ldquo;Interconnection Networks&rdquo; and &ldquo;GPU Architecture&rdquo; at graduate level. Outreach efforts have been made to broaden participation of underrepresented minorities in computing research and education through this project.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/30/2019<br>      Modified by: Lizhong&nbsp;Chen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The overall goal of the research project has been to develop cross-cutting approaches and techniques to improve the effectiveness of on-chip networks (a.k.a., NoCs) in GPU systems where thousands of processing cores are integrated on a single chip. It is critical to support on-chip communications among the vast number of processing cores and provide fast and efficient data transfer to/from memory to feed concurrent computations. To that end, this research has conducted investigation to increase fundamental understanding of various key aspects of on-chip networks in GPUs; explore the opportunities in utilizing coordinated designs among NoC components to address the potential bottlenecks; investigate innovative approaches and techniques that co-optimize NoCs with other GPU subsystems to achieve higher overall efficiency and better consistency; evaluate and assess the benefits of the proposed techniques under a wide range of configurations and application workloads; and, furthermore, provide full understanding of the trade-offs not only within the on-chip network but also across the GPU system taking into account performance, energy, and resource efficiency.  There are a number of specific outcomes of this research, three of which are the following. One significant outcome is to characterize GPU NoC traffic behaviors by employing machine learning methods (DNN and T-SNE algorithms to be specific) to identify eight major spatial traffic patterns and several temporal traffic patterns that are manifested across various workloads. This is the first work to reveal these interesting data communication patterns in GPUs. Another specific outcome is to address the performance bottleneck in GPU NoCs by providing fast and high throughput injection from both supply and consumption aspects of the injection. In the supply aspect, the network interface is augmented with a split injection queue structure and widened interconnects, allowing data to reach the injection points at the raw injection rate of memory controllers. In the consumption aspect, ports in crossbar switches are allocated to injection ports in a more balanced way to help quickly transfer the injected data packets out of the "hot regions" around memory controllers. Multi-level packet prioritization and silicon interposers are also utilized to further alleviate the bottleneck. Yet another specific outcome is to explore the opportunities of co-optimizing NoCs with GPU cache subsystems. A dynamically linked MSHR (DL-MSHR) architecture was proposed, where MSHR entries are dynamically formed from a pool of available slots. DL-MSHR can self-adapt to primary-miss-predominant applications by forming more entries with fewer slots, and self-adapt to secondary-miss-predominant applications by having fewer entries but more slots per entry. There are several other specific outcomes and, ultimately, the research carried out in this project has established systematic and comprehensive, empirically-based methods, soundly grounded by theoretical support, to allow the investigation of promising new approaches and techniques to increase the effectiveness of GPU NoC architectures.  Four Ph.D. students and three Master students received training on, and contributed significantly to, the research carried out on this project. Over nine publications, including two Master theses, and publicly available simulation infrastructure are additional outcomes of this research. Findings of this research have been incorporated into curriculum and course content at the PI?s home institution, e.g., in "Computer Architecture" courses at undergraduate and graduate levels, and special topic courses "Interconnection Networks" and "GPU Architecture" at graduate level. Outreach efforts have been made to broaden participation of underrepresented minorities in computing research and education through this project.          Last Modified: 06/30/2019       Submitted by: Lizhong Chen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
