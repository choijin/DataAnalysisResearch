<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Computational Models of Essay Rewritings</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>299912.00</AwardTotalIntnAmount>
<AwardAmount>307912</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Natural language processing (NLP) is an integral part of an intelligent tutoring system for writing; it allows the system to automatically analyze student writings and provide feedback to help students to learn. For example, methods have been developed to automatically detect and correct grammar usage errors and to assess aspects of student writing. However, current technology does not offer enough support for teaching students to revise their writings. Unlike mechanical error corrections, the rationales behind revisions are harder to determine. There may be multiple possible changes for an unclear passage in a draft; conversely, one specific writing change might be due to several possible underlying reasons. This EAGER award investigates whether NLP methods can help students to learn to make a more concrete connection between the abstract principles of rewriting (e.g., "A paper should have a clear thesis") and the particular contexts in which the revision is carried out. The success of this project would enable educational applications that benefit the society.&lt;br/&gt; &lt;br/&gt;This project evaluates the viability of revision as a pedagogical technique by determining whether student interactions with the revision assistant enables them to learn to write better -- that is, whether certain forms of the feedback (in terms of the perceived purposes and scopes of changes) encourage students to learn to make more effective revisions. More specifically, the project works toward three objectives: &lt;br/&gt;(1) Define a schema for characterizing the types of changes that occur at different levels of the rewriting.  For example, the writer might add one or more sentences to provide evidence to support a thesis; or the writer might add just one or two words to make a phrase more precise. &lt;br/&gt;(2) Based on the schema, design a computational model for recognizing the purpose and scope of each change within a revision. One application of such a model is a revision assistant that serves as a sounding board for students as they experiment with different revision alternatives. &lt;br/&gt;(3) Conduct experiments to study the interactions between students and the revision writing environment in which variations of idealized computational models are simulated. The findings of the experiments pave the way for developing better technologies to support for student learning.</AbstractNarration>
<MinAmdLetterDate>08/05/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1550635</AwardID>
<Investigator>
<FirstName>Diane</FirstName>
<LastName>Litman</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Diane J Litman</PI_FULL_NAME>
<EmailAddress>litman@cs.pitt.edu</EmailAddress>
<PI_PHON>4126241261</PI_PHON>
<NSF_ID>000233759</NSF_ID>
<StartDate>08/05/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rebecca</FirstName>
<LastName>Hwa</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rebecca Hwa</PI_FULL_NAME>
<EmailAddress>hwa@cs.pitt.edu</EmailAddress>
<PI_PHON>4126247400</PI_PHON>
<NSF_ID>000462757</NSF_ID>
<StartDate>08/05/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pittsburgh</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152133203</ZipCode>
<PhoneNumber>4126247400</PhoneNumber>
<StreetAddress>300 Murdoch Building</StreetAddress>
<StreetAddress2><![CDATA[3420 Forbes Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004514360</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF PITTSBURGH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004514360</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pittsburgh]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152132303</ZipCode>
<StreetAddress><![CDATA[210 S. Bouquet St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~299912</FUND_OBLG>
<FUND_OBLG>2016~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="section"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>This project explores the extent to which a computer writing environment can offer effective guidance to student writers and encourage them to revise their drafts more effectively. More specifically, our project asks whether students would benefit from a system that can recognize the intentions behind their revisions. Our vision is that one day, computers might serve as a sounding board, an intelligent feedback system that evaluates whether the students' revisions are succeeding in improving what the students hope to communicate. To move toward that end, this project focuses on three exploratory activities:</span></p> <div class="column">1) Define a schema for characterizing the types of revisions&nbsp;that occur at different levels of the rewriting. For example, the writer might add one or more sentences to provide evidence to support a thesis; or the writer might add just one or two words to make a phrase more precise.</div> <div class="column">2) Design computational models for recognizing the intention of each revision based on the schema we defined. We also design appropriate interface prototypes so that the student writers may easily interact with the underlying computational revision model.</div> <div class="column">3) Conduct a series of "Wizard of Oz" experiments (i.e., the "computer outputs" are actually provided by trained humans) to study the interactions between the students and different revision prototypes.&nbsp; This experimental paradigm allows us to separate the problem of developing a good computational model from the problem of understanding the interactions between students and the interface. The experiment aims to evaluate the viability of revision as a pedagogical technique by determining whether interactions with the revision assistant can improve student writing -- that is, whether certain forms of the feedback (in terms of the perceived purposes and scopes of changes) encourage effective revision more than others.</div> <ol> </ol></div> </div> </div> </div> </div> <div class="page" title="Page 2"> <div class="section"> <div class="section"> <div class="layoutArea"> <div class="column"> <p>The outcomes of this project are as follow:</p> <div class="column">We have defined a hierarchical revision schema. At a coarser level, revisions are distinguished between "surface" revisions, which are more about mechanical issues, and "content" revisions, which are more about the semantics of what the writers want to communicate. The surface revisions are further divided into specific categories such as "grammatical revisions," "fluency,"&nbsp; and "reordering"; The content revisions are further divded into revisions about "thesis/ideas," "evidence," "rebuttal," "reasoning," and "precision."</div> <div class="column"></div> <div class="column">We have developed an automatic classifier that predicts the intention of a revision sentence pair. While it is reasonably good at distinguishing between "surface" and "content" revisions, classification at the finer categories is much more challenging. One obstacle is the lack of enough&nbsp; training data, which we are continuing to address through our user studies.</div> <div class="column"></div> <div class="column">As a part of our "Wizard of Oz" user study, we have developed a corpus of revision data. This collection consists of three essays (one original plus two revisions) written by sixty participants. Each pair of sequential drafts is manually compared and coded by two annotators according to our revision schema. The drafts have also been graded according to a writing evaluation rubric.</div> <div class="column">At the conclusion of our "Wizard of Oz" user study, we find that feedback on revisions do impact how students review and rewrite their drafts. However, there are many factors at play, including the interface design and the students' linguistic backgrounds. For instance, participants who use an interface that highlights their revision intentions are more likely to agree that "The system helps me to recognize the weakness of my essay;" however, the number of revisions they make are not significantly more numerous than participants who used the control interface. Yet, if students were alerted to how their revisions are perceived, they overwhelmingly opted to re-revise it when the system perceived a different intention for the change (84.9% of the time).</div> <div class="column"></div> <div class="column">With respect to improvements in writing quality, we find that typically students' writings do improve with further revisions;&nbsp; however, we do not observe significant overall differences between participants who used our interface over the control interface. These findings suggest further future work in terms of interface design and data collection in order to better tease out the nuanced relationship between all these factors that might influence a student's revision process.</div> <div class="column"><ol> </ol></div> </div> </div> </div> </div> </div><br> <p>            Last Modified: 12/25/2018<br>      Modified by: Rebecca&nbsp;Hwa</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      This project explores the extent to which a computer writing environment can offer effective guidance to student writers and encourage them to revise their drafts more effectively. More specifically, our project asks whether students would benefit from a system that can recognize the intentions behind their revisions. Our vision is that one day, computers might serve as a sounding board, an intelligent feedback system that evaluates whether the students' revisions are succeeding in improving what the students hope to communicate. To move toward that end, this project focuses on three exploratory activities: 1) Define a schema for characterizing the types of revisions that occur at different levels of the rewriting. For example, the writer might add one or more sentences to provide evidence to support a thesis; or the writer might add just one or two words to make a phrase more precise. 2) Design computational models for recognizing the intention of each revision based on the schema we defined. We also design appropriate interface prototypes so that the student writers may easily interact with the underlying computational revision model. 3) Conduct a series of "Wizard of Oz" experiments (i.e., the "computer outputs" are actually provided by trained humans) to study the interactions between the students and different revision prototypes.  This experimental paradigm allows us to separate the problem of developing a good computational model from the problem of understanding the interactions between students and the interface. The experiment aims to evaluate the viability of revision as a pedagogical technique by determining whether interactions with the revision assistant can improve student writing -- that is, whether certain forms of the feedback (in terms of the perceived purposes and scopes of changes) encourage effective revision more than others.             The outcomes of this project are as follow: We have defined a hierarchical revision schema. At a coarser level, revisions are distinguished between "surface" revisions, which are more about mechanical issues, and "content" revisions, which are more about the semantics of what the writers want to communicate. The surface revisions are further divided into specific categories such as "grammatical revisions," "fluency,"  and "reordering"; The content revisions are further divded into revisions about "thesis/ideas," "evidence," "rebuttal," "reasoning," and "precision."  We have developed an automatic classifier that predicts the intention of a revision sentence pair. While it is reasonably good at distinguishing between "surface" and "content" revisions, classification at the finer categories is much more challenging. One obstacle is the lack of enough  training data, which we are continuing to address through our user studies.  As a part of our "Wizard of Oz" user study, we have developed a corpus of revision data. This collection consists of three essays (one original plus two revisions) written by sixty participants. Each pair of sequential drafts is manually compared and coded by two annotators according to our revision schema. The drafts have also been graded according to a writing evaluation rubric. At the conclusion of our "Wizard of Oz" user study, we find that feedback on revisions do impact how students review and rewrite their drafts. However, there are many factors at play, including the interface design and the students' linguistic backgrounds. For instance, participants who use an interface that highlights their revision intentions are more likely to agree that "The system helps me to recognize the weakness of my essay;" however, the number of revisions they make are not significantly more numerous than participants who used the control interface. Yet, if students were alerted to how their revisions are perceived, they overwhelmingly opted to re-revise it when the system perceived a different intention for the change (84.9% of the time).  With respect to improvements in writing quality, we find that typically students' writings do improve with further revisions;  however, we do not observe significant overall differences between participants who used our interface over the control interface. These findings suggest further future work in terms of interface design and data collection in order to better tease out the nuanced relationship between all these factors that might influence a student's revision process.              Last Modified: 12/25/2018       Submitted by: Rebecca Hwa]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
