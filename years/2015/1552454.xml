<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Social Networks Based Concept Learning in Images</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The need for easy, quick, and intuitive search of visual data at the conceptual level is universal. This project will explore a novel network analysis-based approach to searching visual data, ultimately leading to visual search engines that would make searching images as easy as searching by keywords is today. Accomplishing this requires new approaches to machine learning of visual concepts. This project proposes a formal framework that unifies the ideas from social networks and semantic concept learning so multiple semantic concepts can be learned with high confidence. Specifically, the approach utilizes hierarchical co-occurrence correlation among concepts as cues to help the detection of individual visual concepts. The sources for robustness are the learning of co-occurrence patterns, similar to community structures in a social network, and their refinement over time. The success of concept co-occurrence detection will simplify management of personal image data with automatic tagging. With semantically organized personal content, the preferences of a user can be learned to provide personalization of various contents that he/she consumes online. This will have a broad impact for diverse applications ranging from information technology to physical, life and social sciences to intelligence organizations to news bureaus. Forensics analysis of digital data would be greatly speeded up as humans do not have to sift through large amount of data. Extension of the techniques to video, music, and multi-modal data would also provide similar ease for content consumption. The research team provides an environment integrating education and workforce development with research, and with recruiting and retaining a diverse group of students. Complementing the research activities will be new initiatives in education and public outreach. &lt;br/&gt;&lt;br/&gt;The project develops a transformative approach to explore the acquisition and refinement of semantic visual concepts systematically. First, it discovers the hierarchical co-occurrence patterns of concepts as underlying community structures in the co-occurrence network. The co-occurrence patterns play roles similar to underlying scene concepts at a higher level of semantics. Second, it proposes an approach for selecting visually-consistent-semantic concepts. Since concepts vary in their visual complexity, visual-semantic relatedness of each concept is investigated by quantitatively measuring the within-concept visual variability and the visual distances to the other concepts such that they can be modeled more reliably and detected more easily. Third, the project introduces a novel image content descriptor called concept signature that can record both the semantic concept and the corresponding confidence value inferred from low-level features. Finally, the project proposes techniques for scalability to handle and evaluate the performance on large databases by developing open source techniques and software tools. The results will be broadly disseminated through the project website (http://vislab.ucr.edu/RESEARCH/VisualSemanticConcepts/VSC.php), via regular releases of software tools and offering tutorials/workshops at major IEEE/ACM conferences.</AbstractNarration>
<MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1552454</AwardID>
<Investigator>
<FirstName>Bir</FirstName>
<LastName>Bhanu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bir Bhanu</PI_FULL_NAME>
<EmailAddress>bhanu@ece.ucr.edu</EmailAddress>
<PI_PHON>9518273954</PI_PHON>
<NSF_ID>000126207</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Riverside</Name>
<CityName>RIVERSIDE</CityName>
<ZipCode>925210217</ZipCode>
<PhoneNumber>9518275535</PhoneNumber>
<StreetAddress>Research &amp; Economic Development</StreetAddress>
<StreetAddress2><![CDATA[245 University Office Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>44</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA44</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>627797426</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Riverside]]></Name>
<CityName>Riverside</CityName>
<StateCode>CA</StateCode>
<ZipCode>925210001</ZipCode>
<StreetAddress><![CDATA[200  University Office Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>41</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA41</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The need for search of visual data at the conceptual level is universal. This project develops machine learning frameworks that unify ideas from social networks and spatio-temporal semantic concept learning so multiple concepts can be learned with high confidence. Motivated by the fact that multiple concepts that frequently co-occur across images form patterns which could provide contextual cues for individual concept inference, the objectives of the project were: (a) Develop a social network inspired formal framework for finding hierarchical co-occurrence correlation among concepts, and use these patterns of co-occurrence as contextual cues to improve the detection of individual concepts in multimedia databases. (b) Develop algorithms to select visually consistent semantic concepts. (c) Develop an image content descriptor called concept signature that can record both the semantic concept and the corresponding confidence value inferred from low level image features. (d) Evaluate the effectiveness of the proposed approach in two application domains: automatic image annotation and concept based image/video retrieval.</p> <p><strong>&nbsp;</strong></p> <p><strong>Intellectual Merit:</strong> The project made three key contributions which will have a broad impact for diverse applications ranging from information technology to physical, life and social sciences to intelligence organizations to news bureaus.</p> <p>&nbsp;</p> <p>1)&nbsp;&nbsp;&nbsp;&nbsp; It developed methods for co-occurrence pattern detection that leverage both the global and local co-occurrences as well as utilize both the semantic and visual information. It developed methods for automated image annotation that provide superior performance when the images have higher complexities. For image retrieval the proposed hierarchical concept co-occurrence patterns can boost the individual concept inference. This work is presented in detail in a paper by Feng and Bhanu in Semantic Concept Co-Occurrence Patterns for Image Annotation and Retrieval. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>. 38 (4), 2016.</p> <p>2)&nbsp;&nbsp;&nbsp;&nbsp; It developed a method based on visual attributes for person re-identification where the challenges include illumination, pose and viewpoint changes among non-overlapping camera views. First, detect the attributes in images/videos by using deep learning-based convolutional neural networks. Second, compute the dependencies among attributes by mining association rules that are used to refine the attributes classification results. Third, transfer the attribute learning task to person re-identification in video by using metric learning technique. Finally, integrate the attributes-based approach into an appearance-based method for video-based person re-identification and evaluate the results on benchmark datasets. Experimental results on two benchmark datasets indicate that attributes can provide improvements both in accuracy and generalization capabilities. This work was presented at <em>14th IEEE International Conference on Advanced Video and Signal Based Surveillance</em>, 2017 by X. Zhang, F. Pala and B. Bhanu, ?Attributes co-occurrence pattern mining for video-based person re-identification.?</p> <p>3)&nbsp;&nbsp;&nbsp;&nbsp; It developed a method to learn the unbiased spatial and temporal representation for video. For video analytic tasks, most of the current research aims to encode the temporal and spatial information by using convolutional neural networks (CNNs) to extract spatial features and recurrent neural networks (RNNs) or their variations to discover the time dependencies. However, it ignores the effect of the complex background, which leads to a biased spatial representation. A new method is developed to learn an unbiased semantic representation for video-based person re-identification. To handle the background clutter and occlusions, a pedestrian segmentation method is used to obtain the silhouette of the body. After the segmentation, an unbiased Siamese bi-directional recurrent convolutional neural network architecture is developed. Experimental results on three public datasets demonstrate the effectiveness of the proposed method. The proposed method is capable of learning discriminative spatial representation by substituting invariant background and identifying the weights of frames independent of their positions in a video for learning a semantic concept.&nbsp; This work was presented in a paper by Zhang and Bhanu, ?An unbiased temporal representation for video-based person re-identification,? <em>IEEE International Conference on Image Processing</em>, 2018.</p> <p>&nbsp;</p> <p><strong>Broader Impacts:</strong> The project trained a female PhD student in computer science and engineering who was supported on this project. There was a strong collaboration with the other students in the laboratory as well. All the software was developed on GPU hardware. Papers were presented at international conferences which attracted an international student and a faculty to join our laboratory as a visitor for one to two years. The project provided continuous mentoring, teaching, improvement of written/oral communication skills and leadership opportunities, internship in industry and weekly seminars. Research was presented to underrepresented students to encourage them for a career in computer science and engineering. The success of concept co-occurrence detection will simplify management of personal image data with automatic tagging. With semantically organized personal content, the preferences of a user can be learned to provide personalization of various contents that he/she consumes online. The capture of unbiased spatio-temporal semantics will open new opportunities for video-based querying and searching video databases for many practical applications.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/14/2018<br>      Modified by: Bir&nbsp;Bhanu</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1552454/1552454_10393665_1544829617908_Framework--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1552454/1552454_10393665_1544829617908_Framework--rgov-800width.jpg" title="System for semantic concept co-occurrence"><img src="/por/images/Reports/POR/2018/1552454/1552454_10393665_1544829617908_Framework--rgov-66x44.jpg" alt="System for semantic concept co-occurrence"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Semantic concept co-occurrence patterns for image annotation and retrieval,</div> <div class="imageCredit">Linan Feng and Bir Bhanu</div> <div class="imageSubmitted">Bir&nbsp;Bhanu</div> <div class="imageTitle">System for semantic concept co-occurrence</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1552454/1552454_10393665_1544830218355_Fig7--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1552454/1552454_10393665_1544830218355_Fig7--rgov-800width.jpg" title="Examples of video retrieval"><img src="/por/images/Reports/POR/2018/1552454/1552454_10393665_1544830218355_Fig7--rgov-66x44.jpg" alt="Examples of video retrieval"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Unbiased spatio-temporal semantics in video</div> <div class="imageCredit">Xiu Zhang and Bir Bhanu</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Bir&nbsp;Bhanu</div> <div class="imageTitle">Examples of video retrieval</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The need for search of visual data at the conceptual level is universal. This project develops machine learning frameworks that unify ideas from social networks and spatio-temporal semantic concept learning so multiple concepts can be learned with high confidence. Motivated by the fact that multiple concepts that frequently co-occur across images form patterns which could provide contextual cues for individual concept inference, the objectives of the project were: (a) Develop a social network inspired formal framework for finding hierarchical co-occurrence correlation among concepts, and use these patterns of co-occurrence as contextual cues to improve the detection of individual concepts in multimedia databases. (b) Develop algorithms to select visually consistent semantic concepts. (c) Develop an image content descriptor called concept signature that can record both the semantic concept and the corresponding confidence value inferred from low level image features. (d) Evaluate the effectiveness of the proposed approach in two application domains: automatic image annotation and concept based image/video retrieval.     Intellectual Merit: The project made three key contributions which will have a broad impact for diverse applications ranging from information technology to physical, life and social sciences to intelligence organizations to news bureaus.     1)     It developed methods for co-occurrence pattern detection that leverage both the global and local co-occurrences as well as utilize both the semantic and visual information. It developed methods for automated image annotation that provide superior performance when the images have higher complexities. For image retrieval the proposed hierarchical concept co-occurrence patterns can boost the individual concept inference. This work is presented in detail in a paper by Feng and Bhanu in Semantic Concept Co-Occurrence Patterns for Image Annotation and Retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence. 38 (4), 2016.  2)     It developed a method based on visual attributes for person re-identification where the challenges include illumination, pose and viewpoint changes among non-overlapping camera views. First, detect the attributes in images/videos by using deep learning-based convolutional neural networks. Second, compute the dependencies among attributes by mining association rules that are used to refine the attributes classification results. Third, transfer the attribute learning task to person re-identification in video by using metric learning technique. Finally, integrate the attributes-based approach into an appearance-based method for video-based person re-identification and evaluate the results on benchmark datasets. Experimental results on two benchmark datasets indicate that attributes can provide improvements both in accuracy and generalization capabilities. This work was presented at 14th IEEE International Conference on Advanced Video and Signal Based Surveillance, 2017 by X. Zhang, F. Pala and B. Bhanu, ?Attributes co-occurrence pattern mining for video-based person re-identification.?  3)     It developed a method to learn the unbiased spatial and temporal representation for video. For video analytic tasks, most of the current research aims to encode the temporal and spatial information by using convolutional neural networks (CNNs) to extract spatial features and recurrent neural networks (RNNs) or their variations to discover the time dependencies. However, it ignores the effect of the complex background, which leads to a biased spatial representation. A new method is developed to learn an unbiased semantic representation for video-based person re-identification. To handle the background clutter and occlusions, a pedestrian segmentation method is used to obtain the silhouette of the body. After the segmentation, an unbiased Siamese bi-directional recurrent convolutional neural network architecture is developed. Experimental results on three public datasets demonstrate the effectiveness of the proposed method. The proposed method is capable of learning discriminative spatial representation by substituting invariant background and identifying the weights of frames independent of their positions in a video for learning a semantic concept.  This work was presented in a paper by Zhang and Bhanu, ?An unbiased temporal representation for video-based person re-identification,? IEEE International Conference on Image Processing, 2018.     Broader Impacts: The project trained a female PhD student in computer science and engineering who was supported on this project. There was a strong collaboration with the other students in the laboratory as well. All the software was developed on GPU hardware. Papers were presented at international conferences which attracted an international student and a faculty to join our laboratory as a visitor for one to two years. The project provided continuous mentoring, teaching, improvement of written/oral communication skills and leadership opportunities, internship in industry and weekly seminars. Research was presented to underrepresented students to encourage them for a career in computer science and engineering. The success of concept co-occurrence detection will simplify management of personal image data with automatic tagging. With semantically organized personal content, the preferences of a user can be learned to provide personalization of various contents that he/she consumes online. The capture of unbiased spatio-temporal semantics will open new opportunities for video-based querying and searching video databases for many practical applications.          Last Modified: 12/14/2018       Submitted by: Bir Bhanu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
