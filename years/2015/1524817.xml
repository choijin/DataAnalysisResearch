<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Advancing Visual Recognition with Feature Visualizations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>460000.00</AwardTotalIntnAmount>
<AwardAmount>460000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this work is to develop a set of tools to visualize the information extracted by computer vision systems so that it is easier for researchers, and users, to understand their behavior. With the success of new computational architectures for visual processing, such as deep neural networks with many processing layers (e.g., convolutional neural networks) and access to large databases with millions of annotated images (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly, and becoming integrated into many commercial products. But these advances come with the price that systems are becoming more complex, and it becomes harder for researchers and users to diagnose and understand the representations built by these systems. The goal of this work is to develop new techniques for visualizing what the algorithms are doing in order to elucidate their behavior. &lt;br/&gt;&lt;br/&gt;The work will focus on developing algorithms for generic feature inversion. Most features perform complex non-linear operations over the image and it is not always possible to obtain analytic expressions to invert those computations. The goal of the proposal is to introduce new techniques that will allow inverting descriptors without constraining the descriptors. The second challenge will consists in understanding the inversion properties in order to allow comparison among different descriptions. If the inversion contains approximations, comparisons among descriptors might not be possible. Therefore it will be important to understand the convergence properties of the inversion algorithms. Another issue arises from the compressive nature of most descriptors. In general, some part of the input image information will be lost when encoded by an image descriptor. Therefore, the inversion will have to be a one-to-many function. Understanding the space of equivalent images under a particular descriptor will provide insights about what will be the likely errors made by a recognition system using them. This proposal will perform a variety of experiments with the feature visualizations, such as examining invariances in both engineered features and learned features from deep learning, visualizing learned models and decision boundaries, and diagnosing false alarms and missed detections.</AbstractNarration>
<MinAmdLetterDate>08/26/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1524817</AwardID>
<Investigator>
<FirstName>Antonio</FirstName>
<LastName>Torralba</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Antonio Torralba</PI_FULL_NAME>
<EmailAddress>torralba@csail.mit.edu</EmailAddress>
<PI_PHON>6172531000</PI_PHON>
<NSF_ID>000096240</NSF_ID>
<StartDate>08/26/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~230000</FUND_OBLG>
<FUND_OBLG>2016~230000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>It is an exciting time for computer vision. With the success of new computational architectures for visual processing, such as deep neural networks (e.g., convNets) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. Computer vision has enormous impact in areas such as healthcare, autonomous driving, security, among many others. The performance achieved by convNets is remarkable and constitute the state of the art on most tasks. But why does it work so well? what is the nature of the internal representation learned by a network? The goal of the project is to use feature visualizations to better understand the learned representation by a computer vision system. More specifically, when training a system to perform a task such as object detection or scene recognition, the system will learn an internal representation that consists on the extraction of image descriptors that can be used to perform the task at hand. In most cases, the nature of the learned representation remains hidden and the full system is treated as a black box. In this project our goal is to develop tools to uncover the nature of the learned representation. By understanding the nature of the representation, we can gain insights on how to improve the system.</p> <p>As a result of this project we have developed a new approach to uncover the representation learned by a deep neural network when trained to solve vision tasks. The new technique developed is called "network dissection". <em>Network Dissection</em> is our method for quantifying interpretability of individual units in a deep convNet.</p> <p>We have applied these tools to a diverse set of computer vision systems. In particular, we have shown that generative adversarial neural networks (GANs) learn an interpretable representation when trained to render images. We have found that there are hidden units in the network that had learnt to render specific object classes. Examples, publications, all the code and datasets are available at the project website:</p> <p>http://netdissect.csail.mit.edu/</p> <p>One of the resulting products from our research on understanding what GANs learn is GANPaint. GANPaint is a starting point to show how creative tools in the future could work. The tool takes a natural image of a specific category, e.g. churches or kitchen, and allows modifications with brushes that do not just draw simple strokes, but actually draw semantically meaningful units such as trees, brick-texture, or domes.&nbsp; The core part of GANPaint Studio is a neural network (GAN) that can produce its own images of a certain category, e.g. kitchen images. GANPaint builds on out work on visualizing the features learned by a GAN (project GANDissect). This allowed us to modify images that the network produced by "drawing" neurons.&nbsp; The novelty we added for GANPaint Studio is that a natural image (of this category) can now be ingested and modified with semantic brushes that produce or remove units such as trees, brick-texture, or domes. This tool has also been adapted for the programing language Scratch (developed at the MediaLab). The adaptation of GANPaint to Scratch has been done in order to allow kids to learn about neural networks and AI. This tool is part of a larger project that tries to develop educational tools for K12.</p> <p>This goal of this work is to improve the transparency and interpretability of computer vision systems by introducing tools that allow the user visualizing how the system works. The tools developed under this project, together with many others built by the research community at large, will contribute to improve the trustworthiness and robustness of computer vision systems.</p><br> <p>            Last Modified: 06/19/2020<br>      Modified by: Antonio&nbsp;Torralba</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1524817/1524817_10392208_1592614192393_fig1--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1524817/1524817_10392208_1592614192393_fig1--rgov-800width.jpg" title="GANs"><img src="/por/images/Reports/POR/2020/1524817/1524817_10392208_1592614192393_fig1--rgov-66x44.jpg" alt="GANs"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A GAN learns to synthesize realistic images by itself, without human annotations. Our results suggest that the GAN has learned, not just how to synthesize real looking images, but also has learned units that are specialized on rendering specific objects such as door, trees, windows, towers, ?</div> <div class="imageCredit">Bau et al., ICLR 2019</div> <div class="imageSubmitted">Antonio&nbsp;Torralba</div> <div class="imageTitle">GANs</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1524817/1524817_10392208_1592614501596_fig2--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1524817/1524817_10392208_1592614501596_fig2--rgov-800width.jpg" title="Painting by manipulating GAN neurons."><img src="/por/images/Reports/POR/2020/1524817/1524817_10392208_1592614501596_fig2--rgov-66x44.jpg" alt="Painting by manipulating GAN neurons."></a> <div class="imageCaptionContainer"> <div class="imageCaption">An interactive interface allows a user to choose several high-level semantic visual concepts and paint them on to an image. After the user adds a window in the specified location, the result is a modified image in which a window has been added in place of the original wall.</div> <div class="imageCredit">Bau et al., ICLR 2019</div> <div class="imageSubmitted">Antonio&nbsp;Torralba</div> <div class="imageTitle">Painting by manipulating GAN neurons.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ It is an exciting time for computer vision. With the success of new computational architectures for visual processing, such as deep neural networks (e.g., convNets) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. Computer vision has enormous impact in areas such as healthcare, autonomous driving, security, among many others. The performance achieved by convNets is remarkable and constitute the state of the art on most tasks. But why does it work so well? what is the nature of the internal representation learned by a network? The goal of the project is to use feature visualizations to better understand the learned representation by a computer vision system. More specifically, when training a system to perform a task such as object detection or scene recognition, the system will learn an internal representation that consists on the extraction of image descriptors that can be used to perform the task at hand. In most cases, the nature of the learned representation remains hidden and the full system is treated as a black box. In this project our goal is to develop tools to uncover the nature of the learned representation. By understanding the nature of the representation, we can gain insights on how to improve the system.  As a result of this project we have developed a new approach to uncover the representation learned by a deep neural network when trained to solve vision tasks. The new technique developed is called "network dissection". Network Dissection is our method for quantifying interpretability of individual units in a deep convNet.  We have applied these tools to a diverse set of computer vision systems. In particular, we have shown that generative adversarial neural networks (GANs) learn an interpretable representation when trained to render images. We have found that there are hidden units in the network that had learnt to render specific object classes. Examples, publications, all the code and datasets are available at the project website:  http://netdissect.csail.mit.edu/  One of the resulting products from our research on understanding what GANs learn is GANPaint. GANPaint is a starting point to show how creative tools in the future could work. The tool takes a natural image of a specific category, e.g. churches or kitchen, and allows modifications with brushes that do not just draw simple strokes, but actually draw semantically meaningful units such as trees, brick-texture, or domes.  The core part of GANPaint Studio is a neural network (GAN) that can produce its own images of a certain category, e.g. kitchen images. GANPaint builds on out work on visualizing the features learned by a GAN (project GANDissect). This allowed us to modify images that the network produced by "drawing" neurons.  The novelty we added for GANPaint Studio is that a natural image (of this category) can now be ingested and modified with semantic brushes that produce or remove units such as trees, brick-texture, or domes. This tool has also been adapted for the programing language Scratch (developed at the MediaLab). The adaptation of GANPaint to Scratch has been done in order to allow kids to learn about neural networks and AI. This tool is part of a larger project that tries to develop educational tools for K12.  This goal of this work is to improve the transparency and interpretability of computer vision systems by introducing tools that allow the user visualizing how the system works. The tools developed under this project, together with many others built by the research community at large, will contribute to improve the trustworthiness and robustness of computer vision systems.       Last Modified: 06/19/2020       Submitted by: Antonio Torralba]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
