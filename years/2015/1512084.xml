<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Valid Inference when Analytical Models are Approximations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>531966.00</AwardTotalIntnAmount>
<AwardAmount>531966</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Statistical inferential methods are used to answer questions throughout modern life.  For example, what affects the crime rate in a town; which factors are important influences on the housing market; which genes are associated to a certain disease; what are the most important elements to control in order to mitigate climate change?  Statistical methods are used to address questions such as these. However, often the statistical data structure and the mathematical model developed for the analysis do not agree. This project arises from a broadly based statistical concern about the mismatch between standard inferential analyses and the statistics of the world they are trying to describe. This research draws a distinction between the statistical models that conventionally describe the correlational relations in experimental and observational data and the inferential models that are used in their analysis. To this end, the project investigates a paradigm in which sampling models are meant to be faithful representations of the real-world structure of the data they are describing.  At the same time, the analytical models to be applied to the data are viewed only as approximate descriptions of that reality.  The statistical-sampling representations need not match the analytical models, though the two should harmonize in certain important respects. There is a significant disparity between accurate characterization and what is claimed by classical procedures that ignore this distinction. The distinction has been noted by many previous statistical researchers, and various partially adequate approaches have been suggested. Nevertheless, clarifying this distinction in the directions under study and then pursuing the consequences leads to a theory of inference somewhat different from that in common use for relational and observational data. Acknowledging and properly accommodating this duality then leads to new methodology for some important statistical problems. One such new methodology is within the setting of randomized clinical trials in which one wishes to estimate the effect of certain treatment(s) relative to others or to placebo controls. Another is within the setting of semi-supervised learning that occurs in various big-data contexts. &lt;br/&gt;&lt;br/&gt;The core of the current research is designed for linear analytical models. These involve observations on a vector of explanatory covariates (X-variables) and a numerical dependent variable (Y). The analytical model constructs the best linear approximant of Y as a linear function of the X variables. Virtually no assumptions are made about the (X,Y) pairs in the sample, other than that they form a statistical sample drawn from some unknown joint distribution of (X,Y) pairs and possess desired low-order moments. The notion of "best" is defined in a statistically natural fashion related to minimizing squared prediction error. It follows that the ordinary least squares estimators of parameters still have desirable asymptotic properties. Inference about their (asymptotic) performance can be derived via the standard sandwich estimator. However, a newly derived iterated pairs-bootstrap is shown to give substantially more accurate inferential information for realistic sample sizes. If more information is available about the distribution of X (such as knowledge of its mean and variance) then the usual least-squares solutions can be improved. This observation leads via an indirect path to suggestions that improve the standard methodology for estimating average treatment effect in randomized clinical trials and for producing linear predictions of numerical outcomes in settings of semi-supervised learning. Various additional issues are exposed in the course of the above developments. We also plan to investigate generalizations of the above setting -- for example to models having categorical Y-variables (classification) and to other generalized-linear analytical models. Our earlier research involved post-selection inference in the classical setting in which models for the data and its analysis coincide, and we now intend to pursue analogous issues in the current context in which they do not.</AbstractNarration>
<MinAmdLetterDate>07/10/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/20/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1512084</AwardID>
<Investigator>
<FirstName>Lawrence</FirstName>
<LastName>Brown</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lawrence D Brown</PI_FULL_NAME>
<EmailAddress>lbrown@wharton.upenn.edu</EmailAddress>
<PI_PHON>2158984753</PI_PHON>
<NSF_ID>000465732</NSF_ID>
<StartDate>07/10/2015</StartDate>
<EndDate>08/20/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Linda</FirstName>
<LastName>Zhao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Linda Zhao</PI_FULL_NAME>
<EmailAddress>lzhao@wharton.upenn.edu</EmailAddress>
<PI_PHON>2158988228</PI_PHON>
<NSF_ID>000476092</NSF_ID>
<StartDate>08/20/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Linda</FirstName>
<LastName>Zhao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Linda Zhao</PI_FULL_NAME>
<EmailAddress>lzhao@wharton.upenn.edu</EmailAddress>
<PI_PHON>2158988228</PI_PHON>
<NSF_ID>000476092</NSF_ID>
<StartDate>07/10/2015</StartDate>
<EndDate>08/20/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Trustees of the University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191043615</ZipCode>
<StreetAddress><![CDATA[Wharton Statistics Dept.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~531966</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Statistical data analysis often rely on simplifying assumptions on the data generating process. This reliance is taken further in to statistical inference, including confidence statements, hypothesis testing, without regard to possible deviance from the simplifying assumptions. Most often than not, these simplifying assumptions do not match the reality and can, sometimes, lead to disastrous consequences (for practice). Our broad goal in this respect is to acknowledge the misspecified assumptions and bridge the gap between reality and practice by providing a comprehensive theory as well as methodology with valid inference. Some existing literature on the topic provides a starting point which we extend much beyond by focusing on interpretation of "parameters" under misspecification and developing diagnostics to understand effects of different types misspecification for inference.</p> <p>A fundamental goal of this proposal is to work with classical linear regression models (including linear regression, generalized linear models etc) which is the common workhorse for day-to-day data analysts but without the classical assumption-laden framework. One important difference in assumptions that we emphasize is that covariates in classical framework are assumed fixed/non-stochastic but we note that most common data sources are observational studies and hence covariates are as much as random as the response itself. Acknowledging this crucial fact brings in additional variation in inference. Additionally, the reported models in practice are not the first choice and are a product of data snooping. A common practice is to simply ignore that such a data snooping ever took place and use classical inferential tools. This has no scientific basis and such data snooping coupled with mis-match between assumptions lead to serious reproducibility issues. As part of our proposal, we studied and implemented valid inference procedures in light of variable selection and misspecification in a large class of linear regression models.</p> <p>The interpretations proposed in this line of work lead to some new perspectives on more specific types of statistical inference. Several papers came out of this proposal some of which are published and others are at various points of the publication process. We have also capitalized on this re-interpretation and modified understanding of traditional methodology in order to begin investigation of improved methodologies for some other statistical settings, including applications inside the umbrella of ?big data? and ?machine learning?. This promising research has continued beyond the conclusion of the current proposal.</p> <p>The basic perspectives and paradigms we propose are somewhat different from those customarily taught in basic statistics courses. Because of this we have been developing new educational materials designed to convey these at appropriate stages within the statistical curriculum.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/03/2019<br>      Modified by: Linda&nbsp;Zhao</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Statistical data analysis often rely on simplifying assumptions on the data generating process. This reliance is taken further in to statistical inference, including confidence statements, hypothesis testing, without regard to possible deviance from the simplifying assumptions. Most often than not, these simplifying assumptions do not match the reality and can, sometimes, lead to disastrous consequences (for practice). Our broad goal in this respect is to acknowledge the misspecified assumptions and bridge the gap between reality and practice by providing a comprehensive theory as well as methodology with valid inference. Some existing literature on the topic provides a starting point which we extend much beyond by focusing on interpretation of "parameters" under misspecification and developing diagnostics to understand effects of different types misspecification for inference.  A fundamental goal of this proposal is to work with classical linear regression models (including linear regression, generalized linear models etc) which is the common workhorse for day-to-day data analysts but without the classical assumption-laden framework. One important difference in assumptions that we emphasize is that covariates in classical framework are assumed fixed/non-stochastic but we note that most common data sources are observational studies and hence covariates are as much as random as the response itself. Acknowledging this crucial fact brings in additional variation in inference. Additionally, the reported models in practice are not the first choice and are a product of data snooping. A common practice is to simply ignore that such a data snooping ever took place and use classical inferential tools. This has no scientific basis and such data snooping coupled with mis-match between assumptions lead to serious reproducibility issues. As part of our proposal, we studied and implemented valid inference procedures in light of variable selection and misspecification in a large class of linear regression models.  The interpretations proposed in this line of work lead to some new perspectives on more specific types of statistical inference. Several papers came out of this proposal some of which are published and others are at various points of the publication process. We have also capitalized on this re-interpretation and modified understanding of traditional methodology in order to begin investigation of improved methodologies for some other statistical settings, including applications inside the umbrella of ?big data? and ?machine learning?. This promising research has continued beyond the conclusion of the current proposal.  The basic perspectives and paradigms we propose are somewhat different from those customarily taught in basic statistics courses. Because of this we have been developing new educational materials designed to convey these at appropriate stages within the statistical curriculum.          Last Modified: 07/03/2019       Submitted by: Linda Zhao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
