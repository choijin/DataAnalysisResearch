<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Acquisition of Conflux, A Novel Platform for Data-Driven Computational Physics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>2422972.00</AwardTotalIntnAmount>
<AwardAmount>2422972</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alejandro Suarez</SignBlockName>
<PO_EMAI>alsuarez@nsf.gov</PO_EMAI>
<PO_PHON>7032927092</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops an instrument, called ConFlux, hosted at the University of Michigan (UM), specifically designed to enable High Performance Computing (HPC) clusters to communicate seamlessly and at interactive speeds with data-intensive operations.  The project establishes a hardware and software ecosystem to enable large scale data-driven modeling of multiscale physical systems.  ConFlux will produce advances in predictive modeling in several disciplines including turbulent flows, materials physics, cosmology, climate science and cardiovascular flow modeling. &lt;br/&gt;&lt;br/&gt;A wide range of phenomena exhibit emergent behavior that makes modeling very challenging. In this project, physics-constrained data-driven modeling approaches are pursued to account for the underlying complexity. These techniques require HPC applications (running on external clusters) to interact with large data sets at run time. ConFlux provides low latency communications for in- and out-of-core data, cross-platform storage, as well as high throughput interconnects and massive memory allocations. The file-system and scheduler natively handle extreme-scale machine learning and traditional HPC modules in a tightly integrated workflow---rather than in segregated operations--leading to significantly lower latencies, fewer algorithmic barriers and less data movement.  &lt;br/&gt;&lt;br/&gt;Course material developed from the usage of ConFlux is being integrated into the educational curriculum via several degree and certificate programs offered by two UM institutes dedicated to computational and data sciences. Use of the ConFlux cluster will be extended to research groups outside of UM utilizing a number of Extreme Science and Engineering Discovery Environment (XSEDE) bridging tools and file-systems. Connections established through UM's Office of Outreach and Diversity are being leveraged to extend the use of ConFlux to minority serving institutions and Historically Black Colleges and Universities. Using the programs developed by the Society of Women Engineers at UM, middle and high school students will be engaged in hands-on educational modules in computing, physics and data.</AbstractNarration>
<MinAmdLetterDate>08/27/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1531752</AwardID>
<Investigator>
<FirstName>August</FirstName>
<LastName>Evrard</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>August E Evrard</PI_FULL_NAME>
<EmailAddress>evrard@umich.edu</EmailAddress>
<PI_PHON>7347644366</PI_PHON>
<NSF_ID>000221790</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Krishnakumar</FirstName>
<LastName>Garikipati</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Krishnakumar Garikipati</PI_FULL_NAME>
<EmailAddress>krishna@engin.umich.edu</EmailAddress>
<PI_PHON>7349360329</PI_PHON>
<NSF_ID>000149929</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Barzan</FirstName>
<LastName>Mozafari</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Barzan Mozafari</PI_FULL_NAME>
<EmailAddress>mozafari@umich.edu</EmailAddress>
<PI_PHON>7347647247</PI_PHON>
<NSF_ID>000648726</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Karthikeyan</FirstName>
<LastName>Duraisamy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karthikeyan Duraisamy</PI_FULL_NAME>
<EmailAddress>kdur@umich.edu</EmailAddress>
<PI_PHON>7347636438</PI_PHON>
<NSF_ID>000682308</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Carlos</FirstName>
<LastName>Figueroa</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carlos A Figueroa</PI_FULL_NAME>
<EmailAddress>figueroc@med.umich.edu</EmailAddress>
<PI_PHON>7347638680</PI_PHON>
<NSF_ID>000691419</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName/>
<StateCode>MI</StateCode>
<ZipCode>481091274</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~2422972</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project established a hardware and software ecosystem to support data-enabled modeling of complex physical problems, by enabling High Performance Computing (HPC) clusters to communicate seamlessly and at interactive speeds with data-intensive operations. Conflux is a unique resource that was designed with the view that computational physics and large-scale predictive modeling are natural allies whose tight integration would benefit the scientific endeavor greatly.</p> <p>The ConFlux cluster enables<br />1. A Large-memory tier for operations such as data decomposition, machine learning training.<br />2. A rapid testing tier with integrated CPU/GPU computations (NVLINK/CAPI architecture) to promote frequent interactions between the data and the computation. GPUs perform specific tasks such as fast machine learning evaluations or low-rank updates and are&nbsp;augmented with interfaces to allow low latency data transfers to CPU cores.<br />3. Ultra-low latency communications (enabled by state-of-the-art interconnects),&nbsp;<br />4. Heterogeneous scheduling (to offer a native hardware/software infrastructure for compute- and data-intensive tasks), and&nbsp;<br />5. Petabyte-scale unified storage (to handle data generation/analytics/computations and to minimize data movements between native and Hadoop-type file systems).</p> <p><br />On the software side, a new memory disaggregation protocol has been developed to pool together unused memory across the cluster to improve application-level performance and overall cluster memory utilization.&nbsp; A fast job switching and memory sharing routine has also been developed to achieve fine-grained GPU sharing among multiple distributed deep learning applications.</p> <p><br />Conflux has been used in several applications of data-driven computational physics. Examples include materials modeling and discovery, turbulence modeling, hydrogen storage, cardiovascular modeling, repair and surgery planning. This research has resulted in several high impact journal publications, publicly available data respositories and software repositories.</p> <p><br />Conflux is now a well-established computing resource at the University of Michigan. It has been continously utilized at high operational rates over the past 3 years by around 100 users from different parts of the campus as well as from other universities. A number of students have been exposed to the interface between HPC and machine learning. Further, workshops and courses have been conducted using Conflux.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/01/2019<br>      Modified by: Karthikeyan&nbsp;Duraisamy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project established a hardware and software ecosystem to support data-enabled modeling of complex physical problems, by enabling High Performance Computing (HPC) clusters to communicate seamlessly and at interactive speeds with data-intensive operations. Conflux is a unique resource that was designed with the view that computational physics and large-scale predictive modeling are natural allies whose tight integration would benefit the scientific endeavor greatly.  The ConFlux cluster enables 1. A Large-memory tier for operations such as data decomposition, machine learning training. 2. A rapid testing tier with integrated CPU/GPU computations (NVLINK/CAPI architecture) to promote frequent interactions between the data and the computation. GPUs perform specific tasks such as fast machine learning evaluations or low-rank updates and are augmented with interfaces to allow low latency data transfers to CPU cores. 3. Ultra-low latency communications (enabled by state-of-the-art interconnects),  4. Heterogeneous scheduling (to offer a native hardware/software infrastructure for compute- and data-intensive tasks), and  5. Petabyte-scale unified storage (to handle data generation/analytics/computations and to minimize data movements between native and Hadoop-type file systems).   On the software side, a new memory disaggregation protocol has been developed to pool together unused memory across the cluster to improve application-level performance and overall cluster memory utilization.  A fast job switching and memory sharing routine has also been developed to achieve fine-grained GPU sharing among multiple distributed deep learning applications.   Conflux has been used in several applications of data-driven computational physics. Examples include materials modeling and discovery, turbulence modeling, hydrogen storage, cardiovascular modeling, repair and surgery planning. This research has resulted in several high impact journal publications, publicly available data respositories and software repositories.   Conflux is now a well-established computing resource at the University of Michigan. It has been continously utilized at high operational rates over the past 3 years by around 100 users from different parts of the campus as well as from other universities. A number of students have been exposed to the interface between HPC and machine learning. Further, workshops and courses have been conducted using Conflux.          Last Modified: 12/01/2019       Submitted by: Karthikeyan Duraisamy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
