<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: ASPIRE: Automation Supporting Prolonged Independent Residence for the Elderly</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1295917.00</AwardTotalIntnAmount>
<AwardAmount>1295917</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Irina Dolinskaya</SignBlockName>
<PO_EMAI>idolinsk@nsf.gov</PO_EMAI>
<PO_PHON>7032927078</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Because of the graying of the population, there is a growing need for new assistive technologies to aid the elderly in their daily living.  Based on figures provided by the U.S. Census Bureau, and due largely to the aging of the "baby boomer" generation, the population of U.S. adults who are 65 and older is projected to be twice as large in 2030 as it was in 2000, increasing from 35 million to 71.5 million and representing nearly 20 percent of the total U.S. population. This trend is placing enormous burdens on health care costs and causing disruptive changes to how individuals and families manage key late-in-life decisions, including residence. A recent (2015) report by the U.S. Department of Housing and Urban Development concluded that most seniors would prefer to age in place, and a 2010 survey found that 88% of respondents over 65 preferred to remain in their homes as long as possible. It is important to note that these residential preferences must typically be viewed in light of alternatives that are likely to be much more expensive and/or more socially taxing, such as older adults living instead in hospitals, assistive living facilities or with family members. While estimates of the costs associated with these trends vary greatly, it is almost certain that extending the portions of older adults' life spans in which they can live safely and independently through technological means could have enormous positive societal impact. In order to assist in successful aging in place, assistive robots have been developed in the past few decades. However, very few of them have become commercially available, and their use in domesticated environments remains highly limited. The major cause of the problem is that assistive robots typically take the form of full-size humanoid devices or something equally as cumbersome, expensive, and limited in movement and function. We propose a novel assistive robotic system that provides: (i) flexibility, allowing  the designed system  to be personalized based on users' needs without demanding any home modification upon installation; (ii) safety, ensuring that the system development process accounts for perceived safety by the user, and that the underlying  theoretical framework  guarantees  collision avoidance; (iii) usability, consisting of a minimal and intuitive user interface  to provide acceptable controls; (iv) reduced costs, with respect to currently available solutions on the market. The idea behind this research project is the development of a general framework that enables a team of unmanned ground vehicles and small multirotor unmanned aerial vehicles to safely cooperate with the elderly in a home environment. Equipped with appropriate human-machine interfaces, the co-robots will be able to accomplish a number of tasks as demanded by the users. &lt;br/&gt;&lt;br/&gt;The project addresses fundamental problems in the domain of multi-agent cooperative systems, comprised of humans and co-robots interacting in shared, highly constrained spaces. In order to assist humans, the co-robots have to be trusted by humans, implying that their behaviors be predictable and consistent with principles of human spatial perception, and their appearance must foster a high level of comfort and not create high cognitive demands on the user. Inspired by these challenges, this proposal focuses on the design and control of co-robots, which can adapt to unstructured and rapidly changing environments in a manner consistent with human perception and cognition, thus enhancing safety and robustness. The key focus areas include the design and acceptance of mobile ground and aerial robots that coexist in environments inhabited by humans and the development of a multi-objective control framework to allow intuitive user control over an ensemble of co-robots, which includes the design of both low-level controllers (LLC) and a supervisory, high-level controller (HLC). To demonstrate the benefits of the framework and to engage student groups from various, diverse populations, the following scenario will be considered as a test case: multirotor unmanned aerial vehicles and ground robots acting as domestic assistive devices for healthy older adults in a research laboratory. These co-robots will safely navigate the shared space and accomplish domestic tasks requested by humans while displaying behaviors and appearances that are perceived as safe and trusted. Humans will use an intuitively designed interface for both controlling and monitoring co-robots on a tablet or a smartphone device. A motion capture system and virtual reality Cube at the Beckman Institute will provide the context for data collection, iterative testing and validation.</AbstractNarration>
<MinAmdLetterDate>08/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1528036</AwardID>
<Investigator>
<FirstName>Alex</FirstName>
<LastName>Kirlik</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alex C Kirlik</PI_FULL_NAME>
<EmailAddress>kirlik@illinois.edu</EmailAddress>
<PI_PHON>2172448972</PI_PHON>
<NSF_ID>000281974</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ranxiao</FirstName>
<LastName>Wang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ranxiao Wang</PI_FULL_NAME>
<EmailAddress>wang18@illinois.edu</EmailAddress>
<PI_PHON>2172443664</PI_PHON>
<NSF_ID>000166426</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Naira</FirstName>
<LastName>Hovakimyan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Naira Hovakimyan</PI_FULL_NAME>
<EmailAddress>nhovakim@illinois.edu</EmailAddress>
<PI_PHON>2172441672</PI_PHON>
<NSF_ID>000494200</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dusan</FirstName>
<LastName>Stipanovic</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dusan Stipanovic</PI_FULL_NAME>
<EmailAddress>dusan@illinois.edu</EmailAddress>
<PI_PHON>2172440907</PI_PHON>
<NSF_ID>000488099</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amy</FirstName>
<LastName>LaViers</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amy E LaViers</PI_FULL_NAME>
<EmailAddress>amy@theradlab.xyz</EmailAddress>
<PI_PHON>8655489232</PI_PHON>
<NSF_ID>000652815</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Urbana</CityName>
<StateCode>IL</StateCode>
<ZipCode>618013620</ZipCode>
<StreetAddress><![CDATA[506 South Wright Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~1295917</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-fe0c7f44-7fff-371a-731a-1ce5c3a03472"> </span></p> <p dir="ltr"><span>The objective of this research is to design and develop a robotic system consisting of autonomous ground and flying robots to aid independent residence of the elderly. The achievements are summarized as follows.&nbsp;</span></p> <p dir="ltr"><span><br /></span></p> <p dir="ltr"><span>We designed and conducted physio-psychological experiments based on a virtual reality (AR) testbed built&nbsp; to study human's perceived safety in close proximity to flying robots. This work revealed the big gap between the perceived safety and the actual safety (which mainly focuses on avoiding collisions with humans), and the critical need to incorporate human's perceived safety in design and control of assistive robotic systems for wide acceptability. We quantitatively studied the effect of robotic motion parameters such as position, velocity and acceleration profiles on human's anxiety level using the experimental data and developed a mathematical model of perceived safety using machine learning techniques. With the help of this model we developed a socially-aware motion planning framework that will cause minimal or no discomfort to nearby humans. These efforts are crucial attempts to address some fundamental problems in popularizing the use of mobile robots for assistive purposes and becoming trustworthy.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr">We have developed mathematical methods for generating distinct motion patterns for&nbsp; mobile and aerial robots to facilitate their communication with humans. We have studied how context, including environment and embedded character references, changes the interpretation of motion. We've run user studies inside different environments (real and virtual), with different character embeddings, on a developed expressive layer for a mobile robot. This work was conducted in synergy with an artist-in-residence and led to several student-developed and student-contributed public performances where human subjects data was collected. &nbsp;Including expertise in the arts in the development of expressive robotic systems presents pioneering new ways of working across disciplines, which can substantially impact the ability for human counterparts to interact with robots without extensive prior training and without discomfort or fear. We have also begun a commercialization effort, through the NSF I-Corps program (local and national node), which has resulted in a startup company.</p> <p dir="ltr">&nbsp;</p> <p dir="ltr"><span>To endow the developers the freedom to design proprietary applications that control a set of ground and flying robots in a household, we established the task delegation Interface (TDI). The TDI comprises two main units, namely the Input Interpreter (I2) that serves as the main application gateway for human interface devices, e.g. tablet, smartphone, or others, and the high-level manager (HLM) that deconstructs the request sent by I2 and constructs the argument list used by the low-level controller. Android applications have been developed and used to test the integrity of commands and connection reliability.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>For the robots to safely and optimally execute the commands received from a user, we also developed algorithms for low-level control with a focus on path generation and collision avoidance. We have developed motion planning algorithms, using Bezier curves, that can handle a complex set of constraints while ensuring (near) real-time generation of trajectories. For collision avoidance, a non-parametric Bayesian approach was developed to estimate the trajectories of moving obstacles under the realistic assumption of partial knowledge of these obstacles. We have formulated algorithms which are capable of computing the proximity of agents' paths with respect to the estimated obstacles' trajectories in near real-time. Inspired by ways animals navigate the environment, we developed reactive collision avoidance methods, which rely solely on inexpensive and light onboard monocular cameras. Focusing on cost-effectiveness through deployment of cheap and light-weight cameras is important for wide acceptability and affordability.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Beyond the advancement of science and technology, a large number of events have also been organized to induce positive impact on society and development of human resources. At the secondary school level, we built a partnership with the Champaign Public School District through their AVID and 4-H programs, which were established to target underrepresented students. We worked with Angela Chang at Westinghouse College, who implemented the engineering curriculum&nbsp; and presented at the Connections Conference in Chicago on March 8, 2016. We showcased robotics to a group of elementary school students at the Montessori School of Champaign-Urbana on July 12, 2019. We presented&nbsp; in public venues at multiple universities and performance spaces, including the Ferst Center for the Arts at Georgia Tech, the Ammerman Center at Connecticut College, and the Granoff Center for the Creative and Performing Arts at Brown University. We held outreach activities for K-12 students at multiple venues that target underrepresented groups, including Martin Luther King Elementary School in Urbana, a Girl Scout troop in Champaign, Ctrl-Z robotics camp, and the MechSE Games Camp. Naira Hovakimyan was an invited speaker in Aging 2.0 conference in San Francisco in 2016. </span></p> <p dir="ltr">&nbsp;</p> <p dir="ltr"><span>This NSF project was featured in The New York Times on December 8th, 2015; it was captured by Xploration Earth 2050 on Fox TV on October 22, 2016; it was publicized on CNBC, in Washington Post and many other venues.</span></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/24/2019<br>      Modified by: Naira&nbsp;Hovakimyan</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568903448881_Nairanytimes--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568903448881_Nairanytimes--rgov-800width.jpg" title="A small drone for elderly care"><img src="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568903448881_Nairanytimes--rgov-66x44.jpg" alt="A small drone for elderly care"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Naira Hovakimyan with the small drone</div> <div class="imageCredit">Daniel Acker for The New York Times</div> <div class="imageSubmitted">Naira&nbsp;Hovakimyan</div> <div class="imageTitle">A small drone for elderly care</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568903235913_66307662_10157860116574610_4013458414817509376_o--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568903235913_66307662_10157860116574610_4013458414817509376_o--rgov-800width.jpg" title="At Montessori Elementary School of Champaign"><img src="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568903235913_66307662_10157860116574610_4013458414817509376_o--rgov-66x44.jpg" alt="At Montessori Elementary School of Champaign"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Graduate students Thiago Marinho and Arun Lakshmanan are explaining the role of robots for society</div> <div class="imageCredit">Kerry Rossow</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Naira&nbsp;Hovakimyan</div> <div class="imageTitle">At Montessori Elementary School of Champaign</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568913116154_vr-simulation--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568913116154_vr-simulation--rgov-800width.jpg" title="Virtual Reality simulation"><img src="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568913116154_vr-simulation--rgov-66x44.jpg" alt="Virtual Reality simulation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Virtual Reality simulator used as testbed for data collection towards building the human anxiety model for safe path planning</div> <div class="imageCredit">Arun Lakshmanan</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Naira&nbsp;Hovakimyan</div> <div class="imageTitle">Virtual Reality simulation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568913264070_crazyflie-medicine-pill--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568913264070_crazyflie-medicine-pill--rgov-800width.jpg" title="Pill carrying drone"><img src="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568913264070_crazyflie-medicine-pill--rgov-66x44.jpg" alt="Pill carrying drone"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Crazyflie for delivering medicine to elderly</div> <div class="imageCredit">Arun Lakshmanan</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Naira&nbsp;Hovakimyan</div> <div class="imageTitle">Pill carrying drone</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568926112456_pickup-small--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568926112456_pickup-small--rgov-800width.jpg" title="Aerial manipulator"><img src="/por/images/Reports/POR/2019/1528036/1528036_10388920_1568926112456_pickup-small--rgov-66x44.jpg" alt="Aerial manipulator"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Aerial manipulator picking a small object at home.</div> <div class="imageCredit">Arun Lakshmanan</div> <div class="imageSubmitted">Naira&nbsp;Hovakimyan</div> <div class="imageTitle">Aerial manipulator</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1528036/1528036_10388920_1569274223813_dd--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1528036/1528036_10388920_1569274223813_dd--rgov-800width.jpg" title="Performance/user-study with expressive robot"><img src="/por/images/Reports/POR/2019/1528036/1528036_10388920_1569274223813_dd--rgov-66x44.jpg" alt="Performance/user-study with expressive robot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Performance/user-study with expressive robot</div> <div class="imageCredit">Keira Heu-Jwyn Chang</div> <div class="imageSubmitted">Amy&nbsp;E&nbsp;Laviers</div> <div class="imageTitle">Performance/user-study with expressive robot</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The objective of this research is to design and develop a robotic system consisting of autonomous ground and flying robots to aid independent residence of the elderly. The achievements are summarized as follows.    We designed and conducted physio-psychological experiments based on a virtual reality (AR) testbed built  to study human's perceived safety in close proximity to flying robots. This work revealed the big gap between the perceived safety and the actual safety (which mainly focuses on avoiding collisions with humans), and the critical need to incorporate human's perceived safety in design and control of assistive robotic systems for wide acceptability. We quantitatively studied the effect of robotic motion parameters such as position, velocity and acceleration profiles on human's anxiety level using the experimental data and developed a mathematical model of perceived safety using machine learning techniques. With the help of this model we developed a socially-aware motion planning framework that will cause minimal or no discomfort to nearby humans. These efforts are crucial attempts to address some fundamental problems in popularizing the use of mobile robots for assistive purposes and becoming trustworthy.     We have developed mathematical methods for generating distinct motion patterns for  mobile and aerial robots to facilitate their communication with humans. We have studied how context, including environment and embedded character references, changes the interpretation of motion. We've run user studies inside different environments (real and virtual), with different character embeddings, on a developed expressive layer for a mobile robot. This work was conducted in synergy with an artist-in-residence and led to several student-developed and student-contributed public performances where human subjects data was collected.  Including expertise in the arts in the development of expressive robotic systems presents pioneering new ways of working across disciplines, which can substantially impact the ability for human counterparts to interact with robots without extensive prior training and without discomfort or fear. We have also begun a commercialization effort, through the NSF I-Corps program (local and national node), which has resulted in a startup company.   To endow the developers the freedom to design proprietary applications that control a set of ground and flying robots in a household, we established the task delegation Interface (TDI). The TDI comprises two main units, namely the Input Interpreter (I2) that serves as the main application gateway for human interface devices, e.g. tablet, smartphone, or others, and the high-level manager (HLM) that deconstructs the request sent by I2 and constructs the argument list used by the low-level controller. Android applications have been developed and used to test the integrity of commands and connection reliability.     For the robots to safely and optimally execute the commands received from a user, we also developed algorithms for low-level control with a focus on path generation and collision avoidance. We have developed motion planning algorithms, using Bezier curves, that can handle a complex set of constraints while ensuring (near) real-time generation of trajectories. For collision avoidance, a non-parametric Bayesian approach was developed to estimate the trajectories of moving obstacles under the realistic assumption of partial knowledge of these obstacles. We have formulated algorithms which are capable of computing the proximity of agents' paths with respect to the estimated obstacles' trajectories in near real-time. Inspired by ways animals navigate the environment, we developed reactive collision avoidance methods, which rely solely on inexpensive and light onboard monocular cameras. Focusing on cost-effectiveness through deployment of cheap and light-weight cameras is important for wide acceptability and affordability.    Beyond the advancement of science and technology, a large number of events have also been organized to induce positive impact on society and development of human resources. At the secondary school level, we built a partnership with the Champaign Public School District through their AVID and 4-H programs, which were established to target underrepresented students. We worked with Angela Chang at Westinghouse College, who implemented the engineering curriculum  and presented at the Connections Conference in Chicago on March 8, 2016. We showcased robotics to a group of elementary school students at the Montessori School of Champaign-Urbana on July 12, 2019. We presented  in public venues at multiple universities and performance spaces, including the Ferst Center for the Arts at Georgia Tech, the Ammerman Center at Connecticut College, and the Granoff Center for the Creative and Performing Arts at Brown University. We held outreach activities for K-12 students at multiple venues that target underrepresented groups, including Martin Luther King Elementary School in Urbana, a Girl Scout troop in Champaign, Ctrl-Z robotics camp, and the MechSE Games Camp. Naira Hovakimyan was an invited speaker in Aging 2.0 conference in San Francisco in 2016.    This NSF project was featured in The New York Times on December 8th, 2015; it was captured by Xploration Earth 2050 on Fox TV on October 22, 2016; it was publicized on CNBC, in Washington Post and many other venues.                Last Modified: 09/24/2019       Submitted by: Naira Hovakimyan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
