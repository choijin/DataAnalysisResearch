<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Characterizing Physical Interaction in Instrument Manipulations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2016</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>299887.00</AwardTotalIntnAmount>
<AwardAmount>299887</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Reid Simmons</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>As personal robots become common in our homes, they will perform a broad range of helpful tasks for their owners.  In doing so, all sorts of physical interactions will occur between the robot and the home environment, for example, picking up a mug requires the robot to carefully control contact forces as it secures a grasp. As tasks become more advanced, for example, operating a can opener, the robot will have to "understand" how to grasp the can and the can opener, so that it can operate the opener.  The goal of this project is to gather data and develop algorithms that will allow a robot to understand how to grasp tools in a way that facilitates its ability to operate that tool safely and effectively. &lt;br/&gt;&lt;br/&gt;This research aims to improve the understanding of the physical interactions between the instruments and the environment in daily manipulation tasks with the goal of developing effective robotic grasp and manipulation planners to facilitate the tasks. The research designs and develops a physical-interaction observation system to observe both the interactive motion and wrench between the instruments and environment in several representative instrumental manipulation tasks by a number of participants. Each manipulation task is characterized with its instrumental motion models and wrench distribution models. Using the models, optimal grasps are generated using the task wrench coverage measure and evaluated using a real robotic arm and hand platform. The data collected in this work enables researchers to fully explore daily living tasks and provide excellent training and testing data sets for other related research.  The optimal manipulation grasping approach equips robot manipulators with the ability to hold instruments with a firm grasp to withstand the disturbance in daily living interactions and perform tasks efficiently.</AbstractNarration>
<MinAmdLetterDate>02/25/2016</MinAmdLetterDate>
<MaxAmdLetterDate>02/25/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1560761</AwardID>
<Investigator>
<FirstName>Yu</FirstName>
<LastName>Sun</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yu Sun</PI_FULL_NAME>
<EmailAddress>yusun@mail.usf.edu</EmailAddress>
<PI_PHON>8139745465</PI_PHON>
<NSF_ID>000552980</NSF_ID>
<StartDate>02/25/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of South Florida</Name>
<CityName>Tampa</CityName>
<ZipCode>336172008</ZipCode>
<PhoneNumber>8139742897</PhoneNumber>
<StreetAddress>4019 E. Fowler Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 100]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL14</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>069687242</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTH FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>069687242</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of South Florida]]></Name>
<CityName>Tampa</CityName>
<StateCode>FL</StateCode>
<ZipCode>336129446</ZipCode>
<StreetAddress><![CDATA[4202 E. Fowler Ave., ENB 118]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL14</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~299887</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project has developed a physical-interaction observation system that observes how humans use tools. The observation system measures the motion of the tools and the interactive force between the tool and the manipulated objects by the tool. For example, the system observes the knife motion during slicing cucumbers and the force between the knife and the cucumber. The recorded physical interactions from the human&rsquo;s experience are then used to train grasping models and manipulation models. With the models, the robot will know how to grasp a tool and use it to perform manipulation tasks.</p> <p>Using the physical-interaction observation, the project has collected 32 different types of daily-living motions such as stirring with a spatula, peeling a cucumber, brushing teeth, etc. Each motion has been performed multiple times by multiple participants. The project has collected about 3,000 trials in total.</p> <p>The developed physical-interaction observation system is the first of its kind. Previous observation systems focused mainly on the human arm and hand motion. This system focuses on the tool&rsquo;s motion and the force applied between the tool and the manipulated objects by the tool in great detail. The recording includes RGBD, 6-axis force and 6-axis motion. Since we release the setup, it has been replicated by several other researchers in the robotics field for their studies.</p> <p>The dataset has been released on the project&rsquo;s webpage along with its visualization source code (<a href="http://rpal.cse.usf.edu/datasets_manipulation.html" target="_blank">http://rpal.cse.usf.edu/datasets_manipulation.html</a><span>)</span>. The data set along with the data collection setup has been released as a data paper. Using the collected data, we have developed a recurrent neural network approach that can perform human-like pouring tasks without accurate modeling of the environment or the task. It moves one step closer to deploying robots in a daily living environment. The dataset would also provide a verification dataset to evaluation dynamic simulations of physical interactions. Based on the data, the project has produced a manipulation motion taxonomy that can be used for studying the relationships between different types of manipulation motions. Inspired by the manipulations, the PI of the project has organized two Robotic Grasping and Manipulation Competitions during IROS 2016 and 207. The PI will organize the third Robotic Grasping and Manipulation Competitions in IROS 2019.</p> <p>The project has produced three journal papers, two conference papers, and two book chapters. The project has trained two Ph.D. students and two REU students in performing robotics research. The data collection setup and the dataset have been incorporated into course projects in the PI&rsquo;s an undergraduate-level class Intro to Robotics and graduate-level class Neural Network and Deep Learning class.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/30/2019<br>      Modified by: Yu&nbsp;Sun</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project has developed a physical-interaction observation system that observes how humans use tools. The observation system measures the motion of the tools and the interactive force between the tool and the manipulated objects by the tool. For example, the system observes the knife motion during slicing cucumbers and the force between the knife and the cucumber. The recorded physical interactions from the human?s experience are then used to train grasping models and manipulation models. With the models, the robot will know how to grasp a tool and use it to perform manipulation tasks.  Using the physical-interaction observation, the project has collected 32 different types of daily-living motions such as stirring with a spatula, peeling a cucumber, brushing teeth, etc. Each motion has been performed multiple times by multiple participants. The project has collected about 3,000 trials in total.  The developed physical-interaction observation system is the first of its kind. Previous observation systems focused mainly on the human arm and hand motion. This system focuses on the tool?s motion and the force applied between the tool and the manipulated objects by the tool in great detail. The recording includes RGBD, 6-axis force and 6-axis motion. Since we release the setup, it has been replicated by several other researchers in the robotics field for their studies.  The dataset has been released on the project?s webpage along with its visualization source code (http://rpal.cse.usf.edu/datasets_manipulation.html). The data set along with the data collection setup has been released as a data paper. Using the collected data, we have developed a recurrent neural network approach that can perform human-like pouring tasks without accurate modeling of the environment or the task. It moves one step closer to deploying robots in a daily living environment. The dataset would also provide a verification dataset to evaluation dynamic simulations of physical interactions. Based on the data, the project has produced a manipulation motion taxonomy that can be used for studying the relationships between different types of manipulation motions. Inspired by the manipulations, the PI of the project has organized two Robotic Grasping and Manipulation Competitions during IROS 2016 and 207. The PI will organize the third Robotic Grasping and Manipulation Competitions in IROS 2019.  The project has produced three journal papers, two conference papers, and two book chapters. The project has trained two Ph.D. students and two REU students in performing robotics research. The data collection setup and the dataset have been incorporated into course projects in the PI?s an undergraduate-level class Intro to Robotics and graduate-level class Neural Network and Deep Learning class.          Last Modified: 06/30/2019       Submitted by: Yu Sun]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
