<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Ubiquitous Sensing Using Computational Light</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2016</AwardEffectiveDate>
<AwardExpirationDate>02/28/2022</AwardExpirationDate>
<AwardTotalIntnAmount>542403.00</AwardTotalIntnAmount>
<AwardAmount>542403</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alexander Sprintson</SignBlockName>
<PO_EMAI>asprints@nsf.gov</PO_EMAI>
<PO_PHON>7032922170</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The ability to sense and detect human movement is critical to the development of data-driven mobile health systems. It can help detect disease and foster behavioral changes to cultivate healthy lifestyles. Existing sensing technologies either require users to constantly wear or carry on-body potentially cumbersome devices, are vulnerable to electromagnetic interference, or present severe privacy risks involving leaking of sensitive data and images. This project takes a entirely different approach to addressing these issues. It exploits the use of ubiquitous light as a low-cost, unobtrusive, and accurate sensing medium capable of simultaneously sensing people and their surrounding context. The proposed vision " LightSense " consists of off-the-shelf LED lights on the ceiling and a few low-cost photodiode sensors sprinkled in the environment. The photodiodes passively capture light blockage created by the human body and reconstruct fine-grained user behaviors in real time. LightSense leverages light to turn a space into a cognitive space, which recognizes our presence, senses our behaviors such as postures and high-level activities while monitoring our health status indicators such as levels of stress. &lt;br/&gt;&lt;br/&gt;LightSense is empowered by Visible Light Communication (VLC) that turns the visible light into computational light. It contains the following novel systems and algorithmic designs: 1) a novel VLC network architecture with LED panels and sparse photodiodes to ease system deployment; 2) algorithmic and systems designs to separate light rays from dense LEDs, optimize the placement of photodiodes, and overcome the blockage of other objects (e.g., furniture, other users); 3) a new VLC primitive that allows light communication and sensing to be sustained even under extremely low light conditions; and 4) learning algorithms to infer physical activities, derive movement characteristics, and monitor psychological state. LightSense will be evaluated using real-scale testbeds and user studies. Results from this project will establish the foundational pieces to define a new research space (visible light sensing), and will generate far-reaching impact on promoting innovative interaction designs and enabling new types of precise health monitoring.</AbstractNarration>
<MinAmdLetterDate>02/25/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/07/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1552924</AwardID>
<Investigator>
<FirstName>Xia</FirstName>
<LastName>Zhou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xia Zhou</PI_FULL_NAME>
<EmailAddress>Xia.Zhou@Dartmouth.edu</EmailAddress>
<PI_PHON>6036468871</PI_PHON>
<NSF_ID>000659637</NSF_ID>
<StartDate>02/25/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Dartmouth College</Name>
<CityName>HANOVER</CityName>
<ZipCode>037551421</ZipCode>
<PhoneNumber>6036463007</PhoneNumber>
<StreetAddress>OFFICE OF SPONSORED PROJECTS</StreetAddress>
<StreetAddress2><![CDATA[11 ROPE FERRY RD #6210]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<StateCode>NH</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NH02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041027822</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF DARTMOUTH COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041027822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Dartmouth College]]></Name>
<CityName>Hanover</CityName>
<StateCode>NH</StateCode>
<ZipCode>037553510</ZipCode>
<StreetAddress><![CDATA[6211 Sudikoff Laboratory]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NH02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~97789</FUND_OBLG>
<FUND_OBLG>2017~149738</FUND_OBLG>
<FUND_OBLG>2018~56221</FUND_OBLG>
<FUND_OBLG>2019~116954</FUND_OBLG>
<FUND_OBLG>2020~121701</FUND_OBLG>
</Award>
</rootTag>
