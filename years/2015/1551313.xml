<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Incremental Semantic Sentence Processing Models</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>116601.00</AwardTotalIntnAmount>
<AwardAmount>116601</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>D.  Langendoen</SignBlockName>
<PO_EMAI>dlangend@nsf.gov</PO_EMAI>
<PO_PHON>7032925088</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Extracting a single meaning from the many possible interpretations of a complex sentence is one of the most sophisticated of human abilities, and is still beyond the reach of most artificial language processing systems.  Current computational models of human sentence processing can simulate human reading behavior using probability estimates of words and syntactic patterns, but are not yet sophisticated enough to estimate the probability of complex underlying ideas that are expressed across multiple sentences. This exploratory EAGER project extends human sentence processing models beyond these word- and syntax-based techniques to model complex cross-sentential meaning involving coreference relationships between pronouns and their antecedents, and quantificational relationships between individuals and groups. The proposed extensions are based on a graphical representation of discourse structure, which can be constructed incrementally in time order as sentences are processed. Probabilities associated with individual elements of these graphs are then combined to obtain probability estimates over possible meanings of input sentences, which can then be compared based on these probabilities. The resulting computational sentence processing models are then evaluated on explanatory text from on-line encyclopedia articles and on existing broad-coverage psycholinguistic datasets.&lt;br/&gt;&lt;br/&gt;Accurate models of how these complex relationships are decoded from natural language could further our understanding of how the brain works, and may someday allow non-programmer domain experts to explain desired products, goals and constraints to machines. But current broad-coverage sentence processing models are focused primarily on modeling syntax, in particular using probabilistic context-free grammar (PCFG) surprisal. Despite their syntactic sophistication, PCFG models make unrealistic assumptions that word sequences are generated without any continuity of referential meaning or any preferences among possible coreference and quantifier scope orderings. The proposed work will develop a more human-like semantic processing model by augmenting an existing incremental parser with a graphical dependency-based adaptation of discourse representation structures. The proposed semantic processing model will define complete semantic dependency representations of sentences, including quantifier scope and coreference relationships, even those that cross sentence boundaries. The model will then exploit the graphical nature of these dependency representations by estimating the probability of each analysis as the product of the probabilities of its component dependencies, based on the distributional similarity of each dependency's source predicate to the other predicates connected to its destination.</AbstractNarration>
<MinAmdLetterDate>08/03/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1551313</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Schuler</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William E Schuler</PI_FULL_NAME>
<EmailAddress>schuler.77@osu.edu</EmailAddress>
<PI_PHON>6142929504</PI_PHON>
<NSF_ID>000326703</NSF_ID>
<StartDate>08/03/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName>Columbus</CityName>
<StateCode>OH</StateCode>
<ZipCode>432101298</ZipCode>
<StreetAddress><![CDATA[1712 Neil Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~116601</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Much of the success of the human species can be attributed to our unique ability to encode and decode complex ideas into linguistic expressions that can be shared and debated and deployed in coordinated plans. &nbsp;Better models of how our ideas are constructed may help us understand what we are as conscious beings.</p> <p>This project has produced a freely available dataset of sentences about basic knowledge from encyclopedia articles annotated with compositional meanings at a level of detail that would support the automatic formulation of novel plans. These annotations specify the linguistic source of each piece of information in a way that could be incrementally decoded in a human-like associative memory.</p> <p>This project has also produced a working freely available sentence processing model that can be trained on this data to decode multi-sentence discourses into a graphical representation of formal meaning that are compatible with existing mathematical models of associative memory.</p> <p>Linguistic evaluations of this model against human annotations and psycholinguistic evaluations of this model against human reading times show that this model can accurately reproduce human-attested annotations, and show that aspects of this model indeed predict known effects in sentence processing, which together suggest that this level of linguistic analysis is viable for cognitive modeling research.</p><br> <p>            Last Modified: 11/11/2018<br>      Modified by: William&nbsp;E&nbsp;Schuler</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Much of the success of the human species can be attributed to our unique ability to encode and decode complex ideas into linguistic expressions that can be shared and debated and deployed in coordinated plans.  Better models of how our ideas are constructed may help us understand what we are as conscious beings.  This project has produced a freely available dataset of sentences about basic knowledge from encyclopedia articles annotated with compositional meanings at a level of detail that would support the automatic formulation of novel plans. These annotations specify the linguistic source of each piece of information in a way that could be incrementally decoded in a human-like associative memory.  This project has also produced a working freely available sentence processing model that can be trained on this data to decode multi-sentence discourses into a graphical representation of formal meaning that are compatible with existing mathematical models of associative memory.  Linguistic evaluations of this model against human annotations and psycholinguistic evaluations of this model against human reading times show that this model can accurately reproduce human-attested annotations, and show that aspects of this model indeed predict known effects in sentence processing, which together suggest that this level of linguistic analysis is viable for cognitive modeling research.       Last Modified: 11/11/2018       Submitted by: William E Schuler]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
