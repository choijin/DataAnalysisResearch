<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Learning to Predict Temporal Interestingness for Videos</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2016</AwardEffectiveDate>
<AwardExpirationDate>03/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>174674.00</AwardTotalIntnAmount>
<AwardAmount>182634</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project examines the role that implicit feedback from viewers can play in learning a temporal interestingness function for videos. The key insight is that by leveraging pupil dilation as ground truth, supervised machine learning approaches can be applied to this problem. The ubiquitous presence of cameras in every phone, and the ability to share content with the entire world have made videos a powerful tool in the hands of everyday people. This project is to address the challenge that viewers have ever-shortening attention spans, and constructing a succinct message is hard. The project provides research and training opportunities for both undergraduate and graduate students in computer vision, machine learning, and human-centered computing.&lt;br/&gt; &lt;br/&gt;This project collects a corpus of eye-tracking data as viewers watch a collection of videos, via an off-the-shelf eye-tracking device with the objective of investigating the effectiveness of a controlled brightness calibration method to separate pupillary light reflex from pupillary emotional response. The emotional response data is leveraged as dense labels for a supervised learning approach towards predicting a video interestingness function, and algorithms are developed to cut videos to their most succinct portions based on this interestingness function. A predictive model of video interestingness could potentially impact video retrieval, summarization, and search. This would impact fields as diverse as communication and online education. Further, just as research in image saliency was hugely accelerated by the use of eye-tracking data for training, validation, and benchmarking, this project can lead to a similar unification of diverse approaches, and efforts, around an implicit, temporally dense source of ground truth for temporal interestingness in videos.</AbstractNarration>
<MinAmdLetterDate>04/14/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1566481</AwardID>
<Investigator>
<FirstName>Eakta</FirstName>
<LastName>Jain</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eakta Jain</PI_FULL_NAME>
<EmailAddress>ejain@cise.ufl.edu</EmailAddress>
<PI_PHON>3523883081</PI_PHON>
<NSF_ID>000679375</NSF_ID>
<StartDate>04/14/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName/>
<StateCode>FL</StateCode>
<ZipCode>326115500</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026y</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~174674</FUND_OBLG>
<FUND_OBLG>2017~7960</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="background: white;"><span style="font-size: 10.0pt;">The major goal of this project is to leverage eye-tracking data, including gaze positions and pupil diameter changes, as an implicit annotation for video interesting-ness prediction. Eye tracking users as they watch videos provides a stream of rich data that contains patterns of attention and emotion. Gaze positions reveal where users focused their attention, and pupil diameter changes index their arousal state. Toward the major goal, the research funded by this award has developed pupillary light response models to account for brightness related pupil diameter changes and methods to measure and visualize interesting regions in images and videos, including omnidirectional 360 content. The premise of the project is the abundant availability of eye tracking data. With eye trackers being designed into laptops, phones, and mixed reality headsets, large scale eye tracking creates novel security and privacy challenges. The activities supported by this project pushed forward this new frontier for eye tracking research through research papers on specific privacy and security threats and mitigations. The project has produced eleven publications in total (eight in peer-reviewed conferences and journals) and two manuscripts currently under review. The award has funded six graduate students (three women, one Native American) and nine undergraduate students (four women, two African-American). </span></p> <!--    @font-face  {font-family:"Cambria Math";  panose-1:2 4 5 3 5 4 6 3 2 4;  mso-font-charset:0;  mso-generic-font-family:roman;  mso-font-pitch:variable;  mso-font-signature:-536869121 1107305727 33554432 0 415 0;} @font-face  {font-family:Calibri;  panose-1:2 15 5 2 2 2 4 3 2 4;  mso-font-charset:0;  mso-generic-font-family:swiss;  mso-font-pitch:variable;  mso-font-signature:-469750017 -1073732485 9 0 511 0;} @font-face  {font-family:ArialMT;  panose-1:2 11 6 4 2 2 2 2 2 4;  mso-font-alt:Arial;  mso-font-charset:0;  mso-generic-font-family:roman;  mso-font-pitch:auto;  mso-font-signature:0 0 0 0 0 0;}    p.MsoNormal, li.MsoNormal, div.MsoNormal  {mso-style-unhide:no;  mso-style-qformat:yes;  mso-style-parent:";  margin-top:0in;  margin-right:0in;  margin-bottom:8.0pt;  margin-left:0in;  line-height:107%;  mso-pagination:widow-orphan;  font-size:12.0pt;  mso-bidi-font-size:11.0pt;  font-family:"Times New Roman",serif;  mso-fareast-font-family:Calibri;  mso-bidi-font-family:Calibri;  mso-fareast-language:ZH-CN;} p  {mso-style-priority:99;  mso-margin-top-alt:auto;  margin-right:0in;  mso-margin-bottom-alt:auto;  margin-left:0in;  mso-pagination:widow-orphan;  font-size:12.0pt;  font-family:"Times New Roman",serif;  mso-fareast-font-family:"Times New Roman";} .MsoChpDefault  {mso-style-type:export-only;  mso-default-props:yes;  font-family:"Calibri",sans-serif;  mso-ascii-font-family:Calibri;  mso-ascii-theme-font:minor-latin;  mso-fareast-font-family:Calibri;  mso-hansi-font-family:Calibri;  mso-hansi-theme-font:minor-latin;  mso-bidi-font-family:"Times New Roman";  mso-bidi-theme-font:minor-bidi;}size:8.5in 11.0in;  margin:1.0in 1.0in 1.0in 1.0in;  mso-header-margin:.5in;  mso-footer-margin:.5in;  mso-paper-source:0;} div.WordSection1  {page:WordSection1;} --><br> <p>            Last Modified: 05/25/2020<br>      Modified by: Eakta&nbsp;Jain</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The major goal of this project is to leverage eye-tracking data, including gaze positions and pupil diameter changes, as an implicit annotation for video interesting-ness prediction. Eye tracking users as they watch videos provides a stream of rich data that contains patterns of attention and emotion. Gaze positions reveal where users focused their attention, and pupil diameter changes index their arousal state. Toward the major goal, the research funded by this award has developed pupillary light response models to account for brightness related pupil diameter changes and methods to measure and visualize interesting regions in images and videos, including omnidirectional 360 content. The premise of the project is the abundant availability of eye tracking data. With eye trackers being designed into laptops, phones, and mixed reality headsets, large scale eye tracking creates novel security and privacy challenges. The activities supported by this project pushed forward this new frontier for eye tracking research through research papers on specific privacy and security threats and mitigations. The project has produced eleven publications in total (eight in peer-reviewed conferences and journals) and two manuscripts currently under review. The award has funded six graduate students (three women, one Native American) and nine undergraduate students (four women, two African-American).         Last Modified: 05/25/2020       Submitted by: Eakta Jain]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
