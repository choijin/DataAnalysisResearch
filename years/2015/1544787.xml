<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: MONA LISA -  Monitoring and Assisting with Actions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>800000.00</AwardTotalIntnAmount>
<AwardAmount>800000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ralph Wachter</SignBlockName>
<PO_EMAI>rwachter@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Cyber-physical systems of the near future will collaborate with humans. Such cognitive systems will need to understand what the humans are doing. They will need to interpret human action in real-time and predict the humans' immediate intention in complex, noisy and cluttered environments. This proposal puts forward a new architecture for cognitive cyber-physical systems that can understand complex human activities, and focuses specifically on manipulation activities. The proposed architecture, motivated by biological perception and control, consists of three layers. At the bottom layer are vision processes that detect, recognize and track humans, their body parts, objects, tools, and object geometry. The middle layer contains symbolic models of the human activity, and it assembles through a grammatical description the recognized signal components of the previous layer into a representation of the ongoing activity. Finally, at the top layer is the cognitive control, which decides which parts of the scene will be processed next and which algorithms will be applied where. It modulates the vision processes by fetching additional knowledge when needed, and directs the attention by controlling the active vision system to direct its sensors to specific places. Thus, the bottom layer is the perception, the middle layer is the cognition, and the top layer is the control. All layers have access to a knowledge base, built in offline processes, which contains the semantics about the actions.&lt;br/&gt;&lt;br/&gt;The feasibility of the approach will be demonstrated through the development of a smart manufacturing system, called MONA LISA, which assists humans in assembly tasks. This system will monitor humans as they perform assembly task. It will recognize the assembly action and determine whether it is correct and will communicate to the human possible errors and suggest ways to proceed. The system will have advanced visual sensing and perception; action understanding grounded in robotics and human studies; semantic and procedural-like memory and reasoning, and a control module linking high-level reasoning and low-level perception for real time, reactive and proactive engagement with the human assembler. &lt;br/&gt;&lt;br/&gt;The proposed work will bring new tools and methodology to the areas of sensor networks and robotics and is applicable, besides smart manufacturing, to a large variety of sectors and applications. Being able to analyze human behavior using vision sensors will have impact on many sectors, ranging from healthcare and advanced driver assistance to human robot collaboration. The project will also catalyze K-12 outreach, new courseware (undergraduate and graduate), publication and open-source software.</AbstractNarration>
<MinAmdLetterDate>09/11/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/11/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1544787</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Baras</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John S Baras</PI_FULL_NAME>
<EmailAddress>baras@umd.edu</EmailAddress>
<PI_PHON>3014056606</PI_PHON>
<NSF_ID>000107720</NSF_ID>
<StartDate>09/11/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John (Yiannis)</FirstName>
<LastName>Aloimonos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John (Yiannis) Aloimonos</PI_FULL_NAME>
<EmailAddress>yiannis@cfar.umd.edu</EmailAddress>
<PI_PHON>3014054526</PI_PHON>
<NSF_ID>000176899</NSF_ID>
<StartDate>09/11/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Cornelia</FirstName>
<LastName>Fermuller</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cornelia M Fermuller</PI_FULL_NAME>
<EmailAddress>fer@cfar.umd.edu</EmailAddress>
<PI_PHON>3014054526</PI_PHON>
<NSF_ID>000235233</NSF_ID>
<StartDate>09/11/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7918</Code>
<Text>CYBER-PHYSICAL SYSTEMS (CPS)</Text>
</ProgramReference>
<ProgramReference>
<Code>8235</Code>
<Text>CPS-Synergy</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~800000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this cyberphysical systems award was to advance human machine interaction by creating new technology for human action interpretation. Researchers from computer vision, cognitive knowledge engineering, robotics, control, and systems engineering collaborated to develop a framework for designing cognitive systems that understand and predict complex human fine-motor activities, and demonstrate it in a prototype system that assists humans in assembly tasks. The proposed architecture consists of three layers: at the bottom are vision processes for monitoring objects and actions; in the middle reside symbolic models of the activity to be performed and mechanisms for parsing the ongoing action; and at the top is the cognitive control, which decides which parts of the scene to monitor, where to look and what to process.</p> <p>Novel vision processes developed in this project include point cloud software libraries with modules for reconstructing complex movements of humans and objects, modules for detecting, segmenting, and reconstructing objects in cluttered scenes utilizing the concept of symmetry, and &nbsp;software for predicting from video the next movements in dexterous actions by leveraging additional information learned off-line from tactile and IMU data.</p> <p>The mid-level representation is a grammar of actions. Considering the goals of actions, videos are partitioned into segments, and these segments are organized in a hierarchical structure. The semantics of actions is learned from videos and text, and enhanced with reasoning using common-sense knowledge acquired from language.</p> <p>The cognitive control was modeled using a control framework for dynamical systems with temporal logical constraints. Specific modules include an active viewpoint control mechanism for monitoring dynamic targets, and task-driven target detection selection mechanisms to optimize the computational costs.</p> <p>The work brought new tools and methodology to the areas of sensor networks and robotics and is applicable, besides smart manufacturing, to a large variety of sectors and applications. Being able to analyze human behavior using vision sensors will have an impact on healthcare, advanced driver assistance, augmented reality, cyber-learning, and human-robot collaboration. The project also educated 9 graduate students, catalyzed K-12 outreach, new courseware (undergraduate and graduate), and led to open-source software.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/24/2018<br>      Modified by: Cornelia&nbsp;M&nbsp;Fermuller</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this cyberphysical systems award was to advance human machine interaction by creating new technology for human action interpretation. Researchers from computer vision, cognitive knowledge engineering, robotics, control, and systems engineering collaborated to develop a framework for designing cognitive systems that understand and predict complex human fine-motor activities, and demonstrate it in a prototype system that assists humans in assembly tasks. The proposed architecture consists of three layers: at the bottom are vision processes for monitoring objects and actions; in the middle reside symbolic models of the activity to be performed and mechanisms for parsing the ongoing action; and at the top is the cognitive control, which decides which parts of the scene to monitor, where to look and what to process.  Novel vision processes developed in this project include point cloud software libraries with modules for reconstructing complex movements of humans and objects, modules for detecting, segmenting, and reconstructing objects in cluttered scenes utilizing the concept of symmetry, and  software for predicting from video the next movements in dexterous actions by leveraging additional information learned off-line from tactile and IMU data.  The mid-level representation is a grammar of actions. Considering the goals of actions, videos are partitioned into segments, and these segments are organized in a hierarchical structure. The semantics of actions is learned from videos and text, and enhanced with reasoning using common-sense knowledge acquired from language.  The cognitive control was modeled using a control framework for dynamical systems with temporal logical constraints. Specific modules include an active viewpoint control mechanism for monitoring dynamic targets, and task-driven target detection selection mechanisms to optimize the computational costs.  The work brought new tools and methodology to the areas of sensor networks and robotics and is applicable, besides smart manufacturing, to a large variety of sectors and applications. Being able to analyze human behavior using vision sensors will have an impact on healthcare, advanced driver assistance, augmented reality, cyber-learning, and human-robot collaboration. The project also educated 9 graduate students, catalyzed K-12 outreach, new courseware (undergraduate and graduate), and led to open-source software.             Last Modified: 11/24/2018       Submitted by: Cornelia M Fermuller]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
