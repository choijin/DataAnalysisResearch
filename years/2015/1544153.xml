<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: The Role of Instructor and Peer Feedback in Improving the Cognitive, Interpersonal, and Intrapersonal Competencies of Student Writers in STEM Courses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>70748.00</AwardTotalIntnAmount>
<AwardAmount>70748</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Connie Della-Piana</SignBlockName>
<PO_EMAI>cdellapi@nsf.gov</PO_EMAI>
<PO_PHON>7032925309</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field.&lt;br/&gt;&lt;br/&gt;This project will have critical significance for Science, Technology, Engineering, and Mathematics (STEM) educators by increasing writing and collaboration skills in students, areas of importance to economics, science, and national security. This study focuses on teacher and peer interactions and writing quality and improvement in the context of undergraduate STEM courses. Specifically, the project will map the development of three competency domains (cognitive, interpersonal and intrapersonal) by researching the effects of teacher and peer response on writing improvement and knowledge adaptation in STEM courses. The project utilizes a web-based assessment tool called My Reviewers (MyR). The tool will be piloted by STEM faculty in college-level Introductory Biology or Chemistry on the campuses of University of South Florida (USF), North Carolina State University (NCSU), Dartmouth, Massachusetts Institute of Technology (MIT), and University of Pennsylvania (UPenn). Research domains include both academic performance and inter/intra-personal competencies. Project deliverables will provide new tools and procedures to assist in the assessment of students' knowledge, skills, and attitudes for project and program evaluation.&lt;br/&gt;&lt;br/&gt;Approximately 10,000 students enrolled in STEM courses at USF, NCSU, Dartmouth, MIT, and UPenn will upload their course-based writing to My Reviewers, an assessment tool, and use the tool to conduct peer reviews and team projects.  This information is supplemented by surveys of demographics and dispositions along with click patterns within the toolset. Researchers will subsequently analyze this wealth of data using predictive modeling of student writing ability and improvement, including text-based methods to identify useful features of comments, papers, peer reviews, student evaluations of other peers? reviews, and instructor and student meta-reflections. Outcome goals are to (1) demonstrate ways the assessment community can use real-time assessment tools to create valid measures of writing development; (2) provide quantitative evidence regarding the likely effects of particular commenting and scoring patterns on cohorts of students; (3) offer a domain map to help STEM educators better understand student success in the STEM curriculum; and (4) inform STEM faculty regarding the efficacy of peer review.</AbstractNarration>
<MinAmdLetterDate>09/16/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/16/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1544153</AwardID>
<Investigator>
<FirstName>Roger</FirstName>
<LastName>Sloboda</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Roger D Sloboda</PI_FULL_NAME>
<EmailAddress>rds@dartmouth.edu</EmailAddress>
<PI_PHON>6036462377</PI_PHON>
<NSF_ID>000183515</NSF_ID>
<StartDate>09/16/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christiane</FirstName>
<LastName>Donahue</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christiane Donahue</PI_FULL_NAME>
<EmailAddress>Christian.K.Donahue@dartmouth.edu</EmailAddress>
<PI_PHON>6036463007</PI_PHON>
<NSF_ID>000695265</NSF_ID>
<StartDate>09/16/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Dartmouth College</Name>
<CityName>HANOVER</CityName>
<ZipCode>037551421</ZipCode>
<PhoneNumber>6036463007</PhoneNumber>
<StreetAddress>OFFICE OF SPONSORED PROJECTS</StreetAddress>
<StreetAddress2><![CDATA[11 ROPE FERRY RD #6210]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<StateCode>NH</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NH02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041027822</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF DARTMOUTH COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041027822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Dartmouth College]]></Name>
<CityName>Hanover</CityName>
<StateCode>NH</StateCode>
<ZipCode>037551404</ZipCode>
<StreetAddress><![CDATA[11 Rope Ferry Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NH02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>009Z</Code>
<Text>PRIME - Promoting Research and Innovatio</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~70748</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project involves a collaboration among five institutions: the University of South Florida, Dartmouth College, the University of Pennsylvania, MIT, and North Carolina State University. The project focuses on how university students in STEM (Science, Technology, Engineering, and Mathematics) disciplines interact in peer reviews when they are asked to draft, review, and revise written assignments. It also develops new research capacities for scholars interested in studying writing and written responses with big data methodologies, and it offers insights into the complications of trying to do multi-site research across diverse higher education institutions.</p> <p>The project&rsquo;s initial goals included:</p> <p>- Demonstrating ways the assessment community can use big data assessment tools to create valid measures of writing development and knowledge adaptation</p> <p>- Providing quantitative evidence regarding the likely effects of particular writing feedback patterns on particular cohorts of students</p> <p>- Informing STEM faculty regarding how well students&rsquo; peer review of STEM writing works</p> <p>From the project, which introduced peer review into three different Biology classes over three years via an online platform, MyReviewers, we were able to analyze hundreds of peer review texts and better understand how students in Biology classes, in comparison to students in writing contexts such as first-year composition or other STEM fields such as Chemistry, developed and used peer reviews: what kinds of comments do students make, how similar are they across contexts, and how much do they line up with what experts in the field of writing consider to be the most important terms and approaches to use?</p> <p>We found that there is not much overlap between what writing experts consider to be the best terms and what STEM students actually use. This suggests that an expanded definition of high quality peer review may be necessary for STEM writing. We also saw a correspondence between the results and the differences in assignments between chemistry and biology. Peer reviews from the chemistry course focused on meaning, content, and procedure while biology reviews contained evaluative terms. We found that the nature of the assignments is much more likely to lead to evaluative terms when the rubrics or the assignments called out these terms. The clear link between the assignments prompts, rubrics, and peer reviews indicates that faculty must think carefully about the design of assignments and adapt rubrics to successfully capture the interplay between discipline, levels, and types of learning.&nbsp;</p> <p>We hope this work, overall, demonstrates that peer review is important to STEM writing, needs to be conceived differently than first-year writing peer review, and can be studied using big-data approaches to analysis to understand dominant patterns. We see additional research questions to pursue in the data, focused on understanding how peer review helps STEM students to better communicate about science.</p><br> <p>            Last Modified: 11/30/2019<br>      Modified by: Christiane&nbsp;Donahue</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project involves a collaboration among five institutions: the University of South Florida, Dartmouth College, the University of Pennsylvania, MIT, and North Carolina State University. The project focuses on how university students in STEM (Science, Technology, Engineering, and Mathematics) disciplines interact in peer reviews when they are asked to draft, review, and revise written assignments. It also develops new research capacities for scholars interested in studying writing and written responses with big data methodologies, and it offers insights into the complications of trying to do multi-site research across diverse higher education institutions.  The project’s initial goals included:  - Demonstrating ways the assessment community can use big data assessment tools to create valid measures of writing development and knowledge adaptation  - Providing quantitative evidence regarding the likely effects of particular writing feedback patterns on particular cohorts of students  - Informing STEM faculty regarding how well students’ peer review of STEM writing works  From the project, which introduced peer review into three different Biology classes over three years via an online platform, MyReviewers, we were able to analyze hundreds of peer review texts and better understand how students in Biology classes, in comparison to students in writing contexts such as first-year composition or other STEM fields such as Chemistry, developed and used peer reviews: what kinds of comments do students make, how similar are they across contexts, and how much do they line up with what experts in the field of writing consider to be the most important terms and approaches to use?  We found that there is not much overlap between what writing experts consider to be the best terms and what STEM students actually use. This suggests that an expanded definition of high quality peer review may be necessary for STEM writing. We also saw a correspondence between the results and the differences in assignments between chemistry and biology. Peer reviews from the chemistry course focused on meaning, content, and procedure while biology reviews contained evaluative terms. We found that the nature of the assignments is much more likely to lead to evaluative terms when the rubrics or the assignments called out these terms. The clear link between the assignments prompts, rubrics, and peer reviews indicates that faculty must think carefully about the design of assignments and adapt rubrics to successfully capture the interplay between discipline, levels, and types of learning.   We hope this work, overall, demonstrates that peer review is important to STEM writing, needs to be conceived differently than first-year writing peer review, and can be studied using big-data approaches to analysis to understand dominant patterns. We see additional research questions to pursue in the data, focused on understanding how peer review helps STEM students to better communicate about science.       Last Modified: 11/30/2019       Submitted by: Christiane Donahue]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
