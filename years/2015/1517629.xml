<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: The Ever-Changing Network: How Changes in Architecture Shape Neural Computations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>329445.00</AwardTotalIntnAmount>
<AwardAmount>329445</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Junping Wang</SignBlockName>
<PO_EMAI>jwang@nsf.gov</PO_EMAI>
<PO_PHON>7032924488</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Our brains are constantly changing. Experiences and memories leave their imprints on connections between neurons. Understanding this process is fundamental to understanding how the brain works. While this question has been of central importance to neuroscience for decades, at this moment researchers are well positioned to make significant progress -- new recording devices and imaging techniques are revealing the activity and changes within the networks of the brain at unprecedented scale and resolution. Sound mathematical models are essential to keep up with the mounting avalanche of data. The goal of this project is to develop mathematical tools to assist with improving understanding how networks of neurons are shaped by experiences. Developing this theory is crucial for understanding learning, as well as associated disorders. The project will focus on how learning improves the brain's ability to make decisions and store memories. Graduate students and postdocs joining this project will be part of an established, interdisciplinary mathematics research community. Trainees will gain a wide perspective of mathematical neuroscience through integrated research at three institutions, including extensive visits among them. &lt;br/&gt;&lt;br/&gt;This research project builds on earlier results of this team to address a central challenge in the mathematical analysis of biophysically realistic neuronal networks: How brain activity changes brain structure over time. Understanding neural computation demands a description of how network dynamics co-evolves with network architecture. The research team will address this challenge by answering specific questions about the interplay between spatiotemporal patterns of neural activity, the attendant changes in network architectures, and the resulting neural computations. This project focuses on two main questions. First, what mathematical techniques can describe the co-evolution of network dynamics and network connectivity toward stable assemblies of neurons? To address this question this project will build a theory describing how global network structure evolves under the dynamics of biophysically realistic plasticity rules that operate on the scale of individual spikes and synapses. Analysis of these models requires novel multiscale and averaging methods. The resulting equations allow analysis of the stability of network architectures and their dependence on stimulus drive. With these results, the second question can be addressed: How does network plasticity create spatiotemporal dynamics that support the basic building blocks of neural computation? Models to understand how plasticity forms networks whose dynamics underlie specific operations on incoming stimuli will be developed to address this question. The mechanism by which long-term plasticity can reshape the connectivity of a network to encode a precise temporal sequence of events will also be investigated.</AbstractNarration>
<MinAmdLetterDate>07/02/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1517629</AwardID>
<Investigator>
<FirstName>Kresimir</FirstName>
<LastName>Josic</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kresimir Josic</PI_FULL_NAME>
<EmailAddress>josic@math.uh.edu</EmailAddress>
<PI_PHON>7137433485</PI_PHON>
<NSF_ID>000492739</NSF_ID>
<StartDate>07/02/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Zachary</FirstName>
<LastName>Kilpatrick</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zachary P Kilpatrick</PI_FULL_NAME>
<EmailAddress>zpkilpat@colorado.edu</EmailAddress>
<PI_PHON>9194341135</PI_PHON>
<NSF_ID>000539544</NSF_ID>
<StartDate>07/02/2015</StartDate>
<EndDate>06/10/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036837920</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042916627</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>772042015</ZipCode>
<StreetAddress><![CDATA[4800 Calhoun Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7275</Code>
<Text>Cross-BIO Activities</Text>
</ProgramElement>
<ProgramElement>
<Code>7334</Code>
<Text>MATHEMATICAL BIOLOGY</Text>
</ProgramElement>
<ProgramElement>
<Code>7454</Code>
<Text>MSPA-INTERDISCIPLINARY</Text>
</ProgramElement>
<ProgramElement>
<Code>7713</Code>
<Text>Activation</Text>
</ProgramElement>
<ProgramReference>
<Code>5500</Code>
<Text>NEURAL SYSTEMS CLUSTER</Text>
</ProgramReference>
<ProgramReference>
<Code>8007</Code>
<Text>BioMaPS</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~329445</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The collective activity of neurons determines how we perceive the world, combine perceptions with perviously obtained information, and translate the results into actions. However, the world is not static, and we continuously gather experiences that chage all stages of this process. It is thought that such learning occurs largely through the reshaping of the networks of neurons in our brains. Yet, it is still not clear how the intrinsic activity within, and inputs to these networks shape their stracture. In this project we used a combination of computational modeling and mathematical analysis to understand how such reshaping occurs.&nbsp;</p> <p>We started by deriving equations that relate changes in synaptic weights to the patterns of activity and inputs to a neural network. An analysis of these equations then allowed us to understand how local interactions between cells, and coarse features of the whole network drive changes that can reshape both spontaneous activity, and how a network responds to its inputs. In addition, we have developed statistical techniques to better identify interactions between cells, and the evolution of such interactions in experimental data.</p> <p>We next asked how neural networks can fluidly integrate information to guide decisions in a world that is in flux. To answer this question we developed abstract computational models of information processing whose changes in activity and connectivity are adapted to the timescale of changes of the environment. We started by considering a concrete, experimentally motivated example of a subject gathering noisy evidence to choose between two options, only one of which leads to a reward. This simplified setting allowed us to understand the best way to maximize rewards by accumulating evidence to make decisions. While these computations are too complex to be carried out by humans or other animals, we were able to develope &nbsp;an aproximation that suggests how neural networks can be continuosly reshaped to infer the current best choice, and learn to predict the next in a sequence of events.&nbsp;&nbsp;</p> <p>We also asked how new information interacts with knowledge that is already encoded in the structure of the network. We showed how neural networks, whose connections undergo short term changes, can store past stimuli even after the activity due to these inputs is extinguished. Such changes then shape future activity patterns in a way that can account for some common human cognitive biases. Overall this work examines how plasticity can shape working memory across long timescales and multiple trials, and suggest that the history-dependence of plasticity may be essential to understanding the mechanics of &nbsp;memory.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/17/2019<br>      Modified by: Kresimir&nbsp;Josic</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1517629/1517629_10373743_1568774133397_figure--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1517629/1517629_10373743_1568774133397_figure--rgov-800width.jpg" title="Model development"><img src="/por/images/Reports/POR/2019/1517629/1517629_10373743_1568774133397_figure--rgov-66x44.jpg" alt="Model development"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We developed a model neural network that can fluidly integrate information to guide decisions in a world that is in flux.  By approximating the computations in an abstract model, we obtained a biologically feasible model that  makes near-optimal decisions.</div> <div class="imageCredit">A. Radillo, A. Veliz-Cuba, K. Josic, Z. Kilpatrick</div> <div class="imageSubmitted">Kresimir&nbsp;Josic</div> <div class="imageTitle">Model development</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The collective activity of neurons determines how we perceive the world, combine perceptions with perviously obtained information, and translate the results into actions. However, the world is not static, and we continuously gather experiences that chage all stages of this process. It is thought that such learning occurs largely through the reshaping of the networks of neurons in our brains. Yet, it is still not clear how the intrinsic activity within, and inputs to these networks shape their stracture. In this project we used a combination of computational modeling and mathematical analysis to understand how such reshaping occurs.   We started by deriving equations that relate changes in synaptic weights to the patterns of activity and inputs to a neural network. An analysis of these equations then allowed us to understand how local interactions between cells, and coarse features of the whole network drive changes that can reshape both spontaneous activity, and how a network responds to its inputs. In addition, we have developed statistical techniques to better identify interactions between cells, and the evolution of such interactions in experimental data.  We next asked how neural networks can fluidly integrate information to guide decisions in a world that is in flux. To answer this question we developed abstract computational models of information processing whose changes in activity and connectivity are adapted to the timescale of changes of the environment. We started by considering a concrete, experimentally motivated example of a subject gathering noisy evidence to choose between two options, only one of which leads to a reward. This simplified setting allowed us to understand the best way to maximize rewards by accumulating evidence to make decisions. While these computations are too complex to be carried out by humans or other animals, we were able to develope  an aproximation that suggests how neural networks can be continuosly reshaped to infer the current best choice, and learn to predict the next in a sequence of events.    We also asked how new information interacts with knowledge that is already encoded in the structure of the network. We showed how neural networks, whose connections undergo short term changes, can store past stimuli even after the activity due to these inputs is extinguished. Such changes then shape future activity patterns in a way that can account for some common human cognitive biases. Overall this work examines how plasticity can shape working memory across long timescales and multiple trials, and suggest that the history-dependence of plasticity may be essential to understanding the mechanics of  memory.          Last Modified: 09/17/2019       Submitted by: Kresimir Josic]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
