<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Medium: Improving Network Performance and Efficiency through Multi-Channel Network Links</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>1100000.00</AwardTotalIntnAmount>
<AwardAmount>1100000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Deepankar Medhi</SignBlockName>
<PO_EMAI>dmedhi@nsf.gov</PO_EMAI>
<PO_PHON>7032922935</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project seeks to challenge the conventional abstract model of a high-speed network link as a single, logical point of attachment.  Instead, the proposed approach exposes the inherent parallelism that exists within the end host, network links, and the network fabric as a whole, to applications and the network control plane. The result is a network fabric consisting of a number of distinct physical networks that coexist within a single physical topology. By decoupling the channels making up network links, the project radically redesigns the network fabric to address today's requirements and challenges. A key hypothesis of this work is that composing multiple, potentially heterogeneous networks provides for greater scaling, performance, service quality, and manageability than maintaining the legacy fat-pipe link abstraction.&lt;br/&gt;&lt;br/&gt;The proposed research will impact the broader community in four ways: (1) by addressing the societal need of large-scale networking infrastructure to support next-generation clusters and data centers, by (2) engaging with industry to help inform the design and construction of new devices, by (3) interacting with other scientific communities through interdisciplinary research, and by (4) engaging with graduate and undergraduate students to translate the resulting research into structured courses and hands-on learning experiences for traditionally under-represented&lt;br/&gt;student groups.</AbstractNarration>
<MinAmdLetterDate>08/11/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/02/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1564185</AwardID>
<Investigator>
<FirstName>Alex</FirstName>
<LastName>Snoeren</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alex C Snoeren</PI_FULL_NAME>
<EmailAddress>snoeren@cs.ucsd.edu</EmailAddress>
<PI_PHON>8588222289</PI_PHON>
<NSF_ID>000482412</NSF_ID>
<StartDate>08/11/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>George</FirstName>
<LastName>Porter</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George Porter</PI_FULL_NAME>
<EmailAddress>gmporter@cs.ucsd.edu</EmailAddress>
<PI_PHON>8588226818</PI_PHON>
<NSF_ID>000553493</NSF_ID>
<StartDate>08/11/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~717344</FUND_OBLG>
<FUND_OBLG>2018~382656</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Processing vast amounts of data requires harnessing a large number of compute, storage, and accelerator functions so that they operate as a single unit. While scaling individual servers and storage devices is simple, interconnecting them is increasingly hard, placing new challenges on the underlying network fabric. Housed in large-scale Internet ?datacenters?, these large networks push the limits of commercial packet switches. Because of limitations in the ability of these packet switches to scale to increasing fast link rates, operators have had to construct multi-chip <em>chassis</em> which serve to deliver what is in effect a high-radix, high-bandwidth switch platform, albeit at high cost and power demands. The goal of this project has been to dramatically reduce the cost, energy, and complexity of high-speed datacenter networks by instead restructuring the network fabrics as entirely parallel designs. Rather than relying on multi-chip chassis to implement the abstraction of a single link, instead we expose the inherent parallelism of the underlying packet switch chips, much like modern PCs expose the inherent parallelism of multi-core CPUs to the operating system and application programs. Furthermore, by scaling network bandwidth through parallelism, we permit novel designs that compose entirely homogeneous parallel networks, entirely heterogeneous parallel networks, or interestingly a combination of networks into a single logical, though physically parallel, network interconnect. The end result is a network capable of higher bandwidth than existing networks at lower power and cost, due primarily to requiring fewer merchant silicon packet switch chips per endhost.</p> <p>The goal of this project is to support parallel endhost network stacks, network interfaces, and transport protocols that support disjoint parallel dataplanes. We began by evaluating the performance of Intel?s DPDK userlevel network stack for injecting packets into each of the dataplanes, acting as a software NIC device. In particular, we focused on the effect of the memory hierarchy on software NIC performance. Focusing on the BESS[1] software NIC framework, we found that up to 10 or 40-Gb/s it performed well, but past then the packet rate was too high to entirely control in software. Our next major investigation looked at the applicability of FPGA-based NICs to precise control over packet injection into parallel dataplanes. Here we found that the FPGA was capable, however careful attention has to be paid to where the source (or destination) data resides in the memory hierarchy. We have shown that careful use of Intel?s ISA-level cache control instructions are necessary to ensure that data is accessible at next-generation network speeds.</p> <p>We next evaluated the choice of transport/routing protocol for selecting paths through the dataplanes, and placing packets onto each of those paths. What we found was that the standard ECMP-based path selection approach, common to folded-Clos topologies, resulted in lower-than-expected performance when considering parallel homogeneous folded-Clos networks. The reason for the low performance was the explosion of paths. Instead, we found that choosing a k-shortest paths approach along with the MPTCP[2] transport led to good performance on heterogeneous folded-Clos networks.</p> <p>Once ECMP is eschewed, it opens up new possibilities for implementing the parallel dataplanes with heterogeneous networks. We found that Expander-graph networks have good properties, since their random design allows for multiple ?instantiations? of each Expander. The end result is that endhosts have the ability to choose the shortest path not just within one network, but across network planes. The end result is a very low-latency network interconnect.</p> <p>To demonstrate these research designs, we next focused on application design, in particular designs that would lend themselves well to parallel network designs. Building on recent advancements in ?serverless? cloud functions, we designed a number of what we call ?Burst-Parallel? applications. These applications harness hundreds or thousands of serverless functions to break work into small components which can be executed in parallel. In addition to improving performance, these parallel threads layer on the underlying parallel network design, ensuring that the large number of paths are utilized by the parallel serverless threads.</p> <p>In all, this project has resulted in a number of software prototypes, a simulation platform for evaluating heterogeneous and homogeneous parallel network fabrics, and both FPGA- and DPDK-based endhost software NIC designs. This grant has supported the integration of four underrepresented minority (URM) directly into the above research efforts through UC San Diego?s Early Research Scholar?s Program (ERSP). These undergraduates gained first-hand experience with the research project and contributed to the development of burst-parallel applications.</p> <p>&nbsp;</p> <p>[1] S. Han, K. Jang, A. Panda, S. Palkar, D. Han, and S. Ratnasamy. Softnic: A software NIC to augment hardware. Dept. EECS, Univ. California, Berkeley, Berkeley, CA, USA, Tech. Rep. UCB/EECS-2015-155, 2015.</p> <p><br /> [2] Wischik, Damon, Costin Raiciu, Adam Greenhalgh, and Mark Handley. "Design, Implementation and Evaluation of Congestion Control for Multipath TCP." In NSDI, 2011.</p><br> <p>            Last Modified: 02/22/2021<br>      Modified by: George&nbsp;Porter</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1564185/1564185_10449487_1614051154788_diagram.fat-tree.multi-channel--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1564185/1564185_10449487_1614051154788_diagram.fat-tree.multi-channel--rgov-800width.jpg" title="Parallel Datacenter Network"><img src="/por/images/Reports/POR/2021/1564185/1564185_10449487_1614051154788_diagram.fat-tree.multi-channel--rgov-66x44.jpg" alt="Parallel Datacenter Network"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An example of a parallel datacenter network consisting of homogeneous FatTree (sub)-networks</div> <div class="imageCredit">Max Mellette</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">George&nbsp;Porter</div> <div class="imageTitle">Parallel Datacenter Network</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Processing vast amounts of data requires harnessing a large number of compute, storage, and accelerator functions so that they operate as a single unit. While scaling individual servers and storage devices is simple, interconnecting them is increasingly hard, placing new challenges on the underlying network fabric. Housed in large-scale Internet ?datacenters?, these large networks push the limits of commercial packet switches. Because of limitations in the ability of these packet switches to scale to increasing fast link rates, operators have had to construct multi-chip chassis which serve to deliver what is in effect a high-radix, high-bandwidth switch platform, albeit at high cost and power demands. The goal of this project has been to dramatically reduce the cost, energy, and complexity of high-speed datacenter networks by instead restructuring the network fabrics as entirely parallel designs. Rather than relying on multi-chip chassis to implement the abstraction of a single link, instead we expose the inherent parallelism of the underlying packet switch chips, much like modern PCs expose the inherent parallelism of multi-core CPUs to the operating system and application programs. Furthermore, by scaling network bandwidth through parallelism, we permit novel designs that compose entirely homogeneous parallel networks, entirely heterogeneous parallel networks, or interestingly a combination of networks into a single logical, though physically parallel, network interconnect. The end result is a network capable of higher bandwidth than existing networks at lower power and cost, due primarily to requiring fewer merchant silicon packet switch chips per endhost.  The goal of this project is to support parallel endhost network stacks, network interfaces, and transport protocols that support disjoint parallel dataplanes. We began by evaluating the performance of Intel?s DPDK userlevel network stack for injecting packets into each of the dataplanes, acting as a software NIC device. In particular, we focused on the effect of the memory hierarchy on software NIC performance. Focusing on the BESS[1] software NIC framework, we found that up to 10 or 40-Gb/s it performed well, but past then the packet rate was too high to entirely control in software. Our next major investigation looked at the applicability of FPGA-based NICs to precise control over packet injection into parallel dataplanes. Here we found that the FPGA was capable, however careful attention has to be paid to where the source (or destination) data resides in the memory hierarchy. We have shown that careful use of Intel?s ISA-level cache control instructions are necessary to ensure that data is accessible at next-generation network speeds.  We next evaluated the choice of transport/routing protocol for selecting paths through the dataplanes, and placing packets onto each of those paths. What we found was that the standard ECMP-based path selection approach, common to folded-Clos topologies, resulted in lower-than-expected performance when considering parallel homogeneous folded-Clos networks. The reason for the low performance was the explosion of paths. Instead, we found that choosing a k-shortest paths approach along with the MPTCP[2] transport led to good performance on heterogeneous folded-Clos networks.  Once ECMP is eschewed, it opens up new possibilities for implementing the parallel dataplanes with heterogeneous networks. We found that Expander-graph networks have good properties, since their random design allows for multiple ?instantiations? of each Expander. The end result is that endhosts have the ability to choose the shortest path not just within one network, but across network planes. The end result is a very low-latency network interconnect.  To demonstrate these research designs, we next focused on application design, in particular designs that would lend themselves well to parallel network designs. Building on recent advancements in ?serverless? cloud functions, we designed a number of what we call ?Burst-Parallel? applications. These applications harness hundreds or thousands of serverless functions to break work into small components which can be executed in parallel. In addition to improving performance, these parallel threads layer on the underlying parallel network design, ensuring that the large number of paths are utilized by the parallel serverless threads.  In all, this project has resulted in a number of software prototypes, a simulation platform for evaluating heterogeneous and homogeneous parallel network fabrics, and both FPGA- and DPDK-based endhost software NIC designs. This grant has supported the integration of four underrepresented minority (URM) directly into the above research efforts through UC San Diego?s Early Research Scholar?s Program (ERSP). These undergraduates gained first-hand experience with the research project and contributed to the development of burst-parallel applications.     [1] S. Han, K. Jang, A. Panda, S. Palkar, D. Han, and S. Ratnasamy. Softnic: A software NIC to augment hardware. Dept. EECS, Univ. California, Berkeley, Berkeley, CA, USA, Tech. Rep. UCB/EECS-2015-155, 2015.    [2] Wischik, Damon, Costin Raiciu, Adam Greenhalgh, and Mark Handley. "Design, Implementation and Evaluation of Congestion Control for Multipath TCP." In NSDI, 2011.       Last Modified: 02/22/2021       Submitted by: George Porter]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
