<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Creating Speech Synthesizers for Low Resource Languages</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recent advances in speech technology have resulted in wide use of Spoken Dialogue Systems (SDS) such as Siri (iPhone) and Voice Search (Android).  These systems support major improvements in information access by voice for High Resource Languages  (HRLS) such as English, French, Mandarin, Japanese, and Spanish.  For these HRLs, researchers have built dictionaries, parsers, part-of-speech taggers, language models, search engines, and machine translation engines to support speech technologies.  However, there are ~6500 world languages, including Tagalog, Tamil, Swahili, Vietnamese and Pashto, many of which are spoken by millions of people, but which do not enjoy the computational resources necessary to build SDS. These are termed Low Resource Languages (LRLs).  Speakers of LRLs do not enjoy the same communication and search capabilities speakers of HRLs do.   In particular, there is little research and few resources supporting the development of Text-to-Speech Synthesis (TTS) systems to produce Siri-like speech for SDS in these languages.&lt;br/&gt;&lt;br/&gt;New paradigms for TTS synthesis are now being developed which make it theoretically possible to build systems quickly and cheaply without recording large, special-purpose speech corpora using data recorded for other purposes such as training speech recognizers.  This EArly Grant for Exploratory Research investigates the use of these techniques to produce TTS systems for LRL.   Three major problems will be explored:  1)  Can one develop automatic techniques to filter found data (removing data that is too loud, too noisy or disfluent, for example) to obtain intelligible and natural-sounding results? 2) Can one obtain pronunciation dictionaries from online sources that, with crowd-sourced validation, suffice to generate intelligible and natural speech?  3) Can one use clustering techniques on found data to identify pitch contours that can be crowd-sourced to identify meanings such as question vs. statement contours without prior knowledge of a language's phonology?  These methods are tested on two languages:  Standard American English, to develop the techniques rapidly, and a language similar in writing system and phonology, Lithuanian, to evaluate on an initial LRL. Both evaluations are made in terms of intelligibility and naturalness using crowd-sourcing techniques with native speakers of each language.  The ultimate goal of this exploratory work will be to test these techniques on a broad variety of LRLs which have been collected for purposes of developing speech recognizers.</AbstractNarration>
<MinAmdLetterDate>07/09/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1548092</AwardID>
<Investigator>
<FirstName>Julia</FirstName>
<LastName>Hirschberg</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Julia B Hirschberg</PI_FULL_NAME>
<EmailAddress>julia@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397114</PI_PHON>
<NSF_ID>000399629</NSF_ID>
<StartDate>07/09/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In "Creating Speech Synthesizers for Low Resource Languages" we developed tools and methods for creating Text-to-Speech Synthesis (TTS) for Low Resource Languages (LRLs), languages which lack the computational tools needed for creating parsers, part-of-speech taggers, morphological analyzers and other analyzers needed for building machine translation, speech recognition, and TTS systems. &nbsp;Such LRLs include inter alia&nbsp;Swahili, Tagalog, Mongolian, Amharic, Dholou, Igbo, Paraguayan Guarani, Halh Mongolian, Javanese, Pashto, Telegu, Kurmanji Kurdish,Tok Pisin, Kazakh, Zulu, and Lithuanian, many of which have millions of native speakers who lack the means of communicating successfully on digital devices in their own language.</p> <p>Most LRLs in particular lack TTS systems which are required for building Spoken Dialogue Systems such as those used on smart phones and virtual assistants. &nbsp;One reason for this lack of tools in general and TTS systems in particular is the expense required to build them; a new TTS voice, for example, costs industries at least $1M to produce in terms of carefully recorded data collection typically from professional speakers, hand annotation, and salaries to support software production.</p> <p>Our goal in this research has been to develop tools and processes for developing TTS systems in LRLs from "found" data collected for non-TTS-specific purposes such as Automatic Speech Recognition (ASR) or freely available on the web as audiobooks, news broadcasts, online talks and presentations, and language teaching websites. &nbsp;Part of our task has been to find and document such resources for LRLs, which we have done we considerable success. &nbsp;Using these resources we have produced TTS systems using free software system such as Edinburgh's Festival and Merlin libraries and developed many additional tools to prepare the data for these systems. &nbsp;To develop our techniques for preparing "found" data for synthesis we conducted hundreds of experiments, first on English sources and then on LRLs such as Amharic and Turkish to see whether methods developed for English generalize. &nbsp;Methods tested have included different types of data selection from found corpora such as 1) "noise" removal (non-human noise, filled pauses (e.g. &ldquo;um&rdquo;), self-repairs), 2) acoustic feature selection (pitch, loudness, voice quality measures, speaking rate), 3) outlier removal of (2), 4) gender-based voice selection. We have also experimented with size of corpus used, with adaptation of one genre of "found" data with another, such as large corpora of telephone conversation with less &ldquo;noisy&rdquo; news broadcasts. &nbsp;Finally, we have compared selection based on utterance regardless of speaker with speaker-based utterance selection for corpora containing a large number of speakers. &nbsp;</p> <p>We evaluated the hundreds of systems we have built in two ways: &nbsp;objective measures such as ASR word error rate (WER) and mel-cepstral distance (MCD) and crowd-sourced nonsense-sentence transcription (which strongly correlated with our objective measures) to assess intelligibility and crowd-sourced human judgments of naturalness comparing a voice trained on a selected subset of a corpus vs. the entire corpus to identifying best features selection approaches.&nbsp;&nbsp;From these evaluations we have learned the following general strategies: &nbsp;First, we have learned which types of features generally tend to produce better voices.&nbsp;&nbsp;These include pitch, speaking rate, and hypo-articulation (less carefully pronounced).&nbsp;We have also found that using high, medium, low ranges for acoustic features and selecting these at the synthesis front-end improves over simply using these features to select data for synthesis. In addition, we have found that speaker-based selection produces more intelligible voices than data selected simply at the utterance level, independent of speaker. &nbsp; While we have developed these techniques on English, we are now testing them on LRLs including Amharic and Turkish.&nbsp;&nbsp;</p> <p>Another goal in our research has been to discover which types of &ldquo;found&rdquo; speech data are closer to TTS-specific recordings than others.&nbsp;&nbsp;In these comparisons we have tested many of the same things we studied in our TTS selection experiments, especially focusing on acoustic features of pitch, speaking rate, intensity and voice quality.&nbsp;&nbsp;We examine audiobooks, news broadcasts, telephone conversations in many languages and compared them acoustically to speech data recorded specifically for TTS creation.&nbsp;&nbsp;We found that both news broadcasts and audiobooks were most similar to TTS-recorded data, which is not surprising.&nbsp;&nbsp;Telephone speech varied in many ways.&nbsp;&nbsp;However, we also found that some aspects of TTS recordings did not in fact exhibit characteristics that are widely believed to characterize such recordings:&nbsp;&nbsp;in particular, having less deviation in pitch appears less important in TTS data, so finding corpora with this characteristic may not be as important as one might imagine.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/04/2018<br>      Modified by: Julia&nbsp;B&nbsp;Hirschberg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In "Creating Speech Synthesizers for Low Resource Languages" we developed tools and methods for creating Text-to-Speech Synthesis (TTS) for Low Resource Languages (LRLs), languages which lack the computational tools needed for creating parsers, part-of-speech taggers, morphological analyzers and other analyzers needed for building machine translation, speech recognition, and TTS systems.  Such LRLs include inter alia Swahili, Tagalog, Mongolian, Amharic, Dholou, Igbo, Paraguayan Guarani, Halh Mongolian, Javanese, Pashto, Telegu, Kurmanji Kurdish,Tok Pisin, Kazakh, Zulu, and Lithuanian, many of which have millions of native speakers who lack the means of communicating successfully on digital devices in their own language.  Most LRLs in particular lack TTS systems which are required for building Spoken Dialogue Systems such as those used on smart phones and virtual assistants.  One reason for this lack of tools in general and TTS systems in particular is the expense required to build them; a new TTS voice, for example, costs industries at least $1M to produce in terms of carefully recorded data collection typically from professional speakers, hand annotation, and salaries to support software production.  Our goal in this research has been to develop tools and processes for developing TTS systems in LRLs from "found" data collected for non-TTS-specific purposes such as Automatic Speech Recognition (ASR) or freely available on the web as audiobooks, news broadcasts, online talks and presentations, and language teaching websites.  Part of our task has been to find and document such resources for LRLs, which we have done we considerable success.  Using these resources we have produced TTS systems using free software system such as Edinburgh's Festival and Merlin libraries and developed many additional tools to prepare the data for these systems.  To develop our techniques for preparing "found" data for synthesis we conducted hundreds of experiments, first on English sources and then on LRLs such as Amharic and Turkish to see whether methods developed for English generalize.  Methods tested have included different types of data selection from found corpora such as 1) "noise" removal (non-human noise, filled pauses (e.g. "um"), self-repairs), 2) acoustic feature selection (pitch, loudness, voice quality measures, speaking rate), 3) outlier removal of (2), 4) gender-based voice selection. We have also experimented with size of corpus used, with adaptation of one genre of "found" data with another, such as large corpora of telephone conversation with less "noisy" news broadcasts.  Finally, we have compared selection based on utterance regardless of speaker with speaker-based utterance selection for corpora containing a large number of speakers.    We evaluated the hundreds of systems we have built in two ways:  objective measures such as ASR word error rate (WER) and mel-cepstral distance (MCD) and crowd-sourced nonsense-sentence transcription (which strongly correlated with our objective measures) to assess intelligibility and crowd-sourced human judgments of naturalness comparing a voice trained on a selected subset of a corpus vs. the entire corpus to identifying best features selection approaches.  From these evaluations we have learned the following general strategies:  First, we have learned which types of features generally tend to produce better voices.  These include pitch, speaking rate, and hypo-articulation (less carefully pronounced). We have also found that using high, medium, low ranges for acoustic features and selecting these at the synthesis front-end improves over simply using these features to select data for synthesis. In addition, we have found that speaker-based selection produces more intelligible voices than data selected simply at the utterance level, independent of speaker.   While we have developed these techniques on English, we are now testing them on LRLs including Amharic and Turkish.    Another goal in our research has been to discover which types of "found" speech data are closer to TTS-specific recordings than others.  In these comparisons we have tested many of the same things we studied in our TTS selection experiments, especially focusing on acoustic features of pitch, speaking rate, intensity and voice quality.  We examine audiobooks, news broadcasts, telephone conversations in many languages and compared them acoustically to speech data recorded specifically for TTS creation.  We found that both news broadcasts and audiobooks were most similar to TTS-recorded data, which is not surprising.  Telephone speech varied in many ways.  However, we also found that some aspects of TTS recordings did not in fact exhibit characteristics that are widely believed to characterize such recordings:  in particular, having less deviation in pitch appears less important in TTS data, so finding corpora with this characteristic may not be as important as one might imagine.             Last Modified: 09/04/2018       Submitted by: Julia B Hirschberg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
