<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Locality-Aware Concurrency Platforms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>449947.00</AwardTotalIntnAmount>
<AwardAmount>481947</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Title: SHF: Small: Locality-Aware Concurrency Platforms&lt;br/&gt;&lt;br/&gt;Modern machines have complex memory hierarchies consisting of many levels of "cache" that must be utilized effectively to gain performance.  A program is said to have good locality or cache efficiency if its execution can utilize the underlying cache memory hierarchy effectively.  Since the speed of processors is growing faster than the memory access latency, it is often more important to optimize one's program for locality than to minimize the number of instructions executed on the processor. With the prevalence of "multicore" systems, optimizing for locality becomes even more critical, since increase in the number of cores on a processor puts pressure on both the memory bandwidth and capacity available for each core. Writing parallel programs that utilize the cache hierarchy effectively for existing multicore machines is challenging, however. This project aims to develop a parallel programming platform that enables the programmer to program a parallel machine without worrying about locality, and the platform transforms and executes the program in a cache-efficient way.  In terms of intellectual merits, this research will advance the understanding of scheduling algorithms, compiler transformations, and runtime automation. The project's broader importance is that programmers will be able to write portable, high-performant and cache-efficient programs for multicore systems while significantly reducing the programming effort compared to the traditional approach in writing cache-efficient code. The platform produced by this project will be made freely available on the World Wide Web, which can enable highly-efficient software.&lt;br/&gt;&lt;br/&gt;Writing cache-efficient parallel programs is challenging, because locality in a parallel program is a function of algorithm design, scheduling, and underlying machine configuration, and these factors are often tightly coupled. In addition, on parallel platforms, the performance depends on both load-balancing and cache efficiency, and these are often competing objectives. To design a locality-aware parallel programming platform, the project will take an integrated approach that combines efforts in scheduling theory, algorithmic design, runtime system support, and compiler transformations.  In the realm of scheduling theory, the PIs will systematically study the trade-offs between load-balancing and cache efficiency, and design schedulers that provide sensible guarantees for both. Since cache-efficiency also depends on the program itself, the PIs will study the design patterns of cache-efficient algorithms and draw on this experience to investigate a set of program transformations to convert an ordinary program into one that can be executed in a cache efficient manner.  These insights will be implemented in a prototype concurrency platform consisting of a compiler and a runtime system in order to study efficient mechanisms to support provably good policies developed for scheduling parallel programs in a cache-efficient manner.</AbstractNarration>
<MinAmdLetterDate>06/17/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/20/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1527692</AwardID>
<Investigator>
<FirstName>Kunal</FirstName>
<LastName>Agrawal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kunal Agrawal</PI_FULL_NAME>
<EmailAddress>kunal@cse.wustl.edu</EmailAddress>
<PI_PHON>3149354838</PI_PHON>
<NSF_ID>000555177</NSF_ID>
<StartDate>06/17/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>I-Ting</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>I-Ting A Lee</PI_FULL_NAME>
<EmailAddress>angelee@wustl.edu</EmailAddress>
<PI_PHON>3149354621</PI_PHON>
<NSF_ID>000678883</NSF_ID>
<StartDate>06/17/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Washington University</Name>
<CityName>Saint Louis</CityName>
<ZipCode>631304862</ZipCode>
<PhoneNumber>3147474134</PhoneNumber>
<StreetAddress>CAMPUS BOX 1054</StreetAddress>
<StreetAddress2><![CDATA[1 Brookings Drive]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<StateCode>MO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MO01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>068552207</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068552207</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Washington University]]></Name>
<CityName/>
<StateCode>MO</StateCode>
<ZipCode>631304899</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~449947</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<FUND_OBLG>2018~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1"><span class="s1">Modern machines have complex memory hierarchies consisting of many levels of&nbsp;</span>cache and main memory that must be utilized effectively to gain performance.&nbsp; A program is said to have good locality if its execution can utilize the underlying memory hierarchy effectively.&nbsp; With the prevalence of multicore systems, optimizing for locality becomes even more critical, since increase in the number of cores on a processor puts pressure on both the memory bandwidth and capacity available for each core.</p> <p class="p1"><span class="s1">Writing parallel programs that utilize the cache hierarchy effectively for&nbsp;</span>existing multicore machines is challenging, however, because locality in a parallel program is a function of algorithm design, scheduling, and underlying machine configuration, and these factors are often tightly coupled.&nbsp; This project aims to design and develop a parallel programming platform that enables the programmer to easily write parallel programs that can utilize underlying memory hierarchy efficiently.&nbsp;</p> <p class="p1"><span class="s1">We have designed provably efficient runtime schedulers for scheduling&nbsp;</span>task-parallel computations where the scheduler employs heuristics to improve efficient utilization of the underlying memory hierarchy.&nbsp; In this work, we focus on task-parallel code, a parallel programming paradigm designed to simplify the task of programming multicore machines.&nbsp; In task-parallel code, the programmer expresses the logical parallelism of the computation, and let an underlying runtime scheduler automates load balancing and synchronization. This automation is a double-edged sword, however, because automation can lead to scheduling decisions that are sub-optimal with respect to locality, but the programmer has little control over it.&nbsp; Moreover, a trade-off exists between load-balancing and memory efficiency, and obtaining efficiency in both criteria can be challenging for general computations.</p> <p class="p1"><span class="s1">This project has focused on designing provably efficient schedulers for two&nbsp;</span>different types of computations: 1) recursive divide-and-conquer task-parallel code where the data accessed by tasks are also recursively subdivided at each level of recursions, and 2) iterative data-parallel loops where the data accessed don't recursively subdivide but the sequence of parallel loops tend to access the same data over and over.&nbsp; We have developed schedulers that can exploit the locality in these two types of computations in such a way that incurs little scheduling overhead and provides guarantees on efficient execution times.&nbsp; &nbsp;</p> <p class="p1"><span class="s1">In terms of intellectual merits, the research conducted has advanced the&nbsp;</span>understanding of scheduling algorithms and runtime automation.&nbsp; The project's broader impact in improving programmer productivity and increasing efficiency of hardware utilization.&nbsp; Using the platforms developed, the programmer can write portable and performant task-parallel programs for multicore systems where the runtime automates scheduling in such a way that accounts for locality.&nbsp; All software and publications resulted from the project have been made freely available via world wide web. The knowledge acquired during the duration of project has been incorporated into PI's graduate-level course, which allows for broader dissemination of the knowledge.</p><br> <p>            Last Modified: 10/29/2020<br>      Modified by: I-Ting&nbsp;A&nbsp;Lee</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Modern machines have complex memory hierarchies consisting of many levels of cache and main memory that must be utilized effectively to gain performance.  A program is said to have good locality if its execution can utilize the underlying memory hierarchy effectively.  With the prevalence of multicore systems, optimizing for locality becomes even more critical, since increase in the number of cores on a processor puts pressure on both the memory bandwidth and capacity available for each core. Writing parallel programs that utilize the cache hierarchy effectively for existing multicore machines is challenging, however, because locality in a parallel program is a function of algorithm design, scheduling, and underlying machine configuration, and these factors are often tightly coupled.  This project aims to design and develop a parallel programming platform that enables the programmer to easily write parallel programs that can utilize underlying memory hierarchy efficiently.  We have designed provably efficient runtime schedulers for scheduling task-parallel computations where the scheduler employs heuristics to improve efficient utilization of the underlying memory hierarchy.  In this work, we focus on task-parallel code, a parallel programming paradigm designed to simplify the task of programming multicore machines.  In task-parallel code, the programmer expresses the logical parallelism of the computation, and let an underlying runtime scheduler automates load balancing and synchronization. This automation is a double-edged sword, however, because automation can lead to scheduling decisions that are sub-optimal with respect to locality, but the programmer has little control over it.  Moreover, a trade-off exists between load-balancing and memory efficiency, and obtaining efficiency in both criteria can be challenging for general computations. This project has focused on designing provably efficient schedulers for two different types of computations: 1) recursive divide-and-conquer task-parallel code where the data accessed by tasks are also recursively subdivided at each level of recursions, and 2) iterative data-parallel loops where the data accessed don't recursively subdivide but the sequence of parallel loops tend to access the same data over and over.  We have developed schedulers that can exploit the locality in these two types of computations in such a way that incurs little scheduling overhead and provides guarantees on efficient execution times.    In terms of intellectual merits, the research conducted has advanced the understanding of scheduling algorithms and runtime automation.  The project's broader impact in improving programmer productivity and increasing efficiency of hardware utilization.  Using the platforms developed, the programmer can write portable and performant task-parallel programs for multicore systems where the runtime automates scheduling in such a way that accounts for locality.  All software and publications resulted from the project have been made freely available via world wide web. The knowledge acquired during the duration of project has been incorporated into PI's graduate-level course, which allows for broader dissemination of the knowledge.       Last Modified: 10/29/2020       Submitted by: I-Ting A Lee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
