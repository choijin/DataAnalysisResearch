<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>PFI:AIR-TT:  Kinect-Based Low-Cost Motion Analysis System for Medical Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>199888.00</AwardTotalIntnAmount>
<AwardAmount>199888</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jesus Soriano Molla</SignBlockName>
<PO_EMAI>jsoriano@nsf.gov</PO_EMAI>
<PO_PHON>7032927795</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This PFI: AIR Technology Translation project focuses on translating innovative single-view motion capture technology to fill the need of medical professionals to quantify a patient?s body movement.   This single-view motion capture system is important because it will be a transformational technological improvement that is available to physicians and physical therapists to diagnosis, treat, and monitor patients with a movement disorder. The project will result in a prototype system that will provide a live demonstration to potential customers and investors. This prototype system will consist of a single Kinect camera and a laptop. It will acquire motion data, process the data close to real-time, and provide visual feedback to the patients as well as the clinicians. In particular the performance measures for the clinicians will be anatomically correct and accurate so progress in treating movement dysfunction can be quantified.  Compared to the leading motion capture system in the market place today, this project will lead to a portable, low-cost single-camera system that will give medical professionals accurate and precise 3D measurements of movement dysfunction in any environment. &lt;br/&gt;&lt;br/&gt;This project addresses the following technology gaps as it translates from research discovery toward commercial application. The first and foremost is the need to automatically identify anthropometric landmarks in the input data to define an anatomically correct coordinate system. A novel data-driven method is to going to be investigated. The second is the need to achieve faster processing speed and consolidate the processing pipeline. Simplified tracking algorithm and code optimization will be implemented to achieve the required real-time performance. In addition, personnel involved in this project, in particular both graduate and undergraduate students, will not only receive innovation experiences throughout the research and development cycles, but also cultivate their entrepreneurship through the business development activities.</AbstractNarration>
<MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>02/06/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1543172</AwardID>
<Investigator>
<FirstName>Ruigang</FirstName>
<LastName>Yang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ruigang Yang</PI_FULL_NAME>
<EmailAddress>ryang@cs.uky.edu</EmailAddress>
<PI_PHON>8592573886</PI_PHON>
<NSF_ID>000129435</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Noehren</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian W Noehren</PI_FULL_NAME>
<EmailAddress>b.noehren@uky.edu</EmailAddress>
<PI_PHON>8592579420</PI_PHON>
<NSF_ID>000541946</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Kentucky Research Foundation</Name>
<CityName>Lexington</CityName>
<ZipCode>405260001</ZipCode>
<PhoneNumber>8592579420</PhoneNumber>
<StreetAddress>109 Kinkead Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<StateCode>KY</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>KY06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>939017877</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF KENTUCKY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007400724</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Kentucky Research Foundation]]></Name>
<CityName>Lexington</CityName>
<StateCode>KY</StateCode>
<ZipCode>405260001</ZipCode>
<StreetAddress><![CDATA[500 S Limestone 109 Kinkead Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>KY06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8019</Code>
<Text>Accelerating Innovation Rsrch</Text>
</ProgramElement>
<ProgramReference>
<Code>8019</Code>
<Text>Accelerating Innovation Rsrch</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~199888</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project aims to fully develop a working prototype and leverages the state-of-the-art motion tracking algorithms that uses only a single depth camera to capture <strong>clinically critical and anatomically correct motion trajectory </strong>of human motion, in particular for the lower body movement. Different from these motion tracking systems that have been used in computer games, the emphasis of this project is accuracy. We have developed algorithms that are over 10 times more accurate than previous state of the art. The outcomes can be summarized from the two aspects listed below.<strong>&nbsp;</strong></p> <p><strong>Intellectual Merit:</strong></p> <p>Through the course of the project, we have developed and evaluated a number of approaches, from using a personalized template for high-accuracy tracking, dual-depth camera setup, to data-driven tracking methods. In the end, we have settled on a hybrid tracking method that uses data-driven approach (e.g., deep-learning based) to initialize the pose, followed by a geometry-driven refinement approach. Based on this approach, we have developed a novel system that combines the automatically identified key marker points from color image and the depth information to recover motion trajectory of human subjects, and by leveraging high throughput of GPUs to achieve real-time tracking. With our new system we can provide pose information of a human subject to the users such as physical therapists. <strong>&nbsp;</strong></p> <p><strong>Broader Impacts:</strong></p> <p>The software tools we have developed allow automatic identification of anthropometric landmarks and interactive adjustment, as well as provide live pose data of a human subject with a single setup. This can be used by clinicians who is interested in human body and pose measurement. These will provide valuable aid and convenience for health science professionals in physical therapy and many other fields.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/23/2018<br>      Modified by: Ruigang&nbsp;Yang</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1543172/1543172_10394140_1545584235123_title-pic1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1543172/1543172_10394140_1545584235123_title-pic1--rgov-800width.jpg" title="Single-view Pose Estimation"><img src="/por/images/Reports/POR/2018/1543172/1543172_10394140_1545584235123_title-pic1--rgov-66x44.jpg" alt="Single-view Pose Estimation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">(Left) pose estimated by the  Kinect system; (middle) joints detected by our method; and (right) the refined 3D mesh with skeleton overlay.</div> <div class="imageCredit">University of Kentucky</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ruigang&nbsp;Yang</div> <div class="imageTitle">Single-view Pose Estimation</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project aims to fully develop a working prototype and leverages the state-of-the-art motion tracking algorithms that uses only a single depth camera to capture clinically critical and anatomically correct motion trajectory of human motion, in particular for the lower body movement. Different from these motion tracking systems that have been used in computer games, the emphasis of this project is accuracy. We have developed algorithms that are over 10 times more accurate than previous state of the art. The outcomes can be summarized from the two aspects listed below.   Intellectual Merit:  Through the course of the project, we have developed and evaluated a number of approaches, from using a personalized template for high-accuracy tracking, dual-depth camera setup, to data-driven tracking methods. In the end, we have settled on a hybrid tracking method that uses data-driven approach (e.g., deep-learning based) to initialize the pose, followed by a geometry-driven refinement approach. Based on this approach, we have developed a novel system that combines the automatically identified key marker points from color image and the depth information to recover motion trajectory of human subjects, and by leveraging high throughput of GPUs to achieve real-time tracking. With our new system we can provide pose information of a human subject to the users such as physical therapists.    Broader Impacts:  The software tools we have developed allow automatic identification of anthropometric landmarks and interactive adjustment, as well as provide live pose data of a human subject with a single setup. This can be used by clinicians who is interested in human body and pose measurement. These will provide valuable aid and convenience for health science professionals in physical therapy and many other fields.          Last Modified: 12/23/2018       Submitted by: Ruigang Yang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
