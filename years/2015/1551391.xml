<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Toward Descriptive Mapping for Underwater Exploration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>94995.00</AwardTotalIntnAmount>
<AwardAmount>94995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Reid Simmons</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project will provide a new solution for 3D mapping and exploration of underwater environments characterized by sparse and noisy data. It will enhance the capability of underwater robots to autonomously map, navigate, and inspect a previously unknown shallow-water environment. The development of this project will benefit from its long association with United States Coast Guard.&lt;br/&gt;&lt;br/&gt;This project will advance the state of the art in three-dimensional occupancy mapping using Gaussian process regression, a supervised learning method with great promise for its application to robot mapping. This project will apply these methods to online 3D mapping with a field robot, mapping underwater structures with a scanning sonar.</AbstractNarration>
<MinAmdLetterDate>08/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1551391</AwardID>
<Investigator>
<FirstName>Brendan</FirstName>
<LastName>Englot</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brendan Englot</PI_FULL_NAME>
<EmailAddress>brendan.englot@stevens.edu</EmailAddress>
<PI_PHON>2012168762</PI_PHON>
<NSF_ID>000690515</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stevens Institute of Technology</Name>
<CityName>HOBOKEN</CityName>
<ZipCode>070305991</ZipCode>
<PhoneNumber>2012168762</PhoneNumber>
<StreetAddress>CASTLE POINT ON HUDSON</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>064271570</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>STEVENS INSTITUTE OF TECHNOLOGY (INC)</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>064271570</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stevens Institute of Technology]]></Name>
<CityName>Hoboken</CityName>
<StateCode>NJ</StateCode>
<ZipCode>070305991</ZipCode>
<StreetAddress><![CDATA[Castle Point on Hudson]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~94995</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The robot sensing technologies currently available to map the underwater environment (especially in cloudy water, over long distances) are not as fast, dense or precise as the camera and laser-based technologies that can map the environment on the ground and in the air. The goal of this project was to design, implement, and test 3D mapping techniques that can make predictions using only an incomplete, sparse, and noisy picture of the environment provided by underwater sonar data. The techniques incorporate mathematical tools from supervised machine learning to infer the contents of an entire grid-based map using only a sparse set of "training data" that offers a partial view of the environment's contents. An example of the training data itself (a sonar-derived point cloud), an example of a standard grid-based map populated using this data, and an example of a learning-enabled map using the products of this research project are shown in the attached figure, where they are used to map complex underwater structures. A key step forward enabled by this research was to produce three different classes of learning-aided mapping algorithms capable of accurate, predictive 3D mapping that can be updated in real-time, one scan at a time.</p> <p>The mapping algorithms have been described and documented in a series of published papers, and their implementations have been made freely available for use by the public in the Learning-Aided 3D Mapping Library (LA3DM). Our long-term goal is for learning-aided maps of this nature to aid autonomous robots in decision-making that governs how they can explore complex, unknown 3D environments when a prior map is not available. Such maps will hopefully provide a more descriptive basis for decision-making about where to explore next, and also to offer a more accurate and predictive picture of where obstacles are located in the environment, even when a robot has not observed the surrounding obstacles in their entirety.</p><br> <p>            Last Modified: 10/31/2017<br>      Modified by: Brendan&nbsp;Englot</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1551391/1551391_10389209_1509468314731_Englot_EAGER_Outcomes_Figure--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1551391/1551391_10389209_1509468314731_Englot_EAGER_Outcomes_Figure--rgov-800width.jpg" title="Learning-Aided 3D Mapping Applied to Underwater Sonar Point Clouds"><img src="/por/images/Reports/POR/2017/1551391/1551391_10389209_1509468314731_Englot_EAGER_Outcomes_Figure--rgov-66x44.jpg" alt="Learning-Aided 3D Mapping Applied to Underwater Sonar Point Clouds"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Underwater sonar mapping results from raw point clouds (left), OctoMaps (center) and GP occupancy maps (right) using data collected at a seawall (top: a-c) and a pier (bottom: d-f). Raw point clouds are visualized from above, and occupancy maps are shown from an isometric view.</div> <div class="imageCredit">Jinkun Wang</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Brendan&nbsp;Englot</div> <div class="imageTitle">Learning-Aided 3D Mapping Applied to Underwater Sonar Point Clouds</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The robot sensing technologies currently available to map the underwater environment (especially in cloudy water, over long distances) are not as fast, dense or precise as the camera and laser-based technologies that can map the environment on the ground and in the air. The goal of this project was to design, implement, and test 3D mapping techniques that can make predictions using only an incomplete, sparse, and noisy picture of the environment provided by underwater sonar data. The techniques incorporate mathematical tools from supervised machine learning to infer the contents of an entire grid-based map using only a sparse set of "training data" that offers a partial view of the environment's contents. An example of the training data itself (a sonar-derived point cloud), an example of a standard grid-based map populated using this data, and an example of a learning-enabled map using the products of this research project are shown in the attached figure, where they are used to map complex underwater structures. A key step forward enabled by this research was to produce three different classes of learning-aided mapping algorithms capable of accurate, predictive 3D mapping that can be updated in real-time, one scan at a time.  The mapping algorithms have been described and documented in a series of published papers, and their implementations have been made freely available for use by the public in the Learning-Aided 3D Mapping Library (LA3DM). Our long-term goal is for learning-aided maps of this nature to aid autonomous robots in decision-making that governs how they can explore complex, unknown 3D environments when a prior map is not available. Such maps will hopefully provide a more descriptive basis for decision-making about where to explore next, and also to offer a more accurate and predictive picture of where obstacles are located in the environment, even when a robot has not observed the surrounding obstacles in their entirety.       Last Modified: 10/31/2017       Submitted by: Brendan Englot]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
