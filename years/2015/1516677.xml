<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Variational Problems on Random Structures: Analysis and Applications to Data Science</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>181107.00</AwardTotalIntnAmount>
<AwardAmount>181107</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Victor Roytburd</SignBlockName>
<PO_EMAI>vroytbur@nsf.gov</PO_EMAI>
<PO_PHON>7032928584</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project is concerned with applying modern tools from mathematical analysis to the study of currently important topics in data science, including massive data analysis (data clouds) and machine learning.  Modern data-acquisition techniques produce a wealth of data about the world we live in.  Extracting the information from the data leads to machine learning/statistics tasks such as clustering, classification, low-dimensional embedding, and others.  This project introduces new mathematical tools for understanding of some of the state-of-art approaches to important data analysis tasks.  The conclusions gained will improve their reliability, robustness, speed, and scalability.  The insights of the analysis are expected to lay the foundation to create new models for data analysis and new approaches to the pertinent tasks.&lt;br/&gt;&lt;br/&gt;This project will investigate variational problems that arise in data analysis and machine learning.  It will do so by considering variational descriptions of these problems in which the answer is obtained by minimizing an objective functional.  The project will develop a mathematical framework suitable for studies of asymptotic properties of variational problems posed on random samples and related random geometries.  In particular, it will investigate the relationship between variational problems on random discrete structures, such as a neighborhood graph of a data cloud, and continuum variational problems.  It combines the techniques of calculus of variations, applied analysis, optimal transportation, probability, and statistics to gain insights about discrete variational problems in a random setting.</AbstractNarration>
<MinAmdLetterDate>08/25/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1516677</AwardID>
<Investigator>
<FirstName>Dejan</FirstName>
<LastName>Slepcev</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dejan S Slepcev</PI_FULL_NAME>
<EmailAddress>slepcev@math.cmu.edu</EmailAddress>
<PI_PHON>4122682562</PI_PHON>
<NSF_ID>000332633</NSF_ID>
<StartDate>08/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue WQED Bldg.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1266</Code>
<Text>APPLIED MATHEMATICS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~181107</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project investigated variational problems that arise in data science. Modern data-acquisition techniques produce a wealth of data about the world we live in. Extracting the information from the data leads to machine learning tasks to cluster the data (group the data in meaningful groups), determine the class of new data points based on class labbels provided for training data, or recover a function on the data set based on its values at a potentially small number of data samples. These problems are often modeled by introducing an objective functional which measures the desired properties of the object sought. One then designs and implements algorithms to find the best object by minimizing the objective functional. The goal of the projects was to investigate how to design the models and algorithms so that the answer improves as the number of available sample points increases and furthermore converges to a desirable limit. A further goal was to use the tools of applied analysis and the insights obtained to design better models for some of the above tasks.&nbsp;</p> <p>The project significantly contributed to development of a mathematical framework suitable for studying asymptotic properties of variational problems posed on random samples and related random geometries. In particular it investigated the relationship between variational problems on random discrete structures, like a neighborhood graph of a data cloud, and continuum variational problems. For several frequently used functionals in machine learning (based on graph Laplacians, p-Laplacians and higher powers of the Laplacian) the project established precise conditions under which the minimizers of the discrete objective functionals, based on a random sample, approach the corresponding, ideal, continuum limit as the sample size increases. The results are optimal in terms of scaling. At the source of the many of such functionals is the graph Laplacian constructed on the finite sample. In particular many graph based descriptions rely on the spectrum (eigenvalues and eigenvectors) of the graph Laplacian. This project investigated how close are the spectra of the graph Laplacians to the spectra of the continuum Laplacian corresponding to having infinite amount of data. This is important in order to be able to estimate the error one can expect when dealing with finite data available. The principal investigator and the collaborators obtained precise quantitative estimates on the difference. These were the first quantitative estimates on the spectra between graph Laplacians in random setting relevant to machine learning and the Laplace-Beltrami operator, and have thus substantially advanced the state of knowledge in the field.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2019<br>      Modified by: Dejan&nbsp;S&nbsp;Slepcev</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project investigated variational problems that arise in data science. Modern data-acquisition techniques produce a wealth of data about the world we live in. Extracting the information from the data leads to machine learning tasks to cluster the data (group the data in meaningful groups), determine the class of new data points based on class labbels provided for training data, or recover a function on the data set based on its values at a potentially small number of data samples. These problems are often modeled by introducing an objective functional which measures the desired properties of the object sought. One then designs and implements algorithms to find the best object by minimizing the objective functional. The goal of the projects was to investigate how to design the models and algorithms so that the answer improves as the number of available sample points increases and furthermore converges to a desirable limit. A further goal was to use the tools of applied analysis and the insights obtained to design better models for some of the above tasks.   The project significantly contributed to development of a mathematical framework suitable for studying asymptotic properties of variational problems posed on random samples and related random geometries. In particular it investigated the relationship between variational problems on random discrete structures, like a neighborhood graph of a data cloud, and continuum variational problems. For several frequently used functionals in machine learning (based on graph Laplacians, p-Laplacians and higher powers of the Laplacian) the project established precise conditions under which the minimizers of the discrete objective functionals, based on a random sample, approach the corresponding, ideal, continuum limit as the sample size increases. The results are optimal in terms of scaling. At the source of the many of such functionals is the graph Laplacian constructed on the finite sample. In particular many graph based descriptions rely on the spectrum (eigenvalues and eigenvectors) of the graph Laplacian. This project investigated how close are the spectra of the graph Laplacians to the spectra of the continuum Laplacian corresponding to having infinite amount of data. This is important in order to be able to estimate the error one can expect when dealing with finite data available. The principal investigator and the collaborators obtained precise quantitative estimates on the difference. These were the first quantitative estimates on the spectra between graph Laplacians in random setting relevant to machine learning and the Laplace-Beltrami operator, and have thus substantially advanced the state of knowledge in the field.          Last Modified: 11/30/2019       Submitted by: Dejan S Slepcev]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
