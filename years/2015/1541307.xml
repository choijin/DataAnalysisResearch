<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC*DNI Campus Design: Building a State-Of-The-Art Research Network at Franklin and Marshall College</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>350000.00</AwardTotalIntnAmount>
<AwardAmount>350000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Science education at Franklin &amp; Marshall College prioritizes student-faculty research led by highly accomplished scholar-teachers. In embracing that role within the context of projects that increasingly involve large amounts of shared data, present local network infrastructure limitations emerge when such transfers compete with other campus network traffic and pass through campus firewalls. This project creates a local area network optimized for high-performance scientific applications, a Science DMZ, which will be architected with particular attention paid to identifying and sharing replicable techniques, while ensuring institutional information security requirements are not compromised. The addition of a local data transfer node, combined with increased local network capacity, improves Franklin &amp; Marshall's ability to exchange data with researchers at universities worldwide.&lt;br/&gt;&lt;br/&gt;This project expands end-to-end performance monitoring, improves inter-institutional data exchange capabilities, and strengthens current and future research support for faculty via collaboration with a Pennsylvania-wide education and research computing network (KINBER). The network infrastructure improvements also enable new experimentation with off-site high performance computing solutions that cannot presently be evaluated due to infrastructure constraints. This project prepares undergraduate students for postgraduate work and study in data-intensive fields. Franklin &amp; Marshall plans to share the results of the project with similar-sized institutions seeking to support their faculty and undergraduate researchers.</AbstractNarration>
<MinAmdLetterDate>08/12/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/12/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1541307</AwardID>
<Investigator>
<FirstName>Jaime</FirstName>
<LastName>Blair</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jaime Blair</PI_FULL_NAME>
<EmailAddress>jaime.blair@fandm.edu</EmailAddress>
<PI_PHON>7172913959</PI_PHON>
<NSF_ID>000509685</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christina</FirstName>
<LastName>Weaver</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christina M Weaver</PI_FULL_NAME>
<EmailAddress>christina.weaver@fandm.edu</EmailAddress>
<PI_PHON>7172913872</PI_PHON>
<NSF_ID>000542982</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Fronefield</FirstName>
<LastName>Crawford</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Fronefield Crawford</PI_FULL_NAME>
<EmailAddress>fronefield.crawford@fandm.edu</EmailAddress>
<PI_PHON>7173584517</PI_PHON>
<NSF_ID>000547778</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Carrie</FirstName>
<LastName>Rampp</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carrie Rampp</PI_FULL_NAME>
<EmailAddress>crampp@fandm.edu</EmailAddress>
<PI_PHON>7173584517</PI_PHON>
<NSF_ID>000693784</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Franklin and Marshall College</Name>
<CityName>Lancaster</CityName>
<ZipCode>176043003</ZipCode>
<PhoneNumber>7173584517</PhoneNumber>
<StreetAddress>415 Harrisburg Ave.</StreetAddress>
<StreetAddress2><![CDATA[PO Box 3003]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA11</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>069773646</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FRANKLIN AND MARSHALL COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>069773646</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Franklin and Marshall College]]></Name>
<CityName>Lancaster</CityName>
<StateCode>PA</StateCode>
<ZipCode>176043003</ZipCode>
<StreetAddress><![CDATA[415 Harrisburg Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA11</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~350000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This award allowed Franklin &amp; Marshall College to improve upon our ability to move scientific research data on campus and between our campus and other research facilities around the world.&nbsp; Being able to move large amounts of data (a few super large files, or many small files) quickly allows researchers to advance research work more rapidly.&nbsp; If it takes days or weeks to move data, those are days or weeks you do not yet have access to the relevant data to conduct studies, analysis, etc.&nbsp; The specific work of the project included:&nbsp;</p> <ul> <li>Increasing our Internet Bandwidth</li> <li>Getting Connected to Internet2</li> <li>Building a Science DMZ</li> <li>Establishing a Data Transfer Node (DTN)</li> <li>Deploying a PerfSONAR node and associated monitoring tools</li> <li>Increasing our ability to do more sophisticated logging</li> </ul> <p><strong>Increasing our internet bandwidth</strong> allowed us to have greater capacity to send and receive data.&nbsp; As an example, being able to access data from a research telescope on another continent is something several F&amp;M faculty do regularly.&nbsp; Greater bandwidth allows us to receive greater data flows per second.</p> <p><strong>Getting connected to</strong> <strong>Internet2</strong> allowed us to begin taking advantage of the worldwide network dedicated to research and education.&nbsp; If you imagine the internet as a super-highway clogged up with all of the things people do on the internet, Internet2 is akin to getting on a separate highway that is dedicated to research and education and not impacted by all of the commercial activity on the commodity internet.&nbsp; As an example, getting connected to Internet2 allows us to transmit data from/to the National Technical University of Athens where an F&amp;M faculty member has a research partner.</p> <p>A <strong>Science DMZ</strong> is an approach to architecting your network so that research data, for example, can bypass your firewall both as it leaves campus and as data comes to campus.&nbsp; A firewall is an essential part of any network design because it protects the network from hackers and other attempts to do harm.&nbsp; However, a firewall does this in part by examining every packet of data that passes through it.&nbsp; By architecting a means to move trusted research data between trusted institutions allows you to move that data much more quickly since no packet examination is involved.</p> <p>To build a Science DMZ, you need a <strong>data transfer node</strong>.&nbsp; A data transfer node is where the research data is staged that is going to bypass your firewall.&nbsp; Then, using a cloud service like Globus, you initiate the transfer by logging into Globus and telling it where the data is and where it needs to go.&nbsp; The data transfer node, Globus and Science DMZ work together to allow for trusted data exchange in support of research.</p> <p>Because your data may traverse many paths, tools like <strong>more sophisticated logging and perfSONAR</strong> let you analyize that transfer, figure our slow legs in that journey and then make adjustments if appropriate.&nbsp; Institutions around the world have deployed perfSONAR nodes to make it easier for everyone to better identify those places where some tuning is required.&nbsp; As an example, F&amp;M has faculty that have been granted awards to take advantage of XSEDE.&nbsp; XSEDE is an NSF-funded initiative that makes supercomputing resources available to researchers to conduct analysis on large data sets.&nbsp; However, you have to transfer your data to XSEDE and then use those resources.&nbsp; If we found that our data transfer rate seemed slower than expected, tools like perfSONAR would allow us to analyze our data's path from our campus to XSEDE and identify where we or another institution through which our data passed needs to make some adjustments to allow that data to move more quickly.</p> <p>The six components described above work together to advance F&amp;M's ability to support scientific research conducted by both our faculty and students.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/27/2018<br>      Modified by: Carrie&nbsp;Rampp</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This award allowed Franklin &amp; Marshall College to improve upon our ability to move scientific research data on campus and between our campus and other research facilities around the world.  Being able to move large amounts of data (a few super large files, or many small files) quickly allows researchers to advance research work more rapidly.  If it takes days or weeks to move data, those are days or weeks you do not yet have access to the relevant data to conduct studies, analysis, etc.  The specific work of the project included:   Increasing our Internet Bandwidth Getting Connected to Internet2 Building a Science DMZ Establishing a Data Transfer Node (DTN) Deploying a PerfSONAR node and associated monitoring tools Increasing our ability to do more sophisticated logging   Increasing our internet bandwidth allowed us to have greater capacity to send and receive data.  As an example, being able to access data from a research telescope on another continent is something several F&amp;M faculty do regularly.  Greater bandwidth allows us to receive greater data flows per second.  Getting connected to Internet2 allowed us to begin taking advantage of the worldwide network dedicated to research and education.  If you imagine the internet as a super-highway clogged up with all of the things people do on the internet, Internet2 is akin to getting on a separate highway that is dedicated to research and education and not impacted by all of the commercial activity on the commodity internet.  As an example, getting connected to Internet2 allows us to transmit data from/to the National Technical University of Athens where an F&amp;M faculty member has a research partner.  A Science DMZ is an approach to architecting your network so that research data, for example, can bypass your firewall both as it leaves campus and as data comes to campus.  A firewall is an essential part of any network design because it protects the network from hackers and other attempts to do harm.  However, a firewall does this in part by examining every packet of data that passes through it.  By architecting a means to move trusted research data between trusted institutions allows you to move that data much more quickly since no packet examination is involved.  To build a Science DMZ, you need a data transfer node.  A data transfer node is where the research data is staged that is going to bypass your firewall.  Then, using a cloud service like Globus, you initiate the transfer by logging into Globus and telling it where the data is and where it needs to go.  The data transfer node, Globus and Science DMZ work together to allow for trusted data exchange in support of research.  Because your data may traverse many paths, tools like more sophisticated logging and perfSONAR let you analyize that transfer, figure our slow legs in that journey and then make adjustments if appropriate.  Institutions around the world have deployed perfSONAR nodes to make it easier for everyone to better identify those places where some tuning is required.  As an example, F&amp;M has faculty that have been granted awards to take advantage of XSEDE.  XSEDE is an NSF-funded initiative that makes supercomputing resources available to researchers to conduct analysis on large data sets.  However, you have to transfer your data to XSEDE and then use those resources.  If we found that our data transfer rate seemed slower than expected, tools like perfSONAR would allow us to analyze our data's path from our campus to XSEDE and identify where we or another institution through which our data passed needs to make some adjustments to allow that data to move more quickly.  The six components described above work together to advance F&amp;M's ability to support scientific research conducted by both our faculty and students.          Last Modified: 11/27/2018       Submitted by: Carrie Rampp]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
