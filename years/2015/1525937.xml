<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Touchscreen Computer Interfaces for Working Dogs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>499352.00</AwardTotalIntnAmount>
<AwardAmount>499352</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>For nearly a century, assistance dogs have improved the lives of thousands of people with disabilities.  These working dogs, which include hearing dogs, guide dogs, service dogs, and medical alert dogs, can provide independence and significantly enhance quality of life.  Recent research has shown that the dogs can interact with wearable technology to communicate important information to their handlers, such as a tornado siren sounding.  However, in a home environment, they rarely wear their service dog vests or harnesses even though the need for clear communication between dog and handler still exists.  The main goal of this research is to explore fundamental aspects of touchscreen interaction for dogs, to support communication between working dogs and their handlers or other humans.  Project outcomes will extend the state of the art in virtual touchscreen interfaces to animals, which is a new domain.  The work will contribute to the new field of Animal-Computer Interfaces, providing us with a better understanding of the physical and cognitive abilities of dogs, and to what extent they can interact with humans through technology.  In addition to the direct impact of further improving the quality of life for people with disabilities, the technologies developed as part of this research will ultimately benefit all users; pet dogs could be trained to use a touchscreen to request to go out, or to alert their owners of an intruder on the property, while medical alert dogs could directly summon help through a touchscreen.  Providing dogs with the ability to express specific needs to a handler could benefit the dogs as well, enhancing their safety and well-being.&lt;br/&gt;&lt;br/&gt;The PI and her team will examine and adapt usability analysis and design techniques from the human field, such as Fitt's law and Power Law of Practice, to animal interactions, expanding the body of knowledge in interactive computing.   They will explore the physical and cognitive abilities of dogs to interact with touchscreens, as well as determining what dogs can sense and comprehend from virtual displays.  Three specific objectives will be addressed.&lt;br/&gt;&lt;br/&gt;1. To examine factors in the design of dog-appropriate touchscreen affordances and to create corresponding design guidelines for developing touchscreen interfaces for working dogs.  The PI will experiment with size, color, shape, and placement of icons to determine what dogs can best comprehend.  She will also test the limits of canine cognitive load in remembering and differentiating patterns.&lt;br/&gt;&lt;br/&gt;2. To create a working dog touchscreen interface prototype and perform a formal usability study with a variety of working dogs.  Building on the results of the first objective, the PI will create a touchscreen system and design a training protocol to familiarize working dogs with touchscreen tasks.  She will then perform a study employing usability metrics grounded in human interaction theory to develop fundamental canine-computer interaction theory for virtual displays.&lt;br/&gt;&lt;br/&gt;3. To perform a technological probe field case study in the home of an assistance dog and handler to evaluate the effectiveness of a touchscreen system for a working dog.  Using the findings from the first two objectives, the PI will create a system and perform an ethnographic study in a home-use scenario.</AbstractNarration>
<MinAmdLetterDate>08/26/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1525937</AwardID>
<Investigator>
<FirstName>Melody</FirstName>
<LastName>Jackson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Melody Jackson</PI_FULL_NAME>
<EmailAddress>melody@cc.gatech.edu</EmailAddress>
<PI_PHON>4043850866</PI_PHON>
<NSF_ID>000071319</NSF_ID>
<StartDate>08/26/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Charles</FirstName>
<LastName>Zeagler</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Charles Zeagler</PI_FULL_NAME>
<EmailAddress>clintzeagler@gatech.edu</EmailAddress>
<PI_PHON>4043459776</PI_PHON>
<NSF_ID>000584855</NSF_ID>
<StartDate>08/26/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Tech Research Corporation]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303080760</ZipCode>
<StreetAddress><![CDATA[85 Fifth St NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~499352</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The DogTouch project &nbsp;team sought to understand how working dogs can interact with virtual interfaces in the form of touchscreen interactions. &nbsp;The researchers implemented a system that allows virtual interfaces to be prototyped quickly and easily, and the team was able to test a variety of interface designs.</p> <p>They then performed three major studies. &nbsp;In the first study, they compared human movement laws (Fitts' law) to dogs selecting icons with their noses, and found that the human laws hold for dogs, creating the first canine interaction theory. &nbsp;They then experimented with color, shape, distance, and size of icons to generate guidelines for canine interaction designs. &nbsp;In addition to testing selection interfaces with dogs, the team also created and tested proportional interfaces (sliders) to determine if dogs could communicate quantified information, which was successful. &nbsp;The last study was a field study of a medical alert system that allowed a dog to call emergency services in a medical crisis.&nbsp;</p> <p><span>The DogTouch project opened a realm of new possibilities for dog-as-sensor systems. &nbsp;In addition to the medical alert scenario, in which a dog can summon help to avert a medical crisis, touchscreen systems could be used by cancer detection dogs to indicate what type of cancer has been found. &nbsp;A touchscreen could also be used for a bomb detection dog to indicate what explosive has been found, and what amount. &nbsp;Hearing dogs could use a touchscreen to indicate to their deaf human partners what sound they just heard. &nbsp;The possibilities for dogs to help humans, and even save lives, are signifcant. &nbsp;DogTouch represents the first formal study of virtual interactions for dogs.&nbsp;</span></p><br> <p>            Last Modified: 11/30/2018<br>      Modified by: Melody&nbsp;Jackson</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1525937/1525937_10392370_1543557432090_Touchscreen911Skyedited--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1525937/1525937_10392370_1543557432090_Touchscreen911Skyedited--rgov-800width.jpg" title="Virtual interfaces for working dogs"><img src="/por/images/Reports/POR/2018/1525937/1525937_10392370_1543557432090_Touchscreen911Skyedited--rgov-66x44.jpg" alt="Virtual interfaces for working dogs"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A medical alert dog uses a touchscreen to call emergency services in a simulated medical crisis</div> <div class="imageCredit">Melody Jackson</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Melody&nbsp;Jackson</div> <div class="imageTitle">Virtual interfaces for working dogs</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The DogTouch project  team sought to understand how working dogs can interact with virtual interfaces in the form of touchscreen interactions.  The researchers implemented a system that allows virtual interfaces to be prototyped quickly and easily, and the team was able to test a variety of interface designs.  They then performed three major studies.  In the first study, they compared human movement laws (Fitts' law) to dogs selecting icons with their noses, and found that the human laws hold for dogs, creating the first canine interaction theory.  They then experimented with color, shape, distance, and size of icons to generate guidelines for canine interaction designs.  In addition to testing selection interfaces with dogs, the team also created and tested proportional interfaces (sliders) to determine if dogs could communicate quantified information, which was successful.  The last study was a field study of a medical alert system that allowed a dog to call emergency services in a medical crisis.   The DogTouch project opened a realm of new possibilities for dog-as-sensor systems.  In addition to the medical alert scenario, in which a dog can summon help to avert a medical crisis, touchscreen systems could be used by cancer detection dogs to indicate what type of cancer has been found.  A touchscreen could also be used for a bomb detection dog to indicate what explosive has been found, and what amount.  Hearing dogs could use a touchscreen to indicate to their deaf human partners what sound they just heard.  The possibilities for dogs to help humans, and even save lives, are signifcant.  DogTouch represents the first formal study of virtual interactions for dogs.        Last Modified: 11/30/2018       Submitted by: Melody Jackson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
