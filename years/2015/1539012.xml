<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Affect-Based Video Retrieval</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>482000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops advanced machine learning and computer vision technologies for affect-based video retrieval to retrieve videos according to their emotional content.  Introducing such a personal touch into video retrieval can improve user's interaction experiences with videos by allowing user to retrieve and organize videos based on their specific emotional needs.  In addition, the project also has impacts on a wide range of fields including advertisement and education, allowing the video creators such as advertisers, and educators to effectively customize the videos to best serve the users' emotional needs. The project also contributes to education and student training.  The project is integrated with education by (a) introducing a course on computer vision for affective computing; (b) involving undergraduate and graduate students in this project, especially those from under-represented groups; and (c) organizing workshops and tutorials in major computer vision and affective computing conferences on topics related to this research for further dissemination of the research ideas and results.&lt;br/&gt;&lt;br/&gt;This research addresses problems in video affective content analysis. Affect-based video retrieval faces two major challenges. First, there exists a significant semantic gap between the low level video features and the high level affective content of the video. Second, due to the subjective nature of user's perception of emotion, user's emotional responses to videos vary significantly with people. For the first challenge, the PI develops a novel generative deep model to automatically learn an affect-sensitive multi-modal middle level video representation from the raw video data.  To further improve the characterization of the video's affective content, the PI augments it with semantic video attributes derived from well-established video production knowledge to produce a hybrid multi-modal middle level video representation. The hybrid multi-modal middle level representation can effectively bridge the gap between the raw video and its affective content.  For the second challenge, the PI employs a multi-task deep learning method to tailor the middle level representation to each user's specific affective preferences in order to maximize their experience with videos.</AbstractNarration>
<MinAmdLetterDate>09/03/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/27/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1539012</AwardID>
<Investigator>
<FirstName>Qiang</FirstName>
<LastName>Ji</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qiang Ji</PI_FULL_NAME>
<EmailAddress>qji@ecse.rpi.edu</EmailAddress>
<PI_PHON>5182766440</PI_PHON>
<NSF_ID>000350734</NSF_ID>
<StartDate>09/03/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rensselaer Polytechnic Institute</Name>
<CityName>Troy</CityName>
<ZipCode>121803522</ZipCode>
<PhoneNumber>5182766000</PhoneNumber>
<StreetAddress>110 8TH ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002430742</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RENSSELAER POLYTECHNIC INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002430742</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rensselaer Polytechnic Institute]]></Name>
<CityName>troy</CityName>
<StateCode>NY</StateCode>
<ZipCode>121803522</ZipCode>
<StreetAddress><![CDATA[110 8th street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>002Z</Code>
<Text>Intel/NSF VEC Partnership</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~450000</FUND_OBLG>
<FUND_OBLG>2016~8000</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<FUND_OBLG>2020~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To meet users' increasing needs to retrieve and organize videos according to their emotional content, the goal of this research is developing advanced probabilistic machine learning models for human emotion modeling, recognition and retrieval from videos.&nbsp; The specific objectives include: 1) develop deep probabilistic graphical models (DPGMs) and the associated learning and inference methods to capture and encode video's affective content, and 2) apply the DPGM models to video-based human emotion recognition, retrieval, and tagging.</p> <p>Technically, this research produces several deep probabilistic graphical models, including 1) the multimodal Deep Regression Bayesian Network (DRBN) that captures the dependencies between audio and visual data to produce the hybrid multi-modal representation of video's affective content; 2) the interval temporal restricted Boltzmann machine (IT-RBM) that captures the complex spatial temporal dependencies among primitive facial behaviors through Allen's interval algebra; 3) the hybrid Bayesian Network that captures the inherent dependencies among facial actions and between facial actions and their image measurements; and 4) a method to encode the well-established video production principles for emotion elicitation into the DRBN model learning to produce the knowledge augmented multi-modal representation of video's affective content. &nbsp;&nbsp;The models were evaluated on benchmark databases for both emotion recognition and video emotion tagging and retrieval. &nbsp;Evaluations show that they outperform state of the art methods in both within-dataset and cross-dataset performance as well as in data learning efficiency.</p> <p>Besides improving users' personal experience with videos, this research has broad impacts in several fields, including <span>advertisement,&nbsp;</span>entertainment, education, etc., allowing the video creators in these fields to effectively customize the videos to produce the desired emotional responses.&nbsp; This research also contributes to the education and training of 7 Ph.D students and 5 undergraduate students in computer vision, machine learning, and in affective computing.&nbsp; The research results for this work are widely disseminated in the research communities through 10 publications in top AI/ML journals and conferences, including 5 journal papers in IEEE TPAMI, IEEE TAC, and IEEE TIP, and 5 conference papers in CVPR, ICCV, NIPS, and AAAI.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/23/2021<br>      Modified by: Qiang&nbsp;Ji</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To meet users' increasing needs to retrieve and organize videos according to their emotional content, the goal of this research is developing advanced probabilistic machine learning models for human emotion modeling, recognition and retrieval from videos.  The specific objectives include: 1) develop deep probabilistic graphical models (DPGMs) and the associated learning and inference methods to capture and encode video's affective content, and 2) apply the DPGM models to video-based human emotion recognition, retrieval, and tagging.  Technically, this research produces several deep probabilistic graphical models, including 1) the multimodal Deep Regression Bayesian Network (DRBN) that captures the dependencies between audio and visual data to produce the hybrid multi-modal representation of video's affective content; 2) the interval temporal restricted Boltzmann machine (IT-RBM) that captures the complex spatial temporal dependencies among primitive facial behaviors through Allen's interval algebra; 3) the hybrid Bayesian Network that captures the inherent dependencies among facial actions and between facial actions and their image measurements; and 4) a method to encode the well-established video production principles for emotion elicitation into the DRBN model learning to produce the knowledge augmented multi-modal representation of video's affective content.   The models were evaluated on benchmark databases for both emotion recognition and video emotion tagging and retrieval.  Evaluations show that they outperform state of the art methods in both within-dataset and cross-dataset performance as well as in data learning efficiency.  Besides improving users' personal experience with videos, this research has broad impacts in several fields, including advertisement, entertainment, education, etc., allowing the video creators in these fields to effectively customize the videos to produce the desired emotional responses.  This research also contributes to the education and training of 7 Ph.D students and 5 undergraduate students in computer vision, machine learning, and in affective computing.  The research results for this work are widely disseminated in the research communities through 10 publications in top AI/ML journals and conferences, including 5 journal papers in IEEE TPAMI, IEEE TAC, and IEEE TIP, and 5 conference papers in CVPR, ICCV, NIPS, and AAAI.          Last Modified: 05/23/2021       Submitted by: Qiang Ji]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
