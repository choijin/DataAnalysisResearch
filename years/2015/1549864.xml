<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STTR Phase I:  Dynamic Robust Hand Model for Gesture Intent Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2016</AwardEffectiveDate>
<AwardExpirationDate>12/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this project stems from addressing the important hand gesture based input challenges of VR and AR industries that are expected to grow to $150B by 2020. Piper Jaffray identifies VR as the next mega trend and estimates the VR market to be worth more than $60B by 2025. Piper Jaffray highlights new market opportunities for peripheral devices that bring hands and feet into VR. This technology if successful in mitigating the high technical risks represents a huge leap in the state of the art in 3D hand models for gesture recognition and has the potential to be the industry standard for AR, VR and 3D applications. Our company will commercialize the project by licensing this technology as a hand model SDK to the AR/VR and 3D camera device makers and application developers to bring highly interactive VR/AR and 3D gesture applications to gaming, entertainment, education, healthcare, design, architecture, and manufacturing.&lt;br/&gt;&lt;br/&gt;This Small Business Technology Transfer Research (STTR) Phase I project develops a breakthrough innovation in 3D hand gesture intent recognition that can robustly work across different 3D cameras, orientations, positions and occlusions. It addresses a key challenge in gesture recognition while enabling natural spatial interactions for Virtual and Augmented Reality (VR/AR) and many other applications enabled by 3D depth cameras. It solves the following key challenges faced by existing academic and commercial hand models and involves very high technical risks: 1) robustness under heavy occlusions 2) invariance to view-point changes 3) low computational training and tracking complexity 4) discriminative to frequent gesture/micro-gesture sequences. We will tackle these by developing a novel dynamic, robust hand-tracking model inspired by a machine learning technique that is not commonly used by computer vision community. We will achieve this by developing the following objectives 1) hand pose hypothesis generation using trained classifiers 2) hand model fitting using joint matrix factorization and completion 3) user study and evaluation of the hand model.</AbstractNarration>
<MinAmdLetterDate>12/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>12/18/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1549864</AwardID>
<Investigator>
<FirstName>Karthik</FirstName>
<LastName>Ramani</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karthik Ramani</PI_FULL_NAME>
<EmailAddress>ramani@purdue.edu</EmailAddress>
<PI_PHON>7654945725</PI_PHON>
<NSF_ID>000284152</NSF_ID>
<StartDate>12/18/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Raja</FirstName>
<LastName>Jasti</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raja Jasti</PI_FULL_NAME>
<EmailAddress>raja@zeroui.com</EmailAddress>
<PI_PHON>4084898622</PI_PHON>
<NSF_ID>000632415</NSF_ID>
<StartDate>12/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>ZeroUI Inc</Name>
<CityName>Cupertino</CityName>
<ZipCode>950144442</ZipCode>
<PhoneNumber>4088630555</PhoneNumber>
<StreetAddress>10570 Whitney Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078658977</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ZEROUI, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[ZeroUI Inc]]></Name>
<CityName>Cupertino</CityName>
<StateCode>CA</StateCode>
<ZipCode>950144442</ZipCode>
<StreetAddress><![CDATA[10570 Whitney Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1505</Code>
<Text>STTR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>1505</Code>
<Text>STTR PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>4080</Code>
<Text>ADVANCED COMP RESEARCH PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8035</Code>
<Text>Hardware Devices</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-513bac4e-9f80-4230-d6a4-dc79441edaa8"> </span></p> <p dir="ltr"><span>This STTR Phase I project developed and tested iHand, a break through dynamic hand model for gesture intent recognition for applications in Virtual Reality (VR) and Augmented Reality (AR). It leverages 3D depth sensing cameras with new recommender system based machine learning algorithms resulting in a transformative leap above the current state of the art and overcomes key technical hurdles that proved difficult to overcome prior to this model.</span></p> <p><span id="docs-internal-guid-513bac4e-9f82-51dc-c882-f3f1f33aa69c"> <p dir="ltr"><span> This transformative solution for a dynamic hand model engineers informative features directly from the point cloud data, and use our prior (geometric, temporal etc.) knowledge of hand articulations to &lsquo;collaboratively&rsquo; solve the problem of hand pose estimation. To do this, it leverages 3D mesh processing techniques to engineer features, and uses collaborative filtering techniques in recommender systems and more generally in knowledge discovery and data mining (KDD), lesser known to the vision community. These new mathematical techniques to solve an implicit multi-objective function brought into machine vision from KDD forms a key disruptive aspect of our innovation.</span></p> </span></p> <p><span id="docs-internal-guid-513bac4e-9f81-7bf2-6cbb-e06157405a78"> <p dir="ltr"><span>The following objectives were successfully completed: (1) Hand pose hypothesis generation using trained classifiers (2) Hand model fitting using joint matrix factorization (3) User study and evaluation of the proposed iHand model. These objectives helped us successfully address key technical gaps of the current research solutions and commercial systems: (1) Robust solution under heavy occlusions (2) Invariance to view-point changes (3) Low computational training and hase pose estimation complexity (a) discriminative to frequent gesture/micro-gesture sequences.</span></p> </span></p> <p><span id="docs-internal-guid-513bac4e-9f80-b481-941b-9f5498f849ee"><span>This project has successfully for the first time overcome the practical barriers to hand pose estimation with commercial depth camera&rsquo;s. This includes the reduced computational complexity reduction through joint matrix factorization and completion approach, invariance to viewpoints and a solution under heavy occlusions.</span></span></p><br> <p>            Last Modified: 01/14/2017<br>      Modified by: Raja&nbsp;Jasti</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   This STTR Phase I project developed and tested iHand, a break through dynamic hand model for gesture intent recognition for applications in Virtual Reality (VR) and Augmented Reality (AR). It leverages 3D depth sensing cameras with new recommender system based machine learning algorithms resulting in a transformative leap above the current state of the art and overcomes key technical hurdles that proved difficult to overcome prior to this model.    This transformative solution for a dynamic hand model engineers informative features directly from the point cloud data, and use our prior (geometric, temporal etc.) knowledge of hand articulations to ?collaboratively? solve the problem of hand pose estimation. To do this, it leverages 3D mesh processing techniques to engineer features, and uses collaborative filtering techniques in recommender systems and more generally in knowledge discovery and data mining (KDD), lesser known to the vision community. These new mathematical techniques to solve an implicit multi-objective function brought into machine vision from KDD forms a key disruptive aspect of our innovation.    The following objectives were successfully completed: (1) Hand pose hypothesis generation using trained classifiers (2) Hand model fitting using joint matrix factorization (3) User study and evaluation of the proposed iHand model. These objectives helped us successfully address key technical gaps of the current research solutions and commercial systems: (1) Robust solution under heavy occlusions (2) Invariance to view-point changes (3) Low computational training and hase pose estimation complexity (a) discriminative to frequent gesture/micro-gesture sequences.   This project has successfully for the first time overcome the practical barriers to hand pose estimation with commercial depth camera?s. This includes the reduced computational complexity reduction through joint matrix factorization and completion approach, invariance to viewpoints and a solution under heavy occlusions.       Last Modified: 01/14/2017       Submitted by: Raja Jasti]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
