<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Matching Non-Native Transcribers to the Distinctive Features of the Language Transcribed</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Automatic speech recognition (ASR) systems must be trained using hundreds of hours of speech, with synchronized text transcriptions.  Transcribing that much speech is beyond the means of most language communities; therefore ASR systems do not exist for most languages.   To overcome this bottleneck, this exploratory EAGER project asks people who don't understand a particular language to transcribe it as if they were listening to nonsense syllables.  Of course, when people try to transcribe speech in a language they don't understand, they make mistakes.  However there are patterns to those mistakes which can be modeled using decoding strategies developed for telephone and wireless communication, and used to route each transcription task to people whose native language helps them to perform it. The resulting transcriptions are then fused in order to recover correct transcriptions.  Five different languages are to be tested, including languages with lexical tone, and languages with a variety of consonant contrasts very different from English.  The resulting transcriptions can then train ASR systems in all five languages, and the quality of the research evaluated based on its ability to train those systems without using transcriptions produced by native speakers. &lt;br/&gt; &lt;br/&gt;Mismatched crowdsourcing is formalized as a noisy channel; the talker encodes meaning in a string of symbols (phonemes) not all of which are reliably distinguishable by the perceiver.  Models of second-language speech perception for each transcriber can be initialized using a perceptual assimilation model, then specialized.  In particular, this proposal seeks increases in the scale and robustness of mismatched crowdsourcing by using error-correcting codes to divide the transcription task, and by then distributing each sub-task to transcribers whose native language contains the distinctive feature requested.  It also seeks to develop new theory at the intersection of the current fields of crowdsourcing (the learnability of a function under conditions of label noise) and grammar induction (the learnability of a function from one language to another), and to perform grammar induction under conditions of label noise.  Preliminary bounds exist for some aspects of this problem; the proposed research is designed to develop more detailed theoretic results, and test and apply them to determine the feasibility of creating serviceable ASR systems for under-resourced languages without having to use fluent speakers of those languages to transcribe speech in those languages.</AbstractNarration>
<MinAmdLetterDate>08/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1550145</AwardID>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Hasegawa-Johnson</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark A Hasegawa-Johnson</PI_FULL_NAME>
<EmailAddress>jhasegaw@illinois.edu</EmailAddress>
<PI_PHON>2173330925</PI_PHON>
<NSF_ID>000431210</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lav</FirstName>
<LastName>Varshney</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lav Varshney</PI_FULL_NAME>
<EmailAddress>Varshney@illinois.edu</EmailAddress>
<PI_PHON>2173332187</PI_PHON>
<NSF_ID>000669276</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Preethi</FirstName>
<LastName>Jyothi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Preethi Jyothi</PI_FULL_NAME>
<EmailAddress>pjyothi@illinois.edu</EmailAddress>
<PI_PHON>2173332187</PI_PHON>
<NSF_ID>000671742</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207449</ZipCode>
<StreetAddress><![CDATA[1901 S. First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Of the many thousand languages spoken today, telephone-based internet search using automatic speech recognition (ASR) exists in perhaps sixty. The technique of mismatched crowdsourcing is a method for reducing the development cost of ASR in every other language.<br />The basic idea of mismatched crowdsourcing is quite simple. &nbsp;Suppose we can find audio data, but it is not transcribed. &nbsp;Suppose that we can't find, on any standard crowdsourcing platform, a transcriber who speaks the utterance language. &nbsp;Mismatched crowdsourcing proposes that, since we can't find the people we want, we should hire the people we can find. &nbsp;People who speak some other language (the annotator language) are hired to write down what they hear, using annotator-language orthography. &nbsp;The mapping from utterance language phonemes to annotator language orthography is modeled as a noisy communication channel (the ``mismatch channel''), each of whose substitution probabilities can be estimated or learned. &nbsp;By inverting the mismatch channel, we derive a probabilistic transcription: a probability mass function over the set of all possible utterance-language phoneme transcriptions. &nbsp;Based on these observations, we proposed to select annotation languages, for any given utterance language, by choosing the smallest possible set of non-native annotator languages (from a set of 35 languages represented on Mechanical Turk). &nbsp;We demonstrated that any language in our 35-language set can be completely transcribed by annotators speaking only 2 mismatched annotation languages. &nbsp;If languages are weighted by the number of available annotators (such that less frequently attested annotation languages are more costly), then the transcription of Somali requires 3 annotation languages (English, Spanish, and Vietnamese annotators); every other language in our 35 can be covered by at most 2 mismatched annotation languages.<br />This project was initiated with two primary goals. &nbsp;First, we seek to leverage the power of crowd-sourcing to develop speech technology in as many languages as possible. &nbsp;As part of this research project, the method of mismatched crowdsourcing has now been used to train automatic speech recognizers in Dutch, Hungarian, Mandarin, Swahili, Japanese, Hebrew, Amharic, Dinka, Tigrinya, Oromo, Vietnamese, Georgian, Singapore Hokkien, Greek, Cantonese, Russian, Tamil, Uzbek, Uyghur, Hindi, Urdu, and Arabic.<br />Second, we seek to answer a scientific question: which of the sounds of a language are hard to distinguish if you don't speak the language? &nbsp;The second question is answered by asking non-speakers of a language to transcribe it, as if it were nonsense speech in their own language. &nbsp;Their transcripts are automatically aligned to a native transcript, and we accumulate statistics about the probability of various phoneme substitution errors, as a function of the language pair (utterance, annotator). &nbsp;Obviously, a non-native listeners is far more accurate in distinguishing phonemes that also exist in is or her own native language, and her accuracy deteriorates if one of the two phonemes does not exist in her language. &nbsp;On the other hand, far more detailed models are possible. &nbsp;For example, articulutory features (the language-independent sound categories used by linguists to organize the phoneme sets of all of the world's languages) predict cross-language perceptual confusions pretty well: the more articulatory features shared by a pair of phonemes, the more likely it is that a non-native listener will confuse one for the other. &nbsp; &nbsp;Furthermore, for lexical tone languages like Mandarin, Vietnamese, Cantonese, and Singapore Hokkien, there isn't any distinctive feature system that linguists agree on that is capable of describing lexical tone, so this research has created such a system on the basis of cross-language perceptual data. &nbsp;The most salient factors cross-linguistically seem to be the difference between static and dynamic tones (rising versus flat), and the difference between breathy, creaky, and other voice qualities.<br /><br /><br /></p><br> <p>            Last Modified: 09/02/2018<br>      Modified by: Mark&nbsp;A&nbsp;Hasegawa-Johnson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Of the many thousand languages spoken today, telephone-based internet search using automatic speech recognition (ASR) exists in perhaps sixty. The technique of mismatched crowdsourcing is a method for reducing the development cost of ASR in every other language. The basic idea of mismatched crowdsourcing is quite simple.  Suppose we can find audio data, but it is not transcribed.  Suppose that we can't find, on any standard crowdsourcing platform, a transcriber who speaks the utterance language.  Mismatched crowdsourcing proposes that, since we can't find the people we want, we should hire the people we can find.  People who speak some other language (the annotator language) are hired to write down what they hear, using annotator-language orthography.  The mapping from utterance language phonemes to annotator language orthography is modeled as a noisy communication channel (the ``mismatch channel''), each of whose substitution probabilities can be estimated or learned.  By inverting the mismatch channel, we derive a probabilistic transcription: a probability mass function over the set of all possible utterance-language phoneme transcriptions.  Based on these observations, we proposed to select annotation languages, for any given utterance language, by choosing the smallest possible set of non-native annotator languages (from a set of 35 languages represented on Mechanical Turk).  We demonstrated that any language in our 35-language set can be completely transcribed by annotators speaking only 2 mismatched annotation languages.  If languages are weighted by the number of available annotators (such that less frequently attested annotation languages are more costly), then the transcription of Somali requires 3 annotation languages (English, Spanish, and Vietnamese annotators); every other language in our 35 can be covered by at most 2 mismatched annotation languages. This project was initiated with two primary goals.  First, we seek to leverage the power of crowd-sourcing to develop speech technology in as many languages as possible.  As part of this research project, the method of mismatched crowdsourcing has now been used to train automatic speech recognizers in Dutch, Hungarian, Mandarin, Swahili, Japanese, Hebrew, Amharic, Dinka, Tigrinya, Oromo, Vietnamese, Georgian, Singapore Hokkien, Greek, Cantonese, Russian, Tamil, Uzbek, Uyghur, Hindi, Urdu, and Arabic. Second, we seek to answer a scientific question: which of the sounds of a language are hard to distinguish if you don't speak the language?  The second question is answered by asking non-speakers of a language to transcribe it, as if it were nonsense speech in their own language.  Their transcripts are automatically aligned to a native transcript, and we accumulate statistics about the probability of various phoneme substitution errors, as a function of the language pair (utterance, annotator).  Obviously, a non-native listeners is far more accurate in distinguishing phonemes that also exist in is or her own native language, and her accuracy deteriorates if one of the two phonemes does not exist in her language.  On the other hand, far more detailed models are possible.  For example, articulutory features (the language-independent sound categories used by linguists to organize the phoneme sets of all of the world's languages) predict cross-language perceptual confusions pretty well: the more articulatory features shared by a pair of phonemes, the more likely it is that a non-native listener will confuse one for the other.    Furthermore, for lexical tone languages like Mandarin, Vietnamese, Cantonese, and Singapore Hokkien, there isn't any distinctive feature system that linguists agree on that is capable of describing lexical tone, so this research has created such a system on the basis of cross-language perceptual data.  The most salient factors cross-linguistically seem to be the difference between static and dynamic tones (rising versus flat), and the difference between breathy, creaky, and other voice qualities.          Last Modified: 09/02/2018       Submitted by: Mark A Hasegawa-Johnson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
