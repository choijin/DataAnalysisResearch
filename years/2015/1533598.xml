<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NCS-FO: Real-time optical readout and control of population neural activity with cellular resolution</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>700000.00</AwardTotalIntnAmount>
<AwardAmount>700000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Shubhra Gangopadhyay</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>ECCS- Prop. No. 1533598&lt;br/&gt;&lt;br/&gt;PI:  Gong, Yiyang &lt;br/&gt;Institute: Duke University&lt;br/&gt;Title: NCS-FO: Real-time optical readout and control of population neural activity with cellular resolution&lt;br/&gt;Objective: &lt;br/&gt;This project will develop a mechanism for simultaneously controlling and reading out neural activity when being activated by optogenetic techniques; this capability will surpass a previous limitation in neural studies.  The proposal is separated into three aims: 1) develop calcium sensors and optogenetic channels active on different wavelength ranges to allow simultaneous readout and control, 2) develop a dual-beam two photon microscope, and 3) develop imaging software that can process neural activity in real-time. &lt;br/&gt;&lt;br/&gt;Nontechnical &lt;br/&gt;Understanding neural function requires examining specific subsets of the vast numbers of neurons in the brain. Recently developed optogenetics tools, such as optogenetic stimulation and calcium imaging, have partially fulfilled the need to target these specific sets of neurons and study their function. These techniques deliver engineered genes to targeted neural populations, and use light to manipulate or measure neural activity. Current optogenetic tools lack the spatiotemporal resolution to causally study many individual neurons in parallel on fast time scales; they only make broad conclusions either on near-millimeter sized brain regions, or over the timescale of many action potentials. We propose to integrate the design and implementation of optical and genetic tools to greatly refine the scale of investigating neural activity. Specifically, we will create two optically independent channels: one channel for fast, spatially precise optical patterning to control individual neurons; and one channel for independent recording of neural activity from individual neurons. We will then integrate these two channels by creating software that instantaneously patterns optical excitation based on the optical recording. Integrative design and engineering of this expansive set of tools will enable neuroscientists to quickly manipulate and control large populations of single neurons, a capability that does not exist presently. Our technology will allow the community to directly explore how neural activity patterns of many individual neurons in one brain region drive downstream neural activity. This novel probing of functional connectivity is exactly the type of study needed to better understand the coordination of neural activity in healthy and diseased brains. Beyond the specific application of neuroscience, training students within our multidisciplinary setting will create the next generation of scientists capable of tackling the broad set of technical challenges facing society today. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Technical &lt;br/&gt;Optical imaging of brain activity has steadily developed into a staple technique within neuroscience labs over the past decade. In combination with genetically encoded sensors of neural activity, optical methods enable genetic targeting and chronic, simultaneous imaging of many individual neurons. One significant weakness of existing optical techniques when compared to electrophysiology is the inability to simultaneously measure and control the activity of a neuron in real time. We propose to address this shortcoming by developing an optical imaging system and data processing software suite that will enable real-time optical readout of neural activity and real-time neural feedback via optical excitation, all with cellular level specificity and in parallel over a large population of neurons. This new ability to optically record and manipulate many genetically or functionally specified neurons individually will augment current studies using bulk neural activation or inhibition; the fine scale perturbations of neurons will tease apart the details of neural circuits. Specifically, we will engineer a set of optogenetic actuators, fluorescent sensors, and microscopy tools that will enable optical readout and control of neurons in different wavelength channels. We will also develop fast image processing algorithms that quickly convert images to neural activity of individual neurons, thereby enabling real-time control of neural activity based on the optical readout. Recently, improvements to these tools occurred independently. Our proposal will address the integrated development of the tool set, and effectively employ trade-offs between the individual components. For example, simultaneous engineering of the protein sensor, imaging processing software, and optical imaging hardware will optimize the readout fidelity. Similarly, joint design of the genetic tools' spectral separation and the optical spatiotemporal resolution will extend optical control precision. Integration of these developments to examine neural function at the cellular level is unprecedented: successful advancement of this research will enable novel examination of the brain and help guide targeted biomedical therapi</AbstractNarration>
<MinAmdLetterDate>08/12/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/12/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1533598</AwardID>
<Investigator>
<FirstName>Yiyang</FirstName>
<LastName>Gong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yiyang Gong</PI_FULL_NAME>
<EmailAddress>yiyang.gong@duke.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000688484</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sina</FirstName>
<LastName>Farsiu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sina Farsiu</PI_FULL_NAME>
<EmailAddress>sina.farsiu@dm.duke.edu</EmailAddress>
<PI_PHON>9196843030</PI_PHON>
<NSF_ID>000691339</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054677</ZipCode>
<StreetAddress><![CDATA[2200 W Main Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5345</Code>
<Text>Engineering of Biomed Systems</Text>
</ProgramElement>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<ProgramReference>
<Code>8551</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~700000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Intellectual Merit</p> <p>This award developed the foundation for rapidly examining neural activity. The intellectual merit of our work falls into three distinct categories ? protein tools, optical tools, and data processing tools. Together, these tools enable the capability to rapidly assess neural activity and stimulate neural activity. These twin capabilities will enable neuroscientists to not only record the contribution of individual neurons on hundreds of millisecond timescales, but also perturb the neural activity based on the types of activity that are recorded. Such capabilities would define how neural circuits input and output information, as well as generate a deeper understanding of how neural circuits compute information.</p> <p>First, for protein tools, we have developed a new set of calcium sensors; these sensors report calcium dynamics associated with neural activity. Our new sensors are very bright, and thus enhance the contrast and signal fidelity of recorded neural activity. Our sensor employs a pair of fluorescent proteins (eGFP and mScarlet, all bright green or red fluorescent proteins) as a FRET pair, and links them with a calcium binding domain. Our new sensor responds to calcium activity in cultured neurons and in live mice.</p> <p>Second, for optical tools, we have developed a two channel optical microscope that can record neural activity and stimulate neural activity simultaneously. This microscope combines two laser sources with mechanical scanning and optical pattern generation to target tens to hundreds of neurons simultaneously. Together, these optical tools will enable the control of neural activity on the timescale of 10-20 milliseconds for groups of individual neurons.</p> <p>Lastly, we have developed the software to interface between the protein tools and optical tools, between the optical stimulation and the optical recordings. We developed an artificial intelligence deep learning framework to accurately segment neurons from the imaging movies. This complete algorithm was as accurate as humans at segmenting the neurons, but operated automatically, non-stop, at 10 to 100 times the speed of humans. More recently, we have further increased the speed of segmentation by reducing the complexity of the deep learning architecture. Our algorithm now processes movies one frame at a time at a speed approximately seven fold faster than the algorithm of our previous work, with comparable accuracy.</p> <p>Our technologies are immediately able to integrate into existing experiments that investigate memory. We have constructed a virtual reality (VR) setup for mice with computer monitors surrounding the optical imaging setup. Our VR environment randomly teleports the mice subjects and tries to observe if these teleports induce new memories. The optical tools developed in our work will enable us to rapidly record the neural response to these teleports, and potentially perturb the neural activity to replicate the recorded neural responses.</p> <p>&nbsp;</p> <p>Broader Impacts</p> <p>This project has provided the opportunity to broadly impact both human and software resources. For human resources, this project has hosted a broad diversity of students. At the undergraduate level, the project has hosted 15 undergraduates, including 5 URMs (2 African-American, 3 Hispanic) students, and 9 women. The undergraduate population has been integral in the data analysis and deep learning component of the project; multiple undergraduates contributed to the labeling of neurons within the datasets to create the final ground truth that trained the artificial intelligence algorithms. Three of the undergraduates have earned co-authorship on published papers or conference abstracts. Of the undergraduates that have graduated, approximately half are seeking continued scientific training in graduate school or medical school. The undergraduate presence in the lab has also provided substantial opportunities for my graduate students to train and mentor undergraduates. My graduate students Emily Redington and Somayyeh Soltanian-Zadeh has trained 8 of the undergraduates. These opportunities have increased my graduate students? understanding of mentorship and communication skills.</p> <p>For software resources, this project has made our deep-learning algorithm freely available on-line. We have included installation and example scripts to help users of our work to rapidly implement our algorithm for their own data. In addition, we have also supplied the complete set of manual labels of neurons from our datasets. This resource is one of the most accurate set of labels available; it will supply future developers of segmentation algorithms with accurate ground truth data to train or assess the performance of their algorithms.</p><br> <p>            Last Modified: 10/26/2019<br>      Modified by: Yiyang&nbsp;Gong</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138623998_1--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138623998_1--rgov-800width.jpg" title="Calcium sensing"><img src="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138623998_1--rgov-66x44.jpg" alt="Calcium sensing"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 1 ? Our new genetically encoded FRET calcium sensor using bright green and red fluorescent proteins senses calcium in cultured neurons and neurons of a live mouse.</div> <div class="imageCredit">Yiyang Gong</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Yiyang&nbsp;Gong</div> <div class="imageTitle">Calcium sensing</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138706559_2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138706559_2--rgov-800width.jpg" title="Microscope"><img src="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138706559_2--rgov-66x44.jpg" alt="Microscope"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Dual channel two-photon microscope enables simultaneous imaging and stimulation.</div> <div class="imageCredit">Yiyang Gong</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Yiyang&nbsp;Gong</div> <div class="imageTitle">Microscope</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138772142_3--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138772142_3--rgov-800width.jpg" title="Deeplearning"><img src="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138772142_3--rgov-66x44.jpg" alt="Deeplearning"></a> <div class="imageCaptionContainer"> <div class="imageCaption">CNNs can quickly and accurately process 2D two-photon calcium imaging data.</div> <div class="imageCredit">Yiyang Gong</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Yiyang&nbsp;Gong</div> <div class="imageTitle">Deeplearning</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138925903_4--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138925903_4--rgov-800width.jpg" title="hippocampalimaging"><img src="/por/images/Reports/POR/2019/1533598/1533598_10386814_1572138925903_4--rgov-66x44.jpg" alt="hippocampalimaging"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Place cells in VR are disrupted during teleportation. Calcium imaging detects changes in sensory perception and memory during teleportation.</div> <div class="imageCredit">Yiyang Gong</div> <div class="imageSubmitted">Yiyang&nbsp;Gong</div> <div class="imageTitle">hippocampalimaging</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit  This award developed the foundation for rapidly examining neural activity. The intellectual merit of our work falls into three distinct categories ? protein tools, optical tools, and data processing tools. Together, these tools enable the capability to rapidly assess neural activity and stimulate neural activity. These twin capabilities will enable neuroscientists to not only record the contribution of individual neurons on hundreds of millisecond timescales, but also perturb the neural activity based on the types of activity that are recorded. Such capabilities would define how neural circuits input and output information, as well as generate a deeper understanding of how neural circuits compute information.  First, for protein tools, we have developed a new set of calcium sensors; these sensors report calcium dynamics associated with neural activity. Our new sensors are very bright, and thus enhance the contrast and signal fidelity of recorded neural activity. Our sensor employs a pair of fluorescent proteins (eGFP and mScarlet, all bright green or red fluorescent proteins) as a FRET pair, and links them with a calcium binding domain. Our new sensor responds to calcium activity in cultured neurons and in live mice.  Second, for optical tools, we have developed a two channel optical microscope that can record neural activity and stimulate neural activity simultaneously. This microscope combines two laser sources with mechanical scanning and optical pattern generation to target tens to hundreds of neurons simultaneously. Together, these optical tools will enable the control of neural activity on the timescale of 10-20 milliseconds for groups of individual neurons.  Lastly, we have developed the software to interface between the protein tools and optical tools, between the optical stimulation and the optical recordings. We developed an artificial intelligence deep learning framework to accurately segment neurons from the imaging movies. This complete algorithm was as accurate as humans at segmenting the neurons, but operated automatically, non-stop, at 10 to 100 times the speed of humans. More recently, we have further increased the speed of segmentation by reducing the complexity of the deep learning architecture. Our algorithm now processes movies one frame at a time at a speed approximately seven fold faster than the algorithm of our previous work, with comparable accuracy.  Our technologies are immediately able to integrate into existing experiments that investigate memory. We have constructed a virtual reality (VR) setup for mice with computer monitors surrounding the optical imaging setup. Our VR environment randomly teleports the mice subjects and tries to observe if these teleports induce new memories. The optical tools developed in our work will enable us to rapidly record the neural response to these teleports, and potentially perturb the neural activity to replicate the recorded neural responses.     Broader Impacts  This project has provided the opportunity to broadly impact both human and software resources. For human resources, this project has hosted a broad diversity of students. At the undergraduate level, the project has hosted 15 undergraduates, including 5 URMs (2 African-American, 3 Hispanic) students, and 9 women. The undergraduate population has been integral in the data analysis and deep learning component of the project; multiple undergraduates contributed to the labeling of neurons within the datasets to create the final ground truth that trained the artificial intelligence algorithms. Three of the undergraduates have earned co-authorship on published papers or conference abstracts. Of the undergraduates that have graduated, approximately half are seeking continued scientific training in graduate school or medical school. The undergraduate presence in the lab has also provided substantial opportunities for my graduate students to train and mentor undergraduates. My graduate students Emily Redington and Somayyeh Soltanian-Zadeh has trained 8 of the undergraduates. These opportunities have increased my graduate students? understanding of mentorship and communication skills.  For software resources, this project has made our deep-learning algorithm freely available on-line. We have included installation and example scripts to help users of our work to rapidly implement our algorithm for their own data. In addition, we have also supplied the complete set of manual labels of neurons from our datasets. This resource is one of the most accurate set of labels available; it will supply future developers of segmentation algorithms with accurate ground truth data to train or assess the performance of their algorithms.       Last Modified: 10/26/2019       Submitted by: Yiyang Gong]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
