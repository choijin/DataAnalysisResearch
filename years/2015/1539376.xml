<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Neural-cognitive analysis of spatial scenes with competing, dynamic sound sources</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>337759.00</AwardTotalIntnAmount>
<AwardAmount>337759</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kurt Thoroughman</SignBlockName>
<PO_EMAI>kthoroug@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates neurocognitive mechanisms that extract important information from a mixture of sound sources. Imagine a day where you could no longer distinguish the honking horn of a car coming right at you from other street sounds. This cognitive ability to attend to one sound source while ignoring others presents an everyday challenge for people with hearing impairments. While the basic neural mechanisms for detecting and localizing single sounds are known, we do not know how the brain accomplishes auditory scene analysis with multiple sound sources. So far, studies have focused on lower brain centers in rodents and carnivores, while the neural mechanisms for source segregation are expected to be at higher levels, in the auditory cortex. This study will record the responses of single cortical neurons and conduct human-subject experiments for the same acoustic scenarios.  Based on the integration of these results, a functional auditory model will be developed.  This will provide new scientific insights and enable intelligent algorithms for hearing aids, social robotics, and surveillance systems. The project will provide research opportunities for graduate and undergraduate students and include outreach activities and online learning resources for high-school and college students to increase the public awareness of neuroscience. The research results and the model will be shared with the academic community. &lt;br/&gt;&lt;br/&gt;This proposal will use an interdisciplinary approach to gain understanding of the central mechanisms of auditory scene analysis by integrating psychoacoustical experiments with single-unit electrophysiology. The study will investigate how the auditory system localizes a target sound temporally embedded in a spatially separated masker. Single-unit recording will target the caudal region of the auditory cortex, the putative "where" pathway for complex sound analysis. We hypothesize that cortical activity represents both the old and new sounds, so that the internal representation of the "old" masking source can be subtracted from the overall mixture. This facilitates a clearer perception of the "new" target element, demonstrating a fundamental psychophysical phenomenon within auditory scene analysis. To test this hypothesis, we will identify the neural signals for individual sound sources separately and in combination. We will then interpret these signals based on the perceptual data gained from sound localization tests with multiple moving and stationary sound sources. Discovering the fundamental brain mechanisms for auditory scene analysis will provide new neurophysiological insight into a well-established psychophysical field and offer potential technical solutions for sound-source segregation.</AbstractNarration>
<MinAmdLetterDate>08/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1539376</AwardID>
<Investigator>
<FirstName>Yi</FirstName>
<LastName>Zhou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yi Zhou</PI_FULL_NAME>
<EmailAddress>yizhou@asu.edu</EmailAddress>
<PI_PHON>4807270650</PI_PHON>
<NSF_ID>000690676</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<StreetAddress2><![CDATA[660 South Mill Avenue, Suite 310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>943360412</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ARIZONA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>806345658</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Arizona State University]]></Name>
<CityName>Tempe</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852876011</ZipCode>
<StreetAddress><![CDATA[PO Box 876011]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1699</Code>
<Text>Cognitive Neuroscience</Text>
</ProgramElement>
<ProgramReference>
<Code>7298</Code>
<Text>COLLABORATIVE RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~337759</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-1ae28e87-7fff-b250-94c8-3cd4cf077549" style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Auditory perception of complex environments involves the neurocognitive analysis of sound features associated with many different sound sources. Auditory scene analysis, the process through which a human listener segregates sounds in a complex environment, is a difficult problem to solve, especially for hearing impaired listeners. The goal of this collaborative research between Arizona State University (Award ID: 1539376) and Rensselaer Polytechnic Institute (Award ID: 1539276) is to gain an understanding of the neural mechanisms of auditory scene analysis by integrating behavioral assessments from single-unit electrophysiology and computational modeling. We designed experiments to test the hypothesis that the auditory system makes use of the features of different sound sources to separate them from each other and from the interference of distractors in the environment. We discovered that neurons in auditory cortex respond selectively to a wide range of sound locations in both frontal and rear space. Further, we found that the effect of a background masker on the spatial selectivity of these neurons was reduced when the masker had a slightly earlier arrival time than the target, consistent with results from human studies. At the behavioral level, we observed that frontal visual stimulation can influence auditory spatial perception in both the frontal and rear space of human listeners, especially so for listeners with moderate to severe hearing loss - see Figure 1.&nbsp;</span></p> <p style="line-height: 1.2; margin-top: 12pt; margin-bottom: 12pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">We developed a modular set of auditory models that can be combined to perform auditory scene analysis tasks and to robustly extract features from complex environments. The model suite includes: algorithms to perform sound-source segregation and sound-source localization in the presence of room reflections; binaural models that improve performance by utilizing head movements; as well as two pitch models. The latter can accurately predict various pitch phenomena, such as mistuned harmonics in a tone complex and noise edge pitch, by strategically selecting cues from an autocorrelation function. To some degree, all models are physiologically motivated and tuned to physiological and psychophysical data that were collected during the course of this project or obtained from the literature.</span></p> <p><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">By combining models that simulate the auditory pathway using digital signal processing techniques and neural networks (Figure 2), we were able to correctly predict the lateral position of a direct signal in the presence of a room reflection and, equally importantly, also estimate the lateral position and delay of the reflection. Consequently, this model is able to provide information about the acoustic room impression a human listener gains while listening to sounds presented in a room. Another localization model creates spatial maps of potential sound source positions. By virtually turning the model's head sideways, these maps can be updated and tuned to accurately predict the azimuth and elevation of a sound by using the phenomenon of acoustical parallax - see Figure 3.</span></p> <p><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Intellectual Merit -</span><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> Our collaboration helped to advance our understanding of the fundamental brain mechanisms of auditory scene analysis. This knowledge provides new insight into an important psychophysical field and offers new technical solutions for sound-source segregation. The models we developed specifically link human performance with neuronal circuit functions in the auditory system. Therefore, they will have broad applications in sensory and cognitive neuroscience, contributing to the ultimate goal of understanding the relationship between perception and stimuli from the sensory environment. </span></p> <p><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Broader Impact -</span><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> Spoken language is our primary interpersonal communication tool. Complex acoustic scenarios with multiple sound sources present people with hearing impairments and technological systems, like speech recognition software, with extremely difficult, if not unconquerable challenges. The outcomes of this project help us to emulate the auditory system's robust mechanisms and can be used in technical applications like intelligent hearing aids. The project has provided participating students with the opportunity to work on an interdisciplinary research team bridging neurophysiology, psychophysics, and computational modeling, all while developing a strong awareness of applications in artificially intelligent listening systems.</span></p><br> <p>            Last Modified: 11/30/2019<br>      Modified by: Yi&nbsp;Zhou</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575136895680_ScreenShot2019-11-30at12.35.34PM--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575136895680_ScreenShot2019-11-30at12.35.34PM--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575136895680_ScreenShot2019-11-30at12.35.34PM--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 1: Frontal visual stimulation affects auditory localization in both frontal and rear space. (A) Stereo localization results. (B) Effects of light direction. (C) Hearing impairment increases visual capture of sound source direction.</div> <div class="imageCredit">Montagne and Zhou 2018; Venskytis et al. 2019.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Yi&nbsp;Zhou</div> <div class="imageTitle">Figure 1</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575137010721_Figure2_NSF--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575137010721_Figure2_NSF--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575137010721_Figure2_NSF--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 2: Auditory model to localize a direct signal, its reflection and the delay between the direct signal and the reflection. The model uses the BICAM signal processing framework to generate visual maps with binaural activity patterns, which are then analyzed by three deep neural networks.</div> <div class="imageCredit">Deshpande and Braasch 2019</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Yi&nbsp;Zhou</div> <div class="imageTitle">Figure 2</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575137089553_Figure3_NSF--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575137089553_Figure3_NSF--rgov-800width.jpg" title="Figure 3"><img src="/por/images/Reports/POR/2019/1539376/1539376_10389264_1575137089553_Figure3_NSF--rgov-66x44.jpg" alt="Figure 3"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 3:  Demonstration of a head-movement algorithm to estimate elevation. The initial model performance before (left) and after (right) head movement. Light areas indicate a high likelihood of estimated source position; dark brown areas indicate a low possibility that the source is present.</div> <div class="imageCredit">Braasch, 2019</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Yi&nbsp;Zhou</div> <div class="imageTitle">Figure 3</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Auditory perception of complex environments involves the neurocognitive analysis of sound features associated with many different sound sources. Auditory scene analysis, the process through which a human listener segregates sounds in a complex environment, is a difficult problem to solve, especially for hearing impaired listeners. The goal of this collaborative research between Arizona State University (Award ID: 1539376) and Rensselaer Polytechnic Institute (Award ID: 1539276) is to gain an understanding of the neural mechanisms of auditory scene analysis by integrating behavioral assessments from single-unit electrophysiology and computational modeling. We designed experiments to test the hypothesis that the auditory system makes use of the features of different sound sources to separate them from each other and from the interference of distractors in the environment. We discovered that neurons in auditory cortex respond selectively to a wide range of sound locations in both frontal and rear space. Further, we found that the effect of a background masker on the spatial selectivity of these neurons was reduced when the masker had a slightly earlier arrival time than the target, consistent with results from human studies. At the behavioral level, we observed that frontal visual stimulation can influence auditory spatial perception in both the frontal and rear space of human listeners, especially so for listeners with moderate to severe hearing loss - see Figure 1.  We developed a modular set of auditory models that can be combined to perform auditory scene analysis tasks and to robustly extract features from complex environments. The model suite includes: algorithms to perform sound-source segregation and sound-source localization in the presence of room reflections; binaural models that improve performance by utilizing head movements; as well as two pitch models. The latter can accurately predict various pitch phenomena, such as mistuned harmonics in a tone complex and noise edge pitch, by strategically selecting cues from an autocorrelation function. To some degree, all models are physiologically motivated and tuned to physiological and psychophysical data that were collected during the course of this project or obtained from the literature.  By combining models that simulate the auditory pathway using digital signal processing techniques and neural networks (Figure 2), we were able to correctly predict the lateral position of a direct signal in the presence of a room reflection and, equally importantly, also estimate the lateral position and delay of the reflection. Consequently, this model is able to provide information about the acoustic room impression a human listener gains while listening to sounds presented in a room. Another localization model creates spatial maps of potential sound source positions. By virtually turning the model's head sideways, these maps can be updated and tuned to accurately predict the azimuth and elevation of a sound by using the phenomenon of acoustical parallax - see Figure 3.  Intellectual Merit - Our collaboration helped to advance our understanding of the fundamental brain mechanisms of auditory scene analysis. This knowledge provides new insight into an important psychophysical field and offers new technical solutions for sound-source segregation. The models we developed specifically link human performance with neuronal circuit functions in the auditory system. Therefore, they will have broad applications in sensory and cognitive neuroscience, contributing to the ultimate goal of understanding the relationship between perception and stimuli from the sensory environment.   Broader Impact - Spoken language is our primary interpersonal communication tool. Complex acoustic scenarios with multiple sound sources present people with hearing impairments and technological systems, like speech recognition software, with extremely difficult, if not unconquerable challenges. The outcomes of this project help us to emulate the auditory system's robust mechanisms and can be used in technical applications like intelligent hearing aids. The project has provided participating students with the opportunity to work on an interdisciplinary research team bridging neurophysiology, psychophysics, and computational modeling, all while developing a strong awareness of applications in artificially intelligent listening systems.       Last Modified: 11/30/2019       Submitted by: Yi Zhou]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
