<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>VEC: Small: Collaborative Research: Joint Compressive Spectral Imaging and 3D Ranging Sensing Using a Commodity Time-Of-Flight Range Sensor</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>274620.00</AwardTotalIntnAmount>
<AwardAmount>274620</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The commercial Kinect input device uses both a camera and a time-of-flight depth sensor to capture 3d interactions ? initially for gaming, it has found novel applications, including autism diagnosis and allowing surgeons to control imaging in the operating room. This project aims to integrate the time-of-flight (TOF) depth with Coded Aperture Snapshot Spectral Imaging (CASSI) in a single sensor, giving 3d images not only of visible light, but extending into the infrared.  This will have advantages of improving the resolution of the depth information, allowing the two sensors to work together to improve the quality of both.  The result will be the first generation of 3D-based ?spectrophotographic? cameras for use on robotic platforms and as the heart of machine vision systems used in manufacturing and metrology.  With their higher spatial and spectral resolution, these could open new and unexplored markets. &lt;br/&gt;&lt;br/&gt;This project develops a time-of-flight, spectral imaging camera by addressing the problems of (a) novel coded aperture design and optimization under light level constraints; (b) fast compressive inverse reconstruction algorithms for real-time implementations; (c) characterization on non-ideal optical elements and calibration mechanisms, and (d) super-resolution enhancement schemes. The PIs will design and assemble a proof-of-concept prototype camera that integrates a DMD array with a CMOS, TOF image sensor in a package comparable to a pico-projector light engine. In order to mitigate the computational complexity of CS inverse image reconstruction algorithms that often preclude their use in real-time implementations, the PIs will build on their history of developing coded aperture schemes that give rise to divide-and-conquer image reconstruction algorithms that can be implemented on GPU and other multi-core processors.   This project also incorporates education activities including: (1) the development of a compressive optical imaging course with an emphasis on spectral and TOF modalities; (2) technical seminars at the annual Society of Hispanic Professional Engineers (SHPE) Conference; (3) open-source algorithms and measurement data for the scientific community.</AbstractNarration>
<MinAmdLetterDate>08/27/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/11/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1538950</AwardID>
<Investigator>
<FirstName>Gonzalo</FirstName>
<LastName>Arce</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gonzalo R Arce</PI_FULL_NAME>
<EmailAddress>arce@udel.edu</EmailAddress>
<PI_PHON>3028318030</PI_PHON>
<NSF_ID>000355622</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Delaware</Name>
<CityName>Newark</CityName>
<ZipCode>197160099</ZipCode>
<PhoneNumber>3028312136</PhoneNumber>
<StreetAddress>210 Hullihen Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<StateCode>DE</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DE00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>059007500</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF DELAWARE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>059007500</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Delaware]]></Name>
<CityName>Newark</CityName>
<StateCode>DE</StateCode>
<ZipCode>197163130</ZipCode>
<StreetAddress><![CDATA[Evans Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DE00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>002Z</Code>
<Text>Intel/NSF VEC Partnership</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~149810</FUND_OBLG>
<FUND_OBLG>2016~36025</FUND_OBLG>
<FUND_OBLG>2017~88785</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Multispectral and depth imaging both provide important capabilities in computer vision, robotics, surveillance, and virtual and/or augmented reality. &nbsp;These imaging modalities boost 3D perception-based scene understanding since better decisions are possible due to information provided by these sensors on the location of objects in the scene and their material characterization. Efforts along this path have given raise to RGB+D imaging systems such as Microsoft?s Kinect and Asus Xtion sensors, which conventionally use an RGB color sensor and a passive or active ranging sensor to estimate depth and spectral information, simultaneously. These systems, however, are limited in their spectral capabilities to only 3 channels and the imaging modalities are carried out separately via multiple sensors whose individually captured data is fused into a single image.</p> <p>This research effort developed the first of its kind, single-aperture snapshot, time-of flight and multispectral (ToF+spectral) camera, capable of capturing multispectral and depth imagery at once. It employs a ToF sensor that can measure both ambient and modulated light and a color-coded aperture (CCA) that permits the spectral encoding of ambient light while the modulated light is left unchanged. The use of a CCA is also motivated by its passivity as a static element that does not require electronics and actuation to work. Optimal spatial and spectral were found where the distribution and the spectral response of each of its pixels are optimized via blue-noise spectral shaping, so that better image reconstruction can be attained compared with random distributions. This new imager provides spectrally resolved point clouds in 3D. &nbsp;A proof-of-concept optical prototype was built in our laboratory. The testbed (Figs. 1) is composed of an 8-LED board in the visible to NIR wavelengths attached to the front lens, the CCA with optimized patterns of 3 filters, the relay lens, a custom VIS-NIR double Amici prism as the dispersive element, and the ToF sensor (ESPROS Photonics EPC660). &nbsp;Note that the filters in the aperture permit the transmission of the LED light at 800 nm. The snapshot effective measurement rate of the proposed imaging system is 31.5 frames/second with 16 spectral channels and depth estimation at the centimeter level. For testing, a dynamic target scene is illuminated with a broadband illumination lamp source, and with the modulated LED light at 12 and 24 MHz. Given the pixel count of the ToF sensor, the reconstructions exhibit N x N = 256 x 256 pixels and L = 16 spectral channels between 486 and 656 nm. Figures 2(a) and 2(b) shows the reconstruction RGB-D reconstructions when a single snapshot is taken. Similarly, Fig. 3 displays 8 out 16 monochromatic wavelengths reconstructed via TwIST algorithm. Finally, Fig. 4 shows temporal multispectral point clouds obtained by merging both spectral and shape information at different time frames.<strong> </strong>The ToF+spectral snapshot camera developed in this research effort thus provides multispectral resolved point clouds of static or dynamic scenes, which can significantly enhance visual perception and scene understanding in computer vision, robotics, and virtual and/or augmented reality. The research effort was carried by two STEM underrepresented Hispanic PhD students who where recruited to carry out the research.&nbsp; A computational imaging module covering the principles of spectral and ToF imaging was added and taught in the digital imaging course, which is taught yearly by the PI at the University of Delaware.<strong></strong></p><br> <p>            Last Modified: 03/22/2019<br>      Modified by: Gonzalo&nbsp;R&nbsp;Arce</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258123724_Figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258123724_Figure1--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258123724_Figure1--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 1: Lateral view of the designed ToF+Spectral imaging system.</div> <div class="imageCredit">Gonzalo Arce</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Gonzalo&nbsp;R&nbsp;Arce</div> <div class="imageTitle">Figure 1</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258188983_Figure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258188983_Figure2--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258188983_Figure2--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 2. (a) Depth reconstructions at 12 and 24 MHz, and (b) RGB reconstructions via three different recovery algorithms for different time frames, displayed along the rows.</div> <div class="imageCredit">Gonzalo Arce</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Gonzalo&nbsp;R&nbsp;Arce</div> <div class="imageTitle">Figure 2</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258284631_Figure3--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258284631_Figure3--rgov-800width.jpg" title="Figure 3"><img src="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258284631_Figure3--rgov-66x44.jpg" alt="Figure 3"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 3. Multispectral reconstructions of the dynamic scene using TwIST. In each subfigure, rows are different frames (1, 21, 241 and 301), and columns are different spectral bands (only 8 out of the 16 bands are shown).</div> <div class="imageCredit">Gonzalo Arce</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Gonzalo&nbsp;R&nbsp;Arce</div> <div class="imageTitle">Figure 3</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258361576_Figure4--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258361576_Figure4--rgov-800width.jpg" title="Figure 4"><img src="/por/images/Reports/POR/2019/1538950/1538950_10393127_1553258361576_Figure4--rgov-66x44.jpg" alt="Figure 4"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 4. Point clouds of the multispectral reconstructions of the dynamic scene using TwIST. In each subfigure, rows are different frames (1, 21, 241 and 301), and columns are different spectral bands (only 7 out of the 16 bands are shown).</div> <div class="imageCredit">Gonzalo Arce</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Gonzalo&nbsp;R&nbsp;Arce</div> <div class="imageTitle">Figure 4</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Multispectral and depth imaging both provide important capabilities in computer vision, robotics, surveillance, and virtual and/or augmented reality.  These imaging modalities boost 3D perception-based scene understanding since better decisions are possible due to information provided by these sensors on the location of objects in the scene and their material characterization. Efforts along this path have given raise to RGB+D imaging systems such as Microsoft?s Kinect and Asus Xtion sensors, which conventionally use an RGB color sensor and a passive or active ranging sensor to estimate depth and spectral information, simultaneously. These systems, however, are limited in their spectral capabilities to only 3 channels and the imaging modalities are carried out separately via multiple sensors whose individually captured data is fused into a single image.  This research effort developed the first of its kind, single-aperture snapshot, time-of flight and multispectral (ToF+spectral) camera, capable of capturing multispectral and depth imagery at once. It employs a ToF sensor that can measure both ambient and modulated light and a color-coded aperture (CCA) that permits the spectral encoding of ambient light while the modulated light is left unchanged. The use of a CCA is also motivated by its passivity as a static element that does not require electronics and actuation to work. Optimal spatial and spectral were found where the distribution and the spectral response of each of its pixels are optimized via blue-noise spectral shaping, so that better image reconstruction can be attained compared with random distributions. This new imager provides spectrally resolved point clouds in 3D.  A proof-of-concept optical prototype was built in our laboratory. The testbed (Figs. 1) is composed of an 8-LED board in the visible to NIR wavelengths attached to the front lens, the CCA with optimized patterns of 3 filters, the relay lens, a custom VIS-NIR double Amici prism as the dispersive element, and the ToF sensor (ESPROS Photonics EPC660).  Note that the filters in the aperture permit the transmission of the LED light at 800 nm. The snapshot effective measurement rate of the proposed imaging system is 31.5 frames/second with 16 spectral channels and depth estimation at the centimeter level. For testing, a dynamic target scene is illuminated with a broadband illumination lamp source, and with the modulated LED light at 12 and 24 MHz. Given the pixel count of the ToF sensor, the reconstructions exhibit N x N = 256 x 256 pixels and L = 16 spectral channels between 486 and 656 nm. Figures 2(a) and 2(b) shows the reconstruction RGB-D reconstructions when a single snapshot is taken. Similarly, Fig. 3 displays 8 out 16 monochromatic wavelengths reconstructed via TwIST algorithm. Finally, Fig. 4 shows temporal multispectral point clouds obtained by merging both spectral and shape information at different time frames. The ToF+spectral snapshot camera developed in this research effort thus provides multispectral resolved point clouds of static or dynamic scenes, which can significantly enhance visual perception and scene understanding in computer vision, robotics, and virtual and/or augmented reality. The research effort was carried by two STEM underrepresented Hispanic PhD students who where recruited to carry out the research.  A computational imaging module covering the principles of spectral and ToF imaging was added and taught in the digital imaging course, which is taught yearly by the PI at the University of Delaware.       Last Modified: 03/22/2019       Submitted by: Gonzalo R Arce]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
