<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: Predictability and Recoverability of High Vowel Reduction in Speech</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2015</AwardEffectiveDate>
<AwardExpirationDate>05/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>10786.00</AwardTotalIntnAmount>
<AwardAmount>10786</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Joan Maling</SignBlockName>
<PO_EMAI>jmaling@nsf.gov</PO_EMAI>
<PO_PHON>7032928046</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Human cognition relies on direct and indirect information to draw conclusions. Processes that rely on direct information are called bottom-up processes, where a conclusion is drawn based solely on the provided information; processes that rely on indirect information are called top-down processes, where elements such as context, pattern-recognition, and probability are used to form an expectation for a particular outcome. Studies have shown that a strong top-down process often blocks bottom-up processing of certain information available in the stimulus, but how the two types of processes interact is poorly understood. As an integral part of human cognition, language also shows interactions of top-down and bottom-up processes, where differences that are meaningful in one language may be readily ignored as uninformative in another. Language, therefore, is an excellent tool for shedding light on how the processing of direct and indirect information interacts in the mind.&lt;br/&gt;&lt;br/&gt;This research focuses on Japanese as a case study. The syllable structure of Japanese overwhelmingly follows a simple consonant-vowel structure. However, the process of high vowel reduction in Japanese, where unaccented vowels /i, u/ are weakened between two voiceless consonants (e.g., /suki/ &gt; [ski] 'like'), often results in the complete deletion of a vowel during speech production. However Japanese speakers often report perceiving a vowel between two consecutive voiceless consonants, even in the complete absence of a vowel (e.g., [ski] perceived as /suki/). It is unclear at this point what drives this misperception of apparently non-existent vowels, but there are two likely sources. The first is an indirect source. Given the simple syllable structure of Japanese, Japanese speakers are conditioned to expect a vowel after a consonant, in which case the misperception of a vowel would be the result of a strong top-down process. The alternative is a direct source. Because high vowel reduction is common in Japanese, Japanese speakers may be hypersensitive even to subtle acoustic information that indicates the presence of a vowel, in which case the perception of a vowel would be the result of a bottom-up process. A production experiment and a corpus study will first test what acoustic information for a weakened vowel exists in naturally produced speech. A perception experiment will then manipulate the direct and indirect information to test which of these Japanese listeners rely on more when perceiving a weakened vowel. The experimental results will inform a computational model, which investigates how a reliance on the direct and indirect cues might be learned. The experiments, together with the model, will lead to a deeper understanding of the extent in which bottom-up and top-down processes are shaped by experience both in language and more broadly in human cognition as a whole.</AbstractNarration>
<MinAmdLetterDate>05/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1524133</AwardID>
<Investigator>
<FirstName>Lisa</FirstName>
<LastName>Davidson</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lisa B Davidson</PI_FULL_NAME>
<EmailAddress>lisa.davidson@nyu.edu</EmailAddress>
<PI_PHON>2129928761</PI_PHON>
<NSF_ID>000186579</NSF_ID>
<StartDate>05/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Whang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James Whang</PI_FULL_NAME>
<EmailAddress>james.whang@nyu.edu</EmailAddress>
<PI_PHON>2129982121</PI_PHON>
<NSF_ID>000688975</NSF_ID>
<StartDate>05/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121019</ZipCode>
<StreetAddress><![CDATA[70 WASHINGTON SQUARE S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8374</Code>
<Text>DDRI Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~10786</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 4"> <div class="layoutArea"> <div class="column"> <p><span>Recoverability refers to the ease of recovering the stored mental representations of speech units, such as words and sounds, based on the actual, variable speech signals. For example, the final /t/ in the word 'cat' can be pronounced &nbsp;either with aspiration or without, but regardless, the word is perceived as the same word</span><span>. Recovery can be achieved in one of two ways: directly from the sounds in the speech signal and through prediction from the context. Using the word "cat" again as an example, a listener can recognize the word simply from listening to the sounds. A listener can also predict the word "cat" in sentences such as "The child meowed like a _____". Recoverability can be compromised when the information in the signal is insufficient (due to absence or noise) or the predictability in a given context is not high enough (e.g., "I have a ____ at home."). This project to investigated through experiments and computational modeling how&nbsp;phonotactic predictability, which is the probability of speech sounds occurring adjacent to each other ("bd" less likely than&nbsp;"bl"&nbsp;word-initially in English) affects (i) how well-enunciated individual sounds are during speech production, (ii) the attention paid to the the sounds during speech perception, and (iii) how reliance on speech cues vs. phonotactics might be learned. The language in focus is Japanese, a language well-known for its restriction against consonant clusters, and the process of high vowel reduction, which often deletes the vowels /i, u/ during production, often resulting in the very consonant clusters that violate this phonotactic restriction.</span></p> <p><span>The project comprised a production experiment as well as a perception experiment with monolingual Japanese speakers recruited in Tokyo, Japan. The results suggest that language users prioritize predictability during speech processing, paying more attention to speech cues as predictability becomes less reliable.</span></p> <p><span>A computational model was also built as part of this project, investigating how seemingly contradictory aspects of language might be acquired by an infant. The model was trained on a corpus of spoken Japanese, and had a two-tier architecture, where phonotactic (sound-to-sound probability) and word (sounds-to-meaning connection) learning mechanisms occured separately but from the same input.&nbsp;</span>The simulations showed that while the phonotactic grammar learned by the model exhibited a strong bias against consonant clusters, this bias can be overcome during speech production by allowing word-level processes to evaluate the output first. Furthermore, the model that had both mechanisms active made more accurate predictions for the experimental results than a model that had just one mechanism active, confirming the idea that phonotactic and word-level knowledge can fine tune each other.</p> <p>Beyond the realm of language, the experimental and modeing results suggest that human cognition processes information in "chunks" of varying sizes, relying on bigger chunks to get an overview first, then delving into details when necessary.</p> </div> </div> </div><br> <p>            Last Modified: 07/10/2017<br>      Modified by: James&nbsp;Whang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Recoverability refers to the ease of recovering the stored mental representations of speech units, such as words and sounds, based on the actual, variable speech signals. For example, the final /t/ in the word 'cat' can be pronounced  either with aspiration or without, but regardless, the word is perceived as the same word. Recovery can be achieved in one of two ways: directly from the sounds in the speech signal and through prediction from the context. Using the word "cat" again as an example, a listener can recognize the word simply from listening to the sounds. A listener can also predict the word "cat" in sentences such as "The child meowed like a _____". Recoverability can be compromised when the information in the signal is insufficient (due to absence or noise) or the predictability in a given context is not high enough (e.g., "I have a ____ at home."). This project to investigated through experiments and computational modeling how phonotactic predictability, which is the probability of speech sounds occurring adjacent to each other ("bd" less likely than "bl" word-initially in English) affects (i) how well-enunciated individual sounds are during speech production, (ii) the attention paid to the the sounds during speech perception, and (iii) how reliance on speech cues vs. phonotactics might be learned. The language in focus is Japanese, a language well-known for its restriction against consonant clusters, and the process of high vowel reduction, which often deletes the vowels /i, u/ during production, often resulting in the very consonant clusters that violate this phonotactic restriction.  The project comprised a production experiment as well as a perception experiment with monolingual Japanese speakers recruited in Tokyo, Japan. The results suggest that language users prioritize predictability during speech processing, paying more attention to speech cues as predictability becomes less reliable.  A computational model was also built as part of this project, investigating how seemingly contradictory aspects of language might be acquired by an infant. The model was trained on a corpus of spoken Japanese, and had a two-tier architecture, where phonotactic (sound-to-sound probability) and word (sounds-to-meaning connection) learning mechanisms occured separately but from the same input. The simulations showed that while the phonotactic grammar learned by the model exhibited a strong bias against consonant clusters, this bias can be overcome during speech production by allowing word-level processes to evaluate the output first. Furthermore, the model that had both mechanisms active made more accurate predictions for the experimental results than a model that had just one mechanism active, confirming the idea that phonotactic and word-level knowledge can fine tune each other.  Beyond the realm of language, the experimental and modeing results suggest that human cognition processes information in "chunks" of varying sizes, relying on bigger chunks to get an overview first, then delving into details when necessary.          Last Modified: 07/10/2017       Submitted by: James Whang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
