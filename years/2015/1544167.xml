<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Developing Methodological Foundations for Empirical Evaluations of Non-Experimental Methods in STEM Intervention Evaluations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>366707.00</AwardTotalIntnAmount>
<AwardAmount>366707</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field. Three types of proposals will be supported by the program: Exploratory Projects that include proof-of-concept and feasibility studies; more extensive Full-Scale Projects; and workshops and conferences. The proposed research attends carefully to item 1 above.  &lt;br/&gt;     &lt;br/&gt;This research will establish a coherent framework for the design, implementation, and analysis of within-study comparisons for evaluating non-experimental methods. It will also establish an infrastructure for conducting an ongoing quantitative synthesis of results from within-study comparison designs.&lt;br/&gt;&lt;br/&gt;This research study will develop the methodological foundations for using within-study comparison designs (WSCs) to evaluate non-experimental methods in STEM evaluation settings. In WSC designs, treatment effects from a non-experiment are compared to those produced by a randomized experiment that shares the same target population. The purpose of a WSC is to determine whether the non-experiment can replicate results from an experimental benchmark, and the contexts and conditions under which these methods perform well in field settings. The project has two overarching research aims.</AbstractNarration>
<MinAmdLetterDate>09/16/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/18/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1544167</AwardID>
<Investigator>
<FirstName>Vivian</FirstName>
<LastName>Wong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vivian Wong</PI_FULL_NAME>
<EmailAddress>vcw2n@virginia.edu</EmailAddress>
<PI_PHON>4349240768</PI_PHON>
<NSF_ID>000667529</NSF_ID>
<StartDate>09/16/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia Main Campus]]></Name>
<CityName/>
<StateCode>VA</StateCode>
<ZipCode>229044270</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>009Z</Code>
<Text>PRIME - Promoting Research and Innovatio</Text>
</ProgramReference>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~366707</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Given the widespread use of non-experimental approaches for assessing the causal impact of interventions in program evaluations, there is a strong need to identify which non-experimental methods can produce credible impact estimates in field settings. Over the last three decades, the within-study comparison design has emerged as a method to evaluate the performance of non-experimental methods in field settings. In the traditional WSC design, treatment effects from a randomized experiment are compared to those produced by a non-experimental approach that shares at least the same target population and intervention. The non-experiment may be a regression-discontinuity design, an interrupted time series design, a difference-in-differences design or a matching design. The goals of the within-study comparison are to determine whether the non-experiment can replicate results from a high-quality randomized experiment (which provides the causal benchmark estimate), and the contexts and conditions under which these methods work or do not work in practice.</p> <p><strong>Intellectual Merit</strong>. There were two over-arching aims of this project. The first was to develop the methodological foundations for the design, implementation and analysis of within-study comparisons for evaluating non-experimental methods in STEM research. To this end, the project developed theory that improves the design of within-study comparisons, including introducing and identifying the multiple purposes of WSCs, required design components, common threats to validity, and design variants and their assumptions. The project also strengthened the implementation of WSC designs for evaluating non-experimental methods in field settings. Here, we published guidelines for addressing deviations from the protocol. Finally, the project improved the analysis of WSCs -- and replication studies in general -- for assessing correspondence between two study results. Here, we described the advantages and disadvantages of each approach for evaluating correspondence in study results, emphasizing statistical power issues for conducting inference tests.</p> <p>The second aim was to establish infrastructure for conducting an ongoing quantitative synthesis of within-study comparison results to develop an empirically-based theory of "best practice" on quasi-experimental designs. To achieve this aim, we developed a database of all existing within-study comparison results. We coded within-study comparison results and characteristics of all existing studies, and quantitatively synthesized results. The quantitative synthesis focused on identifying contexts and conditions under which non-experimental methods produce causal impact estimates in STEM evaluation settings. Because research on non-experimental methods is an ongoing enterprise, we developed an infrastructure for ongoing data collection of within-study results that allows for continued exploratory analyses of non-experimental methods.&nbsp;&nbsp;</p> <p><strong>Broader Impacts and Outcomes. </strong>The project generated multiple products that have directly improved the evaluation of non-experimental methods in field settings as well as the understanding of non-experimental method performance for identifying effective interventions. First, the project resulted in a two-volume special issue on within-study comparison approaches in Evaluation Review that provided researchers with both examples of how within-study comparison approaches may be applied, as well as introduced new methodological theory for conducting high-quality evaluations of non-experimental approaches. Second, the project has yielded a database that researchers may use for providing guidance about conditions and contexts under which non-experimental methods may perform well in field settings. Currently, project staff are working with the What Works Clearinghouse to develop standards for covariate adjustment in high quality evaluation studies based on results from the meta-analytic database. Finally, the most important outcome from the project is the extension of methods development for within-study comparison approaches to systematic replication studies more generally. Methodological development from this project has been used to demonstrate requirements for the rigorous design, implementation, and analysis of replication studies. This is a critical discovery because of the current lack of consensus in the social and health sciences about what replication efforts are, and what constitutes as high-quality replication efforts. Given recent funding efforts to support and promote systematic replication studies, methodological guidance on the best ways to design, implement, and analyzing replication studies are needed. Results produced from this project have been used to guide the replication efforts for multiple large-scale studies that are currently underway in education, including the development of a crowdsourcing platform for replication results in special education, a systematic replication effort for providing coaching supports in practice-based learning in teacher education, and a replication effort to evaluate the ReadWell intervention for struggling readers. In conjunction with support from the Institute of Education Sciences, work products developed from this project have been used to create an online resource for applied researchers interested in conducting high quality within-study comparisons and systematical replication studies: <a href="https://www.edreplication.org/">https://www.edreplication.org/</a>.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/02/2021<br>      Modified by: Vivian&nbsp;Wong</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1544167/1544167_10399713_1624959781458_Picture1--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1544167/1544167_10399713_1624959781458_Picture1--rgov-800width.jpg" title="CollaboratoryReplication"><img src="/por/images/Reports/POR/2021/1544167/1544167_10399713_1624959781458_Picture1--rgov-66x44.jpg" alt="CollaboratoryReplication"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Snapshot of Collaboratory Replication website</div> <div class="imageCredit">Vivian Wong</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Vivian&nbsp;Wong</div> <div class="imageTitle">CollaboratoryReplication</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Given the widespread use of non-experimental approaches for assessing the causal impact of interventions in program evaluations, there is a strong need to identify which non-experimental methods can produce credible impact estimates in field settings. Over the last three decades, the within-study comparison design has emerged as a method to evaluate the performance of non-experimental methods in field settings. In the traditional WSC design, treatment effects from a randomized experiment are compared to those produced by a non-experimental approach that shares at least the same target population and intervention. The non-experiment may be a regression-discontinuity design, an interrupted time series design, a difference-in-differences design or a matching design. The goals of the within-study comparison are to determine whether the non-experiment can replicate results from a high-quality randomized experiment (which provides the causal benchmark estimate), and the contexts and conditions under which these methods work or do not work in practice.  Intellectual Merit. There were two over-arching aims of this project. The first was to develop the methodological foundations for the design, implementation and analysis of within-study comparisons for evaluating non-experimental methods in STEM research. To this end, the project developed theory that improves the design of within-study comparisons, including introducing and identifying the multiple purposes of WSCs, required design components, common threats to validity, and design variants and their assumptions. The project also strengthened the implementation of WSC designs for evaluating non-experimental methods in field settings. Here, we published guidelines for addressing deviations from the protocol. Finally, the project improved the analysis of WSCs -- and replication studies in general -- for assessing correspondence between two study results. Here, we described the advantages and disadvantages of each approach for evaluating correspondence in study results, emphasizing statistical power issues for conducting inference tests.  The second aim was to establish infrastructure for conducting an ongoing quantitative synthesis of within-study comparison results to develop an empirically-based theory of "best practice" on quasi-experimental designs. To achieve this aim, we developed a database of all existing within-study comparison results. We coded within-study comparison results and characteristics of all existing studies, and quantitatively synthesized results. The quantitative synthesis focused on identifying contexts and conditions under which non-experimental methods produce causal impact estimates in STEM evaluation settings. Because research on non-experimental methods is an ongoing enterprise, we developed an infrastructure for ongoing data collection of within-study results that allows for continued exploratory analyses of non-experimental methods.    Broader Impacts and Outcomes. The project generated multiple products that have directly improved the evaluation of non-experimental methods in field settings as well as the understanding of non-experimental method performance for identifying effective interventions. First, the project resulted in a two-volume special issue on within-study comparison approaches in Evaluation Review that provided researchers with both examples of how within-study comparison approaches may be applied, as well as introduced new methodological theory for conducting high-quality evaluations of non-experimental approaches. Second, the project has yielded a database that researchers may use for providing guidance about conditions and contexts under which non-experimental methods may perform well in field settings. Currently, project staff are working with the What Works Clearinghouse to develop standards for covariate adjustment in high quality evaluation studies based on results from the meta-analytic database. Finally, the most important outcome from the project is the extension of methods development for within-study comparison approaches to systematic replication studies more generally. Methodological development from this project has been used to demonstrate requirements for the rigorous design, implementation, and analysis of replication studies. This is a critical discovery because of the current lack of consensus in the social and health sciences about what replication efforts are, and what constitutes as high-quality replication efforts. Given recent funding efforts to support and promote systematic replication studies, methodological guidance on the best ways to design, implement, and analyzing replication studies are needed. Results produced from this project have been used to guide the replication efforts for multiple large-scale studies that are currently underway in education, including the development of a crowdsourcing platform for replication results in special education, a systematic replication effort for providing coaching supports in practice-based learning in teacher education, and a replication effort to evaluate the ReadWell intervention for struggling readers. In conjunction with support from the Institute of Education Sciences, work products developed from this project have been used to create an online resource for applied researchers interested in conducting high quality within-study comparisons and systematical replication studies: https://www.edreplication.org/.          Last Modified: 07/02/2021       Submitted by: Vivian Wong]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
