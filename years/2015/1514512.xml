<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Modeling rich inter-image relationships in big visual collections</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>225906.00</AwardTotalIntnAmount>
<AwardAmount>225906</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04010000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SMA</Abbreviation>
<LongName>SBE Off Of Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Josie S. Welkom</SignBlockName>
<PO_EMAI>jwelkom@nsf.gov</PO_EMAI>
<PO_PHON>7032927376</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Directorate of Social, Behavioral and Economic Sciences offers postdoctoral research fellowships to provide opportunities for recent doctoral graduates to obtain additional training, to gain research experience under the sponsorship of established scientists, and to broaden their scientific horizons beyond their undergraduate and graduate training. Postdoctoral fellowships are further designed to assist new scientists to direct their research efforts across traditional disciplinary lines and to avail themselves of unique research resources, sites, and facilities, including at foreign locations. This postdoctoral fellowship supports a rising scientist in the interdisciplinary area overlapping computer vision and psychology, with a research project that investigates the web of relationships within visual data in both humans and machines. To a human observer, no photograph is an island: it is connected to the rest of the visual world by a web of similarities, associations, and other relationships. For example, two photos of Paris share a certain similarity; images of boats are associated with images of water; a photo of a tadpole and a photo of a frog show the same organism at two stages of life. In each of these cases, a human can readily reason about the link between two images. Not only can people identify that a relationship exists, but can also identify the nature of this relationship. These relationships shed light on how the human brain organizes visual information, and also give insight into how to build intelligent systems that automatically make visual connections. The latter will bring the field closer to producing an intelligent visual web, able to organize visual information in the same way as the current Internet is able to organize text. &lt;br/&gt;&lt;br/&gt;Computer vision scientists and psychologists have both studied relationships between visual data, but from different directions. In computer vision, the focus has been on models of natural image similarity. These models handle complex stimuli but are usually limited to one simple kind of relationship, namely similarity in appearance. Psychologists have studied a richer set of relationships -  association, causation, analogy, antonymy, transformation, etc. - but their models usually only apply to simple, artificial stimuli. This project unites the best of both fields by modeling subtle visual relationships between complex, natural images. The objective is to model both which images humans consider to be related and how are those images related. An additional objective is to study how certain relationships, such as visual associations, can arise in an unsupervised manner from natural visual experience. This will help explain how humans might learn about the relationships in the first place. Better models of inter-image relationships will have deep implications across cognitive psychology. In particular, similarity and association play fundamental roles in theories of human learning and memory. A sense of similarity underlies our ability to learn from one visual experience and then apply our knowledge in a future, similar setting. The associations made from the experience additionally impact human memory of it. The present project also has applications toward computer vision systems. Reverse image search has recently become a popular tool. However, current systems are only able to retrieve look-alike images. If the computer is instead able to retrieve images linked by more diverse kinds of relationships, many possibilities open up. For example, one could imagine a system that lets users navigate through artistic styles, or that recommends shoes that match a pair of pants. If successful, this project could pave the way toward a world-wide web of visual connections that parallels the current web of hypertext connections.</AbstractNarration>
<MinAmdLetterDate>05/08/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1514512</AwardID>
<Investigator>
<FirstName>Alexei</FirstName>
<LastName>Efros</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexei A Efros</PI_FULL_NAME>
<EmailAddress>efros@eecs.berkeley.edu</EmailAddress>
<PI_PHON>5106430805</PI_PHON>
<NSF_ID>000487848</NSF_ID>
<StartDate>05/08/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Phillip</FirstName>
<LastName>Isola</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Phillip Isola</PI_FULL_NAME>
<EmailAddress>phillipi@mit.edu</EmailAddress>
<PI_PHON>6177687983</PI_PHON>
<NSF_ID>000684584</NSF_ID>
<StartDate>05/08/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947045940</ZipCode>
<StreetAddress><![CDATA[724 Sutardja Dai Hal]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8209</Code>
<Text>SPRF-IBSS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~112953</FUND_OBLG>
<FUND_OBLG>2016~112953</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 17.0px; font: 14.0px Verdana; color: #000000; -webkit-text-stroke: #000000; background-color: #ffffff} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 17.0px; font: 14.0px Verdana; color: #000000; -webkit-text-stroke: #000000; background-color: #ffffff; min-height: 17.0px} span.s1 {font-kerning: none} --> <p class="p1"><span class="s1">This project studied the problem of how to form powerful representations of the visual world. In particular, we were curious about how to relate two different visual representations of the same underlying scene. We introduced the problem of image-to-image translation, where the goal is to translate a scene represented one way, e.g., as a photograph, into a different visual representation, e.g., as a semantic layout map. Many problems in computer vision and graphics can be seen as special cases of this framework, for example, translating photos into edge maps, translating a Monet depiction of a scene into a Van Gogh painting, or translating a black and white photo into a color one. Our tack was that all these problems are just mapping pixels to pixels, so there should be a common way of solving them all.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">We embraced a data-driven, learning-based approach: given a bunch of examples of the input-output mapping, we can fit a function to approximate this mapping, which we did using convolutional neural networks (CNNs). However, training CNNs to translate between different visual representations is only half the battle. It&rsquo;s also tricky to specify to the CNN what exactly we consider a good translation and what is a bad translation. To solve this problem, we used a framework called conditional generative adversarial networks (cGANs). These are models that train a mapping such that the output aims to be indistinguishable from real data. The key is that rather than hand-defining what &ldquo;indistinguishable&rdquo; means, we use another CNN to try to distinguish the predicted outputs from real outputs. If the first CNN can fool the second, then it has achieved outputs that are &ldquo;indistinguishable&rdquo; from reality.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">We demonstrated that this framework can solve many image-to-image translation problems, without requiring any human-tuning for each individual problem. However, the approach still required having many training examples of input-output pairs. There are many interesting visual transformations that we as humans can imagine &mdash; for example, adding stripes to a horse to &ldquo;translate&rdquo; it into a zebra &mdash; but for which nature doesn&rsquo;t provide us input-output pairs. We next became interested in this human ability to relate different visual domains without ever seeing examples of a translation between the domains. To achieve this, we employed a similar setup to the cGANs used above. However, without paired input-output examples, the mapping is much less constrained, which called for adding additional constraints. Therefore, we added the constraint that the translation should be &ldquo;cycle-consistent&rdquo;: if you translate from English to French, and then back, you should arrive where you started. This allowed us to transform horses to zebras and apples to oranges, but also opens up many more practical problems, like translating between different medical imaging modalities, where paired examples are hard to come by.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 12/27/2017<br>      Modified by: Phillip&nbsp;Isola</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project studied the problem of how to form powerful representations of the visual world. In particular, we were curious about how to relate two different visual representations of the same underlying scene. We introduced the problem of image-to-image translation, where the goal is to translate a scene represented one way, e.g., as a photograph, into a different visual representation, e.g., as a semantic layout map. Many problems in computer vision and graphics can be seen as special cases of this framework, for example, translating photos into edge maps, translating a Monet depiction of a scene into a Van Gogh painting, or translating a black and white photo into a color one. Our tack was that all these problems are just mapping pixels to pixels, so there should be a common way of solving them all.   We embraced a data-driven, learning-based approach: given a bunch of examples of the input-output mapping, we can fit a function to approximate this mapping, which we did using convolutional neural networks (CNNs). However, training CNNs to translate between different visual representations is only half the battle. It?s also tricky to specify to the CNN what exactly we consider a good translation and what is a bad translation. To solve this problem, we used a framework called conditional generative adversarial networks (cGANs). These are models that train a mapping such that the output aims to be indistinguishable from real data. The key is that rather than hand-defining what "indistinguishable" means, we use another CNN to try to distinguish the predicted outputs from real outputs. If the first CNN can fool the second, then it has achieved outputs that are "indistinguishable" from reality.   We demonstrated that this framework can solve many image-to-image translation problems, without requiring any human-tuning for each individual problem. However, the approach still required having many training examples of input-output pairs. There are many interesting visual transformations that we as humans can imagine &mdash; for example, adding stripes to a horse to "translate" it into a zebra &mdash; but for which nature doesn?t provide us input-output pairs. We next became interested in this human ability to relate different visual domains without ever seeing examples of a translation between the domains. To achieve this, we employed a similar setup to the cGANs used above. However, without paired input-output examples, the mapping is much less constrained, which called for adding additional constraints. Therefore, we added the constraint that the translation should be "cycle-consistent": if you translate from English to French, and then back, you should arrive where you started. This allowed us to transform horses to zebras and apples to oranges, but also opens up many more practical problems, like translating between different medical imaging modalities, where paired examples are hard to come by.          Last Modified: 12/27/2017       Submitted by: Phillip Isola]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
