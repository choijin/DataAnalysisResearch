<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: F: Scalable Bayes Uncertainty Quantification with Guarantees</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>11/01/2015</AwardEffectiveDate>
<AwardExpirationDate>10/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>985882.00</AwardTotalIntnAmount>
<AwardAmount>985882</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei Ding</SignBlockName>
<PO_EMAI>weiding@nsf.gov</PO_EMAI>
<PO_PHON>7032928017</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Increasing volume and variety of data opens opportunities, but much of these data are not carefully curated, leading to uncertainty.  Data analysis techniques are needed that accurately characterize uncertainty.  This project develops principled approaches to managing uncertainty, particularly through clustering and subsetting data, and then combining results from analysis of the subsets.  Dividing data into smaller problems promises scalability to Big Data, while the ability to combine results in a theoretically sound manner manages the uncertainty inherent in large data collections.&lt;br/&gt;&lt;br/&gt;The key idea is that Wasserstein barycenter of subset posteriors can be used to efficiently perform posterior approximation.  The project extends the theoretical understanding of Wasserstein barycenters, enhancing ability to model uncertainty.  New mathematical tools are being developed to bound the accuracy of approximations in terms of the problem's size and nature, and computational time.  The algorithms are evaluated on a rich variety of massive data sets, ranging from large-scale networks to biomedical data sets collecting huge numbers of biomarkers.  In addition, the project provides interdisciplinary training to young talent in big data analytics to improve competitiveness of the workforce and increase the cohort of data science researchers.</AbstractNarration>
<MinAmdLetterDate>09/14/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/14/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1546130</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Mattingly</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan C Mattingly</PI_FULL_NAME>
<EmailAddress>jonm@math.duke.edu</EmailAddress>
<PI_PHON>9196602800</PI_PHON>
<NSF_ID>000253026</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Dunson</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David B Dunson</PI_FULL_NAME>
<EmailAddress>dunson@duke.edu</EmailAddress>
<PI_PHON>9192606615</PI_PHON>
<NSF_ID>000515728</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277080251</ZipCode>
<StreetAddress><![CDATA[218 Old Chemistry]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~985882</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>It is now routine in the sciences, engineering and industry to collect massive amounts of data.&nbsp;&nbsp;There is a pressing need for automated methods for using such data to further our knowledge, test scientific hypotheses and improve predictions; for example, of extreme weather events and the occurrence of disease.&nbsp;&nbsp;Unfortunately, statistical methods that can provide a reliable characterization of uncertainty in modeling of complex data tend to lack computational scalability.&nbsp;&nbsp;To address the scalability problem, it has become routine to use optimization algorithms that provide a single best guess based on the data without any characterization of uncertainty in that guess.&nbsp;&nbsp;</p> <p>The overarching aim of this project is to address this gap and to develop the theory and fundamental methodology needed to scale up methods for modeling of complex data with uncertainty quantification.&nbsp;&nbsp;The project has been extremely successful, developing a rich toolbox of new scalable algorithms along with corresponding theory supporting the performance of such algorithms.&nbsp;&nbsp;These algorithms have been published in leading scientific journals, with freely accessible versions easily available through arXiv and code for implementation provided on GitHub. Overview lectures highlighting some of the methods are available online. In addition, the project has been highly successful in training many different young researchers in the art and science of developing algorithms for scalable modeling of complex data.&nbsp;&nbsp;These talented trainees have gone on to top positions in industry and academics. &nbsp;The algorithms developed in this project are directly motivated by concrete real world problems that arise in analyzing data from a variety of scientific studies ranging from genomics to neurosciences to ecology. &nbsp;The tools are already having an important impact on the interpretation of data in these and other areas. &nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/07/2021<br>      Modified by: David&nbsp;B&nbsp;Dunson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ It is now routine in the sciences, engineering and industry to collect massive amounts of data.  There is a pressing need for automated methods for using such data to further our knowledge, test scientific hypotheses and improve predictions; for example, of extreme weather events and the occurrence of disease.  Unfortunately, statistical methods that can provide a reliable characterization of uncertainty in modeling of complex data tend to lack computational scalability.  To address the scalability problem, it has become routine to use optimization algorithms that provide a single best guess based on the data without any characterization of uncertainty in that guess.    The overarching aim of this project is to address this gap and to develop the theory and fundamental methodology needed to scale up methods for modeling of complex data with uncertainty quantification.  The project has been extremely successful, developing a rich toolbox of new scalable algorithms along with corresponding theory supporting the performance of such algorithms.  These algorithms have been published in leading scientific journals, with freely accessible versions easily available through arXiv and code for implementation provided on GitHub. Overview lectures highlighting some of the methods are available online. In addition, the project has been highly successful in training many different young researchers in the art and science of developing algorithms for scalable modeling of complex data.  These talented trainees have gone on to top positions in industry and academics.  The algorithms developed in this project are directly motivated by concrete real world problems that arise in analyzing data from a variety of scientific studies ranging from genomics to neurosciences to ecology.  The tools are already having an important impact on the interpretation of data in these and other areas.                  Last Modified: 03/07/2021       Submitted by: David B Dunson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
