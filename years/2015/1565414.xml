<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Large: Collaborative Research: Next Generation Communication Mechanisms exploiting Heterogeneity, Hierarchy and Concurrency for Emerging HPC Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>1171893.00</AwardTotalIntnAmount>
<AwardAmount>1171893</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This award was partially supported by the CIF21 Software Reuse Venture whose goals are to support pathways towards sustainable software elements through their reuse, and to emphasize the critical role of reusable software elements in a sustainable software cyberinfrastructure to support computational and data-enabled science and engineering.&lt;br/&gt;&lt;br/&gt;Parallel programming based on MPI (Message Passing Interface) is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics. The emergence of Dense Many-Core (DMC) architectures like Intel's Knights Landing (KNL) and accelerator/co-processor architectures like NVIDIA GPGPUs are enabling the design of systems with high compute density. This, coupled with the availability of Remote Direct Memory Access (RDMA)-enabled commodity networking technologies like InfiniBand, RoCE, and 10/40GigE with iWARP, is fueling the growth of multi-petaflop and ExaFlop systems. These DMC architectures have the following unique characteristics: deeper levels of hierarchical memory; revolutionary network interconnects; and heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level). &lt;br/&gt;For these emerging systems, a combination of MPI  and other programming models, known as MPI+X (where X can be PGAS, Tasks, OpenMP, OpenACC, or CUDA), are being targeted.  The current generation communication protocols and mechanisms for MPI+X programming models cannot efficiently support the emerging DMC architectures.  This leads to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next generation applications be designed/co-designed with the proposed communication mechanisms?&lt;br/&gt;&lt;br/&gt;A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions.  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on Stampede and Comet and other systems at OSC and OSU.  The proposed designs will be integrated into the widely-used MVAPICH2 library and made available for public use.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials will be organized at XSEDE, SC and other conferences to share the research results and experience with the community.</AbstractNarration>
<MinAmdLetterDate>08/04/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1565414</AwardID>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dhabaleswar K Panda</PI_FULL_NAME>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142925199</PI_PHON>
<NSF_ID>000487085</NSF_ID>
<StartDate>08/04/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Karen</FirstName>
<LastName>Tomko</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karen A Tomko</PI_FULL_NAME>
<EmailAddress>ktomko@osc.edu</EmailAddress>
<PI_PHON>6142921091</PI_PHON>
<NSF_ID>000330142</NSF_ID>
<StartDate>08/04/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hari</FirstName>
<LastName>Subramoni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hari Subramoni</PI_FULL_NAME>
<EmailAddress>subramoni.1@osu.edu</EmailAddress>
<PI_PHON>6146888735</PI_PHON>
<NSF_ID>000704577</NSF_ID>
<StartDate>08/04/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Khaled</FirstName>
<LastName>Hamidouche</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Khaled Hamidouche</PI_FULL_NAME>
<EmailAddress>hamidouche.2@osu.edu</EmailAddress>
<PI_PHON>6146888735</PI_PHON>
<NSF_ID>000731050</NSF_ID>
<StartDate>09/12/2016</StartDate>
<EndDate>08/25/2017</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName/>
<StateCode>OH</StateCode>
<ZipCode>432101206</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~1171893</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Current generation multi-petascale systems are being powered by modern multi-core architectures like Intel Cascade Lake and AMD Rome, and accelerators from NVIDIA and AMD. These systems have tight integration with Remote Direct Memory Access (RDMA) enabled high-performance networking technologies like InfiniBand, Omni-Path, and RDMA over Converged Enhanced Ethernet (RoCE). Such architectures are being defined as Dense Many-Core (DMC) systems. These systems are starkly different from homogeneous clusters of the past. These evolving DMC systems are targeted for emerging exascale computing and are characterized by: 1) Deeper levels of hierarchical memory, 2) Revolutionary network interconnects, and 3) Heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level).</p> <p>The Message Passing Interface (MPI) has been the de-facto parallel programming model for the past two decades and is very successful in implementing regular and iterative parallel algorithms with well-defined communication patterns.&nbsp; The Remote Memory Access (RMA) features of the MPI-3 standard have shown promise for expressing algorithms that have irregular computation and communication patterns by enabling light-weight one-sided communication and synchronization operations.&nbsp; On the other hand, owing to dramatic changes in the architectures (high concurrency and low memory per-core), hybrid programming models such as MPI+OpenMP and MPI+OpenACC/CUDA are being adopted as some of the primary programming models for High-Performance Computing (HPC) applications. The evolution and diversity of programming models and their hybrid usage modes for next-generation systems is being defined in a generic manner in the community as the `MPI+X' model.</p> <p>On the other hand, task-based programming models and runtimes such as the Asynchronous PGAS (APGAS) models seem to be able to achieve efficient load balancing, fault tolerance and latency hiding for highly irregular communication patterns. However, it may not be ideal to express global control flow and global communication. Thus, MPI+Task (as another form of X) has been gaining momentum in the community.&nbsp; However, designing a unified resource progression mechanism to avoid resource starvation and/or deadlocks for the MPI+Task model is opening up several research challenges due to the fundamental differences in the flow of control between the two models - MPI being user-driven control flow and APGAS/Task-based&nbsp; model using system/runtime scheduler driven control. These trends lead to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next-generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next-generation applications be designed/co-designed with the proposed communication mechanisms?</p> <p>To address the above outlined challenges, in this project, we have adopted a multi-year and multi-tiered approach to exploit emerging DMC architectures and design optimized runtimes for supporting MPI and MPI+X programming models. Challenges have been addressed along the following directions:</p> <p>1. Designing and developing high-performance, contention-aware, and scalable point-to-point and collective communication protocols and algorithms for DMC heterogeneous systems with latest generation CPUs and GPUs.</p> <p>2. Designing and developing dynamic and adaptive communication protocols for contiguous and non-contiguous data layout in MPI.</p> <p>3. Designing efficient communication and synchronization schemes for MPI+PGAS and MPI+X programming models.</p> <p>4. Carrying out in-depth study of the new designs with a range of computing and networking technologies.</p> <p>5. Co-designing a set of&nbsp; applications with the new runtimes and studied performance and scalability on a set of contemporary multi-petaflop systems.</p> <p>6. Deploying the new frameworks and runtimes on various HPC systems at Ohio Supercomputer Center (OSC), Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and carrying out continuous engagement with their users to improve and optimize the designs and deliver better performance and scalability for a large number of applications.</p> <p>The results of this research (new designs, performance results, benchmarks, etc.) have been made available to the community through the MVAPICH2 MPI&nbsp; libraries. Multiple releases of these libraries have been made during the project period. More than 400,000 copies of the MVAPICH2 MPI libraries have been downloaded from the project's web site during this project period. In each of these releases, features, performance numbers and scalability information have been shared with the MVAPICH user community through mailing lists and the project's web site.&nbsp; In addition to the software distribution, the results have been presented at various conferences and journals and events through Keynote talks, invited talks, tutorials, and hands-on sessions.&nbsp; The research has also led to thesis for several M.S. and Ph.D. students.</p><br> <p>            Last Modified: 11/28/2020<br>      Modified by: Dhabaleswar&nbsp;K&nbsp;Panda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Current generation multi-petascale systems are being powered by modern multi-core architectures like Intel Cascade Lake and AMD Rome, and accelerators from NVIDIA and AMD. These systems have tight integration with Remote Direct Memory Access (RDMA) enabled high-performance networking technologies like InfiniBand, Omni-Path, and RDMA over Converged Enhanced Ethernet (RoCE). Such architectures are being defined as Dense Many-Core (DMC) systems. These systems are starkly different from homogeneous clusters of the past. These evolving DMC systems are targeted for emerging exascale computing and are characterized by: 1) Deeper levels of hierarchical memory, 2) Revolutionary network interconnects, and 3) Heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level).  The Message Passing Interface (MPI) has been the de-facto parallel programming model for the past two decades and is very successful in implementing regular and iterative parallel algorithms with well-defined communication patterns.  The Remote Memory Access (RMA) features of the MPI-3 standard have shown promise for expressing algorithms that have irregular computation and communication patterns by enabling light-weight one-sided communication and synchronization operations.  On the other hand, owing to dramatic changes in the architectures (high concurrency and low memory per-core), hybrid programming models such as MPI+OpenMP and MPI+OpenACC/CUDA are being adopted as some of the primary programming models for High-Performance Computing (HPC) applications. The evolution and diversity of programming models and their hybrid usage modes for next-generation systems is being defined in a generic manner in the community as the `MPI+X' model.  On the other hand, task-based programming models and runtimes such as the Asynchronous PGAS (APGAS) models seem to be able to achieve efficient load balancing, fault tolerance and latency hiding for highly irregular communication patterns. However, it may not be ideal to express global control flow and global communication. Thus, MPI+Task (as another form of X) has been gaining momentum in the community.  However, designing a unified resource progression mechanism to avoid resource starvation and/or deadlocks for the MPI+Task model is opening up several research challenges due to the fundamental differences in the flow of control between the two models - MPI being user-driven control flow and APGAS/Task-based  model using system/runtime scheduler driven control. These trends lead to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next-generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next-generation applications be designed/co-designed with the proposed communication mechanisms?  To address the above outlined challenges, in this project, we have adopted a multi-year and multi-tiered approach to exploit emerging DMC architectures and design optimized runtimes for supporting MPI and MPI+X programming models. Challenges have been addressed along the following directions:  1. Designing and developing high-performance, contention-aware, and scalable point-to-point and collective communication protocols and algorithms for DMC heterogeneous systems with latest generation CPUs and GPUs.  2. Designing and developing dynamic and adaptive communication protocols for contiguous and non-contiguous data layout in MPI.  3. Designing efficient communication and synchronization schemes for MPI+PGAS and MPI+X programming models.  4. Carrying out in-depth study of the new designs with a range of computing and networking technologies.  5. Co-designing a set of  applications with the new runtimes and studied performance and scalability on a set of contemporary multi-petaflop systems.  6. Deploying the new frameworks and runtimes on various HPC systems at Ohio Supercomputer Center (OSC), Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and carrying out continuous engagement with their users to improve and optimize the designs and deliver better performance and scalability for a large number of applications.  The results of this research (new designs, performance results, benchmarks, etc.) have been made available to the community through the MVAPICH2 MPI  libraries. Multiple releases of these libraries have been made during the project period. More than 400,000 copies of the MVAPICH2 MPI libraries have been downloaded from the project's web site during this project period. In each of these releases, features, performance numbers and scalability information have been shared with the MVAPICH user community through mailing lists and the project's web site.  In addition to the software distribution, the results have been presented at various conferences and journals and events through Keynote talks, invited talks, tutorials, and hands-on sessions.  The research has also led to thesis for several M.S. and Ph.D. students.       Last Modified: 11/28/2020       Submitted by: Dhabaleswar K Panda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
