<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Intrusive Effects of Task Irrelevant Semantic Information on Visual Selective Attention</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>564472.00</AwardTotalIntnAmount>
<AwardAmount>593677</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michael Hout</SignBlockName>
<PO_EMAI>mhout@nsf.gov</PO_EMAI>
<PO_PHON>7032922163</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Selective attention involves the remarkable ability to dynamically search for and select information from the environment that is relevant for the current goal. How the human cognitive system achieves this is a fundamental question in cognitive science. Given that what humans consciously perceive will, ultimately, depend on where attention is directed, understanding attentional mechanisms is an important first step toward revealing the neural mechanisms that support conscious awareness. It has long been established that physical features, such as spatial location or object appearance, can guide attentional selection in a spatial task. These researchers will examine the extent to which attention can be inadvertently grabbed by a visual item that is irrelevant to the spatial-orienting task at hand if that item happens to be semantically related to something recently experienced. The research team will investigate hypotheses regarding the underlying mechanism by which this type of attentional intrusion occurs during the process of trying to visually locate something. To further understanding of the basic mechanism, the research team will also use neuroimaging to examine at what neural stage in the visual system this intrusion effect takes place. Understanding what grabs visual attention and why could have important practical applications, such as better designing dynamic, interactive visual displays on devices such as tablets, smart phones, or computers.  &lt;br/&gt;&lt;br/&gt;The proposed research project aims to test a set of novel predictions regarding the influence of high-level properties of a scene to attentional selection. A major goal is to rigorously test the hypothesis that task-irrelevant semantic information constrains attentional selection by directly acting on space- and object-based representations: task-irrelevant objects that semantically relate to something seen recently are more likely to be attended. Predictions will be tested using real-world scenes, and by utilizing real-world objects. In addition to the rigorous behavioral experiments aimed at understanding the mechanism by which task-irrelevant but semantically primed stimuli exert their intrusive effects on attention, the researchers will use neuroimaging methodology to examine hypotheses regarding at what stage in the human visual system the effects occur.</AbstractNarration>
<MinAmdLetterDate>08/10/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/22/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1534823</AwardID>
<Investigator>
<FirstName>Sarah</FirstName>
<LastName>Shomstein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sarah Shomstein</PI_FULL_NAME>
<EmailAddress>shom@gwu.edu</EmailAddress>
<PI_PHON>2029945957</PI_PHON>
<NSF_ID>000535750</NSF_ID>
<StartDate>08/10/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043990498</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520086</ZipCode>
<StreetAddress><![CDATA[2125 G St NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1397</Code>
<Text>Cross-Directorate  Activities</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~564472</FUND_OBLG>
<FUND_OBLG>2016~18000</FUND_OBLG>
<FUND_OBLG>2017~4761</FUND_OBLG>
<FUND_OBLG>2019~6444</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The proposed research program aimed to test a set of novel predictions regarding the influence of high-level properties of the scene to attentional selection. Our goal was to rigorously test the hypothesis that task-irrelevant semantic information constrains attentional selection by directly acting on space- and object-based representations. We directly tested the prediction that task-irrelevant semantically related objects are more likely to be attended. We asked a fundamental question whether semantic information intrudes itself on attentional control even when it is not directly relevant to the current goals of the observer. Predictions were tested in real world scenes, and by utilizing real world objects. Both, behavioral profile (with the use of various psychophysics and eye-movement techniques) as well as the neural underpinnings of this mechanism (by employing neuroimaging techniques), were examined.&nbsp; The proposed research program not only enhanced discovery but also dovetailed with numerous activities that promote teaching, training, and learning within academia, as well as the general public.</p> <p>Twelve research peer reviewed papers were published as a direct result of work performed on this grant. Across all experiments and papers we uncovered that: (a) semantic information is processed by the brain no matter whether the objects are relevant to the task at hand; (b) semantic information guides attentional selection; (c) semantic guidance influences attention withing 750 milliseconds from onset; (d) semantic information influences reallocation of attention in visual short-term memory; (e) semantics is not limited to relationships but also to knowing how large or small and object is independent of its size on the retina. The results of the grant were disseminated in multiple science outlets such as conference presentations, invited colloquia, and workshops. The work on this grant also resulted in five public lectures at local middle and high schools as part of the US Science and Engineering Festival focused on increasing interest in STEM fields. More than 10 trainees were trained under this grant, starting and facilitating careers of diverse and promising early career scientists.</p> <p>A better understanding of critical factors that determine how attentional selection is distributed in a scene could be used in various applied fields such as: design of interactive and ergonomic panels (e.g., car dashboards, instrumental panels, airplane cockpits, air traffic control monitors); enhance training programs across multiple industries ranging from training drivers and machine operators to security personnel (e.g., airport baggage screeners) to neurologists reading X-rays for evidence of cancer; and develop new beneficial rehabilitative programs for persons suffering from various attentional disorders (Attention Deficit Disorder, visuo-spatial neglect, etc.).</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/11/2021<br>      Modified by: Sarah&nbsp;Shomstein</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The proposed research program aimed to test a set of novel predictions regarding the influence of high-level properties of the scene to attentional selection. Our goal was to rigorously test the hypothesis that task-irrelevant semantic information constrains attentional selection by directly acting on space- and object-based representations. We directly tested the prediction that task-irrelevant semantically related objects are more likely to be attended. We asked a fundamental question whether semantic information intrudes itself on attentional control even when it is not directly relevant to the current goals of the observer. Predictions were tested in real world scenes, and by utilizing real world objects. Both, behavioral profile (with the use of various psychophysics and eye-movement techniques) as well as the neural underpinnings of this mechanism (by employing neuroimaging techniques), were examined.  The proposed research program not only enhanced discovery but also dovetailed with numerous activities that promote teaching, training, and learning within academia, as well as the general public.  Twelve research peer reviewed papers were published as a direct result of work performed on this grant. Across all experiments and papers we uncovered that: (a) semantic information is processed by the brain no matter whether the objects are relevant to the task at hand; (b) semantic information guides attentional selection; (c) semantic guidance influences attention withing 750 milliseconds from onset; (d) semantic information influences reallocation of attention in visual short-term memory; (e) semantics is not limited to relationships but also to knowing how large or small and object is independent of its size on the retina. The results of the grant were disseminated in multiple science outlets such as conference presentations, invited colloquia, and workshops. The work on this grant also resulted in five public lectures at local middle and high schools as part of the US Science and Engineering Festival focused on increasing interest in STEM fields. More than 10 trainees were trained under this grant, starting and facilitating careers of diverse and promising early career scientists.  A better understanding of critical factors that determine how attentional selection is distributed in a scene could be used in various applied fields such as: design of interactive and ergonomic panels (e.g., car dashboards, instrumental panels, airplane cockpits, air traffic control monitors); enhance training programs across multiple industries ranging from training drivers and machine operators to security personnel (e.g., airport baggage screeners) to neurologists reading X-rays for evidence of cancer; and develop new beneficial rehabilitative programs for persons suffering from various attentional disorders (Attention Deficit Disorder, visuo-spatial neglect, etc.).          Last Modified: 02/11/2021       Submitted by: Sarah Shomstein]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
