<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>US-German Research Proposal: Neural Dynamics of the Integration of Egocentric and Allocentric Cues in the Formation of Spatial Maps During Fully-Mobile Human Navigation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/01/2015</AwardEffectiveDate>
<AwardExpirationDate>11/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>504665.00</AwardTotalIntnAmount>
<AwardAmount>504665</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>How do we learn our way around a new city, building, or other environment? Spatial learning occurs as we integrate the visual, auditory, and other sensory impressions we gather as we move through the new environment. This collaborative project will investigate the brain dynamics of human participants as they actively navigate new types of laboratory mazes. The goal is to observe and model, for the first time, the distributed brain dynamics that support spatial learning during active human navigation. To do this, the investigators will use an original, non-invasive "mobile brain/body imaging" (MoBI) data recording approach that combines simultaneous full body motion capture and brain electrical (EEG or "brainwave") recording. Advanced signal processing methods will allow them to use non-invasively recorded scalp EEG data to follow the time courses of electrical brain activity within the cerebral cortex  as&amp;#8232;subjects actively explore computer-defined mazes. Beyond introducing new methods and software to the field of cognitive neuroscience, the project could enable improved design of living and work environments, development of new and effective approaches to improving spatial navigation abilities of children and people with spatial disabilities, guide better training for and evaluation of first responder operations, enable more effective operation of remote observation vehicles, and spur development of methods to maintain spatial orienting abilities in the elderly.&lt;br/&gt;&lt;br/&gt;The project will have three technical objectives: 1) To further develop and exploit a new computational approach to studying brain source network dynamics of freely moving individuals, 2) to observe and model transient cortical network interactions supporting physical navigation in 3-D space, and 3) to examine to what extent sensory information about the spatial structure of the physical environment is processed in brain networks that support modality-specific vs. supra-modal information processing. To accomplish these objectives, the project will capitalize on unique strengths of the collaborators and the unique facilities available in the investigators' pioneering MoBI laboratories. The project will use a novel "Sparse-AR" (perceptually sparse augmented-reality) approach in which information about the participant's spatial environment is delivered in the form of brief bursts of sound or light. The Sparse-AR approach will allow modeling of how our brain activities change as we receive and integrate new bits of evidence about unfamiliar or familiar features of the spatial environment.&lt;br/&gt;&lt;br/&gt;A companion project is being funded by the Federal Ministry of Education and Research, Germany (BMBF).</AbstractNarration>
<MinAmdLetterDate>08/27/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1516107</AwardID>
<Investigator>
<FirstName>Scott</FirstName>
<LastName>Makeig</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Scott Makeig</PI_FULL_NAME>
<EmailAddress>smakeig@ucsd.edu</EmailAddress>
<PI_PHON>8588227539</PI_PHON>
<NSF_ID>000056748</NSF_ID>
<StartDate>08/27/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Dr. #0934]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7327</Code>
<Text>CRCNS</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~504665</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-af0f3ded-7fff-2254-2550-a90b5ad276b3"> </span></p> <p dir="ltr"><span>This collaborative project investigated how the brain enables us to navigate through space, using highly innovative methods to study the brain during &lsquo;real-world&rsquo; behaviors. Past work on navigation has been limited because participants were seated in front of a computer screen, or totally immobilized in traditional brain scanners, making such studies of navigation highly artificial. We were interested in studying natural human navigation using full body movement in a wide open space. To achieve this goal, we developed a unique mobile brain/body imaging (MoBI) method that allows simultaneous recording of full-body motion and high-density recording of brain activity (using electroencephalography, or EEG, which measures brain waves). MoBI is the first brain imaging modality to allow study of &lsquo;natural cognition&rsquo; involving natural, purposive motor behaviors. This breakthrough became possible by advanced signal processing techniques that enable the reliable separation of signals from the brain from the (much larger) noise signals due to muscle, eye, and heart. We de-mixed these signals using noise reduction, followed by Independent Component Analysis.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>To investigate naturali human navigation in natural behavior, we developed a novel experiment that we call the &lsquo;audiomaze.&rsquo; The audiomaze is a virtual maze in a large 8 x 8 m room where during the experiment participants navigate with their eyes closed by &lsquo;touching&rsquo; virtual walls that returns sound feedback. Since the walls are not physically there, we used motion capture of the body position to generate sounds whenever their hand neared a virtual wall. This feedback was effective in letting people understand where walls and doors of the environment were located. This somewhat unusual maze experience in total darkness was devised to control the amount of information the participant can obtain at a time. Here, information about the world is collected one wall touch at a time. Usually, people with normal vision take in all the 3-D spatial information about their surroundings at a glance. However, to investigate brain function related to navigation, analyzing such an high-throughput information process is impossible. Rather, we need to artificially slow down the entire process, as if we use slow motion video to analyze the processes in a short moment. We termed this novel approach &lsquo;sparse augmented reality (AR)&rsquo; to transform navigation into discrete, sparse, and quantifiable element of processes which we termed &lsquo;atoms&rsquo;. The reason for the somewhat unusual experimental setup was to implement this novel approach to allow us to access the each &lsquo;atom&rsquo; of the processes with established methods. By analyzing each touch, we discovered that there are more brain regions involved during the critical moment of wall touch than had been previously understood. From previous studies using humans and animals, we expected that there should be one core center of the brain that plays an important role in navigation. In our results, we found this core, as expected, but importantly we found other brain regions in both sides of the brain that showed critical responses earlier in time. The result revealed broader network of brain dynamics in both time and space, which compels us to update the conventional idea about the brain mechanisms behind human neuroscience of navigation. To conclude, our research used highly novel approaches in real-world neuroscience, including experimental designs and brain data analysis and obtained novel findings that extend the current understanding of how the brain supports navigation in the real world.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 09/14/2019<br>      Modified by: Scott&nbsp;Makeig</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   This collaborative project investigated how the brain enables us to navigate through space, using highly innovative methods to study the brain during ?real-world? behaviors. Past work on navigation has been limited because participants were seated in front of a computer screen, or totally immobilized in traditional brain scanners, making such studies of navigation highly artificial. We were interested in studying natural human navigation using full body movement in a wide open space. To achieve this goal, we developed a unique mobile brain/body imaging (MoBI) method that allows simultaneous recording of full-body motion and high-density recording of brain activity (using electroencephalography, or EEG, which measures brain waves). MoBI is the first brain imaging modality to allow study of ?natural cognition? involving natural, purposive motor behaviors. This breakthrough became possible by advanced signal processing techniques that enable the reliable separation of signals from the brain from the (much larger) noise signals due to muscle, eye, and heart. We de-mixed these signals using noise reduction, followed by Independent Component Analysis.    To investigate naturali human navigation in natural behavior, we developed a novel experiment that we call the ?audiomaze.? The audiomaze is a virtual maze in a large 8 x 8 m room where during the experiment participants navigate with their eyes closed by ?touching? virtual walls that returns sound feedback. Since the walls are not physically there, we used motion capture of the body position to generate sounds whenever their hand neared a virtual wall. This feedback was effective in letting people understand where walls and doors of the environment were located. This somewhat unusual maze experience in total darkness was devised to control the amount of information the participant can obtain at a time. Here, information about the world is collected one wall touch at a time. Usually, people with normal vision take in all the 3-D spatial information about their surroundings at a glance. However, to investigate brain function related to navigation, analyzing such an high-throughput information process is impossible. Rather, we need to artificially slow down the entire process, as if we use slow motion video to analyze the processes in a short moment. We termed this novel approach ?sparse augmented reality (AR)? to transform navigation into discrete, sparse, and quantifiable element of processes which we termed ?atoms?. The reason for the somewhat unusual experimental setup was to implement this novel approach to allow us to access the each ?atom? of the processes with established methods. By analyzing each touch, we discovered that there are more brain regions involved during the critical moment of wall touch than had been previously understood. From previous studies using humans and animals, we expected that there should be one core center of the brain that plays an important role in navigation. In our results, we found this core, as expected, but importantly we found other brain regions in both sides of the brain that showed critical responses earlier in time. The result revealed broader network of brain dynamics in both time and space, which compels us to update the conventional idea about the brain mechanisms behind human neuroscience of navigation. To conclude, our research used highly novel approaches in real-world neuroscience, including experimental designs and brain data analysis and obtained novel findings that extend the current understanding of how the brain supports navigation in the real world.          Last Modified: 09/14/2019       Submitted by: Scott Makeig]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
