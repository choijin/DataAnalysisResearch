<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Breakthrough: Collaborative Research: The Interweaving of Humans and Physical Systems: A Perspective From Power Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Corman</SignBlockName>
<PO_EMAI>dcorman@nsf.gov</PO_EMAI>
<PO_PHON>7032928754</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As information technology has transformed physical systems such as the power grid, the interface between these systems and their human users has become both richer and much more complex. For example, from the perspective of an electricity consumer, a whole host of devices and technologies are transforming how they interact with the grid: demand response programs; electric vehicles; "smart" thermostats and appliances; etc. These novel technologies are also forcing us to rethink how the grid interacts with its users, because critical objectives such as stability and robustness require effective integration among the many diverse users in the grid. This project studies the complex interweaving of humans and physical systems. Traditionally, a separation principle has been used to isolate humans from physical systems. This principle requires users to have preferences that are well-defined, stable, and quickly discoverable. These assumptions are increasingly violated in practice: users' preferences are often not well-defined; unstable over time; and take time to discover. Our project articulates a new framework for interactions between physical systems and their users, where users' preferences must be repeatedly learned over time while the system continually operates with respect to imperfect preference information.&lt;br/&gt;&lt;br/&gt;We focus on the area of power systems. Our project has three main thrusts. First, user models are rethought to reflect the fact this new dynamic view of user preferences, where even the users are learning over time. The second thrust focuses on developing a new system model that learns about users, since we cannot understand users in a "single-shot"; rather, repeated interaction with the user is required. We then focus on the integration of these two new models. How do we control and operate a physical system, in the presence of the interacting "learning loops", while mediating between many competing users? We apply ideas from mean field games and optimal power flow to capture, analyze, and transform the interaction between the system and the ongoing preference discovery process. Our methods will yield guidance for market design in power systems where user preferences are constantly evolving. If successful, our project will usher in a fundamental change in interfacing physical systems and users. For example, in the power grid, our project directly impacts how utilities design demand response programs; how smart devices learn from users; and how the smart grid operates. In support of this goal, the PIs intend to develop avenues for knowledge transfer through interactions with industry. The PIs will also change their education programs to reflect a greater entanglement between physical systems and users.</AbstractNarration>
<MinAmdLetterDate>09/15/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1544548</AwardID>
<Investigator>
<FirstName>Ramesh</FirstName>
<LastName>Johari</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ramesh Johari</PI_FULL_NAME>
<EmailAddress>rjohari@stanford.edu</EmailAddress>
<PI_PHON>6507230937</PI_PHON>
<NSF_ID>000058591</NSF_ID>
<StartDate>09/15/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943054121</ZipCode>
<StreetAddress><![CDATA[Huang Eng. Ctr., 475 Via Ortega]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7918</Code>
<Text>CPS-Cyber-Physical Systems</Text>
</ProgramElement>
<ProgramReference>
<Code>7918</Code>
<Text>CYBER-PHYSICAL SYSTEMS (CPS)</Text>
</ProgramReference>
<ProgramReference>
<Code>8234</Code>
<Text>CPS-Breakthrough</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span> </span>Every day, algorithmically-powered platforms influence the choices we make. &nbsp;For example, suppose that you decide to enroll in the demand response program offered by your local power utility. &nbsp;Such a program typically offers you discounts on your utility bill, in return for giving up some level of control to the utility to alter the use pattern of major appliances within your household. &nbsp;You might grant the utility some control over exactly what times the washer/dryer is used, or over what times the air conditioner is allowed to turn on. &nbsp;These programs afford the utility greater control over the load generated on the power grid, and particularly at times of acute stress, can help prevent power outages and other adverse events. &nbsp;Of course, once control has been granted to the platform, it relies on complex algorithms to decide how to make these choices. &nbsp;These algorithms must *learn* to make good decisions over time, based on their observations of the outcomes realized from the choices they make in the past.<br />&nbsp; &nbsp; &nbsp; &nbsp; An extensive literature details the design of good learning algorithms for such problems. &nbsp;However, this literature largely ignores an important element of the system above: *the human user*! &nbsp;The algorithmic frameworks within which methods are proposed typically assume a "passive" user, who does not respond in any meaningful way to the decisions being made. &nbsp;In reality, of course, the human users of platforms respond in many and nuanced ways to the actions the platforms takes. &nbsp;For example, suppose that the utility in the example above takes actions that are too extreme regarding the appliances of the household (e.g., delaying the laundry by more than the household finds tolerable). &nbsp;In this case, the user may choose to completely *abandon* the demand response program. &nbsp;Therefore, in learning how to behave optimally, the platform must remain wary that it does not overplay its hand; this constraint --- that the user might abandon -- fundamentally alters the learning problem faced by the platform.<br />&nbsp; &nbsp; A major contribution of this project is to develop a framework for reasoning about the interaction of algorithmic platforms with human users, in contexts such as those described above. &nbsp;We briefly describe two such contributions achieved in the context of this project. &nbsp;First, in work with Sven Schmit (a Ph.D.~student supported by the project), we studied how to optimally choose actions when users may abandon, as in the demand response example above. &nbsp;Second, in work with Virag Shah (a postdoctoral scholar supported by the project) and Jose Blanchet, we studied how to optimally learn in the presence of *network externalities*, or peer effects. &nbsp;Peer effects arise when users are more likely to use a service that previously yielded positive experiences for other users like them (e.g., their friends). &nbsp;Again, peer effects fundamentally change the learning problem for the platform, since the future population will be influenced by current decisions. &nbsp;Importantly, in both pieces of work, we find that the optimal policy can look substantially different than the optimal policy in the absence of such behavioral responses. &nbsp;These two pieces of work were each published and presented in premier venues for machine learning research. &nbsp;<br />&nbsp; &nbsp; &nbsp;These examples highlight the importance of considering human behavioral responses in the design of algorithmic systems. &nbsp;Increasingly, as the interface between humans and algorithmic systems becomes richer and more complex, it will be imperative for researchers to develop insights such as those found in this project. &nbsp;We view the results of this project as a significant early step in this direction.</p><br> <p>            Last Modified: 01/14/2020<br>      Modified by: Ramesh&nbsp;Johari</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Every day, algorithmically-powered platforms influence the choices we make.  For example, suppose that you decide to enroll in the demand response program offered by your local power utility.  Such a program typically offers you discounts on your utility bill, in return for giving up some level of control to the utility to alter the use pattern of major appliances within your household.  You might grant the utility some control over exactly what times the washer/dryer is used, or over what times the air conditioner is allowed to turn on.  These programs afford the utility greater control over the load generated on the power grid, and particularly at times of acute stress, can help prevent power outages and other adverse events.  Of course, once control has been granted to the platform, it relies on complex algorithms to decide how to make these choices.  These algorithms must *learn* to make good decisions over time, based on their observations of the outcomes realized from the choices they make in the past.         An extensive literature details the design of good learning algorithms for such problems.  However, this literature largely ignores an important element of the system above: *the human user*!  The algorithmic frameworks within which methods are proposed typically assume a "passive" user, who does not respond in any meaningful way to the decisions being made.  In reality, of course, the human users of platforms respond in many and nuanced ways to the actions the platforms takes.  For example, suppose that the utility in the example above takes actions that are too extreme regarding the appliances of the household (e.g., delaying the laundry by more than the household finds tolerable).  In this case, the user may choose to completely *abandon* the demand response program.  Therefore, in learning how to behave optimally, the platform must remain wary that it does not overplay its hand; this constraint --- that the user might abandon -- fundamentally alters the learning problem faced by the platform.     A major contribution of this project is to develop a framework for reasoning about the interaction of algorithmic platforms with human users, in contexts such as those described above.  We briefly describe two such contributions achieved in the context of this project.  First, in work with Sven Schmit (a Ph.D.~student supported by the project), we studied how to optimally choose actions when users may abandon, as in the demand response example above.  Second, in work with Virag Shah (a postdoctoral scholar supported by the project) and Jose Blanchet, we studied how to optimally learn in the presence of *network externalities*, or peer effects.  Peer effects arise when users are more likely to use a service that previously yielded positive experiences for other users like them (e.g., their friends).  Again, peer effects fundamentally change the learning problem for the platform, since the future population will be influenced by current decisions.  Importantly, in both pieces of work, we find that the optimal policy can look substantially different than the optimal policy in the absence of such behavioral responses.  These two pieces of work were each published and presented in premier venues for machine learning research.        These examples highlight the importance of considering human behavioral responses in the design of algorithmic systems.  Increasingly, as the interface between humans and algorithmic systems becomes richer and more complex, it will be imperative for researchers to develop insights such as those found in this project.  We view the results of this project as a significant early step in this direction.       Last Modified: 01/14/2020       Submitted by: Ramesh Johari]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
