<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SI2-SSE: EASE: Improving Research Accountability through Artifact Evaluation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>499515.00</AwardTotalIntnAmount>
<AwardAmount>499515</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Research in computer systems, particularly in the first stages of creating a new innovation, relies almost exclusively on software prototypes, simulators, benchmarks, and data sets to understand the benefits and costs of new ideas in computers, ranging from consumer devices to exascale systems. These artifacts are used to evaluate new capabilities, algorithms, bottlenecks and trade-offs. Empirical study is behind the rapid pace of innovation in creating faster, lower energy and more reliable systems. This experimental approach lies at the core of development that fuels the nation's information economy.  Given the critical importance of experimental study to developing new computer systems, several efforts are underway to curate experimental results through accountable research.  One effort, Artifact Evaluation (AE), is being adopted to promote high quality artifacts  and experimentation, including making public the experimental information necessary for reproducibility. However, the rapid adoption of AE is hampered by technical challenges that create a high barrier to the process: there is no consistent or simple environment, or mechanism, to package and reproduce experiments for AE. Authors rely on their own approaches, leading to much time consumed, as well as considerable variability in the ways materials are prepared and evaluated, unnecessarily obstructing the AE process.  &lt;br/&gt;&lt;br/&gt;To overcome the technical challenges with AE, and to more broadly encourage adoption of AE in computer science and engineering research, this project is developing a software infrastructure, Experiment and Artifact System for Evaluation (EASE), to create and run experiments specifically for AE, in which authors create, conduct and share artifacts and experiments. It allows for repeating,  modifying, and extending experiments. Authors may also use EASE to  package and upload their experiments for archival storage in a digital library. EASE is being developed and deployed for two use cases, namely compilers and real-time systems, keeping the project tractable to address specific needs. These communities have overlapping but also distinct requirements, helping to ensure EASE can also be extended and&lt;br/&gt;used by other computer systems research communities as well.&lt;br/&gt;&lt;br/&gt;EASE will be release as open source software, based on an Experiment Management System (EMS) previously developed by the project investigator in a project call Open Curation for Computer Architecture Modeling (OCCAM), used to define and conduct experiments using computer architecture simulators. Using EMS as a starting point, EASE will provide AE support, by: 1) separating EMS from OCCAM's repository and hardware services, transforming the EMS infrastructure into EASE, a fully standalone, sustainable, and extensible platform for AE; 2) supporting record and replay (for repeating and reproducing results, as well as provenance) of artifacts and experiments as part of normal development and experimental practice to ease participation in AE by authors and evaluators; 3) supporting artifacts,workflows of artifacts and experiments that run directly on a machine, including specialized hardware and software, and run indirectly on a simulator or emulator; 4) allowing both user-level (artifacts and experiments as user processes) and system-level (artifacts and experiments involving kernel changes) innovations; 5) providing consistent/uniform access, whether locally or remotely, to artifacts and experiments; 6) simplifying viewing, running, modifying, and comparing experiments by innovators (i.e., during innovation development), artifact evaluators (during AE), and archive users (after publication); 7) enabling indexing (object locators and search tags) and packaging of artifacts and experiments for AE and for archival deployment (e.g., to ACM?s or IEEE?s Digital Library); and 8) refining, expanding, generalizing, and documenting EASE to ensure it is robust, maintainable and extensible, and that it can be used and sustained by different CSR communities (starting with real-time and compilers, given their different artifacts, data and methods).</AbstractNarration>
<MinAmdLetterDate>06/16/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/16/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1535232</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Mosse'</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Mosse'</PI_FULL_NAME>
<EmailAddress>mosse@cs.pitt.edu</EmailAddress>
<PI_PHON>4126248923</PI_PHON>
<NSF_ID>000229674</NSF_ID>
<StartDate>06/16/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bruce</FirstName>
<LastName>Childers</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bruce Childers</PI_FULL_NAME>
<EmailAddress>childers@cs.pitt.edu</EmailAddress>
<PI_PHON>4126248421</PI_PHON>
<NSF_ID>000245880</NSF_ID>
<StartDate>06/16/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pittsburgh</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152133203</ZipCode>
<PhoneNumber>4126247400</PhoneNumber>
<StreetAddress>300 Murdoch Building</StreetAddress>
<StreetAddress2><![CDATA[3420 Forbes Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004514360</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF PITTSBURGH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004514360</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pittsburgh]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152132303</ZipCode>
<StreetAddress><![CDATA[123 University Club]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8005</Code>
<Text>Scientific Software Elements</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~499515</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Research in computer systems, particularly in the first stages of creating a new innovation, relies on simulators, benchmarks, and data sets to analyze the benefits and costs of new ideas in computers. These artifacts are used to evaluate capabilities, algorithms, bottlenecks and trade-offs through empirical study. Given the critical importance of experimental study to developing new computer systems, several efforts are underway to curate experimental results. One effort, Artifact Evaluation (AE), promotes high quality artifacts and experimentation, including making public the experimental information necessary for reproducibility. However, AE has been hampered by technical challenges that create a high barrier to the process: there are few consistent or simple mechanisms to package and reproduce experiments for AE. Authors often use their own approaches, leading to much time consumed and variability in the ways materials are prepared and evaluated.</p> <p>To overcome these challenges and to encourage adoption of AE, this project developed a software infrastructure, Experiment and Artifact System for Evaluation (EASE), to create and run experiments specifically for peer review of experimental artifacts. Using this software infrastructure, authors can create, conduct and share artifacts and experiments for AE. Peer evaluators can repeat, modify, and extend experiments for AE. The EASE project levaraged and significantly built on the base OCCAM software by fully redesigning and refactoring the original system to make it standalone, sustainable and extensible for AE. The user interface was redesigned to simplify describing and running experiments by authors, and sharing them with evaluators for artifact evaluation. The interface was designed to ease the effort required by evaluators to conduct peer review of experiments. Social capabilities to comment on experiments and share the comments between authors and evaluators were examined to facilitate interactivity to simplify evaluation of experiments. Functionality to better control access to experiments, required for AE, was also incorporated. The software is open source and publicly available, with continued development and maintenance by a core set of developers.</p> <p>As a result of this project, we found that the computer systems community is gradual in moving to using workflow systems, like EASE, for artifact evaluation. Although the need for standardization is well recognized and many researchers have commented that EASE and similar tools are beneficial, simple, and powerful, the community tends to "roll their own" with packaging technologies, like virtual machines (VMs) and containers, to share artifacts for AE. This may be partly due to tradition -- after all, this is the very community that developed these lower-level technologies and is most familiar with them. Advances in VM and container technologies over the last few years (driven largely by requirements for cloud computing) have made these technologies easier to use for AE.&nbsp;Other shortcomings of AE include the disparity in languages used with respect to the implementations and proprietary software.</p> <p>Although moving to standard tools for AE is gradual, there is much interest in adopting end-to-end workflow software, like EASE, to support Artifact Evaluation. Interestingly, scientific communities that are less familiar with VM and container technologies are more quickly moving toward using workflow systems for reproducibility, reuse and supporting peer review of experiments, data and software artifacts. Indeed, through this project, we found that science communities outside of computer systems, such as world modeling and epidemiology, are eager to adopt EASE and similar tools for these purposes.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/11/2020<br>      Modified by: Bruce&nbsp;Childers</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Research in computer systems, particularly in the first stages of creating a new innovation, relies on simulators, benchmarks, and data sets to analyze the benefits and costs of new ideas in computers. These artifacts are used to evaluate capabilities, algorithms, bottlenecks and trade-offs through empirical study. Given the critical importance of experimental study to developing new computer systems, several efforts are underway to curate experimental results. One effort, Artifact Evaluation (AE), promotes high quality artifacts and experimentation, including making public the experimental information necessary for reproducibility. However, AE has been hampered by technical challenges that create a high barrier to the process: there are few consistent or simple mechanisms to package and reproduce experiments for AE. Authors often use their own approaches, leading to much time consumed and variability in the ways materials are prepared and evaluated.  To overcome these challenges and to encourage adoption of AE, this project developed a software infrastructure, Experiment and Artifact System for Evaluation (EASE), to create and run experiments specifically for peer review of experimental artifacts. Using this software infrastructure, authors can create, conduct and share artifacts and experiments for AE. Peer evaluators can repeat, modify, and extend experiments for AE. The EASE project levaraged and significantly built on the base OCCAM software by fully redesigning and refactoring the original system to make it standalone, sustainable and extensible for AE. The user interface was redesigned to simplify describing and running experiments by authors, and sharing them with evaluators for artifact evaluation. The interface was designed to ease the effort required by evaluators to conduct peer review of experiments. Social capabilities to comment on experiments and share the comments between authors and evaluators were examined to facilitate interactivity to simplify evaluation of experiments. Functionality to better control access to experiments, required for AE, was also incorporated. The software is open source and publicly available, with continued development and maintenance by a core set of developers.  As a result of this project, we found that the computer systems community is gradual in moving to using workflow systems, like EASE, for artifact evaluation. Although the need for standardization is well recognized and many researchers have commented that EASE and similar tools are beneficial, simple, and powerful, the community tends to "roll their own" with packaging technologies, like virtual machines (VMs) and containers, to share artifacts for AE. This may be partly due to tradition -- after all, this is the very community that developed these lower-level technologies and is most familiar with them. Advances in VM and container technologies over the last few years (driven largely by requirements for cloud computing) have made these technologies easier to use for AE. Other shortcomings of AE include the disparity in languages used with respect to the implementations and proprietary software.  Although moving to standard tools for AE is gradual, there is much interest in adopting end-to-end workflow software, like EASE, to support Artifact Evaluation. Interestingly, scientific communities that are less familiar with VM and container technologies are more quickly moving toward using workflow systems for reproducibility, reuse and supporting peer review of experiments, data and software artifacts. Indeed, through this project, we found that science communities outside of computer systems, such as world modeling and epidemiology, are eager to adopt EASE and similar tools for these purposes.              Last Modified: 01/11/2020       Submitted by: Bruce Childers]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
