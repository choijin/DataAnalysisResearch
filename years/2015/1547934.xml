<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: GOALI: Distributed Embedded Vision System for Multi-Unmanned Ground Vehicle Coordination in Indoor Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>273557.00</AwardTotalIntnAmount>
<AwardAmount>305557</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This collaborative project between academics and industry provides a configurable and portable vision-based infrastructure for decentralized coordination of industrial trucks, also known as Automated Guided Vehicles (AGVs) in indoor environments. AGVs have the potential to revolutionize operations in areas such as manufacturing and distribution, health care, and military by efficiently accomplishing the mundane and often repetitive task of transporting materials in distribution and manufacturing. The research addresses limitations of recent successful introduction of commercial robot systems in distribution centers and manufacturing to transport items, but typically only to specific and restrictive environments.  The research extends these technologies to environments that are quickly reconfigured by moving stations and objects around. Generic models, tools, and technologies are developed to actively capture the world with semantically labeled objects, actions and events, and to generate goals, priorities, and plans. &lt;br/&gt;&lt;br/&gt;The solutions proposed in this research address the requirements of decentralized coordination, real-time environmental changes. The approach uses a set of distributed ceiling-mounted smart cameras with overlapping field-of-view for global view and coordination at the facility level, and cameras mounted on AGVs for short-range truck navigation. Multi-truck coordination is then framed as the problem of routing packets in a dynamic and hierarchical network where cameras represent routers and trucks represent packets. To address the complexity of image processing tasks, the hardware/software implementation utilizes an FPGA-based target platform from a previous NSF-Funded project, where low-level inherent parallel image processing tasks are mapped onto hardware while high-level reasoning is kept in software. This project will provide an interface formalism to specify component integration and system composition along with methods to optimize run-time and resource usage. &lt;br/&gt;&lt;br/&gt;This research is expected to produce an infrastructure and methods for decentralized vision-based coordination of autonomous vehicles activities in a dynamically changing indoor environment. The developed infrastructure will enable manufacturing and distribution companies, including lean and agile entities, to optimize indoor transportation activities in existing arrangement, without modification of available infrastructure, and reduce labor and operating costs by redeploying employees to value-added roles. In addition, AGVs have the potential to enable autonomous mobile robot applications in numerous other unstructured environments, including hospitals, malls, retail stores, critical infrastructure, airports, schools, and sports venues. The project is conducted as a joint effort between the University of Arkansas in Fayetteville and R-Dex in Atlanta and will provide undergraduate and graduate students opportunities to perform their work in academic and industrial environments. The involvement of under-represented groups will be increased with the support of the University of Arkansas Engineering Career Awareness Program, and the University of Arkansas chapters of the National Society of Black Engineer (NSBE) and the Society of Hispanic Professional Engineers (SHPE).</AbstractNarration>
<MinAmdLetterDate>08/20/2015</MinAmdLetterDate>
<MaxAmdLetterDate>10/18/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1547934</AwardID>
<Investigator>
<FirstName>Christophe</FirstName>
<LastName>Bobda</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christophe Bobda</PI_FULL_NAME>
<EmailAddress>cbobda@ufl.edu</EmailAddress>
<PI_PHON>3523929267</PI_PHON>
<NSF_ID>000583623</NSF_ID>
<StartDate>08/20/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Bock</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert D Bock</PI_FULL_NAME>
<EmailAddress>robert@r-dex.com</EmailAddress>
<PI_PHON>6786412380</PI_PHON>
<NSF_ID>000669903</NSF_ID>
<StartDate>08/20/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arkansas</Name>
<CityName>Fayetteville</CityName>
<ZipCode>727013124</ZipCode>
<PhoneNumber>4795753845</PhoneNumber>
<StreetAddress>1125 W. Maple Street</StreetAddress>
<StreetAddress2><![CDATA[316 Administration Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arkansas</StateName>
<StateCode>AR</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AR03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>191429745</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARKANSAS SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>055600001</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arkansas]]></Name>
<CityName>Fayetteville</CityName>
<StateCode>AR</StateCode>
<ZipCode>727011201</ZipCode>
<StreetAddress><![CDATA[1 University of Arkansas]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arkansas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AR03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1504</Code>
<Text>GOALI-Grnt Opp Acad Lia wIndus</Text>
</ProgramElement>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>1504</Code>
<Text>GRANT OPP FOR ACAD LIA W/INDUS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~273557</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<FUND_OBLG>2018~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has investigated the design and deployment of a network of ceiling-mounted distributed smart cameras for the guidance of indoor automated ground vehicles (AGV), in applications such as distribution and manufacturing.</p> <p>Robots on the ground transport items from one point the next and receive guidance from a set of ceiling-mounted cameras. The problem of guiding a robot from a starting point to a destination is formulated as a hierarchical packet routing problem, where the robots are the packets and cameras act as routers. Whenever a robot enters the field of view of a camera, it is recognized by that camera that reads information regarding the robots from the labeling on the robot. The camera then identifies the next neighbor to send the robot toward and compute a route within its field of view. This route is extracted from the image underneath the robot. The highest level of the hierarchy in the routing is made of the cameras which act as routers. The lower level is within the area covered by a camera field of view. Each robot is responsible for driving the path and using traditional methods of collision detection such as depth camera and possibly LiDAR. The computation of pathways within the field of view of a camera is done using an image taken from that camera. Having computed the pathway, the path planning is done using a novel algorithm call D-A*, which is a modified version of A* suitable to plan a path across a set of decentralized, connected parent nodes. In our target environment, we have a set of parent nodes creating a path through their children which the robots will follow.</p> <p>Our approach was prototyped on a setup with a dozen robots including the Arlo Complete Robotic System by Parallax which includes a frame, motors, wheels, optical encoders, a power distribution board, a motor controller, and a Parallax Activity Board. The computing platform was replace the Activity Board with a much more capable FPGA SoC for acceleration of image processing on board, running Linux along with the middleware Robotic Operating System (ROS) for integration of cameras and robots. In addition to the camera on the ceiling and the Intel Real Sense stereo camera for depth computation, a low-cost LiDAR device has been integrated into the robot for navigation and map building.</p> <p>This project has demonstrated a novel direction in the guidance of indoor robots, using a noninvasive technology that does not require changes in the environment such as tracks on the floor. This achievement makes it possible for factories with high frequencies in reconfiguration to deploy robots that can perform the mundane tasks of moving object autonomously. A further benefit of the distributed smart camera network has been identified for applications in healthcare, where those cameras could be used for behavioral understanding in home setups for fall prevention while preserving the privacy of patients. In manufacturing where labor is an issue, the network can be used for better safety monitoring and better coordination of activities in a multi-robot-human work cell. Safety is a huge problem in construction and real-time safety monitoring and assessment can be efficiently achieved with a network of distributed smart cameras such as the one designed and deployed in this research.</p> <p>The work in this project has had a major impact in the field of autonomous robots and human-robot collaboration, as well as health care. The PI, in collaboration with colleagues from industrial engineering recently, received a DURIP grant to set up an infrastructure using the smart camera network for coordination in work cell where human and robot collaboratively perform various tasks. The PI has also received an REU that has been useful to the involved undergraduate students in research on robotics and machine learning for behavioral understanding. The use of multiple camera enhance the process of map building in robot navigation, particularly in large area where the construction of a map using LiDAR and SLAM methodologies is very time consuming and inefficient in dynamic environment.</p> <p>The project has led to the design of a new class autonomous mobile robots and motivated the creation of an innovative lab at the University of Arkansas to allow freshman students to independently conduct development of student projects in robotic.</p> <p>In regard to technology transfer, the project has led to half a dozen of publications, a capstone best project award, and has helped in designing a new class in autonomous mobile robots at the University of Arkansas. Furthermore, the main ideas of the project have led to a patent ("Distributed ceiling-mounted smart cameras for multi-unmanned ground vehicle routing and coordination&rdquo; Patent number: 10191495). The principal investigator and one of the students involved in the project are currently working with the project collaborator Dr. Robert Bock toward the commercialization of project results.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/10/2019<br>      Modified by: Christophe&nbsp;Bobda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has investigated the design and deployment of a network of ceiling-mounted distributed smart cameras for the guidance of indoor automated ground vehicles (AGV), in applications such as distribution and manufacturing.  Robots on the ground transport items from one point the next and receive guidance from a set of ceiling-mounted cameras. The problem of guiding a robot from a starting point to a destination is formulated as a hierarchical packet routing problem, where the robots are the packets and cameras act as routers. Whenever a robot enters the field of view of a camera, it is recognized by that camera that reads information regarding the robots from the labeling on the robot. The camera then identifies the next neighbor to send the robot toward and compute a route within its field of view. This route is extracted from the image underneath the robot. The highest level of the hierarchy in the routing is made of the cameras which act as routers. The lower level is within the area covered by a camera field of view. Each robot is responsible for driving the path and using traditional methods of collision detection such as depth camera and possibly LiDAR. The computation of pathways within the field of view of a camera is done using an image taken from that camera. Having computed the pathway, the path planning is done using a novel algorithm call D-A*, which is a modified version of A* suitable to plan a path across a set of decentralized, connected parent nodes. In our target environment, we have a set of parent nodes creating a path through their children which the robots will follow.  Our approach was prototyped on a setup with a dozen robots including the Arlo Complete Robotic System by Parallax which includes a frame, motors, wheels, optical encoders, a power distribution board, a motor controller, and a Parallax Activity Board. The computing platform was replace the Activity Board with a much more capable FPGA SoC for acceleration of image processing on board, running Linux along with the middleware Robotic Operating System (ROS) for integration of cameras and robots. In addition to the camera on the ceiling and the Intel Real Sense stereo camera for depth computation, a low-cost LiDAR device has been integrated into the robot for navigation and map building.  This project has demonstrated a novel direction in the guidance of indoor robots, using a noninvasive technology that does not require changes in the environment such as tracks on the floor. This achievement makes it possible for factories with high frequencies in reconfiguration to deploy robots that can perform the mundane tasks of moving object autonomously. A further benefit of the distributed smart camera network has been identified for applications in healthcare, where those cameras could be used for behavioral understanding in home setups for fall prevention while preserving the privacy of patients. In manufacturing where labor is an issue, the network can be used for better safety monitoring and better coordination of activities in a multi-robot-human work cell. Safety is a huge problem in construction and real-time safety monitoring and assessment can be efficiently achieved with a network of distributed smart cameras such as the one designed and deployed in this research.  The work in this project has had a major impact in the field of autonomous robots and human-robot collaboration, as well as health care. The PI, in collaboration with colleagues from industrial engineering recently, received a DURIP grant to set up an infrastructure using the smart camera network for coordination in work cell where human and robot collaboratively perform various tasks. The PI has also received an REU that has been useful to the involved undergraduate students in research on robotics and machine learning for behavioral understanding. The use of multiple camera enhance the process of map building in robot navigation, particularly in large area where the construction of a map using LiDAR and SLAM methodologies is very time consuming and inefficient in dynamic environment.  The project has led to the design of a new class autonomous mobile robots and motivated the creation of an innovative lab at the University of Arkansas to allow freshman students to independently conduct development of student projects in robotic.  In regard to technology transfer, the project has led to half a dozen of publications, a capstone best project award, and has helped in designing a new class in autonomous mobile robots at the University of Arkansas. Furthermore, the main ideas of the project have led to a patent ("Distributed ceiling-mounted smart cameras for multi-unmanned ground vehicle routing and coordination" Patent number: 10191495). The principal investigator and one of the students involved in the project are currently working with the project collaborator Dr. Robert Bock toward the commercialization of project results.           Last Modified: 03/10/2019       Submitted by: Christophe Bobda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
