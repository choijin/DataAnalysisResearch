<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII:  CHS:  Perceptual Data-Guided Computational Design</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>173006.00</AwardTotalIntnAmount>
<AwardAmount>186706</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Perception determines how humans interpret and make use of designs, and is therefore an important consideration for design. This CRII project investigates how perceptual data can be used to guide computational design processes. As a showcase, the PI will investigate how visual perception data obtained from virtual environments can be used to devise novel computational approaches capable of analyzing, visualizing and modeling the human affordance aspects of given architectural layouts. Such approaches can be applied for generating functional layout designs for a variety of built environments, to enhance movement comfort, efficiency, productivity and safety. The proposed research will bring broad impacts on improving transportation, tourism, commerce and public safety. In addition, the PI will contribute by disseminating the collected visual perception datasets; organizing vision and cognition workshops regularly; mentoring a team of undergraduate, master and PhD students to conduct research, including minority students whom the PI will mentor via the McNair Scholars Program.&lt;br/&gt;&lt;br/&gt;In particular, the PI will set up a virtual reality platform equipped with eyetracking capabilities for studying human visual perception and reactions towards different geometric, spatial and semantic features in 3D environments. The collected data will form the basis for developing a 3D scene analysis tool which designers can use to visualize and understand the effects of layout designs on human visual perception and people flow, to make informed design choices. Designers can also easily obtain ergonomic design suggestions via suggestive interfaces driven by an optimization component which considers multiple visual perception and human affordance factors.  Based on the visual perception data, an optimization approach will also be devised capable of automatically generating and enhancing 3D layout designs.</AbstractNarration>
<MinAmdLetterDate>06/16/2016</MinAmdLetterDate>
<MaxAmdLetterDate>01/29/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1565978</AwardID>
<Investigator>
<FirstName>Marc</FirstName>
<LastName>Pomplun</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marc Pomplun</PI_FULL_NAME>
<EmailAddress>marc@cs.umb.edu</EmailAddress>
<PI_PHON>6172876485</PI_PHON>
<NSF_ID>000090280</NSF_ID>
<StartDate>01/29/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lap Fai</FirstName>
<LastName>Yu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lap Fai Yu</PI_FULL_NAME>
<EmailAddress>craigyu@gmu.edu</EmailAddress>
<PI_PHON>7039934813</PI_PHON>
<NSF_ID>000701257</NSF_ID>
<StartDate>06/16/2016</StartDate>
<EndDate>01/29/2019</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Boston</Name>
<CityName>Dorchester</CityName>
<ZipCode>021253300</ZipCode>
<PhoneNumber>6172875370</PhoneNumber>
<StreetAddress>100 Morrissey Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>808008122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Boston]]></Name>
<CityName>Dorchester</CityName>
<StateCode>MA</StateCode>
<ZipCode>021253300</ZipCode>
<StreetAddress><![CDATA[100 Morrissey Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~90124</FUND_OBLG>
<FUND_OBLG>2017~96582</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-ee7c44ee-7fff-e1c5-f3f1-bab32d4984cc"> </span></p> <p dir="ltr"><span>Sponsored by the award, PI Yu obtained substantial preliminary research results on virtual reality, human perception analysis, and 3D content creation which form the basis for the proposed research on human performance-guided synthesis of virtual environments for personalized training.</span></p> <h2 dir="ltr"><span>Intellectual Merit</span></h2> <p dir="ltr"><span>The CRII project which spanned 3 years has resulted in more than 20 high-quality journal and conference papers in the topic of perceptual-data guided computational design, including 3 ACM TOG papers (SIGGRAPH Asia 16,&nbsp;</span><span>SIGGRAPH Asia 17 &amp; SIGGRAPH Asia 19</span><span>), 3 ACM CHI papers</span><span>, 3 IEEE TVCG (special issue of IEEE Virtual Reality) papers</span><span>, 3 IEEE Virtual Reality papers</span><span>, 1 ICCV paper</span><span>, 1 ECCV paper</span><span>, 1 AAAI paper</span><span>, 1 CogSci paper</span><span>, 1 3DV paper</span><span>, 3 IEEE TVCG papers</span><span>, 2 ICRA papers</span><span>, 2 other journal papers.</span></p> <p dir="ltr"><span>The CHI 2019 paper ?Pose-Guided Level Design?</span><span>, which proposed a new optimization-based approach for synthesizing level designs based on human body poses (for applications such as VR rehabilitation), was selected for a </span><span>Best Paper Honorable Mention Award</span><span>. The 3DV 2016 paper </span><span>?SceneNN: a Scene Meshes Dataset with aNNotations? which proposed a robust pipeline for 3D scene reconstruction and annotation (for applications such as VR navigation &amp; training), was selected for a </span><span>Best Paper Honorable Mention award</span><span>. The TVCG (IEEE VR 2018) paper </span><span>?Exercise Intensity-driven Level Design? was featured in the headline news of the IEEE Xplore Innovation Spotlight</span><span>, receiving more than 1,400 likes and 125 shares on IEEE?s social media pages.&nbsp;</span></p> <h2 dir="ltr"><span>Broader Impacts</span></h2> <p dir="ltr"><span>With NSF support, a website (</span><a href="http://www.scenenn.net"><span>www.scenenn.net</span></a><span>) was set up that publicly hosts a scene meshes dataset, as well as the code for the 3D reconstruction and annotation pipeline. Based on the dataset and code, the team organized the RGB-D Object-to-CAD Retrieval Contests at EUROGRAPHICS 2017 and 2018, which attracted 13 international teams to compete and solicited the state-of-the-art RGB-D object to CAD retrieval algorithm. Performance of different algorithms was published in 2 survey papers</span><span>.&nbsp;</span></p> <p dir="ltr"><span>In addition, 1 course, 2 tutorials and 7 workshops have been organized in conjunction with the CVPR, SIGGRAPH Asia, CogSci, ICRA, and IROS conferences on the topics of 3D reconstruction, modeling, understanding of scenes with a focus on VR and robotics applications. Research results were disseminated to the public and more than 40 invited talks were hosted through these events. The presentation slides of these 10 events are publicly available on their respective websites.</span></p> <p><span>Through NSF support, PI Yu has mentored 10 undergraduate students (5 of them are underrepresented minorities in STEM) to conduct research, resulting in 1 CHI paper</span><span>, 2 TVCG papers</span><span>, and 1 CogSci paper</span><span>, with the undergraduate students as the lead authors. In total, 20 fellowships and awards have been received by the undergraduate students, including a CHI 2019 Best Paper Honorable Mention Award. Two female students received an NSF Graduate Research Fellowship and an NSF-GRFP Honorable Mention Award. One female student will continue with her PhD study in computer science in Fall 2019.</span></p><br> <p>            Last Modified: 02/03/2020<br>      Modified by: Marc&nbsp;Pomplun</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1565978/1565978_10447433_1565493309265_xplore--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1565978/1565978_10447433_1565493309265_xplore--rgov-800width.jpg" title="Featured article by IEEE Xplore Innovation Spotlight"><img src="/por/images/Reports/POR/2019/1565978/1565978_10447433_1565493309265_xplore--rgov-66x44.jpg" alt="Featured article by IEEE Xplore Innovation Spotlight"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our project "Exercise Intensity-driven Level Design" was featured in the headline news of the IEEE Xplore Innovation Spotlight.</div> <div class="imageCredit">IEEE</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Marc&nbsp;Pomplun</div> <div class="imageTitle">Featured article by IEEE Xplore Innovation Spotlight</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Sponsored by the award, PI Yu obtained substantial preliminary research results on virtual reality, human perception analysis, and 3D content creation which form the basis for the proposed research on human performance-guided synthesis of virtual environments for personalized training. Intellectual Merit The CRII project which spanned 3 years has resulted in more than 20 high-quality journal and conference papers in the topic of perceptual-data guided computational design, including 3 ACM TOG papers (SIGGRAPH Asia 16, SIGGRAPH Asia 17 &amp; SIGGRAPH Asia 19), 3 ACM CHI papers, 3 IEEE TVCG (special issue of IEEE Virtual Reality) papers, 3 IEEE Virtual Reality papers, 1 ICCV paper, 1 ECCV paper, 1 AAAI paper, 1 CogSci paper, 1 3DV paper, 3 IEEE TVCG papers, 2 ICRA papers, 2 other journal papers. The CHI 2019 paper ?Pose-Guided Level Design?, which proposed a new optimization-based approach for synthesizing level designs based on human body poses (for applications such as VR rehabilitation), was selected for a Best Paper Honorable Mention Award. The 3DV 2016 paper ?SceneNN: a Scene Meshes Dataset with aNNotations? which proposed a robust pipeline for 3D scene reconstruction and annotation (for applications such as VR navigation &amp; training), was selected for a Best Paper Honorable Mention award. The TVCG (IEEE VR 2018) paper ?Exercise Intensity-driven Level Design? was featured in the headline news of the IEEE Xplore Innovation Spotlight, receiving more than 1,400 likes and 125 shares on IEEE?s social media pages.  Broader Impacts With NSF support, a website (www.scenenn.net) was set up that publicly hosts a scene meshes dataset, as well as the code for the 3D reconstruction and annotation pipeline. Based on the dataset and code, the team organized the RGB-D Object-to-CAD Retrieval Contests at EUROGRAPHICS 2017 and 2018, which attracted 13 international teams to compete and solicited the state-of-the-art RGB-D object to CAD retrieval algorithm. Performance of different algorithms was published in 2 survey papers.  In addition, 1 course, 2 tutorials and 7 workshops have been organized in conjunction with the CVPR, SIGGRAPH Asia, CogSci, ICRA, and IROS conferences on the topics of 3D reconstruction, modeling, understanding of scenes with a focus on VR and robotics applications. Research results were disseminated to the public and more than 40 invited talks were hosted through these events. The presentation slides of these 10 events are publicly available on their respective websites.  Through NSF support, PI Yu has mentored 10 undergraduate students (5 of them are underrepresented minorities in STEM) to conduct research, resulting in 1 CHI paper, 2 TVCG papers, and 1 CogSci paper, with the undergraduate students as the lead authors. In total, 20 fellowships and awards have been received by the undergraduate students, including a CHI 2019 Best Paper Honorable Mention Award. Two female students received an NSF Graduate Research Fellowship and an NSF-GRFP Honorable Mention Award. One female student will continue with her PhD study in computer science in Fall 2019.       Last Modified: 02/03/2020       Submitted by: Marc Pomplun]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
