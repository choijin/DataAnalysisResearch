<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>The Internal Validity of External Validity: Using Experiments to Validate Three Approaches to Extrapolating Causal Inferences Beyond the Cutoff in Regression Discontinuity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>794631.00</AwardTotalIntnAmount>
<AwardAmount>794631</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field. Three types of proposals will be supported by the program: Exploratory Projects that include proof-of-concept and feasibility studies; more extensive Full-Scale Projects; and workshops and conferences. The proposed research attends carefully to item 1 above. In addition, the dissemination plan also attends to item 3.&lt;br/&gt; &lt;br/&gt;Randomized control trials in STEM education are considered the gold standard for causal inference.  Often these models are overly restrictive in practice (for ethical and other reasons). Moreover, many such studies collapse as the research progresses. For example, senior school leadership changes and a school is withdrawn from a study.  When this occurs the randomizing process is compromised and so are the resulting inferences. The proposed research will assess the validity and robustness of new approaches for using the regression discontinuity design (RDD) to evaluate educational interventions affecting STEM outcomes. &lt;br/&gt;&lt;br/&gt;This study will systematically compare and contrast the new approaches for generalizing RDD, assessing how well they produce unbiased causal estimates away from the cutoff score. The research will be conducted using 12 heterogeneous datasets from past educational interventions that (a) were originally evaluated through an RE and (b) included math achievement as an outcome measure. Using the within-study comparison method (Cook et al. 2008), this study will assess the extent to which the RE and a given RDD approach produce similar causal estimates away from the cutoff for the same population. Analyzing multiple datasets with STEM outcomes will provide information about the robustness of the new RDD methods across different types of interventions and student populations.</AbstractNarration>
<MinAmdLetterDate>09/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/18/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1544301</AwardID>
<Investigator>
<FirstName>Henrich</FirstName>
<LastName>Hock</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Henrich Hock</PI_FULL_NAME>
<EmailAddress>hhock@mathematica-mpr.com</EmailAddress>
<PI_PHON>2022503557</PI_PHON>
<NSF_ID>000695816</NSF_ID>
<StartDate>09/18/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Cook</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Cook</PI_FULL_NAME>
<EmailAddress>TomCook6@gwu.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000756433</NSF_ID>
<StartDate>09/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>MATHEMATICA POLICY RESEARCH INC</Name>
<CityName>PRINCETON</CityName>
<ZipCode>085432393</ZipCode>
<PhoneNumber>6097993535</PhoneNumber>
<StreetAddress>600 ALEXANDER PARK DRIVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>154308522</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MATHEMATICA INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>154125140</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Mathematica Policy Research, Inc.]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200024221</ZipCode>
<StreetAddress><![CDATA[1100 1st Street, NE, 12th Floor]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>009Z</Code>
<Text>PRIME - Promoting Research and Innovatio</Text>
</ProgramReference>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~794631</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Background. </strong>This project sought to produce new information about the value of nonexperimental research methods for assessing the effectiveness of education and training interventions. A 2013 strategic plan from the National Science and Technology Council emphasized a need for rigorous research to support evidence-based federal investments in STEM education. Although a randomized experiment is typically considered the strongest research design for producing causal evidence, practical and operational considerations can make experiments challenging or infeasible to implement in educational settings. Hence, this project considered the circumstances under which nonexperimental methods can produce results that are (1) valid, in meaning that the results correctly characterize the effectiveness of an intervention; and (2) generalizable, meaning that the result can be applied to a broad population of interest for education policy.</p> <p>Much of this project focused on regression discontinuity designs (RDDs), which are increasingly being used to test interventions and inform policy. RDDs can be used when eligibility for an intervention is based on an initial ?score,? which can measure knowledge, need, merit, age, or other factors. Individuals are offered the intervention when their score is above (or below) a cutoff threshold. The nonexperimental RDDs is thought to have high validity for estimating the impacts of an intervention near the cutoff score?because individuals just above and below the cutoff would have fared similarly had it not been for the sharp difference in their eligibility for the intervention. However, traditional RDD studies produce estimates of an intervention?s effects are obtained <em>only</em> for individuals at the cutoff score, who often represent a modest fraction of all who might be eligible for the intervention.</p> <p>&nbsp;</p> <p><strong>Performance of the ?comparative? regression discontinuity (CRD) approach for generalizing RDD. </strong>A major aim of this project was to better understand the performance of the CRD approach, which may be used to produce information about effectiveness for a broad population individuals with scores away from the cutoff. This approach augments the traditional RDD to produce estimates of how those eligible for an education or training intervention would have fared if they had not, in fact, had access to the intervention. CRD methods leverage quantitative measures that are strong predictors of post-intervention outcomes?either (1) past measures of the outcomes (?pretests?), or (2) information about how a comparison group of people without access to the intervention ultimately fared. To assess the validity of these methods, the project used a ?within-study comparison? (WSC) approach. This approach compares estimates of effectiveness to benchmarks from randomized experiments based on data from past evaluations of education and training programs.</p> <p>Key findings from the project indicate that CRD can produce reliable estimates of effectiveness to inform policy:</p> <ul> <li>Using CRD to generalize away from the RDD cutoff score produces results that are similar to experimental benchmarks when certain partially testable conditions are clearly met (Tang et al. 2017, Kisbu-Sakarya et al. 2018), which bolsters earlier evidence from Wing and Cook (2013) on the promise of CRD. Ongoing research is using simulation methods to better the circumstances under which CRD yields valid estimates when those conditions are violated. </li> <li>CRD may produce generalizable results that are somewhat less precise than a randomized experiment, particularly when relying on a comparison group (Tang, Cook, &amp; Kisbu-Sakarya 2017, 2018). However, in some situations it might be more practical or economical to consider adding more subjects to an evaluation to offset this aspect of CRD than it would be to implement a randomized experiment. </li> </ul> <p>In addition, this research showed that CRD can typically improve the precision of traditional RD estimates of effectiveness at the cutoff score.</p> <p>&nbsp;</p> <p><strong>Other information related to generalizing causal inference using RDDs. </strong>The project also included three additional studies about whether and how RDD estimates of effectiveness can provide rigorous evidence that has broad applicability. First, a meta-analysis of 15 previous WSC studies of the traditional RDD showed that effectiveness at a cutoff score tended to correspond well with experimental estimates across a range of settings, interventions, and scoring measures (Chaplin et al. 2017). Second, ongoing research is assessing the performance of an alternative to CRD that considers using a comparison group of people who were assigned to the intervention based on the RDD cutoff score, but did not actually take it up. Third, other ongoing research is using WSC techniques to learn more about the contexts in which the pretest (a key element of many RDDs) may help improve the validity of nonexperimental estimates of effectiveness.</p> <p>&nbsp;</p> <p><strong>Summary. </strong>Findings from this project are expected to enhance the research toolkit available to evaluators when trying to ascertain which interventions work for which populations when randomized experiments are not feasible or practical. Attached is a list of publications that provide more information about both the issues described above and other project findings related to producing reliable assessments of educational and training interventions.</p><br> <p>            Last Modified: 03/31/2019<br>      Modified by: Henrich&nbsp;Hock</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1544301/1544301_10400775_1554059238713_DGE-1544301OutcomesReport-SupplementalInformation--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1544301/1544301_10400775_1554059238713_DGE-1544301OutcomesReport-SupplementalInformation--rgov-800width.jpg" title="Supplemental Information for Outcomes Report of NSF Grant DGE-1544301"><img src="/por/images/Reports/POR/2019/1544301/1544301_10400775_1554059238713_DGE-1544301OutcomesReport-SupplementalInformation--rgov-66x44.jpg" alt="Supplemental Information for Outcomes Report of NSF Grant DGE-1544301"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This file displays lists of grant publications cited in the outcomes report and other grant products. (It also displays an additional reference cited in the outcomes report that was not produced under this grant.) Please use OCR for rendering by screen reader.</div> <div class="imageCredit">DGE-1544301</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Henrich&nbsp;Hock</div> <div class="imageTitle">Supplemental Information for Outcomes Report of NSF Grant DGE-1544301</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Background. This project sought to produce new information about the value of nonexperimental research methods for assessing the effectiveness of education and training interventions. A 2013 strategic plan from the National Science and Technology Council emphasized a need for rigorous research to support evidence-based federal investments in STEM education. Although a randomized experiment is typically considered the strongest research design for producing causal evidence, practical and operational considerations can make experiments challenging or infeasible to implement in educational settings. Hence, this project considered the circumstances under which nonexperimental methods can produce results that are (1) valid, in meaning that the results correctly characterize the effectiveness of an intervention; and (2) generalizable, meaning that the result can be applied to a broad population of interest for education policy.  Much of this project focused on regression discontinuity designs (RDDs), which are increasingly being used to test interventions and inform policy. RDDs can be used when eligibility for an intervention is based on an initial ?score,? which can measure knowledge, need, merit, age, or other factors. Individuals are offered the intervention when their score is above (or below) a cutoff threshold. The nonexperimental RDDs is thought to have high validity for estimating the impacts of an intervention near the cutoff score?because individuals just above and below the cutoff would have fared similarly had it not been for the sharp difference in their eligibility for the intervention. However, traditional RDD studies produce estimates of an intervention?s effects are obtained only for individuals at the cutoff score, who often represent a modest fraction of all who might be eligible for the intervention.     Performance of the ?comparative? regression discontinuity (CRD) approach for generalizing RDD. A major aim of this project was to better understand the performance of the CRD approach, which may be used to produce information about effectiveness for a broad population individuals with scores away from the cutoff. This approach augments the traditional RDD to produce estimates of how those eligible for an education or training intervention would have fared if they had not, in fact, had access to the intervention. CRD methods leverage quantitative measures that are strong predictors of post-intervention outcomes?either (1) past measures of the outcomes (?pretests?), or (2) information about how a comparison group of people without access to the intervention ultimately fared. To assess the validity of these methods, the project used a ?within-study comparison? (WSC) approach. This approach compares estimates of effectiveness to benchmarks from randomized experiments based on data from past evaluations of education and training programs.  Key findings from the project indicate that CRD can produce reliable estimates of effectiveness to inform policy:  Using CRD to generalize away from the RDD cutoff score produces results that are similar to experimental benchmarks when certain partially testable conditions are clearly met (Tang et al. 2017, Kisbu-Sakarya et al. 2018), which bolsters earlier evidence from Wing and Cook (2013) on the promise of CRD. Ongoing research is using simulation methods to better the circumstances under which CRD yields valid estimates when those conditions are violated.  CRD may produce generalizable results that are somewhat less precise than a randomized experiment, particularly when relying on a comparison group (Tang, Cook, &amp; Kisbu-Sakarya 2017, 2018). However, in some situations it might be more practical or economical to consider adding more subjects to an evaluation to offset this aspect of CRD than it would be to implement a randomized experiment.    In addition, this research showed that CRD can typically improve the precision of traditional RD estimates of effectiveness at the cutoff score.     Other information related to generalizing causal inference using RDDs. The project also included three additional studies about whether and how RDD estimates of effectiveness can provide rigorous evidence that has broad applicability. First, a meta-analysis of 15 previous WSC studies of the traditional RDD showed that effectiveness at a cutoff score tended to correspond well with experimental estimates across a range of settings, interventions, and scoring measures (Chaplin et al. 2017). Second, ongoing research is assessing the performance of an alternative to CRD that considers using a comparison group of people who were assigned to the intervention based on the RDD cutoff score, but did not actually take it up. Third, other ongoing research is using WSC techniques to learn more about the contexts in which the pretest (a key element of many RDDs) may help improve the validity of nonexperimental estimates of effectiveness.     Summary. Findings from this project are expected to enhance the research toolkit available to evaluators when trying to ascertain which interventions work for which populations when randomized experiments are not feasible or practical. Attached is a list of publications that provide more information about both the issues described above and other project findings related to producing reliable assessments of educational and training interventions.       Last Modified: 03/31/2019       Submitted by: Henrich Hock]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
