<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Developing a Consensus on Metrics that Measure the Impact of Federal R&amp;D Investments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>05/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>94783.00</AwardTotalIntnAmount>
<AwardAmount>94783</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04010000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SMA</Abbreviation>
<LongName>SBE Off Of Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maryann Feldman</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The appropriate metrics to use evaluate R&amp;D programs is a debated and unresolved question.  There are currently no standards for the specific impacts that should be assessed when making investment decisions.  This lack of uniformity inhibits comparisons between programs and weakens the value of these evaluations in providing actionable information.  This s project seeks to advance knowledge within the science of science policy community by identifying a core set of metrics that can be used to evaluate Federal R&amp;D programs. &lt;br/&gt;&lt;br/&gt;This project will create a comprehensive set of potential metrics that may be relevant when evaluating the outcomes, impacts, and innovations that result from an R&amp;D program. The research will use RAND?s ExpertLens, an online modified Delphi system, to elicit opinions from a diverse set of Federal stakeholders and academic researchers on the usefulness of the identified metrics. This process will provide information on the most valuable metrics for evaluating a diverse set of R&amp;D programs&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1550217</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Basco</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Basco</PI_FULL_NAME>
<EmailAddress>dbasco@rand.org</EmailAddress>
<PI_PHON>3103930411</PI_PHON>
<NSF_ID>000690220</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dave</FirstName>
<LastName>Baiocchi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dave Baiocchi</PI_FULL_NAME>
<EmailAddress>baiocchi@rand.org</EmailAddress>
<PI_PHON>3103930411</PI_PHON>
<NSF_ID>000697801</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rand Corporation</Name>
<CityName>Santa Monica</CityName>
<ZipCode>904013297</ZipCode>
<PhoneNumber>3103930411</PhoneNumber>
<StreetAddress>1776 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006914071</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RAND CORPORATION, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006914071</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rand Corporation]]></Name>
<CityName>Santa Monica</CityName>
<StateCode>CA</StateCode>
<ZipCode>904013297</ZipCode>
<StreetAddress><![CDATA[1776 Main Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7626</Code>
<Text>SciSIP-Sci of Sci Innov Policy</Text>
</ProgramElement>
<ProgramReference>
<Code>7626</Code>
<Text>SCIENCE OF SCIENCE POLICY</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~94783</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project aimed to identify a consensus among U.S. Federal Government officials and U.S. researchers on the most useful metrics for measuring the impacts of Federal research and development (R&amp;D) investments. In the United States, there is little agreement on the specific metrics needed to comprehensively evaluate the impacts of various Federal R&amp;D programs. Without agreement, science agencies cannot develop data collection instruments to fully capture the results of R&amp;D; compare the effects of R&amp;D programs within and across agencies and scientific areas; or use evaluations to determine how to optimally structure future R&amp;D investments. This project aimed to identify the full range of impacts that could occur across R&amp;D programs and then develop a consensus on a core set of impact metrics.</p> <p>We utilized the Delphi method, an iterative consensus building approach&nbsp;to decision making, to explore whether agreement could be reached on a set of impact metrics that would be useful for U.S. policymakers in evaluating or comparing R&amp;D investments. Using an online platform, we convened a panel of Federal Government officials and a separate panel of U.S.-based researchers. Each panel completed a three round modified Delphi process. In Round 1, participants rated a series of impact metrics and provided rationales for ther decisions. In Round 2, participants viewed the ratings and rationales of the entire panel and used discussion boards to anonymously debate differences in perspectives. Participants were also able to recommend new metrics to rate. In Round 3, participants re-rated metrics, including new metrics, to ultimately identify a core set.</p> <p>Overall, this expert elicitation process identified 33 metrics that would be useful to U.S. policymakers in evaluating and comparing R&amp;D programs. These metrics addressed impacts on academia, government, the economy, and society. The panels endorsed metrics that already being used, such as patent applications, and endorsed new metrics that would require new data collection, such as improved business processes resulting from Federal R&amp;D.</p> <p>To our knowledge, this is the first time officials from Congress, the White House, and science agencies have come together to agree on the most important metrics needed to measure the results of R&amp;D investments. Our research highlights that the information policymakers desire will require new data collection efforts and methods for analyzing that data.&nbsp;These results provide a clear research direction for science policy community as it aims to be more relavent to policymakers. In addition, these results could spark new conversations across the Federal Government on how to document the results of R&amp;D.&nbsp;</p> <p>The outcomes of this work will also be presented at an international conference.</p><br> <p>            Last Modified: 08/15/2016<br>      Modified by: Daniel&nbsp;Basco</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project aimed to identify a consensus among U.S. Federal Government officials and U.S. researchers on the most useful metrics for measuring the impacts of Federal research and development (R&amp;D) investments. In the United States, there is little agreement on the specific metrics needed to comprehensively evaluate the impacts of various Federal R&amp;D programs. Without agreement, science agencies cannot develop data collection instruments to fully capture the results of R&amp;D; compare the effects of R&amp;D programs within and across agencies and scientific areas; or use evaluations to determine how to optimally structure future R&amp;D investments. This project aimed to identify the full range of impacts that could occur across R&amp;D programs and then develop a consensus on a core set of impact metrics.  We utilized the Delphi method, an iterative consensus building approach to decision making, to explore whether agreement could be reached on a set of impact metrics that would be useful for U.S. policymakers in evaluating or comparing R&amp;D investments. Using an online platform, we convened a panel of Federal Government officials and a separate panel of U.S.-based researchers. Each panel completed a three round modified Delphi process. In Round 1, participants rated a series of impact metrics and provided rationales for ther decisions. In Round 2, participants viewed the ratings and rationales of the entire panel and used discussion boards to anonymously debate differences in perspectives. Participants were also able to recommend new metrics to rate. In Round 3, participants re-rated metrics, including new metrics, to ultimately identify a core set.  Overall, this expert elicitation process identified 33 metrics that would be useful to U.S. policymakers in evaluating and comparing R&amp;D programs. These metrics addressed impacts on academia, government, the economy, and society. The panels endorsed metrics that already being used, such as patent applications, and endorsed new metrics that would require new data collection, such as improved business processes resulting from Federal R&amp;D.  To our knowledge, this is the first time officials from Congress, the White House, and science agencies have come together to agree on the most important metrics needed to measure the results of R&amp;D investments. Our research highlights that the information policymakers desire will require new data collection efforts and methods for analyzing that data. These results provide a clear research direction for science policy community as it aims to be more relavent to policymakers. In addition, these results could spark new conversations across the Federal Government on how to document the results of R&amp;D.   The outcomes of this work will also be presented at an international conference.       Last Modified: 08/15/2016       Submitted by: Daniel Basco]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
