<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Integrating neural interfaces and machine intelligence for advanced neural prosthetics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>257000.00</AwardTotalIntnAmount>
<AwardAmount>257000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Shubhra Gangopadhyay</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Brain-machine interfaces (BMI) read signals directly from the brain to control external devices such as robotic limbs.  While this technology has great potential to benefit people who are paralyzed, BMIs often have poor performance because they use noisy, low-level signals to simultaneously control many aspects of the robotic limb's movements.  In contrast, this project will address this shortcoming by reading high-level intents from the brain in order to control an intelligent robotic system.  These changes reflect cutting-edge advances in neuroscience and machine intelligence and will require close cooperation between scientists, engineers, and physicians.  The project aims to leverage expertise across these diverse fields in order to generate significant improvements in BMI technology to advance the national health, increase scientific understanding of the brain, and lead to dramatic improvements in the quality of life for these severely disabled persons.&lt;br/&gt;&lt;br/&gt;This collaborative project will decode high-level cognitive actions from neural signals recorded in the parietal cortex of a tetraplegic human, then carry out those intents using a smart robotic prosthesis.  Persons with tetraplegia who have multielectrode arrays (MEA) implanted in reach and grasp areas of the posterior parietal cortex (PPC), will participate in experiments to explore the neural representation of cognitive intentions in human PPC including object selection, action intention, and neural control of robotic limbs.  Experimental results will be used to construct BMI control algorithms optimized to decode these cognitive signals.  In parallel, a modular, semi-autonomous robotic prosthesis will be developed that can identify household objects and plan reach-and-grasp movements to manipulate or transport the objects.  These scientific and technological efforts will be supported by continued clinical care of the tetraplegic participants. The study will explore increasingly capable iterations of the BMI system, culminating in testing of the fully developed BMI system in the participants' own home environment where they will practice activities of daily living. The resulting system will leverage deep insights in cognitive neuroscience and advanced capabilities in machine sensing and robotic control systems to substantially improve the ease of use and capability of brain-machine interfaces.</AbstractNarration>
<MinAmdLetterDate>08/10/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1532778</AwardID>
<Investigator>
<FirstName>Kapil</FirstName>
<LastName>Katyal</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kapil D Katyal</PI_FULL_NAME>
<EmailAddress>Kapil.Katyal@jhuapl.edu</EmailAddress>
<PI_PHON>2402289471</PI_PHON>
<NSF_ID>000691345</NSF_ID>
<StartDate>08/10/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University Applied Physics Laboratory]]></Name>
<CityName>Laurel</CityName>
<StateCode>MD</StateCode>
<ZipCode>207236099</ZipCode>
<StreetAddress><![CDATA[11100 Johns Hopkins Rd.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5345</Code>
<Text>Engineering of Biomed Systems</Text>
</ProgramElement>
<ProgramElement>
<Code>7633</Code>
<Text>EFRI Research Projects</Text>
</ProgramElement>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>004E</Code>
<Text>BIOMEDICAL ENG AND DIAGNOSTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<ProgramReference>
<Code>8551</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~257000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As part of this award, we have developed a significant number of tools that enable brain computer interfaces (BCI) to efficiently interact with robotic manipulators to perform everyday household tasks.&nbsp; To accomplish this, we developed algorithms for object recognition, object pose estimation and autonomous manipulator controls to perform low level manipulation tasks, while relying on the BCI signal to provide high level, supervisory commands.&nbsp;</p> <p>Our object recognition and pose estimation algorithms work by using an integrated RGB and depth camera to obtain a point cloud representing raw pixel and depth values of the scene.&nbsp; We then filter and cluster the point cloud data to obtain potential interactable candidate objects.&nbsp; Using a 3D feature extraction algorithm, we collect 3D features from the scene and match them with known objects to semantically label the clusters of interactable objects.&nbsp; We also developed an autonomous control algorithm that provides point to point navigation and grasp strategy given the geometric properties of the object to manipulate.&nbsp;</p> <p>We have demonstrated this capability allow individuals to perform everyday tasks including brewing a cup of coffee. &nbsp;The BCI interface allows an individual to provide high level supervisory signals or customizations to the manipulator trajectory.&nbsp;</p> <p>This system enabled our collaborators at California Institute of Technology (Caltech) to conduct a variety of experiments to understand how various regions of the brain and representations can be used to control high degree of freedom manipulation systems and has led to 3 posters presented at the Society for Neuroscience conference.</p><br> <p>            Last Modified: 11/02/2018<br>      Modified by: Kapil&nbsp;D&nbsp;Katyal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As part of this award, we have developed a significant number of tools that enable brain computer interfaces (BCI) to efficiently interact with robotic manipulators to perform everyday household tasks.  To accomplish this, we developed algorithms for object recognition, object pose estimation and autonomous manipulator controls to perform low level manipulation tasks, while relying on the BCI signal to provide high level, supervisory commands.   Our object recognition and pose estimation algorithms work by using an integrated RGB and depth camera to obtain a point cloud representing raw pixel and depth values of the scene.  We then filter and cluster the point cloud data to obtain potential interactable candidate objects.  Using a 3D feature extraction algorithm, we collect 3D features from the scene and match them with known objects to semantically label the clusters of interactable objects.  We also developed an autonomous control algorithm that provides point to point navigation and grasp strategy given the geometric properties of the object to manipulate.   We have demonstrated this capability allow individuals to perform everyday tasks including brewing a cup of coffee.  The BCI interface allows an individual to provide high level supervisory signals or customizations to the manipulator trajectory.   This system enabled our collaborators at California Institute of Technology (Caltech) to conduct a variety of experiments to understand how various regions of the brain and representations can be used to control high degree of freedom manipulation systems and has led to 3 posters presented at the Society for Neuroscience conference.       Last Modified: 11/02/2018       Submitted by: Kapil D Katyal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
