<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Auditory Navigation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04010000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SMA</Abbreviation>
<LongName>SBE Off Of Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Soo-Siang Lim</SignBlockName>
<PO_EMAI>slim@nsf.gov</PO_EMAI>
<PO_PHON>7032927878</PO_PHON>
</ProgramOfficer>
<AbstractNarration>All animals must navigate and find their way in space, in order to find food and shelter and avoid harm. Humans overwhelmingly rely on visual landscape to learn spatial maps and navigate. Unfortunately, millions of blind people cannot do that. They can avoid obstacles using a walking stick but this does not provide information about the spatial layout on a larger scale, defined by visual landmarks such as pillars, buildings, mountains etc. Without this information about the visual landscape blind subjects cannot develop a sense of spatial map of the surrounding world and navigate within it. This inability to learn spatial maps is profoundly debilitating and has serious consequences for other forms of learning as well. &lt;br/&gt; &lt;br/&gt;The proposed project will test a novel and multidisciplinary solution that can be implemented using recently developed techniques developed by the Principal Investigator?s (PI's) laboratory.  The researchers hypothesize that subjects could form navigational maps through an artificially created auditory landscape. To test this, the investigators will create a landscape defined only by auditory cues and ask subjects to navigate. This requires techniques to eliminate nonspecific cues such as light and smells that can accidentally provide spatial information.  Using these recently developed techniques to eliminate such confounds and ensure that subjects are navigating only with auditory cues, the researchers  will also develop data analysis methods to determine how well and how quickly the subjects are learning the auditory landscape. The results would provide valuable insights about whether mammals, even with the ones with good visual abilities in terms of their genetic heritage (e.g. humans), can learn to rely on the auditory cues alone to learn spatial maps. In the long run this information would be quite useful in helping blind people learn to navigate using auditory cues.</AbstractNarration>
<MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1550678</AwardID>
<Investigator>
<FirstName>Mayank</FirstName>
<LastName>Mehta</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mayank R Mehta</PI_FULL_NAME>
<EmailAddress>MayankMehta@UCLA.edu</EmailAddress>
<PI_PHON>3102674653</PI_PHON>
<NSF_ID>000485278</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[UCLA Physics and Astronomy]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951547</ZipCode>
<StreetAddress><![CDATA[475 Portola Plaza, 5-146 Knudsen]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7704</Code>
<Text>Science of Learning Activities</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Typical laboratory experiments on learning are very restricted, involving only a few restricted behaviors. In contrast, learning of subjects in the wild is quite diverse and varied. This disparity has been often criticized. On the other hand, natural learning involves many variables that cannot even be measured, let alone controlled. The experiments conducted here would help bridge the laboratory versus natural behavior divide in two ways. First we have built a virtual reality that allows a greater range of behaviors than typical laboratory experiments. Second virtual reality allows for precise control and monitoring of the behavior and environmental stimuli. These results thus go a long way towards understanding natural learning.</p> <p>Hence, the main goal of the project was to determine the multisensory mechanisms of navigational learning. There were several sub-goals. First, we wanted to understand how well can rats navigate using visual cues versus auditory cues. The results could be of potential use for helping blind humans with navigation. The other sub-goal was to how neurons in a space-mapping part of the brain responded to navigation in the virtual world. This could be of relevance for determining the effects of using virtual reality on brain health.</p> <p>Our findings show that rats are able to use the auditory cues to determine where expect the reward, but they are unable to learn a navigational map of the maze. This is very surprising, as it suggests the simultaneous existence of multiple mental maps --one for navigation, another one for reward expectancy etc.&nbsp;</p> <p>Surprising, we found that the space-mapping part of the brain responded very abnormally when the subjects were exploring virtual reality. More than half the neurons shut down in virtual reality and the remaining neurons showed scrambled spatial maps or abnormal activity patterns. This occurred in a purely visual virtual reality, that is similar to but better than many commercially available VR systems. These findings have important implications for the long-term impact of virtual reality use on learning and brain health.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/14/2018<br>      Modified by: Mayank&nbsp;R&nbsp;Mehta</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Typical laboratory experiments on learning are very restricted, involving only a few restricted behaviors. In contrast, learning of subjects in the wild is quite diverse and varied. This disparity has been often criticized. On the other hand, natural learning involves many variables that cannot even be measured, let alone controlled. The experiments conducted here would help bridge the laboratory versus natural behavior divide in two ways. First we have built a virtual reality that allows a greater range of behaviors than typical laboratory experiments. Second virtual reality allows for precise control and monitoring of the behavior and environmental stimuli. These results thus go a long way towards understanding natural learning.  Hence, the main goal of the project was to determine the multisensory mechanisms of navigational learning. There were several sub-goals. First, we wanted to understand how well can rats navigate using visual cues versus auditory cues. The results could be of potential use for helping blind humans with navigation. The other sub-goal was to how neurons in a space-mapping part of the brain responded to navigation in the virtual world. This could be of relevance for determining the effects of using virtual reality on brain health.  Our findings show that rats are able to use the auditory cues to determine where expect the reward, but they are unable to learn a navigational map of the maze. This is very surprising, as it suggests the simultaneous existence of multiple mental maps --one for navigation, another one for reward expectancy etc.   Surprising, we found that the space-mapping part of the brain responded very abnormally when the subjects were exploring virtual reality. More than half the neurons shut down in virtual reality and the remaining neurons showed scrambled spatial maps or abnormal activity patterns. This occurred in a purely visual virtual reality, that is similar to but better than many commercially available VR systems. These findings have important implications for the long-term impact of virtual reality use on learning and brain health.          Last Modified: 08/14/2018       Submitted by: Mayank R Mehta]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
