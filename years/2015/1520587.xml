<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Say What I Feel</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>149964.00</AwardTotalIntnAmount>
<AwardAmount>149964</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Glenn H. Larsen</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project aims to create an emotionally expressive software-based speech-generating communication system, which is designed for individuals with little or no verbal communication ability -- particularly those with autism spectrum disorder. Current alternative communication technology does not offer the ability to express emotional content: something that is crucial to effective, comprehensible communication at home, in the community, and at work. The ability to convey emotions -- sadness, anger, or happiness, for example -- through one's tone of voice or facial expressions has far-reaching educational and vocational benefits for individuals with communication challenges. In other words, emotional content helps clarify the communicative intent behind a spoken message. When non-verbal individuals can communicate thoroughly and effectively, they increase their ability to receive a free and appropriate public education, and they expand their vocational opportunities. When individuals can receive an education and find a job, they can contribute back to society. The proposed work supports progress in science and engineering, yet it also enhances the potential for current educational applications and future research studies. Finally, this project builds on Federal and State efforts to offer people with disabilities educational and vocational services.&lt;br/&gt;&lt;br/&gt; &lt;br/&gt;This project will develop and combine novel software algorithms that target digital facial recognition patterns and speech-based waveforms, in order to enhance the emotional content of conversational communication for individuals who cannot speak volitionally. The ability to add emotional content onto synthesized speech is a new technology that could substantially benefit individuals with communication challenges, such as people with autism spectrum disorder. This project seeks to facilitate and augment emotional expression and, therefore, increase communicative competence for individuals who use software-based synthesized speech systems to express their ideas, feelings, and wishes. Measuring whether or not this project accurately conveys human emotion in speech and facial expressions requires a perceptual test involving neurotypical adults. Specifically, the perceptual test will include both trained and untrained judges. The purpose of this is to ensure that the facial expressions as well as the pitch contour and vocal emphasis align with the emotion labels (i.e., happy, sad, or mad). In order to accurately calculate the judges' inter-rater reliability, the study will use a method that determines the extent to which judges' ratings agree, relative to how much they would likely conform, if they were to randomly rate the same stimuli.</AbstractNarration>
<MinAmdLetterDate>05/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1520587</AwardID>
<Investigator>
<FirstName>Lois</FirstName>
<LastName>Brady</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lois J Brady</PI_FULL_NAME>
<EmailAddress>loisjeanbrady@gmail.com</EmailAddress>
<PI_PHON>9258120037</PI_PHON>
<NSF_ID>000668110</NSF_ID>
<StartDate>05/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>iTherapy LLC</Name>
<CityName>Martinez</CityName>
<ZipCode>945531313</ZipCode>
<PhoneNumber>9258120037</PhoneNumber>
<StreetAddress>649 Main Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078838337</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ITHERAPY, LLC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[iTherapy]]></Name>
<CityName>Martinez</CityName>
<StateCode>CA</StateCode>
<ZipCode>945531313</ZipCode>
<StreetAddress><![CDATA[649 Main Street #229]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>118E</Code>
<Text>GRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8031</Code>
<Text>Education Products</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<ProgramReference>
<Code>8039</Code>
<Text>Information, Communication &amp; Computing</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~149964</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Project Outcomes Report</strong></p> <p><strong>SBIR-1520587 Say What I Feel</strong><strong>&nbsp;</strong></p> <div> <p>Introducing Emotional Content in Synthesized Speech via Facial Expression and Vocal Prosody</p> </div> <p>&nbsp;This SBIR Phase I project aims to create an emotionally expressive software-based speech-generating communication system, which is designed for individuals with little or no verbal communication ability -- particularly those with autism spectrum disorder. Current alternative communication technology does not offer the ability to express emotional content: something that is crucial to effective, comprehensible communication at home, in the community, and at work. The ability to convey emotions -- sadness, anger, or happiness, for example -- through one's tone of voice or facial expressions has far-reaching educational and vocational benefits for individuals with communication challenges. In other words, emotional content helps clarify the communicative intent behind a spoken message. When non-verbal individuals can communicate thoroughly and effectively, they increase their ability to receive a free and appropriate public education, and they expand their vocational opportunities. When individuals can receive an education and find a job, they can contribute back to society. The proposed work supports progress in science and engineering, yet it also enhances the potential for current educational applications and future research studies. Finally, this project builds on Federal and State efforts to offer people with disabilities educational and vocational services.&nbsp;</p> <p>This project developed and combine novel software algorithms that target digital facial recognition patterns and speech-based waveforms, in order to enhance the emotional content of conversational communication for individuals who cannot speak volitionally. The ability to add emotional content onto synthesized speech is a new technology that could substantially benefit individuals with communication challenges, such as people with autism spectrum disorder. This project developed a prototype to facilitate and augment emotional expression and, therefore, increase communicative competence for individuals who use software-based synthesized speech systems to express their ideas, feelings, and wishes. Measuring whether or not this project accurately conveys human emotion in speech and facial expressions requires a perceptual test involving neurotypical adults. The purpose of this is to ensure that the facial expressions as well as the pitch contour and vocal emphasis align with the emotion labels (i.e., happy, sad, or angry). In order to accurately calculate the judges' inter-rater reliability, the study will used a method that determines the extent to which judges' ratings agree, relative to how much they would likely conform, if they were to randomly rate the same stimuli.</p> <p>The results of Phase 1 research have shown that neurotypical adults most accurately receive and identify synthesized emotional communication content when those messages are sent using facial expressions and speech software, which specifically incorporates tone-of-voice algorithms (i.e., prosody, or the up-and-down pitch and volume contours that accompany emotional content during speech). In the area of communicative sciences, this is new &mdash; and possibly paradigm-shifting &mdash; information. Due to recent advances in mobile communication, facial recognition technology, and software development, communicative sciences have rapidly progressed past studying real-time face-to-face conversations and have advanced toward researching computer-mediated exchanges that involve a variety of media: texts, emails, pictures, short videos &mdash; and even symbolic representations of people, or avatars. Given the mobile technology industry&rsquo;s recent rush toward developing virtual reality produ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Project Outcomes Report  SBIR-1520587 Say What I Feel    Introducing Emotional Content in Synthesized Speech via Facial Expression and Vocal Prosody    This SBIR Phase I project aims to create an emotionally expressive software-based speech-generating communication system, which is designed for individuals with little or no verbal communication ability -- particularly those with autism spectrum disorder. Current alternative communication technology does not offer the ability to express emotional content: something that is crucial to effective, comprehensible communication at home, in the community, and at work. The ability to convey emotions -- sadness, anger, or happiness, for example -- through one's tone of voice or facial expressions has far-reaching educational and vocational benefits for individuals with communication challenges. In other words, emotional content helps clarify the communicative intent behind a spoken message. When non-verbal individuals can communicate thoroughly and effectively, they increase their ability to receive a free and appropriate public education, and they expand their vocational opportunities. When individuals can receive an education and find a job, they can contribute back to society. The proposed work supports progress in science and engineering, yet it also enhances the potential for current educational applications and future research studies. Finally, this project builds on Federal and State efforts to offer people with disabilities educational and vocational services.   This project developed and combine novel software algorithms that target digital facial recognition patterns and speech-based waveforms, in order to enhance the emotional content of conversational communication for individuals who cannot speak volitionally. The ability to add emotional content onto synthesized speech is a new technology that could substantially benefit individuals with communication challenges, such as people with autism spectrum disorder. This project developed a prototype to facilitate and augment emotional expression and, therefore, increase communicative competence for individuals who use software-based synthesized speech systems to express their ideas, feelings, and wishes. Measuring whether or not this project accurately conveys human emotion in speech and facial expressions requires a perceptual test involving neurotypical adults. The purpose of this is to ensure that the facial expressions as well as the pitch contour and vocal emphasis align with the emotion labels (i.e., happy, sad, or angry). In order to accurately calculate the judges' inter-rater reliability, the study will used a method that determines the extent to which judges' ratings agree, relative to how much they would likely conform, if they were to randomly rate the same stimuli.  The results of Phase 1 research have shown that neurotypical adults most accurately receive and identify synthesized emotional communication content when those messages are sent using facial expressions and speech software, which specifically incorporates tone-of-voice algorithms (i.e., prosody, or the up-and-down pitch and volume contours that accompany emotional content during speech). In the area of communicative sciences, this is new &mdash; and possibly paradigm-shifting &mdash; information. Due to recent advances in mobile communication, facial recognition technology, and software development, communicative sciences have rapidly progressed past studying real-time face-to-face conversations and have advanced toward researching computer-mediated exchanges that involve a variety of media: texts, emails, pictures, short videos &mdash; and even symbolic representations of people, or avatars. Given the mobile technology industryÆs recent rush toward developing virtual reality products (CNN Money 2016), our teamÆs findings could contribute to new branches of communicative sciences &mdash; mainly the study of communication exchanges in computer-mediated c...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
