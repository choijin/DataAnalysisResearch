<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Text-to-Image Reference Resolution for Image Understanding and Manipulation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>275000.00</AwardTotalIntnAmount>
<AwardAmount>275000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops new technologies at the interface of computer vision and natural language processing to understand text-to-image relationships. For example, given a captioned image, the project develops techniques which determine which words (e.g. "woman talking on phone", "The farther vehicle") correspond to which image parts. From robotics to human-computer interaction, there are numerous real-world tasks that benefit from practical systems to identify objects in scenes based on language and understand language based on visual context. In particular, the project develops the first language-based image authoring tool which allows users to edit or synthesize realistic imagery using only natural language (e.g. "delete the garbage truck from this photo" or "make an image with three boys chasing a shaggy dog"). Beyond the immediate impact of creating new ways for users to access and author digital images, the broader impacts of this work include three focus areas: the development of new benchmarks for the vision and language communities, outreach and undergraduate research, and leadership in promoting diversity. &lt;br/&gt;&lt;br/&gt;At the core of the project are new techniques for large-scale text-to-image reference resolution (TIRR) that enable systems to automatically identify the image regions that depict entities described in natural language sentences or commands. These techniques advance image interpretation by enabling systems to perform partial matching between images and sentences, referring expression understanding, and image-based question answering. They also advance image manipulation by enabling systems that can synthesize images starting from a textual description, or modify images based on natural language commands. The main technical contributions of the project are:  (1) benchmark datasets for TIRR with comprehensive large-scale gold standard annotations that will make TIRR a standard task for recognition; (2) principled new representations for text-to-image annotations that expose the compositional nature of language using the formalism of the denotation graph; (3) new models for TIRR that perform an explicit alignment (grounding) of words and phrases to image regions guided by the structure of the denotation graph; (4) applications of TIRR methods to referring expression understanding and visual question answering; and (5) applications of TIRR to image creation and manipulation based on natural language input.</AbstractNarration>
<MinAmdLetterDate>03/23/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/01/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1561968</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Hays</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James H Hays</PI_FULL_NAME>
<EmailAddress>hays@gatech.edu</EmailAddress>
<PI_PHON>4014020486</PI_PHON>
<NSF_ID>000561883</NSF_ID>
<StartDate>03/23/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Ave., NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~105170</FUND_OBLG>
<FUND_OBLG>2017~169830</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Intellectual Merit: My lab's work for this award demonstrated how deep networks can be used for controllable image synthesis and manipulation. Four of our projects were published at high profile computer vision conferences:</p> <p><strong>SwapNet: Garment Transfer in Single View Images</strong><br /> <a href="https://amitraj93.github.io/">Amit Raj</a><a>, </a><a href="http://www.cc.gatech.edu/~psangklo/">Patsorn Sangkloy</a>,  Huiwen Chang, James Hays, <a href="http://www.duygu-ceylan.com/">Duygu Ceylan</a>,  and <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>.<br /> ECCV 2018.  <a href="http://www.eye.gatech.edu/swapnet/">Project page</a>,  <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf">Paper</a></p> <p><strong>SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis.</strong><br /> Wengling Chen and James Hays.<br /> CVPR 2018.  <a href="https://arxiv.org/abs/1801.02753">Paper (arXiv)</a></p> <p><strong>TextureGAN: Controlling Deep Image Synthesis with Texture Patches.</strong><br /> Wenqi Xian,  <a href="http://www.cc.gatech.edu/~psangklo/">Patsorn Sangkloy</a>,  Varun Agrawal, Amit Raj, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>,  <a href="https://research.adobe.com/person/chen-fang/">Chen Fang</a>,  <a href="http://www.yf.io/">Fisher Yu</a>, and James Hays.<br /> CVPR 2018.&nbsp;  <a href="https://arxiv.org/abs/1706.02823">Paper (arXiv)</a></p> <p><strong>Scribbler: Controlling Deep Image Synthesis with Sketch and Color.</strong><br /> <a href="http://www.cc.gatech.edu/~psangklo/">Patsorn Sangkloy</a>,  <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>,  <a href="https://research.adobe.com/person/chen-fang/">Chen Fang</a>,  <a href="http://www.yf.io/">Fisher Yu</a>, and James Hays.<br /> CVPR 2017.  <a href="http://scribbler.eye.gatech.edu/">Project Page</a>,  <a href="https://arxiv.org/abs/1612.00835">Paper (arXiv)</a>, <a href="https://www.youtube.com/watch?v=seBbuYiBXSc">Adobe Max Demo</a></p> <p>Broader impacts: The work for this award can have real world impact through content creation tools such as those created by Adobe. We collaborated with Adobe on "Scribbler" for instance and they featured this work at their Adobe Max Conference: https://www.youtube.com/watch?v=seBbuYiBXSc . This project also supported the training of underrepresented female researchers Patsorn Sankloy (PhD student, Georgia Tech) and Wenqi Xian (MS student at Georgia Tech and how a PhD student at Cornell).</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/20/2020<br>      Modified by: James&nbsp;H&nbsp;Hays</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1561968/1561968_10416134_1582228007837_paper_figure--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1561968/1561968_10416134_1582228007837_paper_figure--rgov-800width.jpg" title="Sketch 2 Photo examples"><img src="/por/images/Reports/POR/2020/1561968/1561968_10416134_1582228007837_paper_figure--rgov-66x44.jpg" alt="Sketch 2 Photo examples"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We train deep networks which convert sketches into realistic photographs. Users can add color strokes to constrain the output, as demonstrated on the right.</div> <div class="imageCredit">Patsorn Sangkloy</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">James&nbsp;H&nbsp;Hays</div> <div class="imageTitle">Sketch 2 Photo examples</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1561968/1561968_10416134_1582228134256_ExampleTransfer--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1561968/1561968_10416134_1582228134256_ExampleTransfer--rgov-800width.jpg" title="Garment transfer examples"><img src="/por/images/Reports/POR/2020/1561968/1561968_10416134_1582228134256_ExampleTransfer--rgov-66x44.jpg" alt="Garment transfer examples"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We train algorithms which disentangle body pose and clothing appearance which allows us to transfer clothing between photographs.</div> <div class="imageCredit">Amit Raj</div> <div class="imageSubmitted">James&nbsp;H&nbsp;Hays</div> <div class="imageTitle">Garment transfer examples</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit: My lab's work for this award demonstrated how deep networks can be used for controllable image synthesis and manipulation. Four of our projects were published at high profile computer vision conferences:  SwapNet: Garment Transfer in Single View Images  Amit Raj, Patsorn Sangkloy,  Huiwen Chang, James Hays, Duygu Ceylan,  and Jingwan Lu.  ECCV 2018.  Project page,  Paper  SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis.  Wengling Chen and James Hays.  CVPR 2018.  Paper (arXiv)  TextureGAN: Controlling Deep Image Synthesis with Texture Patches.  Wenqi Xian,  Patsorn Sangkloy,  Varun Agrawal, Amit Raj, Jingwan Lu,  Chen Fang,  Fisher Yu, and James Hays.  CVPR 2018.   Paper (arXiv)  Scribbler: Controlling Deep Image Synthesis with Sketch and Color.  Patsorn Sangkloy,  Jingwan Lu,  Chen Fang,  Fisher Yu, and James Hays.  CVPR 2017.  Project Page,  Paper (arXiv), Adobe Max Demo  Broader impacts: The work for this award can have real world impact through content creation tools such as those created by Adobe. We collaborated with Adobe on "Scribbler" for instance and they featured this work at their Adobe Max Conference: https://www.youtube.com/watch?v=seBbuYiBXSc . This project also supported the training of underrepresented female researchers Patsorn Sankloy (PhD student, Georgia Tech) and Wenqi Xian (MS student at Georgia Tech and how a PhD student at Cornell).          Last Modified: 02/20/2020       Submitted by: James H Hays]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
