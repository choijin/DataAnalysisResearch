<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Leveraging Implicit Human Cues to Design Effective Behaviors for Collaborative Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>174300.00</AwardTotalIntnAmount>
<AwardAmount>174300</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Robots have the potential to significantly benefit society by actively collaborating with people in critical domains including manufacturing, healthcare, and space exploration.  But to provide effective assistance, robots must be able to work with people in a natural, intuitive, and socially adept manner.  Current human-robot collaborations require that people explicitly communicate their goals and desired responses to robotic partners.  As a result, joint human-robot activities bear little resemblance to scenarios involving human-human teamwork, where people are able to understand their partner's implicit cues, such as eye gaze, facial expressions, and intonations, and intuit appropriate responses, such as moving to a certain location, preemptively fetching a tool, or providing a clarification.  The PI's goal in this project is to establish a research program that will explore the design of effective behaviors for collaborative robots by developing computational models that enable them to sense implicit human communicative cues and guide robot responses by inferring cue intent, and to evaluate the effectiveness of the new algorithms in human-robot studies.  The research holds significant promise of benefiting society by helping to achieve a vision of robots acting as key contributors, partners, and assistants in human work, with applications across a range of activities including domestic housework, manufacturing, construction, healthcare, and space exploration.  In addition to disseminating project outcomes to the larger research community, the PI will build on his successful past outreach activities to provide opportunities for K-12 summer programs centered on robotics and computer science education.&lt;br/&gt;&lt;br/&gt;To these ends, the PI will address the challenge of designing effective collaborative robots by developing a preliminary framework, process, and set of methods to sense and respond to implicit human communicative behaviors.  His approach will involve (1) observing and classifying implicit cues and responses for human-human teams engaged in an archetypical collaborative task, (2) developing computational models of the relationships between goals, cues, and responses using features and parameters extracted from observed behaviors, (3) integrating implicit cue sensing and response algorithms to guide robot behaviors in specific collaborative use cases, and (4) evaluating the effectiveness of these behaviors on collaborative task outcomes.  This research will produce a set of generalizable design principles for collaborative robots, generate open-source algorithms showcasing practical implementations, and advance knowledge regarding computational understanding of human behaviors.  Overall, the work will lead to robots that are able to work more effectively with people and accelerate the integration of assistive robots into society.  It will synthesize theories of human communication and explore their application to human-robot interaction, as well as advancing knowledge regarding how robots might provide assistance as human collaborators and the types of sensors necessary for robots working closely with human partners.  Implicit sensing and response algorithms that have been empirically validated in HRI experiments will be disseminated as modules for the open-source Robotic Operating System (ROS).</AbstractNarration>
<MinAmdLetterDate>06/14/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/01/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1566612</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Szafir</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Szafir</PI_FULL_NAME>
<EmailAddress>daniel.szafir@colorado.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000705038</NSF_ID>
<StartDate>06/14/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090572</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, 572 UCB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~94775</FUND_OBLG>
<FUND_OBLG>2017~79525</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This research investigated how to improve human-robot teaming. Robots have the potential to significantly benefit society by actively collaborating with people in critical domains such as manufacturing, healthcare, and space exploration. In order to provide such assistance effectively, robots must be able to work with people in a natural, intuitive, and socially adept manner. Currently, robots typically lack adequate abilities to understand and reason about humans and thus human-robot collaborations often require that people explicitly communicate their goals and desired responses to robotic partners. As a result, joint human-robot activities are often inefficient and may even break down completely, bearing little resemblance to scenarios involving human-human teamwork, where people are able to understand their partner's implicit cues, such as eye gaze, facial expressions, and intonations, and intuit appropriate responses, such as moving to a certain location, preemptively fetching a tool, or providing a clarification. </span></p> <p>The goal of this project was to develop computational models that might enable robots to better sense implicit human communicative cues, infer human goals, and generate appropriate robot responses. We accomplished this goal through work on several sub-projects. In one sub-project, we recorded humans working in teams to assemble a small structure and trained a computational model to learn what features of human communication indicated various types of assembly practices, such as searching for parts or connecting two pieces. We then implemented this model on a physical robot and ran an experiment in which the robot assisted a human in a similar type of assembly task. We found that our model enabled the robot to assist the user even though the robot had no underlying knowledge about what the user was actually trying to assemble.</p> <p>In another sub-project, we examined robot teleoperation, where a user directly controls a robot. Teleoperation is a challenging task as robot manipulators typically have many joints and thus high degrees-of-freedom. One approach towards mitigating issues that arise in teleoperation is known as shared autonomy, where an autonomous system attempts to learn what the user is trying to achieve through teleoperation and, once the goal(s) is inferred, the system may take direct control over the robot to automate the task or influence user inputs in an arbitration scheme. Our research extended past models in shared autonomy to enable the system to learn user goals more quickly in scenarios where there are many potential user goals. Our approach was based on active learning and involved putting the robot into states where future operator inputs would better inform the system about the user goals.</p> <p>In a third sub-project, we focused on natural language, which is a very powerful communicative tool in human-human teamwork. One common difficulty that robots have when working with people is that humans often use prepositional phrases involving some amount of ambiguity (e.g., "put that hammer over there next to the box"). In this research, we collected large amounts of data on how humans use various prepositional phrases ("on", "above", "left of", etc.) to develop models based on empirical data that enable robots to better understand such phrases. We evaluated these models and found significant improvements in prepositional phrase disambiguation and also contributed an added benefit by enabling the robot to reason about the confidence of how well it believes it understands the phrase.</p> <p>Overall, this research focused on benefiting society by helping to achieve a vision of robots acting as key contributors, partners, and assistants in human work. Our findings may have applications across a range of activities including domestic housework, manufacturing, construction, healthcare, and space exploration. In addition to disseminating our research outcomes to the larger research community, we also engaged in outreach activities to provide opportunities to local middle and high school students centered around robotics and computer science education.</p><br> <p>            Last Modified: 12/10/2019<br>      Modified by: Daniel&nbsp;Szafir</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This research investigated how to improve human-robot teaming. Robots have the potential to significantly benefit society by actively collaborating with people in critical domains such as manufacturing, healthcare, and space exploration. In order to provide such assistance effectively, robots must be able to work with people in a natural, intuitive, and socially adept manner. Currently, robots typically lack adequate abilities to understand and reason about humans and thus human-robot collaborations often require that people explicitly communicate their goals and desired responses to robotic partners. As a result, joint human-robot activities are often inefficient and may even break down completely, bearing little resemblance to scenarios involving human-human teamwork, where people are able to understand their partner's implicit cues, such as eye gaze, facial expressions, and intonations, and intuit appropriate responses, such as moving to a certain location, preemptively fetching a tool, or providing a clarification.   The goal of this project was to develop computational models that might enable robots to better sense implicit human communicative cues, infer human goals, and generate appropriate robot responses. We accomplished this goal through work on several sub-projects. In one sub-project, we recorded humans working in teams to assemble a small structure and trained a computational model to learn what features of human communication indicated various types of assembly practices, such as searching for parts or connecting two pieces. We then implemented this model on a physical robot and ran an experiment in which the robot assisted a human in a similar type of assembly task. We found that our model enabled the robot to assist the user even though the robot had no underlying knowledge about what the user was actually trying to assemble.  In another sub-project, we examined robot teleoperation, where a user directly controls a robot. Teleoperation is a challenging task as robot manipulators typically have many joints and thus high degrees-of-freedom. One approach towards mitigating issues that arise in teleoperation is known as shared autonomy, where an autonomous system attempts to learn what the user is trying to achieve through teleoperation and, once the goal(s) is inferred, the system may take direct control over the robot to automate the task or influence user inputs in an arbitration scheme. Our research extended past models in shared autonomy to enable the system to learn user goals more quickly in scenarios where there are many potential user goals. Our approach was based on active learning and involved putting the robot into states where future operator inputs would better inform the system about the user goals.  In a third sub-project, we focused on natural language, which is a very powerful communicative tool in human-human teamwork. One common difficulty that robots have when working with people is that humans often use prepositional phrases involving some amount of ambiguity (e.g., "put that hammer over there next to the box"). In this research, we collected large amounts of data on how humans use various prepositional phrases ("on", "above", "left of", etc.) to develop models based on empirical data that enable robots to better understand such phrases. We evaluated these models and found significant improvements in prepositional phrase disambiguation and also contributed an added benefit by enabling the robot to reason about the confidence of how well it believes it understands the phrase.  Overall, this research focused on benefiting society by helping to achieve a vision of robots acting as key contributors, partners, and assistants in human work. Our findings may have applications across a range of activities including domestic housework, manufacturing, construction, healthcare, and space exploration. In addition to disseminating our research outcomes to the larger research community, we also engaged in outreach activities to provide opportunities to local middle and high school students centered around robotics and computer science education.       Last Modified: 12/10/2019       Submitted by: Daniel Szafir]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
