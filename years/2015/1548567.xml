<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Robots that Learn to Communicate with Humans Tthrough Natural Dialog</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This EArly Grant for Exploratory Research explores the possibility of developing more user-friendly and capable robots that learn to understand commands in natural human language. The experimental system developed aims to engage users in natural conversation, clarifying linguistic instructions that cannot be understood, and learning from this interaction to more robustly interpret future commands. This fundamentally new approach is hypothesized to overcome limitations of more-costly previous approaches that require either direct programming or detailed annotation of per-assembled linguistic data, and still frequently fail to cover issues that arise in real user interactions. The resulting exploratory prototype is evaluated on real interactions with human users, experimentally testing its ability to improve its accuracy and flexibility at interpreting human instructions over time, through normal everyday use. This novel approach aims to improve human interaction with intelligent multi-robot systems that aid the residents and visitors of a large, multi-use building. This fundamental research also supports computer-science education in the growing areas of natural-language processing, human-robot interaction, and machine learning, where there is significant national demand for knowledgeable personnel.&lt;br/&gt;&lt;br/&gt;The technical approach explored is a novel integration of learning techniques from three currently disparate areas: semantic parsing, spoken dialog management, and perceptual language grounding. Semantic parsing is the task of mapping natural language to a formal computer-interpretable language using compositional semantics based on syntactic linguistic structure. Dialog management concerns controlling multi-turn natural language interaction to aid comprehension and task completion. Perceptual grounding concerns associating words and phrases in language to objects, properties and relations in the world as perceived by the robot's sensors. Although there has been recent significant progress in each of these individual areas, no one has previously explored integrating them to support learning for human-robot communication through natural dialog. This exploratory research adapts and integrates techniques for semantic-parser learning using combinatory categorial grammar, dialog management using Partially Observable Markov Decision Processes, and multi-modal language grounding using both visual and haptic sensors, in order to develop a novel dialog system for communicating with robots that comprise the innovative Building Wide Intelligence system being developed at the University of Texas at Austin. The exploratory methods are evaluated using controlled experiments on a range of tasks using both on-line simulations and crowdsourced users, and natural user interaction with a mobile robot platform consisting of a wheeled Segway base and a Kinova robot arm.</AbstractNarration>
<MinAmdLetterDate>07/13/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1548567</AwardID>
<Investigator>
<FirstName>Raymond</FirstName>
<LastName>Mooney</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raymond J Mooney</PI_FULL_NAME>
<EmailAddress>mooney@cs.utexas.edu</EmailAddress>
<PI_PHON>5124719558</PI_PHON>
<NSF_ID>000308265</NSF_ID>
<StartDate>07/13/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Stone</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter H Stone</PI_FULL_NAME>
<EmailAddress>pstone@cs.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000156504</NSF_ID>
<StartDate>07/13/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121757</ZipCode>
<StreetAddress><![CDATA[2317 Speedway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This project has developed human-robot dialog systems that learn to communicate with users through natural dialog, learning from repeated user interactions to become more robust and capable through normal use. By learning from dialogs that clarify English commands from human users, the system incrementally improves both its ability to understand English instructions as well as engage in more effective clarification dialogs. </span></p> <p><span>The project has also developed methods that allow robots to learn to identify objects in the environment based on natural English descriptions such "the full metallic bottle."&nbsp; By enaging users in an interactive "I Spy" game, it learns the meaning of English words by connecting them to multiple robotic senses (visual, auditory, and tactile). Experiments in which human users describe a range of everyday objects demonstrated that the system learns to identify objects more accurately over time and that robotic senses beyond vision contribtute significantly to this improvement.</span></p> <p>Finally, the project has developed new active learning methods that allow robots to ask productive questions during normal use that improve their understanding of linguistic descriptions.&nbsp; The robot picks questions "opportunistically," taking advantage of situations in which its understanding of an object in the current environment could be improved, such as "Would you call this object 'heavy'?"&nbsp; Experimental results demonstrated that such questions improve its ability to identify objects, and that human users do not mind answering such questions, in fact they actually improves their perception of the robot's intelligence.</p><br> <p>            Last Modified: 11/26/2017<br>      Modified by: Raymond&nbsp;J&nbsp;Mooney</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1548567/1548567_10376226_1511709330336_robot-ispy1--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1548567/1548567_10376226_1511709330336_robot-ispy1--rgov-800width.jpg" title="Robot Playing &quot;I Spy&quot;"><img src="/por/images/Reports/POR/2017/1548567/1548567_10376226_1511709330336_robot-ispy1--rgov-66x44.jpg" alt="Robot Playing &quot;I Spy&quot;"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Human user describes an object in English and the robot must identify it.</div> <div class="imageCredit">Public domain</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Raymond&nbsp;J&nbsp;Mooney</div> <div class="imageTitle">Robot Playing "I Spy"</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1548567/1548567_10376226_1511709500289_robot-ispy2--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1548567/1548567_10376226_1511709500289_robot-ispy2--rgov-800width.jpg" title="Robot asking a question during &quot;I Spy&quot;"><img src="/por/images/Reports/POR/2017/1548567/1548567_10376226_1511709500289_robot-ispy2--rgov-66x44.jpg" alt="Robot asking a question during &quot;I Spy&quot;"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The robot actively asks an opportunistic  question about another object in order improve its understanding of a word.</div> <div class="imageCredit">Public Domain</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Raymond&nbsp;J&nbsp;Mooney</div> <div class="imageTitle">Robot asking a question during "I Spy"</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has developed human-robot dialog systems that learn to communicate with users through natural dialog, learning from repeated user interactions to become more robust and capable through normal use. By learning from dialogs that clarify English commands from human users, the system incrementally improves both its ability to understand English instructions as well as engage in more effective clarification dialogs.   The project has also developed methods that allow robots to learn to identify objects in the environment based on natural English descriptions such "the full metallic bottle."  By enaging users in an interactive "I Spy" game, it learns the meaning of English words by connecting them to multiple robotic senses (visual, auditory, and tactile). Experiments in which human users describe a range of everyday objects demonstrated that the system learns to identify objects more accurately over time and that robotic senses beyond vision contribtute significantly to this improvement.  Finally, the project has developed new active learning methods that allow robots to ask productive questions during normal use that improve their understanding of linguistic descriptions.  The robot picks questions "opportunistically," taking advantage of situations in which its understanding of an object in the current environment could be improved, such as "Would you call this object 'heavy'?"  Experimental results demonstrated that such questions improve its ability to identify objects, and that human users do not mind answering such questions, in fact they actually improves their perception of the robot's intelligence.       Last Modified: 11/26/2017       Submitted by: Raymond J Mooney]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
