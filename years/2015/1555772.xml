<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Model-Parallel Collaborative Filtering in Apache Spark</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>68790.00</AwardTotalIntnAmount>
<AwardAmount>68790</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sushil K Prasad</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>With data rapidly growing in size and complexity, many organizations are eager to train collaborative filtering methods on massive datasets using distributed computing environments. For instance, Netflix has hundreds of thousands of online programs to recommend to its millions of users, and Facebook has millions of users who could potentially form new links between one another. However, leading methods introduce significant algorithmic challenges in the distributed setting.  The PI proposes to study a novel algorithm designed to be efficient for large-scale data science applications. Preliminary studies demonstrate the promise of this method, and the PI proposes to formally characterize the algorithm's behavior, perform an extensive empirical evaluation, and incorporate ideas inspired by this proposal into an upcoming online course PI will be teaching.&lt;br/&gt;&lt;br/&gt;Collaborative filtering, and in particular matrix factorization, is a widely used method for devising recommender systems. However, the size of these models grows linearly with the number of users and items, and leading methods for matrix factorization introduce significant challenges in the distributed setting due to their high communication costs.  The PI proposes to study a novel model-parallel algorithm designed for Apache Spark that leverages the sparsity of the underlying data to drastically reduce this communication burden. Preliminary studies demonstrate the promise of this method, and the PI proposes to formally characterize the algorithm's behavior, perform an extensive empirical evaluation, and explore the paradigm of model-parallelism in Spark more generally for other learning settings. The PI will also incorporate ideas related to model-parallelism inspired by this proposal into an upcoming MOOC that be taught on the edX platform.</AbstractNarration>
<MinAmdLetterDate>09/03/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1555772</AwardID>
<Investigator>
<FirstName>Ameet</FirstName>
<LastName>Talwalkar</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ameet S Talwalkar</PI_FULL_NAME>
<EmailAddress>talwalkar@cmu.edu</EmailAddress>
<PI_PHON>2036752266</PI_PHON>
<NSF_ID>000585693</NSF_ID>
<StartDate>09/03/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[UCLA Computer Science]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951596</ZipCode>
<StreetAddress><![CDATA[BOX 951596, 4532N Boelter]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7361</Code>
<Text>EDUCATION AND WORKFORCE</Text>
</ProgramElement>
<ProgramReference>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~68790</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-4c3b31a2-55a0-d010-ad2b-aaad6b3bdcbb"> </span></p> <p dir="ltr"><span>Recommender systems are an important class of predictive models that aim to predict a user's rating or preference for a particular item. Collaborative filtering is a widely used method for devising recommender systems, and in particular, matrix factorization is arguably the most commonly deployed technique for collaborative filtering. With data rapidly growing in size and complexity, many organizations are eager to train matrix factorization models on massive datasets using distributed computing environments. &nbsp;For instance, Netflix has hundreds of thousands of online programs to recommend to its millions of users, and Facebook has millions of users who could potentially form new links between one another. &nbsp;However, the size of matrix factorization models grows linearly with the number of users and items, and the two leading methods for matrix factorization -- alternating least squares (ALS) and stochastic gradient descent (SGD) -- introduce significant challenges in the distributed setting due to their high communication costs. </span></p> <p dir="ltr"><span>In this project, we studied a model-parallel approach for ALS to address these challenges. Our approach leverages the sparsity of the underlying data to drastically reduce this communication burden. &nbsp;This work was motivated by our existing open-source implementation of ALS in MLlib, a leading distributed machine learning library on the Apache Spark platform. &nbsp;Working with students at UCLA, we carefully studied our method, and demonstrated its merits via extensive benchmarking studies against various competing approaches. &nbsp;We will present our work at the upcoming Southern California Machine Learning Symposium, and are currently preparing a manuscript for submission to a leading machine learning conference.</span></p> <p dir="ltr"><span> </span></p> <p dir="ltr"><span>In terms of intellectual merit, our work involved a careful study of the model-parallel paradigm for large-scale learning in bulk-synchronous computing systems like Spark. &nbsp;In particular, our work involved formally characterizing our algorithm's computational and communication complexity, highlighting its interpretation as a graph-based algorithm involving a sparse bipartite graph, and performing an extensive empirical evaluation against alternative approaches that rely on asynchronous &lsquo;parameter servers.&rsquo;</span></p> <p dir="ltr"><span> </span></p> <p><span>In terms of broader impact, our work improved the understanding of model-parallelism in systems like &nbsp;Spark. Given Spark's emergence as a leading platform for scalable data science and the increasing popularity of MLlib, this work could have a wide impact on any organization training large statistical models for a variety of learning settings, including collaborative filtering. This work also offered training opportunities. This grant supported a doctoral student for one year, and the project offered research opportunities for two graduate students in the areas of statistical machine learning and distributed computing.</span></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/11/2016<br>      Modified by: Ameet&nbsp;S&nbsp;Talwalkar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Recommender systems are an important class of predictive models that aim to predict a user's rating or preference for a particular item. Collaborative filtering is a widely used method for devising recommender systems, and in particular, matrix factorization is arguably the most commonly deployed technique for collaborative filtering. With data rapidly growing in size and complexity, many organizations are eager to train matrix factorization models on massive datasets using distributed computing environments.  For instance, Netflix has hundreds of thousands of online programs to recommend to its millions of users, and Facebook has millions of users who could potentially form new links between one another.  However, the size of matrix factorization models grows linearly with the number of users and items, and the two leading methods for matrix factorization -- alternating least squares (ALS) and stochastic gradient descent (SGD) -- introduce significant challenges in the distributed setting due to their high communication costs.  In this project, we studied a model-parallel approach for ALS to address these challenges. Our approach leverages the sparsity of the underlying data to drastically reduce this communication burden.  This work was motivated by our existing open-source implementation of ALS in MLlib, a leading distributed machine learning library on the Apache Spark platform.  Working with students at UCLA, we carefully studied our method, and demonstrated its merits via extensive benchmarking studies against various competing approaches.  We will present our work at the upcoming Southern California Machine Learning Symposium, and are currently preparing a manuscript for submission to a leading machine learning conference.   In terms of intellectual merit, our work involved a careful study of the model-parallel paradigm for large-scale learning in bulk-synchronous computing systems like Spark.  In particular, our work involved formally characterizing our algorithm's computational and communication complexity, highlighting its interpretation as a graph-based algorithm involving a sparse bipartite graph, and performing an extensive empirical evaluation against alternative approaches that rely on asynchronous ?parameter servers.?    In terms of broader impact, our work improved the understanding of model-parallelism in systems like  Spark. Given Spark's emergence as a leading platform for scalable data science and the increasing popularity of MLlib, this work could have a wide impact on any organization training large statistical models for a variety of learning settings, including collaborative filtering. This work also offered training opportunities. This grant supported a doctoral student for one year, and the project offered research opportunities for two graduate students in the areas of statistical machine learning and distributed computing.             Last Modified: 11/11/2016       Submitted by: Ameet S Talwalkar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
