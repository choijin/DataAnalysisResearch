<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Deep Learning Unmanned Aircraft Systems for High-Throughput Agricultural Disease Phenotyping</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1149273.00</AwardTotalIntnAmount>
<AwardAmount>1149273</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>An estimated 13% of crops are lost globally to plant diseases. Disease detection, identification, and tracking is performed today by crop scouts, a process that is expensive, slow and difficult, and is impractical to expand to cover all crops. The project involves developing AI-drones that work side-by-side with farmers and identify specific diseases and assess their progress.  The use of intelligent drones for crop monitoring will allow farmers to respond quickly to emergent diseases, nutrient stress, and other potentially devastating damages without the prohibitive expense of hiring a crop scout. This ability could increase productivity, and may also help predict, track and respond to epidemics for national and global food security. In addition, this technology will also be used for ongoing collection of precise plant performance data for breeding resistance.&lt;br/&gt;&lt;br/&gt;The central hypothesis of this proposal is that drones equipped with trained Convolutional Neural Networks can provide a transformative increase in actionable crop disease identification. A secondary hypothesis is that the proposed phenotyping at the individual plant level will also provide unprecedented resolution of data for future modeling, breeding, and data-driven yield optimization. In order to test this hypothesis, we will develop a UAS platform to collect images over university owned experimental crops, and aim to develop AI to identify pathologies at an accuracy that is on par with human experts. The UASs will consult human experts in ambiguous cases and gradually learn to make decisions autonomously. The key challenge will be development of AI that can reliably diagnose disease with a good accuracy of detection to false alarms. Automatic identification of disease is a challenging machine vision task given the complexity of images, exacerbated by variable lighting and weather conditions, and navigation/stability control.</AbstractNarration>
<MinAmdLetterDate>08/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>10/20/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1527232</AwardID>
<Investigator>
<FirstName>Hod</FirstName>
<LastName>Lipson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hod Lipson</PI_FULL_NAME>
<EmailAddress>hod.lipson@columbia.edu</EmailAddress>
<PI_PHON>6075924383</PI_PHON>
<NSF_ID>000097015</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate>09/14/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rebecca</FirstName>
<LastName>Nelson</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rebecca J Nelson</PI_FULL_NAME>
<EmailAddress>rjn7@cornell.edu</EmailAddress>
<PI_PHON>6072546499</PI_PHON>
<NSF_ID>000228063</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Gore</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael A Gore</PI_FULL_NAME>
<EmailAddress>mag87@cornell.edu</EmailAddress>
<PI_PHON>6072555014</PI_PHON>
<NSF_ID>000643489</NSF_ID>
<StartDate>10/20/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Gore</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael A Gore</PI_FULL_NAME>
<EmailAddress>mag87@cornell.edu</EmailAddress>
<PI_PHON>6072555014</PI_PHON>
<NSF_ID>000643489</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate>09/14/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148537501</ZipCode>
<StreetAddress><![CDATA[124 Hoy Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~1149273</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Globally, it is estimated that more than 10% of crop yields are reduced by plant diseases each year. In the United States and parts of Canada, northern leaf blight (NLB), a fungal leaf disease of maize, has progressively become more severe in the past 5 years. In maize, breeding for genetic resistance is the most effective and economical approach for control of NLB. When breeding for NLB resistance, the visual scoring of gray-brown necrotic NLB lesions at multiple time points throughout the growing season is essential, but this effort is very time-consuming and prone to discrepancies between different human raters. To this end, our team of plant pathologists, plant geneticists, and computer scientists made great strides towards developing a non-destructive, image-based phenotyping system that allows for rapid and accurate detection and quantification of NLB lesions. Deep learning algorithms were trained on expert-generated or crowdsourced data to accurately detect and segment individual NLB lesions at high spatial resolution in field images taken by unoccupied aerial vehicles (UAVs). We also trained a deep learning algorithm to detect grassy weeds in UAV imagery collected from maize fields. Additionally, we developed and demonstrated a UAV platform that autonomously detects, tracks, and follows another UAV. The integration of these trained deep learning algorithms with UAVs has the potential to allow for more precise selection of resistant plants in breeding programs and monitoring disease outbreaks in growers' fields. All of the collected image data, generated disease lesion annotations, and developed computer code produced by this project have been deposited in the public domain.</p><br> <p>            Last Modified: 10/23/2019<br>      Modified by: Michael&nbsp;A&nbsp;Gore</p> </div> <div class="porSideCol"></div> </div> <div class="porColContainerHR"> <div class="porContentCol"> <h2>Addendum # 1</h2><p>Bullock, D., Mangeni, A., Wiesner-Hanks, T., DeChant, C., Stewart, E.L., Kaczmar, N., Nelson, R.J., Gore, M.A., and Lipson, H. 2019. Automated weed detection in aerial imagery with context. arXiv:1910.00652v3</p> <p>Wiesner-Hanks, T., Wu, H., Stewart, E., DeChant, C., Kaczmar, N., Lipson, H., Gore, M.A., and Nelson, R.J. 2019. Millimeter-level plant disease detection from aerial photographs via deep learning and crowdsourced data. Frontiers in Plant Science 10:1550 doi: 10.3389/fpls.2019.01550</p> <p>Stewart, E.L., Wiesner-Hanks, T., Kaczmar, N., DeChant, C., Wu, H., Lipson, H., Nelson, R.J., and Gore, M.A. 2019. Quantitative phenotyping of northern blight in UAV images using deep learning. Remote Sensing 11:2209. doi.org/10.3390/rs1119220</p> <p>Wu, H., Wiesner-Hanks, T., Stewart, E.L., DeChant, C., Kaczmar, N., Gore, M.A., Nelson, R.J., and Lipson, H. 2019. Autonomous detection of plant disease symptoms directly from aerial imagery. The Plant Phenome Journal 2:190006 doi: 10.2135/tppj2019.03.0006</p> <p>Wiesner-Hanks, T., Stewart, E. L., Kaczmar, N., DeChant, C., Wu, H., Nelson, R. J., Lipson, H., and Gore, M. A. 2018. Image set for deep learning: field images of maize annotated with disease symptoms. BMC Research Notes 11:440. doi:10.1186/s13104-018-3548-6</p> <p>DeChant, C., Wiesner-Hanks, T., Chen, S., Stewart, E., Yosinski, J., Gore, M. A., Nelson, R., Lipson, H. 2017. Automated identification of northern leaf blight-infected maize plants from field imagery using deep learning. Phytopathology 107:1426-1432. doi:10.1094/PHYTO-11-16-0417-R</p><br> <p>Added: 12/23/2019<br>Submitted by: Michael&nbsp;A&nbsp;Gore</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Globally, it is estimated that more than 10% of crop yields are reduced by plant diseases each year. In the United States and parts of Canada, northern leaf blight (NLB), a fungal leaf disease of maize, has progressively become more severe in the past 5 years. In maize, breeding for genetic resistance is the most effective and economical approach for control of NLB. When breeding for NLB resistance, the visual scoring of gray-brown necrotic NLB lesions at multiple time points throughout the growing season is essential, but this effort is very time-consuming and prone to discrepancies between different human raters. To this end, our team of plant pathologists, plant geneticists, and computer scientists made great strides towards developing a non-destructive, image-based phenotyping system that allows for rapid and accurate detection and quantification of NLB lesions. Deep learning algorithms were trained on expert-generated or crowdsourced data to accurately detect and segment individual NLB lesions at high spatial resolution in field images taken by unoccupied aerial vehicles (UAVs). We also trained a deep learning algorithm to detect grassy weeds in UAV imagery collected from maize fields. Additionally, we developed and demonstrated a UAV platform that autonomously detects, tracks, and follows another UAV. The integration of these trained deep learning algorithms with UAVs has the potential to allow for more precise selection of resistant plants in breeding programs and monitoring disease outbreaks in growers' fields. All of the collected image data, generated disease lesion annotations, and developed computer code produced by this project have been deposited in the public domain.       Last Modified: 10/23/2019       Submitted by: Michael A Gore Bullock, D., Mangeni, A., Wiesner-Hanks, T., DeChant, C., Stewart, E.L., Kaczmar, N., Nelson, R.J., Gore, M.A., and Lipson, H. 2019. Automated weed detection in aerial imagery with context. arXiv:1910.00652v3  Wiesner-Hanks, T., Wu, H., Stewart, E., DeChant, C., Kaczmar, N., Lipson, H., Gore, M.A., and Nelson, R.J. 2019. Millimeter-level plant disease detection from aerial photographs via deep learning and crowdsourced data. Frontiers in Plant Science 10:1550 doi: 10.3389/fpls.2019.01550  Stewart, E.L., Wiesner-Hanks, T., Kaczmar, N., DeChant, C., Wu, H., Lipson, H., Nelson, R.J., and Gore, M.A. 2019. Quantitative phenotyping of northern blight in UAV images using deep learning. Remote Sensing 11:2209. doi.org/10.3390/rs1119220  Wu, H., Wiesner-Hanks, T., Stewart, E.L., DeChant, C., Kaczmar, N., Gore, M.A., Nelson, R.J., and Lipson, H. 2019. Autonomous detection of plant disease symptoms directly from aerial imagery. The Plant Phenome Journal 2:190006 doi: 10.2135/tppj2019.03.0006  Wiesner-Hanks, T., Stewart, E. L., Kaczmar, N., DeChant, C., Wu, H., Nelson, R. J., Lipson, H., and Gore, M. A. 2018. Image set for deep learning: field images of maize annotated with disease symptoms. BMC Research Notes 11:440. doi:10.1186/s13104-018-3548-6  DeChant, C., Wiesner-Hanks, T., Chen, S., Stewart, E., Yosinski, J., Gore, M. A., Nelson, R., Lipson, H. 2017. Automated identification of northern leaf blight-infected maize plants from field imagery using deep learning. Phytopathology 107:1426-1432. doi:10.1094/PHYTO-11-16-0417-R Last Modified: 12/23/2019 Submitted by: Michael A Gore]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
