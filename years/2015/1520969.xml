<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Particle Tracking at High Luminosity on Heterogeneous, Parallel Processor Architectures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03010000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>PHY</Abbreviation>
<LongName>Division Of Physics</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Particle physics experiments at the Large Hadron Collider (LHC) at CERN seek to explore fundamental questions in modern physics, such as how particles attain mass, why gravity is weak, and the nature of dark matter. The large quantity of data produced at experimental facilities such as the LHC requires the development of complex pattern recognition algorithms and software techniques to achieve the scientific goals of these physics programs. This project will investigate new algorithms and techniques for data analysis using emerging computing processor architectures. These activities will enable the LHC experiments to take data more efficiently and improve the quality of the data that is recorded in order to extend the reach of the next generation of discoveries from planned hardware upgrades at the LHC over the next decade. The results of this research will significantly reduce the cost of computing for all LHC experiments. Software source code tools will be made available to the particle physics community. The investigators will host workshops to train post-doctoral fellows and graduate students from all areas of particle physics on how to use these advanced techniques.  This training is valuable preparation for dealing with big data science in general.&lt;br/&gt;&lt;br/&gt;This project will support research focused on novel compute architectures for parallelized and vectorized charged particle track reconstruction. This research will improve the reach of energy-frontier particle physics experiments, such as the ATLAS, CMS, LHCb and ALICE experiments at the LHC and any other fields where studying the passage of charged particles is of critical importance.  The science targeted in this project includes studying the properties of the Higgs boson, probing dark matter by searching for supersymmetry, and exploring the unknown by looking for such proposed effects as large extra space-time dimensions.</AbstractNarration>
<MinAmdLetterDate>07/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1520969</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Wittich</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter Wittich</PI_FULL_NAME>
<EmailAddress>wittich@cornell.edu</EmailAddress>
<PI_PHON>6072553368</PI_PHON>
<NSF_ID>000231810</NSF_ID>
<StartDate>07/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>148502820</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1221</Code>
<Text>HEP-High Energy Physics</Text>
</ProgramElement>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7569</Code>
<Text>CYBERINFRASTRUCTURE/SCIENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~150000</FUND_OBLG>
<FUND_OBLG>2016~150000</FUND_OBLG>
<FUND_OBLG>2017~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is one of the largest scientific instruments ever created. In it we recreate in the lab some of the conditions that existed a ten-billionth of a second after the big bang. Experiments like the Compact Muon Solenoid (CMS) try to detect the remnants of particles created in the proton-proton collisions of the collider and reconstruct the particles that are created in the collision.&nbsp;</p> <p>Charged-particle reconstruction (both finding and fitting) is one of the most important features of a collider detector experiment. Efficient and accurate reconstruction of charged-particles is critically important for the physics performance of such experiments. It is also the most complex, time-consuming, and costly part of a collision event reconstruction. This task gets more complex (and expensive) with increased occupancy in the detector.&nbsp;</p> <p>The next major upgrade of the LHC accelerator aims at unprecedented brightness of beams that will result in much more physics data collected but also &ldquo;dirtier&rdquo; conditions; each collision of interest will be accompanied by ~200 parasitic collisions. This will pose major challenges to all detector components, but especially to charge-particle detection and&nbsp;reconstruction.</p> <p>The famous &ldquo;Moore&rsquo;s law&rdquo; states that chip transistor counts increases exponentially over time for the same cost. Over four decades, these decreases in transistor cost turned into exponential gains in the performance of software applications&nbsp;like those used in particle physics. Around 2005, the computing processor market reached an epochal turning point: power density limitations in chips ended this trend, and our applications no longer immediately run exponentially faster on subsequent generations of processors. This is true even though the underlying transistor count continues to increase per Moore&rsquo;s law. What changed is the types of processors. Due to this change, the performance improvements are now more on the order of 20-25% per year, and many of the improvements in FLOP counts come from addition of specialized hardware such as vector units and many cores. Over the next decade, this corresponds to a factor of six growth instead of a factor of&nbsp;&sim;&nbsp;30 as one would expect from Moore&rsquo;s Law, and not enough to keep up with the exponential growth in the reconstruction time due to the harsher conditions expected during the High-Luminosity (HL) run of the LHC. A change is required to move from the sequential applications of today to vectorized, parallelized applications of tomorrow.&nbsp;</p> <p>Using these new computing resources also requires new techniques and new strategies. That was the goal of this 'physics at the information frontier' proposal -- to merge the well-understood, well-performing serial architectures used in charged particle reconstruction on these new hardware platforms, to take advantage of this new form of Moore's Law. &nbsp;</p> <p>We were able deploy the traditional sequential and serial charged-particle reconstruction on modern architectures by parallelizing and vectorizing them to fit to the modern computer architectures. It required redesign of data structures, and finding ways to avoid bottlenecks due to specific local challenges invariably present in different parts of the reconstructed event.</p> <p>In results shown in multiple international conferences (CHEP, ACAT, CTD) as well as in internal CMS collaboration meetings we have demonstrated charged-particle reconstruction that is very similar (but not quite as good yet) physics performance (efficiency and fake rates) to the traditional one but approximately 8 times faster.&nbsp;&nbsp;The project will lead to a deployment of new software code to address the challenges of the high luminosity LHC running in the near future.&nbsp;Work will be followed up&nbsp;under the auspices of the IRIS-HEP project.</p><br> <p>            Last Modified: 08/11/2020<br>      Modified by: Peter&nbsp;Wittich</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is one of the largest scientific instruments ever created. In it we recreate in the lab some of the conditions that existed a ten-billionth of a second after the big bang. Experiments like the Compact Muon Solenoid (CMS) try to detect the remnants of particles created in the proton-proton collisions of the collider and reconstruct the particles that are created in the collision.   Charged-particle reconstruction (both finding and fitting) is one of the most important features of a collider detector experiment. Efficient and accurate reconstruction of charged-particles is critically important for the physics performance of such experiments. It is also the most complex, time-consuming, and costly part of a collision event reconstruction. This task gets more complex (and expensive) with increased occupancy in the detector.   The next major upgrade of the LHC accelerator aims at unprecedented brightness of beams that will result in much more physics data collected but also "dirtier" conditions; each collision of interest will be accompanied by ~200 parasitic collisions. This will pose major challenges to all detector components, but especially to charge-particle detection and reconstruction.  The famous "Moore’s law" states that chip transistor counts increases exponentially over time for the same cost. Over four decades, these decreases in transistor cost turned into exponential gains in the performance of software applications like those used in particle physics. Around 2005, the computing processor market reached an epochal turning point: power density limitations in chips ended this trend, and our applications no longer immediately run exponentially faster on subsequent generations of processors. This is true even though the underlying transistor count continues to increase per Moore’s law. What changed is the types of processors. Due to this change, the performance improvements are now more on the order of 20-25% per year, and many of the improvements in FLOP counts come from addition of specialized hardware such as vector units and many cores. Over the next decade, this corresponds to a factor of six growth instead of a factor of &sim; 30 as one would expect from Moore’s Law, and not enough to keep up with the exponential growth in the reconstruction time due to the harsher conditions expected during the High-Luminosity (HL) run of the LHC. A change is required to move from the sequential applications of today to vectorized, parallelized applications of tomorrow.   Using these new computing resources also requires new techniques and new strategies. That was the goal of this 'physics at the information frontier' proposal -- to merge the well-understood, well-performing serial architectures used in charged particle reconstruction on these new hardware platforms, to take advantage of this new form of Moore's Law.    We were able deploy the traditional sequential and serial charged-particle reconstruction on modern architectures by parallelizing and vectorizing them to fit to the modern computer architectures. It required redesign of data structures, and finding ways to avoid bottlenecks due to specific local challenges invariably present in different parts of the reconstructed event.  In results shown in multiple international conferences (CHEP, ACAT, CTD) as well as in internal CMS collaboration meetings we have demonstrated charged-particle reconstruction that is very similar (but not quite as good yet) physics performance (efficiency and fake rates) to the traditional one but approximately 8 times faster.  The project will lead to a deployment of new software code to address the challenges of the high luminosity LHC running in the near future. Work will be followed up under the auspices of the IRIS-HEP project.       Last Modified: 08/11/2020       Submitted by: Peter Wittich]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
