<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>VEC: Small: Collaborative Research: The Visual Computing Database: A Platform for Visual Data Processing and Analysis at Internet Scale</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>110228.00</AwardTotalIntnAmount>
<AwardAmount>110228</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project develops a new parallel computing platform, namely Visual Computing Database, that facilitates the development of applications that require visual data analysis at massive scale. The developed system combines ideas from traditional relational database management systems (to more easily and powerfully organize and manage visual data collections) with modern graphics programming abstractions for efficiently manipulating pixel data. This project implements a prototype of the visual computing database, release it as an open source project to the community, and deploys the system at scale as a service to scientists and researchers on the Google Cloud Platform. There is strong evidence that in domains ranging from personal digital assistants that interpret one's surroundings, to management of critical infrastructure in smart cities, and to scientific data analysis, a fundamental requirement of the next generation of visual and experiential computing (VEC) applications will be the efficient analysis and mining of large repositories of visual data (images, videos, RGBD, etc.). Scaling visual data analysis applications to operate on collections such as the photos and videos on Facebook and YouTube, the traffic cameras in a city, or petabytes of images in a digital sky survey, presents significant computer science challenges due to the size of visual data representations and the computational expense of algorithms understanding and manipulating large image datasets. The difficulty of developing efficient, supercomputing scale applications from scratch inhibits the field's ability to explore advanced data-driven VEC applications. &lt;br/&gt;&lt;br/&gt;A central aspect of the project is the design of a new visual data query language that integrates concepts from high performance, functional image processing languages with relational operators and spatial and temporal predicates, providing the ability to execute sequences of complex image/video analysis operations with high efficiency in the database (near the data store). Since visual analysis workloads involve tight integration of data retrieval operations and processing of the result sets (e.g., largescale machine learning, image registration/alignment, and 3D reconstruction), a key design challenge is making the results of database operations easily accessible to non-relational, supercomputing scale computations. All together the project addresses fundamental systems design questions such as: what is a good visual query language for future visual data analysis tasks? How can key operations be implemented efficiently on throughput hardware at scale? What are the appropriate benchmarks for evaluating visual data analysis systems at scale?&lt;br/&gt;&lt;br/&gt;URL: http://graphics.cs.cmu.edu/projects/visualdb</AbstractNarration>
<MinAmdLetterDate>09/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/22/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1539069</AwardID>
<Investigator>
<FirstName>Kayvon</FirstName>
<LastName>Fatahalian</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kayvon Fatahalian</PI_FULL_NAME>
<EmailAddress>kayvonf@cs.stanford.edu</EmailAddress>
<PI_PHON>6504976043</PI_PHON>
<NSF_ID>000624440</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>002Z</Code>
<Text>Intel/NSF VEC Partnership</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~110228</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-82c3a247-7fff-fd0e-9b9c-318316adc837"> <span id="docs-internal-guid-cd1214d7-7fff-4ebd-6de1-9f35b27d947b">&nbsp;</span></span></p> <p><span id="docs-internal-guid-298b3ffc-7fff-e380-4056-edbae53da353"> <p dir="ltr"><span>An increasing number of computer graphics and vision applications depend on processing and analyzing large video collections. Unfortunately, very few programmers have the capability to operate efficiently at these scales, inhibiting the field's ability to explore advanced data-driven visual computing applications. In response, this project has focused on the design of new systems for efficient and productive video analysis at scale.</span></p> <p dir="ltr"><span>In particular, our efforts have revolved around two central questions. &nbsp;(1) What are the requirements of a runtime system for efficiently executing video processing computations at scale? (2) What is the right programming system for the emerging field of large-scale visual data analytics? </span></p> <p dir="ltr"><span>We addressed the first question via the design of Scanner, a distributed system for video processing at scale. &nbsp;</span><span>Scanner organizes video collections as tables in a data store optimized for sampling frames from compressed video, and executes pixel processing computations, expressed as dataflow graphs, on these frames. The Scanner runtime schedules video analysis applications expressed using these abstractions onto heterogeneous hardware, such as multi-core CPUs, GPUs, and media processing ASICs, for high-throughput pixel processing. &nbsp;&nbsp;We have demonstrated Scanner&rsquo;s use on both graphics and video analytics applications running on hundreds of GPUs and thousands of CPU cores in the cloud. Scanner is available as an open source project, and has since been used in industry settings, such as for VR video processing at Facebook.</span></p> <p dir="ltr"><span>Scanner provides the ability to process large video collections, but a second aspect of the project centered on developing new systems that help analysts extract value from these datasets. &nbsp;Specifically, we have developed systems that </span><span>allow a user to go from a high-level question about a large collection of video, to a visual query, and then to an answer in a short period of time. &nbsp;For example, &ldquo;</span><span>What is the screen time given to hosts during TV news interviews&rdquo; in the last decade of CNN broadcasts? Or &ldquo;How often does a cinematic idiom such as &ldquo;shot-reverse-shot&rdquo; editing appear in a database of hundreds of Hollywood films&rdquo;?</span><span> &nbsp;</span><span>Our system is based on programmatic composition of multi-modal, spatiotemporal labels extracted from video. &nbsp;By combining basic primitives such as face detections, human pose estimates, phrases in transcripts, etc, we demonstrated it is possible to rapidly build higher-level labels that are useful for answering questions like the examples provided above. These queries execute in minutes on large video collections and failures are debuggable, which facilitates rapid iteration on the design of queries. We demonstrated use of this system to answer questions about a database of over 500 Hollywood films, and to address questions about gender representation and bias in a collection of over 200,000 hours of US TV News. </span></p> <p dir="ltr"><span>We are proud that over the three-year duration of the project, multiple undergraduate and masters students were afforded the opportunity to particulate in research associated with our research efforts, and have since matriculated to computer science Ph.D. programs at Stanford, MIT, and Berkeley.</span></p> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 03/11/2019<br>      Modified by: Kayvon&nbsp;Fatahalian</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      An increasing number of computer graphics and vision applications depend on processing and analyzing large video collections. Unfortunately, very few programmers have the capability to operate efficiently at these scales, inhibiting the field's ability to explore advanced data-driven visual computing applications. In response, this project has focused on the design of new systems for efficient and productive video analysis at scale. In particular, our efforts have revolved around two central questions.  (1) What are the requirements of a runtime system for efficiently executing video processing computations at scale? (2) What is the right programming system for the emerging field of large-scale visual data analytics?  We addressed the first question via the design of Scanner, a distributed system for video processing at scale.  Scanner organizes video collections as tables in a data store optimized for sampling frames from compressed video, and executes pixel processing computations, expressed as dataflow graphs, on these frames. The Scanner runtime schedules video analysis applications expressed using these abstractions onto heterogeneous hardware, such as multi-core CPUs, GPUs, and media processing ASICs, for high-throughput pixel processing.   We have demonstrated Scanner?s use on both graphics and video analytics applications running on hundreds of GPUs and thousands of CPU cores in the cloud. Scanner is available as an open source project, and has since been used in industry settings, such as for VR video processing at Facebook. Scanner provides the ability to process large video collections, but a second aspect of the project centered on developing new systems that help analysts extract value from these datasets.  Specifically, we have developed systems that allow a user to go from a high-level question about a large collection of video, to a visual query, and then to an answer in a short period of time.  For example, "What is the screen time given to hosts during TV news interviews" in the last decade of CNN broadcasts? Or "How often does a cinematic idiom such as "shot-reverse-shot" editing appear in a database of hundreds of Hollywood films"?  Our system is based on programmatic composition of multi-modal, spatiotemporal labels extracted from video.  By combining basic primitives such as face detections, human pose estimates, phrases in transcripts, etc, we demonstrated it is possible to rapidly build higher-level labels that are useful for answering questions like the examples provided above. These queries execute in minutes on large video collections and failures are debuggable, which facilitates rapid iteration on the design of queries. We demonstrated use of this system to answer questions about a database of over 500 Hollywood films, and to address questions about gender representation and bias in a collection of over 200,000 hours of US TV News.  We are proud that over the three-year duration of the project, multiple undergraduate and masters students were afforded the opportunity to particulate in research associated with our research efforts, and have since matriculated to computer science Ph.D. programs at Stanford, MIT, and Berkeley.           Last Modified: 03/11/2019       Submitted by: Kayvon Fatahalian]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
