<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI Collaborative Consortium:  Acquisition of a Shared Supercomputer by the Rocky Mountain Advanced Computing Consortium</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>700000.00</AwardTotalIntnAmount>
<AwardAmount>700000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Stefan Robila</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A cluster supercomputer is deployed by the University of Colorado Boulder (CU-Boulder) and Colorado State University (CSU) for the Rocky Mountain Advanced Computing Consortium (RMACC). This high-performance computing (HPC) system supports multiple research groups across the Rocky Mountain region in fields including astrophysics, bioinformatics, chemistry, computational fluid dynamics, earth system science, life science, material science, physics, and social sciences with advanced computing capabilities. It also provides a platform to investigate and address the impact of many-core processors on the applications that support research in these fields. &lt;br/&gt;&lt;br/&gt;The system integrates nodes populated with Intel's conventional multicore Xeon processors and Many-Integrated-Core (MIC) 'Knights Landing' Phi processors interconnected by Intel's new Omni-Path networking technology. Users of the new HPC system have access to existing data management services including data storage, data sharing, metadata consulting, and data publishing, leveraging the NSF-funded high-performance networking infrastructure and long term storage system, as well as additional cyberinfrastructure, at CU-Boulder and CSU. The many-core feature of this HPC system enhances graduate and undergraduate students' education and training as they develop, deploy, test, and run optimized applications for next generation many-core architectures. Training for researchers and students is provided through workshops appropriate for introducing diverse audiences to the efficient and effective use of HPC systems, the challenges of vectorization for single core performance, shared memory parallelism, and issues of data management. Additionally, advanced workshops on large-scale distributed computing, high-throughput computing, and data-intensive computing are offered during the year and at the annual RMACC student-centric HPC Symposium. The Symposium brings together hundreds of students, researchers, and professionals from universities, national laboratories and industry to exchange ideas and best practices in all areas of cyberinfrastructure. For-credit HPC classes will be delivered for online participation, educating the next generation of computational scientists in state-of-the-art computational techniques.</AbstractNarration>
<MinAmdLetterDate>08/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1532235</AwardID>
<Investigator>
<FirstName>Howard</FirstName>
<LastName>Siegel</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Howard J Siegel</PI_FULL_NAME>
<EmailAddress>hj@colostate.edu</EmailAddress>
<PI_PHON>9704917982</PI_PHON>
<NSF_ID>000160596</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Patrick</FirstName>
<LastName>Burns</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patrick J Burns</PI_FULL_NAME>
<EmailAddress>pburns@colostate.edu</EmailAddress>
<PI_PHON>9704911833</PI_PHON>
<NSF_ID>000235688</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Edwin</FirstName>
<LastName>Chong</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edwin K Chong</PI_FULL_NAME>
<EmailAddress>edwin.chong@colostate.edu</EmailAddress>
<PI_PHON>9704917858</PI_PHON>
<NSF_ID>000199922</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jessica</FirstName>
<LastName>Prenni</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jessica E Prenni</PI_FULL_NAME>
<EmailAddress>jessica.prenni@colostate.edu</EmailAddress>
<PI_PHON>9704910961</PI_PHON>
<NSF_ID>000522836</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Colorado State University</Name>
<CityName>Fort Collins</CityName>
<ZipCode>805232002</ZipCode>
<PhoneNumber>9704916355</PhoneNumber>
<StreetAddress>601 S Howes St</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>785979618</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLORADO STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>948905492</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Colorado State University]]></Name>
<CityName>Fort Collins</CityName>
<StateCode>CO</StateCode>
<ZipCode>805214593</ZipCode>
<StreetAddress><![CDATA[200 W. Lake St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~700000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>An advanced cluster supercomputer is in production for researchers, staff, and students at the University of Colorado Boulder and Colorado State University. This supercomputer supports over 700 researchers in areas ranging from astrophysics, bioinformatics, chemistry, computational fluid dynamics, earth system science, life science, material science, physics, and social sciences with advanced computing capabilities. Users of this computing system have access to existing data management services including data storage, data sharing, metadata consulting, and data publishing, leveraging the NSF-funded high-performance networking infrastructure and long-term storage system at CU-Boulder and CSU. The system is utilized over 90% by the researchers from both universities. Researchers from smaller schools that are members of the Rocky Mountain Advanced Computing Consortium (RMACC) have started to use the system facilitated through the NSF-funded XSEDE project.</span><br /><br /><span>Training for all faculty and students interested in using the supercomputing system was provided through about 30 workshops per year reaching about 300 attendees per year. We have started organizing workshops at smaller universities without local computing resources to enable faculty and students at these schools to use advanced computing in their education and research. RMACC hosts a yearly HighPerformance Computing (HPC) Symposium to bring together students, faculty, researchers, practitioners, and industry to discuss HPC technology and advancements. The RMACC Symposium also provides a platform for attendees to meet with and gain knowledge from other attendees and expand regional contacts and facilitate collaboration. As part of the yearly symposium, RMACC provides travel and registration scholarships to students and individuals from minority-serving institutions to increase participation in the HPC community and introduce new users to the services that the RMACC institutions provide.</span><br /><br /><span>To broaden the participation of users of the supercomputer, we are offering two new services: containers and access through JupyterHub. Containers enable workflows to be packaged and shared across platforms, enhancing portability and reproducibility. Users of the system can now bring their workflows developed somewhere else to this supercomputer and get started on their computational work without length application porting. JupyterHub enables access to the supercomputer through a web interface that combines code, data, graphics, and text. This notebook interface significantly lowers the barrier of entry because users of the systems do not need to know the underlying system details and can get their computation executed on the supercomputer by running their code in these notebooks.</span></p><br> <p>            Last Modified: 11/28/2018<br>      Modified by: Patrick&nbsp;J&nbsp;Burns</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ An advanced cluster supercomputer is in production for researchers, staff, and students at the University of Colorado Boulder and Colorado State University. This supercomputer supports over 700 researchers in areas ranging from astrophysics, bioinformatics, chemistry, computational fluid dynamics, earth system science, life science, material science, physics, and social sciences with advanced computing capabilities. Users of this computing system have access to existing data management services including data storage, data sharing, metadata consulting, and data publishing, leveraging the NSF-funded high-performance networking infrastructure and long-term storage system at CU-Boulder and CSU. The system is utilized over 90% by the researchers from both universities. Researchers from smaller schools that are members of the Rocky Mountain Advanced Computing Consortium (RMACC) have started to use the system facilitated through the NSF-funded XSEDE project.  Training for all faculty and students interested in using the supercomputing system was provided through about 30 workshops per year reaching about 300 attendees per year. We have started organizing workshops at smaller universities without local computing resources to enable faculty and students at these schools to use advanced computing in their education and research. RMACC hosts a yearly HighPerformance Computing (HPC) Symposium to bring together students, faculty, researchers, practitioners, and industry to discuss HPC technology and advancements. The RMACC Symposium also provides a platform for attendees to meet with and gain knowledge from other attendees and expand regional contacts and facilitate collaboration. As part of the yearly symposium, RMACC provides travel and registration scholarships to students and individuals from minority-serving institutions to increase participation in the HPC community and introduce new users to the services that the RMACC institutions provide.  To broaden the participation of users of the supercomputer, we are offering two new services: containers and access through JupyterHub. Containers enable workflows to be packaged and shared across platforms, enhancing portability and reproducibility. Users of the system can now bring their workflows developed somewhere else to this supercomputer and get started on their computational work without length application porting. JupyterHub enables access to the supercomputer through a web interface that combines code, data, graphics, and text. This notebook interface significantly lowers the barrier of entry because users of the systems do not need to know the underlying system details and can get their computation executed on the supercomputer by running their code in these notebooks.       Last Modified: 11/28/2018       Submitted by: Patrick J Burns]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
