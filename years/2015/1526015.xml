<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Rethinking System Software for Overprovisioned, High-Performance Computing Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>489996.00</AwardTotalIntnAmount>
<AwardAmount>505882</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Currently, the high-performance computing (HPC) community is focused on achieving exaflop performance, which is about a 30-fold improvement from the performance of the best supercomputer in the world today. Because of practical, financial, and environmental concerns, the Department of Energy is setting a power limit for achieving an exaflop at 20 megawatts.  As today's top machines generally consume between five and 20 megawatts---and yet are an order of magnitude or more away from the exaflop performance target, significant hardware and software advances in HPC systems are necessary.  One way to improve hardware is to use overprovisioned systems, which contain more machines than can be fully powered simultaneously.  While overprovisioned systems have the potential to significantly improve power and performance, software will need to be redesigned to support such systems.&lt;br/&gt;&lt;br/&gt;The focus of this proposal is to design and implement software infrastructure that will support overprovisioned systems.  The key advance in the infrastructure is support of system-wide optimizations, i.e., optimizations that span multiple applications. This is in stark contrast to the current focus in HPC systems of optimizing on a per-application basis.  The developed software will consist of a job profiler, a scheduler that performs analysis on multiple jobs at a time, and a cluster-wide run-time system that jointly optimizes multiple applications based on the output of the scheduler analysis.&lt;br/&gt;&lt;br/&gt;Achieving exascale computing is an important national priority and will impact many critical application domains, such as climate/weather, renewable energy, nuclear energy, materials science, and national security.  The work described here will improve whole-system performance on power-constrained HPC systems, which is one important step towards the exascale goal.  The project plans to transfer technology resulting from this research in the form of the proposed software stack via longstanding collaborations with several national laboratories.</AbstractNarration>
<MinAmdLetterDate>08/26/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/18/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1526015</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Lowenthal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Lowenthal</PI_FULL_NAME>
<EmailAddress>dkl@cs.arizona.edu</EmailAddress>
<PI_PHON>5206268282</PI_PHON>
<NSF_ID>000516447</NSF_ID>
<StartDate>08/26/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arizona</Name>
<CityName>Tucson</CityName>
<ZipCode>857194824</ZipCode>
<PhoneNumber>5206266000</PhoneNumber>
<StreetAddress>888 N Euclid Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>806345617</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARIZONA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072459266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arizona]]></Name>
<CityName>Tucson</CityName>
<StateCode>AZ</StateCode>
<ZipCode>857210001</ZipCode>
<StreetAddress><![CDATA[888 N Euclid Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~489996</FUND_OBLG>
<FUND_OBLG>2018~15886</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Currently, the high-performance computing (HPC) community is focused<br />on achieving practical exascale computing, where nontrivial programs<br />can reach a sustained exaflop.&nbsp; One of the key barriers to exascale is<br />solving the power problem.&nbsp; Hardware overprovisioning---where the<br />supercomputer center has more nodes than can be fully powered<br />simultaneously---has been proposed as one way to help alleviate the<br />power problem.&nbsp; However, while overprovisioning has shown potential<br />for performance improvement, there is currently no support for it from<br />HPC system software.<br /><br />The focus of this proposal was to design and implement software<br />infrastructure that will support overprovisioned systems.&nbsp; The key<br />advance in our infrastructure was that we will support system-wide<br />optimizations, i.e., optimizations that span multiple applications.<br />This is in stark contrast to the current focus in HPC systems of<br />optimizing on a per-application basis.<br /><br />We carried out three primary projects in this proposal.&nbsp; The first was<br />on the what we term inter-job power sharing problem.&nbsp; The key<br />observation was that applications assigned a fixed power bound may be<br />forced to slow down during high-power computation phases, but may not<br />consume their full power allocation during low-power I/O phases.&nbsp; This<br />project explored algorithms that leverage application<br />semantics---phase frequency, duration and power needs---to shift<br />unused power from applications in I/O phases to applications in<br />computation phases, thus improving system-wide performance.&nbsp; We<br />designed novel techniques that include explicit staggering of<br />applications to improve power shifting.&nbsp; Compared to executing without<br />power shifting, our algorithms improved average performance by up to<br />8% or a single, high-priority application by up to 32%.&nbsp; The results<br />were published in IPDPS 2016.</p> <p>The second was analyzing and mitigating network interference on<br />different types of modern interconnects.&nbsp; One project consisted of a<br />performance study and re-routing strategy for inter-job interference<br />on a fat-tree architecture (the "Cab" cluster, located at Lawrence<br />Livermore National Lab).&nbsp; Our results revealed two insights.&nbsp; First,<br />when multiple jobs compete for the interconnect, there is slowdown due<br />to a few localized hot spots.&nbsp; This was known for one job, but not for<br />multiple jobs.&nbsp; Second, we developed a re-routing strategy that<br />mitigates most of this slowdown by shifting load to lesser-loaded<br />areas of the interconnect.&nbsp; Performance results showed nearly a 50%<br />reduction in execution time with our technique.&nbsp; The results were<br />published in SC 2018 in a paper that was nominated for a Best Student<br />Paper award.&nbsp; The second project on understanding network variation<br />due to interference on Dragonfly interconnects was published in IPDPS<br />2020.<br /><br />The third project was developing an HPC job scheduler for fat-tree<br />interconnects that provides each job with an *interference-free node<br />allocation*---and still achieves high node utilization.&nbsp; There are<br />known techniques for fat trees for each of these goals separately, but<br />our job scheduler is the first to achieve both simultaneously.&nbsp; In<br />addition, we proved that our technique allows each allocation to<br />achieve the same full bandwidth guaranteed by fat trees.&nbsp; We are<br />preparing a submission to HPDC 2021 that describes these results.</p><br> <p>            Last Modified: 12/29/2020<br>      Modified by: David&nbsp;Lowenthal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Currently, the high-performance computing (HPC) community is focused on achieving practical exascale computing, where nontrivial programs can reach a sustained exaflop.  One of the key barriers to exascale is solving the power problem.  Hardware overprovisioning---where the supercomputer center has more nodes than can be fully powered simultaneously---has been proposed as one way to help alleviate the power problem.  However, while overprovisioning has shown potential for performance improvement, there is currently no support for it from HPC system software.  The focus of this proposal was to design and implement software infrastructure that will support overprovisioned systems.  The key advance in our infrastructure was that we will support system-wide optimizations, i.e., optimizations that span multiple applications. This is in stark contrast to the current focus in HPC systems of optimizing on a per-application basis.  We carried out three primary projects in this proposal.  The first was on the what we term inter-job power sharing problem.  The key observation was that applications assigned a fixed power bound may be forced to slow down during high-power computation phases, but may not consume their full power allocation during low-power I/O phases.  This project explored algorithms that leverage application semantics---phase frequency, duration and power needs---to shift unused power from applications in I/O phases to applications in computation phases, thus improving system-wide performance.  We designed novel techniques that include explicit staggering of applications to improve power shifting.  Compared to executing without power shifting, our algorithms improved average performance by up to 8% or a single, high-priority application by up to 32%.  The results were published in IPDPS 2016.  The second was analyzing and mitigating network interference on different types of modern interconnects.  One project consisted of a performance study and re-routing strategy for inter-job interference on a fat-tree architecture (the "Cab" cluster, located at Lawrence Livermore National Lab).  Our results revealed two insights.  First, when multiple jobs compete for the interconnect, there is slowdown due to a few localized hot spots.  This was known for one job, but not for multiple jobs.  Second, we developed a re-routing strategy that mitigates most of this slowdown by shifting load to lesser-loaded areas of the interconnect.  Performance results showed nearly a 50% reduction in execution time with our technique.  The results were published in SC 2018 in a paper that was nominated for a Best Student Paper award.  The second project on understanding network variation due to interference on Dragonfly interconnects was published in IPDPS 2020.  The third project was developing an HPC job scheduler for fat-tree interconnects that provides each job with an *interference-free node allocation*---and still achieves high node utilization.  There are known techniques for fat trees for each of these goals separately, but our job scheduler is the first to achieve both simultaneously.  In addition, we proved that our technique allows each allocation to achieve the same full bandwidth guaranteed by fat trees.  We are preparing a submission to HPDC 2021 that describes these results.       Last Modified: 12/29/2020       Submitted by: David Lowenthal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
