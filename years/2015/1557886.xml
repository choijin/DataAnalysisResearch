<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Multimodal State Estimation through Neural Coherence in the Parieto-Frontal Network</AwardTitle>
<AwardEffectiveDate>09/15/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>242243.00</AwardTotalIntnAmount>
<AwardAmount>242243</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>08090000</Code>
<Directorate>
<Abbreviation>BIO</Abbreviation>
<LongName>Direct For Biological Sciences</LongName>
</Directorate>
<Division>
<Abbreviation>IOS</Abbreviation>
<LongName>Division Of Integrative Organismal Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Edda Thiels</SignBlockName>
</ProgramOfficer>
<AbstractNarration>To distinguish parts of our body from other objects around us, the brain needs to build an internal image of our body by merging information from the skin, muscles and joints with information from our eyes.  This project is aimed at characterizing how we build our sense of self, by using multisite brain recordings and virtual reality technologies.  The results will impact national needs in the consumer, healthcare, military, and industrial settings by advancing the fundamental engineering and neuroscience knowledge necessary to create the next generation of brain-machine interfaces, which are envisioned to support the integration of artificial and natural sensory information.  Optimizing these systems depends critically on understanding how natural sensory signals interact within and among brain areas, a knowledge gap that directly addressed by this project.  The educational goals associated with the project are designed to advance discovery and understanding of engineering and neuroscience, while also promoting teaching, training, and learning beyond the regular bounds of these disciplines. These goals are achieved by: 1) Engaging high school students underrepresented in STEM fields through the development of hands-on instructional modules, 2) Promoting interdisciplinary undergraduate research opportunities via internships at Arizona State University, 3) Mentoring students in the broader implications of scientific research through exposure to organizations engaged in the ethical, societal, and policy implications of neuroscience research, and 4) Engaging the public in scientific discourse through public lectures and exhibits, and thereby promoting broad dissemination of the work to enhance scientific and technological understanding.&lt;br/&gt;&lt;br/&gt;Estimating the state of the body through the integration of available sensory cues (multimodal state estimation) is a critical integrative function for most organisms.  Although much is known about state estimation for the upper limb at the behavioral level, the underlying neural mechanisms remain poorly understood in cortical areas, particularly at the network level.  This is due to several factors: 1) the cortical areas believed to play a role in limb state estimation are heterogenous with regard to the relative strength of their sensory inputs and display both multisensory enhancement and suppression depending on context; 2) technical limitations mean functional interactions among these areas have been challenging to characterize; 3) the relation between sensitivity to visual and somatic cues and prevailing computational theories of multisensory integration have been incompletely explored; 4) multimodal areas are thought to contribute to both perceptual and action-based body representations but how these representations interact at the neural and behavioral levels is not well understood.  As a result, it is unclear how a coherent multimodal estimate of the state of the upper limb is constructed and maintained.  The proposed series of studies address these issues by quantifying changes in neural spiking, local field potentials, and neural coherence within and among fronto-parietal areas of the monkey implicated in state estimation using virtual reaching tasks that alter the reliability and semantic information of visual cues.</AbstractNarration>
<MinAmdLetterDate>09/15/2016</MinAmdLetterDate>
<MaxAmdLetterDate>10/15/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1557886</AwardID>
<Investigator>
<FirstName>Bijan</FirstName>
<LastName>Pesaran</LastName>
<EmailAddress>bijan@nyu.edu</EmailAddress>
<StartDate>09/15/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramElement>
<Code>7714</Code>
<Text>Modulation</Text>
</ProgramElement>
<ProgramReference>
<Code>1096</Code>
<Text>NEURAL SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>1228</Code>
<Text>MINORITY INVOLVEMENT -- BIO</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
</Appropriation>
<Appropriation>
<Code>0117</Code>
</Appropriation>
<Appropriation>
<Code>0119</Code>
</Appropriation>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
