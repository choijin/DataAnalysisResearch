<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Cooperative Motion Planning for Human-Operated Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>378465.00</AwardTotalIntnAmount>
<AwardAmount>378465</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Reid Simmons</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This proposal outlines a research and educational plan to advance decision-making techniques for robots that cooperate with human operators. Because humans far exceed the abilities of state-of-the-art robots in vision, creativity, and adaptability, interest is rapidly growing in a human-centered approach to robotics: combining the strengths of humans with the superior precision and repeatability of robots. And yet, our available motion planning tools, while powerful at computing motions for complex autonomous tasks, are poorly suited for human-centered applications that demand responsive and natural motions. This proposal hypothesizes that a new cooperative motion planning paradigm will support major advances in intuitiveness and task performance of human-operated robots such as intelligent vehicles, tele-surgery systems, search and-rescue robots, and household robots. This hypothesis is echoed in an educational plan that aims to train engineers with cross-disciplinary strengths that bridge both the technical and social dimensions of robotics. Initial human subjects studies on novice operators with the PI's cooperative motion planning algorithms suggest that the technique leads to dramatic reductions in task completion time and collision rate in cluttered environments. The proposed work will conduct further investigations along this line of research to 1) identify characteristics of cooperative planners - such as optimality, responsiveness, and completeness - that yield effective human-operator systems, both in terms of objective performance metrics and subjective preferences, 2) to design planners that optimize cooperativity metrics under computational resource and communication constraints, and 3) to enhance the capabilities of such planners to assist operators in complex manipulation tasks.&lt;br/&gt;&lt;br/&gt;The planners developed in this research and the rich datasets acquired via user studies will serve as resources to help human-robot interaction (HRI) researchers design safe and socially acceptable robot behaviors. Moreover, advances in cooperative motion planning may have long-term social and economic impact by enabling new applications of robotics in driver assist systems, space exploration, medicine, household robotics, manufacturing, and construction. Research is integrated with education in a range of activities that include CS curriculum development, development of a new graduate course on optimization and machine learning, and in new software libraries for robotics education. New modules on motion planning, behavior recognition, and HRI will be incorporated in AI and robotics courses. An REU is requested for each summer of the grant and will be recruited from a minority-serving institution in cooperation with the Alliance for the Advancement of African-American Researchers in Computing (A4RC). One or more IU undergraduates will be involved in research and mentored according to the Undergraduate Research Opportunities in Computing (UROC) program, with preference given to minority and women students.</AbstractNarration>
<MinAmdLetterDate>04/01/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1503177</AwardID>
<Investigator>
<FirstName>Kris</FirstName>
<LastName>Hauser</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kris Hauser</PI_FULL_NAME>
<EmailAddress>kkhauser@illinois.edu</EmailAddress>
<PI_PHON>5103840564</PI_PHON>
<NSF_ID>000554065</NSF_ID>
<StartDate>04/01/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277080291</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1045</Code>
<Text>CAREER: FACULTY EARLY CAR DEV</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~180369</FUND_OBLG>
<FUND_OBLG>2016~198095</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to better understand how human-controlled robots can complement their operator's guidance by performing low-level motion control functions.&nbsp; Robots currently used in many fields -- robotic surgery, explosive ordinance disposal, military drones, deep-sea exploration and repair, robotic prosthetics, and telepresence robots -- are controlled directly using a "joystick" style interface.&nbsp; This can require considerable training and even experienced operators can make mistakes.&nbsp; The robot could handle certain functions, like avoiding obstacles that the operator can't see, and maneuvering in tight spaces, which would alleviate burden on the operator.&nbsp; But in order for this type of interface to be convenient and useful, it needs to make the robot behave predictably, responsively, and cooperatively.&nbsp; The primary goal of this research is to develop cooperative motion planning algorithms that are capable of generating optimized user-guided motion.</p> <p>The intellectual merit of this project includes several new algorithms that address the needs of cooperative motion planning. First, the algorithms should calculate optimized responses quickly. Second, the algorithms should accept different criteria to optimize, since the notion of a "predictable response" is not mathematically well defined, so these criteria will need to be explored experimentally.&nbsp; This project has generated algorithms that accept arbitrary optimization criteria and are faster than other state-of-the-art planners in generating motions for robot arms in the presence of complex obstacles.&nbsp;</p> <p>On the applied side, the project has also helped develop a teleoperated mobile manipulator robot, TRINA, that is designed to help human nurses accomplish nursing tasks in quarantine environments. The robot has two arms, an omnidirectional mobile base, and two three-finger grippers, which makes it difficult for humans to operate by direct control alone.&nbsp; The project demonstrated that the robot can automatically assist its operator in maintaining hand orientation control, moving both hands simultaneously, and obstacle avoidance, which makes it safer and easier to use. Experiments in a nursing simulation lab have shown that a trained operator using this robot can complete 19 of 26 tested basic nursing skills. Although it failed to perform some tasks, this was promising preliminary work, and suggested improvements to hardware and the interface that are currently being studied in a follow-up NSF project that began in 2018.</p> <p>Broader impacts of this project included mentoring and training of female researchers in STEM, including several undergraduate students, two graduate students, and one postdoctoral researcher.&nbsp; Research was also integrated with undergraduate education at Duke University and Indiana University, with two new robotics classes introduced and new robotics education software developed with partial support from this project.&nbsp; It also enhanced infrastructure for research, as a second TRINA platform was built at Worcester Polytechnic Institute. Software infrastructure has been shared between the platforms and is also being used by several researchers there.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/09/2019<br>      Modified by: Kris&nbsp;Hauser</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551240961633_IMG_2658(2)--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551240961633_IMG_2658(2)--rgov-800width.jpg" title="The TRINA robot"><img src="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551240961633_IMG_2658(2)--rgov-66x44.jpg" alt="The TRINA robot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The TRINA robot, depicted in the Duke University School of Nursing simulation lab.</div> <div class="imageCredit">Duke University</div> <div class="imageSubmitted">Kris&nbsp;Hauser</div> <div class="imageTitle">The TRINA robot</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551241420969_tx90--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551241420969_tx90--rgov-800width.jpg" title="Industrial robot motion plan"><img src="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551241420969_tx90--rgov-66x44.jpg" alt="Industrial robot motion plan"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A optimized plan for a robot arm to reach between two obstacles, as calculated by the Lazy-RRG* algorithm.</div> <div class="imageCredit">Duke University</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Kris&nbsp;Hauser</div> <div class="imageTitle">Industrial robot motion plan</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551241498576_baxter--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551241498576_baxter--rgov-800width.jpg" title="Untangling two arms"><img src="/por/images/Reports/POR/2019/1503177/1503177_10276932_1551241498576_baxter--rgov-66x44.jpg" alt="Untangling two arms"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An optimized motion plan for the Baxter robot to untangle and re-tangle its arms in a different orientation, calculated using the Lazy-PRM* algorithm.</div> <div class="imageCredit">Duke University</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Kris&nbsp;Hauser</div> <div class="imageTitle">Untangling two arms</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to better understand how human-controlled robots can complement their operator's guidance by performing low-level motion control functions.  Robots currently used in many fields -- robotic surgery, explosive ordinance disposal, military drones, deep-sea exploration and repair, robotic prosthetics, and telepresence robots -- are controlled directly using a "joystick" style interface.  This can require considerable training and even experienced operators can make mistakes.  The robot could handle certain functions, like avoiding obstacles that the operator can't see, and maneuvering in tight spaces, which would alleviate burden on the operator.  But in order for this type of interface to be convenient and useful, it needs to make the robot behave predictably, responsively, and cooperatively.  The primary goal of this research is to develop cooperative motion planning algorithms that are capable of generating optimized user-guided motion.  The intellectual merit of this project includes several new algorithms that address the needs of cooperative motion planning. First, the algorithms should calculate optimized responses quickly. Second, the algorithms should accept different criteria to optimize, since the notion of a "predictable response" is not mathematically well defined, so these criteria will need to be explored experimentally.  This project has generated algorithms that accept arbitrary optimization criteria and are faster than other state-of-the-art planners in generating motions for robot arms in the presence of complex obstacles.   On the applied side, the project has also helped develop a teleoperated mobile manipulator robot, TRINA, that is designed to help human nurses accomplish nursing tasks in quarantine environments. The robot has two arms, an omnidirectional mobile base, and two three-finger grippers, which makes it difficult for humans to operate by direct control alone.  The project demonstrated that the robot can automatically assist its operator in maintaining hand orientation control, moving both hands simultaneously, and obstacle avoidance, which makes it safer and easier to use. Experiments in a nursing simulation lab have shown that a trained operator using this robot can complete 19 of 26 tested basic nursing skills. Although it failed to perform some tasks, this was promising preliminary work, and suggested improvements to hardware and the interface that are currently being studied in a follow-up NSF project that began in 2018.  Broader impacts of this project included mentoring and training of female researchers in STEM, including several undergraduate students, two graduate students, and one postdoctoral researcher.  Research was also integrated with undergraduate education at Duke University and Indiana University, with two new robotics classes introduced and new robotics education software developed with partial support from this project.  It also enhanced infrastructure for research, as a second TRINA platform was built at Worcester Polytechnic Institute. Software infrastructure has been shared between the platforms and is also being used by several researchers there.          Last Modified: 04/09/2019       Submitted by: Kris Hauser]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
