<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Tasking on Natural Image Statistics: 2D and 3D Object and Category Detection in the Wild</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>441933.00</AwardTotalIntnAmount>
<AwardAmount>441933</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops "distortion-aware" computer vision models and algorithms suitable for today's mobile camera devices, such as are found in cell phones. Today's mobile camera devices contain remarkably powerful computing capability, sufficient, in fact to contemplate performing sophisticated computer vision problems such as three-dimensional depth estimation, object detection and object recognition. However, mobile camera capabilities are much more limited due to distortions on capture, such as low-light noise, blur, saturation, over/under exposure, and processing artifacts such as compression. These distortions cause most computer vision algorithms to "break," making them unable to accurately recreate the 3D world or to find and recognize objects in it.  This project creates computer vision algorithms with similar capability, using new and emerging models of visual neuroscience (how people see) and detailed and accurate statistical models of the three dimensional visual world (called natural scene statistic models). The project can impact many other camera devices, including low-cost surveillance and security cameras, mobile medical cameras, military cameras operating under battlefield conditions, and more.&lt;br/&gt;&lt;br/&gt;This research develops principled approaches to using natural scene statistics models to solve difficult single-image visual tasking problems under poor imaging conditions.  Specifically, the research team studies robust 'distortion-aware' statistical image models and algorithms for single-image 2D and 3D object and object category detection and synergistic 3D depth estimation. The research work includes (1) developing algorithms for fast, generic object detection and categorization "in-the-wild" that operate on single photographic images suffering authentic artifacts from digital cameras; (2) designing object and object class detection mechanisms augmented by 3D depth estimation processes, driven by powerful 2D and 3D prior natural image constraints; and  (3) constructing a new annotated Color+3D database of HD precision-calibrated RGBD data using a Reigl VZ-400 Terrestrial Lidar Scanner on object categories of interest, yielding data of higher resolution and richness than existing datasets, complete with image labels as well as hand annotations of bounding-box object locations. This database is free to the community at large once it is available.</AbstractNarration>
<MinAmdLetterDate>08/25/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1526423</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Bovik</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan C Bovik</PI_FULL_NAME>
<EmailAddress>bovik@ece.utexas.edu</EmailAddress>
<PI_PHON>5124715370</PI_PHON>
<NSF_ID>000305764</NSF_ID>
<StartDate>08/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName/>
<StateCode>TX</StateCode>
<ZipCode>787121084</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~150000</FUND_OBLG>
<FUND_OBLG>2016~291933</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Project Outcomes</p> <p>&nbsp;</p> <p>We developed fast algorithms for detecting objects and for computing depths categorization "in-the-wild" that operate on single digital photographic images suffering from real-world artifacts. This work led to the invention of new distortion-aware models and algorithms for single-image object category detection and synergistic 3D depth estimation. It also resulted in new models for 3D visual discomfort prediction, and for predicting and improving the performance of visual tasks, such as object detection, on previously un-modelled modalities. This work resulted in several important new computer vision datasets related to depth estimation. These databases are freely available to the community at large.</p> <p>Some of the specific accomplishments include:</p> <p>- New algorithms for fast, generic object detection and categorization "in-the-wild" on distorted single photographic images. These models innovated by using perceptually relevant, regular statistical models of real-world distorted images to supply resilience under conditions encountered by casual mobile camera photographers.</p> <p>- New object and object class detection algorithms augmented by 3D depth estimation algorithms, driven by powerful 2D and 3D statistical image models. These produce both better object / object class detection results and better depth estimates.</p> <p>- One of the important contributions is a new database: an Annotated Color+3D Database of HD precision-calibrated RGBD data obtained using a Reigl VZ-400 Terrestrial Lidar Scanner having higher resolution and richness than existing datasets, complete with image labels as well as hand annotations of bounding-box object locations. This unique database is made freely available to the community at large.</p> <p>- Another contribution is a much larger database of co-registered RGB images and depth maps using a simulated SV camera con&#64257;guration. The database was &#64257;rst captured using &#64257;sheye cameras with known intrinsic parameters, and the &#64257;sheye distortions were then removed to create the non-&#64257;sheye portion of the database. The images depict a synthetic-but-realistic city scene where the images and depth maps were captured along with the methodology for generating such a large, varied, and generalizable dataset.</p><br> <p>            Last Modified: 12/17/2019<br>      Modified by: Alan&nbsp;C&nbsp;Bovik</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Project Outcomes     We developed fast algorithms for detecting objects and for computing depths categorization "in-the-wild" that operate on single digital photographic images suffering from real-world artifacts. This work led to the invention of new distortion-aware models and algorithms for single-image object category detection and synergistic 3D depth estimation. It also resulted in new models for 3D visual discomfort prediction, and for predicting and improving the performance of visual tasks, such as object detection, on previously un-modelled modalities. This work resulted in several important new computer vision datasets related to depth estimation. These databases are freely available to the community at large.  Some of the specific accomplishments include:  - New algorithms for fast, generic object detection and categorization "in-the-wild" on distorted single photographic images. These models innovated by using perceptually relevant, regular statistical models of real-world distorted images to supply resilience under conditions encountered by casual mobile camera photographers.  - New object and object class detection algorithms augmented by 3D depth estimation algorithms, driven by powerful 2D and 3D statistical image models. These produce both better object / object class detection results and better depth estimates.  - One of the important contributions is a new database: an Annotated Color+3D Database of HD precision-calibrated RGBD data obtained using a Reigl VZ-400 Terrestrial Lidar Scanner having higher resolution and richness than existing datasets, complete with image labels as well as hand annotations of bounding-box object locations. This unique database is made freely available to the community at large.  - Another contribution is a much larger database of co-registered RGB images and depth maps using a simulated SV camera con&#64257;guration. The database was &#64257;rst captured using &#64257;sheye cameras with known intrinsic parameters, and the &#64257;sheye distortions were then removed to create the non-&#64257;sheye portion of the database. The images depict a synthetic-but-realistic city scene where the images and depth maps were captured along with the methodology for generating such a large, varied, and generalizable dataset.       Last Modified: 12/17/2019       Submitted by: Alan C Bovik]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
