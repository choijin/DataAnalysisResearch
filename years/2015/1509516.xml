<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Adaptive dynamic programming for uncertain nonlinear systems through coupling of nonlinear analysis and data-based learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>325543.00</AwardTotalIntnAmount>
<AwardAmount>325543</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anil Pahwa</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Optimal control methods provide a means to associate a user-defined cost with control actions or decisions. These methods have made pervasive impacts in a wide class of application domains. The shift towards autonomy by the automotive industry in examples such as replacing mechanical systems with computer controlled electronic control systems, has resulted in efficiencies in fuel injection, braking, throttling, etc. For electric vehicles, fuel economy, drivability, and emission control are functions of the engine design and the control strategies. For robotic systems, the desire to achieve optimal behavior is essential for efficient task execution. Likewise, aerospace systems have always been a mainstream application domain for optimal control methods because there has always been (and always will be) a tight coupling between performance and energy/fuel costs. As the cost of energy and the awareness of the environmental impacts of producing energy have risen, optimal control may now play a timely role in a broader spectrum of application domains. The Energy Efficiency and Renewable Energy office of the U.S. Department of Energy indicates that as much as 10-20 percent of American energy use could be saved by optimizing industrial systems. It is self-evident that optimal control solutions can have significant impacts in a wide range of industries, but the development of optimal control solutions for real engineering systems is limited by numerous technical barriers. The most fundamental and open-ended problems arise from barriers associated with developing optimal solutions in the presence of uncertainty. The driving question in this project is how to arbitrate between gaining knowledge about a system while simultaneously making the optimal control decision for engineering systems that are uncertain and complex.&lt;br/&gt;&lt;br/&gt;The technical aims of this project are motivated by the hypothesis and observations from preliminary efforts that nonlinear analysis methods can be exploited to design real-time approximate optimal solutions, while concurrent background processing methods can be used to update the optimal control approximation for improved performance. Intellectual merits in this project are realized through the development of classes of closed-loop controllers with associated stability analysis, and advanced function approximation methods, that ensure sufficient exploration of the system response while learning the approximate optimal control solution. Outcomes of this research would allow for optimal control implementation in a broader class of application domains where the system exhibits nonlinear behaviors and uncertainty. The broad impact of this framework is a merger of methods that bridge the gap between the computational intelligence community and control systems community to enable data-based learning methods to optimize control performance.</AbstractNarration>
<MinAmdLetterDate>07/22/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/22/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1509516</AwardID>
<Investigator>
<FirstName>Warren</FirstName>
<LastName>Dixon</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Warren E Dixon</PI_FULL_NAME>
<EmailAddress>wdixon@ufl.edu</EmailAddress>
<PI_PHON>3528461463</PI_PHON>
<NSF_ID>000250994</NSF_ID>
<StartDate>07/22/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>Gainesville</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7607</Code>
<Text>EPCN-Energy-Power-Ctrl-Netwrks</Text>
</ProgramElement>
<ProgramReference>
<Code>030E</Code>
<Text>CONTROL SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>1653</Code>
<Text>Adaptive &amp; intelligent systems</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~325543</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Making the best decision in the presence of uncertainty, especially under a time constraint, is a daunting task for both people and machines. For automated systems, an optimal decision is defined by a weighted cost to arbitrate between the goal and the expense involved to achieve the goal. In general, such mathematical problems can not be solved exactly and often rely on numerical approximations that can be so large that they can not be made in real-time, especially under the presence of uncertainty. Uncertainty inherently involves a tradeoff between making decisions without understanding the ramifications versus waiting to act until the ramifications have been understood. This tradeoff is a fundamental problem called exploration versus exploitation.&nbsp;</p> <p>Despite uncertainty and complex nonlinear dynamics, this project developed new computationally efficient, real-time adaptation methods that are proven to&nbsp; learn the uncertainties and the approximate optimal decision. Specifically, reinforcement learning methods were developed to learn the approximate optimal decision, while our new discoveries in function approximation made it possible to allow parallel learning while executing (i.e., enabling simultaneous exploration and exploitation, rather than exploration versus exploitation). Simultaneously learning the governing dynamics enabled a strategy where the learned model could be used to predict the outcome of decisions. Such a&nbsp; strategy, allowed massive parallel computations (i.e., simulation of experience) resulting in order of magnitude time reductions in learning the optimal solution. We demonstrated an impact of such computationally efficient learning methods by designing an optimal trajectory to a goal location, in real-time, despite the presence of an unknown number of moving obstacles with unknown dynamics.</p> <p>These concepts were extended to also include optimal decisions between multiple agents. Specifically, the use of differential game theory allowed for the discovery of an equitable compromise (i.e., an equilibrium), despite opposing goals and uncertainty about the dynamics of each agent, their goals, and the way in which they interact. To illustrate an implication of this strategy, we designed a satellite control method that could pursue, intercept and change the orbital status of another resident space object (e.g., debris).&nbsp;</p> <p>The project resulted in juried conference and journal publications. Moreover, a research monograph was published that provides an in-depth and comprehensive examination of the methods and discoveries of this project.&nbsp;</p> <p>The project outcomes were the result of research by the principal investigator and his Ph.D. students. To develop the results, the Ph.D. students were trained in advanced concepts in nonlinear control and estimation theory, adaptation and function approximation, and optimal control and differential game theory. In additional to developing their mathematical skills, they also learned how to investigate problems, discover constraints, find alternate solutions, develop strategies, and document and disseminate their findings. Students also learned to reach out to younger students and teach them the value of science, technology, engineering and mathematics (STEM). Specifically, students taught merit badges in robotics at a large-scale regional STEM event, where the optimal decision making principles were incorporated within the design of competition mobile robots.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/25/2019<br>      Modified by: Warren&nbsp;E&nbsp;Dixon</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Making the best decision in the presence of uncertainty, especially under a time constraint, is a daunting task for both people and machines. For automated systems, an optimal decision is defined by a weighted cost to arbitrate between the goal and the expense involved to achieve the goal. In general, such mathematical problems can not be solved exactly and often rely on numerical approximations that can be so large that they can not be made in real-time, especially under the presence of uncertainty. Uncertainty inherently involves a tradeoff between making decisions without understanding the ramifications versus waiting to act until the ramifications have been understood. This tradeoff is a fundamental problem called exploration versus exploitation.   Despite uncertainty and complex nonlinear dynamics, this project developed new computationally efficient, real-time adaptation methods that are proven to  learn the uncertainties and the approximate optimal decision. Specifically, reinforcement learning methods were developed to learn the approximate optimal decision, while our new discoveries in function approximation made it possible to allow parallel learning while executing (i.e., enabling simultaneous exploration and exploitation, rather than exploration versus exploitation). Simultaneously learning the governing dynamics enabled a strategy where the learned model could be used to predict the outcome of decisions. Such a  strategy, allowed massive parallel computations (i.e., simulation of experience) resulting in order of magnitude time reductions in learning the optimal solution. We demonstrated an impact of such computationally efficient learning methods by designing an optimal trajectory to a goal location, in real-time, despite the presence of an unknown number of moving obstacles with unknown dynamics.  These concepts were extended to also include optimal decisions between multiple agents. Specifically, the use of differential game theory allowed for the discovery of an equitable compromise (i.e., an equilibrium), despite opposing goals and uncertainty about the dynamics of each agent, their goals, and the way in which they interact. To illustrate an implication of this strategy, we designed a satellite control method that could pursue, intercept and change the orbital status of another resident space object (e.g., debris).   The project resulted in juried conference and journal publications. Moreover, a research monograph was published that provides an in-depth and comprehensive examination of the methods and discoveries of this project.   The project outcomes were the result of research by the principal investigator and his Ph.D. students. To develop the results, the Ph.D. students were trained in advanced concepts in nonlinear control and estimation theory, adaptation and function approximation, and optimal control and differential game theory. In additional to developing their mathematical skills, they also learned how to investigate problems, discover constraints, find alternate solutions, develop strategies, and document and disseminate their findings. Students also learned to reach out to younger students and teach them the value of science, technology, engineering and mathematics (STEM). Specifically, students taught merit badges in robotics at a large-scale regional STEM event, where the optimal decision making principles were incorporated within the design of competition mobile robots.           Last Modified: 09/25/2019       Submitted by: Warren E Dixon]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
