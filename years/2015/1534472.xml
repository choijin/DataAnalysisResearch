<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Bayesian Methods for Meta-Analysis in the Presence of Publication Bias</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>260000.00</AwardTotalIntnAmount>
<AwardAmount>260000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The differential rate of publishing between positive and negative results, which has been called publication bias, is of increasing concern in the social and behavioral sciences.  This research project will develop a new approach to meta-analysis that explicitly takes into account the possibility of a biased publication process.  Meta-analysis has been instrumental in interpreting the claims made in the academic literature.  However, academic journals, especially in the social and behavioral sciences, seem to strongly prefer manuscripts that posit the existence of an effect rather than non-significant outcomes.  This hinders classical meta-analysis methods because the aggregate of a biased set of empirical results will be biased as well.  The new approach will allow for better aggregation of published results and will provide a more accurate view of the effect of various experimental manipulations and treatments.  Software will be developed and published that implements this approach for a variety of situations.&lt;br/&gt;&lt;br/&gt;This research project will develop a new approach to meta-analysis called "statistical mitigation" that combines behavioral models with state-of-the-art statistical methods.  The approach will be based on a Bayesian model averaging technique in which effect size estimates are computed using a set of plausible selection models and averaging across these selection models.  With this approach, it will be possible to isolate the signal of true effects within the noise of measurement error.  The investigator will test the method under various circumstances, compare the new approach to existing methods for inference in the presence of publication bias, and perform simulations to assess the efficiency of the method.  With a single approach to meta-analysis, researchers will be able to account for the possibility of publication bias, confirm or disconfirm null and non-null hypotheses, and do effect size estimation.</AbstractNarration>
<MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1534472</AwardID>
<Investigator>
<FirstName>Joachim</FirstName>
<LastName>Vandekerckhove</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joachim Vandekerckhove</PI_FULL_NAME>
<EmailAddress>joachim@uci.edu</EmailAddress>
<PI_PHON>9495278092</PI_PHON>
<NSF_ID>000689786</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926975100</ZipCode>
<StreetAddress><![CDATA[SSL 477]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~260000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-e83b6014-7fff-91b0-49b4-7df6ca1be714"> </span></p> <p dir="ltr"><span>A "crisis of confidence," caused initially by highly-publicized cases of academic fraud, has rapidly led to an inward-facing, self-critical movement in the field of psychology.&nbsp; The focus of this critical movement is currently on the prevalence of questionable research practices and particularly on the lack of replicability of many published results.</span></p> <p dir="ltr"><span>One systemic issue that has received scrutiny is that of publication bias. &nbsp;Publication bias occurs when researchers or professional journals preferentially publish empirical results of successful treatments or manipulations, rather than unsuccessful ones. &nbsp;While this may on the surface seem innocuous, it is problematic because successful results are known to occasionally come about by accident.  Publication bias p revents such fluke results from being appropriately offset by failures to replicate because such failures do not pass through the publication filter.</span></p> <p dir="ltr"><span>The biased nature of the literature is a serious concern for consumers of the scientific literature if they make the erroneous assumption that the literature provides a representative sample of the data collected in scientific labs across the world.&nbsp; In the current project, we have constructed a new statistical method that explicitly takes into account the possibility of a biased publication process.&nbsp; This would permit consumers to view the literature with more appropriate criticism.&nbsp;&nbsp;</span>We call the approach &ldquo;statistical mitigation&rdquo; of publication bias.</p> <p dir="ltr"><span>The project has produced a total of 16 academic papers, in which we (a) describe the method in technical and mathematical detail; (b) extend the method for different use cases; (c) instruct users on the statistical underpinnings of the method and related methodological issues; (d) rigorously test the method under various circumstances; and (e) apply the method to example topics. &nbsp;All papers and software created created were made freely available online.&nbsp;&nbsp;Results were also disseminated through standard academic channels (i.e., conferences).</span></p> <p dir="ltr"><span>Additionally, the project has contributed to the training of two graduate students at the University of California, Irvine.</span></p><br> <p>            Last Modified: 09/25/2018<br>      Modified by: Joachim&nbsp;Vandekerckhove</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   A "crisis of confidence," caused initially by highly-publicized cases of academic fraud, has rapidly led to an inward-facing, self-critical movement in the field of psychology.  The focus of this critical movement is currently on the prevalence of questionable research practices and particularly on the lack of replicability of many published results. One systemic issue that has received scrutiny is that of publication bias.  Publication bias occurs when researchers or professional journals preferentially publish empirical results of successful treatments or manipulations, rather than unsuccessful ones.  While this may on the surface seem innocuous, it is problematic because successful results are known to occasionally come about by accident.  Publication bias p revents such fluke results from being appropriately offset by failures to replicate because such failures do not pass through the publication filter. The biased nature of the literature is a serious concern for consumers of the scientific literature if they make the erroneous assumption that the literature provides a representative sample of the data collected in scientific labs across the world.  In the current project, we have constructed a new statistical method that explicitly takes into account the possibility of a biased publication process.  This would permit consumers to view the literature with more appropriate criticism.  We call the approach "statistical mitigation" of publication bias. The project has produced a total of 16 academic papers, in which we (a) describe the method in technical and mathematical detail; (b) extend the method for different use cases; (c) instruct users on the statistical underpinnings of the method and related methodological issues; (d) rigorously test the method under various circumstances; and (e) apply the method to example topics.  All papers and software created created were made freely available online.  Results were also disseminated through standard academic channels (i.e., conferences). Additionally, the project has contributed to the training of two graduate students at the University of California, Irvine.       Last Modified: 09/25/2018       Submitted by: Joachim Vandekerckhove]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
