<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Towards Large-Scale Recognition and Fine-Grain Analysis of Human Actions: Pulling Actions Out of Context</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>174855.00</AwardTotalIntnAmount>
<AwardAmount>174855</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates problems of human action recognition in video. A human action does not occur in isolation, and it is not the only thing recorded in a video sequence. A video clip of a human action also contains many other components, including the background scene, the interacting objects, the camera motion, and the activity of other people. Some of these components are contextual elements that frequently co-occur with the category of action in consideration. The project develops technologies that separated human actions from co-occurring factors for large-scale recognition and fine-grain visual interpretation of human actions. The developed technologies can have many practical applications in a wide range of fields, ranging from human computer interaction and robotics to security and health-care. &lt;br/&gt; &lt;br/&gt;This research develops an approach to human action recognition by explicitly factorizing human actions from context. The key idea is to exploit the benefits of the information from conjugate samples of human actions. A conjugate sample is defined as a video clip that is contextually similar to an action sample, but does not contain the action. For instance, a conjugate sample of a handshake sample can be the video sequence showing two people approaching each other prior to the handshake. The handshake clip and the video sequence preceding it have many similar or even the same contextual elements, including the people, the background scene, the camera angle, and the lighting condition. The only thing that sets these two video clips apart is the actual human action itself. A conjugate sample provides complementary information to the action sample; it can be used to suppress contextual irrelevance and magnify the action signal. The specific research objectives of this project include: (1) collecting human action samples for many action classes; (2) developing algorithms to mine and extract conjugate human action samples; and (3) developing a framework that utilizes the benefits of conjugate samples for separating actions from context to learn classifiers for large-scale recognition and fine-grain understanding of human actions.</AbstractNarration>
<MinAmdLetterDate>03/02/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/02/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1566248</AwardID>
<Investigator>
<FirstName>Minh Hoai</FirstName>
<LastName>Nguyen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Minh Hoai Nguyen</PI_FULL_NAME>
<EmailAddress>minhhoai@cs.stonybrook.edu</EmailAddress>
<PI_PHON>6316328460</PI_PHON>
<NSF_ID>000678789</NSF_ID>
<StartDate>03/02/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stony Brook University]]></Name>
<CityName>Stony Brook</CityName>
<StateCode>NY</StateCode>
<ZipCode>117944400</ZipCode>
<StreetAddress><![CDATA[CS153, Computer Science, Stony B]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~105546</FUND_OBLG>
<FUND_OBLG>2017~69309</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed novel algorithms for training a human action classifier. The uniqueness of the project was its focus on developing algorithms that can: (1) explicitly factorize human actions from the co-occurring factors; (2) deliberately build a model for human actions and a separate model for all correlated contextual elements; and (3) effectively combine the models for large-scale recognition and fine-grain visual interpretation of human actions.&nbsp;</p> <p>&nbsp;</p> <p>This project has developed such algorithms, and the key novelty was to mine and subsequently utilize conjugate samples of human actions. Conjugate samples are video clips that are contextually similar to the action samples but do not contain the actions. By comparing the similarities and differences between an action sample and a conjugate sample, the algorithms were able to separate the action components from the co-occurring context components, increasing the generalizability of the trained classifiers to test domains with dissimilar contexts. The project also collected a large-scale dataset of human action and conjugate samples. Our dataset is different from existing datasets is that it contains conjugate samples of human actions.&nbsp;</p> <p>&nbsp;</p> <p>This project provided partial support for training 3 PhD students. It also created summer projects that provided research experience to one Masters student and one high-school student.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/30/2019<br>      Modified by: Minh Hoai&nbsp;Nguyen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed novel algorithms for training a human action classifier. The uniqueness of the project was its focus on developing algorithms that can: (1) explicitly factorize human actions from the co-occurring factors; (2) deliberately build a model for human actions and a separate model for all correlated contextual elements; and (3) effectively combine the models for large-scale recognition and fine-grain visual interpretation of human actions.      This project has developed such algorithms, and the key novelty was to mine and subsequently utilize conjugate samples of human actions. Conjugate samples are video clips that are contextually similar to the action samples but do not contain the actions. By comparing the similarities and differences between an action sample and a conjugate sample, the algorithms were able to separate the action components from the co-occurring context components, increasing the generalizability of the trained classifiers to test domains with dissimilar contexts. The project also collected a large-scale dataset of human action and conjugate samples. Our dataset is different from existing datasets is that it contains conjugate samples of human actions.      This project provided partial support for training 3 PhD students. It also created summer projects that provided research experience to one Masters student and one high-school student.          Last Modified: 08/30/2019       Submitted by: Minh Hoai Nguyen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
