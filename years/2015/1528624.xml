<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Updating the Militarized Dispute Data Through Crowdsourcing: MID5</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>367432.00</AwardTotalIntnAmount>
<AwardAmount>367432</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Brian Humes</SignBlockName>
<PO_EMAI>bhumes@nsf.gov</PO_EMAI>
<PO_PHON>7032927284</PO_PHON>
</ProgramOfficer>
<AbstractNarration>General Summary &lt;br/&gt;&lt;br/&gt;The Correlates of War Project's Militarized Interstate Dispute (MID) Data is the most prominent and heavily used data collection in the study of international conflict. The most recent version (MID4) was released in 2014 and brings the period covered to 1816-2010. The MID4 project utilized automated text classification procedures to make the process of identifying relevant news stories more efficient. Over the course of that project, the PIs determined the primary bottleneck in the workflow was the coding of those news documents. To address this inefficiency, The PIs completed a pilot project to determine whether crowdsourcing techniques could be used to code these documents. In the pilot, non-expert workers were paid small sums to read documents and to answer sets of questions, the answers to which were used to identify features of possible militarized incidents (the events that comprise MIDs). A systematic comparison of the crowdsourced responses with those of MID4 Project's trained coders revealed that the crowdsourced codings were completely accurate for 68 percent of the news reports coded; more importantly, high agreement among crowd responses on specific reports was strongly associated with correct coding. This enables the PIs to detect which documents require further expert involvement. As a result, the PIs can produce a majority of the MID data in near-realtime and at limited financial cost. These procedures are applied on the MID5 Project, which will update the MID data for the period 2011-2017.&lt;br/&gt;&lt;br/&gt;Technical Summary &lt;br/&gt;&lt;br/&gt;The MID5 project workflow begins with document retrieval from LexisNexis and document classification using the software and methods implemented in MID4. We discard the negatively classified documents, and proceed to extract metadata from the positively classified documents including the document title, the news agency that published the report, the date, and any actors mentioned in the text. Crowd workers are recruited through Amazon's Mechanical Turk and paid a wage to read one of these documents and answer a line of simple, objective questions about it. The questionnaire is predefined, but some extracted metadata is automatically inserted into the questionnaire to improve the quality of responses. Several workers complete a questionnaire for each document, leaving the PIs with problems of aggregation: how to combine multiple worker responses, possibly regarding multiple related questions, into usable data necessary to code the militarized incident. In the pilot study, the PIs show that Bayesian networks are the most effective way to achieve this aggregation. Recently, the PIs have made advances in semi-supervised text classification with hybrid, Deep Restricted Boltzmann Machines, which outperform previous methods in this task.</AbstractNarration>
<MinAmdLetterDate>09/17/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/29/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1528624</AwardID>
<Investigator>
<FirstName>Vito</FirstName>
<LastName>D'Orazio</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vito D'Orazio</PI_FULL_NAME>
<EmailAddress>dorazio@utdallas.edu</EmailAddress>
<PI_PHON>9728836212</PI_PHON>
<NSF_ID>000690300</NSF_ID>
<StartDate>09/17/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Dallas</Name>
<CityName>Richardson</CityName>
<ZipCode>750803021</ZipCode>
<PhoneNumber>9728832313</PhoneNumber>
<StreetAddress>800 W. Campbell Rd., AD15</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX32</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>800188161</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT DALLAS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Dallas]]></Name>
<CityName>Richardson</CityName>
<StateCode>TX</StateCode>
<ZipCode>750803021</ZipCode>
<StreetAddress><![CDATA[800 W. Campbell Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX32</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1371</Code>
<Text>Political Science</Text>
</ProgramElement>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~125357</FUND_OBLG>
<FUND_OBLG>2016~120429</FUND_OBLG>
<FUND_OBLG>2017~121646</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This NSF project has had two main goals throughout. The chronologically first has been to develop a system by which newspaper reports can be crowdsourced to ask precise questions, the answers to which can be used to code militarized interstate incidents (MIIs), the conflict-related events that comprise Militarized Interstate Disputes (MIDs). The general idea here has been to replace expert coders (graduate research assistants) with non-expert crowd workers (recruited from Amazon Mechanical Turk) to annotate news reports. The second goal, and one that has motivated the work, was to expand the MID dataset.&nbsp;</p> <p>As this project has progressed, we have had to revise these initial goals. Despite our many efforts to improve the quality of answers, we found that the crowd-produced information was not accurate enough to replace expert coders. Ultimately, we coded the way we had in earlier projects, using trained research assistants. We anticipate releasing new MID data through 2014 by the summer of 2020.</p> <p>The central finding from our crowdsourcing experiments is that annotating MIIs requires too much contextual knowledge for untrained readers to answer correctly, even in the aggregate. We reached this conclusion through systematic experimentation along two dimensions.</p> <p>First, we attempted to improve the likelihood that an individual, non-expert worker classifies the event correctly. Our initial approach was to construct a question tree, where individuals answer simple questions, many of which are yes/no, and are directed through the tree based on their answers. The results show that crowd-workers are more likely to annotate correctly if asked a single, more complicated question. In our second experiment, we presented crowd workers with up to four automatically generated suggestions, and sometimes that set included the correct answer. That is, we wanted to see if machine learning could be used to present a subset of suggestions, guiding workers to the correct answer. We found that providing workers with suggested answers actually decreases worker performance. Finally, we recruited workers to label a number of documents and revealed the true answer after each. This set did not perform significantly better than the workers who did not see the true answers. Ultimately, we concluded that inducing worker learning is very difficult, and that researchers are better off identifying and incentivizing naturally skilled workers than trying to improve the performance of mediocre ones.&nbsp;</p> <p>Second, we attempted to improve the likelihood that non-expert workers, in aggregate, would classify the event correctly. To improve the performance of the aggregate classifications, we developed a tailored process for estimating worker ability. Then, we weighted responses by ability, so that the contributions of highly skilled workers had greater influence. We incentivized workers by increasing their pay if they labeled the document correctly. Still, the highest accuracy that we were able to obtain using crowd-workers and imitating a real coding process was 68%. This means that our aggregation algorithm correctly used the crowd-coded data to predict the&nbsp; initiator and target states, as well as the specific MII action type, 68% of the time. If we allow the algorithm to make two predictions, and count it as correct if either prediction is correct, the accuracy jumps to 79%. There may be applications where an algorithm makes two predictions and then a reviewer decides between the two. There may also be applications where 68% accuracy is an improvement over alternative annotation methods.</p> <p>Conflict event classification is best characterized as a complex classification task. For such tasks, increasing the number of crowd workers does not necessarily mean you increase your overall accuracy. This is likely the case for scaling tasks (e.g., how conservative or liberal is Policy X) and simple classification (e.g., is this a picture of a cat), but not complex classification tasks. Additionally, higher accuracy rates are likely when all the information necessary to label a concept is located in a single text, and thus requires no additional documents or contextual knowledge. To use crowdsourcing for complex classification, the optimal approach we found was to identify, incentivize, and upweight skilled workers.</p> <p>Crowdsourced data collection is, and will continue to be, an important method of data collection in the social sciences. However, our results demonstrate that, at least for complex classification tasks, trained research assistants are still required to achieve the highest levels of data validity. We expect this finding to have broad impact on the development and application of the crowdsourcing methodology.</p><br> <p>            Last Modified: 05/08/2020<br>      Modified by: Vito&nbsp;D'orazio</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This NSF project has had two main goals throughout. The chronologically first has been to develop a system by which newspaper reports can be crowdsourced to ask precise questions, the answers to which can be used to code militarized interstate incidents (MIIs), the conflict-related events that comprise Militarized Interstate Disputes (MIDs). The general idea here has been to replace expert coders (graduate research assistants) with non-expert crowd workers (recruited from Amazon Mechanical Turk) to annotate news reports. The second goal, and one that has motivated the work, was to expand the MID dataset.   As this project has progressed, we have had to revise these initial goals. Despite our many efforts to improve the quality of answers, we found that the crowd-produced information was not accurate enough to replace expert coders. Ultimately, we coded the way we had in earlier projects, using trained research assistants. We anticipate releasing new MID data through 2014 by the summer of 2020.  The central finding from our crowdsourcing experiments is that annotating MIIs requires too much contextual knowledge for untrained readers to answer correctly, even in the aggregate. We reached this conclusion through systematic experimentation along two dimensions.  First, we attempted to improve the likelihood that an individual, non-expert worker classifies the event correctly. Our initial approach was to construct a question tree, where individuals answer simple questions, many of which are yes/no, and are directed through the tree based on their answers. The results show that crowd-workers are more likely to annotate correctly if asked a single, more complicated question. In our second experiment, we presented crowd workers with up to four automatically generated suggestions, and sometimes that set included the correct answer. That is, we wanted to see if machine learning could be used to present a subset of suggestions, guiding workers to the correct answer. We found that providing workers with suggested answers actually decreases worker performance. Finally, we recruited workers to label a number of documents and revealed the true answer after each. This set did not perform significantly better than the workers who did not see the true answers. Ultimately, we concluded that inducing worker learning is very difficult, and that researchers are better off identifying and incentivizing naturally skilled workers than trying to improve the performance of mediocre ones.   Second, we attempted to improve the likelihood that non-expert workers, in aggregate, would classify the event correctly. To improve the performance of the aggregate classifications, we developed a tailored process for estimating worker ability. Then, we weighted responses by ability, so that the contributions of highly skilled workers had greater influence. We incentivized workers by increasing their pay if they labeled the document correctly. Still, the highest accuracy that we were able to obtain using crowd-workers and imitating a real coding process was 68%. This means that our aggregation algorithm correctly used the crowd-coded data to predict the  initiator and target states, as well as the specific MII action type, 68% of the time. If we allow the algorithm to make two predictions, and count it as correct if either prediction is correct, the accuracy jumps to 79%. There may be applications where an algorithm makes two predictions and then a reviewer decides between the two. There may also be applications where 68% accuracy is an improvement over alternative annotation methods.  Conflict event classification is best characterized as a complex classification task. For such tasks, increasing the number of crowd workers does not necessarily mean you increase your overall accuracy. This is likely the case for scaling tasks (e.g., how conservative or liberal is Policy X) and simple classification (e.g., is this a picture of a cat), but not complex classification tasks. Additionally, higher accuracy rates are likely when all the information necessary to label a concept is located in a single text, and thus requires no additional documents or contextual knowledge. To use crowdsourcing for complex classification, the optimal approach we found was to identify, incentivize, and upweight skilled workers.  Crowdsourced data collection is, and will continue to be, an important method of data collection in the social sciences. However, our results demonstrate that, at least for complex classification tasks, trained research assistants are still required to achieve the highest levels of data validity. We expect this finding to have broad impact on the development and application of the crowdsourcing methodology.       Last Modified: 05/08/2020       Submitted by: Vito D'orazio]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
