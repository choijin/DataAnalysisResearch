<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DRMS: Improving Public Response to Weather Warnings</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>388092.00</AwardTotalIntnAmount>
<AwardAmount>487688</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert O'Connor</SignBlockName>
<PO_EMAI>roconnor@nsf.gov</PO_EMAI>
<PO_PHON>7032927263</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Despite improvements in weather forecasts both in terms of timeliness and accuracy, weather-related injury and death remain a serious problem. There is growing consensus that at least part of the problem is public distrust in the warnings themselves. This project investigates three psychological issues related to public distrust in weather warnings. The first has to do with inconsistency. Forecasts for high-impact weather events are often made days in advance to allow residents time to prepare. Subsequent forecasts for the same event may differ from earlier forecasts, giving the impression of inconsistency. Forecasters tend to assume that people distrust inconsistent forecasts and are reluctant to change forecasts even when better information becomes available, preferring to sacrifice accuracy for consistency. This project tests whether inconsistency or inaccuracy is more injurious to trust. Distrust may also arise from the fact that severe weather events are usually presented as certain because forecasters worry that admitting uncertainty signals incompetence. Most people, however, understand that weather prediction involves some level of uncertainty. Therefore too much certainty may seem implausible. This project tests whether adding an uncertainty estimate (e.g. the probability of a tornedo at your location) increases or decreases trust and compliance with warnings. Finally, distrust in warning forecasts may lead to delaying precautionary action in order to gather more information. If people wait too long, they may not have enough time to adequately protect themselves before severe weather hits. This project will determine the appropriate information to include in weather warnings to inspire trust and allow people to make timely decisions. Thus, the results of this project will influence forecast communication practices to provide people with better and more trustworthy information upon which to base critical weather related decisions, ultimately saving lives. The value of accurate weather warnings with generous lead times will be realized only if the public trusts them and acts accordingly. This research will inform "best practices" in warning communication procedures to promote both trust in the forecast and timely responses. As such, results of this work may well improve compliance with warnings and save lives.&lt;br/&gt;&lt;br/&gt;The research team investigates these issues in experimental studies, using realistic weather-related decision tasks. Establishing the hypothesized effects on trust and decisions in a controlled laboratory environment will permit testing forecast communication methods to address them. Of particular interest is the inclusion of specific uncertainty estimates. Conveying the notion that the forecast was intended as probabilistic may reduce effects of both inconsistency and inaccuracy. Uncertainty estimates may also attenuate delay beyond optimal stopping by satisfying the need for additional information. Likelihood communication, however, may need to be simplified in dynamic, time pressured situations so color coded risk scales will also be tested. Thus, these experiments will compare identical situations in which people receive either probabilistic forecasts, color-codes or conventional warnings, to determine which method leads to greater trust and better decisions. In sum, the goal is to carve out answers to a set of specific but critical questions, using careful experimental procedures that make direct comparisons between one situation and another and one form of communication and another, holding all other factors constant to yield firm conclusions about their effects. It is a cognitive-experimental approach to what has heretofore been a problem tackled primarily with other social science tools. This work will benefit the scientific community at large by providing a unique theoretical understanding of the cognitive processes involved in interpreting, trusting and acting on complex and dynamic predictions, with implications in diverse domains.</AbstractNarration>
<MinAmdLetterDate>04/20/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1559126</AwardID>
<Investigator>
<FirstName>Susan</FirstName>
<LastName>Joslyn</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Susan Joslyn</PI_FULL_NAME>
<EmailAddress>susanj@u.washington.edu</EmailAddress>
<PI_PHON>2066167183</PI_PHON>
<NSF_ID>000430240</NSF_ID>
<StartDate>04/20/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1321</Code>
<Text>Decision, Risk &amp; Mgmt Sci</Text>
</ProgramElement>
<ProgramElement>
<Code>1638</Code>
<Text>HDBE-Humans, Disasters, and th</Text>
</ProgramElement>
<ProgramElement>
<Code>p394</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>041E</Code>
<Text>HAZARD AND DISASTER REDUCTION</Text>
</ProgramReference>
<ProgramReference>
<Code>042E</Code>
<Text>HAZARD AND DISASTER RESPONSE</Text>
</ProgramReference>
<ProgramReference>
<Code>043E</Code>
<Text>EARTHQUAKE ENGINEERING</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~487688</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to investigate factors that might lead to reluctance in complying with severe weather warnings, due to distrust in the forecast. The first is failure to acknowledge uncertainty. This is true of many severe weather forecasts, such as a tornado or snowstorm, despite the fact that forecasters themselves are aware that the event is not certain and often have good estimates of event likelihood (e.g. 30% chance). Nonetheless, forecasters fear that members of the public will not be able to understand likelihood expressions such as this. Research in behavioral economics, showing that people make many errors when reasoning with uncertainty as compared to economically rational standards, tend to support this view (Kahneman &amp; Tversky, 1979). &nbsp;</p> <p>&nbsp;Here we provided evidence to the contrary. &nbsp;In several experimental studies we demonstrated that participants had greater trust in hypothetical forecasts and made better decisions when a numeric likelihood expression was included than when they were omitted. This remained true even when the tasks became more complex. In one study, in which there were 7 updates for the same hypothetical tornado event, participants made better decisions when each update had a likelihood estimate compared to a condition in which that information was omitted.&nbsp; In another study in which participants made decisions that included three options, each with its own decision threshold, participants made better decisions when likelihood estimates were provided for each threshold, compared to when they were omitted.&nbsp; This suggests that people can handle numeric likelihood expressions, even in fairly complex decision environments. Importantly, participants had greater trust in forecasts that acknowledged uncertainty and the added information helped them to make better decisions. <em></em></p> <p><strong>&nbsp;</strong>However adding color coding (e.g. orange=40% chance) may be problematic. The results of this project suggest that participants tended to overestimate the likelihood when color was added and misinterpreted color-coding (intended to convey likelihood) as indicating something about the severity of the event as well. In addition, participants tended to make more cautious decisions with color coding, which could be an advantage. However overcautiousness was mainly because participants mistakenly thought that the chance of the weather event was higher than what was intended. This, in turn, could reduce trust in future forecasts.</p> <p>&nbsp;The other major goal for this project was to determine the impact on user trust of forecast inconsistency. Forecasts for high-impact weather events are first made days in advance to allow residents time to prepare. Subsequent forecasts for the same event may differ, giving rise to the impression of inconsistency which may in turn reduce trust. Therefore, forecasters are often reluctant to update a forecast even when they believe they have more accurate information for fear that people will lose trust. In a series of experimental studies, we found that although there is a small reduction in trust with inconsistent forecasts when presented one after the other by the same hypothetical forecaster, the loss in trust due to inaccurate forecasts was much greater. Moreover adding an uncertainty estimate attenuated both effects. In addition analyses suggested that participants tended to weight the most recent forecast more heavily in their decision, suggesting that they understood that more recent information is likely to be more accurate. However, participants tended to be more cautious in their decisions when there was inconsistency, suggesting they understood that there may be more uncertainty when forecasts are less consistent. Somewhat surprisingly, there was no reduction in trust for inconsistent forecasts presented simultaneously by different hypothetical forecasters. Taken together, these results suggest that forecasters could provide updates when they become available, especially if they think the information is more accurate, because people can handle inconsistency such as this.</p> <p>From a theoretical perspective, this work advances our understanding of human decision making. Unlike earlier work that compared human decisions to standards of rational choice, here we compared human decisions with, to human decisions without explicit numeric uncertainty estimates. We found that people make better decisions when they have this information, although they are not perfectly rational decisions. Moreover participants had numerous intuitions about forecast uncertainty that allowed them to focus on the most relevant information in fairly complex decision environments.</p> <p>In addition this work has had broader impacts, allowing us to work closely with the National Weather Service to improve forecast communication. In general, these experimental findings suggest that members of the public could understand and cope with more relevant forecast information than is commonly available to them. Moreover, updated forecasts and uncertainty estimates could help people to make better decisions for themselves and their families.</p><br> <p>            Last Modified: 10/11/2020<br>      Modified by: Susan&nbsp;Joslyn</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to investigate factors that might lead to reluctance in complying with severe weather warnings, due to distrust in the forecast. The first is failure to acknowledge uncertainty. This is true of many severe weather forecasts, such as a tornado or snowstorm, despite the fact that forecasters themselves are aware that the event is not certain and often have good estimates of event likelihood (e.g. 30% chance). Nonetheless, forecasters fear that members of the public will not be able to understand likelihood expressions such as this. Research in behavioral economics, showing that people make many errors when reasoning with uncertainty as compared to economically rational standards, tend to support this view (Kahneman &amp; Tversky, 1979).     Here we provided evidence to the contrary.  In several experimental studies we demonstrated that participants had greater trust in hypothetical forecasts and made better decisions when a numeric likelihood expression was included than when they were omitted. This remained true even when the tasks became more complex. In one study, in which there were 7 updates for the same hypothetical tornado event, participants made better decisions when each update had a likelihood estimate compared to a condition in which that information was omitted.  In another study in which participants made decisions that included three options, each with its own decision threshold, participants made better decisions when likelihood estimates were provided for each threshold, compared to when they were omitted.  This suggests that people can handle numeric likelihood expressions, even in fairly complex decision environments. Importantly, participants had greater trust in forecasts that acknowledged uncertainty and the added information helped them to make better decisions.    However adding color coding (e.g. orange=40% chance) may be problematic. The results of this project suggest that participants tended to overestimate the likelihood when color was added and misinterpreted color-coding (intended to convey likelihood) as indicating something about the severity of the event as well. In addition, participants tended to make more cautious decisions with color coding, which could be an advantage. However overcautiousness was mainly because participants mistakenly thought that the chance of the weather event was higher than what was intended. This, in turn, could reduce trust in future forecasts.   The other major goal for this project was to determine the impact on user trust of forecast inconsistency. Forecasts for high-impact weather events are first made days in advance to allow residents time to prepare. Subsequent forecasts for the same event may differ, giving rise to the impression of inconsistency which may in turn reduce trust. Therefore, forecasters are often reluctant to update a forecast even when they believe they have more accurate information for fear that people will lose trust. In a series of experimental studies, we found that although there is a small reduction in trust with inconsistent forecasts when presented one after the other by the same hypothetical forecaster, the loss in trust due to inaccurate forecasts was much greater. Moreover adding an uncertainty estimate attenuated both effects. In addition analyses suggested that participants tended to weight the most recent forecast more heavily in their decision, suggesting that they understood that more recent information is likely to be more accurate. However, participants tended to be more cautious in their decisions when there was inconsistency, suggesting they understood that there may be more uncertainty when forecasts are less consistent. Somewhat surprisingly, there was no reduction in trust for inconsistent forecasts presented simultaneously by different hypothetical forecasters. Taken together, these results suggest that forecasters could provide updates when they become available, especially if they think the information is more accurate, because people can handle inconsistency such as this.  From a theoretical perspective, this work advances our understanding of human decision making. Unlike earlier work that compared human decisions to standards of rational choice, here we compared human decisions with, to human decisions without explicit numeric uncertainty estimates. We found that people make better decisions when they have this information, although they are not perfectly rational decisions. Moreover participants had numerous intuitions about forecast uncertainty that allowed them to focus on the most relevant information in fairly complex decision environments.  In addition this work has had broader impacts, allowing us to work closely with the National Weather Service to improve forecast communication. In general, these experimental findings suggest that members of the public could understand and cope with more relevant forecast information than is commonly available to them. Moreover, updated forecasts and uncertainty estimates could help people to make better decisions for themselves and their families.       Last Modified: 10/11/2020       Submitted by: Susan Joslyn]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
