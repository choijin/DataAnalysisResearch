<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Active data screening for efficient feature learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>160000.00</AwardTotalIntnAmount>
<AwardAmount>160000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Advances in sensing and data acquisition technologies make it easy to generate vast quantities of data that must be stored, communicated, processed, and understood. This data may be of variable quality and the nature of the data may vary over time -- this variability can cause difficulties for existing approaches to efficiently represent the data. Current methods use economical representations of the data in terms of a smaller number of properties, or features, of the raw data. Standard feature representations such as Fourier and wavelet representations may not be efficient at representing the data from these new acquisition technologies. One paradigm to overcome this mismatch is the data-driven approach, in which an algorithm processes the data to learn a novel and efficient feature representation for the given data. While these are more useful, such approaches may not scale well to massive data sets.&lt;br/&gt;&lt;br/&gt;This work designs new methods for data-driven feature learning that are scalable and robust to noisy, time-varying data. It proposes an "active screening" approach to learning new features; the data processing algorithm uses a low-complexity criterion to screen for useful and informative points for feature learning. Advantages of active screening include a reduction in the computational and storage overhead as well as the ability to reject outliers or other spurious and misleading data. The investigators develop active screening methods for consistent estimation under generative models for the data, analyze the tradeoff between representation and classification in active screening for discriminative dictionary learning, and extend the active screening analysis to distributed settings for distributed dictionary learning. They investigate the promise of these methods on two large-scale electrocardiography (ECG) datasets of 170+ patients. This work combines ideas from statistics (feature screening) and machine learning (active learning and selective sampling) to design efficient representations of complex signals from massive data sets and may inform the design of new data acquisition technologies by incorporating screening ideas into the technologies themselves.</AbstractNarration>
<MinAmdLetterDate>08/18/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1525276</AwardID>
<Investigator>
<FirstName>Waheed</FirstName>
<LastName>Bajwa</LastName>
<PI_MID_INIT>U</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Waheed U Bajwa</PI_FULL_NAME>
<EmailAddress>waheed.bajwa@rutgers.edu</EmailAddress>
<PI_PHON>8484458541</PI_PHON>
<NSF_ID>000601209</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anand</FirstName>
<LastName>Sarwate</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anand Sarwate</PI_FULL_NAME>
<EmailAddress>anand.sarwate@rutgers.edu</EmailAddress>
<PI_PHON>8484458516</PI_PHON>
<NSF_ID>000608994</NSF_ID>
<StartDate>08/18/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548058</ZipCode>
<StreetAddress><![CDATA[94 Brett Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~160000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="line-start-11">We are increasingly living in the age of information. "Data," collected from countless sources, are the raison d'etre for this "information age." It is anticipated that the large-scale collection of data spanning all aspects of our lives, coupled with advances in machine learning, will revolutionize our world for the better. In order for us to realize this vision, however, we need to be able to succinctly represent massive datasets. This compact representation will allow computationally efficient extraction of information. To do this, algorithms have to identify important features that represent the data: because these features may not be known a priori, algorithms must learn the important features from the data. This task, known as "representation learning" or "feature learning," is one of the most important tasks in a system that relies on machine learning for automated decision making.</p> <p id="line-start-13">While our understanding of feature learning for artificial intelligence has improved significantly over the last decade or so in the case of data collected by "single mode" sensors (e.g., microphones, accelerometers, and electrocardiogram), many modern datasets tend to be multimodal (also referred to as multiway or tensor-valued) in nature. Examples of such datasets include (multispectral) images and videos, dynamic magnetic resonance imaging, and models for next-generation (5G) communication channels. The main objective of this project was providing a foundational understanding of feature learning for such multimodal datasets.</p> <p id="line-start-15">To this end, the project focused on two specific feature learning techniques, namely, "low-rank representation learning" and "dictionary learning," and addressed the following fundamental problems pertaining to representation learning for multimodal data:</p> <ul> <li> <p id="line-start-17">Representation of human actions as a sequence of human body movements or action attributes enables development of models for human activity recognition and summarization from video data. This project introduced a novel representation learning model, termed the clustering-aware structure-constrained low-rank representation (CS-LRR) model, for unsupervised learning of human action attributes from video data. In addition, it developed a novel hierarchical subspace clustering approach, termed hierarchical CS-LRR, to learn the attributes without the need for a priori specification of their number. By visualizing and labeling these action attributes, the hierarchical model can be used to semantically summarize long video sequences of human actions at multiple resolutions. A human action or activity can also be uniquely represented as a sequence of transitions from one action attribute to another, which can then be used for human action recognition.</p> </li> <li> <p id="line-start-19">Many of the existing algorithms can be used for learning representations of multimodal data. This, however, involves "rearranging" multimodal data into a single mode. Unfortunately, such rearrangements result in loss of precious information that is encoded within the different modes of data. But how much, if any, loss is associated with such arrangements? This project helped address this question and explicitly quantified the loss associated with dictionary learning from rearranged multimodal data. One of the main insights of this project in this regard is that feature learning that explicitly takes into account the multimodal structure of data can lead to significant improvement in the fidelity of learned features.</p> </li> <li> <p id="line-start-21">What kind of computational algorithms can be used for dictionary learning from multimodal data? To this end, this project developed a novel algorithm, termed STARK, that learns dictionaries for data with any number of modes in an efficient manner. The developed algorithm, in particular, has been shown to outperform existing feature learning methods for multimodal data.</p> </li> <li> <p id="line-start-23">How many multimodal data samples are needed to guarantee that an algorithm can learn high-fidelity dictionaries for the multimodal data? This project formally addressed this question, with the final results again emphasizing the advantages associated with feature learning algorithms that explicitly take into account the multimodal nature of the data.</p> </li> </ul> <p id="line-start-25">The intellectual merit of this project lay in its ability to address the aforementioned research questions. Together, the answers to these research questions help us better understand the all-important problem of feature learning from multimodal data for machine learning tasks. The broader impacts of the outcomes of this research project include advances in the state-of-the-art of machine learning and artificial intelligence, which will directly benefit the national economy and effect all of our lives in the decades to come. In addition, this project helped train three PhD students (including a female student) and one female undergraduate student in the field of artificial intelligence. The principal investigators on this project also widely disseminated their research findings through talks, conference/workshop presentations, and journal papers, and posting of preprints on the internet.</p><br> <p>            Last Modified: 11/28/2017<br>      Modified by: Waheed&nbsp;U&nbsp;Bajwa</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[We are increasingly living in the age of information. "Data," collected from countless sources, are the raison d'etre for this "information age." It is anticipated that the large-scale collection of data spanning all aspects of our lives, coupled with advances in machine learning, will revolutionize our world for the better. In order for us to realize this vision, however, we need to be able to succinctly represent massive datasets. This compact representation will allow computationally efficient extraction of information. To do this, algorithms have to identify important features that represent the data: because these features may not be known a priori, algorithms must learn the important features from the data. This task, known as "representation learning" or "feature learning," is one of the most important tasks in a system that relies on machine learning for automated decision making. While our understanding of feature learning for artificial intelligence has improved significantly over the last decade or so in the case of data collected by "single mode" sensors (e.g., microphones, accelerometers, and electrocardiogram), many modern datasets tend to be multimodal (also referred to as multiway or tensor-valued) in nature. Examples of such datasets include (multispectral) images and videos, dynamic magnetic resonance imaging, and models for next-generation (5G) communication channels. The main objective of this project was providing a foundational understanding of feature learning for such multimodal datasets. To this end, the project focused on two specific feature learning techniques, namely, "low-rank representation learning" and "dictionary learning," and addressed the following fundamental problems pertaining to representation learning for multimodal data:   Representation of human actions as a sequence of human body movements or action attributes enables development of models for human activity recognition and summarization from video data. This project introduced a novel representation learning model, termed the clustering-aware structure-constrained low-rank representation (CS-LRR) model, for unsupervised learning of human action attributes from video data. In addition, it developed a novel hierarchical subspace clustering approach, termed hierarchical CS-LRR, to learn the attributes without the need for a priori specification of their number. By visualizing and labeling these action attributes, the hierarchical model can be used to semantically summarize long video sequences of human actions at multiple resolutions. A human action or activity can also be uniquely represented as a sequence of transitions from one action attribute to another, which can then be used for human action recognition.   Many of the existing algorithms can be used for learning representations of multimodal data. This, however, involves "rearranging" multimodal data into a single mode. Unfortunately, such rearrangements result in loss of precious information that is encoded within the different modes of data. But how much, if any, loss is associated with such arrangements? This project helped address this question and explicitly quantified the loss associated with dictionary learning from rearranged multimodal data. One of the main insights of this project in this regard is that feature learning that explicitly takes into account the multimodal structure of data can lead to significant improvement in the fidelity of learned features.   What kind of computational algorithms can be used for dictionary learning from multimodal data? To this end, this project developed a novel algorithm, termed STARK, that learns dictionaries for data with any number of modes in an efficient manner. The developed algorithm, in particular, has been shown to outperform existing feature learning methods for multimodal data.   How many multimodal data samples are needed to guarantee that an algorithm can learn high-fidelity dictionaries for the multimodal data? This project formally addressed this question, with the final results again emphasizing the advantages associated with feature learning algorithms that explicitly take into account the multimodal nature of the data.   The intellectual merit of this project lay in its ability to address the aforementioned research questions. Together, the answers to these research questions help us better understand the all-important problem of feature learning from multimodal data for machine learning tasks. The broader impacts of the outcomes of this research project include advances in the state-of-the-art of machine learning and artificial intelligence, which will directly benefit the national economy and effect all of our lives in the decades to come. In addition, this project helped train three PhD students (including a female student) and one female undergraduate student in the field of artificial intelligence. The principal investigators on this project also widely disseminated their research findings through talks, conference/workshop presentations, and journal papers, and posting of preprints on the internet.       Last Modified: 11/28/2017       Submitted by: Waheed U Bajwa]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
