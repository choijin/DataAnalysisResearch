<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: CCA: Cymric: A Flexible Processor-Near-Memory System Architecture</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>750000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Emerging software applications in data analytics, graph analysis, and machine learning must process increasing volumes of data. Such data-intensive applications stress the communication and storage parts of a computer system more than the computation parts. Yet most of today?s systems are ?compute-centric? in the sense that their designs focus on improving the efficiency of computation. Computing near data or processing-near-memory can significantly reduce data communication between memory and processor providing potential for significant improvement in energy-efficiency of next generation computing systems.  &lt;br/&gt;&lt;br/&gt;This project investigates and addresses the challenges facing the design of the next generation of memory-centric processors ? specifically processing-near-memory (PNM) architectures.  The project will characterize the energy and time behavior of future applications and use this understanding to assess how to best architect new systems that combine memory and compute in close proximity. Specifically, the activities will address i) programming models, ii) processing-near-memory architectures, and iii) power &amp; thermal managements with the development of cross-cutting data movement, compiler and microarchitectural optimizations. The results of this work can influence the design of future enterprise and high performance computing systems. The major principles, insights, and outcomes of this project will be integrated as modules into mainstream courses in Computer Architecture. The investigators will also continue their participation in Institute outreach programs that have the goal of promoting participation of undergraduate, minority, and female students in engineering research and higher education.</AbstractNarration>
<MinAmdLetterDate>08/12/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/12/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1533767</AwardID>
<Investigator>
<FirstName>Sudhakar</FirstName>
<LastName>Yalamanchili</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sudhakar Yalamanchili</PI_FULL_NAME>
<EmailAddress>sudha@ece.gatech.edu</EmailAddress>
<PI_PHON>4048942940</PI_PHON>
<NSF_ID>000161439</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Saibal</FirstName>
<LastName>Mukhopadhyay</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Saibal Mukhopadhyay</PI_FULL_NAME>
<EmailAddress>saibal@ece.gatech.edu</EmailAddress>
<PI_PHON>4043850866</PI_PHON>
<NSF_ID>000083185</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hyesoon</FirstName>
<LastName>Kim</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hyesoon Kim</PI_FULL_NAME>
<EmailAddress>hyesoon@cc.gatech.edu</EmailAddress>
<PI_PHON>4043850866</PI_PHON>
<NSF_ID>000084212</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~750000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The energy consumption for computing platforms has been increasing significantly. As we are in the big-data era, energy consumption resulting from data movement is one of the main reasons. Traditional computing platforms move data to the computing units, but this study explores the other option: moving the computation into the data. Understanding the characteristics of the energy and the time behavior for future applications and the best architecture to address the data movement cost are one of the main goals of this program. The form of these architectures is called near-memory computation or processing-in-memory. The recent development of 3D stacked memory enables processing-in-memory. HMC from Micron is one example of an earlier version of such a technology. This research advances programming models and architecture designs for processing-in-memory.</p> <p>&nbsp;</p> <p>The major intellectual outcomes of this program are several and focus on emerging applications (graphs and machine learning applications). Graphs are popular data structures for representing social networks, biological networks, transport networks, etc. In this program, we developed mechanisms to accelerate graph computation on processing-in-memory. The nature of graph processing demands a high data supply (memory bandwidth) and irregular memory accesses. Such complex memory accesses can be performed directly in the processing-in-memory rather than bringing the data into the computing units. Furthermore, graph data does not have a strong temporal locality, so the traditional memory hierarchy is not suitable for graph computation. By offloading graph processing in the memory, the traditional memory hierarchy would work more efficiently by reducing contention from the graph data. In the second work, we develop a performance and power model to decide when to offload computation into processing in the memory. It first decides which computation is good for offloading to the memory at compile time. It then performs benefit analysis. The model considers memory bandwidth effects, data processing cost, and the locality effects. In the third work, we demonstrate a possible processing-in-memory architecture using an FPGA. We develop an in-housing tool frame to explore different processing-in-memory architecture design options to take advantage of high data supply bandwidth. In the fourth work, we analyze the power and thermal behavior of in-processing memory and 3D stacked memory. This increases the understandability of thermal behavior and bandwidth relationships. Last, we study the effect of running machine learning workloads in the near-memory computation and propose a new architecture. Similar to processing-in-memory, &nbsp;we have also studied processing-near-sensors: moving the computation into the sensor where the data is produced so that machine learning workloads can also get the benefits of saving energy. &nbsp;This study shows the feasibility of achieving very high throughput as well as low energy performance. As an outcome, the program develops new types of near-memory processing for machine learning-based applications: NeuroCube for an acceleration of inference and training of Deep Neural Network (DNN) applications. These contributions represent a cross-cutting effort among algorithms, architecture, and circuit knowledge.</p> <p>&nbsp;</p> <p>The engineering contributions of this program translate the preceding intellectual contributions into open source software artifacts to benefit the larger research and development community and enable further developments. These include i) a new application benchmark suite for graph processing on near-memory computation, ii) architecture design tool explorations that prototype high-level architecture simulation to low-level implementations.</p> <p>This research program advances programming models and architecture design options for in-memory computing in which the computation is moved to the location of the data.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/06/2019<br>      Modified by: Hyesoon&nbsp;Kim</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The energy consumption for computing platforms has been increasing significantly. As we are in the big-data era, energy consumption resulting from data movement is one of the main reasons. Traditional computing platforms move data to the computing units, but this study explores the other option: moving the computation into the data. Understanding the characteristics of the energy and the time behavior for future applications and the best architecture to address the data movement cost are one of the main goals of this program. The form of these architectures is called near-memory computation or processing-in-memory. The recent development of 3D stacked memory enables processing-in-memory. HMC from Micron is one example of an earlier version of such a technology. This research advances programming models and architecture designs for processing-in-memory.     The major intellectual outcomes of this program are several and focus on emerging applications (graphs and machine learning applications). Graphs are popular data structures for representing social networks, biological networks, transport networks, etc. In this program, we developed mechanisms to accelerate graph computation on processing-in-memory. The nature of graph processing demands a high data supply (memory bandwidth) and irregular memory accesses. Such complex memory accesses can be performed directly in the processing-in-memory rather than bringing the data into the computing units. Furthermore, graph data does not have a strong temporal locality, so the traditional memory hierarchy is not suitable for graph computation. By offloading graph processing in the memory, the traditional memory hierarchy would work more efficiently by reducing contention from the graph data. In the second work, we develop a performance and power model to decide when to offload computation into processing in the memory. It first decides which computation is good for offloading to the memory at compile time. It then performs benefit analysis. The model considers memory bandwidth effects, data processing cost, and the locality effects. In the third work, we demonstrate a possible processing-in-memory architecture using an FPGA. We develop an in-housing tool frame to explore different processing-in-memory architecture design options to take advantage of high data supply bandwidth. In the fourth work, we analyze the power and thermal behavior of in-processing memory and 3D stacked memory. This increases the understandability of thermal behavior and bandwidth relationships. Last, we study the effect of running machine learning workloads in the near-memory computation and propose a new architecture. Similar to processing-in-memory,  we have also studied processing-near-sensors: moving the computation into the sensor where the data is produced so that machine learning workloads can also get the benefits of saving energy.  This study shows the feasibility of achieving very high throughput as well as low energy performance. As an outcome, the program develops new types of near-memory processing for machine learning-based applications: NeuroCube for an acceleration of inference and training of Deep Neural Network (DNN) applications. These contributions represent a cross-cutting effort among algorithms, architecture, and circuit knowledge.     The engineering contributions of this program translate the preceding intellectual contributions into open source software artifacts to benefit the larger research and development community and enable further developments. These include i) a new application benchmark suite for graph processing on near-memory computation, ii) architecture design tool explorations that prototype high-level architecture simulation to low-level implementations.  This research program advances programming models and architecture design options for in-memory computing in which the computation is moved to the location of the data.             Last Modified: 11/06/2019       Submitted by: Hyesoon Kim]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
