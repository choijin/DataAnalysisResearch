<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Medium: Collaborative Research: Information Compression in Algorithm Design and Statistical Physics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2015</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>461342.00</AwardTotalIntnAmount>
<AwardAmount>461342</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The existence of connections between probabilistic algorithms, statistical physics and information theory has been known for decades and has yielded a number of unexpected breakthroughs. Recent discoveries of the PIs and other researchers give clear indications that these connections go much deeper than previously thought. A key new idea is the realization that stochastic local search algorithms can be judged by their capacity to compress the randomness they consume, with convergence following as a consequence of compressibility. Further exploration of this idea is expected to have significant impact, both conceptual and technical, in multiple scientific fields. This includes algorithm design by information theoretic methods, the study of phase transitions in statistical mechanical systems based on information bottleneck arguments, and non-constructive proofs of existence of combinatorial objects. The project will offer a wide range of research opportunities at various levels of sophistication for graduate and undergraduate students in three state universities.&lt;br/&gt;&lt;br/&gt;Information compression arguments have recently found striking applications in computer science and combinatorics. A glowing example is Moser's proof of the algorithmic Lovasz Local Lemma, which suggested an entirely new way of reasoning about randomized algorithms. Inspired by the work of Moser, one of the PIs with a collaborator has very recently created a general framework for analyzing stochastic local search algorithms using information compression. The framework is purely algorithmic, completely bypassing the Probabilistic Method. Besides helping to analyze the running times of existing algorithms it can also be used as a powerful new tool for designing novel, non-obvious randomized algorithms. The proposed research further develops this framework with the aim of unearthing completely new applications in computer science and combinatorics, while establishing mathematically rigorous connections to statistical physics. Concrete examples of such applications to be investigated include new tools for bounding the mixing time of Markov chains and algebraic connections between randomized algorithms and the classical theory of phase transitions in statistical physics.</AbstractNarration>
<MinAmdLetterDate>06/08/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/13/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1514164</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Allender</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric W Allender</PI_FULL_NAME>
<EmailAddress>Allender@cs.rutgers.edu</EmailAddress>
<PI_PHON>8484457296</PI_PHON>
<NSF_ID>000391611</NSF_ID>
<StartDate>04/13/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mario</FirstName>
<LastName>Szegedy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mario Szegedy</PI_FULL_NAME>
<EmailAddress>szegedy@cs.rutgers.edu</EmailAddress>
<PI_PHON>2019321766</PI_PHON>
<NSF_ID>000297467</NSF_ID>
<StartDate>06/08/2015</StartDate>
<EndDate>04/13/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548072</ZipCode>
<StreetAddress><![CDATA[110 Frelinghuysen Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~461342</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Which functions are hard to compute, and which are easy?&nbsp; Which strings  contain a lot of information, and which do not?&nbsp; Although these  questions may seem unrelated, there are actually close connections  between the two.&nbsp; The best known approaches to measuring the amount of  information in a string center on the question of how much a string can  be compressed.&nbsp; More precisely, algorithmic information theory (also  known as Kolmogorov complexity) equates the information content of a  string with the length of its shortest description.&nbsp;&nbsp; This notion of  information is also closely connected with (and can provide an  alternative definition of) entropy, which arises in many fields of  study, including physics.&nbsp; A variant of Kolmogorov complexity, called  KT, takes into account not only the length of the shortest description  of a string, but also the time required to obtain the string from its  description.&nbsp; The KT complexity of the graph of a function turns out to  be an approximation of the circuit size that is required to compute the  function.</p> <p class="indentedCitation">The fifteen  publications that acknowledge the support of this grant (one of which  won a best student paper award) span a variety of topics related  compression and complexity.&nbsp; Much of the work centers on the Minimum  Circuit Size Problem (MCSP), and the related problem of determining the  KT complexity of a string (called MKTP).&nbsp; Work supported by the grant  established that certain approximations to MCSP and MKTP are of  intermediate complexity (neither tractable nor NP-complete) if  cryptographically-secure one-way functions exist, and other  approximations are provably not NP-hard under a well-studied notion of  reducibility.&nbsp; In addition, tighter connections were established between the  problem of computing the entropy of a distribution and computing KT  complexity; it remains an important open question whether this  connection also holds between computing entropy and MCSP.</p> <p class="indentedCitation">Work supported  by the grant also made progress on phase transitions in physical systems  (using the techniques of theoretical computer science to attack a  problem in physics), and on phase transitions in algorithmic behavior  (between classes of input instances where an algorithm performs quickly,  and where it requires a long run-time).</p><br> <p>            Last Modified: 07/08/2020<br>      Modified by: Eric&nbsp;W&nbsp;Allender</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Which functions are hard to compute, and which are easy?  Which strings  contain a lot of information, and which do not?  Although these  questions may seem unrelated, there are actually close connections  between the two.  The best known approaches to measuring the amount of  information in a string center on the question of how much a string can  be compressed.  More precisely, algorithmic information theory (also  known as Kolmogorov complexity) equates the information content of a  string with the length of its shortest description.   This notion of  information is also closely connected with (and can provide an  alternative definition of) entropy, which arises in many fields of  study, including physics.  A variant of Kolmogorov complexity, called  KT, takes into account not only the length of the shortest description  of a string, but also the time required to obtain the string from its  description.  The KT complexity of the graph of a function turns out to  be an approximation of the circuit size that is required to compute the  function. The fifteen  publications that acknowledge the support of this grant (one of which  won a best student paper award) span a variety of topics related  compression and complexity.  Much of the work centers on the Minimum  Circuit Size Problem (MCSP), and the related problem of determining the  KT complexity of a string (called MKTP).  Work supported by the grant  established that certain approximations to MCSP and MKTP are of  intermediate complexity (neither tractable nor NP-complete) if  cryptographically-secure one-way functions exist, and other  approximations are provably not NP-hard under a well-studied notion of  reducibility.  In addition, tighter connections were established between the  problem of computing the entropy of a distribution and computing KT  complexity; it remains an important open question whether this  connection also holds between computing entropy and MCSP. Work supported  by the grant also made progress on phase transitions in physical systems  (using the techniques of theoretical computer science to attack a  problem in physics), and on phase transitions in algorithmic behavior  (between classes of input instances where an algorithm performs quickly,  and where it requires a long run-time).       Last Modified: 07/08/2020       Submitted by: Eric W Allender]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
