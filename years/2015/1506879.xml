<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Optimal Bayesian Concentration Rates from Double Empirical Priors</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>125576.00</AwardTotalIntnAmount>
<AwardAmount>125576</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Statisticians frequently encounter problems that involve complicated models with high-dimensional parameters, particularly in "big data" settings. From a Bayesian perspective, it is imperative in these problems that the prior distribution be chosen to sit in a good position. Information about where is a good starting position can come from the data. There is a potential danger with this basic strategy, namely, that a double use of data might cause the model to track the data too closely, resulting on over fitting. To avoid this, the PIs introduce a regularization technique that suitably re-weights the likelihood, preventing the model from learning too quickly. This general "double empirical Bayes" strategy, where the prior is centered on the data and the likelihood is re-weighted, will be applied to several important and challenging high-dimensional problems, including estimation of sparse high-dimensional precision matrices, which is relevant to estimation of large complex networks. &lt;br/&gt;&lt;br/&gt;In this project, the PIs will develop this new double empirical Bayes framework for inference on high-dimensional parameters with a relatively low "complexity" or "effective dimension". For example, in function estimation problems, posited smoothness on the function is a constraint on its complexity. The first step of the double empirical Bayes strategy is to use a prior, indexed by the complexity of the parameter, centered at a complexity-specific estimate of the parameter based on data. To prevent the posterior from tracking the data too closely, the second step is to re-weight the likelihood to be combined with the data-dependent prior. The result is a sort of posterior distribution on the parameter space, and the PIs will provide general conditions for this posterior to concentrate around the truth at optimal rates. An additional advantage of this new approach is that the complexity-specific priors, for suitable centering, can be taken of relatively simple form, which facilitates computation. The PIs will investigate the double empirical Bayes analysis of several important high-dimensional inference problems, including density and function estimation, variable selection problems in non-linear models, and estimation of sparse precision matrices. Software will be developed for each application.</AbstractNarration>
<MinAmdLetterDate>06/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1506879</AwardID>
<Investigator>
<FirstName>Stephen</FirstName>
<LastName>Walker</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephen Walker</PI_FULL_NAME>
<EmailAddress>s.g.walker@math.utexas.edu</EmailAddress>
<PI_PHON>5124717154</PI_PHON>
<NSF_ID>000658016</NSF_ID>
<StartDate>06/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName/>
<StateCode>TX</StateCode>
<ZipCode>787121532</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~125576</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The aim of the project was to find empirical (data driven) prior distributions for Bayesian inference which allows for easy derivation of optimal posterior rates. That is, statistical learning about unknowns can be shown to be optimal. The idea behind the new priors is to let the math complexity usually associated with such calculations of obtaining optimal rates be reduced to simple levels.&nbsp;&nbsp;</p> <p>Posterior rates are an important topic and area of current research. Most researchers require optimal properties from their posterior distribution. Establishing such properties is at the mathematical end of the statistics spectrum and is not a trivial problem. Hence, any procedure or methodology which reduces this complexity will have a big impact. At the very least, the easier math allows simpler conditions on the prior for obtaining optimal properties. This is the achieved outcome of the project.&nbsp;</p> <p>With Ryan Martin, a paper is submitted to the Electronic Journal of Statistics, where we achieved precisely this goal. For a number of important models, finite dimensional parametric model; density estimation; mixture model estimation; sparse normal mean model; nonparametric density model and a regression function model, we obtained posteriors with optimal properties. Indeed, the math is relatively simple, compared to other procedures in the literature, and hence we make the topic more accessible to non mathematical researchers interested in properties of posterior distributions.&nbsp;</p> <p>I got two PhD students, in SDS at the University of Texas at Austin, intereted in the topic.</p> <p>With one student (SC) we worked on the topic of variable selection in high-dimensional sparse linear models. Variable selection is about understanding which inputs have a significant influence on the output. In the high-dimensional sparse case there are many inputs which could influence the output and yet it is ony a few of them which actually do so.</p> <!-- p, li { white-space: pre-wrap; } --> <p><!--StartFragment-->SC spent time reading about Bayesian consistency in general and specifically on papers involving sparse high--dimensional models. One such paper was on variable selection for the linear model and the use of spike and slab prior distributions, by Narisetty and He, in Annals of Statistics, 2014. This paper grabbed our attention, we realized there was a nice way we could improve on the settings. An issue with Bayesian variable selection procedures is that they require Markov chain Monte Carlo (MCMC) methods and with a large number of covariates, the number being much larger than sample size, the number of models the chain needs to visit is an extraordinarily large one, and even then it needs to visit in sufficient numbers in order to estimate the associated probabilities of each model.  In such cases it is preferable to estimate the inclusion or not of a covariate in the model using marginal properties, and to reject the badly estimated dependence structure of the posterior.</p> <p>Given this, we find a way to marginalize the model a priori and hence we can find the marginal properties explicitly without the need for MCMC. This means we can establish the same asymptotic properties as Narisetty and He but with simpler conditions -- a consequence of the explicit representations of the posterior.&nbsp; It took some time to write and make precise and the paper SC and Walker, ``Fast Bayesian variable selection for high--dimensional linear models: Solo spike and slab priors'' is submitted to the Electronic Journal of Statistics, and is currently undergoing a revision, after receiving favourable reports for the original submission.</p> <p>With the other student (MW) we worked on a more recent idea which avoids the need for a prior altogether. A sequence of posterior distributions with optimal properties is constructed directly from the data. The basic idea appeared in a paper by Belitser, in Annals of Statistics, 2017. However, while this paper advocated prior free posteriors, in practice the author used empirical priors. We developed a procedure to eliminate the prior.&nbsp;&nbsp;</p> <!-- p, li { white-space: pre-wrap; } --> <p><!--StartFragment-->This idea is now the foundation for the PhD research of MW. The work is ongoing, the PhD thesis, ``A class of data dependent posteriors'',&nbsp; is in progress.</p> <p>In summary, we; i.e. Ryan Martin and I, have demonstrated that there are simple mathematical constructions of posterior distributions which achieve optimal properties in a simple math framework. This should&nbsp; bring the topic of asymptotic Bayesian research to a more mainstream statistical audience.&nbsp;</p> <p>Two PhD students at the University of Texas at Austin became involved.</p> <p>One used the reading background to solve asymptotic problems associated with Bayesian variable selection - avoiding the need to use highly inefficient sampling techniques&nbsp; for estimation, and found explicit solutions which yield fast answers which are as accurate as it is currently possible to obtain.</p> <p>The other, MW, is progressing to make the sequence of posterior distributions even more tractable for finding their optimal properties by removing the prior altogether. This is a natural progression of the idea outlined in the proposal.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/03/2018<br>      Modified by: Stephen&nbsp;Walker</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The aim of the project was to find empirical (data driven) prior distributions for Bayesian inference which allows for easy derivation of optimal posterior rates. That is, statistical learning about unknowns can be shown to be optimal. The idea behind the new priors is to let the math complexity usually associated with such calculations of obtaining optimal rates be reduced to simple levels.    Posterior rates are an important topic and area of current research. Most researchers require optimal properties from their posterior distribution. Establishing such properties is at the mathematical end of the statistics spectrum and is not a trivial problem. Hence, any procedure or methodology which reduces this complexity will have a big impact. At the very least, the easier math allows simpler conditions on the prior for obtaining optimal properties. This is the achieved outcome of the project.   With Ryan Martin, a paper is submitted to the Electronic Journal of Statistics, where we achieved precisely this goal. For a number of important models, finite dimensional parametric model; density estimation; mixture model estimation; sparse normal mean model; nonparametric density model and a regression function model, we obtained posteriors with optimal properties. Indeed, the math is relatively simple, compared to other procedures in the literature, and hence we make the topic more accessible to non mathematical researchers interested in properties of posterior distributions.   I got two PhD students, in SDS at the University of Texas at Austin, intereted in the topic.  With one student (SC) we worked on the topic of variable selection in high-dimensional sparse linear models. Variable selection is about understanding which inputs have a significant influence on the output. In the high-dimensional sparse case there are many inputs which could influence the output and yet it is ony a few of them which actually do so.   SC spent time reading about Bayesian consistency in general and specifically on papers involving sparse high--dimensional models. One such paper was on variable selection for the linear model and the use of spike and slab prior distributions, by Narisetty and He, in Annals of Statistics, 2014. This paper grabbed our attention, we realized there was a nice way we could improve on the settings. An issue with Bayesian variable selection procedures is that they require Markov chain Monte Carlo (MCMC) methods and with a large number of covariates, the number being much larger than sample size, the number of models the chain needs to visit is an extraordinarily large one, and even then it needs to visit in sufficient numbers in order to estimate the associated probabilities of each model.  In such cases it is preferable to estimate the inclusion or not of a covariate in the model using marginal properties, and to reject the badly estimated dependence structure of the posterior.  Given this, we find a way to marginalize the model a priori and hence we can find the marginal properties explicitly without the need for MCMC. This means we can establish the same asymptotic properties as Narisetty and He but with simpler conditions -- a consequence of the explicit representations of the posterior.  It took some time to write and make precise and the paper SC and Walker, ``Fast Bayesian variable selection for high--dimensional linear models: Solo spike and slab priors'' is submitted to the Electronic Journal of Statistics, and is currently undergoing a revision, after receiving favourable reports for the original submission.  With the other student (MW) we worked on a more recent idea which avoids the need for a prior altogether. A sequence of posterior distributions with optimal properties is constructed directly from the data. The basic idea appeared in a paper by Belitser, in Annals of Statistics, 2017. However, while this paper advocated prior free posteriors, in practice the author used empirical priors. We developed a procedure to eliminate the prior.     This idea is now the foundation for the PhD research of MW. The work is ongoing, the PhD thesis, ``A class of data dependent posteriors'',  is in progress.  In summary, we; i.e. Ryan Martin and I, have demonstrated that there are simple mathematical constructions of posterior distributions which achieve optimal properties in a simple math framework. This should  bring the topic of asymptotic Bayesian research to a more mainstream statistical audience.   Two PhD students at the University of Texas at Austin became involved.  One used the reading background to solve asymptotic problems associated with Bayesian variable selection - avoiding the need to use highly inefficient sampling techniques  for estimation, and found explicit solutions which yield fast answers which are as accurate as it is currently possible to obtain.  The other, MW, is progressing to make the sequence of posterior distributions even more tractable for finding their optimal properties by removing the prior altogether. This is a natural progression of the idea outlined in the proposal.                          Last Modified: 10/03/2018       Submitted by: Stephen Walker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
