<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Deep Natural Language Understanding with Probabilistic Logic and Distributional Similarity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>408287.00</AwardTotalIntnAmount>
<AwardAmount>416287</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>D.  Langendoen</SignBlockName>
<PO_EMAI>dlangend@nsf.gov</PO_EMAI>
<PO_PHON>7032925088</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The web offers huge amounts of information, but that also makes it harder to find and extract relevant information. Natural  language processing has made huge strides in developing tools that extract information and automatically answer questions, often with relatively simple methods aimed at relatively superficial analysis. This project explores methods for a deeper analysis and detailed natural language understanding. Contemporary intelligent systems have long used logic to describe precisely what a sentence means and how its pieces connect. But this precision has a downside: Logic needs the data to exactly match its expectations, or it breaks down. This is problematic for applications like question answering because language is hugely variable. There are often many different ways to say the same thing, or to say things that are not exactly the same but similar enough to be relevant. This project combines logic with a technology that identifies words and passages that are  similar but not exact matches. Also, language often only implies things rather than stating them outright. The project handles this through a mechanism that draws conclusions that are likely but not 100% certain, and that states its level of confidence in a conclusion. &lt;br/&gt;&lt;br/&gt;Being highly interdisciplinary, the project gives students insights into logic and inferences, as well as methods that determine word similarity based on occurrences in large amounts of text. This project also forges new links between computational and theoretical linguistics by transferring ideas in both directions. Through its combination of precision and approximation, this project paves the way for language technology that understands language more deeply and thus will enhance societally important applications such as information extraction and and automatic question answering. Tasks in natural language semantics are requiring increasingly complex and fine-grained inferences. This project pursues the dual hypotheses that (a) logical form is best suited for supporting such inferences, and that (b) it is necessary to reason explicitly about uncertain, probabilistic information at the lexical level. This project combines logical form representations of sentence meaning with weighted inference rules derived from distributional similarity. It uses Markov Logic Networks for probabilistic inference over logical form with weighted rules, testing on the task of Recognizing Textual Entailment. It also develops new methods for describing word meaning in context distributionally in a way that is amenable to determining lexical entailment.</AbstractNarration>
<MinAmdLetterDate>07/21/2015</MinAmdLetterDate>
<MaxAmdLetterDate>11/30/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1523637</AwardID>
<Investigator>
<FirstName>Katrin</FirstName>
<LastName>Erk</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katrin Erk</PI_FULL_NAME>
<EmailAddress>katrin.erk@mail.utexas.edu</EmailAddress>
<PI_PHON>5129837913</PI_PHON>
<NSF_ID>000258351</NSF_ID>
<StartDate>07/21/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121532</ZipCode>
<StreetAddress><![CDATA[101 E. 27th Street, Stop A9000]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~408287</FUND_OBLG>
<FUND_OBLG>2016~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>What is sentence meaning, and how can we represent it in a way that will let a natural language understanding system draw the right conclusions from a text?&nbsp; One way to represent meaning is as a human-readable structure that can be used in rule-based inferences. Another is as a point or region in a "meaning space", where nearby points carry similar meanings. In this project, we have studied a combination of those two approaches, focusing on four questions. <br /><br />First, we showed how degrees of word similarity from a word meaning space can be integrated in a rule-based inference system if it are probabilistic, and we applied this idea to a task of drawing conclusions from text ("natural language inference").<br /><br />Second, we focused on one type of word-level relation that is particularly important for drawing conclusions from text: hypernymy (as in: an apple is-a fruit). We showed how hypernymy relations can be better detected in word-level meaning spaces, and how that helps natural language inference.<br /><br />Third, we inverted how we combined human-readable structures and meaning spaces. We studied how linguistic structure can be injected into text-level meaning spaces, and showed that such linguistic structure can be useful for natural language understanding systems.</p> <p><br />Fourth, the interaction between structure and meaning spaces is relevant not only at the level of natural language processing applications but also as a linguistic question: Meaning seems to be about both things and events in the world, and about patterns and regularities in use that people observe. As a first step, we proposed a mechanism for how observed regularities of word use can influence a person's beliefs about things in the world. We have since integrated the two views of meaning more closely in a framework that has a network of constraints encompassing both knowledge about the world and observed regularities in use. <br /><br />This project was interdisciplinary in nature in that it addressed questions both in natural language processing and in linguistics.<br /><br />Within this project, four graduate students and four undergraduate students have learned to build machine learning models and to integrate them with rule-based approaches, and to draw on both linguistics and computer science in their development of natural language processing approaches. The project has also resulted in the development of an undergraduate course in computational semantics that introduces students to programming and enables them to conduct their own independent projects in computational linguistics.<br /><br /></p><br> <p>            Last Modified: 10/18/2019<br>      Modified by: Katrin&nbsp;Erk</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ What is sentence meaning, and how can we represent it in a way that will let a natural language understanding system draw the right conclusions from a text?  One way to represent meaning is as a human-readable structure that can be used in rule-based inferences. Another is as a point or region in a "meaning space", where nearby points carry similar meanings. In this project, we have studied a combination of those two approaches, focusing on four questions.   First, we showed how degrees of word similarity from a word meaning space can be integrated in a rule-based inference system if it are probabilistic, and we applied this idea to a task of drawing conclusions from text ("natural language inference").  Second, we focused on one type of word-level relation that is particularly important for drawing conclusions from text: hypernymy (as in: an apple is-a fruit). We showed how hypernymy relations can be better detected in word-level meaning spaces, and how that helps natural language inference.  Third, we inverted how we combined human-readable structures and meaning spaces. We studied how linguistic structure can be injected into text-level meaning spaces, and showed that such linguistic structure can be useful for natural language understanding systems.   Fourth, the interaction between structure and meaning spaces is relevant not only at the level of natural language processing applications but also as a linguistic question: Meaning seems to be about both things and events in the world, and about patterns and regularities in use that people observe. As a first step, we proposed a mechanism for how observed regularities of word use can influence a person's beliefs about things in the world. We have since integrated the two views of meaning more closely in a framework that has a network of constraints encompassing both knowledge about the world and observed regularities in use.   This project was interdisciplinary in nature in that it addressed questions both in natural language processing and in linguistics.  Within this project, four graduate students and four undergraduate students have learned to build machine learning models and to integrate them with rule-based approaches, and to draw on both linguistics and computer science in their development of natural language processing approaches. The project has also resulted in the development of an undergraduate course in computational semantics that introduces students to programming and enables them to conduct their own independent projects in computational linguistics.         Last Modified: 10/18/2019       Submitted by: Katrin Erk]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
