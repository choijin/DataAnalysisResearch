<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: The Role of Instructor and Peer Feedback in Improving the Cognitive, Interpersonal, and Intrapersonal Competencies of Student Writers in STEM Courses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>45360.00</AwardTotalIntnAmount>
<AwardAmount>45360</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Connie Della-Piana</SignBlockName>
<PO_EMAI>cdellapi@nsf.gov</PO_EMAI>
<PO_PHON>7032925309</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Promoting Research and Innovation in Methodologies for Evaluation (PRIME) program seeks to support research on evaluation with special emphasis on: (1) exploring innovative approaches for determining the impacts and usefulness of STEM education projects and programs; (2) building on and expanding the theoretical foundations for evaluating STEM education and workforce development initiatives, including translating and adapting approaches from other fields; and (3) growing the capacity and infrastructure of the evaluation field.&lt;br/&gt;&lt;br/&gt;This project will have critical significance for Science, Technology, Engineering, and Mathematics (STEM) educators by increasing writing and collaboration skills in students, areas of importance to economics, science, and national security. This study focuses on teacher and peer interactions and writing quality and improvement in the context of undergraduate STEM courses. Specifically, the project will map the development of three competency domains (cognitive, interpersonal and intrapersonal) by researching the effects of teacher and peer response on writing improvement and knowledge adaptation in STEM courses. The project utilizes a web-based assessment tool called My Reviewers (MyR). The tool will be piloted by STEM faculty in college-level Introductory Biology or Chemistry on the campuses of University of South Florida (USF), North Carolina State University (NCSU), Dartmouth, Massachusetts Institute of Technology (MIT), and University of Pennsylvania (UPenn). Research domains include both academic performance and inter/intra-personal competencies. Project deliverables will provide new tools and procedures to assist in the assessment of students' knowledge, skills, and attitudes for project and program evaluation.&lt;br/&gt;&lt;br/&gt;Approximately 10,000 students enrolled in STEM courses at USF, NCSU, Dartmouth, MIT, and UPenn will upload their course-based writing to My Reviewers, an assessment tool, and use the tool to conduct peer reviews and team projects.  This information is supplemented by surveys of demographics and dispositions along with click patterns within the toolset. Researchers will subsequently analyze this wealth of data using predictive modeling of student writing ability and improvement, including text-based methods to identify useful features of comments, papers, peer reviews, student evaluations of other peers? reviews, and instructor and student meta-reflections. Outcome goals are to (1) demonstrate ways the assessment community can use real-time assessment tools to create valid measures of writing development; (2) provide quantitative evidence regarding the likely effects of particular commenting and scoring patterns on cohorts of students; (3) offer a domain map to help STEM educators better understand student success in the STEM curriculum; and (4) inform STEM faculty regarding the efficacy of peer review.</AbstractNarration>
<MinAmdLetterDate>09/16/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/16/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1544135</AwardID>
<Investigator>
<FirstName>Oleksandr</FirstName>
<LastName>Rudniy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Oleksandr Rudniy</PI_FULL_NAME>
<EmailAddress>arudniy@fdu.edu</EmailAddress>
<PI_PHON>2016922260</PI_PHON>
<NSF_ID>000637603</NSF_ID>
<StartDate>09/16/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Fairleigh Dickinson University</Name>
<CityName>Teaneck</CityName>
<ZipCode>076661914</ZipCode>
<PhoneNumber>2016922221</PhoneNumber>
<StreetAddress>1000 River Road</StreetAddress>
<StreetAddress2><![CDATA[Mail Stop: T-BE2-02]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>045671948</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FAIRLEIGH DICKINSON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>045671948</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Fairleigh Dickinson University]]></Name>
<CityName>Teaneck</CityName>
<StateCode>NJ</StateCode>
<ZipCode>076661914</ZipCode>
<StreetAddress><![CDATA[1000 River Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>009Z</Code>
<Text>PRIME - Promoting Research and Innovatio</Text>
</ProgramReference>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~45360</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Three journal papers and a conference presentation were produced. A new journal paper is being prepared.</p> <ol> <li>Rudniy, A. (2018) De-Identification of Laboratory      Reports in STEM. Journal of Writing Analytics.<em> Accepted</em>.</li> <li>Rudniy, A. (2018) Case Study&mdash;Data Warehouse Design for      Evidence-based Research. IEEE Transactions on Professional Communication.      Special Issue on Data-Driven Approaches to Research and Teaching in      Professional and Technical Communication. <em>Under 2<sup>nd</sup> round of review</em>.</li> <li>Reese, A.N., Rachamalla, R.R., Rudniy, A., Aull, L. and Eubanks, D. (2018). Contemporary Peer Review: Construct Modeling, Measurement Foundations, and the Future of Digital Learning. <em>Accepted</em>.</li> <li>Rudniy, A. (2018). Modeling Lab Report Scores in      Chemistry Courses. The 5th International Conference on Writing Analytics:      Writing Analytics, Data Mining, and Student Success, St. Petersburg, FL.</li> <li>Paper in writing. Deep Learning, Machine Learning and      Latent Semantic Analysis for Automated Scoring and Feedback in STEM.</li> </ol><ol> </ol><ol> </ol><ol> </ol> <p>The following products were developed:</p> <ul> <li>Algorithms for academic data de-identification using users&rsquo; lists, OpenNLP and NeuroNER toolkits. The techniques were designed and evaluated on a STEM dataset of laboratory reports. The goal was to find the most efficient method for de-identification student and faculty entities among (1) search and replace using a lookup table of users&rsquo; personal names, (2) application of a named entity recognition technique from a natural language processing toolkit based on machine learning, and (3) a deep artificial neural network for named entity recognition.</li> <li>&nbsp;Text representation models. Prepared two laboratory reports representation models. First, using GloVe word embeddings trained on the corpus Spring 2017 and Fall 2017 CHM 2045 and CHM 2046 laboratory reports. The second representation model consists of document-term matrices constructed from the laboratory reports corpora. Both models may be used in the future work.</li> <li>Automated Scoring Models. Several automated scoring models were designed: (1) using length measures and the Random Forest algorithm, (2) using document-term matrices and the Support Vector Machines algorithm, (3) using word embeddings and (4)&nbsp; a hybrid ANN with convolutional and recurrent layers.</li> <li>Datasets for automated scoring. Datasets for training machine learning models for the automated scoring task were produced by applying data manipulation with the Structured Query Language, custom programming, extensive analysis and manual processing for CHM 2045L and CHM 2046L.</li> <li>Automated splitting lab reports into sections with ANN-based model achieving high accuracy of 98%.</li> <li>Datasets for automated feedback. Several datasets were built for training ANNs for the automated feedback prediction: (1) a dataset for sequence-to-sequence translation comprised of training (60%), testing (20%) and validation (20%) sets.</li> <li>Automated feedback models. Two ANN models were evaluated for transforming sequence-to-sequence translation and machine comprehension to the task of producing automated feedback. Two other ANN models employing a deep convolutional neural network for feedback generation were built, evaluated and demonstrated high accuracy.</li> </ul><br> <p>            Last Modified: 10/05/2018<br>      Modified by: Oleksandr&nbsp;Rudniy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Three journal papers and a conference presentation were produced. A new journal paper is being prepared.  Rudniy, A. (2018) De-Identification of Laboratory      Reports in STEM. Journal of Writing Analytics. Accepted. Rudniy, A. (2018) Case Study&mdash;Data Warehouse Design for      Evidence-based Research. IEEE Transactions on Professional Communication.      Special Issue on Data-Driven Approaches to Research and Teaching in      Professional and Technical Communication. Under 2nd round of review. Reese, A.N., Rachamalla, R.R., Rudniy, A., Aull, L. and Eubanks, D. (2018). Contemporary Peer Review: Construct Modeling, Measurement Foundations, and the Future of Digital Learning. Accepted. Rudniy, A. (2018). Modeling Lab Report Scores in      Chemistry Courses. The 5th International Conference on Writing Analytics:      Writing Analytics, Data Mining, and Student Success, St. Petersburg, FL. Paper in writing. Deep Learning, Machine Learning and      Latent Semantic Analysis for Automated Scoring and Feedback in STEM.      The following products were developed:  Algorithms for academic data de-identification using users? lists, OpenNLP and NeuroNER toolkits. The techniques were designed and evaluated on a STEM dataset of laboratory reports. The goal was to find the most efficient method for de-identification student and faculty entities among (1) search and replace using a lookup table of users? personal names, (2) application of a named entity recognition technique from a natural language processing toolkit based on machine learning, and (3) a deep artificial neural network for named entity recognition.  Text representation models. Prepared two laboratory reports representation models. First, using GloVe word embeddings trained on the corpus Spring 2017 and Fall 2017 CHM 2045 and CHM 2046 laboratory reports. The second representation model consists of document-term matrices constructed from the laboratory reports corpora. Both models may be used in the future work. Automated Scoring Models. Several automated scoring models were designed: (1) using length measures and the Random Forest algorithm, (2) using document-term matrices and the Support Vector Machines algorithm, (3) using word embeddings and (4)  a hybrid ANN with convolutional and recurrent layers. Datasets for automated scoring. Datasets for training machine learning models for the automated scoring task were produced by applying data manipulation with the Structured Query Language, custom programming, extensive analysis and manual processing for CHM 2045L and CHM 2046L. Automated splitting lab reports into sections with ANN-based model achieving high accuracy of 98%. Datasets for automated feedback. Several datasets were built for training ANNs for the automated feedback prediction: (1) a dataset for sequence-to-sequence translation comprised of training (60%), testing (20%) and validation (20%) sets. Automated feedback models. Two ANN models were evaluated for transforming sequence-to-sequence translation and machine comprehension to the task of producing automated feedback. Two other ANN models employing a deep convolutional neural network for feedback generation were built, evaluated and demonstrated high accuracy.        Last Modified: 10/05/2018       Submitted by: Oleksandr Rudniy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
