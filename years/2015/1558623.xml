<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Topics in Analysis of Big Data and Complex Models</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2016</AwardEffectiveDate>
<AwardExpirationDate>03/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>238080.00</AwardTotalIntnAmount>
<AwardAmount>238080</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nancy Lutz</SignBlockName>
<PO_EMAI>nlutz@nsf.gov</PO_EMAI>
<PO_PHON>7032927280</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The proposed research seeks to provide researchers with new methods to study economic issues of current interest, making effective use of big datasets that are only recently available. The projects deal with issues in data preprocessing, estimation, and hypothesis testing. The emphasis is on methods with broad applicability and that can be put to practical use. The proposed research is also multidisciplinary, combining methodologies from statistics with those from computer science, while providing methods for empirical researchers from any field that uses statistical methods on massive data sets.&lt;br/&gt;&lt;br/&gt;The proposed research consists of three projects. The first project develops methods for efficient and effective analysis of big data for the purpose of understanding micro and macroeconomic phenomenon. While datasets that are terabytes in size are increasingly available, resource constraints often make it necessary to study a smaller set of observations, which raises the question about how to form subsamples. The investigator will develop methods that can efficiently use large datasets, while preserving data features valuable to economic analysis. The second project provides frequentist tools to assess sensitivity of the estimation results to model assumptions and features of the data, which will be particularly useful to assess the results from complex structural models. The third project will assess whether uncertainty is a cause or a consequence of economic fluctuations.  Given that there is no ideal instrument to distinguish uncertainty shocks from real activity shocks, the investigator will develop an iterative method that would purge the unwarranted variations from a potentially invalid instrument in order to arrive at a valid instrument. This generated external IV procedure can generally be used in applications when no valid instrument is available.</AbstractNarration>
<MinAmdLetterDate>03/31/2016</MinAmdLetterDate>
<MaxAmdLetterDate>03/31/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1558623</AwardID>
<Investigator>
<FirstName>Serena</FirstName>
<LastName>Ng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Serena Ng</PI_FULL_NAME>
<EmailAddress>serena.ng@columbia.edu</EmailAddress>
<PI_PHON>2128545488</PI_PHON>
<NSF_ID>000189625</NSF_ID>
<StartDate>03/31/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100277922</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1320</Code>
<Text>Economics</Text>
</ProgramElement>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<ProgramReference>
<Code>1320</Code>
<Text>ECONOMICS</Text>
</ProgramReference>
<ProgramReference>
<Code>1333</Code>
<Text>METHOD, MEASURE &amp; STATS</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~238080</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><ol> <li><strong>Robsut PCA</strong> : The standard PCA solves an unregularized problem and its asymptotic properties have been studied in earlier work supported by the NSF. In machine learning Robust PCA was developed&nbsp; to reduce the sensitivity of PCA estimates to outliers. This involves singular value thresholding and is usually solved as a convex programming problem. This project studies the statistical (rather than algorithmic) properties of factors estimated by RPCA. An important by-product&nbsp; is that we obtain an improved factor selection rule over the one developed in Bai and Ng (2002), which has the shortcoming of giving too much weight to the factors associated with small eigenvalues, and as a consequence, selects too many factors. The RPCA&nbsp; objective function additionally penalizes small eigenvalues and yields a new data dependent rule that fixes the problem.&nbsp;&nbsp;</li> <li><strong>Estimation based on Subsamples:</strong> Improved technology has dramatically reduced the&nbsp; cost of collecting data, and datasets that are&nbsp; terabytes in size are not uncommon.&nbsp; Not only&nbsp; can computation be slow with data of this size,&nbsp; memory and storage constraints may render analysis&nbsp; infeasible. This has motivated computer scientists to devise dimension reduction methods while preserving the structure of the original data. This project introduces these methods to economists. We study the implications of subspace reduction from the viewpoint of inference using the linear regression model as framework. We also propose divide and conquer methods that can make efficient use of the data while minimizing the computational bottlenecks. These methods can be useful in estimation of simple models with lots of data, or in estimation of complex models that can benefit from working with good sketches of the data can significantly reduce debugging time.&nbsp;</li> <li><strong>Seasonal Adjustment of Big Data</strong>: The seasonal adjustment issue with weakly data is that it is not strictly periodic and notoriously difficult to adjust. In a big data environment, the problem is complicated by the fact that we have lots and lots of series to adjust, and we only a few years of data to work with. Seasonal adjustment procedures that build on structural time series methods do not work.&nbsp;We take a machine learning approach to seasonal adjustment.&nbsp; We propose a two step approach. In the first step, we do univariate time series adjustment with the understanding that this is imperfect. In the second second, we pool information in the cross section. Machine learning tools are then used to find the best predictors for the within-year seasonal variations. A demand analysis of the&nbsp; adjusted budget shares finds&nbsp; three factors: one that is trending, and&nbsp; two cyclical ones that are&nbsp; well correlated with the level and change in consumer confidence. The effects of the Great Recession vary across&nbsp; locations and product groups,&nbsp; with&nbsp; consumers&nbsp; substituting towards home cooking away from non-essential goods. The business cycle information in the data are generally consistent with conventional data, making the data a valuable&nbsp; source of&nbsp; information on local economic conditions.</li> <li><strong>Factor Models with Missing Data and Matrix Completion</strong>:<br />This work provides a reconcilation of statistical factor analysis in the presence of missing data,&nbsp; with&nbsp; machine learning analysis of matrix completion. The two approaches use different assumptions, have&nbsp; different assessment of fit, and have&nbsp; different applications as motivation. In this project, we&nbsp; show that when the strong factor assumption used in the econometrics literature holds, the incoherence conditions used in the machine learning literature will also be satisfied. We then show that with missing data, the low rank&nbsp; component can be recovered. The fraction of missing data that permits&nbsp; consistent estimation of the low rank component is can be&nbsp; larger than what is typically assumed in the machine learning literature. Essentially, the strong factor assumptions allow us to put fewer restrictions on the pattern of missingness.&nbsp; The factor structure also&nbsp; permits assessment of sampling uncertainty&nbsp; by means of classical confidence intervals. Such results have no role in the algorithmic approach but are often used in&nbsp; economic analysis.</li> </ol><br> <p>            Last Modified: 05/01/2020<br>      Modified by: Serena&nbsp;Ng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Robsut PCA : The standard PCA solves an unregularized problem and its asymptotic properties have been studied in earlier work supported by the NSF. In machine learning Robust PCA was developed  to reduce the sensitivity of PCA estimates to outliers. This involves singular value thresholding and is usually solved as a convex programming problem. This project studies the statistical (rather than algorithmic) properties of factors estimated by RPCA. An important by-product  is that we obtain an improved factor selection rule over the one developed in Bai and Ng (2002), which has the shortcoming of giving too much weight to the factors associated with small eigenvalues, and as a consequence, selects too many factors. The RPCA  objective function additionally penalizes small eigenvalues and yields a new data dependent rule that fixes the problem.   Estimation based on Subsamples: Improved technology has dramatically reduced the  cost of collecting data, and datasets that are  terabytes in size are not uncommon.  Not only  can computation be slow with data of this size,  memory and storage constraints may render analysis  infeasible. This has motivated computer scientists to devise dimension reduction methods while preserving the structure of the original data. This project introduces these methods to economists. We study the implications of subspace reduction from the viewpoint of inference using the linear regression model as framework. We also propose divide and conquer methods that can make efficient use of the data while minimizing the computational bottlenecks. These methods can be useful in estimation of simple models with lots of data, or in estimation of complex models that can benefit from working with good sketches of the data can significantly reduce debugging time.  Seasonal Adjustment of Big Data: The seasonal adjustment issue with weakly data is that it is not strictly periodic and notoriously difficult to adjust. In a big data environment, the problem is complicated by the fact that we have lots and lots of series to adjust, and we only a few years of data to work with. Seasonal adjustment procedures that build on structural time series methods do not work. We take a machine learning approach to seasonal adjustment.  We propose a two step approach. In the first step, we do univariate time series adjustment with the understanding that this is imperfect. In the second second, we pool information in the cross section. Machine learning tools are then used to find the best predictors for the within-year seasonal variations. A demand analysis of the  adjusted budget shares finds  three factors: one that is trending, and  two cyclical ones that are  well correlated with the level and change in consumer confidence. The effects of the Great Recession vary across  locations and product groups,  with  consumers  substituting towards home cooking away from non-essential goods. The business cycle information in the data are generally consistent with conventional data, making the data a valuable  source of  information on local economic conditions. Factor Models with Missing Data and Matrix Completion: This work provides a reconcilation of statistical factor analysis in the presence of missing data,  with  machine learning analysis of matrix completion. The two approaches use different assumptions, have  different assessment of fit, and have  different applications as motivation. In this project, we  show that when the strong factor assumption used in the econometrics literature holds, the incoherence conditions used in the machine learning literature will also be satisfied. We then show that with missing data, the low rank  component can be recovered. The fraction of missing data that permits  consistent estimation of the low rank component is can be  larger than what is typically assumed in the machine learning literature. Essentially, the strong factor assumptions allow us to put fewer restrictions on the pattern of missingness.  The factor structure also  permits assessment of sampling uncertainty  by means of classical confidence intervals. Such results have no role in the algorithmic approach but are often used in  economic analysis.        Last Modified: 05/01/2020       Submitted by: Serena Ng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
