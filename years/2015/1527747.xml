<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Multimodal Brain Computer Interface for Human-Robot Interaction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2016</AwardEffectiveDate>
<AwardExpirationDate>04/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>736552.00</AwardTotalIntnAmount>
<AwardAmount>736552</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Human Robot Interaction (HRI) is research that is a key component in making robots part of our everyday life. Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface. However, an emerging area is the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain. BCIs vary widely in their capabilities, particularly with respect to spatial resolution, temporal resolution and noise. This project is aimed at exploring the use of multimodal BCIs for HRI. Multimodal BCIs, also referred to as hybrid BCIs (hBCI), have been shown to improve performance over single modality interfaces. This project is focused on using a novel suite of sensors (Electroencephalography (EEG), eye-tracking, pupillary size, computer vision, and functional Near Infrared Spectroscopy (fNIRS)) to improve current HRI systems. Each of these sensing modalities can reinforce and complement each other, and when used together, can address a major shortcoming of current BCIs which is the determination of the user state or situational awareness (SA). SA is a necessary component of any complex interaction between agents, as each agent has its own expectations and assumptions about the environment. Traditional BCI systems have difficulty recognizing state and context, and accordingly can become confusing and unreliable. This project will develop techniques to recognize state from multiple modalities, and will also allow the robot and human to learn about each other's state and expectations using the hBCI we are developing. The goal is to build a usable hBCI for real physical robot environments, with noise, real-time constraints, and added complexity.&lt;br/&gt;&lt;br/&gt;The technical contributions of this project include:&lt;br/&gt;1. Characterization of a novel hBCI interface for visual recognition and labeling tasks with real physical data and environments.&lt;br/&gt;2. Integration of fNIRS sensing with EEG and other modalities in human robot interaction tasks. We will test our ability in the temporal domain to determine at what timescale we can correctly classify movement components that would predict a correct (rewarding) trial or non-rewarding/incorrect movement.&lt;br/&gt;3. Analysis and validation of the hBCI in complex robotic tele-operation tasks with human subject operators such as open door, grasp object on table, pick up item off floor etc.&lt;br/&gt;4. Use of hBCI to characterize human/robot state and create a learning method to recognize state over time.&lt;br/&gt;5. Use of augmented reality for HRI decision making.&lt;br/&gt;6. Further develop hBCI for tracking cognitive states related to reward, motivation, attention and value.&lt;br/&gt;A new class of HRI interfaces will be developed that can expand the ability of humans to work with robots; promote the use and acceptance of robot agent systems in everyday life; expand the use of hBCIs in areas other than robotics for human-machine interaction; further the development of hBCIs as our system will be tapping into reward modulated activity that will be used via reinforcement learning to autonomously update the learning machinery; and bridge the educational divide between Engineering/Computer Science and Neuroscience.</AbstractNarration>
<MinAmdLetterDate>05/27/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1527747</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Allen</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter K Allen</PI_FULL_NAME>
<EmailAddress>allen@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397000</PI_PHON>
<NSF_ID>000183444</NSF_ID>
<StartDate>05/27/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Sajda</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Sajda</PI_FULL_NAME>
<EmailAddress>ps629@columbia.edu</EmailAddress>
<PI_PHON>2128546851</PI_PHON>
<NSF_ID>000153174</NSF_ID>
<StartDate>05/27/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[500 W. 120th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~736552</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Human Robot Interaction (HRI) is a research area that is a key component in making robots part of our everyday life.&nbsp; Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface. &nbsp;This project explored the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain. &nbsp;&nbsp;The intellectual merit of this project is:</p> <ol> <li>&nbsp;Development of a method to integrate the noisy EEG error signal of a human observer monitoring a robot task.&nbsp; From this feedback, we obtain a policy using supervised learning that subsequently augments the behavior policy and guides exploration in the early stages of Reinforcement Learning (RL). This bootstraps the RL learning process to enable learning from sparse rewards. Using a robotic navigation task as a test bed, we have shown that our method achieves a stable obstacle-avoidance policy, outperforming learning from sparse rewards only and alternative rich rewards that struggle to achieve the right obstacle avoidance behavior or fail to advance to the goal.</li> <li>Human Feedback present an effective way to train robot agents via inputs from non-expert humans, without a need for a specially designed reward function. However, this approach needs a human to be present and attentive during robot learning to provide evaluative feedback. In addition, the amount of feedback needed grows with the level of task difficulty and the quality of human feedback might decrease over time because of fatigue. To overcome these limitations and enable learning more robot tasks with higher complexities, there is a need to maximize the quality of expensive feedback received and reduce the amount of human cognitive involvement required.&nbsp; We have developed a method that uses active learning to smartly choose queries for the human supervisor based on the uncertainty of the robot and effectively reduce the amount of feedback needed to learn a given task. &nbsp;This makes it possible to learn tasks with more complexity using lesser amounts of human feedback compared to previous methods. We demonstrated the utility of our proposed method on a robot arm reaching task where the robot learns to reach a location in 3D without colliding with obstacles. Our approach is able to learn this task faster, with less human feedback and cognitive involvement, compared to previous methods that do not use active learning.</li> <li>When humans are engage in demanding tasks, our state of arousal can significantly affect our ability to make optimal decisions, judgments, and actions in real-world dynamic environments. The Yerkes?Dodson law, which posits an inverse-U relationship between arousal and task performance, suggests that there is a state of arousal that is optimal for behavioral performance in a given task. We have been able to show that we can use online neurofeedback to shift an individual?s arousal toward a state of improved performance. We are investigating using the BCI feedback signal as input to an RL agent which learns a control policy which intervenes when arousal state of the human operator suggests an oncoming failure. &nbsp;We believe that this type of "cognitive state" informed autonomy will improve human autonomy teaming and also increase human trust in autonomy.</li> </ol> <p>&nbsp;The broader impacts of this project include:</p> <ol> <li>&nbsp;Development of a new class of HRI interfaces that can expand the ability of humans to work with robots.</li> <li>Experimentally validating that signals from the brain related to cognitive control and arousal can be decoded on-line and used as neurofeedback to a subject, improving their performance on demanding tasks.</li> <li>Development of methods and systems that can bridge the educational divide between Engineering and Neuroscience.</li> </ol><br> <p>            Last Modified: 08/25/2020<br>      Modified by: Peter&nbsp;K&nbsp;Allen</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1527747/1527747_10426839_1594930238726_image1--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1527747/1527747_10426839_1594930238726_image1--rgov-800width.jpg" title="Accelerated Robot Learning via Human Brain Signals"><img src="/por/images/Reports/POR/2020/1527747/1527747_10426839_1594930238726_image1--rgov-66x44.jpg" alt="Accelerated Robot Learning via Human Brain Signals"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Supervised learning learns the policy from feedback - that way we can limit the amount of human feedback needed. The learned suboptimal feedback policy provides enough exploration for R, and learning from sparse reward makes it robust to significant EEG noise. The agent learns a 2D navigation task</div> <div class="imageCredit">olumbia Robotics Lab</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Accelerated Robot Learning via Human Brain Signals</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1527747/1527747_10426839_1594930732607_image2--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1527747/1527747_10426839_1594930732607_image2--rgov-800width.jpg" title="Learning from Simulated Feedback"><img src="/por/images/Reports/POR/2020/1527747/1527747_10426839_1594930732607_image2--rgov-66x44.jpg" alt="Learning from Simulated Feedback"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We simulate human feedback at various detection rates to analyze the robustness of our method to different levels of feedback accuracy.We find that feedback accuracy  accuracy, above 60% is required to learn a good HF policy, while 55% shown in green, is unable to achieve good performance</div> <div class="imageCredit">Columbia Robotics Lab</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Learning from Simulated Feedback</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Human Robot Interaction (HRI) is a research area that is a key component in making robots part of our everyday life.  Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface.  This project explored the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain.   The intellectual merit of this project is:   Development of a method to integrate the noisy EEG error signal of a human observer monitoring a robot task.  From this feedback, we obtain a policy using supervised learning that subsequently augments the behavior policy and guides exploration in the early stages of Reinforcement Learning (RL). This bootstraps the RL learning process to enable learning from sparse rewards. Using a robotic navigation task as a test bed, we have shown that our method achieves a stable obstacle-avoidance policy, outperforming learning from sparse rewards only and alternative rich rewards that struggle to achieve the right obstacle avoidance behavior or fail to advance to the goal. Human Feedback present an effective way to train robot agents via inputs from non-expert humans, without a need for a specially designed reward function. However, this approach needs a human to be present and attentive during robot learning to provide evaluative feedback. In addition, the amount of feedback needed grows with the level of task difficulty and the quality of human feedback might decrease over time because of fatigue. To overcome these limitations and enable learning more robot tasks with higher complexities, there is a need to maximize the quality of expensive feedback received and reduce the amount of human cognitive involvement required.  We have developed a method that uses active learning to smartly choose queries for the human supervisor based on the uncertainty of the robot and effectively reduce the amount of feedback needed to learn a given task.  This makes it possible to learn tasks with more complexity using lesser amounts of human feedback compared to previous methods. We demonstrated the utility of our proposed method on a robot arm reaching task where the robot learns to reach a location in 3D without colliding with obstacles. Our approach is able to learn this task faster, with less human feedback and cognitive involvement, compared to previous methods that do not use active learning. When humans are engage in demanding tasks, our state of arousal can significantly affect our ability to make optimal decisions, judgments, and actions in real-world dynamic environments. The Yerkes?Dodson law, which posits an inverse-U relationship between arousal and task performance, suggests that there is a state of arousal that is optimal for behavioral performance in a given task. We have been able to show that we can use online neurofeedback to shift an individual?s arousal toward a state of improved performance. We are investigating using the BCI feedback signal as input to an RL agent which learns a control policy which intervenes when arousal state of the human operator suggests an oncoming failure.  We believe that this type of "cognitive state" informed autonomy will improve human autonomy teaming and also increase human trust in autonomy.    The broader impacts of this project include:   Development of a new class of HRI interfaces that can expand the ability of humans to work with robots. Experimentally validating that signals from the brain related to cognitive control and arousal can be decoded on-line and used as neurofeedback to a subject, improving their performance on demanding tasks. Development of methods and systems that can bridge the educational divide between Engineering and Neuroscience.        Last Modified: 08/25/2020       Submitted by: Peter K Allen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
