<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: A Theory of Local Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>02/28/2017</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Artificial intelligence draws its inspiration from biological intelligence, and both rely on learning to achieve intelligent behavior. Thus, within AI, the field of machine learning plays a central role having indeed achieved impressive successes in dealing with many complex tasks, ranging from computer vision to language understanding, and thereby benefiting billions of humans. Machine learning uses large networks of artificial neurons, which are simplified versions of biological neurons, where learning is implemented by progressively adjusting the weights of the connections between these neurons. The deep learning problem faced by both biological and artificial neural networks is precisely the problem of how deep neurons, located far away from the network inputs or outputs, can adjust their connection weights to ensure that the networks behave intelligently. This fundamental problem and the space of its possible solutions are not well understood. Because deep learning has wide range of applications in technology and science, from computer vision to protein structure prediction, progress in our fundamental understanding of deep learning is likely to have a broad impact across multiple areas.  Furthermore, the theory of local learning is inspired by biological considerations and it has the potential for strengthening the bridge between AI/machine learning and neuroscience. The resulting theory, algorithms, data, software, and results will be broadly disseminated through multiple channels and integrated into educational and outreach efforts.  The project PI will continue his broad activities bringing research into undergraduate and graduate courses, outreach to local high school students through hosting at a summer program and lectures to high school students.&lt;br/&gt;&lt;br/&gt;To try to address the deep learning problem, over half a century ago D. Hebb proposed a vague strategy often summarized by the expression "neurons that fire together, wire together". The essence of this effort is to improve our fundamental understanding of deep learning by bringing clarity to Hebb's proposal and providing a novel rigorous framework for studying learning rules. The framework requires first introducing the notion of local learning: in a physical neural system, learning rules for adjusting connection weights must be local, i.e. functions of only variables that are available locally. Thus one must separate the definition of local variables from the functional form that ties them together into a learning rule. This separation enables the creation of a systematic program for studying local learning rules, by first stratifying learning rules according to their functional complexity, and then studying their behaviors in networks of increasing complexity, from shallow and linear to deep and non-linear. The proposed program of study is likely to lead to the discovery of new learning rules and a better understanding of the capacity and limitations of local learning, ultimately advancing our theoretical and practical understanding of the deep learning problems and its solutions.</AbstractNarration>
<MinAmdLetterDate>07/23/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/23/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1550705</AwardID>
<Investigator>
<FirstName>Pierre</FirstName>
<LastName>Baldi</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pierre F Baldi</PI_FULL_NAME>
<EmailAddress>pfbaldi@ics.uci.edu</EmailAddress>
<PI_PHON>9498245809</PI_PHON>
<NSF_ID>000440884</NSF_ID>
<StartDate>07/23/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926173067</ZipCode>
<StreetAddress><![CDATA[4038 Bren Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern machine learning methods, in particular deep learning, are having a profound impact on how big data is mined. Deep learning, associated with learning systems comprised of multiple layers of adaptive processing steps, is being used to derive state-of-the-art performance in complex engineering problems ranging from computer vision, to natural language processing, to speech recognition, to robotics. Information technology companies, from Google to Facebook,have mounted significant machine learning operations in these areas, with large data, computing resources, and man power. However, our fundamental understanding of deep learning remains shallow.</p> <p>In this project, we have increased our fundamental understanding of deep learning by developing a theory of local learning, as a generalization of Hebbian learning and gained a better understanding of learning algorithms. The project has led to the discovery of a new fundamental concept: the deep learning channel.</p> <p>Briefly, in a physical learning system (e.g. brain or neuromorphic chip), learning rules for adjusting local parameters, such as synaptic weights, must be functions of locally available variables. This allows one to stratify learning rules by their functional complexity and we have conducted a simple program of studying all polynomial learning rules stratified by the degree of the polynomial. Stacking local learning rules in a deep feedforward network leads to deep local learning. We have shown that it is not possible to learn complex functions using deep local learning.</p> <p>In order to be able to learn complex functions and solve complex task, such as computer vision, the deep weights of the network must depend on the targets. Therefore, for optimal learning of complex functions, there must exist a communication channel that conveys information about the targets from the output layer to the deep weights. This is the deep learning channel. In this effort, we have begun the systematic study of the deep learning channel as a communication channel. In particular, how is the learning channel physically implemented? What information must be communicated through this channel? And what is the rate/capacity of the learning channel?&nbsp;</p> <p>The study of these questions has broad impacts for deep learning and for machine learning in general. First, these questions can be extended to other forms of learning, such as reinforcement learning. Second, because deep learning has a very wide range of applications in science and technology, from computer vision to high-energy physics, &nbsp;this study is relevant for many areas of application of machine learning. &nbsp;Finally, the theory of local learning is inspired by biological considerations and it &nbsp;deepens our conceptual framework for understanding learning in biological systems (as well as neuromorphic chips) &nbsp;and further strengthens the two-way bridge between AI/machine learning and neuroscience.</p> <p>All the results have been broadly disseminated through publications in the peer-reviewed scientific literature. All sofware and data produced in the applications of the theory have also been made public through web servers and public repositories.</p><br> <p>            Last Modified: 03/16/2017<br>      Modified by: Pierre&nbsp;F&nbsp;Baldi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern machine learning methods, in particular deep learning, are having a profound impact on how big data is mined. Deep learning, associated with learning systems comprised of multiple layers of adaptive processing steps, is being used to derive state-of-the-art performance in complex engineering problems ranging from computer vision, to natural language processing, to speech recognition, to robotics. Information technology companies, from Google to Facebook,have mounted significant machine learning operations in these areas, with large data, computing resources, and man power. However, our fundamental understanding of deep learning remains shallow.  In this project, we have increased our fundamental understanding of deep learning by developing a theory of local learning, as a generalization of Hebbian learning and gained a better understanding of learning algorithms. The project has led to the discovery of a new fundamental concept: the deep learning channel.  Briefly, in a physical learning system (e.g. brain or neuromorphic chip), learning rules for adjusting local parameters, such as synaptic weights, must be functions of locally available variables. This allows one to stratify learning rules by their functional complexity and we have conducted a simple program of studying all polynomial learning rules stratified by the degree of the polynomial. Stacking local learning rules in a deep feedforward network leads to deep local learning. We have shown that it is not possible to learn complex functions using deep local learning.  In order to be able to learn complex functions and solve complex task, such as computer vision, the deep weights of the network must depend on the targets. Therefore, for optimal learning of complex functions, there must exist a communication channel that conveys information about the targets from the output layer to the deep weights. This is the deep learning channel. In this effort, we have begun the systematic study of the deep learning channel as a communication channel. In particular, how is the learning channel physically implemented? What information must be communicated through this channel? And what is the rate/capacity of the learning channel?   The study of these questions has broad impacts for deep learning and for machine learning in general. First, these questions can be extended to other forms of learning, such as reinforcement learning. Second, because deep learning has a very wide range of applications in science and technology, from computer vision to high-energy physics,  this study is relevant for many areas of application of machine learning.  Finally, the theory of local learning is inspired by biological considerations and it  deepens our conceptual framework for understanding learning in biological systems (as well as neuromorphic chips)  and further strengthens the two-way bridge between AI/machine learning and neuroscience.  All the results have been broadly disseminated through publications in the peer-reviewed scientific literature. All sofware and data produced in the applications of the theory have also been made public through web servers and public repositories.       Last Modified: 03/16/2017       Submitted by: Pierre F Baldi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
