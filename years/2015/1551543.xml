<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: A Communicative Perspective on Quantitative Syntax</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>11650.00</AwardTotalIntnAmount>
<AwardAmount>11650</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research addresses the question: why are human languages the way they are? The researchers propose that a number of the properties of languages can be explained as design choices that maximize ease of communication for the human brain. Understanding how languages are (or aren't) optimized for communication will be crucial to developing scientific models of human language understanding and learning.  The resulting understanding of the purpose of certain language structures will enable more effective teaching of those structures in second language pedagogy. In addition, understanding why languages are the way they are will also affect computer systems for natural language understanding. Over the last decade there have been major advances in computer understanding of natural language, often using surprisingly simple algorithms. It is not known what properties of language make it possible to get so far with such simple algorithms. Once these properties are understood, it will be possible to leverage them to develop even more effective algorithms. &lt;br/&gt;&lt;br/&gt;The research focuses on predicting the quantitative word order properties of languages as observed in large annotated corpora of text. Data comes from the Universal Dependencies project, a recent collaboration of several universities and Google to develop uniformly annotated dependency-parsed corpora for about 40 languages. The researchers will develop simple models of communication that incorporate the probability of error in communication and certain well-known points of difficulty in human language comprehension (due to factors such as limited working memory resources). The frequency distribution of different word orders in the dependency corpora will be modeled by assuming that approximately rational speakers selects utterances which have a high probability of being understood. The researchers aim to use these models to explain the distribution of dependency length (distance between syntactically related words) and the distribution of the degree of word order freedom (the extent to which words can appear in different orders while keeping the same meaning).</AbstractNarration>
<MinAmdLetterDate>01/14/2016</MinAmdLetterDate>
<MaxAmdLetterDate>01/14/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1551543</AwardID>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Gibson</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward A Gibson</PI_FULL_NAME>
<EmailAddress>egibson@mit.edu</EmailAddress>
<PI_PHON>6172538609</PI_PHON>
<NSF_ID>000215436</NSF_ID>
<StartDate>01/14/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Futrell</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard Futrell</PI_FULL_NAME>
<EmailAddress>rfutrell@uci.edu</EmailAddress>
<PI_PHON>5047223183</PI_PHON>
<NSF_ID>000699693</NSF_ID>
<StartDate>01/14/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8374</Code>
<Text>DDRI Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~11650</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Why are human languages the way they are? If we believe languages are codes for efficient communication, we are faced with a puzzle: scientists in the fields of information theory and telecommunications have studied theoretically optimal codes, and they don't look much like human language at all. How can we reconcile the theory of optimal codes with human languages, in order to have a theory that predicts the properties of human languages? Such a theory would enable us to use information theory to analyze human languages, providing a new mathematical basis for the development of systems for natural language understanding, such as dialogue agents such as Apple's Siri and Amazon's Alexa and chatbots. It would be especially useful for developing technologies for languages where not much language data is available, thus increasing the inclusivity of language technology to linguistically marginalized groups. In addition, such a theory would provide new ways to analyze and identify disorders in linguistic processing and development.</p> <p>The key insight driving this research is to view human languages as optimal codes for communication under human-specific information processing constraints. This research focuses in particular on predicting the word order properties of languages using this theory. For example, in languages where verbs come before their objects, prepositions typically come before nouns: for example in English we say "Sam ate oranges" (with the object "oranges" after the verb "ate") and also "in the park" (with the preposition "in" before its noun "park"). On the other hand, in languages where verbs come after their objects, the preposition typically comes after its noun: so in Japanese, one says the equivalent of "Sam oranges ate", with the object before the verb, and "the park in", with the preposition (now a "post"position) after the noun. What would predict word order patterns such as these?</p> <p>Our goal is to show that these word order patterns emerge from efficient communicaton under information processing constraints. In particular we focus on constraints on working memory that affect people as they incrementally produce and comprehend sentences. Our research involved developing theories of what make sentences hard to process, and then testing whether these theories predicted word order patterns in datasets of natural language text under the assumption that people prefer to use word order patterns that lead to efficient data processing.</p> <p>The major result of this work is the develoment of a new theory of difficulty in human language processing, and the derivation from that theory of a new constraint on efficiently processable sentences: information locality. The basic idea of information locality is just that words that are related to each other should be close in sentences. This old idea that it is preferable to "keep related words together" is formalized and operationalized in information locality, and derived from a new general theory of human sentence processing that unifies two major existing theories of human sentence processing. This new, mathematically precise theory predicts the word order patterns described above in addition to making a host of new predictions about word order.&nbsp;</p> <p>The predictions of information locality were tested by developing statistical models of how words in sentences are put in order, and showing that making these models more accurate requires adding information locality as an additional constraint. These models of word order are useful beyond this scientific enterprise, because they can form a key component of engineering systems for machine translation and text summarization.</p> <p>In addition, our unified theory of sentence processing applies not only to human language processing, but also to language processing by artificial systems such as recurrent neural networks, which are rapidly becoming the industry standard for many natural language understanding tasks. This theory can be used to analyze the output and errors of these systems, thus resolving a crucial barrier to their wider application: that they are difficult to debug.</p> <p>The theory of information locality will also make it easier to develop natural language technologies for linguistically marginalized groups whose languages do not have large resources from which machine learning systems can be trained. Information locality provides a crucial constraint that is expected to hold in all languages, thus enabling us to make educated guesses about how low-resource and under-documented languages work. The result will be greater inclusion of linguistically marginalized groups via language technologies.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/30/2017<br>      Modified by: Richard&nbsp;Futrell</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Why are human languages the way they are? If we believe languages are codes for efficient communication, we are faced with a puzzle: scientists in the fields of information theory and telecommunications have studied theoretically optimal codes, and they don't look much like human language at all. How can we reconcile the theory of optimal codes with human languages, in order to have a theory that predicts the properties of human languages? Such a theory would enable us to use information theory to analyze human languages, providing a new mathematical basis for the development of systems for natural language understanding, such as dialogue agents such as Apple's Siri and Amazon's Alexa and chatbots. It would be especially useful for developing technologies for languages where not much language data is available, thus increasing the inclusivity of language technology to linguistically marginalized groups. In addition, such a theory would provide new ways to analyze and identify disorders in linguistic processing and development.  The key insight driving this research is to view human languages as optimal codes for communication under human-specific information processing constraints. This research focuses in particular on predicting the word order properties of languages using this theory. For example, in languages where verbs come before their objects, prepositions typically come before nouns: for example in English we say "Sam ate oranges" (with the object "oranges" after the verb "ate") and also "in the park" (with the preposition "in" before its noun "park"). On the other hand, in languages where verbs come after their objects, the preposition typically comes after its noun: so in Japanese, one says the equivalent of "Sam oranges ate", with the object before the verb, and "the park in", with the preposition (now a "post"position) after the noun. What would predict word order patterns such as these?  Our goal is to show that these word order patterns emerge from efficient communicaton under information processing constraints. In particular we focus on constraints on working memory that affect people as they incrementally produce and comprehend sentences. Our research involved developing theories of what make sentences hard to process, and then testing whether these theories predicted word order patterns in datasets of natural language text under the assumption that people prefer to use word order patterns that lead to efficient data processing.  The major result of this work is the develoment of a new theory of difficulty in human language processing, and the derivation from that theory of a new constraint on efficiently processable sentences: information locality. The basic idea of information locality is just that words that are related to each other should be close in sentences. This old idea that it is preferable to "keep related words together" is formalized and operationalized in information locality, and derived from a new general theory of human sentence processing that unifies two major existing theories of human sentence processing. This new, mathematically precise theory predicts the word order patterns described above in addition to making a host of new predictions about word order.   The predictions of information locality were tested by developing statistical models of how words in sentences are put in order, and showing that making these models more accurate requires adding information locality as an additional constraint. These models of word order are useful beyond this scientific enterprise, because they can form a key component of engineering systems for machine translation and text summarization.  In addition, our unified theory of sentence processing applies not only to human language processing, but also to language processing by artificial systems such as recurrent neural networks, which are rapidly becoming the industry standard for many natural language understanding tasks. This theory can be used to analyze the output and errors of these systems, thus resolving a crucial barrier to their wider application: that they are difficult to debug.  The theory of information locality will also make it easier to develop natural language technologies for linguistically marginalized groups whose languages do not have large resources from which machine learning systems can be trained. Information locality provides a crucial constraint that is expected to hold in all languages, thus enabling us to make educated guesses about how low-resource and under-documented languages work. The result will be greater inclusion of linguistically marginalized groups via language technologies.                         Last Modified: 08/30/2017       Submitted by: Richard Futrell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
