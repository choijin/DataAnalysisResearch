<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative Research: Computer-Aided Response-to-Intervention for Reading Comprehension Disabilities</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>165787.00</AwardTotalIntnAmount>
<AwardAmount>165787</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Common Core Learning Standards (CCLS), adopted by 44 states and the District of Columbia, define the skills a student should demonstrate by the end of each grade.  One key skill emphasized by CCLS is reading ability, which is the precursor for learning in all content areas.  In New York State, students in grades 3-8 take an English Language Arts (ELA) test each spring to measure their CCLS achievement in reading.  An ELA test contains both multiple choice questions and open-ended questions based on short text passages; to do well, students should be able to read a passage closely for textual evidence and draw logical inferences from it. To report the results, the number of correct student responses is converted into a scale score; this in turn is divided into four performance levels: NYS Level 1 for well below proficient, NYS Level 2 for partially proficient, NYS Level 3 for proficient, and NYS Level 4 for exceptional in grade-level standards.  Schools arrange academic intervention services for students whose performance level is either NYS Level 1 or NYS Level 2. To drive change in students who are at risk for not meeting academic expectations, the Response-to-Intervention model aims to deliver instructions as a function of these assessment outcomes.  But the PIs argue that a single performance score as the assessment outcome is often insufficient for identifying underlying learning problems, especially in reading comprehension.  In this exploratory project they will focus on discovering error patterns in assessment outcomes at the lexical level, in the expectation these will ultimately lead to improved understanding of how the raw data from a pool of underperforming text-based analytic reading assessments can be transformed into an informative and understandable structure for delivery of effective reading comprehension interventions.  Project outcomes will complement the current scoring system by supporting diagnosis at an individual level, and by facilitating grouping of students with similar reading disabilities in the same intervention group in order to optimize school teaching resources. The approach will also support a data-driven instruction framework by maximizing the information gain from each test, which can result in fewer tests taken and more hours for teaching per school year.&lt;br/&gt;&lt;br/&gt;This is an interdisciplinary collaboration between a computer scientist (Tsai) and an expert in literacy education (Zakierski).  PI Tsai will be responsible for computer algorithm development and data analysis, whereas PI Zakierski will be in charge of data collection and evaluation of the proposed approach based on findings in literacy and pedagogy.  The team will build a database containing words with lexical properties from literature for children up to grade 3, assessment materials from NYS ELAs, and intervention records.  They will annually collect ELA assessment materials from a pool of approximately 120 third grade students with performance at NYS Level 2 or below, for both research development and evaluation.  They will develop a computer-aided intervention system that performs data-mining on underperforming individual ELA assessment materials to discover error patterns, which should assist the teacher in identifying a student's underlying reading comprehension problems in order to prepare a more effective instruction plan.  And they will evaluate the performance by doing both formative and summative assessments, the former to consist of questionnaires for teachers and mock ELA tests for students taken during the period of intervention, and the latter being the real ELA tests in April following the intervention. From the computer science perspective, the main challenge is the small size of the dataset.  The PIs will develop new techniques that are domain-knowledge driven for performing meaningful analysis to discover error patterns in such situations; if successful, the approach will open the door to broad research opportunities in other cases where "small data" is easier to come by.  In addition, the exploration of data mining on literacy education itself will constitute a unique contribution, since the marriage of the two fields has not yet received much attention from the research community and there are many interesting questions waiting to be addressed using computational approaches.</AbstractNarration>
<MinAmdLetterDate>07/09/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1543639</AwardID>
<Investigator>
<FirstName>Chia-LIng</FirstName>
<LastName>Tsai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Chia-LIng Tsai</PI_FULL_NAME>
<EmailAddress>CTsai@iona.edu</EmailAddress>
<PI_PHON>9146332241</PI_PHON>
<NSF_ID>000515040</NSF_ID>
<StartDate>07/09/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Iona College</Name>
<CityName>New Rochelle</CityName>
<ZipCode>108011890</ZipCode>
<PhoneNumber>9146332602</PhoneNumber>
<StreetAddress>715 North Avenue</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY16</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078708914</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>IONA COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>078708914</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Iona College]]></Name>
<CityName>New Rochelle</CityName>
<StateCode>NY</StateCode>
<ZipCode>108011830</ZipCode>
<StreetAddress><![CDATA[715 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~165787</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Default">This interdisciplinary project focused on discovering error patterns in assessment outcomes to facilitate effective teaching for reading comprehension in early elementary education<strong>.</strong> We collected ELA (English Language Arts) assessment data for 6 mock examinations over 2 years, involving a total of 365 4th grade students from 2 different schools in NY. All examination questions were labeled by a literacy expert with one of 9 performance indicators defined by NYSED (New York State Education Department), covering 3 main reading skills---making summary, making inference, and locating information. We also compiled a dictionary from a collection of books recommended by NYSED to represent the lexical knowledge base for children up to 4th grade.</p> <p class="Default">We performed several data mining techniques on the collected data to study error patterns. Two error patterns were discovered and both findings contradict common beliefs. First, it is a common understanding that students can better locate information than to make summary. Our finding indicates that students perform significantly better for the summary-type questions, than questions that demand the abilities to locate textual evidence and/or make inference to a various degree. A possible explanation for this finding is strong emphasis on high-level reading comprehension strategies in the intervention program, while many students are still lacking fundamental reading skills at the lexical level. Second, in all statewide assessments, a single performance score is often the only outcome measure to identify underlying learning problems. Our finding indicates that students with similar final scores can vary substantially in terms of reading skills. It is particularly true for students achieving correctness between 20% and 75%. This analysis leads to the conclusion that one single assessment score is not sufficient for a teacher to identify the learning problems of a specific student, especially for reading comprehension.</p> <p class="Default">To analyze underlying factors affecting reading comprehension for a specific age group and to be able to validate the work, we transformed the problem to a K-nearest-neighbor (KNN) matching problem. The framework allows exploration of low-level features (in terms of reading skills) and various matching mechanisms. Given the right feature selection and a robust matching mechanism, if there are multiple assessments from the same student taken few months apart, each assessment should be a considerably close match to other assessments of the same student. If k=1 and the closest assessment is indeed done by the same student, the match is considered a success, else it is a failure. In our current study, when expressing an assessment as a 3-tuple feature vector in the reading skill space by manually labeling each question with one of the three main reading skills under investigation, the average correctness is 17% when k=2, and the correctness increases to 50% when k=7 in a class of 22 students. Possible explanations for the low success rate include high subjectivity of the human-labeled reading skills, and incorrect representation of the reading skill feature space for 4<sup>th</sup> grade students in the intervention program. In addition, the assessment data also suffers from low signal-to-noise ratio. More studies should be done to investigate effective features and robust matching mechanisms. An important application of this framework is to allow an instructor to transfer the intervention from existing students to new students.&nbsp;</p> <p class="Default">For a statewide assessment, PLDs (Performance Level Descriptions) describe the expected range of skills students should demonstrate at a given performance level. Our research can complement PLDs by providing diagnostic data for each individual student. This can facilitate placing students with similar reading disabilities in the same intervention group to optimize corrective actions and make efficient use of school teaching resources. Students with improved reading abilities are better prepared for college-and-career readiness and more apt to compete in a global workforce.</p><br> <p>            Last Modified: 09/05/2018<br>      Modified by: Chia-Ling&nbsp;Tsai</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[This interdisciplinary project focused on discovering error patterns in assessment outcomes to facilitate effective teaching for reading comprehension in early elementary education. We collected ELA (English Language Arts) assessment data for 6 mock examinations over 2 years, involving a total of 365 4th grade students from 2 different schools in NY. All examination questions were labeled by a literacy expert with one of 9 performance indicators defined by NYSED (New York State Education Department), covering 3 main reading skills---making summary, making inference, and locating information. We also compiled a dictionary from a collection of books recommended by NYSED to represent the lexical knowledge base for children up to 4th grade. We performed several data mining techniques on the collected data to study error patterns. Two error patterns were discovered and both findings contradict common beliefs. First, it is a common understanding that students can better locate information than to make summary. Our finding indicates that students perform significantly better for the summary-type questions, than questions that demand the abilities to locate textual evidence and/or make inference to a various degree. A possible explanation for this finding is strong emphasis on high-level reading comprehension strategies in the intervention program, while many students are still lacking fundamental reading skills at the lexical level. Second, in all statewide assessments, a single performance score is often the only outcome measure to identify underlying learning problems. Our finding indicates that students with similar final scores can vary substantially in terms of reading skills. It is particularly true for students achieving correctness between 20% and 75%. This analysis leads to the conclusion that one single assessment score is not sufficient for a teacher to identify the learning problems of a specific student, especially for reading comprehension. To analyze underlying factors affecting reading comprehension for a specific age group and to be able to validate the work, we transformed the problem to a K-nearest-neighbor (KNN) matching problem. The framework allows exploration of low-level features (in terms of reading skills) and various matching mechanisms. Given the right feature selection and a robust matching mechanism, if there are multiple assessments from the same student taken few months apart, each assessment should be a considerably close match to other assessments of the same student. If k=1 and the closest assessment is indeed done by the same student, the match is considered a success, else it is a failure. In our current study, when expressing an assessment as a 3-tuple feature vector in the reading skill space by manually labeling each question with one of the three main reading skills under investigation, the average correctness is 17% when k=2, and the correctness increases to 50% when k=7 in a class of 22 students. Possible explanations for the low success rate include high subjectivity of the human-labeled reading skills, and incorrect representation of the reading skill feature space for 4th grade students in the intervention program. In addition, the assessment data also suffers from low signal-to-noise ratio. More studies should be done to investigate effective features and robust matching mechanisms. An important application of this framework is to allow an instructor to transfer the intervention from existing students to new students.  For a statewide assessment, PLDs (Performance Level Descriptions) describe the expected range of skills students should demonstrate at a given performance level. Our research can complement PLDs by providing diagnostic data for each individual student. This can facilitate placing students with similar reading disabilities in the same intervention group to optimize corrective actions and make efficient use of school teaching resources. Students with improved reading abilities are better prepared for college-and-career readiness and more apt to compete in a global workforce.       Last Modified: 09/05/2018       Submitted by: Chia-Ling Tsai]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
