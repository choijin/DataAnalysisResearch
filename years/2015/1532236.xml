<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI Collaborative Consortium: Acquisition of a Shared Supercomputer by the Rocky Mountain Advanced Computing Consortium</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>2030000.00</AwardTotalIntnAmount>
<AwardAmount>2030000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Stefan Robila</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A cluster supercomputer is deployed by the University of Colorado Boulder (CU-Boulder) and Colorado State University (CSU) for the Rocky Mountain Advanced Computing Consortium (RMACC). This high-performance computing (HPC) system supports multiple research groups across the Rocky Mountain region in fields including astrophysics, bioinformatics, chemistry, computational fluid dynamics, earth system science, life science, material science, physics, and social sciences with advanced computing capabilities. It also provides a platform to investigate and address the impact of many-core processors on the applications that support research in these fields. &lt;br/&gt;&lt;br/&gt;The system integrates nodes populated with Intel's conventional multicore Xeon processors and Many-Integrated-Core (MIC) 'Knights Landing' Phi processors interconnected by Intel's new Omni-Path networking technology. Users of the new HPC system have access to existing data management services including data storage, data sharing, metadata consulting, and data publishing, leveraging the NSF-funded high-performance networking infrastructure and long term storage system, as well as additional cyberinfrastructure, at CU-Boulder and CSU. The many-core feature of this HPC system enhances graduate and undergraduate students' education and training as they develop, deploy, test, and run optimized applications for next generation many-core architectures. Training for researchers and students is provided through workshops appropriate for introducing diverse audiences to the efficient and effective use of HPC systems, the challenges of vectorization for single core performance, shared memory parallelism, and issues of data management. Additionally, advanced workshops on large-scale distributed computing, high-throughput computing, and data-intensive computing are offered during the year and at the annual RMACC student-centric HPC Symposium. The Symposium brings together hundreds of students, researchers, and professionals from universities, national laboratories and industry to exchange ideas and best practices in all areas of cyberinfrastructure. For-credit HPC classes will be delivered for online participation, educating the next generation of computational scientists in state-of-the-art computational techniques.</AbstractNarration>
<MinAmdLetterDate>08/19/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1532236</AwardID>
<Investigator>
<FirstName>Anna</FirstName>
<LastName>Hasenfratz</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anna Hasenfratz</PI_FULL_NAME>
<EmailAddress>anna.hasenfratz@colorado.edu</EmailAddress>
<PI_PHON>3034926972</PI_PHON>
<NSF_ID>000320068</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Syvitski</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James P Syvitski</PI_FULL_NAME>
<EmailAddress>james.syvitski@colorado.edu</EmailAddress>
<PI_PHON>3037355482</PI_PHON>
<NSF_ID>000158743</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kenneth</FirstName>
<LastName>Jansen</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kenneth E Jansen</PI_FULL_NAME>
<EmailAddress>jansenke@colorado.edu</EmailAddress>
<PI_PHON>3034924359</PI_PHON>
<NSF_ID>000395065</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Hauser</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Hauser</PI_FULL_NAME>
<EmailAddress>thauser@ucar.edu</EmailAddress>
<PI_PHON>3034971837</PI_PHON>
<NSF_ID>000345076</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Ruprecht</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter A Ruprecht</PI_FULL_NAME>
<EmailAddress>peter.ruprecht@colorado.edu</EmailAddress>
<PI_PHON>3037353770</PI_PHON>
<NSF_ID>000668893</NSF_ID>
<StartDate>08/19/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803031058</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, Room 481]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~2030000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>An advanced cluster supercomputer is in production for researchers, staff, and students at the University of Colorado Boulder and Colorado State University. This supercomputer supports over 700 researchers in areas ranging from astrophysics, bioinformatics, chemistry, computational fluid dynamics, earth system science, life science, material science, physics, and social sciences with advanced computing capabilities. Users of this computing system have access to existing data management services including data storage, data sharing, metadata consulting, and data publishing, leveraging the NSF-funded high-performance networking infrastructure and long-term storage system at CU-Boulder and CSU. The system is utilized over 90% by the researchers from both universities. Researchers from smaller schools that are members of the Rocky Mountain Advanced Computing Consortium (RMACC) have started to use the system facilitated through the NSF funded XSEDE project.</p> <p>Training for all faculty and students interested in using the supercomputing system was provided through about 30 workshops per year reaching about 300 attendees per year. We have started organizing workshops at smaller universities without local computing resources to enable faculty and students at these schools to use advanced computing in their education and research. RMACC hosts a yearly High-Performance Computing (HPC) Symposium to bring together students, faculty, researchers, practitioners, and industry to discuss HPC technology and advancements. The RMACC Symposium also provides a platform for attendees to meet with and gain knowledge from other attendees and expand regional contacts and facilitate collaboration. As part of the yearly symposium, RMACC provides travel and registration scholarships to students and individuals from minority-serving institutions to increase participation in the HPC community and introduce new users to the services that the RMACC institutions provide.</p> <p>To broaden the participation of users of the supercomputer, we are offering two new services: containers and access through JupyterHub. Containers enable workflows to be packaged and shared across platforms, enhancing portability and reproducibility. Users of the system can now bring their workflows developed somewhere else to this supercomputer and get started on their computational work without length application porting. JupyterHub enables access to the supercomputer through a web interface that combines code, data, graphics, and text. This notebook interface significantly lowers the barrier of entry because users of the systems do not need to know the underlying system details and can get their computation executed on the supercomputer by running their code in these notebooks.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/29/2018<br>      Modified by: Thomas&nbsp;Hauser</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1532236/1532236_10389358_1543526029258_IMG_3721--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1532236/1532236_10389358_1543526029258_IMG_3721--rgov-800width.jpg" title="Racks of the deployed supercomputer"><img src="/por/images/Reports/POR/2018/1532236/1532236_10389358_1543526029258_IMG_3721--rgov-66x44.jpg" alt="Racks of the deployed supercomputer"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This images shows on of the rows of the deployed supercomputer.</div> <div class="imageCredit">Becky Yeager CU Boulder</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Thomas&nbsp;Hauser</div> <div class="imageTitle">Racks of the deployed supercomputer</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1532236/1532236_10389358_1543526171551_IMG_3752--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1532236/1532236_10389358_1543526171551_IMG_3752--rgov-800width.jpg" title="One compute rack of the deployed system"><img src="/por/images/Reports/POR/2018/1532236/1532236_10389358_1543526171551_IMG_3752--rgov-66x44.jpg" alt="One compute rack of the deployed system"></a> <div class="imageCaptionContainer"> <div class="imageCaption">One compute rack of the deployed system with the two high performance interconnect switches in the middle</div> <div class="imageCredit">Research Computing CU Boulder</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Thomas&nbsp;Hauser</div> <div class="imageTitle">One compute rack of the deployed system</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ An advanced cluster supercomputer is in production for researchers, staff, and students at the University of Colorado Boulder and Colorado State University. This supercomputer supports over 700 researchers in areas ranging from astrophysics, bioinformatics, chemistry, computational fluid dynamics, earth system science, life science, material science, physics, and social sciences with advanced computing capabilities. Users of this computing system have access to existing data management services including data storage, data sharing, metadata consulting, and data publishing, leveraging the NSF-funded high-performance networking infrastructure and long-term storage system at CU-Boulder and CSU. The system is utilized over 90% by the researchers from both universities. Researchers from smaller schools that are members of the Rocky Mountain Advanced Computing Consortium (RMACC) have started to use the system facilitated through the NSF funded XSEDE project.  Training for all faculty and students interested in using the supercomputing system was provided through about 30 workshops per year reaching about 300 attendees per year. We have started organizing workshops at smaller universities without local computing resources to enable faculty and students at these schools to use advanced computing in their education and research. RMACC hosts a yearly High-Performance Computing (HPC) Symposium to bring together students, faculty, researchers, practitioners, and industry to discuss HPC technology and advancements. The RMACC Symposium also provides a platform for attendees to meet with and gain knowledge from other attendees and expand regional contacts and facilitate collaboration. As part of the yearly symposium, RMACC provides travel and registration scholarships to students and individuals from minority-serving institutions to increase participation in the HPC community and introduce new users to the services that the RMACC institutions provide.  To broaden the participation of users of the supercomputer, we are offering two new services: containers and access through JupyterHub. Containers enable workflows to be packaged and shared across platforms, enhancing portability and reproducibility. Users of the system can now bring their workflows developed somewhere else to this supercomputer and get started on their computational work without length application porting. JupyterHub enables access to the supercomputer through a web interface that combines code, data, graphics, and text. This notebook interface significantly lowers the barrier of entry because users of the systems do not need to know the underlying system details and can get their computation executed on the supercomputer by running their code in these notebooks.          Last Modified: 11/29/2018       Submitted by: Thomas Hauser]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
