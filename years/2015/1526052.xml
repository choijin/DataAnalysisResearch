<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Small: Mashup Content Harvesting for an Open Internet</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>466913.00</AwardTotalIntnAmount>
<AwardAmount>466913</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Internet has transformed from a small network into a gigantic infrastructure of enormous scale. The amount of publicly-accessible data on the Web, hosted at millions of servers and billions of Web pages around the world, is approaching the Exabyte limit. Given the enormous size of this data, which continues to grow at a rapid pace, this project asks if it is possible to mirror content, i.e., represent it as a function of the massive data already available online. Consequently, a related question is if the entire Internet can be treated as a single Content Distribution Network. That is, can one harvest the desired content, which may not exist at any one particular location on the Internet, by downloading pieces of mirrored content from the publicly available Web services?&lt;br/&gt;&lt;br/&gt;The PI proposes mashup content harvesting as a comprehensive approach that aims to accomplish the above goals. In this approach, users have the ability and means, which the PI will develop, to create or replicate content by representing it in terms of the significant amount of data publicly available on the Web. By enabling users to effectively mirror new data as a function of existing data, it becomes feasible to achieve mass-scale content distribution, yet without hosting any new server- or peer-to-peer infrastructure. Contrary to steganography methods, which explicitly embed information by writing it to a carrier medium, the prosed system effectively mirrors data onto existing publicly-available content without any explicit writing.&lt;br/&gt;&lt;br/&gt;Intellectual Merit: The proposed research will address fundamental questions that are key to developing and deploying mashup content harvesting. The major challenges are (i) how to embed both data and hopping information via information available on publicly accessible sites, (ii) how to select a set of core data segments to maximize mirroring efficiency, (iii) how to devise and deploy methods for scalable and accurate characterization of mashup content harvesting data carriers, (iv) which fundamental Web carrier properties enable effective mashup content mirroring and harvesting, (v) what are the lower-bounds in terms of the Web page diversity and connectivity that affect system performance, (vi) how mashup content harvesting performs in closed national-level Web environments, and (vii) how mashup content harvesting performs when content carriers are limited to Web pages associated with specific world languages.&lt;br/&gt;&lt;br/&gt;Broader Impact: This project has the capacity to make a significant impact by facilitating the development of a free and open Internet. Indeed, free, i.e., non-commercial, Web services have created enormous value to numerous individuals and organizations on the Internet. Common for such services is to leverage community contributions from individuals around the world. Unfortunately, despite enthusiastic collaborative contributions, existing free Web services aren't really free. For example, Wikipedia must raise large amounts of money to support its hosting costs. Mashup content harvesting provides a feasible approach for online communities to sustain, no matter how small they may be. This is possible to achieve neither by threatening nor competing with commercial mainstream Web services, nor affecting their performance. The PI plans to design and disseminate mashup content harvesting as easy-to-use browser plug-ins.</AbstractNarration>
<MinAmdLetterDate>08/14/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1526052</AwardID>
<Investigator>
<FirstName>Aleksandar</FirstName>
<LastName>Kuzmanovic</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aleksandar Kuzmanovic</PI_FULL_NAME>
<EmailAddress>akuzma@northwestern.edu</EmailAddress>
<PI_PHON>8474675519</PI_PHON>
<NSF_ID>000491611</NSF_ID>
<StartDate>08/14/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Evanston</CityName>
<StateCode>IL</StateCode>
<ZipCode>602013149</ZipCode>
<StreetAddress><![CDATA[2145 Sheridan Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~466913</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Broader Impact:</p> <p>Over the next decade, approximately five billion people will become connected to the Internet. The biggest increase will be in societies where the Internet censorship for social, political, religious, and other reasons, is thriving. An open Internet is a tremendously important goal for several reasons. First, the freedom of information, which is a universally recognized human right, can help people who historically have been isolated, to get a chance to become engaged participants in the world community. Second, only a truly open Internet helps fuel the economy, increases productivity, and opens business and innovation opportunities around the world. While the censorship technologies are a multi-billion-dollar industry, the tools to measure and assess digital repression get only a few million dollars in government and private funding. More importantly, while detecting censorship is vital, providing systems to undermine censors, filters and throttles, is even more essential. The key contribution of this project lies in developing novel methods for enabling users to efficiently communicate in censorship environments and beyond.</p> <p>Intellectual Merit:</p> <p><em>Mashup Content Harvesting. </em>The Web has grown to immense proportions, and is home to billions of globally available documents. We designed mashup content mirroring and harvesting, a methodology that uses arbitrary webpages for implicit distribution of content over the Internet. This implicit distribution allows users to communicate at scale in a manner using only webpages that are directly available to them or their consumers. This approach relieves content creators from the burden of having to act as the provider of such content, and instead offloads the task to existing webpages. Such a methodology further eliminates the need for complex infrastructure, in the form of CDNs or peer-to-peer networks, and instead turns the entire Web into a collective CDN and peer provider. We evaluate the core tradeoffs associated with the proposed methodology and provide a theoretical proof of our system's feasibility. We further provide practical guidelines on how to set system parameters to achieve a desired performance goal. We present designs and a prototype which demonstrate mashup content harvesting's ability to publish real-world messages using publicly available webpages.</p> <p><em>Utilizing Network Complexity to Counter Censorship. </em>Domain Name System (DNS) is a core service necessary for proper functioning of the Internet. Hence, it is commonly used in censoring content. We show that DNS could, instead, be used to build an effective counter-censorship system. The key lies in significant network complexity, which is necessarily reflected in DNS traffic. We show that substantial variability in the number of domains a user accesses, the proliferation of public DNS resolvers, which blur the notion of local DNS servers, and the wide deployment of Content Distribution Networks (CDNs), lead to information-hiding opportunities within DNS. We design <em>DNS-sly</em>, a counter-censorship system which enables a covert channel between a DNS client and server. We implement DNS-sly and evaluate it in a known censorship environment, demonstrating its real-world usability.</p> <p><em>A Content Mashup beyond Censorship. </em>We explored the potential for a content mashup to improve Web performance at scale. In particular, the current Internet content delivery model assumes strict mapping between a resource and its descriptor, e.g., a JPEG &#64257;le and its URL. We built a client side mechanism to speedup webpages by replacing the slow original content with fast <em>similar</em> content. We validate our approach both in terms of availability of similar content and user perception of similar webpages. Our experiments via both automated tests and real users show that surrendering content exactness holds great potential for improving user-perceived web performance and beyond.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/03/2019<br>      Modified by: Aleksandar&nbsp;Kuzmanovic</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Broader Impact:  Over the next decade, approximately five billion people will become connected to the Internet. The biggest increase will be in societies where the Internet censorship for social, political, religious, and other reasons, is thriving. An open Internet is a tremendously important goal for several reasons. First, the freedom of information, which is a universally recognized human right, can help people who historically have been isolated, to get a chance to become engaged participants in the world community. Second, only a truly open Internet helps fuel the economy, increases productivity, and opens business and innovation opportunities around the world. While the censorship technologies are a multi-billion-dollar industry, the tools to measure and assess digital repression get only a few million dollars in government and private funding. More importantly, while detecting censorship is vital, providing systems to undermine censors, filters and throttles, is even more essential. The key contribution of this project lies in developing novel methods for enabling users to efficiently communicate in censorship environments and beyond.  Intellectual Merit:  Mashup Content Harvesting. The Web has grown to immense proportions, and is home to billions of globally available documents. We designed mashup content mirroring and harvesting, a methodology that uses arbitrary webpages for implicit distribution of content over the Internet. This implicit distribution allows users to communicate at scale in a manner using only webpages that are directly available to them or their consumers. This approach relieves content creators from the burden of having to act as the provider of such content, and instead offloads the task to existing webpages. Such a methodology further eliminates the need for complex infrastructure, in the form of CDNs or peer-to-peer networks, and instead turns the entire Web into a collective CDN and peer provider. We evaluate the core tradeoffs associated with the proposed methodology and provide a theoretical proof of our system's feasibility. We further provide practical guidelines on how to set system parameters to achieve a desired performance goal. We present designs and a prototype which demonstrate mashup content harvesting's ability to publish real-world messages using publicly available webpages.  Utilizing Network Complexity to Counter Censorship. Domain Name System (DNS) is a core service necessary for proper functioning of the Internet. Hence, it is commonly used in censoring content. We show that DNS could, instead, be used to build an effective counter-censorship system. The key lies in significant network complexity, which is necessarily reflected in DNS traffic. We show that substantial variability in the number of domains a user accesses, the proliferation of public DNS resolvers, which blur the notion of local DNS servers, and the wide deployment of Content Distribution Networks (CDNs), lead to information-hiding opportunities within DNS. We design DNS-sly, a counter-censorship system which enables a covert channel between a DNS client and server. We implement DNS-sly and evaluate it in a known censorship environment, demonstrating its real-world usability.  A Content Mashup beyond Censorship. We explored the potential for a content mashup to improve Web performance at scale. In particular, the current Internet content delivery model assumes strict mapping between a resource and its descriptor, e.g., a JPEG &#64257;le and its URL. We built a client side mechanism to speedup webpages by replacing the slow original content with fast similar content. We validate our approach both in terms of availability of similar content and user perception of similar webpages. Our experiments via both automated tests and real users show that surrendering content exactness holds great potential for improving user-perceived web performance and beyond.          Last Modified: 10/03/2019       Submitted by: Aleksandar Kuzmanovic]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
