<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps:  Capturing Field Data for Mobile Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Steven Konsek</SignBlockName>
<PO_EMAI>skonsek@nsf.gov</PO_EMAI>
<PO_PHON>7032927021</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Mobile applications are prevalent and are increasingly being used for business activities. Because the mobile user environment is not directly under the control of the companies developing such applications, it is difficult for them to observe, reproduce, investigate, and fix failures that occur in the field. Further, mobile environments can be heterogeneous and may lead to different kinds of issues in the application behavior, making it harder to investigate these issues and possible failures associated with them. Delays in addressing these failures lead to high cost in terms of customer support and loss of reputation on the application store due to bad reviews from dis-satisfied customers.  This I-Corps team proposes a novel approach that captures the runtime behavior of a mobile application in the field and makes it available to the developer.&lt;br/&gt;&lt;br/&gt;The goal from this project is to complete the customer validation to develop a proof-of-concept technique and tool for capturing field data relevant to the mobile application. The technique will collect this field data, which consists of different runtime states of the application on specific mobile environments and the sequences of actions leading to such states. This information would be reported in a visual format to allow developers to identify commonalities and differences between the runtime behavior obtained from&lt;br/&gt;different environments. Although the exact details of this technique might change based on interaction with potential customers, it has essential ingredients to support the testing and maintenance activities for the mobile application.</AbstractNarration>
<MinAmdLetterDate>02/10/2015</MinAmdLetterDate>
<MaxAmdLetterDate>02/10/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1522518</AwardID>
<Investigator>
<FirstName>Alessandro</FirstName>
<LastName>Orso</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alessandro Orso</PI_FULL_NAME>
<EmailAddress>orso@cc.gatech.edu</EmailAddress>
<PI_PHON>4043852066</PI_PHON>
<NSF_ID>000489660</NSF_ID>
<StartDate>02/10/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Ave NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Mobile applications are extremely popular and are now being used for critical business activities.&nbsp;However, since the mobile user environment is not directly under the control of the companies developing such applications, it is difficult for them to observe, reproduce, investigate, and fix failures that occur in the field. Delays in addressing these failures leads to high cost in terms of customer support and loss of reputation on the application store due to bad reviews from dissatisfied customers. The intuition behind this project was to prevent such undesirable cases with improved testing and debugging of such applications.</p> <p>The primary goal of this project was to assess the commercial viability of capturing field data for mobile applications for the purposes of improving quality assurance activities for such applications. Specifically, we aimed to assess the usefulness of application runtime data in testing and debugging scenarios. Our approach was planned to be based on customer discovery performed as a part of the project, guided with instructions received by the program instructors during the i-Corps workshops.</p> <p>Through the customer discovery interviews, we learnt that mobile developers were already using monitoring tools for collecting failures (e.g., crashes) from the field. Thus, we did not find a burning need for newer tools in this arena. However, in the process we learned that most testing was performed manually, which made the process time consuming and error-prone. There exist frameworks (such as Google's Espresso and the open source Appium toolkit) for writing automated test cases and alleviate or eliminate some of the problems with manual testing. However, QA testers still need expertise and considerable amount of time to develop test scripts using these frameworks. We therefore realized the need for a tool to help developers and testers effortlessly generate test cases in the mobile environment. &nbsp;</p> <p>In the later part of this project, we focused on this idea. We developed a technique for helping testers easily create platform independent test scripts for their app and automatically run the generated test scripts on multiple devices and operating system versions. One distinctive feature of our technique is that it allows for adding oracles to the tests in a visual and intuitive way. We implemented our technique in a freely available tool called Barista. Our preliminary evaluation of Barista shows that it can be useful and effective in practice and that it improves the state of the art.</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/13/2017<br>      Modified by: Alessandro&nbsp;Orso</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Mobile applications are extremely popular and are now being used for critical business activities. However, since the mobile user environment is not directly under the control of the companies developing such applications, it is difficult for them to observe, reproduce, investigate, and fix failures that occur in the field. Delays in addressing these failures leads to high cost in terms of customer support and loss of reputation on the application store due to bad reviews from dissatisfied customers. The intuition behind this project was to prevent such undesirable cases with improved testing and debugging of such applications.  The primary goal of this project was to assess the commercial viability of capturing field data for mobile applications for the purposes of improving quality assurance activities for such applications. Specifically, we aimed to assess the usefulness of application runtime data in testing and debugging scenarios. Our approach was planned to be based on customer discovery performed as a part of the project, guided with instructions received by the program instructors during the i-Corps workshops.  Through the customer discovery interviews, we learnt that mobile developers were already using monitoring tools for collecting failures (e.g., crashes) from the field. Thus, we did not find a burning need for newer tools in this arena. However, in the process we learned that most testing was performed manually, which made the process time consuming and error-prone. There exist frameworks (such as Google's Espresso and the open source Appium toolkit) for writing automated test cases and alleviate or eliminate some of the problems with manual testing. However, QA testers still need expertise and considerable amount of time to develop test scripts using these frameworks. We therefore realized the need for a tool to help developers and testers effortlessly generate test cases in the mobile environment.    In the later part of this project, we focused on this idea. We developed a technique for helping testers easily create platform independent test scripts for their app and automatically run the generated test scripts on multiple devices and operating system versions. One distinctive feature of our technique is that it allows for adding oracles to the tests in a visual and intuitive way. We implemented our technique in a freely available tool called Barista. Our preliminary evaluation of Barista shows that it can be useful and effective in practice and that it improves the state of the art.          Last Modified: 02/13/2017       Submitted by: Alessandro Orso]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
