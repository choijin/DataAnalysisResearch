<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Using Virtual Reality for the Dynamic, Real-Time Optimization of Human Visual Perception</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>499441.00</AwardTotalIntnAmount>
<AwardAmount>599297</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computational vision and vision science have traditionally looked to the statistics of the natural world and each other for insights into visual processing.  Until recently, these approaches have been primarily static and correlational: the natural world has been treated as a collection of images for which processing should be optimized, and the averaged regularities in natural scenes have been shown to be correlated with perceptual biases.  Any dynamic adjustment to recent experience influencing perception has often been minimized, in large part because there have not been ways to disrupt the environment and test the effects.   But recent advances in computing and virtual reality hardware have made possible the manipulation of visual input in near-real time.&lt;br/&gt;&lt;br/&gt;This research combines mobile computing technology with immersive augmented reality to explore how visual perception dynamically adapts to encountered regularities in the environment.  The PI will investigate perception of orientation, a feature of the first cortical layer of human visual processing, and thus a logical starting point.  If stimuli are encoded under a framework that uses recent environmental statistics to dynamically optimize perception, then altering the typical environmental regularities should have predictable effects on human visual performance.  The PI argues that existing computational models of human perception can be extended to predict which changes in the input will improve (or inhibit) human perceptual performance.  This, in turn, will open up the possibility of training human perception to optimize performance on real world tasks that previously required extensive specialized training or costly, custom-built software. With the goal of creating a more precise model of the flexibility of the human visual system by quantifying the extent to which encoding biases can be altered or obliterated, this project will include three interrelated thrusts.  First, the PI will develop a suite of software tools to process the visual environment in near real-time, and will use these tools to systematically investigate changes in human perception in response to experience with environments whose statistical content is atypical.  He will measure changes in human perceptual performance on a variety of real-world tasks (e.g., object detection), in response to immersive experience with atypical environmental input.  And he will develop and test a computational model of this human perceptual learning.  Preliminary research suggests that the combination of computer image-filtering and virtual reality hardware can be used to change subsequent visual processing in ways that are predictable based on the filtered input.</AbstractNarration>
<MinAmdLetterDate>12/07/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1524888</AwardID>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Essock</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward Essock</PI_FULL_NAME>
<EmailAddress>eaesso01@athena.louisville.edu</EmailAddress>
<PI_PHON>5028525955</PI_PHON>
<NSF_ID>000394458</NSF_ID>
<StartDate>12/07/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Patrick</FirstName>
<LastName>Shafto</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patrick Shafto</PI_FULL_NAME>
<EmailAddress>patrick.shafto@gmail.com</EmailAddress>
<PI_PHON/>
<NSF_ID>000504025</NSF_ID>
<StartDate>12/07/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University Newark</Name>
<CityName>Newark</CityName>
<ZipCode>071021896</ZipCode>
<PhoneNumber>9739720283</PhoneNumber>
<StreetAddress>Blumenthal Hall, Suite 206</StreetAddress>
<StreetAddress2><![CDATA[249 University Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>130029205</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University Newark]]></Name>
<CityName>Newark</CityName>
<StateCode>NJ</StateCode>
<ZipCode>071021813</ZipCode>
<StreetAddress><![CDATA[190 University Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<ProgramReference>
<Code>8551</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~599297</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Computational vision and vision science have argued that human visual perception is adapted to the statistics of the natural world. However, this research has been static and correlational: the natural world has been treated as a collection of images for which processing should be optimized, and the averaged regularities in natural scenes have been shown to correlate with perceptual biases. Any dynamic adjustment to recent experience influencing perception has often been minimized, in large part because there have not been ways to disrupt the environment and test the effects.&nbsp;</p> <p><br />We have developed experimental techniques including psychophysics, EEG, computational modeling and Modified Reality in Head Mounted Displays to understand visual perception and adaptation to the dynamics of the world.&nbsp; Our studies investigated sensitivity to narrowband and broadband visual contrast across orientation and spatial frequency and whether sensitivity can be altered based on recent experience. Thus, our studies were designed to measure human sensitivity to features that were either themselves complex (stimuli that contain multiple orientations or spatial frequencies) or embedded in complex, naturalistic visual environments.</p> <p><br />We have made advances along three lines. First, we have developed software that can modify the statistics of visual scenes in near-real-time. Thus, we have been able to develop new "modified-reality" methods that allow experimental manipulation of human perception while immersed in the real-world. Second, using this method, we have discovered that visual perception of orientation is adapted not just to the average statistics of the environment, but to the environment that has been experienced in the past 30 minutes. We have also shown dynamic adaptation of sensitivity to visual features across space. We have integrated these findings regarding adaptation in space and time. Specifically, we investigated sensitivity to stimuli in dynamic, naturalistic environments---videos changing across space and in time---discovering that concurrent spatial content influenced perception more than antecedent temporal content, which also affected perception. These basic results are supported by ongoing research that investigates the neural bases of these phenomena using Steady State Visual Evoked Potentials. Third, each of these discoveries is accompanied by a computational model that explains when and why we should see these effects, and supports further predictions that will be investigated in subsequent experiments.&nbsp;</p><br> <p>            Last Modified: 12/01/2019<br>      Modified by: Patrick&nbsp;Shafto</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Computational vision and vision science have argued that human visual perception is adapted to the statistics of the natural world. However, this research has been static and correlational: the natural world has been treated as a collection of images for which processing should be optimized, and the averaged regularities in natural scenes have been shown to correlate with perceptual biases. Any dynamic adjustment to recent experience influencing perception has often been minimized, in large part because there have not been ways to disrupt the environment and test the effects.    We have developed experimental techniques including psychophysics, EEG, computational modeling and Modified Reality in Head Mounted Displays to understand visual perception and adaptation to the dynamics of the world.  Our studies investigated sensitivity to narrowband and broadband visual contrast across orientation and spatial frequency and whether sensitivity can be altered based on recent experience. Thus, our studies were designed to measure human sensitivity to features that were either themselves complex (stimuli that contain multiple orientations or spatial frequencies) or embedded in complex, naturalistic visual environments.   We have made advances along three lines. First, we have developed software that can modify the statistics of visual scenes in near-real-time. Thus, we have been able to develop new "modified-reality" methods that allow experimental manipulation of human perception while immersed in the real-world. Second, using this method, we have discovered that visual perception of orientation is adapted not just to the average statistics of the environment, but to the environment that has been experienced in the past 30 minutes. We have also shown dynamic adaptation of sensitivity to visual features across space. We have integrated these findings regarding adaptation in space and time. Specifically, we investigated sensitivity to stimuli in dynamic, naturalistic environments---videos changing across space and in time---discovering that concurrent spatial content influenced perception more than antecedent temporal content, which also affected perception. These basic results are supported by ongoing research that investigates the neural bases of these phenomena using Steady State Visual Evoked Potentials. Third, each of these discoveries is accompanied by a computational model that explains when and why we should see these effects, and supports further predictions that will be investigated in subsequent experiments.        Last Modified: 12/01/2019       Submitted by: Patrick Shafto]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
