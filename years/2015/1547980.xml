<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Architecture Support for Advancing PGAS (ASAP)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>229983.00</AwardTotalIntnAmount>
<AwardAmount>229983</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Architectures of parallel computers and modern manycore processor chips, that contain tens and eventually hundreds of processors, are becoming quite complex for the programmers.  Efficient programming of those systems is very important to achieve high-speed of processing while reducing power.  To do so, programmers must ensure that:  1. the work in the program is broken into as many parallel activities as possible for fast processing; 2. data is located close to the processing cores that will manipulate them to avoid making the data travel long in the system thereby losing more time and wasting energy. Programming models are abstractions that provide the programmer with an easy-to-use logical view that hides the complexity of the underlying systems, while facilitating efficient programming. These are two conflicting requirements and while the current de facto programming methods can offer efficiency in the majority of the cases, they are not easy to use.  The so called, PGAS or the Partitioned Global Address Space programming model has the promise of striking a balance between efficiency and ease-of-use.  However, help is needed from the hardware particularly in simplifying and speeding up the process of finding where the data to be processed is physically located. This work is to investigate hardware solutions for this problem under the PGAS programming model.  The outcome will improve productivity of domain scientists, thereby reducing the time from conceiving an application problem till the solution is attained, which in the long run can mean more rapid discoveries and innovations, as well reduction in the cost of developing the next generation software.     &lt;br/&gt;The PIs propose to investigate a general hardware support for address translation for the PGAS programming model.  PGAS strikes a balance between the locality-aware, but explicit, message-passing model (e.g. MPI) and the easy-to-use, but locality-agnostic, shared memory model (e.g. OpenMP).  However, the PGAS rich memory model comes at a performance cost which can hinder its potential for scalability and performance. Current implementations can be orders of magnitude slower in accessing local shared space as compared to accessing their private space.  Compiler optimizations only handle special cases and hand-tuning renders the PGAS ease-of-use advantage worthless.  The proposed hardware solution can facilitate high-performance execution for out-of-the-box (i.e. non-hand-tuned) PGAS applications.   The PIs are creating PAGS memory model translation architectural support, which can navigate the PGAS memory model converting PGAS shared  references to system's virtual addresses efficiently on-the-fly. This eliminates the need for hand-tuning, while maintaining the performance and productivity of PGAS languages. The hardware support will be available to the compiler through instruction set extensions.  A tool set integrating and adapting existing micro-architecture simulators with compiler and a run-time system will be used as the main testbed and distributed over a cluster for extensive experimentation.  At the end of the project the PIs expect to  release tools and the benchmarks utilized under this project.</AbstractNarration>
<MinAmdLetterDate>08/05/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1547980</AwardID>
<Investigator>
<FirstName>Tarek</FirstName>
<LastName>El-Ghazawi</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tarek A El-Ghazawi</PI_FULL_NAME>
<EmailAddress>tarek@gwu.edu</EmailAddress>
<PI_PHON>2029942607</PI_PHON>
<NSF_ID>000239088</NSF_ID>
<StartDate>08/05/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043990498</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520066</ZipCode>
<StreetAddress><![CDATA[800 22 ST NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~229983</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Although the PGAS shared memory programming model offers significant productivity improvements for the parallel programmer, the model&rsquo;s performance can degrade due to the overhead associated with accessing and traversing the shared memory space. Automatic compiler optimizations may mitigate this overhead, but they are often insufficient to provide competitive performance, especially for complex codes, and they are likely to introduce costly software operations including additional memory accesses.</p> <p>This project focused on a major impediment to the widespread adoption of PGAS languages: supporting the user-friendly memory model of the PGAS abstraction, which is associated with heavy overhead. The core idea was to come up with architectural innovations that can be leveraged by compilers via extensions to the instruction set that define pertinent lightweight operations. This way, application developers can get high performance for their PGAS programs out-of-the-box, without having to worry about non-productive and laborious hand optimizations of their code.</p> <p>We designed a novel processor hardware extension that can provide a transparent mechanism to reduce or eliminate the PGAS shared memory access overhead without creating an additional burden on the programmer. Using two prototype compilers (modified Berkeley UPC compiler and modified Cray Chapel compiler), we demonstrated that the proposed instruction set extension is easily exploitable by compilers. It is important to note that the performance improvement of using the hardware instructions can surpass that of manually optimized code for two reasons. First, programmers have a tendency to focus manual optimization efforts on the shared pointers accessed from inner loops. Second, it is not always possible to manually optimize all shared pointers in the code (e.g., due to complex or random access patterns). Experimental validation was conducted with both FPGA hardware prototypes and the gem5 full-system architecture simulator using seven major kernels. These included the five kernels of the well-accepted NAS Parallel Benchmark suite, matrix multiplication, and a 2D Sobel edge detection benchmark. The results were consistently comparable to manually optimized code without the programmer overhead of hand tuning. Unmodified code compiled with our prototype compilers (using the proposed hardware support) achieved up to a 5.5X performance gain over the same code compiled with full compiler optimizations running without our hardware support. In addition, implementation of the new hardware functionality was shown to require a minimal increase in the overall chip area usage.</p> <p>&nbsp;</p> <p>In sum, following contributions are made as part of this project</p> <p>&nbsp;-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Designed a novel architecture that reduce the performance overheads caused by PGAS abstractions while fully maintaining programmability,</p> <p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Created an architecture support co-design tool that integrates PGAS compilers, runtime systems and architectures to support end-to-end investigations</p> <p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prototyped the proposed hardware implementation using FPGAs and simulated using industry standard full-system simulator gem5,</p> <p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Adapted compilers of two PGAS languages UPC and Chapel to leverage the hardware via ISA extensions</p> <p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Thoroughly analyzed performance and cost using a variety of benchmarks including the NAS Parallel Benchmarks showing several folds of improvements</p> <p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Developed and published a software tool to leverage HPC resources to run multiple hardware simulations efficiently.</p><br> <p>            Last Modified: 11/21/2017<br>      Modified by: Tarek&nbsp;A&nbsp;El-Ghazawi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Although the PGAS shared memory programming model offers significant productivity improvements for the parallel programmer, the model?s performance can degrade due to the overhead associated with accessing and traversing the shared memory space. Automatic compiler optimizations may mitigate this overhead, but they are often insufficient to provide competitive performance, especially for complex codes, and they are likely to introduce costly software operations including additional memory accesses.  This project focused on a major impediment to the widespread adoption of PGAS languages: supporting the user-friendly memory model of the PGAS abstraction, which is associated with heavy overhead. The core idea was to come up with architectural innovations that can be leveraged by compilers via extensions to the instruction set that define pertinent lightweight operations. This way, application developers can get high performance for their PGAS programs out-of-the-box, without having to worry about non-productive and laborious hand optimizations of their code.  We designed a novel processor hardware extension that can provide a transparent mechanism to reduce or eliminate the PGAS shared memory access overhead without creating an additional burden on the programmer. Using two prototype compilers (modified Berkeley UPC compiler and modified Cray Chapel compiler), we demonstrated that the proposed instruction set extension is easily exploitable by compilers. It is important to note that the performance improvement of using the hardware instructions can surpass that of manually optimized code for two reasons. First, programmers have a tendency to focus manual optimization efforts on the shared pointers accessed from inner loops. Second, it is not always possible to manually optimize all shared pointers in the code (e.g., due to complex or random access patterns). Experimental validation was conducted with both FPGA hardware prototypes and the gem5 full-system architecture simulator using seven major kernels. These included the five kernels of the well-accepted NAS Parallel Benchmark suite, matrix multiplication, and a 2D Sobel edge detection benchmark. The results were consistently comparable to manually optimized code without the programmer overhead of hand tuning. Unmodified code compiled with our prototype compilers (using the proposed hardware support) achieved up to a 5.5X performance gain over the same code compiled with full compiler optimizations running without our hardware support. In addition, implementation of the new hardware functionality was shown to require a minimal increase in the overall chip area usage.     In sum, following contributions are made as part of this project   -          Designed a novel architecture that reduce the performance overheads caused by PGAS abstractions while fully maintaining programmability,  -          Created an architecture support co-design tool that integrates PGAS compilers, runtime systems and architectures to support end-to-end investigations  -          Prototyped the proposed hardware implementation using FPGAs and simulated using industry standard full-system simulator gem5,  -          Adapted compilers of two PGAS languages UPC and Chapel to leverage the hardware via ISA extensions  -          Thoroughly analyzed performance and cost using a variety of benchmarks including the NAS Parallel Benchmarks showing several folds of improvements  -          Developed and published a software tool to leverage HPC resources to run multiple hardware simulations efficiently.       Last Modified: 11/21/2017       Submitted by: Tarek A El-Ghazawi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
