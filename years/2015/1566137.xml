<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: AF: Breaking Barriers for Geometric Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2016</AwardEffectiveDate>
<AwardExpirationDate>10/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>161277.00</AwardTotalIntnAmount>
<AwardAmount>161277</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Joseph Maurice Rojas</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>It is surprising how often geometric abstractions help us deal with understanding large systems:  molecules become balls and sticks, complex fluid or combustion simulations are shown as contours or isosurfaces, and movies become points in a high dimensional space to allow recommendations based on which other points are near one's favorite movies.  Computational Geometry, which develops efficient computer algorithms for problems stated in geometric terms, can thus play a central role in data analytics.  Traditionally, the focus in Computational Geometry was on exact algorithms with guaranteed performance on all possible inputs, including worst-case inputs. &lt;br/&gt;&lt;br/&gt;This project recognizes that many practical data analysis tasks do not generate worst-case instances, and seeks to identify structural aspects of given problems that allow existing or new algorithms with better guarantees than the worst-case bounds for realistic cases, often using approximation, probabilistic analysis, parameterized complexity, or output sensitivity.&lt;br/&gt;&lt;br/&gt;Understanding the huge volume of data from a combustion simulation run on a super computer gives a 3d example: Contour trees, a data structure used to summarize interactions between density or temperature isosurfaces in a simulation, take more than linear time to compute in the worst case, but by parameterizing on tree shape one can show that trees that are balanced can be computed in linear time.  Machine learning and clustering problems, like recommendation systems, give higher-dimensional examples in which one desires to extract a smaller and lower dimensional representation of the input, while preserving some feature of interest.  A geometric form of this problem is known as extracting a coreset; in the worst case the coreset size can be exponential in the dimension.  On real inputs however, there is often hidden low dimensional structure; rather than designing an algorithm whose running time depends on the worst case coreset size, the running time should adapt to the size required by the given instance.&lt;br/&gt;&lt;br/&gt;Advancing non-worst-case analysis techniques helps bridge the gap between theory and practice, as there is often a disconnect between running times predicted by worst-case analysis and those seen on real data sets. The investigator will incorporate non-worst-case analysis techniques into his course curricula, as such techniques are essential yet severely lacking in standard algorithms courses. This project will also be used to support student research at the graduate as well as undergraduate levels on this topic.</AbstractNarration>
<MinAmdLetterDate>04/12/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/12/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1566137</AwardID>
<Investigator>
<FirstName>Benjamin</FirstName>
<LastName>Raichel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Benjamin Raichel</PI_FULL_NAME>
<EmailAddress>Benjamin.Raichel@utdallas.edu</EmailAddress>
<PI_PHON>9728832313</PI_PHON>
<NSF_ID>000704745</NSF_ID>
<StartDate>04/12/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Dallas</Name>
<CityName>Richardson</CityName>
<ZipCode>750803021</ZipCode>
<PhoneNumber>9728832313</PhoneNumber>
<StreetAddress>800 W. Campbell Rd., AD15</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX32</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>800188161</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT DALLAS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Dallas]]></Name>
<CityName>Richardson</CityName>
<StateCode>TX</StateCode>
<ZipCode>750803021</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX32</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026y</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>7796</Code>
<Text>ALGORITHMIC FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7929</Code>
<Text>COMPUTATIONAL GEOMETRY</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~161277</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project investigated and pushed the boundaries for several core geometric problems. As data sets are routinely and naturally described geometrically as point sets, the applications of these problems range from low dimensional data sets arising from the physical world to high dimensional data sets that arise in application areas such as machine learning.<br /><br />Many of the problems considered were seemingly stuck at a high complexity bound, as previously the focus had been on exact algorithms whose quality was judged based on worst-case inputs. Instead by taking into account a broader picture of the complexity landscape, we were able to push beyond worst-case analysis bounds, using tools such fixed parameter tractable analysis, input sensitivity, and probabilistic analysis. Specific topics studied included Voronoi diagrams, metric embeddings, coresets, and more. <br /><br />Given a set of input point sites, a Voronoi diagram partitions space into regions, where each region is the set of all points with the same nearest site. Here we gave bounds on the expected structural complexity of such diagrams when the existence, position, or weight of each given site is in some way randomized. In particular, we demonstrated that while the worst case complexity of such structures may be high, their expected complexity can be far lower.<br /><br />Metric embeddings seek to take point sets described in some complex or high dimensional space and map them into simpler or lower dimensional spaces, where they are easier to work with. The aim is to do so while preserving the distances between points as best as possible, often using a measure called distortion. In this project, we gave fixed parameter tractable algorithms for minimum distortion embeddings into trees, by parameterizing on the distortion, doubling dimension, and spread. <br /><br />Coresets are small subsets of the input points which approximately preserve some geometric property of the full set. For certain problems worst case point sets may require large coresets. Thus for this project we considered coresets whose size is measured relative to the optimum coreset size for a given point set. Several variants were considered and the results were applied to both machine learning and database problems, resulting in publications in top conferences in each area.<br /><br />This project resulted in a number of publications in top ranked venues. Two graduate students were supported by this project, and the research they carried out for the project contributed to the advancement of their research careers. One of the students recently obtained his PhD and is currently a postdoctoral researcher and the other is on track to graduate next year.</p><br> <p>            Last Modified: 02/27/2020<br>      Modified by: Benjamin&nbsp;Raichel</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project investigated and pushed the boundaries for several core geometric problems. As data sets are routinely and naturally described geometrically as point sets, the applications of these problems range from low dimensional data sets arising from the physical world to high dimensional data sets that arise in application areas such as machine learning.  Many of the problems considered were seemingly stuck at a high complexity bound, as previously the focus had been on exact algorithms whose quality was judged based on worst-case inputs. Instead by taking into account a broader picture of the complexity landscape, we were able to push beyond worst-case analysis bounds, using tools such fixed parameter tractable analysis, input sensitivity, and probabilistic analysis. Specific topics studied included Voronoi diagrams, metric embeddings, coresets, and more.   Given a set of input point sites, a Voronoi diagram partitions space into regions, where each region is the set of all points with the same nearest site. Here we gave bounds on the expected structural complexity of such diagrams when the existence, position, or weight of each given site is in some way randomized. In particular, we demonstrated that while the worst case complexity of such structures may be high, their expected complexity can be far lower.  Metric embeddings seek to take point sets described in some complex or high dimensional space and map them into simpler or lower dimensional spaces, where they are easier to work with. The aim is to do so while preserving the distances between points as best as possible, often using a measure called distortion. In this project, we gave fixed parameter tractable algorithms for minimum distortion embeddings into trees, by parameterizing on the distortion, doubling dimension, and spread.   Coresets are small subsets of the input points which approximately preserve some geometric property of the full set. For certain problems worst case point sets may require large coresets. Thus for this project we considered coresets whose size is measured relative to the optimum coreset size for a given point set. Several variants were considered and the results were applied to both machine learning and database problems, resulting in publications in top conferences in each area.  This project resulted in a number of publications in top ranked venues. Two graduate students were supported by this project, and the research they carried out for the project contributed to the advancement of their research careers. One of the students recently obtained his PhD and is currently a postdoctoral researcher and the other is on track to graduate next year.       Last Modified: 02/27/2020       Submitted by: Benjamin Raichel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
