<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Collaborative Research: F: IA: Statistical Learning for Big Data with Random Projections</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>161155.00</AwardTotalIntnAmount>
<AwardAmount>161155</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Victor Roytburd</SignBlockName>
<PO_EMAI>vroytbur@nsf.gov</PO_EMAI>
<PO_PHON>7032928584</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Contemporary data-driven science and engineering problems require the development of statistical methods that do not compromise statistical accuracy, yet are computationally feasible.  Data quality, particularly the heterogeneity in data measurements, is a critical factor that affects statistical accuracy in the analysis of large datasets.  This project will explore and demonstrate the impact and feasibility of improving computational and statistical performances simultaneously for Big Data problems with massive datasets.  The research will advance the state of knowledge in predictive statistical learning with Big Data, and be extremely valuable in applications related to financial risk management or commercial operations employing recommender systems, biology, and image analysis. &lt;br/&gt;&lt;br/&gt; A key phenomenon motivating this project is the notion that some refined ensemble methods combined with random projections can simultaneously enable the fast analysis of massive data while enhancing statistical performance.  Specifically, the aims of the project are: (1) Develop new classification methods based on random projections and the random forest.   By defining appropriate projections, the proposed method is shown to improve statistical accuracy for massive datasets with a large number of irrelevant noisy measurements.   The theoretical properties of this method will be analyzed, and an adaptive version of the algorithm developed to optimize the computational and statistical efficiency gains; (2) Propose boosting algorithms with random projections.  The statistical properties, practical performance, and implementation of the proposed random projected boosting algorithms will be investigated; (3) Develop classification methods with heterogeneities.  A classification method that involves the weighted bootstrap and ensemble learning to handle heterogeneity or covariate shifts in measurements in large datasets will be developed.  The random projection method will be applied to improve the proposed method for high-dimensional datasets.</AbstractNarration>
<MinAmdLetterDate>09/01/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/01/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1546087</AwardID>
<Investigator>
<FirstName>Cheng Yong</FirstName>
<LastName>Tang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cheng Yong Tang</PI_FULL_NAME>
<EmailAddress>yongtang@temple.edu</EmailAddress>
<PI_PHON>2152043191</PI_PHON>
<NSF_ID>000632548</NSF_ID>
<StartDate>09/01/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Temple University</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191226003</ZipCode>
<PhoneNumber>2157077547</PhoneNumber>
<StreetAddress>1801 N. Broad Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>057123192</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>057123192</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Temple University]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191226083</ZipCode>
<StreetAddress><![CDATA[1810 North 13th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~161155</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In recent data-enabled investigations, high-dimensional statistical learning problems are common where the number of explanatory variables is huge. Random projection is a simple and computationally efficient dimension reduction technique and has received increasing interests. Meanwhile, there is recent development on using random projects in building various ensemble methods. Boosting, on the other hand, is a powerful device for building ensemble learning methods, and is meritorious in achieving ideal bias-variance trade-off.</p> <p>We investigate a hierarchical framework for constructing ensemble statistical learning methods. Our building blocks are the two effective devices -- random projection and boosting. Our objective is to combine their strengths -- to effectively reduce the huge data dimensionality, and to most adequately exploit the explanatory power from the projected variables. In particular, we propose to apply boosting on the randomly projected variables when constructing the so-called "weak learners". Then an ensembling is developed by combining the weak learners from multiple random projections. We investigate algorithms for implementing the hierarchical ensemble learning. For both the boosting and ensemling steps, there are plenty of choices providing opportunity for enhancing the learning effects. We show that our hierarchical framework achieves promising predictive performance with reasonable computational costs for solving large scale data-enabled problems.</p> <p>Our framework has merits from a few aspects. First, our method attempts to combine outcomes from multiple and potentially many random projections, sharing the same principle of existing ensemble random projection methods. Second, our approach attempts to best exploit the information from a particular random projection by applying the boosting technique. Since the truth is unknown, and the learning effect may be affected by nonlinearity and other difficulties, using boosting techniques is expected to better discover the contribution from the projected variables. The computation cost of the boosting step is relatively low with the low-dimensional projected variables, additional to the fact that boosting typically relies on simple learning algorithms. In return, thanks to the properties of the boosting, information from the data via the random projections is better retained in the resulting outcomes. This feature is particularly appealing when conventional methods and models such as the linear regression/classification may not be adequate to reflect reality of the data. Third, our method has the potential of achieving good balance between the performance and computing cost. Indeed, we observe in our numerical studies that our method provides better performance in terms of testing errors with less computing time compared with existing methods.</p> <p>This project has broad impact on knowledge transferring, student training, integrating research and teaching. Current statistical learning methods have been introduced via graduate courses. PhD students are involved in the research activities for developing new theory and method in preparing their dissertations. During the process, the students have acquired precious experience and techniques.</p><br> <p>            Last Modified: 11/24/2019<br>      Modified by: Cheng Yong&nbsp;Tang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In recent data-enabled investigations, high-dimensional statistical learning problems are common where the number of explanatory variables is huge. Random projection is a simple and computationally efficient dimension reduction technique and has received increasing interests. Meanwhile, there is recent development on using random projects in building various ensemble methods. Boosting, on the other hand, is a powerful device for building ensemble learning methods, and is meritorious in achieving ideal bias-variance trade-off.  We investigate a hierarchical framework for constructing ensemble statistical learning methods. Our building blocks are the two effective devices -- random projection and boosting. Our objective is to combine their strengths -- to effectively reduce the huge data dimensionality, and to most adequately exploit the explanatory power from the projected variables. In particular, we propose to apply boosting on the randomly projected variables when constructing the so-called "weak learners". Then an ensembling is developed by combining the weak learners from multiple random projections. We investigate algorithms for implementing the hierarchical ensemble learning. For both the boosting and ensemling steps, there are plenty of choices providing opportunity for enhancing the learning effects. We show that our hierarchical framework achieves promising predictive performance with reasonable computational costs for solving large scale data-enabled problems.  Our framework has merits from a few aspects. First, our method attempts to combine outcomes from multiple and potentially many random projections, sharing the same principle of existing ensemble random projection methods. Second, our approach attempts to best exploit the information from a particular random projection by applying the boosting technique. Since the truth is unknown, and the learning effect may be affected by nonlinearity and other difficulties, using boosting techniques is expected to better discover the contribution from the projected variables. The computation cost of the boosting step is relatively low with the low-dimensional projected variables, additional to the fact that boosting typically relies on simple learning algorithms. In return, thanks to the properties of the boosting, information from the data via the random projections is better retained in the resulting outcomes. This feature is particularly appealing when conventional methods and models such as the linear regression/classification may not be adequate to reflect reality of the data. Third, our method has the potential of achieving good balance between the performance and computing cost. Indeed, we observe in our numerical studies that our method provides better performance in terms of testing errors with less computing time compared with existing methods.  This project has broad impact on knowledge transferring, student training, integrating research and teaching. Current statistical learning methods have been introduced via graduate courses. PhD students are involved in the research activities for developing new theory and method in preparing their dissertations. During the process, the students have acquired precious experience and techniques.       Last Modified: 11/24/2019       Submitted by: Cheng Yong Tang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
