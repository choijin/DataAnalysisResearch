<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Functional Imitation of Observed Tasks by Co-Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>139968.00</AwardTotalIntnAmount>
<AwardAmount>139968</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Reid Simmons</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Assistive and service robots have made significant strides and have the potential to be transformative in many fields, including health care, aging, disability management, and work in dangerous environments. For this, however, it is important that these robots can be "programmed" and used by largely untrained users, including caregivers or elderly persons. This project aims to develop a novel approach to allow robots of widely varying designs to perform assistive and supportive tasks by observing demonstrations performed by a human. Rather than copying movement, which would require that the robot resembles the human, the proposed approach uses these demonstrations to "infer" the important aspects of the task and translate them into a strategy that can be executed by the robot and in varying situations and settings.&lt;br/&gt;&lt;br/&gt;The proposed approach treats imitation not as copying of observed movements but rather as learning to replicate the function of the demonstration. For this, it transforms observations into a hierarchical Markov task model using learned models of observed environmental dynamics. This probabilistic task model is then mapped onto a hierarchical Semi-Markov Decision model of the robot's behavioral capabilities using an adaptive similarity function that represents the correspondence between attributes in the two models as well as the importance of particular attributes for successful task performance. The cost function is adapted during imitation using Reinforcement Learning and qualitative feedback from the user, allowing the system to improve and personalize its imitation capabilities. This project develops a proof-of-concept system and evaluates it on a wheeled mobile manipulator in the context of common household tasks.</AbstractNarration>
<MinAmdLetterDate>08/11/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/11/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1551312</AwardID>
<Investigator>
<FirstName>Farhad</FirstName>
<LastName>Kamangar</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Farhad A Kamangar</PI_FULL_NAME>
<EmailAddress>kamangar@uta.edu</EmailAddress>
<PI_PHON>8172723617</PI_PHON>
<NSF_ID>000366548</NSF_ID>
<StartDate>08/11/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Manfred</FirstName>
<LastName>Huber</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Manfred Huber</PI_FULL_NAME>
<EmailAddress>huber@cse.uta.edu</EmailAddress>
<PI_PHON>8172722345</PI_PHON>
<NSF_ID>000104021</NSF_ID>
<StartDate>08/11/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gergely</FirstName>
<LastName>Zaruba</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gergely V Zaruba</PI_FULL_NAME>
<EmailAddress>zaruba@uta.edu</EmailAddress>
<PI_PHON>8172723602</PI_PHON>
<NSF_ID>000491762</NSF_ID>
<StartDate>08/11/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Arlington</Name>
<CityName>Arlington</CityName>
<ZipCode>760190145</ZipCode>
<PhoneNumber>8172722105</PhoneNumber>
<StreetAddress>701 S Nedderman Dr, Box 19145</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>064234610</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT ARLINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Arlington]]></Name>
<CityName>Arlington</CityName>
<StateCode>TX</StateCode>
<ZipCode>760190015</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~139968</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Allowing persons to program robots by demonstrating the performance of tasks has significant potential, for example by permitting caregivers to demonstrate in-home care tasks that can subseuqently be performed by a robot system.</p> <p>This project ivestigated the development of a system that leanrs how to efficiently imitate demonstrations observed by watching a human interact with objects int he world. To do this, algorithms were developed that permit the system to interpret the observed actions and interactions of the human as behavior sequences and then map these onto its own capabilities in order to achieve the sam outcomes despite the differneces in capabilities between the robot platform and the human demonstrator. The core of the developed proof-of-concept system is a machine learning techniques that determines how to best map the observed actions of the human onto the action capabilities of the robot using only simple feedback provided by the human which indicates wheter an imitation attempt meets the intended requirements. The goal of this learning technique is to permit the robot system to determine which aspects of a task performance are important in the context of the demonstration (e.g. that medication is retrieved from the medicine cabinet) and which ones are irrelevant and can thus be geenralized (e.g. the precise path that the robot follows to get from the medicine cabinet to the inhabitant whom the system asissts).</p> <p>The developed proof-of concept system showed the feasiblity of this approach and demonstrated that it can learn a observation to robot action mapping that can generalize across multiple tasks, thus permitting the system to imitate new tasks after a very small number of demonstrations. Scaled to more complex domains, these technologies promise the potential to bring in-home service robots that are flexibly usable by persons without significant prior technical training closer to reality.</p> <p>While developing the proof-of-concept system, the project also developed a number of machine learningh and computer planning technologies that have promise in other contexts such as classification and recognition tasks as well as advanced planning to find solutions for complex tasks that reuqire long sequences of actions to be solved.</p> <p>In addition to the technical results, the project also provided a number of students with unque educational opportunities, leading to a completed MS thesis and contributing significantly to the work of multiple additional doctoral and masters level students.</p><br> <p>            Last Modified: 11/30/2017<br>      Modified by: Manfred&nbsp;Huber</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1551312/1551312_10386480_1512101652123_Pioneer--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1551312/1551312_10386480_1512101652123_Pioneer--rgov-800width.jpg" title="Pioneer Robot Platform"><img src="/por/images/Reports/POR/2017/1551312/1551312_10386480_1512101652123_Pioneer--rgov-66x44.jpg" alt="Pioneer Robot Platform"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Robot platform developed for initial proof-of-concept experimentation</div> <div class="imageCredit">UTA LEARN Lab</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Manfred&nbsp;Huber</div> <div class="imageTitle">Pioneer Robot Platform</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Allowing persons to program robots by demonstrating the performance of tasks has significant potential, for example by permitting caregivers to demonstrate in-home care tasks that can subseuqently be performed by a robot system.  This project ivestigated the development of a system that leanrs how to efficiently imitate demonstrations observed by watching a human interact with objects int he world. To do this, algorithms were developed that permit the system to interpret the observed actions and interactions of the human as behavior sequences and then map these onto its own capabilities in order to achieve the sam outcomes despite the differneces in capabilities between the robot platform and the human demonstrator. The core of the developed proof-of-concept system is a machine learning techniques that determines how to best map the observed actions of the human onto the action capabilities of the robot using only simple feedback provided by the human which indicates wheter an imitation attempt meets the intended requirements. The goal of this learning technique is to permit the robot system to determine which aspects of a task performance are important in the context of the demonstration (e.g. that medication is retrieved from the medicine cabinet) and which ones are irrelevant and can thus be geenralized (e.g. the precise path that the robot follows to get from the medicine cabinet to the inhabitant whom the system asissts).  The developed proof-of concept system showed the feasiblity of this approach and demonstrated that it can learn a observation to robot action mapping that can generalize across multiple tasks, thus permitting the system to imitate new tasks after a very small number of demonstrations. Scaled to more complex domains, these technologies promise the potential to bring in-home service robots that are flexibly usable by persons without significant prior technical training closer to reality.  While developing the proof-of-concept system, the project also developed a number of machine learningh and computer planning technologies that have promise in other contexts such as classification and recognition tasks as well as advanced planning to find solutions for complex tasks that reuqire long sequences of actions to be solved.  In addition to the technical results, the project also provided a number of students with unque educational opportunities, leading to a completed MS thesis and contributing significantly to the work of multiple additional doctoral and masters level students.       Last Modified: 11/30/2017       Submitted by: Manfred Huber]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
