<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CI-NEW: Collaborative Research: Computer System Failure Data Repository to Enable Data-Driven Dependability</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>763331.00</AwardTotalIntnAmount>
<AwardAmount>779331</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Dependability has become a necessary requisite property for many of the computer systems that surround us or work behind the scenes to support our personal and professional lives. Heroic progress has been made by computer systems researchers and practitioners working together to build and deploy dependable systems. However, an overwhelming majority of this work is not based on real publicly available failure data. Unfortunately, an open failure data repository for any recent computing infrastructure that is large enough, diverse enough and with enough information about the infrastructure and the applications that run on them does not exist. &lt;br/&gt;&lt;br/&gt;This project will address this pressing need. The research team appreciates that this effort is challenging on many levels. Failure data are considered sensitive and are usually unveiled only before trusting eyes of a small subset of the people at the organization. As part of a current one-year planning grant, this team has collected specific requirements for the repository from a wide audience, collected failure and usage data from the largest centrally managed computing cluster at Purdue and performed preliminary analysis to reveal the workload usage patterns. The goal of this full-scale project is to collect data from a variety of computational infrastructure at the two participating universities, and from several of the NSF-funded large cyberinfrastructure projects.&lt;br/&gt;&lt;br/&gt;The project will collect, curate, and present public failure data of large-scale computing systems in a repository called FRESCO. The data sets will include static information, dynamic information about the workloads, and failure information for both planned and unplanned outages. The data collection from production machines will have to obey several practical constraints -- no changes to the workload, little performance perturbation, and minimal changes to the operating system. Further, the data have to be sanitized for removing sensitive information and processed to make it interpretable by a broad group of researchers. This project will also provide analysis tools to answer certain commonly occurring questions, such as the correlation between workload and failure and the performance implications of using one library over another, as well as an intuitive graphical front-end which will allow people to explore the data sets and download the relevant ones.&lt;br/&gt;&lt;br/&gt;Widespread use of the data and the associated analysis tools will give computer systems researchers an unprecedented ability to do data-driven research and offer computing infrastructure providers an analytic-driven capability to run more efficient reliable infrastructures.</AbstractNarration>
<MinAmdLetterDate>06/29/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/02/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1513197</AwardID>
<Investigator>
<FirstName>Xiaohui Carol</FirstName>
<LastName>Song</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaohui Carol Song</PI_FULL_NAME>
<EmailAddress>cxsong@purdue.edu</EmailAddress>
<PI_PHON>7654967467</PI_PHON>
<NSF_ID>000298986</NSF_ID>
<StartDate>06/29/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Saurabh</FirstName>
<LastName>Bagchi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Saurabh Bagchi</PI_FULL_NAME>
<EmailAddress>sbagchi@purdue.edu</EmailAddress>
<PI_PHON>7654941741</PI_PHON>
<NSF_ID>000309372</NSF_ID>
<StartDate>06/29/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072035</ZipCode>
<StreetAddress><![CDATA[465 Northwestern Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~763331</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Large-scale high performance computing (HPC) systems have become common in academic, industrial, and government for compute intensive applications, including large-scale parallel applications. These HPC systems solve problems that would take millennia on personal computers, but managing such large shared resources can be challenging and requires administrators to balance requirements from a diverse set of users. Large, focused organizations can afford to buy centralized resources, and choose to manage and operate it at academic organizations through a central IT organization. These are funded by federal funding agencies (like the National Science Foundation in the US) and individual researchers write grant proposals to get access to compute time on these systems. Examples of such systems include Comet at the University of California San Diego, BlueWaters at the University of Illinois at Urbana-Champaign, and Frontera at the University of Texas at Austin.</p> <p>&nbsp;</p> <p>Progress in building dependable systems can be made faster if researchers working on dependability challenges can be exposed to problems through quantitative data. Theories in the labs and small demonstrations in prototypes can be transitioned to the demanding realities of large computer systems if they could validate their inventions with real system failure and attack data. There is an astonishing lack of such publicly available data for researchers and as a result many productive avenues of work in dependable system building are lying hidden. A comparison may fruitfully be drawn to the widespread use of benchmarks and reference data sets in performance analysis of computer systems, such as, those put out by SPEC or TPC. We therefore proposed to solve this problem by collecting, cleaning, annotating, and presenting data from the production compute infrastructures at several public universities.&nbsp;&nbsp;</p> <div> <p>&nbsp;</p> <p>We were not Pollyannaish in our effort and appreciated that this effort is challenging, both due to technological and psychological reasons. The technological reason referred to the need to keep the production infrastructure relatively undisturbed as the monitoring and the data collection happens. The psychological reason referred to the fact that such data is considered sensitive by many, to be unveiled only before trusting eyes of a subset of the people at the organization. We mitigated the two factors in our project. The technological factor was mitigated by carefully deciding which monitoring tool to use, when that should be activated, and how to store the data, both online and offline. We sidestepped the psychological factor by focusing on computational infrastructure at public universities (including ours), rather than private commercial organizations, and then working closely with the production IT unit to enable the data collection and data understanding.</p> <p><strong>Intellectual Merit:</strong></p> <p>We performed 4 different categories of analysis on production compute data collected from central computing clusters at three large public universities&frac34;Purdue University, University of Texas at Austin, and University of Illinois at Urbana-Champaign. In the first, we showed the breakdown of node failures into different categories and reasoned about the corresponding up times and recovery times. In the second, we considered the failures of individual jobs and reasoned about their root cause through examination of their exit codes. The third analysis sheds light on the relation between resource usage and job failure rates. We consider the 5 primary kinds of resources, local and remote, memory on a node, local IO, remote IO to the parallel file system, network, and runtime of a job. For the runtime, we considered both the spatial component (i.e., number of nodes) and the temporal component (i.e., the execution time on each node). Finally, we developed a job failure prediction model which can help minimize resource wastage corresponding to job failures due to system related issues.</p> <p><strong>Broader Impacts: </strong></p> </div> <p>The project has created a large public workload and failure data repository from production computing clusters in a university setting, Fresco (for Purdue and UT Austin) and Monet (University of Illinois at Urbana-Champaign). Widespread use of the data and the associated analysis tools will give computer systems researchers an unprecedented ability to do data-driven research and computing infrastructure providers an analytics capability for running the infrastructures more efficiently and more reliably. University researchers will benefit from more available centralized computing clusters. Broad societal impact will result from the development of more efficient and more reliable large-scale computing clusters that can run societal critical applications reliably and efficiently and at large scales.</p><br> <p>            Last Modified: 10/26/2019<br>      Modified by: Saurabh&nbsp;Bagchi</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572096881358_workload_characteristics--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572096881358_workload_characteristics--rgov-800width.jpg" title="Workload Characteristics"><img src="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572096881358_workload_characteristics--rgov-66x44.jpg" alt="Workload Characteristics"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Characteristics of workloads analyzed from two university computing clusters</div> <div class="imageCredit">Purdue, Illinois</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">Workload Characteristics</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572097000892_ml_failure_prediction--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572097000892_ml_failure_prediction--rgov-800width.jpg" title="ML failure prediction"><img src="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572097000892_ml_failure_prediction--rgov-66x44.jpg" alt="ML failure prediction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">ML model for predicting impendingfailure of a job based on various resource usage characteristics</div> <div class="imageCredit">Purdue</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">ML failure prediction</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572097221089_FailureAnalysis_ASPLOS_081519--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572097221089_FailureAnalysis_ASPLOS_081519--rgov-800width.jpg" title="Event-driven checkpointing based on ML prediction"><img src="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572097221089_FailureAnalysis_ASPLOS_081519--rgov-66x44.jpg" alt="Event-driven checkpointing based on ML prediction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">System overview: how our ML model can be used to predict impending failures and trigger event-driven checkpointing, to complement the typical periodic checkpointing</div> <div class="imageCredit">Purdue</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">Event-driven checkpointing based on ML prediction</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572098694164_ampt-ga--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572098694164_ampt-ga--rgov-800width.jpg" title="Approximate computing through GPU mixed precision tuning"><img src="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572098694164_ampt-ga--rgov-66x44.jpg" alt="Approximate computing through GPU mixed precision tuning"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Overview of AMPT-GA. The inputs to AMPT-GA, shaded in green, are the application, the GPGPU properties (such as FP32 vs. FP64 instruction performance, casting cost), the error metric of interest and the corresponding error threshold, a test input, and the fuzzing process to generate similar inputs.</div> <div class="imageCredit">Purdue, LLNL</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">Approximate computing through GPU mixed precision tuning</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572099292765_fisets_overview--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572099292765_fisets_overview--rgov-800width.jpg" title="Fast Imprecise Sets (FISets), a performance-driven approach to tune mixed-precision GPU scientific applications"><img src="/por/images/Reports/POR/2019/1513197/1513197_10373144_1572099292765_fisets_overview--rgov-66x44.jpg" alt="Fast Imprecise Sets (FISets), a performance-driven approach to tune mixed-precision GPU scientific applications"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Overview of Fast Imprecise Sets (FISets), a performance-driven approach to tune mixed-precision GPU scientific applications. In our implementation, FISets correspond to fast mixed-precision configurations, and KTError analysis corresponds to the error-driven analysis component.</div> <div class="imageCredit">Purdue, LLNL</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Saurabh&nbsp;Bagchi</div> <div class="imageTitle">Fast Imprecise Sets (FISets), a performance-driven approach to tune mixed-precision GPU scientific applications</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Large-scale high performance computing (HPC) systems have become common in academic, industrial, and government for compute intensive applications, including large-scale parallel applications. These HPC systems solve problems that would take millennia on personal computers, but managing such large shared resources can be challenging and requires administrators to balance requirements from a diverse set of users. Large, focused organizations can afford to buy centralized resources, and choose to manage and operate it at academic organizations through a central IT organization. These are funded by federal funding agencies (like the National Science Foundation in the US) and individual researchers write grant proposals to get access to compute time on these systems. Examples of such systems include Comet at the University of California San Diego, BlueWaters at the University of Illinois at Urbana-Champaign, and Frontera at the University of Texas at Austin.     Progress in building dependable systems can be made faster if researchers working on dependability challenges can be exposed to problems through quantitative data. Theories in the labs and small demonstrations in prototypes can be transitioned to the demanding realities of large computer systems if they could validate their inventions with real system failure and attack data. There is an astonishing lack of such publicly available data for researchers and as a result many productive avenues of work in dependable system building are lying hidden. A comparison may fruitfully be drawn to the widespread use of benchmarks and reference data sets in performance analysis of computer systems, such as, those put out by SPEC or TPC. We therefore proposed to solve this problem by collecting, cleaning, annotating, and presenting data from the production compute infrastructures at several public universities.        We were not Pollyannaish in our effort and appreciated that this effort is challenging, both due to technological and psychological reasons. The technological reason referred to the need to keep the production infrastructure relatively undisturbed as the monitoring and the data collection happens. The psychological reason referred to the fact that such data is considered sensitive by many, to be unveiled only before trusting eyes of a subset of the people at the organization. We mitigated the two factors in our project. The technological factor was mitigated by carefully deciding which monitoring tool to use, when that should be activated, and how to store the data, both online and offline. We sidestepped the psychological factor by focusing on computational infrastructure at public universities (including ours), rather than private commercial organizations, and then working closely with the production IT unit to enable the data collection and data understanding.  Intellectual Merit:  We performed 4 different categories of analysis on production compute data collected from central computing clusters at three large public universities&frac34;Purdue University, University of Texas at Austin, and University of Illinois at Urbana-Champaign. In the first, we showed the breakdown of node failures into different categories and reasoned about the corresponding up times and recovery times. In the second, we considered the failures of individual jobs and reasoned about their root cause through examination of their exit codes. The third analysis sheds light on the relation between resource usage and job failure rates. We consider the 5 primary kinds of resources, local and remote, memory on a node, local IO, remote IO to the parallel file system, network, and runtime of a job. For the runtime, we considered both the spatial component (i.e., number of nodes) and the temporal component (i.e., the execution time on each node). Finally, we developed a job failure prediction model which can help minimize resource wastage corresponding to job failures due to system related issues.  Broader Impacts:    The project has created a large public workload and failure data repository from production computing clusters in a university setting, Fresco (for Purdue and UT Austin) and Monet (University of Illinois at Urbana-Champaign). Widespread use of the data and the associated analysis tools will give computer systems researchers an unprecedented ability to do data-driven research and computing infrastructure providers an analytics capability for running the infrastructures more efficiently and more reliably. University researchers will benefit from more available centralized computing clusters. Broad societal impact will result from the development of more efficient and more reliable large-scale computing clusters that can run societal critical applications reliably and efficiently and at large scales.       Last Modified: 10/26/2019       Submitted by: Saurabh Bagchi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
