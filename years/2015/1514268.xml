<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Deep Understanding: Integrating Neural and Symbolic Models of Meaning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2015</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1100000.00</AwardTotalIntnAmount>
<AwardAmount>1100000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Natural language understanding, automatically computing the meaning of text, is key for allowing citizens to deal intelligently with the vast amount of digital information surrounding us, from the fine print on credit cards to science textbook chapters or online instructional material. The goal of this project is to develop systems that can build richer understandings of text than current systems. Humans have an incredible ability to integrate the structure of  meaning --- how the meanings of sentences can be built up from the meanings of words --- with statistical knowledge about how words occur together with other words. Humans also effortlessly integrate meaning with 'reference', knowing which people or events in the world the text is talking about. But these tasks are quite difficult for computational systems. This project builds new computational models that integrate deep neural networks --- computational models with great power for representing word meaning in a statistical way --- with computational methods from logic and semantics. These new models allow word meanings to be combined together to build sentence meanings and also allow meanings to be linked with entities and events in the world. The resulting representations should help enable such societally important language understanding applications like question answering or tutorial software.&lt;br/&gt;&lt;br/&gt;This project develops compositional forms of deep learning that bridge between lexical and compositional semantics. This includes new kinds of embeddings that can be used to perform better meaning composition, computing for example that a student with a plaster cast is similar to an injured person just as earlier embeddings computed that injured is similar to hurt, and  extending the virtues (such as lexical coverage) of embeddings to represent the denotations of logical predicates. Another focus is  enriching models of meaning with models of reference, building entity-based models that can resolve coreference in texts to handle problems like bridging anaphora or verb and event coreference, with algorithms for entity-based coreference based on tensors that capture similarity of reference rather than similarity of lexical meaning.  And it includes developing vector space lexicons that represent both natural language dependency tree fragments and logical fragments in a shared vector space, and representing meaning as general programs that can model the effects of events and processes on resources in the world.  The new models are brought to bear on the end-to-end task of learning semantic parsers that map text to a semantic denotation.</AbstractNarration>
<MinAmdLetterDate>06/08/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/14/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1514268</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Jurafsky</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel S Jurafsky</PI_FULL_NAME>
<EmailAddress>jurafsky@stanford.edu</EmailAddress>
<PI_PHON>6507230924</PI_PHON>
<NSF_ID>000140878</NSF_ID>
<StartDate>06/08/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Manning</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher D Manning</PI_FULL_NAME>
<EmailAddress>manning@cs.stanford.edu</EmailAddress>
<PI_PHON>6507237683</PI_PHON>
<NSF_ID>000491177</NSF_ID>
<StartDate>06/08/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Percy</FirstName>
<LastName>Liang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Percy Liang</PI_FULL_NAME>
<EmailAddress>pliang@cs.stanford.edu</EmailAddress>
<PI_PHON>5105299396</PI_PHON>
<NSF_ID>000630416</NSF_ID>
<StartDate>06/08/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Computer Science Department]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943059025</ZipCode>
<StreetAddress><![CDATA[353 Serra Mall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~380818</FUND_OBLG>
<FUND_OBLG>2016~355223</FUND_OBLG>
<FUND_OBLG>2017~363959</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project we advanced our ability to process language computationally, by developing novel neural network algorithms that are capable of understanding and processing sophisticated linguistic meaning, tools that are necessary for the many tasks, both theoretical and practical, that apply computation to language both on the web and offline: question answering, machine translation, summarization, writing aids.</p> <p>First, our new models improved our ability to deal with the rich structured knoweldge that makes up natural language, to parse sentences, to decide who the phrases in the sentences were referring to, to build up meanings for entire texts from the meanings of the individual sentences and paragraphs. Our models are able to measure the way that texts are naturally coherent, helping automatically distinguish well-written from poorly-written text &nbsp; We developed new ways of automatically answering questions from large text repositories on the web, and ways of extracting information from text on the web to automatically create knowledge bases.</p> <p>Second, we ran new analytic experiments to peek under the hood at neural networks. Neural networks are very powerful algorithms for processing language, but they lack interpretability and transparency. We developed ways to investigate &nbsp;neural models of language to see how they were able to combine information from past linguistic structure to predict upcoming linguistic structure. And we found ways to create models that are better able to explain what factors lead to their predicted results.</p> <p>Our work also explored how neural models could help better answer scientific questions. &nbsp;We showed how neural models could help study how meanings change over time in language, developing new laws of semantic change. Change of language over time has practical implications as well; we showed how semantic change over time leads to problems in off-the-shelf tools for computer language understanding, and developed fixes for these problems.</p> <p>Finally, we used these neural newtorks and other language processing tools to explore important questions at the intersection of technology and society. &nbsp;We built models of communities on the web, studying what makes communities successful at attracting new members. &nbsp;And we developed new tools for understanding conflict on the web, &nbsp;studying how communities attack each other, who leads these attacks, how to predict when an attack will happen, and how attacks can be successful resisted.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/27/2019<br>      Modified by: Daniel&nbsp;S&nbsp;Jurafsky</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project we advanced our ability to process language computationally, by developing novel neural network algorithms that are capable of understanding and processing sophisticated linguistic meaning, tools that are necessary for the many tasks, both theoretical and practical, that apply computation to language both on the web and offline: question answering, machine translation, summarization, writing aids.  First, our new models improved our ability to deal with the rich structured knoweldge that makes up natural language, to parse sentences, to decide who the phrases in the sentences were referring to, to build up meanings for entire texts from the meanings of the individual sentences and paragraphs. Our models are able to measure the way that texts are naturally coherent, helping automatically distinguish well-written from poorly-written text   We developed new ways of automatically answering questions from large text repositories on the web, and ways of extracting information from text on the web to automatically create knowledge bases.  Second, we ran new analytic experiments to peek under the hood at neural networks. Neural networks are very powerful algorithms for processing language, but they lack interpretability and transparency. We developed ways to investigate  neural models of language to see how they were able to combine information from past linguistic structure to predict upcoming linguistic structure. And we found ways to create models that are better able to explain what factors lead to their predicted results.  Our work also explored how neural models could help better answer scientific questions.  We showed how neural models could help study how meanings change over time in language, developing new laws of semantic change. Change of language over time has practical implications as well; we showed how semantic change over time leads to problems in off-the-shelf tools for computer language understanding, and developed fixes for these problems.  Finally, we used these neural newtorks and other language processing tools to explore important questions at the intersection of technology and society.  We built models of communities on the web, studying what makes communities successful at attracting new members.  And we developed new tools for understanding conflict on the web,  studying how communities attack each other, who leads these attacks, how to predict when an attack will happen, and how attacks can be successful resisted.          Last Modified: 08/27/2019       Submitted by: Daniel S Jurafsky]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
