<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SI2-SSE: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>488048.00</AwardTotalIntnAmount>
<AwardAmount>488048</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In the era of data-intensive scientific discovery, Big Data scientists in all communities spend the majority of their time and effort collecting, integrating, curating, transforming, and assessing quality before actually performing discovery analysis. Some endeavors may even start from information not being available and accessible in digital form, and when it is available, it is often in non-structured form, not compatible with analytics tools that require structured and uniformly-formatted data. Two main methods to deal with the volume and variety of data as well as to accelerate the rate of digitization have been to apply crowdsourcing or machine-learning solutions. However, very little has been done to simultaneously take advantage of both types of solutions, and to make it easier for different efforts to share and reuse developed software elements. The vision of the Human- and Machine-Intelligent Network (HuMaIN) project is to accelerate scientific data digitization through fundamental advances in the integration and mutual cooperation between human and machine processing in order to handle practical hurdles and bottlenecks present in scientific data digitization. Even though HuMaIN concentrates on digitization tasks faced by the biodiversity community, the software elements being developed are generic in nature, and expected to be applicable to other scientific domains (e.g., exploring the surface of the moon for craters require the same type of crowdsourcing tool as finding words in text, and the same questions of whether machine-learning tools could provide similar results can be tested).&lt;br/&gt;&lt;br/&gt;The HuMaIN project proposes to conduct research and develop the following software elements: (a) configurable Machine-Learning  applications for scientific data digitization (e.g., Optical Character Recognition and Natural Language Processing), which will be made automatically available as RESTful services for increasing the ability of HuMaIN software elements to interoperate with other elements while decreasing the software development time via a new application specification language; (b) workflows leading to a cyber-human coordination system that will take advantage of feedback loops (e.g., based on consensus of crowdsourced data and its quality) for self-adaptation to changes  and increased sustainability of the overall system, (c) new crowdsourcing micro-tasks with ability of being reusable for a variety of scenarios and containing user activity sensors for studying time-effective user interfaces, and (d) services to support automated creation and configuration of crowdsourcing workflows on demand to fit the needs of individual groups. A cloud-based system will be deployed to provide the necessary execution environment with traceability of service executions involved in cyber-human workflows, and cost-effectiveness analysis of all the software elements developed in this project will provide assessment and evaluation of long standing what-if scenarios pertaining human- and machine-intelligent tasks. Crowdsourcing activities will attract a wide range of users with tasks that require low expertise, and at the same time it will expose volunteers to applied science and engineering, potentially attracting interest of K-12 teachers and students.</AbstractNarration>
<MinAmdLetterDate>07/09/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/22/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1535086</AwardID>
<Investigator>
<FirstName>Jose</FirstName>
<LastName>Fortes</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jose A Fortes</PI_FULL_NAME>
<EmailAddress>fortes@ufl.edu</EmailAddress>
<PI_PHON>3523929265</PI_PHON>
<NSF_ID>000415025</NSF_ID>
<StartDate>09/17/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mauricio</FirstName>
<LastName>Tsugawa</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mauricio Tsugawa</PI_FULL_NAME>
<EmailAddress>tsugawa@ufl.edu</EmailAddress>
<PI_PHON>3523925989</PI_PHON>
<NSF_ID>000558923</NSF_ID>
<StartDate>07/09/2015</StartDate>
<EndDate>04/22/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrea</FirstName>
<LastName>Matsunaga</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrea Matsunaga</PI_FULL_NAME>
<EmailAddress>ammatsun@ufl.edu</EmailAddress>
<PI_PHON>3528462466</PI_PHON>
<NSF_ID>000602684</NSF_ID>
<StartDate>07/09/2015</StartDate>
<EndDate>09/17/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>Gainesville</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress><![CDATA[1 University of Florida]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1165</Code>
<Text>ADVANCES IN BIO INFORMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8005</Code>
<Text>Scientific Software Elements</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~488048</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><table border="0" cellspacing="0" cellpadding="0" width="100%"> <tbody> <tr> <td valign="top"> <p>The information provided by labels of biological specimens   held in museums across the world is used by scientists to study species   distributions over time and geographical locations, the evolution of biological   organisms, the relationships among different species and many other biodiversity-related   topics. This Human- and Machine-Intelligent Network (HuMaIN) project addressed   the problem of extracting information from imaged text in digitized museum   specimen labels. This is a challenging task due to both the large number of specimen   labels and the complexity of the text and information to be extracted. Previous   studies considered the use of methods such as Optical Character Recognition and   Natural Language Processing to automate information extraction. However,   these approaches cannot be guaranteed to generate error-free transcriptions. Instead,   human transcriptions have been the preferred solution, relying either on experts   or crowdsourcing involving volunteer citizen scientists. However, human   transcriptions are much slower than automated approaches and, as a result, it   is estimated that many years will be required to transcribe the information   in the specimen labels.</p> <p>The   HuMaIN project investigated software-enabled solutions that support the   combination of machine and human intelligence to accelerate IE from specimen   labels. The key insight of the project is to only use humans when there is no   confidence in automated transcriptions and having automatic approaches to   assess confidence. Among other contributions, the project proposed the use of   self-aware workflows to orchestrate machines and human tasks (called the   SELFIE model), combinations of Optical Character Recognition tools and   Natural Language Processing methods to increase confidence in extracted text,   named-entity recognition techniques to extract from labels terms included in   a standard biodiversity vocabulary (called Darwin Core terms), and a   simulator for the study of these workflows with real-world data. The software   produced by the project has been tested and applied on large biodiversity datasets.   The project showed that these techniques allow the combination of automated   transcription methods and human transcriptions so that machine-like speeds   and human-like accuracy are achieved. In information extraction experiments   done during the project, the proposed solutions reduced by 84% the number of   human tasks required to extract the terms, while preserving human-equivalent   quality. The project also proposed good practices for the design of efficient   crowdsourcing interfaces for the transcription of DC terms by non-expert   humans.</p> <p>Broader   Impacts: This<strong> </strong>project produced one PhD graduate and supported the   training of two additional graduate students, produced open-sourced software   systems with eleven components for information extraction tasks, created a simulator   for future studies and applied the project software to large datasets from   the biodiversity communities in the USA and Australia. The project produced knowledge   and prototype tools that can be leveraged to build information extraction   platforms to accelerate the digitization of specimens in biological collections.</p> <p>The   following papers describe the contributions of this project in detail:</p> <p><strong><em>Generalizable Self-aware Information Extraction from Labels of   Biological Collections</em></strong>. Icaro Alzuru, Andr?a Matsunaga, Maur?cio   Tsugawa, and Jos? A.B. Fortes. Submitted for   publication on 05/15/2020.</p> <p><strong><em>Quality-Aware Text and Information Extraction from   Biocollections by using Ensembles of OCRs</em></strong><em>.</em> Icaro Alzuru,   Rhiannon Stephens, Andr?a Matsunaga, Maur?cio Tsugawa, Paul Flemons, and Jos?   A.B. Fortes. Submitted for publication on 03/20/2020.</p> <p><strong><em>Human-Machine   Information Extraction Simulator for Biological Collections</em></strong>. Icaro Alzuru, Aditi   Malladi, Andr?a Matsunaga, Maur?cio Tsugawa, and Jos? A.B. Fortes. 2019 IEEE   International Conference on Big Data, Los Angeles, CA, USA, 2019, pp.   4565-4572. DOI: <a href="https://doi.org/10.1109/BigData47090.2019.9005601">https://doi.org/10.1109/BigData47090.2019.9005601</a></p> <p><strong><em>Quality-Aware Human-Machine Text Extraction for Biocollections   using Ensembles of OCRs</em></strong>. Icaro Alzuru,   Rhiannon Stephens, Andr?a Matsunaga, Maur?cio Tsugawa, Paul Flemons, and Jos?   A.B. Fortes. 2019 15th International Conference on eScience (eScience), San   Diego, CA, USA, 2019, pp. 116-125. DOI: <a href="https://doi.org/10.1109/eScience.2019.00020">https://doi.org/10.1109/eScience.2019.00020</a></p> <p><strong><em>SELFIE: Self-Aware Information Extraction from Digitized   Biocollections</em></strong>. Icaro Alzuru, Andr?a Matsunaga, Maur?cio   Tsugawa, and Jos? A.B. Fortes. 2017 IEEE 13th   International Conference on e-Science (e-Science), Auckland, 2017, pp. 69-78.   DOI: <a href="https://doi.org/10.1109/eScience.2017.19">https://doi.org/10.1109/eScience.2017.19</a></p> <p><strong><em>Task Design &amp; Crowd Sentiment in Biocollections Information   Extraction</em></strong>. Icaro Alzuru, Andr?a   Matsunaga, Maur?cio Tsugawa, and Jos? A.B. Fortes. 2017 IEEE 3rd   International Conference on Collaboration and Internet Computing (CIC), San   Jose, CA, 2017, pp. 389-398. DOI: <a href="https://doi.org/10.1109/CIC.2017.00056">https://doi.org/10.1109/CIC.2017.00056</a></p> <p><strong><em>Cooperative Human-Machine Data Extraction from Biological   Collections</em></strong>. Icaro Alzuru, Andr?a Matsunaga, Maur?cio   Tsugawa, and Jos? A.B. Fortes. 2016 IEEE 12th   International Conference on e-Science (e-Science), Baltimore, MD, 2016, pp.   41-50. DOI: <a href="https://doi.org/10.1109/eScience.2016.7870884">https://doi.org/10.1109/eScience.2016.7870884</a> &nbsp;</p> </td> </tr> </tbody> </table> <p>&nbsp;</p><br> <p>            Last Modified: 07/09/2020<br>      Modified by: Jose&nbsp;A&nbsp;Fortes</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1535086/1535086_10375342_1594298666050_Humainpic--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1535086/1535086_10375342_1594298666050_Humainpic--rgov-800width.jpg" title="HuMaIN project overview"><img src="/por/images/Reports/POR/2020/1535086/1535086_10375342_1594298666050_Humainpic--rgov-66x44.jpg" alt="HuMaIN project overview"></a> <div class="imageCaptionContainer"> <div class="imageCaption">HuMaIN project overview</div> <div class="imageCredit">Icaro Alzuru, Andrea Matsunaga, Mauricio Tsugawa and Jose Fortes</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jose&nbsp;A&nbsp;Fortes</div> <div class="imageTitle">HuMaIN project overview</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     The information provided by labels of biological specimens   held in museums across the world is used by scientists to study species   distributions over time and geographical locations, the evolution of biological   organisms, the relationships among different species and many other biodiversity-related   topics. This Human- and Machine-Intelligent Network (HuMaIN) project addressed   the problem of extracting information from imaged text in digitized museum   specimen labels. This is a challenging task due to both the large number of specimen   labels and the complexity of the text and information to be extracted. Previous   studies considered the use of methods such as Optical Character Recognition and   Natural Language Processing to automate information extraction. However,   these approaches cannot be guaranteed to generate error-free transcriptions. Instead,   human transcriptions have been the preferred solution, relying either on experts   or crowdsourcing involving volunteer citizen scientists. However, human   transcriptions are much slower than automated approaches and, as a result, it   is estimated that many years will be required to transcribe the information   in the specimen labels.  The   HuMaIN project investigated software-enabled solutions that support the   combination of machine and human intelligence to accelerate IE from specimen   labels. The key insight of the project is to only use humans when there is no   confidence in automated transcriptions and having automatic approaches to   assess confidence. Among other contributions, the project proposed the use of   self-aware workflows to orchestrate machines and human tasks (called the   SELFIE model), combinations of Optical Character Recognition tools and   Natural Language Processing methods to increase confidence in extracted text,   named-entity recognition techniques to extract from labels terms included in   a standard biodiversity vocabulary (called Darwin Core terms), and a   simulator for the study of these workflows with real-world data. The software   produced by the project has been tested and applied on large biodiversity datasets.   The project showed that these techniques allow the combination of automated   transcription methods and human transcriptions so that machine-like speeds   and human-like accuracy are achieved. In information extraction experiments   done during the project, the proposed solutions reduced by 84% the number of   human tasks required to extract the terms, while preserving human-equivalent   quality. The project also proposed good practices for the design of efficient   crowdsourcing interfaces for the transcription of DC terms by non-expert   humans.  Broader   Impacts: This project produced one PhD graduate and supported the   training of two additional graduate students, produced open-sourced software   systems with eleven components for information extraction tasks, created a simulator   for future studies and applied the project software to large datasets from   the biodiversity communities in the USA and Australia. The project produced knowledge   and prototype tools that can be leveraged to build information extraction   platforms to accelerate the digitization of specimens in biological collections.  The   following papers describe the contributions of this project in detail:  Generalizable Self-aware Information Extraction from Labels of   Biological Collections. Icaro Alzuru, Andr?a Matsunaga, Maur?cio   Tsugawa, and Jos? A.B. Fortes. Submitted for   publication on 05/15/2020.  Quality-Aware Text and Information Extraction from   Biocollections by using Ensembles of OCRs. Icaro Alzuru,   Rhiannon Stephens, Andr?a Matsunaga, Maur?cio Tsugawa, Paul Flemons, and Jos?   A.B. Fortes. Submitted for publication on 03/20/2020.  Human-Machine   Information Extraction Simulator for Biological Collections. Icaro Alzuru, Aditi   Malladi, Andr?a Matsunaga, Maur?cio Tsugawa, and Jos? A.B. Fortes. 2019 IEEE   International Conference on Big Data, Los Angeles, CA, USA, 2019, pp.   4565-4572. DOI: https://doi.org/10.1109/BigData47090.2019.9005601  Quality-Aware Human-Machine Text Extraction for Biocollections   using Ensembles of OCRs. Icaro Alzuru,   Rhiannon Stephens, Andr?a Matsunaga, Maur?cio Tsugawa, Paul Flemons, and Jos?   A.B. Fortes. 2019 15th International Conference on eScience (eScience), San   Diego, CA, USA, 2019, pp. 116-125. DOI: https://doi.org/10.1109/eScience.2019.00020  SELFIE: Self-Aware Information Extraction from Digitized   Biocollections. Icaro Alzuru, Andr?a Matsunaga, Maur?cio   Tsugawa, and Jos? A.B. Fortes. 2017 IEEE 13th   International Conference on e-Science (e-Science), Auckland, 2017, pp. 69-78.   DOI: https://doi.org/10.1109/eScience.2017.19  Task Design &amp; Crowd Sentiment in Biocollections Information   Extraction. Icaro Alzuru, Andr?a   Matsunaga, Maur?cio Tsugawa, and Jos? A.B. Fortes. 2017 IEEE 3rd   International Conference on Collaboration and Internet Computing (CIC), San   Jose, CA, 2017, pp. 389-398. DOI: https://doi.org/10.1109/CIC.2017.00056  Cooperative Human-Machine Data Extraction from Biological   Collections. Icaro Alzuru, Andr?a Matsunaga, Maur?cio   Tsugawa, and Jos? A.B. Fortes. 2016 IEEE 12th   International Conference on e-Science (e-Science), Baltimore, MD, 2016, pp.   41-50. DOI: https://doi.org/10.1109/eScience.2016.7870884                Last Modified: 07/09/2020       Submitted by: Jose A Fortes]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
