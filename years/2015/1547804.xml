<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Adaptive Shared Memory Management for Heterogeneous CPU-GPU Architectures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Building efficient memory systems for exascale computing has nowadays become extremely challenging. Applications running in exascale computing environments are becoming increasingly diverse and complicated, making managing shared memory system particularly difficult. Graphic Processing Units (GPUs) are entering exascale computing as accelerators, imposing new technical challenges in sharing memory. Non-volatile memory (NVM) has promising future in the memory/storage hierarchy. It can be used to enlarge memory capacity and/or improve energy efficiency. However, how to efficiently integrate NVM into current memory architecture to build a highly efficient memory system remains an open problem. This project proposes an adaptive shared memory management scheme to address the memory interference problem in heterogeneous CPU-GPU architectures. A flexible framework based on a hybrid memory model for integrating non-volatile memory into a shared memory system will be developed. In addition, the project will explore designs to build a unified address space for heterogeneous architectures. &lt;br/&gt;&lt;br/&gt;This project addresses the challenges in exploring shared memory management schemes for heterogeneous architectures. Proposed techniques of management for memory interference in both homogeneous and heterogeneous architectures are well amenable to the high concurrency and widely different interference patterns in exascale computing environments. These techniques provide a compelling solution to managing shared memory for heterogeneous systems. The proposed work can serve as a starting point to conquer the bigdata and exascale computing challenges in terms of large-scale memory and computer system design. Moreover, the proposed research will facilitate the server clusters for memory-intensive applications to most effectively utilize those existing/emerging architecture and system technologies to tackle the memory challenge in heterogeneous CPU-GPU systems. The proposed efficient shared memory management can benefit numerous memory-intensive applications such as big data analytics, biology, chemistry, earth science, etc. In addition, the project will integrate research and education together. It will provide opportunities for undergraduate and graduate students to participate in the research and help train a new generation of computer scientists and engineers in the area of high-performance computing.</AbstractNarration>
<MinAmdLetterDate>08/12/2015</MinAmdLetterDate>
<MaxAmdLetterDate>12/21/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1547804</AwardID>
<Investigator>
<FirstName>Xubin</FirstName>
<LastName>He</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xubin He</PI_FULL_NAME>
<EmailAddress>xubin.he@temple.edu</EmailAddress>
<PI_PHON>2152042071</PI_PHON>
<NSF_ID>000110014</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Weijun</FirstName>
<LastName>Xiao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Weijun Xiao</PI_FULL_NAME>
<EmailAddress>wxiao@vcu.edu</EmailAddress>
<PI_PHON>8048285339</PI_PHON>
<NSF_ID>000583590</NSF_ID>
<StartDate>08/12/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Virginia Commonwealth University</Name>
<CityName>RICHMOND</CityName>
<ZipCode>232980568</ZipCode>
<PhoneNumber>8048286772</PhoneNumber>
<StreetAddress>P.O. Box 980568</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>105300446</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VIRGINIA COMMONWEALTH UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>105300446</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Virginia Commonwealth University]]></Name>
<CityName>Richmond</CityName>
<StateCode>VA</StateCode>
<ZipCode>232980568</ZipCode>
<StreetAddress><![CDATA[P.O. Box 980568]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="text-align: left;">Among the commonly recognized challenges with BIGDATA and high-performance computing, building a high-performance, scalable, and energy-efficient memory systems is particularly critical. The aim of the project is to explore new schemes and solutions for efficiently managing a unified shared memory system for heterogeneous CPU-GPUs by integrating NVM into the memory system. Through the course of the project, PI/Co-PI and their research team achieve the following major outcomes.</p> <p>Alleviation of DRAM Refresh Overhead: DRAM memory performs periodic refreshes to prevent data loss due to charge leakage, while memory refresh&nbsp;causes performance degradation and energy consumption, referred to as refresh overheads. In this project, Refresh-Oriented Prefetching (ROP) was proposed to alleviate memory refresh overheads. Before a refresh starts, ROP prefetches cache lines from the tobe-refreshed rank into an added SRAM buffer. Furthermore, a practical and effective refresh approach called CAR (Compression-Aware Refresh) was proposed to efficiently mitigate&nbsp;refresh overheads. A data compression algorithm is applied to store data in compressed format so that the memory controller can safely skip refreshing memory rows which contain no useful&nbsp;data without compromising data integrity. In addition, a Rank-level Piggback Caching called RPC was proposed to alleviate DRAM refresh overhead.</p> <p>Flash-aware High-performance and Endurable Cache Management: a&nbsp;new flash-aware read cache design was proposed to leverage out-of-place update property of solid state drives (SSDs) to improve both cache performance and lifetime. When a cache entry is evicted from the flash cache, the eviction only removes the metadata,&nbsp;while the real data is still accessible and resides in the physical flash page until the whole flash block being erased. The main idea of proposed&nbsp;flash-aware cache is to reuse these evicted but still available data for improving cache performance. Furthermore, a reuse distance aware cache management was proposed to improve both the performance and lifetime of SSD-based write cache by compromising the cache hit ratio and the internal garbage collection overhead</p> <p>Memory Architectures for Die-stacked DRAM/NVM Memory Systems: Die-stacked DRAM is typically used as Part of Memory (PoM) for high bandwidth and capacity. Existing PoM designs are either line-based or page-based. A efficient approach to address the drawbacks of existing PoM designs was proposed to SElectively swap Lines in a requested page that are likely to be accessed according to page Footprint (SELF), instead of blindly swapping an entire page. In doing so, SELF allows incoming requests to be serviced from the on-chip memory as much as possible, while avoiding swapping unused lines to reduce memory bandwidth consumption. In addition, a cost-effective and energy efficient hybrid memory architecture for integrating high-bandwidth memory and non-volatile memory called Dual Role HBM (DR-HBM) was proposed to build a high-performance, large-capacity, and energy-efficient memory system.&nbsp;</p> <p>Performance analysis on GPU L1 Data Cache: Through the analysis of GPU L1 cache, a few interesting cache behaviors are discovered. Most benchmarks either access cache in a streaming manner or reuse previous cache line in a short reuse distance. Streaming accesses barely benefit from cache since they have no data reuse. For the accesses with a short reuse distance, they can exploit the data locality in current cache design.</p> <p>The project has significant institutional and community impacts. It helps promote research in emerging memory and storage technologies at both VCU and Temple, two diverse universities in the nation. The project supports the high-performance and GPU computing lab at VCU and the Storage Technology and Architecture (STAR) lab at Temple to conduct cutting-edge research on BIGDATA, high-performance computing, and I/O systems. Proposed techniques and solutions will benefit research community to improve performance and energy efficiency of memory and storage systems.</p> <p>Six graduate students and two undergraduate students actively worked on the project to develop algorithms, propose the solutions, and conduct experiments. Two high school students were involved to help result collections and analysis. The project published 15 peer-reviewed papers to report the research results in prestigious journals and international conferences. Three graduate students including two PhDs and one Master successfully received their degrees and got decent industrial job offers to join the industry. Involving undergraduate students in the project motivates them to pursue higher degrees in computer architecture area. Showing the high-school students for the research helps them to choose college majors in related STEM areas.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/27/2018<br>      Modified by: Weijun&nbsp;Xiao</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Among the commonly recognized challenges with BIGDATA and high-performance computing, building a high-performance, scalable, and energy-efficient memory systems is particularly critical. The aim of the project is to explore new schemes and solutions for efficiently managing a unified shared memory system for heterogeneous CPU-GPUs by integrating NVM into the memory system. Through the course of the project, PI/Co-PI and their research team achieve the following major outcomes.  Alleviation of DRAM Refresh Overhead: DRAM memory performs periodic refreshes to prevent data loss due to charge leakage, while memory refresh causes performance degradation and energy consumption, referred to as refresh overheads. In this project, Refresh-Oriented Prefetching (ROP) was proposed to alleviate memory refresh overheads. Before a refresh starts, ROP prefetches cache lines from the tobe-refreshed rank into an added SRAM buffer. Furthermore, a practical and effective refresh approach called CAR (Compression-Aware Refresh) was proposed to efficiently mitigate refresh overheads. A data compression algorithm is applied to store data in compressed format so that the memory controller can safely skip refreshing memory rows which contain no useful data without compromising data integrity. In addition, a Rank-level Piggback Caching called RPC was proposed to alleviate DRAM refresh overhead.  Flash-aware High-performance and Endurable Cache Management: a new flash-aware read cache design was proposed to leverage out-of-place update property of solid state drives (SSDs) to improve both cache performance and lifetime. When a cache entry is evicted from the flash cache, the eviction only removes the metadata, while the real data is still accessible and resides in the physical flash page until the whole flash block being erased. The main idea of proposed flash-aware cache is to reuse these evicted but still available data for improving cache performance. Furthermore, a reuse distance aware cache management was proposed to improve both the performance and lifetime of SSD-based write cache by compromising the cache hit ratio and the internal garbage collection overhead  Memory Architectures for Die-stacked DRAM/NVM Memory Systems: Die-stacked DRAM is typically used as Part of Memory (PoM) for high bandwidth and capacity. Existing PoM designs are either line-based or page-based. A efficient approach to address the drawbacks of existing PoM designs was proposed to SElectively swap Lines in a requested page that are likely to be accessed according to page Footprint (SELF), instead of blindly swapping an entire page. In doing so, SELF allows incoming requests to be serviced from the on-chip memory as much as possible, while avoiding swapping unused lines to reduce memory bandwidth consumption. In addition, a cost-effective and energy efficient hybrid memory architecture for integrating high-bandwidth memory and non-volatile memory called Dual Role HBM (DR-HBM) was proposed to build a high-performance, large-capacity, and energy-efficient memory system.   Performance analysis on GPU L1 Data Cache: Through the analysis of GPU L1 cache, a few interesting cache behaviors are discovered. Most benchmarks either access cache in a streaming manner or reuse previous cache line in a short reuse distance. Streaming accesses barely benefit from cache since they have no data reuse. For the accesses with a short reuse distance, they can exploit the data locality in current cache design.  The project has significant institutional and community impacts. It helps promote research in emerging memory and storage technologies at both VCU and Temple, two diverse universities in the nation. The project supports the high-performance and GPU computing lab at VCU and the Storage Technology and Architecture (STAR) lab at Temple to conduct cutting-edge research on BIGDATA, high-performance computing, and I/O systems. Proposed techniques and solutions will benefit research community to improve performance and energy efficiency of memory and storage systems.  Six graduate students and two undergraduate students actively worked on the project to develop algorithms, propose the solutions, and conduct experiments. Two high school students were involved to help result collections and analysis. The project published 15 peer-reviewed papers to report the research results in prestigious journals and international conferences. Three graduate students including two PhDs and one Master successfully received their degrees and got decent industrial job offers to join the industry. Involving undergraduate students in the project motivates them to pursue higher degrees in computer architecture area. Showing the high-school students for the research helps them to choose college majors in related STEM areas.           Last Modified: 11/27/2018       Submitted by: Weijun Xiao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
