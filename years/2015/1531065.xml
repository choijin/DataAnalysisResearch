<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>US Ignite: Collaborative Research: Track 1: Industrial Cloud Robotics across Software Defined Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>424261.00</AwardTotalIntnAmount>
<AwardAmount>424261</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bruce Kramer</SignBlockName>
<PO_EMAI>bkramer@nsf.gov</PO_EMAI>
<PO_PHON>7032925348</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Currently, industrial robots are cost-effective for repetitive and high-volume tasks such as welding and painting, but not for lower-volume, mixed-part production.  The need for robotic part handling for unstructured industrial applications is diverse. In manufactured-goods distribution centers, where multiple bins are presented to an operator, a human is required to handle a range of parts that must be boxed and shipped. In the reclamation and recycling industry, humans sort waste streams of mixed products on conveyor belts. Assembly and kitting operations in manufacturing are termed robotic opportunities but they require a solution for handling many part types in the same work-cell. This project will research and integrate technologies to enable the use of industrial robots for low-volume mixed-part production tasks. The proposed solution will include 3D image sensors and high-speed flexible networking, cloud computing, and industrial robots. The inclusion of cutting-edge new software such as the Robot-Operating System Industrial (ROS-I) and Cloud Computing platforms offer excellent educational opportunities for both undergraduate and graduate students. The software developed in this project will be widely distributed to enable further innovations by other teams.&lt;br/&gt;&lt;br/&gt;The project objective is to develop cloud robotics applications that leverage high-performance computing and high-speed software-defined networks (SDN). Specifically, the target applications combine big-data analytics of sensor data (of the type collected from factory floors) with the control of industrial robots for low-volume, mixed-part production tasks. Cloud computers located at a remote facility relative to the factory floor on which industrial robots operate can be used for compute-intensive applications such as object identification from 3D sensor data, and grasp planning for the robots to perform object manipulation.  The project methods will consist of (i) integrating ROS-I components and developing new software as required to transmit the 3D sensor data to remote computers, running the object identification and grasp planning applications, and returning robot instructions to the original site, (ii) running this software on geographically distributed compute clouds, (iii) collecting measurements and enhancing the software to meet real-time delay requirements. The technical challenge lies in meeting these stringent real-time requirements. For example, high-speed networks with the flexibility to connect arbitrary factory floors and datacenters are needed to transfer the 3D sensor data quickly to the remote cloud computers and to deliver the computed robot instructions(hence, SDN).</AbstractNarration>
<MinAmdLetterDate>08/24/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/24/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1531065</AwardID>
<Investigator>
<FirstName>Malathi</FirstName>
<LastName>Veeraraghavan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Malathi Veeraraghavan</PI_FULL_NAME>
<EmailAddress>mv5g@virginia.edu</EmailAddress>
<PI_PHON>4349822208</PI_PHON>
<NSF_ID>000254309</NSF_ID>
<StartDate>08/24/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shaun</FirstName>
<LastName>Edwards</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shaun Edwards</PI_FULL_NAME>
<EmailAddress>shaun.edwards@swri.org</EmailAddress>
<PI_PHON>2105222231</PI_PHON>
<NSF_ID>000690767</NSF_ID>
<StartDate>08/24/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia]]></Name>
<CityName>CHARLOTTESVILLE</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044743</ZipCode>
<StreetAddress><![CDATA[POB 400743, Thornton Hall C222]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2890</Code>
<Text>CISE Research Resources</Text>
</ProgramElement>
<ProgramReference>
<Code>015Z</Code>
<Text>US Ignite</Text>
</ProgramReference>
<ProgramReference>
<Code>082E</Code>
<Text>MFG MACHINES &amp; METROLOGY</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~424261</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="UVA-body">A visitor to a typical manufacturing plant is likely to see robots used for high-volume welding or painting of a low-mix (few types) of products. This is because it is feasible for a manufacturer to pre-program robots for a highly repetitive operation. But, visitors to a typical assembly plant (automotive or aerospace) where a very large variety of parts are handled, or to a small or medium sized manufacturer that produces high-mix low-volume goods, are not very likely to see robots. This is because robots do not currently have the capability to recognize a wide variety of parts and plan how to grasp and manipulate the parts.</p> <p class="UVA-body">This project developed a high-mix low-volume industrial cloud robotics application called Gilbreth that leveraged cloud computing resources. The goal of this application was to enable a robot arm to pick and sort random objects arriving on a conveyor belt.</p> <p class="UVA-body">Different types of objects (industrial parts such as gears and piston rods) arrive at random, in arbitrary position and orientation (referred to as &ldquo;pose&rdquo;) on a conveyor belt. A UR10 robot arm is mounted on a rail that is setup in parallel with the conveyor belt. A Kinect sensor keeps taking 3D pictures of objects arriving on the belt, while a break beam sensor is triggered every time an object crosses the beam (enters the workspace of the robot arm). While the belt moves the object from the location of the break beam sensor to the position where the UR10 robot arm waits to pick up the object, the sensor data is used by the object recognition software to identify the object type and the motion planning algorithm to compute trajectories for all 7 joints of the robot arm. These trajectories were used to move the arm and its vacuum gripper between various poses, such as pick pose (to pick up the object), place pose (to drop the object in the appropriate bin) and home pose (where the robot arm waits for the next object).</p> <p class="UVA-body">The two challenging operations in this application are object recognition and motion planning. A machine-learning based 3D object recognition algorithm proved to provide the best performance, offering a ten-fold improvement in performance over a simpler algorithm in which one-to-one correspondences are checked between stored model information and captured information from a newly arriving object. However, the machine-learning method requires significant training (e.g., 3 hours with just 13 object types). The availability of cloud computing resources enables the use of this machine-learning approach to 3D object recognition.</p> <p class="UVA-body">For motion planning, we used the MoveIt! ROS package. ROS stands for Robot Operating System. ROS and ROS-I (ROS-Industrial) are flexible frameworks that have been developed by large numbers of robotics programmers who create a varied set of open-source software packages. The availability of these packages enabled us to design and implement such a complex application with just 4 developers within 2 years. We then used an open-source robotics simulation environment called Gazebo to evaluate Gilbreth. This evaluation study led to further improvements. For example, while most trajectory computations for the robot arm joints were completed within 0.5 seconds, sometimes these computations took a long time to complete. When the computation was long, invariably, the resulting trajectory was flawed, e.g., the robot arm did not move to the target pose directly, but instead wandered, and even rotated for a while, before reaching the desired pose. By simply limiting the trajectory computation time, we found motion plans that resulted in fewer robot execution failures and smaller coefficient of variance (e.g., 8.5% instead of 20.8% for the piston rod) in robot execution times.</p> <p class="UVA-body">Our first conclusion is that the variety of available ROS and ROS-I software packages allowed us to assemble and evaluate a complex application within just 1.5 years with four developers. Second, machine learning algorithms for 3D object recognition offer impressive speedups when compared to traditional methods; however, a computational cost is incurred for model training. Here, cloud robotics offers an answer for both the computing and storage required in the training phase. Third, our evaluation showed that motion planning and grasping remain complex tasks, and current ROS/ROS-I packages could be improved to reduce failure rates.</p> <p class="UVA-body">The Gilbreth software package is available on this Website: &nbsp;<a href="https://github.com/swri-robotics/gilbreth/">https://github.com/swri-robotics/gilbreth/</a></p> <p class="UVA-body">In Year 1, another application, called Godel, for automating the task of metal surface blending using an industrial robot ABB IRB 2400 was implemented and evaluated. Blending is the operation of smoothing metal surfaces down to an even finish. This application used a similar combination of machine vision for object identification and motion planning.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2019<br>      Modified by: Malathi&nbsp;Veeraraghavan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[A visitor to a typical manufacturing plant is likely to see robots used for high-volume welding or painting of a low-mix (few types) of products. This is because it is feasible for a manufacturer to pre-program robots for a highly repetitive operation. But, visitors to a typical assembly plant (automotive or aerospace) where a very large variety of parts are handled, or to a small or medium sized manufacturer that produces high-mix low-volume goods, are not very likely to see robots. This is because robots do not currently have the capability to recognize a wide variety of parts and plan how to grasp and manipulate the parts. This project developed a high-mix low-volume industrial cloud robotics application called Gilbreth that leveraged cloud computing resources. The goal of this application was to enable a robot arm to pick and sort random objects arriving on a conveyor belt. Different types of objects (industrial parts such as gears and piston rods) arrive at random, in arbitrary position and orientation (referred to as "pose") on a conveyor belt. A UR10 robot arm is mounted on a rail that is setup in parallel with the conveyor belt. A Kinect sensor keeps taking 3D pictures of objects arriving on the belt, while a break beam sensor is triggered every time an object crosses the beam (enters the workspace of the robot arm). While the belt moves the object from the location of the break beam sensor to the position where the UR10 robot arm waits to pick up the object, the sensor data is used by the object recognition software to identify the object type and the motion planning algorithm to compute trajectories for all 7 joints of the robot arm. These trajectories were used to move the arm and its vacuum gripper between various poses, such as pick pose (to pick up the object), place pose (to drop the object in the appropriate bin) and home pose (where the robot arm waits for the next object). The two challenging operations in this application are object recognition and motion planning. A machine-learning based 3D object recognition algorithm proved to provide the best performance, offering a ten-fold improvement in performance over a simpler algorithm in which one-to-one correspondences are checked between stored model information and captured information from a newly arriving object. However, the machine-learning method requires significant training (e.g., 3 hours with just 13 object types). The availability of cloud computing resources enables the use of this machine-learning approach to 3D object recognition. For motion planning, we used the MoveIt! ROS package. ROS stands for Robot Operating System. ROS and ROS-I (ROS-Industrial) are flexible frameworks that have been developed by large numbers of robotics programmers who create a varied set of open-source software packages. The availability of these packages enabled us to design and implement such a complex application with just 4 developers within 2 years. We then used an open-source robotics simulation environment called Gazebo to evaluate Gilbreth. This evaluation study led to further improvements. For example, while most trajectory computations for the robot arm joints were completed within 0.5 seconds, sometimes these computations took a long time to complete. When the computation was long, invariably, the resulting trajectory was flawed, e.g., the robot arm did not move to the target pose directly, but instead wandered, and even rotated for a while, before reaching the desired pose. By simply limiting the trajectory computation time, we found motion plans that resulted in fewer robot execution failures and smaller coefficient of variance (e.g., 8.5% instead of 20.8% for the piston rod) in robot execution times. Our first conclusion is that the variety of available ROS and ROS-I software packages allowed us to assemble and evaluate a complex application within just 1.5 years with four developers. Second, machine learning algorithms for 3D object recognition offer impressive speedups when compared to traditional methods; however, a computational cost is incurred for model training. Here, cloud robotics offers an answer for both the computing and storage required in the training phase. Third, our evaluation showed that motion planning and grasping remain complex tasks, and current ROS/ROS-I packages could be improved to reduce failure rates. The Gilbreth software package is available on this Website:  https://github.com/swri-robotics/gilbreth/ In Year 1, another application, called Godel, for automating the task of metal surface blending using an industrial robot ABB IRB 2400 was implemented and evaluated. Blending is the operation of smoothing metal surfaces down to an even finish. This application used a similar combination of machine vision for object identification and motion planning.          Last Modified: 11/30/2019       Submitted by: Malathi Veeraraghavan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
