<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Efficient Projection-Free Algorithms for Optimization and Online Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The advent of the Internet gives rise to an exponential growth in data collection, availability and complexity. With it increases our need for more efficient data analysis algorithms. Over super-scale datasets, the only feasible data analysis techniques are iterative linear-time first-order optimization methods. &lt;br/&gt;&lt;br/&gt;The computational bottleneck in applying these state-of-the-art iterative methods to machine learning and data analysis is often the so-called "projection step". This project addresses the need to design projection-free optimization algorithms that replace projections by more efficient linear optimization steps. A key contribution of the project is the continual dissemination and transfer of this technology. The open-source software releases will continue to enable large-scale machine learning applications in science and engineering. The broader impact goals of the project, beyond theory and algorithms, include the development of a textbook on efficient optimization techniques in machine learning, as well as the development of a new curriculum focused on preparing students for the scientific and engineering needs in this field.</AbstractNarration>
<MinAmdLetterDate>08/05/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1523815</AwardID>
<Investigator>
<FirstName>Elad</FirstName>
<LastName>Hazan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elad Hazan</PI_FULL_NAME>
<EmailAddress>ehazan@cs.princeton.edu</EmailAddress>
<PI_PHON>6092583090</PI_PHON>
<NSF_ID>000674538</NSF_ID>
<StartDate>08/05/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[87 Prospect Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The subject of this project are efficient optimization methods for machine learnign training. A special emphasis was on projection-free methods that are used to handle constraints.&nbsp;</p> <div class="page" title="Page 1"> <div class="layoutArea"> <div class="column"> <p>The project was very successful, with numerous top-tier publications, including a scientific prize for a new optimization method that we will now describe.&nbsp;</p> <p>Amongst the scienfic discoveries made, one stands out in particular. A main scietific outcome of this project is a new&nbsp;optimization algorithm based on Newton's method for machine learning. While Newton's method is itself an age old and well-studied algorithm, it is not directly applicable to training of machine learning models for two main reasons.</p> <p>The first challenge is that ML problems are usually very large scale, and Newon's method, or methods that use the second derivative more generally, require maintaining and manupulating matrices of the dimension of the data, which is infeasible. The second difficulty comes from the stochastic nature of the problem: in ML we usually observe noisy data examples sequentially, and prefer a method that can make progress over each example rather than observing all training data at once, which is infeasible due to the amount of data.&nbsp;&nbsp;</p> <p>We propose a novel randomized approach for linear time second-order methods that, using the special structure of objectives used in machine learning, avoid any matrix operations and run in linear time, while retaining all the theoretical properties of Newton&rsquo;s method, giving the best known running times for several convex and non-convex optimization problems in ML. This work resulted in a publication that was awarded the INFORMS optimizaiton student prize for two students leading the project.&nbsp;</p> <p>Besides the scientific merit, this project helped support four graduate students, one of whom already graduated, and three others expected to successfully graduate soon.&nbsp;</p> <p>The PI and his team are grateful to the NSF for their generous funding of our research.&nbsp;</p> <p>&nbsp;</p> </div> </div> </div> <p>&nbsp;</p><br> <p>            Last Modified: 09/30/2018<br>      Modified by: Elad&nbsp;Hazan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The subject of this project are efficient optimization methods for machine learnign training. A special emphasis was on projection-free methods that are used to handle constraints.      The project was very successful, with numerous top-tier publications, including a scientific prize for a new optimization method that we will now describe.   Amongst the scienfic discoveries made, one stands out in particular. A main scietific outcome of this project is a new optimization algorithm based on Newton's method for machine learning. While Newton's method is itself an age old and well-studied algorithm, it is not directly applicable to training of machine learning models for two main reasons.  The first challenge is that ML problems are usually very large scale, and Newon's method, or methods that use the second derivative more generally, require maintaining and manupulating matrices of the dimension of the data, which is infeasible. The second difficulty comes from the stochastic nature of the problem: in ML we usually observe noisy data examples sequentially, and prefer a method that can make progress over each example rather than observing all training data at once, which is infeasible due to the amount of data.    We propose a novel randomized approach for linear time second-order methods that, using the special structure of objectives used in machine learning, avoid any matrix operations and run in linear time, while retaining all the theoretical properties of Newton?s method, giving the best known running times for several convex and non-convex optimization problems in ML. This work resulted in a publication that was awarded the INFORMS optimizaiton student prize for two students leading the project.   Besides the scientific merit, this project helped support four graduate students, one of whom already graduated, and three others expected to successfully graduate soon.   The PI and his team are grateful to the NSF for their generous funding of our research.                 Last Modified: 09/30/2018       Submitted by: Elad Hazan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
