<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>114649.00</AwardTotalIntnAmount>
<AwardAmount>114649</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families&lt;br/&gt;Swaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz&lt;br/&gt;&lt;br/&gt;Machine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms  are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that  are robust to outliers. &lt;br/&gt;&lt;br/&gt;The key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases.  For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. &lt;br/&gt;&lt;br/&gt;In partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http://learning.stat.purdue.edu/wiki/tentropy/start</AbstractNarration>
<MinAmdLetterDate>09/24/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/24/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1564765</AwardID>
<Investigator>
<FirstName>Vishwanathan</FirstName>
<LastName>Swaminathan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vishwanathan Swaminathan</PI_FULL_NAME>
<EmailAddress>vishy@ucsc.edu</EmailAddress>
<PI_PHON>7653375616</PI_PHON>
<NSF_ID>000517025</NSF_ID>
<StartDate>09/24/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Cruz</Name>
<CityName>Santa Cruz</CityName>
<ZipCode>950641077</ZipCode>
<PhoneNumber>8314595278</PhoneNumber>
<StreetAddress>1156 High Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>125084723</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA CRUZ</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Cruz]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>950641077</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~114649</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Minimizing convex loss functions is the holy grail of Machine Learning. A plethora of models based on this paradigm have been proposed over the past several decades. A fundamental problem with all such models is that they are not robust to outliers. In contrast, we explored building probabilistic models with a parametric family of distributions that have been proposed in statistical physics. This leads to loss functions that are quasi-convx and flatten out for misclassified points which are far away from the decision boundary. Consequenently, the models are robust to outliers. In our research we found ways to make the models based on these new distribution families practical. We applied them to some challenging real world problems including binary classification in the presence of noisy labels, as well as ranking and recommendation.&nbsp;We showed that&nbsp; our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 &times; 49,824, 519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation. We trained graduate students, submitted research articles to conferences, and also released open source code for all our algorithms.</p><br> <p>            Last Modified: 11/05/2018<br>      Modified by: Vishwanathan&nbsp;Swaminathan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Minimizing convex loss functions is the holy grail of Machine Learning. A plethora of models based on this paradigm have been proposed over the past several decades. A fundamental problem with all such models is that they are not robust to outliers. In contrast, we explored building probabilistic models with a parametric family of distributions that have been proposed in statistical physics. This leads to loss functions that are quasi-convx and flatten out for misclassified points which are far away from the decision boundary. Consequenently, the models are robust to outliers. In our research we found ways to make the models based on these new distribution families practical. We applied them to some challenging real world problems including binary classification in the presence of noisy labels, as well as ranking and recommendation. We showed that  our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 &times; 49,824, 519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation. We trained graduate students, submitted research articles to conferences, and also released open source code for all our algorithms.       Last Modified: 11/05/2018       Submitted by: Vishwanathan Swaminathan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
