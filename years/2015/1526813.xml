<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Reinforcement Learning in Partially Observable Multi-Agent Tasks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>247664.00</AwardTotalIntnAmount>
<AwardAmount>247664</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to develop new techniques for the design of robot controllers (programs that drive individual autonomous robots) for multi-robot tasks, i.e., tasks involving a collaborative team of robots. Existing techniques that have sound decision and game theoretic properties address simple multi-agent systems. In practice, robot controllers are still largely designed manually. Little is known about the decision theoretic optimality of such controllers, especially in multi-robot settings. This project aims to change that by bridging the gap between multi-agent decision theory and multi-robot control, via three main thrusts. Two of these thrusts seek to modify the ways by which multi-agent decision theory describes and computes these controllers, so as to be applicable to multi-robot tasks. The third thrust seeks to make the new computational approach scale to large robot teams. The project has mixed (theoretical and applied) scope, and will lay the foundation for more principled design of future multi-robot systems. The project will also have  broad educational impact. The research will be conducted in collaboration with a graduate and an undergraduate student, and involve hands-on robotics experience for students, as well as generate material for a new undergraduate course on robotics. Any theory and algorithms developed will be shared publicly.&lt;br/&gt;&lt;br/&gt;More specifically, the first main thrust of this project will investigate the expression and game theoretic optimization of behavior based controllers---a popular controller language in the robotics community. Non-linear programming techniques will be used for optimization of modular behaviors, that will be integrated in a hirerchical fashion from simpler to complex tasks. The second main thrust will utilize multi-agent reinforcement learning for the agents/robots to compute their own controllers via joint exploration in task simulations, exploiting inductive knowledge transfer to bootstrap the learning of complex behaviors from simpler, related ones. The third main thrust will develop a new paradigm of multi-agent reinforcement learning---Reinforcement Learning as a Rehearsal (RLaR). Agents will learn in a supervised setting with hidden as well as observable information to reduce sample complexity, but marginalize out the hidden features to yield controllers usable in partially observable settings. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/06/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/01/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1526813</AwardID>
<Investigator>
<FirstName>Bikramjit</FirstName>
<LastName>Banerjee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bikramjit Banerjee</PI_FULL_NAME>
<EmailAddress>Bikramjit.Banerjee@usm.edu</EmailAddress>
<PI_PHON>6012664119</PI_PHON>
<NSF_ID>000364847</NSF_ID>
<StartDate>08/06/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern Mississippi</Name>
<CityName>Hattiesburg</CityName>
<ZipCode>394015876</ZipCode>
<PhoneNumber>6012664119</PhoneNumber>
<StreetAddress>2609 WEST 4TH ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Mississippi</StateName>
<StateCode>MS</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MS04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>623335775</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN MISSISSIPPI, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>623113990</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern Mississippi]]></Name>
<CityName/>
<StateCode>MS</StateCode>
<ZipCode>394060001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Mississippi</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MS04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~85339</FUND_OBLG>
<FUND_OBLG>2016~162325</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has devised techniques to implement a new idea for reinforcement learning in a partially observable multi-agent/robot system, andconducted its systematic evaluation. The idea---called reinforcement learning as a rehearsal (RLaR)---is to allow the agents to perceivenormally hidden features in a simulated training scenario (like a rehearsal), but require them to learn decision strategies that can beexecuted in the actual task without requiring those hidden features. Principled techniques were designed to accomplish this, and theoreticalinsights underpinning the performance of the learners were formally proven, including convergence results under specificconditions. Research was also conducted to address associated questions, such as the representation of decision strategies, and the modelingof other agents. Specifically, behavior trees were investigated as a representation of decision strategies, and a reinforcement learningtechnique for behavior trees was designed. For modeling other agents, various inverse reinforcement learning techniques (model-free,incremental, multi-task), and new techniques for learning from demonstration by other agents, were designed and evaluated. Finally, weevaluated RLaR in a complex multi-robot task of foraging, and established its efficacy---chiefly speed-up of learning compared to regularreinforcement learning---as well as robustness to changes in the task settings.</p> <p>One major impact of this project was the development of an introductory course on robotics at Southern Miss. This course has been approvedby IHL, and offered 3 times recently. The base code for the course projects are currently being updated to work with the latest versions ofrelevant software (Ubuntu, ROS), in preparation for offering again in the near future. Other instructors have expressed interest indeveloping related courses, and this synergy may lead to the development of a new emphasis in Southern Miss's Computer Science curriculum inthe future.</p> <p>This project has provided partial support to 3 doctoral students at various stages of their research. One of these students completed hisdissertation in 2020 and acknowledged partial support from this grant in his thesis. Additionally, 2 successful MS theses (both students arewomen from minority communities) were entirely supported, and an undergraduate Honors thesis was partially supported. These theses alsoacknowledged this grant. All of these students have obtained employment immediately upon graduation. This project has yielded 2 papers inthe Neurocomputing (Elsevier) journal, 2 papers in the Knowledge Engineering Review (Cambridge Univ. Press) journal, and 4 conference papers(1 at AAAI, 2 at AAMAS, and 1 at IROS). Additionally, one journal paper has been accepted for publication in the Autonomous Agents andMulti-agent Systems (Springer) journal, while 2 conference papers and a journal paper are currently under review. Most of these papers wereco-authored by students as well. The PI has collaborated with CMANTIC lab at the University of Nebraska-Omaha, and THINC lab at theUniversity of Georgia, leading to joint publications; the PI has also served on doctoral committees of students at these labs.</p><br> <p>            Last Modified: 11/10/2020<br>      Modified by: Bikramjit&nbsp;Banerjee</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has devised techniques to implement a new idea for reinforcement learning in a partially observable multi-agent/robot system, andconducted its systematic evaluation. The idea---called reinforcement learning as a rehearsal (RLaR)---is to allow the agents to perceivenormally hidden features in a simulated training scenario (like a rehearsal), but require them to learn decision strategies that can beexecuted in the actual task without requiring those hidden features. Principled techniques were designed to accomplish this, and theoreticalinsights underpinning the performance of the learners were formally proven, including convergence results under specificconditions. Research was also conducted to address associated questions, such as the representation of decision strategies, and the modelingof other agents. Specifically, behavior trees were investigated as a representation of decision strategies, and a reinforcement learningtechnique for behavior trees was designed. For modeling other agents, various inverse reinforcement learning techniques (model-free,incremental, multi-task), and new techniques for learning from demonstration by other agents, were designed and evaluated. Finally, weevaluated RLaR in a complex multi-robot task of foraging, and established its efficacy---chiefly speed-up of learning compared to regularreinforcement learning---as well as robustness to changes in the task settings.  One major impact of this project was the development of an introductory course on robotics at Southern Miss. This course has been approvedby IHL, and offered 3 times recently. The base code for the course projects are currently being updated to work with the latest versions ofrelevant software (Ubuntu, ROS), in preparation for offering again in the near future. Other instructors have expressed interest indeveloping related courses, and this synergy may lead to the development of a new emphasis in Southern Miss's Computer Science curriculum inthe future.  This project has provided partial support to 3 doctoral students at various stages of their research. One of these students completed hisdissertation in 2020 and acknowledged partial support from this grant in his thesis. Additionally, 2 successful MS theses (both students arewomen from minority communities) were entirely supported, and an undergraduate Honors thesis was partially supported. These theses alsoacknowledged this grant. All of these students have obtained employment immediately upon graduation. This project has yielded 2 papers inthe Neurocomputing (Elsevier) journal, 2 papers in the Knowledge Engineering Review (Cambridge Univ. Press) journal, and 4 conference papers(1 at AAAI, 2 at AAMAS, and 1 at IROS). Additionally, one journal paper has been accepted for publication in the Autonomous Agents andMulti-agent Systems (Springer) journal, while 2 conference papers and a journal paper are currently under review. Most of these papers wereco-authored by students as well. The PI has collaborated with CMANTIC lab at the University of Nebraska-Omaha, and THINC lab at theUniversity of Georgia, leading to joint publications; the PI has also served on doctoral committees of students at these labs.       Last Modified: 11/10/2020       Submitted by: Bikramjit Banerjee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
