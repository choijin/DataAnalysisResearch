<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I/UCRC FRP:  Collaborative Research: Network-enabled Airborne Autonomy</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>79972.00</AwardTotalIntnAmount>
<AwardAmount>79972</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dmitri Perkins</SignBlockName>
<PO_EMAI>dperkins@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This award will develop foundations for autonomous airborne robotics, ultimately to be based on the cloud. This work has the potential to develop new frameworks and theories for distributed autonomy in networked vehicular networks/systems. The development of novel solutions for robust decision making in vehicular networks in conjunction with airborne autonomous systems has the potential to greatly improve the safety and quality of life of human users. &lt;br/&gt;&lt;br/&gt;Three research directions will be investigated over the duration of this project: (i) co-optimization of cyber-physical elements, (ii) autonomy tasks and (iii) cloud platform. The fundamental research will focus on the concrete example of unmanned aerial systems (UAS), whereby including all three research thrusts. This project aims to study airborne cloud-based autonomy in networked road surveillance networks, i.e., to perform complex tasks relating to perception, planning, and learning.</AbstractNarration>
<MinAmdLetterDate>09/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1535654</AwardID>
<Investigator>
<FirstName>Randal</FirstName>
<LastName>Beard</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Randal W Beard</PI_FULL_NAME>
<EmailAddress>beard@byu.edu</EmailAddress>
<PI_PHON>8014228392</PI_PHON>
<NSF_ID>000441886</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Timothy</FirstName>
<LastName>McLain</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Timothy W McLain</PI_FULL_NAME>
<EmailAddress>mclain@byu.edu</EmailAddress>
<PI_PHON>8014226537</PI_PHON>
<NSF_ID>000365157</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brigham Young University</Name>
<CityName>Provo</CityName>
<ZipCode>846021231</ZipCode>
<PhoneNumber>8014223360</PhoneNumber>
<StreetAddress>A-285 ASB</StreetAddress>
<StreetAddress2><![CDATA[Campus Drive]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009094012</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BRIGHAM YOUNG UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001940170</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brigham Young University]]></Name>
<CityName>Provo</CityName>
<StateCode>UT</StateCode>
<ZipCode>846021231</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramReference>
<Code>5761</Code>
<Text>INDUSTRY/UNIV COOP RES CENTERS</Text>
</ProgramReference>
<ProgramReference>
<Code>8039</Code>
<Text>Information, Communication &amp; Computing</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~79972</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This research focused on increasing the autonomy networked aircraft systems with two specific subprojects: (1) Cooperative navigation with a cloud-based shared map, and (2) Cooperative tracking of targets with real-time labeling. The accomplishments reported below are divided into these two categories.</p> <p><strong>Cooperative Navigation</strong></p> <p>While cloud robotics have been successfully demonstrated for ground robots, the fast dynamics of an MAV make it difficult to offload computation due to the inherent latencies to the cloud-connected server.&nbsp; The balance between processing done onboard and offboard was explored to determine how cloud resources could be leveraged safely and effectively on a MAV.</p> <p>We applied a relative navigation approach to cloud-connected MAVs in which MAVs estimated their pose relative to observed key frames, while a common global optimizer in the cloud solved for the global state of the robots given loop closures and GPS measurements.&nbsp; This method not only allowed for simpler integration of multiple agents, but it also guaranteed observability of all states for all MAVs in the network, and provided global consistency superior to that previously observed in cooperative MAV frameworks.</p> <p>Using this framework creates the potential for future MAVs connected only via a cloud server to build a map cooperatively in real time.&nbsp; This work showed that by leveraging the decoupling of the opportunistic global state estimator from the real-time local state estimation and control, it is possible to safely utilize cloud resources for cooperative autonomy between MAVs.</p> <p><strong>Cooperative Tracking</strong></p> <p>The primary tracking goal for this work was to augment the existing RRANSAC-based tracking algorithm with network-enabled resources to enhance autonomy and the user experience. Specifically, we showed that cloud resources could be leveraged to create a system that provided real-time labeling of tracked targets. This provided to the user a few descriptors for each object of what it may be. If at any point an object was not clearly visible to the user, a recent history of the labeling was available for reference. This is also helpful in the case of many objects that cannot all be observed real-time by the user. It is also of interest to demonstrate these abilities among multiple UAVs in coordination over network. Future goals include the design of machine learning algorithms used with tracked objects for the purpose of recognizing an object after it has been lost from view for an extended amount of time.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/20/2017<br>      Modified by: Timothy&nbsp;W&nbsp;Mclain</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This research focused on increasing the autonomy networked aircraft systems with two specific subprojects: (1) Cooperative navigation with a cloud-based shared map, and (2) Cooperative tracking of targets with real-time labeling. The accomplishments reported below are divided into these two categories.  Cooperative Navigation  While cloud robotics have been successfully demonstrated for ground robots, the fast dynamics of an MAV make it difficult to offload computation due to the inherent latencies to the cloud-connected server.  The balance between processing done onboard and offboard was explored to determine how cloud resources could be leveraged safely and effectively on a MAV.  We applied a relative navigation approach to cloud-connected MAVs in which MAVs estimated their pose relative to observed key frames, while a common global optimizer in the cloud solved for the global state of the robots given loop closures and GPS measurements.  This method not only allowed for simpler integration of multiple agents, but it also guaranteed observability of all states for all MAVs in the network, and provided global consistency superior to that previously observed in cooperative MAV frameworks.  Using this framework creates the potential for future MAVs connected only via a cloud server to build a map cooperatively in real time.  This work showed that by leveraging the decoupling of the opportunistic global state estimator from the real-time local state estimation and control, it is possible to safely utilize cloud resources for cooperative autonomy between MAVs.  Cooperative Tracking  The primary tracking goal for this work was to augment the existing RRANSAC-based tracking algorithm with network-enabled resources to enhance autonomy and the user experience. Specifically, we showed that cloud resources could be leveraged to create a system that provided real-time labeling of tracked targets. This provided to the user a few descriptors for each object of what it may be. If at any point an object was not clearly visible to the user, a recent history of the labeling was available for reference. This is also helpful in the case of many objects that cannot all be observed real-time by the user. It is also of interest to demonstrate these abilities among multiple UAVs in coordination over network. Future goals include the design of machine learning algorithms used with tracked objects for the purpose of recognizing an object after it has been lost from view for an extended amount of time.          Last Modified: 10/20/2017       Submitted by: Timothy W Mclain]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
