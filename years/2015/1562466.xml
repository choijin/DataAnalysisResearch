<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Optimization Algorithms for Decision Problems with Many Variables</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>278817.00</AwardTotalIntnAmount>
<AwardAmount>278817</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Georgia-Ann Klutke</SignBlockName>
<PO_EMAI>gaklutke@nsf.gov</PO_EMAI>
<PO_PHON>7032922443</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Decision-makers often strive to minimize the cost or maximize the performance of a system that depends on many decision variables. If the decision-maker can quantify the cost as a function of the decision variables, then computational methods can be used to obtain or approximate the optimal decision. For complicated cost functions arising in practice it may not be possible to know for sure that a proposed solution is optimal and one must settle for an approximate solution. Typical examples of such problems include choosing well sites and pumping rates for ground water pollution remediation, aligning medical images taken at different times, and determining the configuration of a collection of atoms that minimizes the potential energy. This award supports research into methods for solving such optimization problems and characterizing their inherent difficulty as the number of decision variables increases. These methods will be applicable to a broad range of problems in engineering, science, and industry.&lt;br/&gt;&lt;br/&gt;The optimization problems described above are called global optimization problems. It is well-known that global optimization is intractable in high dimensions in the worst-case complexity setting. The investigator will determine if continuous optimization is tractable in an asymptotic or average-case setting by establishing both upper and lower complexity bounds. The investigator will obtain upper complexity bounds by devising new optimization algorithms and proving their convergence rates. The project will use two approaches to algorithm design. One approach is to subdivide the domain into polytopes, and choose new function evaluation points within the polytope that maximize a criterion based on the size of the polytope and the observed function values at its vertices. The other approach is to use randomized point selection schemes that aim to obtain comparable results to the first approach, on average, but without the computational cost of maintaining the polyhedral subdivisions. The lower complexity bounds will establish the smallest error that can be obtained with any algorithm that uses a given average number of function evaluations. A key question that this research will attempt to answer is whether the lower complexity bounds grow exponentially with the dimension.</AbstractNarration>
<MinAmdLetterDate>03/27/2016</MinAmdLetterDate>
<MaxAmdLetterDate>03/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1562466</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Calvin</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James M Calvin</PI_FULL_NAME>
<EmailAddress>calvin@njit.edu</EmailAddress>
<PI_PHON>9735963378</PI_PHON>
<NSF_ID>000425803</NSF_ID>
<StartDate>03/27/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New Jersey Institute of Technology</Name>
<CityName>Newark</CityName>
<ZipCode>071021982</ZipCode>
<PhoneNumber>9735965275</PhoneNumber>
<StreetAddress>University Heights</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>075162990</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW JERSEY INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>075162990</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New Jersey Institute of Technology]]></Name>
<CityName>Newark</CityName>
<StateCode>NJ</StateCode>
<ZipCode>071021982</ZipCode>
<StreetAddress><![CDATA[University Heights]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>006Y</Code>
<Text>OE Operations Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>071E</Code>
<Text>MFG ENTERPRISE OPERATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>072E</Code>
<Text>NETWORKS &amp; QUEUING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>073E</Code>
<Text>OPTIMIZATION &amp; DECISION MAKING</Text>
</ProgramReference>
<ProgramReference>
<Code>077E</Code>
<Text>SIMULATION MODELS</Text>
</ProgramReference>
<ProgramReference>
<Code>078E</Code>
<Text>ENTERPRISE DESIGN &amp; LOGISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~278817</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Decision-makers often strive to minimize the cost or maximize the performance of a system that depends on many decision variables.&nbsp; If the decision-maker can express the cost as a function of the decision variables, then computer algorithms can be used to obtain or approximate the optimal decision. For complicated cost functions arising in practice local optimization (determining if a small change in the decision variables leads to an improvement) may be practical, while finding the globally optimal solution is difficult. In most cases arising in practice, it is not possible to know that the optimal solution has been obtained from knowing the function value at a finite set of points, and one must settle for an approximation.</p> <p>The principal outcome of this project was the development of an optimization algorithm for multivariate problems that can have many local optima.&nbsp; The algorithm converges to the globally optimal solution and the rate at which the error approaches zero was established.</p> <p><br />The principal investigator and PhD student Cuicui Zheng applied the algorithm to the problems of image registration and clustering.&nbsp; The image registration problem is to optimally align two images taken at different times or from different perspectives, for example medical images taken of the same patient at different times or satellite images produced at different times and positions.&nbsp; It is straight-forward to determine if a small perturbation of the image alignment will improve the similarity, but it is much more challenging to determine if there exists any (possibly large) change in the alignment that will lead to an optimal solution.&nbsp; Maximizing a similarity function of the images results in an optimization problem which is well-suited to our algorithm.</p> <p><br />The clustering problem is to group data into subgroups based on some notion of similarity. One approach is to choose central points and group the data by assigning each data point to the nearest center. The computational problem is then to determine where to place the centers.&nbsp; As with the image registration problem, it is easy to determine if a small perturbation in the centers gives an improvement but difficult to find the global optimal solution. Our algorithm converges to the global optimal solution, in contrast to widely used heuristics that converge to a local optimal solution.<br /><br />For many practical problems, like image registration and clustering, in order to be certain of the best solution all potential solutions must be computed and compared. Unless the parameter set is small, this enumeration approach is not feasible. In practice, instead of seeking the best solution with certainty, one might instead strive to obtain a solution with value near the optimal value with high probability. Then the optimizer chooses a relatively small subset of parameter values to evaluate, and proposes an approximation to the optimal solution that is close enough with high probability.&nbsp; The critical question is how to choose the small set of parameter values.</p> <p><br />An additional outcome of the project was the establishment of run-time bounds for a one-dimensional optimization algorithm in a probabilistic setting. Assuming that the function to be optimized is a random function from a certain class of Gaussian probability models, an upper bound on the expected number of function evaluations required to obtain a prescribed accuracy was established.<br /><br />Cuicui Zheng wrote several papers and completed her PhD thesis with the support of this grant.<br /><br /></p><br> <p>            Last Modified: 11/28/2020<br>      Modified by: James&nbsp;M&nbsp;Calvin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Decision-makers often strive to minimize the cost or maximize the performance of a system that depends on many decision variables.  If the decision-maker can express the cost as a function of the decision variables, then computer algorithms can be used to obtain or approximate the optimal decision. For complicated cost functions arising in practice local optimization (determining if a small change in the decision variables leads to an improvement) may be practical, while finding the globally optimal solution is difficult. In most cases arising in practice, it is not possible to know that the optimal solution has been obtained from knowing the function value at a finite set of points, and one must settle for an approximation.  The principal outcome of this project was the development of an optimization algorithm for multivariate problems that can have many local optima.  The algorithm converges to the globally optimal solution and the rate at which the error approaches zero was established.   The principal investigator and PhD student Cuicui Zheng applied the algorithm to the problems of image registration and clustering.  The image registration problem is to optimally align two images taken at different times or from different perspectives, for example medical images taken of the same patient at different times or satellite images produced at different times and positions.  It is straight-forward to determine if a small perturbation of the image alignment will improve the similarity, but it is much more challenging to determine if there exists any (possibly large) change in the alignment that will lead to an optimal solution.  Maximizing a similarity function of the images results in an optimization problem which is well-suited to our algorithm.   The clustering problem is to group data into subgroups based on some notion of similarity. One approach is to choose central points and group the data by assigning each data point to the nearest center. The computational problem is then to determine where to place the centers.  As with the image registration problem, it is easy to determine if a small perturbation in the centers gives an improvement but difficult to find the global optimal solution. Our algorithm converges to the global optimal solution, in contrast to widely used heuristics that converge to a local optimal solution.  For many practical problems, like image registration and clustering, in order to be certain of the best solution all potential solutions must be computed and compared. Unless the parameter set is small, this enumeration approach is not feasible. In practice, instead of seeking the best solution with certainty, one might instead strive to obtain a solution with value near the optimal value with high probability. Then the optimizer chooses a relatively small subset of parameter values to evaluate, and proposes an approximation to the optimal solution that is close enough with high probability.  The critical question is how to choose the small set of parameter values.   An additional outcome of the project was the establishment of run-time bounds for a one-dimensional optimization algorithm in a probabilistic setting. Assuming that the function to be optimized is a random function from a certain class of Gaussian probability models, an upper bound on the expected number of function evaluations required to obtain a prescribed accuracy was established.  Cuicui Zheng wrote several papers and completed her PhD thesis with the support of this grant.         Last Modified: 11/28/2020       Submitted by: James M Calvin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
