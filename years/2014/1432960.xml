<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I/UCRC FRP:  Collaborative Research: Autonomous Perception and Manipulation in Search and Rescue</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Thyagarajan Nandagopal</SignBlockName>
<PO_EMAI>tnandago@nsf.gov</PO_EMAI>
<PO_PHON>7032924550</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project wants to capitalize on a team of robots that move in order to better sense the environment and then perform basic manipulation tasks. The vision of the project is to integrate robots easily, provide human-team interfaces, and develop manipulation algorithms. The research will involve the development of perception strategies and manipulation schemes that will allow operation of the robot teams in real-world environments during search and rescue missions. In addition, this research will involve working in cluttered scenes where the lighting conditions may not be ideal. The project addresses: (i) Research on the novel problem of robotic perception and manipulation of target objects that interact with other objects as an integral part of the environment, which cannot be fully isolated in views and in physical arrangements before being manipulated; (ii) An appearance-based approach for recognition and pose estimation of 3D objects in cluttered scenes from a single view; (iii) Development of a measure of scene recognizability from each viewpoint to evaluate how accurately partially-occluded objects are recognized and how well their poses are estimated; (iv) Creation of solutions for disassembly analysis of 3D structures, extending our preliminary analysis of 2D structures; (v) Development of grasping based on the results of perception and with the aid of stability analysis of the arrangement of the objects and their interaction with the environment and with one another; and (vi) Experimental validation of the system in real-world settings, in close consultation with our industrial partners.&lt;br/&gt;&lt;br/&gt;This project will allow the creation of manipulation capabilities along with perception schemes to facilitate the development of a multi-robot team for search and rescue missions. The impacts of the project include: (i) Expansion of the annual robot summer camp to include robot-team activities with the objective of attracting middle-schoolers from under-represented groups to computer science/electrical engineering; (ii) Integration of the activities with first responders through the UPenn and UNC Charlotte collaborations; (ii) Experimental validation in SAFL at UMN and the Disaster City in TX; (iii) Offering project themes to REU undergraduates or to the UROP program; (iv) Outreach programs that involve demonstrations to local K-12 institutions; and (v) Inclusion of the project theme to the regular curricula.</AbstractNarration>
<MinAmdLetterDate>08/07/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1432960</AwardID>
<Investigator>
<FirstName>R. Vijay</FirstName>
<LastName>Kumar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>R. Vijay Kumar</PI_FULL_NAME>
<EmailAddress>Kumar@seas.upenn.edu</EmailAddress>
<PI_PHON>2158983630</PI_PHON>
<NSF_ID>000280506</NSF_ID>
<StartDate>08/07/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>08/07/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046389</ZipCode>
<StreetAddress><![CDATA[3330 Walnut Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramReference>
<Code>5761</Code>
<Text>INDUSTRY/UNIV COOP RES CENTERS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-c970db8d-0bdd-f3c2-b9b9-6021e0685747"> </span></p> <p dir="ltr"><strong>Intellectual Merit: </strong><span>We investigated the problem of recognizing an object using three fingers. Each touch creates several pressure points that can be mathematically described as one or more triangles. Each triangle is invariant to rotation and translation transformations. An object can be described as the probabilistic distribution of triangles arising from probing. An object class model is trained based on simulated probes of instances of the class represented as polygonal meshes. A linear classifier is learnt and at test phase the robot is building incrementally the representation. To minimize the number of actual probes during testing phase we employ a policy to find what is the best next touch that would minimize the misclassification over a finite horizon. </span></p> <p><strong>Broader Impact: </strong><span>Robots need to find and classify objects under any illumination or visibility conditions. This includes situations like search and rescue in the dark or in a cluttered environment where the robot has to reach underneath debris. Other scenarios where visual object recognition creates challenges is dealing with transparent and reflective surfaces where structured light or laser based techniques fail while classic cameras have difficulty to create the contrast in the appearance of these surfaces required for object recognition to work. Our new method can be applied with any robot equipped with two of three finger hands and thus enable operations under the above conditions. Our mathematical representation might also help psychophysicists studying tactile perception.</span></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/01/2016<br>      Modified by: R. Vijay&nbsp;Kumar</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1432960/1432960_10329744_1477668908043_TactileObject--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1432960/1432960_10329744_1477668908043_TactileObject--rgov-800width.jpg" title="Tactile object classification"><img src="/por/images/Reports/POR/2016/1432960/1432960_10329744_1477668908043_TactileObject--rgov-66x44.jpg" alt="Tactile object classification"></a> <div class="imageCaptionContainer"> <div class="imageCaption">In an experiment carried out on real robot, a three finger hand is probing a reflective object for the sake of object recognition. Reflections and transparencies create challenges for classic visual recognition.</div> <div class="imageCredit">Mabel Zhang</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">R. Vijay&nbsp;Kumar</div> <div class="imageTitle">Tactile object classification</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Intellectual Merit: We investigated the problem of recognizing an object using three fingers. Each touch creates several pressure points that can be mathematically described as one or more triangles. Each triangle is invariant to rotation and translation transformations. An object can be described as the probabilistic distribution of triangles arising from probing. An object class model is trained based on simulated probes of instances of the class represented as polygonal meshes. A linear classifier is learnt and at test phase the robot is building incrementally the representation. To minimize the number of actual probes during testing phase we employ a policy to find what is the best next touch that would minimize the misclassification over a finite horizon.   Broader Impact: Robots need to find and classify objects under any illumination or visibility conditions. This includes situations like search and rescue in the dark or in a cluttered environment where the robot has to reach underneath debris. Other scenarios where visual object recognition creates challenges is dealing with transparent and reflective surfaces where structured light or laser based techniques fail while classic cameras have difficulty to create the contrast in the appearance of these surfaces required for object recognition to work. Our new method can be applied with any robot equipped with two of three finger hands and thus enable operations under the above conditions. Our mathematical representation might also help psychophysicists studying tactile perception.             Last Modified: 11/01/2016       Submitted by: R. Vijay Kumar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
