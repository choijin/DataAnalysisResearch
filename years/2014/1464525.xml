<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Sparse Principal Component Analysis via the Sparsest Element in a Subspace</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>133789.00</AwardTotalIntnAmount>
<AwardAmount>133789</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Sparse principal component analysis (PCA) is a technique that allows biologists and other scientists to interpret experimental data in terms of very few variables. For example, it can help identify which among thousands of genes are important in distinguishing different types of cancer. In order for scientists and engineers to select the best algorithm for finding sparse principal components, it is important to have a theoretical understanding of the performance of many algorithms under a realistic data model. Most existing theoretical understanding focuses on the simple case where there is a single component that happens to be sparse. The proposed work will introduce a new model in which there are multiple components, of which one is sparse. For a special case of this more realistic model, the proposed work attempts to understand if there are any conditions under which sophisticated convex programs are provably better than very simple algorithms. Either outcome would be informative in helping researchers decide between the many algorithms for sparse PCA. &lt;br/&gt; &lt;br/&gt;In this project, sparse PCA will be studied from the perspective of finding the sparsest element in a subspace. This perspective is motivated by a multispike data model, which the PI calls a sparse-dense model. Under this model, the infinite data limit of sparse PCA becomes the sparsest element problem, which is nontrivial. The objective of this research is to understand the computational-statistical tradeoff in finding the sparsest element in a subspace under the sparse-dense model. The PI would like to determine if there is a scaling gap between the information theoretic limit and the best performance by a computationally efficient algorithm. Ultimately, we would like to understand when sophisticated convex methods are provably better than simple thresholding methods. This objective will be explored by semidefinite relaxations, polynomial optimization, and reductions to the planted clique problem.</AbstractNarration>
<MinAmdLetterDate>10/30/2014</MinAmdLetterDate>
<MaxAmdLetterDate>10/30/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1464525</AwardID>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Hand</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Hand</PI_FULL_NAME>
<EmailAddress>p.hand@northeastern.edu</EmailAddress>
<PI_PHON>6265904727</PI_PHON>
<NSF_ID>000576228</NSF_ID>
<StartDate>10/30/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>HOUSTON</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 MAIN ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~133789</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Intellectual Merit:&nbsp; Under this grant, the PI has explored the role of sparsity and principal component analysis in a wide range of problem areas, ranging from computer vision to machine learning to X-ray crystallography.&nbsp; The PI developed new algorithms for computer vision based on the idea of sparsity.&nbsp; These ideas were the first of their kind that enjoyed a rigorous theoretical performance guarantee.&nbsp; The PI also developed new approaches to the phase retrieval problem from X-ray crystallography.&nbsp; These approaches were the first to show that if a proper initialization was provided, then signal recovery would occur with the optimal scaling of the number of measurements.&nbsp; The PI also developed new approaches to the phase retrieval problem that does not involve the idea of sparsity.&nbsp; In these approaches, the signal recovery can occur using recently invented tools from deep learning and machine learning.&nbsp; The PI was the first to prove that such methods can succeed with the the optimal scaling of the number of measurements.&nbsp; This work provides a strong theoretical justification that sparsity based methods may not be the best way to respresent the naturalness of natural signals.&nbsp; This work will likely lead to similar advances in a variety of related scientific contexts.&nbsp; This work also provides theoretical advances in machine learning that may be useful in a broad variety of applications.</p> <p>Broader Impacts:&nbsp; During the course of this grant, the PI has given science outreach talks; has been the director of curriculum and instruction for a summer STEM camp for 200 underrepresented 8th-12th grade students in Texas and over 60 K12 educators undergoing professional development; and has been the director of the same STEM camp the following sunner for over 200 students and 100 K12 educators.&nbsp; The PI also maintains a mathematics website that has received over 1 million page views.</p><br> <p>            Last Modified: 11/15/2018<br>      Modified by: Paul&nbsp;Hand</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit:  Under this grant, the PI has explored the role of sparsity and principal component analysis in a wide range of problem areas, ranging from computer vision to machine learning to X-ray crystallography.  The PI developed new algorithms for computer vision based on the idea of sparsity.  These ideas were the first of their kind that enjoyed a rigorous theoretical performance guarantee.  The PI also developed new approaches to the phase retrieval problem from X-ray crystallography.  These approaches were the first to show that if a proper initialization was provided, then signal recovery would occur with the optimal scaling of the number of measurements.  The PI also developed new approaches to the phase retrieval problem that does not involve the idea of sparsity.  In these approaches, the signal recovery can occur using recently invented tools from deep learning and machine learning.  The PI was the first to prove that such methods can succeed with the the optimal scaling of the number of measurements.  This work provides a strong theoretical justification that sparsity based methods may not be the best way to respresent the naturalness of natural signals.  This work will likely lead to similar advances in a variety of related scientific contexts.  This work also provides theoretical advances in machine learning that may be useful in a broad variety of applications.  Broader Impacts:  During the course of this grant, the PI has given science outreach talks; has been the director of curriculum and instruction for a summer STEM camp for 200 underrepresented 8th-12th grade students in Texas and over 60 K12 educators undergoing professional development; and has been the director of the same STEM camp the following sunner for over 200 students and 100 K12 educators.  The PI also maintains a mathematics website that has received over 1 million page views.       Last Modified: 11/15/2018       Submitted by: Paul Hand]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
