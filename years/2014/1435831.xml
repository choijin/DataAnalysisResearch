<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Effects of production variability on the acoustic consequences of coordinated articulatory gestures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>32973.00</AwardTotalIntnAmount>
<AwardAmount>32973</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The human voice, our oldest and most reliable communication tool, is now rapidly becoming the input interface of choice that we use everyday to interact with technologies such as car navigation systems, medical  and legal dictation systems, personal assistants like "Siri," automated financial systems, etc.  Thousands of 'apps' have been developed to help consumers use voice to get the information they are looking for.   Speech recognition is the backbone of all of these technologies.  As a result, the performance of speech recognizers is key for customer satisfaction.  Currently, many systems still need to be tuned for a particular speaker to perform well, and the recognition task has be limited in other ways such as requiring (1) usage of a specific vocabulary,  (2) clear pronunciation of most of the words, especially the content words and (3) limited background noise. In this research, speech variability will be studied, and methods and models will be developed that will enable recognizers to be more speaker independent and capable of handling the full range of speech styles from clear articulation to very casually spoken speech. The results will also bear on linguistic models of speech planning and organization, providing evidence for how speakers trade off efficiencies in the production of speech against the need to be intelligible.&lt;br/&gt;&lt;br/&gt;In this project, point source tracking of the speech articulators will be collected concurrently with the corresponding acoustics.  Speakers will record speech at both a normal and rapid pace (the purpose of the latter is to increase significantly the degree of variability in the signal).  This data will allow for the investigation of whether speakers always move their speech articulators in the direction of a desired target (e.g. tongue tip to teeth in producing /t/) even when a rapid production pace occludes the relevant acoustic information (as in "perfect").  If confirmed, this finding will point the way towards making recognition systems more robust through the incorporation of articulatory information.  In addition, such data will support the development of a speech inversion system capable of 'uncovering' hidden articulatory movements potentially masked from the acoustics.</AbstractNarration>
<MinAmdLetterDate>08/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1435831</AwardID>
<Investigator>
<FirstName>Vikramjit</FirstName>
<LastName>Mitra</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vikramjit Mitra</PI_FULL_NAME>
<EmailAddress>vmitra@speech.sri.com</EmailAddress>
<PI_PHON>6508593571</PI_PHON>
<NSF_ID>000602622</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SRI International</Name>
<CityName>Menlo Park</CityName>
<ZipCode>940253493</ZipCode>
<PhoneNumber>7032478529</PhoneNumber>
<StreetAddress>333 RAVENSWOOD AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009232752</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SRI INTERNATIONAL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009232752</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SRI International]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>940253493</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~32973</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project resulted in creation of a dataset consisting of acoustic signals and articulatory measurements from a male and a female speaker speaking at slow, normal and fast rate. This data were used to train speaker-specific models of speech inversion. In addition a large synthetic dataset (consisting of acoustic and tract variable trajectories (or, articulatory constriction-level information)) was also created that consisted of multiple speakers speaking at different rates. Several machine learning approaches (based on deep neural networks) were explored to train speech inversion models that would take acoustic features as input and produce articulatory information as outputs. Results from these explorations were shared with the research community through several publications. The speech inversion models trained on synthetic data were used for speech recognition tasks, where articulatory features were fed along with standard spectral features to train and test speech recognition models and a consistent gain in performance was observed compared to the baseline models trained with standard acoustic features only.</p> <p>This work supported the thesis work of a doctoral student, who received formal training in research and development on signal processing techniques, speech recognition tools and machine learning concepts. The outcomes of the works have been shared with the research community through conference/meeting presentation and posters. The data produced in this work will be shared with the research community through an online website. The work also resulted in novel acoustic modeling architectures, and such architectures have demonstrated reasonable performance gains with respect to strong baseline systems.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/12/2016<br>      Modified by: Vikramjit&nbsp;Mitra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project resulted in creation of a dataset consisting of acoustic signals and articulatory measurements from a male and a female speaker speaking at slow, normal and fast rate. This data were used to train speaker-specific models of speech inversion. In addition a large synthetic dataset (consisting of acoustic and tract variable trajectories (or, articulatory constriction-level information)) was also created that consisted of multiple speakers speaking at different rates. Several machine learning approaches (based on deep neural networks) were explored to train speech inversion models that would take acoustic features as input and produce articulatory information as outputs. Results from these explorations were shared with the research community through several publications. The speech inversion models trained on synthetic data were used for speech recognition tasks, where articulatory features were fed along with standard spectral features to train and test speech recognition models and a consistent gain in performance was observed compared to the baseline models trained with standard acoustic features only.  This work supported the thesis work of a doctoral student, who received formal training in research and development on signal processing techniques, speech recognition tools and machine learning concepts. The outcomes of the works have been shared with the research community through conference/meeting presentation and posters. The data produced in this work will be shared with the research community through an online website. The work also resulted in novel acoustic modeling architectures, and such architectures have demonstrated reasonable performance gains with respect to strong baseline systems.          Last Modified: 10/12/2016       Submitted by: Vikramjit Mitra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
