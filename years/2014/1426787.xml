<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Shall I Touch This?: Navigating the Look and Feel of Complex Surfaces</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>408000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project improves autonomous robotic perception so that future co-robots can glance around any scene and accurately estimate how it would feel to grasp or step on all of the visible surfaces.  Just as people do, robots should use such these physical predictions to guide their interactions with the world, for example avoiding dangerous ice patches on the ground when walking and driving, and adeptly anticipating the grasp force needed to pick up everything from ice cubes to stuffed animals. These research activities are accompanied by significant outreach efforts, including a new program on "Look and Touch Robotics" to get middle-school students, particularly those from under­represented groups, excited about computer science, engineering, and robotics. This program uses simple experiments to highlight the dual importance of visual and haptic information during interactions with physical objects, along with demonstrations of a robot showing visuo-­haptic intelligence.  This project also integrates research and education by involving undergraduates in the research and via hands-on projects in the vision and robotics classes taught by the Principal Investigators.&lt;br/&gt;&lt;br/&gt;This research involves extensive collection of data from real objects and surfaces using both visual and haptic sensors.  The recorded interactions are analyzed to uncover visual clues that can allow a robot to infer the physical characteristics of the surface, such as slipperiness, hardness, and roughness.  This problem is addressed using deep learning, a recently developed approach that has been successful in enabling robots to visually recognize a wide variety of objects in diverse circumstances.  The research team also builds the database of visuo-haptic recordings and the learned cross-modal sensory, and makes it available to other robotics researchers at the end of the project.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>04/29/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1426787</AwardID>
<Investigator>
<FirstName>Katherine</FirstName>
<LastName>Kuchenbecker</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katherine J Kuchenbecker</PI_FULL_NAME>
<EmailAddress>kuchenbe@seas.upenn.edu</EmailAddress>
<PI_PHON>2155732786</PI_PHON>
<NSF_ID>000465871</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046315</ZipCode>
<StreetAddress><![CDATA[220 S. 33rd Stret]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~400000</FUND_OBLG>
<FUND_OBLG>2015~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>When you gaze around a furniture store, you know that a polished white-and-gray marble table will feel slippery, smooth, hard, and cool. In contrast, stroking a shaggy green carpet will require more effort and will feel rough, soft, and relatively warm. In short, you can estimate the mechanical properties of surfaces you've never touched just by looking at them. We call this skill "feeling with your eyes" - it is important for effectively interacting with the physical world, such as deciding where to step and planning how firmly to grasp an object.</span></p> <p><span>Today's robots have many skills that can help humans, but they cannot feel with their eyes; this deficit limits how well robots can physically interact with the world. This research project sought to determine whether it is possible to mathematically model the relationship between how surfaces look and how they feel by gathering and then analyzing a large dataset. Importantly, we needed high quality haptic (touch) and visual measurements from interactions with a wide range of real surfaces, but such a database did not yet exist.</span></p> <p><span>The first step in this project was to create a tool for collecting the desired dataset: we call this tool the Proton Pack. To ensure diverse good data, we chose to create a portable sensing system that the experimenter carries around the world and uses to touch interesting surfaces. The Proton&rsquo;s main part has a tall L shape that you hold in your hands. Three different tips can be attached to the end, one of which is a hard metal sphere, while the other two are softer and include tactile sensing. The Proton also contains a force/torque sensor, high-bandwidth accelerometers, an inertial measurement unit, a camera, and a depth sensor.&nbsp; The operator wears a backpack that contains the batteries and the computer that runs the Proton. They place a C-shaped frame around the chosen surface and use a smart-phone interface to start and stop data collection. All of these sensors were carefully calibrated against ground-truth data to make sure we could trust the measurements.</span></p> <p><span>The second step in this project was collecting the dataset. Our researchers used the Proton to collect haptic and visual data from more than 1000 unique combinations of tool tip and surface.&nbsp; This total includes either one, two, or three tool tips interacting with 418 unique surfaces, including everything from polished marble to shaggy carpet. For each interaction, the experimenter used the chosen tool tip to tap on and drag across the surface of interest with a range of speeds and forces. They also entered the identity of the surface, where they found it, and how it felt (slipperiness, roughness, hardness, and warmth). Each recording is about 60 seconds long. Our software processes the recording to calculate how the operator moved the tool tip at every time step (based on the camera&rsquo;s view of the C-shaped frame), as well as the pressing and friction forces between the tool tip and the surface. We plan to release this dataset for use by others in the near future.</span></p> <p><span>The third step in this project was analyzing the collected data. To check the difficulty of this task, we had humans look at pictures of 100 surfaces from our database and rate their slipperiness, roughness, hardness, and warmth. These answers were somewhat similar to the operator&rsquo;s ratings, particularly for roughness and hardness, but people disagreed a lot, showing that this single-image rating task is probably more difficult than making the same judgment in everyday life. With these human ratings in hand, we calculated important properties of each surface from the recorded haptic data. We then created a hybrid algorithm using a deep neural network and a support vector machine to try to predict the operator&rsquo;s ratings and the haptic properties from images of surfaces that had not been used to train the algorithm. The results were significantly better than chance, showing that it is possible for robots to feel with their eyes. Further performance improvements are likely possible using a larger data set and/or different analytical techniques.</span></p> <p><span>The intellectual merit of this project lies in the study of accurate visual and haptic data gathered during physical interactions, as well as the prediction of how a previously unexperienced surface will feel based on past experiences of other surfaces. Generalizing such judgments across modalities is an ambitious and important research goal.</span></p> <p><span>The broader impacts of this project are threefold. First, future robots may interact more effectively with the world by understanding how all the objects around them will feel to touch.&nbsp; Second, more than ten students took part in open-ended investigation as part of this project.&nbsp; Third, dozens of young people had a chance to learn about engineering through our related outreach efforts.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/01/2018<br>      Modified by: Katherine&nbsp;J&nbsp;Kuchenbecker</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538381733497_datacollection--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538381733497_datacollection--rgov-800width.jpg" title="Experimenter collecting data from a surface, smartphone GUI, and end-effector with accelerometers"><img src="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538381733497_datacollection--rgov-66x44.jpg" alt="Experimenter collecting data from a surface, smartphone GUI, and end-effector with accelerometers"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An experimenter holds the Proton Pack and captures data from a tabletop.  The C-shaped frame enables camera-based self-motion tracking.  Data collection is controlled using a smartphone.  Each Proton end-effector includes accelerometers to measure contact vibrations.</div> <div class="imageCredit">Alex Burka and Katherine J. Kuchenbecker</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Katherine&nbsp;J&nbsp;Kuchenbecker</div> <div class="imageTitle">Experimenter collecting data from a surface, smartphone GUI, and end-effector with accelerometers</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538381911934_ProtonPack--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538381911934_ProtonPack--rgov-800width.jpg" title="Components of the Proton Pack"><img src="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538381911934_ProtonPack--rgov-66x44.jpg" alt="Components of the Proton Pack"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The custom Proton Pack sensing instrument includes a depth camera, an RGB camera, three interchangeable end-effectors, an inertial measurement unit (IMU), a microphone, two high-bandwidth accelerometers, and a force/torque sensor.</div> <div class="imageCredit">Alex Burka and Katherine J. Kuchenbecker</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Katherine&nbsp;J&nbsp;Kuchenbecker</div> <div class="imageTitle">Components of the Proton Pack</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538382052144_surfaces--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538382052144_surfaces--rgov-800width.jpg" title="Ten of the 418 surfaces in our database"><img src="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538382052144_surfaces--rgov-66x44.jpg" alt="Ten of the 418 surfaces in our database"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Images of ten of the 418 surfaces in our database, as seen from the camera on the Proton Pack.  Note the end-effector near the bottom of each image and parts of the C-shaped frame occasionally visible at the side.  Each surface has a unique appearance and unique feel.</div> <div class="imageCredit">Alex Burka and Katherine J. Kuchenbecker</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Katherine&nbsp;J&nbsp;Kuchenbecker</div> <div class="imageTitle">Ten of the 418 surfaces in our database</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538382830473_data--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538382830473_data--rgov-800width.jpg" title="Motion trajectory plus associated position, force, and acceleration data over time"><img src="/por/images/Reports/POR/2018/1426787/1426787_10334030_1538382830473_data--rgov-66x44.jpg" alt="Motion trajectory plus associated position, force, and acceleration data over time"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Sample data from a typical surface recording.  The majority of the data is planar dragging, with several taps on the surface.  The color on the left plots shows the progression of time from cyan to pink.  The plots on the right show tool tip position, force, and acceleration over time.</div> <div class="imageCredit">Alex Burka and Katherine J. Kuchenbecker</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Katherine&nbsp;J&nbsp;Kuchenbecker</div> <div class="imageTitle">Motion trajectory plus associated position, force, and acceleration data over time</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ When you gaze around a furniture store, you know that a polished white-and-gray marble table will feel slippery, smooth, hard, and cool. In contrast, stroking a shaggy green carpet will require more effort and will feel rough, soft, and relatively warm. In short, you can estimate the mechanical properties of surfaces you've never touched just by looking at them. We call this skill "feeling with your eyes" - it is important for effectively interacting with the physical world, such as deciding where to step and planning how firmly to grasp an object.  Today's robots have many skills that can help humans, but they cannot feel with their eyes; this deficit limits how well robots can physically interact with the world. This research project sought to determine whether it is possible to mathematically model the relationship between how surfaces look and how they feel by gathering and then analyzing a large dataset. Importantly, we needed high quality haptic (touch) and visual measurements from interactions with a wide range of real surfaces, but such a database did not yet exist.  The first step in this project was to create a tool for collecting the desired dataset: we call this tool the Proton Pack. To ensure diverse good data, we chose to create a portable sensing system that the experimenter carries around the world and uses to touch interesting surfaces. The Proton?s main part has a tall L shape that you hold in your hands. Three different tips can be attached to the end, one of which is a hard metal sphere, while the other two are softer and include tactile sensing. The Proton also contains a force/torque sensor, high-bandwidth accelerometers, an inertial measurement unit, a camera, and a depth sensor.  The operator wears a backpack that contains the batteries and the computer that runs the Proton. They place a C-shaped frame around the chosen surface and use a smart-phone interface to start and stop data collection. All of these sensors were carefully calibrated against ground-truth data to make sure we could trust the measurements.  The second step in this project was collecting the dataset. Our researchers used the Proton to collect haptic and visual data from more than 1000 unique combinations of tool tip and surface.  This total includes either one, two, or three tool tips interacting with 418 unique surfaces, including everything from polished marble to shaggy carpet. For each interaction, the experimenter used the chosen tool tip to tap on and drag across the surface of interest with a range of speeds and forces. They also entered the identity of the surface, where they found it, and how it felt (slipperiness, roughness, hardness, and warmth). Each recording is about 60 seconds long. Our software processes the recording to calculate how the operator moved the tool tip at every time step (based on the camera?s view of the C-shaped frame), as well as the pressing and friction forces between the tool tip and the surface. We plan to release this dataset for use by others in the near future.  The third step in this project was analyzing the collected data. To check the difficulty of this task, we had humans look at pictures of 100 surfaces from our database and rate their slipperiness, roughness, hardness, and warmth. These answers were somewhat similar to the operator?s ratings, particularly for roughness and hardness, but people disagreed a lot, showing that this single-image rating task is probably more difficult than making the same judgment in everyday life. With these human ratings in hand, we calculated important properties of each surface from the recorded haptic data. We then created a hybrid algorithm using a deep neural network and a support vector machine to try to predict the operator?s ratings and the haptic properties from images of surfaces that had not been used to train the algorithm. The results were significantly better than chance, showing that it is possible for robots to feel with their eyes. Further performance improvements are likely possible using a larger data set and/or different analytical techniques.  The intellectual merit of this project lies in the study of accurate visual and haptic data gathered during physical interactions, as well as the prediction of how a previously unexperienced surface will feel based on past experiences of other surfaces. Generalizing such judgments across modalities is an ambitious and important research goal.  The broader impacts of this project are threefold. First, future robots may interact more effectively with the world by understanding how all the objects around them will feel to touch.  Second, more than ten students took part in open-ended investigation as part of this project.  Third, dozens of young people had a chance to learn about engineering through our related outreach efforts.          Last Modified: 10/01/2018       Submitted by: Katherine J Kuchenbecker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
