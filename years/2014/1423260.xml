<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Advanced Design Principles for Computer Simulated Agents</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>497618.00</AwardTotalIntnAmount>
<AwardAmount>545618</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will investigate human interaction with simulated agents in situations where humans expect empathic communication, such as healthcare.  In domains such as this, the person interacts with computer-based interventions (CBI) as an education process evolves.  Almost all psychosocial interventions currently available on the web are delivered merely via text.  Research indicates that many users will lose interest and drop out, although completion is critical to achieving desired goals.  Human-computer interaction literature suggests that interacting with simulated agents can increase the user's engagement, but that the user will expect social competence when interacting with them.  This project will answer a set of research questions for the design of simulated characters with some empathic intelligence, to create a new modality for the delivery of CBIs.  The specific health-related application area will facilitate development and evaluation of new techniques and design principles, that will have much wider applicability, potentially whenever people interact with computer-based systems through simulated agents. &lt;br/&gt;&lt;br/&gt;This research will advance the ability of computer scientists to create competent simulated agents that can adapt their verbal and non-verbal behavior to the user's affective states, and over time tailor their interaction to the specific user to produce the maximum positive impact in terms of users' engagement and achievement of their goals.  The model of empathy and social competence developed will enable simulated characters to adapt in real time to a user's short-lived emotions over a single interactive session.  Longer-term affective states will be modeled over long-term interaction via follow-up sessions with the same individual.  Thus, development of rapport between a human and an artificial intelligence is a dynamic process over time, and the research is expected to discover new principles that might not be seen in exclusively short-term interactions. The project will provide: (1) a scheme to design tailored interaction by constructing a dynamic user-model with user's demographic information and fluctuating personal characteristics; (2) a model of empathic verbal communication built by combining motivational interviewing techniques with an ontology-based dialog system; (3) a computational model for the integration of verbal and non-verbal communication cues by adapting the character's facial expressions, vocal modulation, and kinesics to its verbal utterances in the context of the session.  In addition, the research will engage scientific questions about diversity in communication style across human groups, facilitated by the fact that the project is housed at the region's principal minority-serving research university.  The fact that the research will involve students in the development of systems that integrate multiple technologies will give them an excellent educational experience, gaining competence that will be valuable for a range of future careers.</AbstractNarration>
<MinAmdLetterDate>08/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>12/17/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423260</AwardID>
<Investigator>
<FirstName>Christine</FirstName>
<LastName>Lisetti</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christine L Lisetti</PI_FULL_NAME>
<EmailAddress>lisetti@cis.fiu.edu</EmailAddress>
<PI_PHON>3053486242</PI_PHON>
<NSF_ID>000482807</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Williams</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Williams</PI_FULL_NAME>
<EmailAddress>mlwillia@fiu.edu</EmailAddress>
<PI_PHON>3053482494</PI_PHON>
<NSF_ID>000663777</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate>12/17/2019</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Maya</FirstName>
<LastName>Boustani</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maya M Boustani</PI_FULL_NAME>
<EmailAddress>mboustani@llu.edu</EmailAddress>
<PI_PHON>9095587680</PI_PHON>
<NSF_ID>000801339</NSF_ID>
<StartDate>12/17/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Florida International University</Name>
<CityName>Miami</CityName>
<ZipCode>331990001</ZipCode>
<PhoneNumber>3053482494</PhoneNumber>
<StreetAddress>11200 SW 8TH ST</StreetAddress>
<StreetAddress2><![CDATA[MARC 430]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL26</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>071298814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FLORIDA INTERNATIONAL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Florida International University]]></Name>
<CityName/>
<StateCode>FL</StateCode>
<ZipCode>331990001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL26</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~148680</FUND_OBLG>
<FUND_OBLG>2015~170914</FUND_OBLG>
<FUND_OBLG>2016~210024</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our Empathic Embodied Virtual Agent (EEVA) project investigated human interactions with simulated agents, which are&nbsp;digital human-looking characters&nbsp;able to interact using the main communication channels that humans innately use to communicate.&nbsp;&nbsp;We studied how the role of empathy and&nbsp;rapport,&nbsp;expressed with multiple social cues (facial expressions, body gestures, and spoken utterances), can be simulated on digital agents.</p> <p>&nbsp;</p> <p>Whereas establishing rapport is important for any successful communication , clinical psychologists become experts at establishing rapport with patients during their training.&nbsp;&nbsp;We therefore&nbsp;identified counseling as a good testing domain&nbsp;for our work on &lsquo;digital rapport&rsquo;.&nbsp;&nbsp;Our tenet is that, if our rapport-enabled agents are well accepted in as sensitive a context as discussing one&rsquo;s at-risk behaviors, they will be defacto well accepted in other, less sensitive contexts (e.g. intelligent digital tutor, digital museum guide).</p> <p>&nbsp;</p> <p>So we developed computational models of rapport based on knowledge we derived from&nbsp;videos of therapy sessions we recorded between a licensed clinical psychologist and patients.&nbsp;&nbsp;We annotated the videos in terms of what social cue(s) the psychologist expressed with what word, and used these annotations to create models of rapport that either (1) use hand-crafted rules to control the agent&rsquo;s messages, or that (2) derives such rules from processing these annotated videos with machine learning AI algorithms.</p> <p>&nbsp;</p> <p>Our approach was further constrained by our goal to model human-agent rapport in conversations conducted over the&nbsp;Internet, which complicates matters significantly.&nbsp;&nbsp;Indeed, our project is one of the very few to attempt to (1) provide interactions with a socially interactive agent online, with (2) real-time processing of the user&rsquo;s social cues and utterances, and (3) corresponding real-time adaptation of the agent&rsquo;s social cues along with its spoken utterances.</p> <p>&nbsp;</p> <p>The project culminated in the design, implementation, and deployment of&nbsp;EEVA, programmed to deliver the content of&nbsp;behavior change intervention previously proven effective for heavy drinkers, using our models of rapport.&nbsp;&nbsp;The result is that users can now access EEVA website, 24/7&nbsp;anytime anywhere,&nbsp;where a rapport-enabled agent asks them about their alcohol consumption, and helps them to find ways to reduce their abusive consumption.</p> <p>To evaluate our EEVA agent, we compared how heavy drinkers rate the acceptability and usefulness of the same behavior change intervention delivered by EEVA versus delivered with a text only user interface.&nbsp;&nbsp;Our results show that both eEVA and text-only interventions are able to reduce alcohol use, decrease alcohol-related problems and consequences, and decrease depression. Both programs are highly acceptable, easy to use, and engaging. However, when surveyed, 82% of psychology trainees (future behavioral health providers) stated that they would recommend eEVA (live health assistant) over a text interface to their clients.&nbsp;</p> <p>Our results therefore have significant societal impact&nbsp;because&nbsp;<strong>we have shown that we can provide an effective health intervention over the internet delivered by a digital health rapport-enabled agent that can&nbsp;</strong><strong>significantly reduce alcohol use, consequences of alcohol use and depression in just one session</strong><em><strong>.&nbsp;</strong></em>This is a timely finding as our country and the world grapple with a pandemic that has made it more challenging than ever to access behavioral health care, as a result of shelter-in-place orders. EEVA provides an alternative form of screening and brief treatment for alcohol users that is&nbsp;<em>engaging</em>,&nbsp;<em>acceptable</em>, and&nbsp;<em>effective</em>.&nbsp;</p> <p>In addition, because EEVA can be adapted to different target behaviors (e.g. eating disorder, drug use, medication adherence), EEVA removes a number of barriers to access treatment including&nbsp;availability of trained providers, proximity of providers, access to transportation, affordability, insurance coverage, scheduling, and stigma. Individuals living in rural settings or in poverty &ndash; where alcohol abuse is more prominent - are disproportionately impacted by these barriers. In rural settings in particular, anonymity is more difficult and increases stigma around help-seeking. Lack of flexible scheduling options for individuals who work full-time further exacerbate barriers to treatment. &nbsp;</p> <p>Given that it is possible to program the agent to speak in multiple languages, it could reduce language barriers for minority and refugee populations. Already with EEVA, consumers can pick a digital health agent from a library of diverse options (gender, age, race and ethnicity) as shown in the image. This is an exciting development given the lack of a diverse mental health workforce.</p> <p>Finally EEVA can be adapted to interact in various contexts and in different roles where empathic communication and rapport are important&nbsp;such as in the role of intelligent digital tutor, digital museum guide, among others.</p> <p>We furthermore&nbsp;created software tools for prototyping digital agent&rsquo;s verbal and non-verbal cues,&nbsp;&nbsp;which can help the design of new digital agents used in different contexts, for testing social communication and emotion theories, or to teach students in computer science or psychology about the subtleties of the human face and body language in social communication, and about how to simulate these subtle social messages for different contexts.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/29/2020<br>      Modified by: Christine&nbsp;L&nbsp;Lisetti</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1423260/1423260_10335277_1609200115792_CounselorChoice-EEVA-wLogo2--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1423260/1423260_10335277_1609200115792_CounselorChoice-EEVA-wLogo2--rgov-800width.jpg" title="EEVA Diverse Empathic Digital Agents"><img src="/por/images/Reports/POR/2020/1423260/1423260_10335277_1609200115792_CounselorChoice-EEVA-wLogo2--rgov-66x44.jpg" alt="EEVA Diverse Empathic Digital Agents"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our EEVA website starts by asking the user to choose which agent to interact with from our collection of 25 diverse digital agents.</div> <div class="imageCredit">FIU VISAGE Lab</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Christine&nbsp;L&nbsp;Lisetti</div> <div class="imageTitle">EEVA Diverse Empathic Digital Agents</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our Empathic Embodied Virtual Agent (EEVA) project investigated human interactions with simulated agents, which are digital human-looking characters able to interact using the main communication channels that humans innately use to communicate.  We studied how the role of empathy and rapport, expressed with multiple social cues (facial expressions, body gestures, and spoken utterances), can be simulated on digital agents.     Whereas establishing rapport is important for any successful communication , clinical psychologists become experts at establishing rapport with patients during their training.  We therefore identified counseling as a good testing domain for our work on ‘digital rapport’.  Our tenet is that, if our rapport-enabled agents are well accepted in as sensitive a context as discussing one’s at-risk behaviors, they will be defacto well accepted in other, less sensitive contexts (e.g. intelligent digital tutor, digital museum guide).     So we developed computational models of rapport based on knowledge we derived from videos of therapy sessions we recorded between a licensed clinical psychologist and patients.  We annotated the videos in terms of what social cue(s) the psychologist expressed with what word, and used these annotations to create models of rapport that either (1) use hand-crafted rules to control the agent’s messages, or that (2) derives such rules from processing these annotated videos with machine learning AI algorithms.     Our approach was further constrained by our goal to model human-agent rapport in conversations conducted over the Internet, which complicates matters significantly.  Indeed, our project is one of the very few to attempt to (1) provide interactions with a socially interactive agent online, with (2) real-time processing of the user’s social cues and utterances, and (3) corresponding real-time adaptation of the agent’s social cues along with its spoken utterances.     The project culminated in the design, implementation, and deployment of EEVA, programmed to deliver the content of behavior change intervention previously proven effective for heavy drinkers, using our models of rapport.  The result is that users can now access EEVA website, 24/7 anytime anywhere, where a rapport-enabled agent asks them about their alcohol consumption, and helps them to find ways to reduce their abusive consumption.  To evaluate our EEVA agent, we compared how heavy drinkers rate the acceptability and usefulness of the same behavior change intervention delivered by EEVA versus delivered with a text only user interface.  Our results show that both eEVA and text-only interventions are able to reduce alcohol use, decrease alcohol-related problems and consequences, and decrease depression. Both programs are highly acceptable, easy to use, and engaging. However, when surveyed, 82% of psychology trainees (future behavioral health providers) stated that they would recommend eEVA (live health assistant) over a text interface to their clients.   Our results therefore have significant societal impact because we have shown that we can provide an effective health intervention over the internet delivered by a digital health rapport-enabled agent that can significantly reduce alcohol use, consequences of alcohol use and depression in just one session. This is a timely finding as our country and the world grapple with a pandemic that has made it more challenging than ever to access behavioral health care, as a result of shelter-in-place orders. EEVA provides an alternative form of screening and brief treatment for alcohol users that is engaging, acceptable, and effective.   In addition, because EEVA can be adapted to different target behaviors (e.g. eating disorder, drug use, medication adherence), EEVA removes a number of barriers to access treatment including availability of trained providers, proximity of providers, access to transportation, affordability, insurance coverage, scheduling, and stigma. Individuals living in rural settings or in poverty &ndash; where alcohol abuse is more prominent - are disproportionately impacted by these barriers. In rural settings in particular, anonymity is more difficult and increases stigma around help-seeking. Lack of flexible scheduling options for individuals who work full-time further exacerbate barriers to treatment.    Given that it is possible to program the agent to speak in multiple languages, it could reduce language barriers for minority and refugee populations. Already with EEVA, consumers can pick a digital health agent from a library of diverse options (gender, age, race and ethnicity) as shown in the image. This is an exciting development given the lack of a diverse mental health workforce.  Finally EEVA can be adapted to interact in various contexts and in different roles where empathic communication and rapport are important such as in the role of intelligent digital tutor, digital museum guide, among others.  We furthermore created software tools for prototyping digital agent’s verbal and non-verbal cues,  which can help the design of new digital agents used in different contexts, for testing social communication and emotion theories, or to teach students in computer science or psychology about the subtleties of the human face and body language in social communication, and about how to simulate these subtle social messages for different contexts.              Last Modified: 12/29/2020       Submitted by: Christine L Lisetti]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
