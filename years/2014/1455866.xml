<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Sensorimotor control of hand-object interactions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>360000.00</AwardTotalIntnAmount>
<AwardAmount>380000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>How people use their hands to interact with objects is one of the most complex and least understood sensorimotor skills. For example, when people pick up the same object multiple times they use different fingertip forces to compensate for differences in their fingertip positions. This compensation is important because it allows people to manipulate an object skillfully without having always to grasp it at the exact same points. Several important questions remain: What is the relative contribution of individual sensory modalities, such as touch and vision, in enabling the compensatory modulation of fingertip force and position? Are fingertip forces and positions represented separately by the central nervous system? What are the mechanisms underlying their generalization to different limbs or tasks? These questions represent a major gap in our understanding of how the central nervous system learns, plans, and executes complex motor behaviors. An  understanding of how sensory modalities are seamlessly integrated to enable the development and implementation of high-level internal representations of hand-object interactions should inspire the design of more dexterous robotic manipulators endowed with sensory feedback, improve neuroprosthetics, and aid development of advanced bioengineering research tools to quantify biological control mechanisms.&lt;br/&gt;&lt;br/&gt;The overall goal of this collaborative research is to elucidate the mechanisms responsible for building high-level representations of hand-object interactions accounting for end-effector position and force. The aims are 1) to quantify the mechanisms underlying the weighting of sensorimotor integration during hand-object interactions and 2) to determine the principles underlying generalization of a learned hand-object interaction to a new context. The investigators will test three hypotheses: 1) the weighting of different sensory modalities is time dependent according to its role within a given task epoch; 2) co-variation between end-effector force and position is a general feature of hand-object interactions, independent of the effectors used (fingertip, whole hand, or two-hands); and 3) generalization of learned hand-object interactions is sensitive to the frame of reference in which they were learned.</AbstractNarration>
<MinAmdLetterDate>04/09/2015</MinAmdLetterDate>
<MaxAmdLetterDate>07/25/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1455866</AwardID>
<Investigator>
<FirstName>Marco</FirstName>
<LastName>Santello</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marco Santello</PI_FULL_NAME>
<EmailAddress>marco.santello@asu.edu</EmailAddress>
<PI_PHON>4809658279</PI_PHON>
<NSF_ID>000379548</NSF_ID>
<StartDate>04/09/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<StreetAddress2><![CDATA[660 South Mill Avenue, Suite 310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>943360412</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ARIZONA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>806345658</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Arizona State University]]></Name>
<CityName>Tempe</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852876011</ZipCode>
<StreetAddress><![CDATA[P.O. Box 876011]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~360000</FUND_OBLG>
<FUND_OBLG>2016~20000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our project was designed to address the mechanisms underlying dexterous manipulation using tasks that resembles every day scenarios, such as lifting a glass full of water without spilling its content. In our tasks, we introduced a novel approach whereby participants can choose where to grasp an object prior to manipulating it.&nbsp;</p> <p><strong>Intellectual Merits.&nbsp;</strong>Unlike previous work constraining contact points, our experimental approach led to several discoveries: (1) to compensate for variability in thumb and index fingertip distance that occurs every time the same object is grasped, participants change digit forces as a function of digit position ? this phenomenon ensures correct performance of dexterous manipulation; (2) the digit force-to-position modulation is a general feature of hand-object interactions independent of whether participants use two digits, five digits, or two hands; (3) digit force-to-position modulation can still occur even when participants cannot see their hand, although the process is slower than when vision is available; (4) whether dexterous manipulation learned in a given context can help performing manipulation in a novel context depends on whether the two contexts share similar features such as the position of the object relative to the hand.&nbsp;</p> <p><strong>Broader Impacts.&nbsp;</strong>Our work has contributed to our understanding of how the brain can use feedback from the hand to rapidly and seamlessly change digit forces every time we interact with objects. This work has inspired parallel projects for the design of more dexterous artificial manipulators endowed with sensory feedback, human-human and human-machine interactions, and neuroprosthetics. This research has accelerated ongoing work in the investigators? laboratories aimed at improving the human condition by understanding the mechanisms responsible movement disorders and the development and testing of rehabilitation protocols. Lastly, the projects have enhanced our rich learning environments for integrating research and education.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/20/2019<br>      Modified by: Marco&nbsp;Santello</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our project was designed to address the mechanisms underlying dexterous manipulation using tasks that resembles every day scenarios, such as lifting a glass full of water without spilling its content. In our tasks, we introduced a novel approach whereby participants can choose where to grasp an object prior to manipulating it.   Intellectual Merits. Unlike previous work constraining contact points, our experimental approach led to several discoveries: (1) to compensate for variability in thumb and index fingertip distance that occurs every time the same object is grasped, participants change digit forces as a function of digit position ? this phenomenon ensures correct performance of dexterous manipulation; (2) the digit force-to-position modulation is a general feature of hand-object interactions independent of whether participants use two digits, five digits, or two hands; (3) digit force-to-position modulation can still occur even when participants cannot see their hand, although the process is slower than when vision is available; (4) whether dexterous manipulation learned in a given context can help performing manipulation in a novel context depends on whether the two contexts share similar features such as the position of the object relative to the hand.   Broader Impacts. Our work has contributed to our understanding of how the brain can use feedback from the hand to rapidly and seamlessly change digit forces every time we interact with objects. This work has inspired parallel projects for the design of more dexterous artificial manipulators endowed with sensory feedback, human-human and human-machine interactions, and neuroprosthetics. This research has accelerated ongoing work in the investigators? laboratories aimed at improving the human condition by understanding the mechanisms responsible movement disorders and the development and testing of rehabilitation protocols. Lastly, the projects have enhanced our rich learning environments for integrating research and education.          Last Modified: 11/20/2019       Submitted by: Marco Santello]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
