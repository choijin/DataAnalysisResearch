<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: EAGER: Collaborative Research: Mapping Software Analysis Problems to Efficient and Accurate Constraints</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>74999.00</AwardTotalIntnAmount>
<AwardAmount>74999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nina Amla</SignBlockName>
<PO_EMAI>namla@nsf.gov</PO_EMAI>
<PO_PHON>7032927991</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Techniques for finding faults in software systems, such as crashes, security vulnerabilities, and deadlocks, have become increasingly powerful over the past two decades.  This is due in no small part to the development of efficient automated satisfiability solvers. The interest in applying these solvers to an ever wider class of software analysis applications has pushed solvers to their limits. As a result, analysis developers are currently forced to approximate analysis?s queries to make use of existing solvers.  Because of this software analyses can mistakenly diagnose an error, miss reporting a true error, and suffer unnecessarily poor performance. This research seeks to establish accuracy as an important missing dimension of solver support and its success will lead to broader and more cost-effective use of solvers to produce high-quality software.&lt;br/&gt;&lt;br/&gt;This project is the first to systematically explore and link the accuracy requirements of a software analysis to the accuracy provided by a solver. This project does this by exploring approaches to specify the accuracy requirements of solver clients and detect, recover and report solution accuracy for integer and string constraints.  These capabilities are being implemented in an existing solver interface framework, called Green, which is applied to perform symbolic execution of Java programs, using Symbolic Pathfinder.  The project will evaluate the extent to this approach simplifies client analysis development, enables clients to use a variety of solvers - even those that do not perfectly match accuracy requirements, and improves analysis performance.</AbstractNarration>
<MinAmdLetterDate>07/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/10/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1449626</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Dwyer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Dwyer</PI_FULL_NAME>
<EmailAddress>md3cn@virginia.edu</EmailAddress>
<PI_PHON>4349247604</PI_PHON>
<NSF_ID>000103915</NSF_ID>
<StartDate>07/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName/>
<StateCode>NE</StateCode>
<ZipCode>685880115</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8206</Code>
<Text>Formal Methods and Verification</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~74999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project explored the use of constraint solving techniques as a basis for modern program analyses. &nbsp; The work on this project led to two findings. &nbsp;</p> <p>&nbsp;</p> <p>First, there is a class of program analyses, called data flow analyses, that are widely used both to find defects in software systems and to improve the efficiency of their execution. &nbsp;Historically, these data flow analyses have been hand-crafted for the most part. &nbsp;Slow progress over the past three decades has allowed for parts of their implementation to be synthesized automatically. &nbsp; In the past decade, the problem of fully automating their implementation has been proposed. &nbsp;In this project, we took another step in automating the synthesis of efficient and correct-by-construction data flow analyses for an important class of problems.</p> <p>Second, there have been two important trends in the past decade in program analysis. &nbsp;Increasingly researchers are building scalable infrastructure that permits the analysis of large-scale software systems and then sharing access to that infrastructure in the hopes of accelerating research progress. &nbsp; At the same time the research community has required a substantially higher burden of evidence that a proposed technique actually improves on the state of the art. &nbsp;In principle, these are both positive trends. &nbsp; In this project we studied the challenges created by these trends. &nbsp;Specifically, that researchers tend to reuse infrastructure "as is" and in doing so they may accidentally bias their research &nbsp;results in ways that are hard to detect, but which may lead them to draw false conclusions about the benefits of a proposed technique. &nbsp; Our study of the Klee symbolic execution infrastructure helps to remind the community of the degree of care that must be taken to control for confounding factors when experimenting with program analysis techniques that build on largscale infrastructure.</p><br> <p>            Last Modified: 09/30/2016<br>      Modified by: Matthew&nbsp;Dwyer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project explored the use of constraint solving techniques as a basis for modern program analyses.   The work on this project led to two findings.       First, there is a class of program analyses, called data flow analyses, that are widely used both to find defects in software systems and to improve the efficiency of their execution.  Historically, these data flow analyses have been hand-crafted for the most part.  Slow progress over the past three decades has allowed for parts of their implementation to be synthesized automatically.   In the past decade, the problem of fully automating their implementation has been proposed.  In this project, we took another step in automating the synthesis of efficient and correct-by-construction data flow analyses for an important class of problems.  Second, there have been two important trends in the past decade in program analysis.  Increasingly researchers are building scalable infrastructure that permits the analysis of large-scale software systems and then sharing access to that infrastructure in the hopes of accelerating research progress.   At the same time the research community has required a substantially higher burden of evidence that a proposed technique actually improves on the state of the art.  In principle, these are both positive trends.   In this project we studied the challenges created by these trends.  Specifically, that researchers tend to reuse infrastructure "as is" and in doing so they may accidentally bias their research  results in ways that are hard to detect, but which may lead them to draw false conclusions about the benefits of a proposed technique.   Our study of the Klee symbolic execution infrastructure helps to remind the community of the degree of care that must be taken to control for confounding factors when experimenting with program analysis techniques that build on largscale infrastructure.       Last Modified: 09/30/2016       Submitted by: Matthew Dwyer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
