<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: Doing More With Less: Cost-Effective Infrastructure for Automotive Vision Capabilities</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>1000000.00</AwardTotalIntnAmount>
<AwardAmount>1046850</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sankar Basu</SignBlockName>
<PO_EMAI>sabasu@nsf.gov</PO_EMAI>
<PO_PHON>7032927843</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many safety-critical cyber-physical systems rely on advanced sensing capabilities to react to changing environmental conditions. One such domain is automotive systems. In this domain, a proliferation of advanced sensor technology is being fueled by an expanding range of autonomous capabilities (blind spot warnings, automatic lane-keeping, etc.). The limit of this expansion is full autonomy, which has been demonstrated in various one-off prototypes, but at the expensive of significant hardware over-provisioning that is not tenable for a consumer product. To enable features approaching full autonomy in a commercial vehicle, software infrastructure will be required that enables multiple sensor-processing streams to be multiplexed onto a common hardware platform at reasonable cost.  This project is directed at the development of such infrastructure.&lt;br/&gt;&lt;br/&gt;The desired infrastructure will be developed by focusing on a particularly compelling challenge problem: enabling cost-effective driver-assist and autonomous-control automotive features that utilize vision-based sensing through cameras. This problem will be studied by (i) examining numerous multicore-based hardware configurations at various fixed price points based on realistic automotive use cases, and by (ii) characterizing the range of vision-based workloads that can be feasibly supported using the software infrastructure to be developed. The research to be conducted will be a collaboration involving academic researchers at UNC and engineers at General Motors Research. The collaborative nature of this effort increases the likelihood that the results obtained will have real impact in the U.S. automotive industry. Additionally, this project is expected to produce new open-source software and tools, new course content, public outreach through participation in UNC's demo program, and lectures and seminars by the investigators at national and international forums.</AbstractNarration>
<MinAmdLetterDate>09/09/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1446631</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Anderson</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James H Anderson</PI_FULL_NAME>
<EmailAddress>anderson@cs.unc.edu</EmailAddress>
<PI_PHON>9195906057</PI_PHON>
<NSF_ID>000481767</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shige</FirstName>
<LastName>Wang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shige Wang</PI_FULL_NAME>
<EmailAddress>shige.wang@gm.com</EmailAddress>
<PI_PHON>7346268922</PI_PHON>
<NSF_ID>000525202</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Berg</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander C Berg</PI_FULL_NAME>
<EmailAddress>aberg@cs.unc.edu</EmailAddress>
<PI_PHON>6464607401</PI_PHON>
<NSF_ID>000569050</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sanjoy</FirstName>
<LastName>Baruah</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sanjoy K Baruah</PI_FULL_NAME>
<EmailAddress>Baruah@wustl.edu</EmailAddress>
<PI_PHON>3149357546</PI_PHON>
<NSF_ID>000762080</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S. Columbia St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7918</Code>
<Text>CPS-Cyber-Physical Systems</Text>
</ProgramElement>
<ProgramElement>
<Code>O318</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>8235</Code>
<Text>CPS-Synergy</Text>
</ProgramReference>
<ProgramReference>
<Code>8237</Code>
<Text>CISE Interagency Agreements</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~1000000</FUND_OBLG>
<FUND_OBLG>2015~46850</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In mass-market vehicles today, semi-autonomous features such as automatic lane keeping, adaptive cruise control, etc., are becoming common.&nbsp; In the coming years, such features are expected to evolve to provide ever more sophisticated driver-assistance features.&nbsp; The hoped-for culmination of this evolution is full autonomy, which will entail endowing automobiles with "thinking capabilities" that enable them to react to complex situations in a timely fashion.&nbsp; Introducing such capabilities in a cost-effective way in mass-market vehicles remains a lofty goal that will likely take many years to achieve.</p> <p><br />At present, full autonomy has been realized only in one-off prototype vehicles.&nbsp; The most press-worthy example of such a vehicle is the Google Car.&nbsp; In these one-off prototypes, autonomy is achieved by equipping the vehicle with a number of computers and sensing devices, at considerable monetary expense.&nbsp; For example, the computing and sensing infrastructure in (at least one version of) the Google Car reportedly cost over $150,000.&nbsp; While this is not a significant expense for Google, it certainly would be for a typical consumer.</p> <p><br />This project was directed at the development of computational infrastructure for realizing autonomous features in vehicles at monetary cost levels that are acceptable for mass-market vehicles.&nbsp; The specific focus of the project was to support real-time computer-vision programs that use cameras as sensors.&nbsp; Cameras are relatively cheap and are commonly used in mass-market vehicles today to provide semi-autonomous features such as automatic lane keeping and adaptive cruise control.&nbsp; Most of the challenge problems investigated in the project pertained to scenarios where multiple computer-vision programs, corresponding to multiple image streams from multiple cameras, were executed on a common hardware platform.&nbsp; Using a single hardware platform is much more economical than devoting separate hardware to each stream, as done in many expensive prototypes.&nbsp; The various hardware platforms that were considered were all multicore platforms that use graphics processing units (GPUs) to accelerate mathematical computations that are common in autonomous driving.&nbsp; A multicore platform has several processing "cores" that can execute different programs, or parts of the same program, in parallel.</p> <p><br />The main intellectual contributions of this project were twofold.&nbsp; First, a range of new methods was developed that enable computer-vision programs to exploit the significant parallelism available on multicore+GPU platforms.&nbsp; Second, new analytical results were produced that enable response-time bounds for computer-vision programs to be certified.&nbsp; This analysis might be used, for example, to certify that any obstacle in the road is detected in enough time to ensure that the vehicle has time to respond appropriately.</p> <p><br />In terms of broader impacts, the investigators presented talks on this work at numerous institutions, conferences, workshops, etc.&nbsp; Additionally, the results of this project formed the basis of the Ph.D. dissertations of three graduate students.&nbsp; Three undergraduate honors theses were also produced under this project.&nbsp; Some of the results from the project were also applied in automotive systems at General Motors as part of summer internship positions undertaken by one of the supported graduate students.&nbsp; A small-scale autonomous car was also developed and exhibited at various open-house demo events at UNC.</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/12/2019<br>      Modified by: James&nbsp;H&nbsp;Anderson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In mass-market vehicles today, semi-autonomous features such as automatic lane keeping, adaptive cruise control, etc., are becoming common.  In the coming years, such features are expected to evolve to provide ever more sophisticated driver-assistance features.  The hoped-for culmination of this evolution is full autonomy, which will entail endowing automobiles with "thinking capabilities" that enable them to react to complex situations in a timely fashion.  Introducing such capabilities in a cost-effective way in mass-market vehicles remains a lofty goal that will likely take many years to achieve.   At present, full autonomy has been realized only in one-off prototype vehicles.  The most press-worthy example of such a vehicle is the Google Car.  In these one-off prototypes, autonomy is achieved by equipping the vehicle with a number of computers and sensing devices, at considerable monetary expense.  For example, the computing and sensing infrastructure in (at least one version of) the Google Car reportedly cost over $150,000.  While this is not a significant expense for Google, it certainly would be for a typical consumer.   This project was directed at the development of computational infrastructure for realizing autonomous features in vehicles at monetary cost levels that are acceptable for mass-market vehicles.  The specific focus of the project was to support real-time computer-vision programs that use cameras as sensors.  Cameras are relatively cheap and are commonly used in mass-market vehicles today to provide semi-autonomous features such as automatic lane keeping and adaptive cruise control.  Most of the challenge problems investigated in the project pertained to scenarios where multiple computer-vision programs, corresponding to multiple image streams from multiple cameras, were executed on a common hardware platform.  Using a single hardware platform is much more economical than devoting separate hardware to each stream, as done in many expensive prototypes.  The various hardware platforms that were considered were all multicore platforms that use graphics processing units (GPUs) to accelerate mathematical computations that are common in autonomous driving.  A multicore platform has several processing "cores" that can execute different programs, or parts of the same program, in parallel.   The main intellectual contributions of this project were twofold.  First, a range of new methods was developed that enable computer-vision programs to exploit the significant parallelism available on multicore+GPU platforms.  Second, new analytical results were produced that enable response-time bounds for computer-vision programs to be certified.  This analysis might be used, for example, to certify that any obstacle in the road is detected in enough time to ensure that the vehicle has time to respond appropriately.   In terms of broader impacts, the investigators presented talks on this work at numerous institutions, conferences, workshops, etc.  Additionally, the results of this project formed the basis of the Ph.D. dissertations of three graduate students.  Three undergraduate honors theses were also produced under this project.  Some of the results from the project were also applied in automotive systems at General Motors as part of summer internship positions undertaken by one of the supported graduate students.  A small-scale autonomous car was also developed and exhibited at various open-house demo events at UNC.          Last Modified: 02/12/2019       Submitted by: James H Anderson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
