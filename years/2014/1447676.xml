<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: F: DKA: Collaborative Research: Theory and Algorithms for Parallel Probabilistic Inference with Big Data, via Big Model, in Realistic Distributed Computing Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops a new framework that enables machine learning (ML) systems to automatically comprehend and mine massive and complex data via parallel Bayesian inference on large computer clusters. The research has a profound impact on the practice and direction of Big Learning. The developed technologies have a catalytic effect on both ML research and applications: ML scientists are able to rapidly experiment on novel, cutting-edge ML models with minimal programming effort, unhindered by the limitations of single machines. Researchers from other fields, like biology and social sciences, are able to run contemporary advanced ML methods that transcend the capabilities of simple models, yielding new scientific insights on data whose size would otherwise be daunting. Data scientists at small start-ups are able to conduct ML analytics with complex models, putting their capabilities on par with huge companies possessing dedicated engineering and infrastructure teams. Students and beginners are able to witness distributed ML in action with just a few lines of code, driving ML education to new heights. &lt;br/&gt;&lt;br/&gt;Technically, this research focuses on scaling up and parallelizing Bayesian machine learning, which provides a powerful, elegant and theoretically justified framework for modeling a wide variety of datasets.  The research team develops a suite of complementary distributed inference algorithms for hierarchical Bayesian models, which cover most commonly used Bayesian ML methods. The project focuses on combining speed and scalability with theoretical guarantees that allow us to assess the accuracy of the resulting methods, and allow practitioners to make trade-offs between speed and accuracy. Rather than focus on a few disconnected models, the project develops techniques applicable to a broad spectrum of hierarchical Bayesian models, resulting in a toolkit of building blocks that can be combined as needed for arbitrary probabilistic models - be they parametric or nonparametric, discriminative or generative. This is in contrast to much existing work on parallel inference, which tends to focus on parallelization in a specific model and cannot be easily extended. The project provides a solid algorithmic foundation for learning on Big Data with powerful models. The research contributes to democratizing advanced and large-scale ML methods for broad applications, by offering the user and developer community a library of general-purpose parallelizable algorithms for working on diverse problems using computer clusters and the cloud, bridging the gap between practical needs from data and basic research in ML.</AbstractNarration>
<MinAmdLetterDate>08/25/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1447676</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Xing</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric P Xing</PI_FULL_NAME>
<EmailAddress>epxing@cs.cmu.edu</EmailAddress>
<PI_PHON>4122682559</PI_PHON>
<NSF_ID>000195787</NSF_ID>
<StartDate>08/25/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-4cd9e2ad-7fff-729d-dedf-0e7dc1a3eb43"> <p dir="ltr"><span>Automatic search and construction of neural network architectures with Bayesian Optimization:</span></p> <p dir="ltr"><span>We have developed NASBOT, a Bayesian optimization framework for neural architecture search. NASBOT finds better architectures for MLPs and CNNs more efficiently than other baselines on several datasets. A key contribution of this work is the efficiently computable OTMANN distance for neural network architectures, which may be of independent interest as it might find applications outside of Bayesian optimization. Our code for NASBOT and OTMANN are &nbsp;made publically available.</span></p> <br /> <p dir="ltr"><span>A Modularized, Versatile, and Extensible Toolkit for Text Generation:</span></p> <p dir="ltr"><span>The key achievement of our work is developing a text generation toolkit that is designed to be versatile to support the broad set of applications and algorithms, to be modularized to enable easy replacement of components, and to be extensible to allow seamless integration of any external modules. The toolkit is open-source so that it can empower the community to accelerate technique development in text generation and beyond.</span></p> <br /> <p dir="ltr"><span>Continuous optimization for Bayesian network structure learning:</span></p> <p dir="ltr"><span>We developed a new approach for score-based learning of DAGs by converting the traditional combinatorial optimization problem into an equivalent continuous program, which eliminates the need for specialized algorithms that are tailored to search over the combinatorial space of DAGs. Instead, we are able to leverage standard numerical algorithms for smooth constrained problems, which makes implementation particularly easy, not requiring any knowledge about graphical models. This is similar in spirit to the situation for undirected graphical models, in which the formulation of a convex log-det program sparked a series of remarkable advances in structure learning for undirected graphs. </span></p> <br /> <p dir="ltr"><span>Learning Pipelines with Limited Data and Domain Knowledge:</span></p> <p dir="ltr"><span>We have developed a system Nuts&amp;Bolts, that learns &nbsp;a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules. It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data. Nuts&amp;Bolts achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. Nuts&amp;Bolts can also be used to achieve improvements on a relation extraction task and on the end task of answering Newtonian physics problems.</span></p> <br /> <p dir="ltr"><span>The above results were published in leading machine learning conferences, and were presented in Dr. Xing&rsquo;s keynotes and tutorials at various conferences.</span></p> </span></p><br> <p>            Last Modified: 04/29/2019<br>      Modified by: Eric&nbsp;P&nbsp;Xing</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Automatic search and construction of neural network architectures with Bayesian Optimization: We have developed NASBOT, a Bayesian optimization framework for neural architecture search. NASBOT finds better architectures for MLPs and CNNs more efficiently than other baselines on several datasets. A key contribution of this work is the efficiently computable OTMANN distance for neural network architectures, which may be of independent interest as it might find applications outside of Bayesian optimization. Our code for NASBOT and OTMANN are  made publically available.   A Modularized, Versatile, and Extensible Toolkit for Text Generation: The key achievement of our work is developing a text generation toolkit that is designed to be versatile to support the broad set of applications and algorithms, to be modularized to enable easy replacement of components, and to be extensible to allow seamless integration of any external modules. The toolkit is open-source so that it can empower the community to accelerate technique development in text generation and beyond.   Continuous optimization for Bayesian network structure learning: We developed a new approach for score-based learning of DAGs by converting the traditional combinatorial optimization problem into an equivalent continuous program, which eliminates the need for specialized algorithms that are tailored to search over the combinatorial space of DAGs. Instead, we are able to leverage standard numerical algorithms for smooth constrained problems, which makes implementation particularly easy, not requiring any knowledge about graphical models. This is similar in spirit to the situation for undirected graphical models, in which the formulation of a convex log-det program sparked a series of remarkable advances in structure learning for undirected graphs.    Learning Pipelines with Limited Data and Domain Knowledge: We have developed a system Nuts&amp;Bolts, that learns  a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules. It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data. Nuts&amp;Bolts achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. Nuts&amp;Bolts can also be used to achieve improvements on a relation extraction task and on the end task of answering Newtonian physics problems.   The above results were published in leading machine learning conferences, and were presented in Dr. Xing?s keynotes and tutorials at various conferences.        Last Modified: 04/29/2019       Submitted by: Eric P Xing]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
