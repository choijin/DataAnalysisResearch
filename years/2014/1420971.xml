<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Robust Interactive Audio Source Separation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>498736.00</AwardTotalIntnAmount>
<AwardAmount>514261</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Algorithms to separate audio sources have many potential uses such as to extract important audio data from historic recordings or to help people with hearing impairments select what to amplify and what to suppress in their hearing aids. Computer processing of audio content can potentially be used to isolate the sound sources of interest and to improve the audio clarity any time that the content exhibits interference from multiple sound sources, such as to extract a single voice of interest from a room full of voices. However, current sound source identification and separation methods are only reliable when there is a single predominant sound. This project will develop the science and technology that is needed to more easily isolate a single sound source from audio content with multiple competing sources, and that is needed to build interactive computer systems that will guide users though an interactive source separation process, to permit the separation and recombining of sound sources in a manner that is beyond the reach of existing audio software. The outcomes of the project will improve the possibility of speech recognition in environments with multiple talkers, will be useful for many scientific inquiries such as in biodiversity monitoring through the automated analysis of field recordings, and will be broadly useful any time that manual tagging of audio data is not practical.&lt;br/&gt;&lt;br/&gt;While many computational auditory scene analysis algorithms have been proposed to separate audio scenes into individual sources, current methods are brittle and difficult to use and as a result have not been broadly adopted by potential users. The methods are brittle in that each algorithm relies on a single cue to separate sources and if the cue is not reliable then the method fails. The methods are difficult to use because the algorithms cannot predict which audio scenes any specific algorithm is likely to work on, and so the user does not know which method to apply in any given case. They are also difficult to use because their control parameters are hard to understand for users who lack expertise in signal processing. This project will research how to integrate multiple source separation algorithms into a single framework, and how to improve the ease of use by exploring interfaces that permit users to interactively define what they wish to isolate in audio scenes, and that permit systems to provide users with guidance on selecting a tool and setting the necessary parameters. The project will produce an open-source audio source separation tool that embodies these scientific research outcomes.</AbstractNarration>
<MinAmdLetterDate>08/01/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1420971</AwardID>
<Investigator>
<FirstName>Bryan</FirstName>
<LastName>Pardo</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bryan A Pardo</PI_FULL_NAME>
<EmailAddress>pardo@northwestern.edu</EmailAddress>
<PI_PHON>8474917184</PI_PHON>
<NSF_ID>000275342</NSF_ID>
<StartDate>08/01/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Evanston</CityName>
<StateCode>IL</StateCode>
<ZipCode>602083109</ZipCode>
<StreetAddress><![CDATA[2145 Sheridan Road, Tech]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~498736</FUND_OBLG>
<FUND_OBLG>2015~15525</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span><strong>Overview:</strong> A fundamental problem in Computer Audition is that of audio source separation. This is the process of extracting elements of interest (like individual voices) from an audio scene (like a cocktail party).&nbsp; While many algorithms have been proposed to separate audio scenes into individual sources, these methods are brittle and difficult to use. Because of this, potential users have not broadly adopted the technology. Audio source separation methods are brittle because each algorithm relies on a single cue to separate sources. When the cue is not reliable, the method fails. Methods are difficult to use because algorithms cannot predict which audio scenes they are likely to work on. Therefore, the user does not know which method to apply in any given case. They are also difficult to use because their control parameters are hard to understand for those not expert in signal processing.&nbsp;</span></p> <p><span>In this work we addressed algorithm brittleness by developing methods to integrate multiple source separation algorithms into a single framework. We also developed new interfaces that let the user easily and interactively define what they wish to separate from the audio scene. We also developed methods for computer algorithms to automatically learn evaluation measures for audio quality so that a system could provide guidance to the user on tool selection and parameter settings.&nbsp;</span></p> <p><span>The outcomes of this research were: (1) Audio source separation algorithms that adaptively combine multiple cues to robustly separate sounds in cases where single-cue approaches fail; (2) Interfaces that let the user guide the separation process towards a goal, without having to understand the complex internals of the algorithms; (3) Methods to automatically learn evaluation measures from past user interactions so systems can suggest approaches and settings likely to work on the current interaction; and (4) Open-source audio source separation tools that embodies these outcomes.</span></p> <p><span>This work is&nbsp;<strong>intellectually transformative</strong> in bringing together techniques from the disparate fields signal processing and human computer interaction in an integrated and synergistic manner. The work <strong>advanced knowledge </strong>by developing <strong>a new unified framework for multi-approach source separation</strong>, as well <strong>as new approaches to deeply integrate cutting-edge signal processing and interactive interfaces that learn from users</strong>. This is of interest to researchers in signal processing, artificial intelligence, speech recognition, multimedia processing, and human computer interaction. <strong>&nbsp;</strong></span></p> <p><span><strong>This work has broad impact because </strong>current sound tagging methods are only reliable where there is a single predominant sound. Robust source separation can facilitate segmentation of audio into meaningful chunks to be recognized and transcribed with existing techniques. This will be <strong>transformative for speech recognition</strong> in environments with multiple talkers, <strong>transformative for biodiversity monitoring</strong> (automatic ID of species in field recordings with multiple concurrent species) and <strong>transformative for search through existing audio/video collections</strong> where manual tagging of the data is not practical. &nbsp;</span></p> <p><span>Algorithms to separation audio sources could be used to <strong>remix existing legacy audio content, </strong>upmixing stereo to surround sound, or<strong> helping the hearing-impaired</strong> select what to amplify and what to suppress in audio. Musicians could <strong>remix and edit recordings without need for an individual microphone on each musician</strong>. More broadly, source separation algorithms <strong>can be applied anywhere signals exhibit interference from multiple sources,</strong> including biomedical imaging and telecommunications.&nbsp;</span></p> <p><span>The work funded by this grant has resulted in 15 peer-reviewed publications presented in internationally-respected journals and conferences. These publications can be found on the Northwestern University Interactive Audio Lab website: <a href="http://music.cs.northwestern.edu"><span>music.cs.northwestern.edu</span></a>.&nbsp;</span></p> <p><span>In addition to publications, this grant has</span><span> </span><span>funded a number of software products. The reader can find our software on the GitHub online open source</span><span> </span><span>software repository (<a href="http://www.github.com"><span>www.github.com</span></a>). The three primary repositories are&nbsp;Web Unmixing Toolbox (WUT),&nbsp;The Northwestern University Source Separation Library (nussl), and the&nbsp;Crowdsourced Audio Quality Evaluation (CAQE) Toolkit.&nbsp;</span></p> <p><span>The</span><span> </span><strong>Web Unmixing Toolbox (WUT)&nbsp;</strong><span> </span><span>is an open source, browser-based, interactive source separation application</span><span> </span><span>for end users. This is the first application that lets the end user edit the audio using a two dimensional Fourier</span><span> </span><span>transform of the spectrogram. </span></p> <p><span><strong>The Northwestern University Source Separation Library (nussl)</strong> is a flexible,</span>object oriented python audio source separation library created by the PI?s lab.&nbsp; It provides implementations of common source separation algorithms as well as an easy-to-use framework for prototyping and adding new algorithms.&nbsp;</p> <p><strong>The Crowdsourced Audio Quality Evaluation (CAQE) Toolkit</strong> is a software package thatenables researchers to easily run perceptual audio quality evaluations over the web.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/30/2018<br>      Modified by: Bryan&nbsp;A&nbsp;Pardo</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Overview: A fundamental problem in Computer Audition is that of audio source separation. This is the process of extracting elements of interest (like individual voices) from an audio scene (like a cocktail party).  While many algorithms have been proposed to separate audio scenes into individual sources, these methods are brittle and difficult to use. Because of this, potential users have not broadly adopted the technology. Audio source separation methods are brittle because each algorithm relies on a single cue to separate sources. When the cue is not reliable, the method fails. Methods are difficult to use because algorithms cannot predict which audio scenes they are likely to work on. Therefore, the user does not know which method to apply in any given case. They are also difficult to use because their control parameters are hard to understand for those not expert in signal processing.   In this work we addressed algorithm brittleness by developing methods to integrate multiple source separation algorithms into a single framework. We also developed new interfaces that let the user easily and interactively define what they wish to separate from the audio scene. We also developed methods for computer algorithms to automatically learn evaluation measures for audio quality so that a system could provide guidance to the user on tool selection and parameter settings.   The outcomes of this research were: (1) Audio source separation algorithms that adaptively combine multiple cues to robustly separate sounds in cases where single-cue approaches fail; (2) Interfaces that let the user guide the separation process towards a goal, without having to understand the complex internals of the algorithms; (3) Methods to automatically learn evaluation measures from past user interactions so systems can suggest approaches and settings likely to work on the current interaction; and (4) Open-source audio source separation tools that embodies these outcomes.  This work is intellectually transformative in bringing together techniques from the disparate fields signal processing and human computer interaction in an integrated and synergistic manner. The work advanced knowledge by developing a new unified framework for multi-approach source separation, as well as new approaches to deeply integrate cutting-edge signal processing and interactive interfaces that learn from users. This is of interest to researchers in signal processing, artificial intelligence, speech recognition, multimedia processing, and human computer interaction.    This work has broad impact because current sound tagging methods are only reliable where there is a single predominant sound. Robust source separation can facilitate segmentation of audio into meaningful chunks to be recognized and transcribed with existing techniques. This will be transformative for speech recognition in environments with multiple talkers, transformative for biodiversity monitoring (automatic ID of species in field recordings with multiple concurrent species) and transformative for search through existing audio/video collections where manual tagging of the data is not practical.    Algorithms to separation audio sources could be used to remix existing legacy audio content, upmixing stereo to surround sound, or helping the hearing-impaired select what to amplify and what to suppress in audio. Musicians could remix and edit recordings without need for an individual microphone on each musician. More broadly, source separation algorithms can be applied anywhere signals exhibit interference from multiple sources, including biomedical imaging and telecommunications.   The work funded by this grant has resulted in 15 peer-reviewed publications presented in internationally-respected journals and conferences. These publications can be found on the Northwestern University Interactive Audio Lab website: music.cs.northwestern.edu.   In addition to publications, this grant has funded a number of software products. The reader can find our software on the GitHub online open source software repository (www.github.com). The three primary repositories are Web Unmixing Toolbox (WUT), The Northwestern University Source Separation Library (nussl), and the Crowdsourced Audio Quality Evaluation (CAQE) Toolkit.   The Web Unmixing Toolbox (WUT)  is an open source, browser-based, interactive source separation application for end users. This is the first application that lets the end user edit the audio using a two dimensional Fourier transform of the spectrogram.   The Northwestern University Source Separation Library (nussl) is a flexible,object oriented python audio source separation library created by the PI?s lab.  It provides implementations of common source separation algorithms as well as an easy-to-use framework for prototyping and adding new algorithms.   The Crowdsourced Audio Quality Evaluation (CAQE) Toolkit is a software package thatenables researchers to easily run perceptual audio quality evaluations over the web.           Last Modified: 12/30/2018       Submitted by: Bryan A Pardo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
