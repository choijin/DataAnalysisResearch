<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Building next-generation cloud infrastructure using RDMA</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>678457.00</AwardTotalIntnAmount>
<AwardAmount>678457</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As computation and data are moved to the "cloud", there is a need to significantly increase the capacity and performance of the underlying data centers through improved hardware and communication infrastructure.  A key component of the communication infrastructure is the network implementation, currently based upon Ethernet. Technological trends suggest that next-generation data center networks will be based upon RDMA (Remote Direct Memory Access), a powerful communciation technology used today in high performance computer systems. While RDMA offers enormous performance potential, current cloud infrastructures are not designed to exploit this potential.  This project will investigate RDMA-based system designs for distributed storage systems and in-memory applications.&lt;br/&gt;&lt;br/&gt;Specifically, the project research agenda consists of (a) building a high performance distributed key-value store and a B-tree based store (b) developing several applications in image search and deep learning whose distribution is made feasible by RDMA's ultra-low-latency and high throughput; (c) evaluating the performance of the resulting systems and applications at scale on the NSF-funded PRObE testbed.</AbstractNarration>
<MinAmdLetterDate>07/30/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409942</AwardID>
<Investigator>
<FirstName>Jinyang</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jinyang Li</PI_FULL_NAME>
<EmailAddress>jinyang@cs.nyu.edu</EmailAddress>
<PI_PHON>2129983372</PI_PHON>
<NSF_ID>000105743</NSF_ID>
<StartDate>07/30/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121185</ZipCode>
<StreetAddress><![CDATA[251 Mercer]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~153723</FUND_OBLG>
<FUND_OBLG>2015~241860</FUND_OBLG>
<FUND_OBLG>2016~139371</FUND_OBLG>
<FUND_OBLG>2017~143503</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In the past several years, data centers for cloud computing have started to adopt network features previously only found in High Performance Computing clusters. &nbsp;One notable network feature is RDMA (Remote Direct Memory Access), which provides very low latency, high throughput and low CPU overhead. &nbsp;The goal of this project is to develop novel cloud systems infrastructures and distributed applications that can exploit RDMA for performance. &nbsp;We have produced several such systems to demonstrate the performance benefits of RDMA.</p> <p><br />Pilaf is a single-server in-memory key-value store. &nbsp;In the traditional client-server architecture, all requests are processed by the server. By contrast, Pilaf combines client-side and server-side processing. &nbsp;In particular, the server processes write requests; the clients process the read requests by directly reading server's remote memory via RDMA. &nbsp;As practical key-value workloads are dominated by reads, this design significantly reduces the server's CPU load. &nbsp;Pilaf achieves 1.3 million ops/sec while utilizing only a single CPU core, compared to 55K for Memcached and 59K for Redis.</p> <p><br />We extend the design of Pilaf to build the Cell sorted key-value store. &nbsp;Cell distributes a global B-tree of ``fat'' (64MB) nodes across machines. &nbsp;Within each fat node, Cell organizes keys as a local B-tree of RDMA-friendly small nodes. &nbsp;Cell clients dynamically select whether to use client-side or server-side processing in response to different CPU and networking resources and the current workload. Our evaluation on a large RDMA-capable cluster shows that Cell scales well and that it can effectively respond to resource availability and workload properties.</p> <p><br />In addition to RDMA-optimized storage systems, we have also developed a distributed array framework called Spartan. &nbsp;Application programmers in domains like machine learning, scientific computing, and computational biology are accustomed to using powerful, high productivity array languages such as MatLab, R and NumPy. &nbsp;Spartan is a distributed array framework that automatically determines how to best partition n-dimensional arrays and to co-locate data with computation to maximize locality. &nbsp;Central to Spartan's design is a small number of carefully chosen parallel high-level operators, e.g. map, fold, filter, which are used to implement the large collection of array operations. &nbsp;Spartan's Python frontend performs lazy-evaluation to capture a dataflow graph of high-level operators. &nbsp;Spartan analyzes this graph to pick the best tiling strategy for execution. &nbsp;We have also implemented a dozen applications from a variety of domains on Spartan.</p> <p><br />Lastly, we have built a distributed image search engine called VertiCut using RDMA. &nbsp;Traditional distributed search engines partition its corpus ``horizontally'' so that each machine stores a subset of documents. &nbsp;The alternative approach of vertically partitioning the index has long been considered as impractical due to its high communication cost. The ultra low latency of RDMA makes our design of VertiCut feasible. &nbsp;VertiCut performs K-Nearest-Neighbor (KNN) search in a high-dimensional binary space occupied by all images. &nbsp;It vertically partitions the indexed binary codes by storing them in Pilaf. &nbsp;Our experiments show that VertiCut achieves better and more scalable performance compared to a horizontally partitioned search engine.</p> <p>Through our development of the above systems and applications, this proposal has made several contributions. &nbsp;First, Pilaf is one of the first system designs that demonstrate how to exploit RDMA to significantly improve performance. It has inspired a large number of follow-on projects to also explore RDMA-optimized systems. &nbsp;Second, the Cell project finds that client-side processing via RDMA is not always the best option and demonstrates how to optimally mix server-side and client-side processing for best performance. Third, Spartan and VertiCut show how compute-heavy applications can leverage the high-throughput and low latency properties of RDMA.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2018<br>      Modified by: Jinyang&nbsp;Li</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the past several years, data centers for cloud computing have started to adopt network features previously only found in High Performance Computing clusters.  One notable network feature is RDMA (Remote Direct Memory Access), which provides very low latency, high throughput and low CPU overhead.  The goal of this project is to develop novel cloud systems infrastructures and distributed applications that can exploit RDMA for performance.  We have produced several such systems to demonstrate the performance benefits of RDMA.   Pilaf is a single-server in-memory key-value store.  In the traditional client-server architecture, all requests are processed by the server. By contrast, Pilaf combines client-side and server-side processing.  In particular, the server processes write requests; the clients process the read requests by directly reading server's remote memory via RDMA.  As practical key-value workloads are dominated by reads, this design significantly reduces the server's CPU load.  Pilaf achieves 1.3 million ops/sec while utilizing only a single CPU core, compared to 55K for Memcached and 59K for Redis.   We extend the design of Pilaf to build the Cell sorted key-value store.  Cell distributes a global B-tree of ``fat'' (64MB) nodes across machines.  Within each fat node, Cell organizes keys as a local B-tree of RDMA-friendly small nodes.  Cell clients dynamically select whether to use client-side or server-side processing in response to different CPU and networking resources and the current workload. Our evaluation on a large RDMA-capable cluster shows that Cell scales well and that it can effectively respond to resource availability and workload properties.   In addition to RDMA-optimized storage systems, we have also developed a distributed array framework called Spartan.  Application programmers in domains like machine learning, scientific computing, and computational biology are accustomed to using powerful, high productivity array languages such as MatLab, R and NumPy.  Spartan is a distributed array framework that automatically determines how to best partition n-dimensional arrays and to co-locate data with computation to maximize locality.  Central to Spartan's design is a small number of carefully chosen parallel high-level operators, e.g. map, fold, filter, which are used to implement the large collection of array operations.  Spartan's Python frontend performs lazy-evaluation to capture a dataflow graph of high-level operators.  Spartan analyzes this graph to pick the best tiling strategy for execution.  We have also implemented a dozen applications from a variety of domains on Spartan.   Lastly, we have built a distributed image search engine called VertiCut using RDMA.  Traditional distributed search engines partition its corpus ``horizontally'' so that each machine stores a subset of documents.  The alternative approach of vertically partitioning the index has long been considered as impractical due to its high communication cost. The ultra low latency of RDMA makes our design of VertiCut feasible.  VertiCut performs K-Nearest-Neighbor (KNN) search in a high-dimensional binary space occupied by all images.  It vertically partitions the indexed binary codes by storing them in Pilaf.  Our experiments show that VertiCut achieves better and more scalable performance compared to a horizontally partitioned search engine.  Through our development of the above systems and applications, this proposal has made several contributions.  First, Pilaf is one of the first system designs that demonstrate how to exploit RDMA to significantly improve performance. It has inspired a large number of follow-on projects to also explore RDMA-optimized systems.  Second, the Cell project finds that client-side processing via RDMA is not always the best option and demonstrates how to optimally mix server-side and client-side processing for best performance. Third, Spartan and VertiCut show how compute-heavy applications can leverage the high-throughput and low latency properties of RDMA.          Last Modified: 11/30/2018       Submitted by: Jinyang Li]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
