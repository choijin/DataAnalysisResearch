<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CompCog: Human Scene Processing Characterized by Computationally-derived Scene Primitives</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>463158.00</AwardTotalIntnAmount>
<AwardAmount>463158</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>How do our brains take the light entering our eyes and turn it into our experience of the world around us? Critically, this experience seems to involve a visual "vocabulary" that allows us to understand new scenes based on our prior knowledge. The investigators explore the nature of this visual language, exploring the specific computations that are realized in the brain mechanisms used for scene perception. The work combines data from state-of-the-art computer vision systems with human neuroimaging to both predict brain responses when viewing complex, real-world scenes, and to analyze and understand the hidden structure embedded in real-world images. This effort is essential for building a theory of how we are able to see and for improving machine vision systems. More broadly, biologically-inspired models of vision are essential for the effective deployment of intelligent technology in navigation systems, assistive devices, security verification, and visual information retrieval.&lt;br/&gt;&lt;br/&gt;The artificial vision system adopted in this research is highly data-driven in that it is learning about the visual world by continuously "looking at" real-world images on the World Wide Web. The model, known as "NEIL" (Never Ending Image Learner, http://www.neil-kb.com/), leverages cutting-edge big-data methods to extract a vocabulary of scene parts and relationships from hundreds of thousands of images. The relevance of this vocabulary to human vision will then be tested using both functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) neuroimaging. The hypothesis is that the application of prior knowledge about scenes expresses itself through learned associations between the specific parts and relations forming the vocabulary for scene perception. Moreover, different kinds of associations may be instantiated within distinct components of the functional brain network responsible for scene perception. Overall, this research will build on a recent, highly-successful artificial vision system in order to provide a more well-specified theory of the parts and relations underlying human scene perception. At the same time, the research will provide information about the human functional relevance of computationally-derived scene parts and relations, thereby helping to refine and improve artificial vision systems.</AbstractNarration>
<MinAmdLetterDate>07/30/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/15/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439237</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Tarr</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael J Tarr</PI_FULL_NAME>
<EmailAddress>michaeltarr@cmu.edu</EmailAddress>
<PI_PHON>4122684379</PI_PHON>
<NSF_ID>000215764</NSF_ID>
<StartDate>07/30/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Elissa</FirstName>
<LastName>Aminoff</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elissa Aminoff</PI_FULL_NAME>
<EmailAddress>eaminoff@fordham.edu</EmailAddress>
<PI_PHON>7188173480</PI_PHON>
<NSF_ID>000668135</NSF_ID>
<StartDate>07/30/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152132685</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1699</Code>
<Text>Cognitive Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>1699</Code>
<Text>COGNEURO</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>7956</Code>
<Text>SBE Interdisciplinary Research</Text>
</ProgramReference>
<ProgramReference>
<Code>8605</Code>
<Text>SBE 2020</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~463158</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>What does it mean to see and understand the environment around us? In this project we explored what visual features in our world are employed by the human brain to reliably interpret visual inputs. This work had two goals: to better understand human perception and to advance computer vision - making it both more effective and better aligned with human visual behavior. Our approach involved using both human and computer vision to inform one another, thereby advancing our knowledge in both domains. To address the question of what information in the visual world leads to efficient visual understanding, we first attempted to define the visual features present in our environment. Because modern computer vision models rely on 1,000,000's of images, we were able to statistically define and characterize appropriate visual features using state-of-the-art vision models. These features were then used to help account for how the human brain represents and processes the visual world. Our human data was derived from two neuroimaging methods: fMRI and magnetoencephalography (MEG), allowing us to examine both where in the brain visual processing is taking place and when in time is this processing occurring. We found that higher-order visual brain regions rely on a combination of low-level visual features (e.g., textures) and high-level semantic features (e.g., objects and actions). Such understanding is accomplished across several processing stages and is iterative in that&nbsp;&nbsp;the selection of low-level visual features is based on their relationship to semantic features - enabling more efficient visual processing. At the same time, comparing computer vision models to human data is challenging in that a typical human neuroimaging experiment provides only 100?s of observations across an even smaller number of different images for each participant. In contrast, as mentioned, computer vision models typically use 1,000,000's of images. To address this field-wide problem, we collected, and made public, a neuroimaging dataset - BOLD5000 - that includes neural activity - as measured by fMRI - in response to viewing over 5,000 individual scenes. Importantly, BOLD5000 enables better collaborations between those studying biological vision and those studying artificial vision. Overall, the project has made advances connecting human vision to computer vision - leading to potential improvements in artificial vision systems and better uses of computer vision systems in the service of understanding human perception.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/22/2019<br>      Modified by: Elissa&nbsp;Aminoff</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ What does it mean to see and understand the environment around us? In this project we explored what visual features in our world are employed by the human brain to reliably interpret visual inputs. This work had two goals: to better understand human perception and to advance computer vision - making it both more effective and better aligned with human visual behavior. Our approach involved using both human and computer vision to inform one another, thereby advancing our knowledge in both domains. To address the question of what information in the visual world leads to efficient visual understanding, we first attempted to define the visual features present in our environment. Because modern computer vision models rely on 1,000,000's of images, we were able to statistically define and characterize appropriate visual features using state-of-the-art vision models. These features were then used to help account for how the human brain represents and processes the visual world. Our human data was derived from two neuroimaging methods: fMRI and magnetoencephalography (MEG), allowing us to examine both where in the brain visual processing is taking place and when in time is this processing occurring. We found that higher-order visual brain regions rely on a combination of low-level visual features (e.g., textures) and high-level semantic features (e.g., objects and actions). Such understanding is accomplished across several processing stages and is iterative in that  the selection of low-level visual features is based on their relationship to semantic features - enabling more efficient visual processing. At the same time, comparing computer vision models to human data is challenging in that a typical human neuroimaging experiment provides only 100?s of observations across an even smaller number of different images for each participant. In contrast, as mentioned, computer vision models typically use 1,000,000's of images. To address this field-wide problem, we collected, and made public, a neuroimaging dataset - BOLD5000 - that includes neural activity - as measured by fMRI - in response to viewing over 5,000 individual scenes. Importantly, BOLD5000 enables better collaborations between those studying biological vision and those studying artificial vision. Overall, the project has made advances connecting human vision to computer vision - leading to potential improvements in artificial vision systems and better uses of computer vision systems in the service of understanding human perception.           Last Modified: 05/22/2019       Submitted by: Elissa Aminoff]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
