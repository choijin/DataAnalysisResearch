<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SCH: INT: Collaborative Research: Learning and Sensory-based Modeling for Adaptive Web-Empowerment Trauma Treatment</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>238795.00</AwardTotalIntnAmount>
<AwardAmount>238795</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Mental trauma following disasters, military service, accidents, domestic violence and other traumatic events is a health issue costing multiple billion of dollars per year. Beyond its direct costs, there are indirect costs including a 45-150% greater use of medical and psychiatric care. While web-based support systems have been developed these are effectively a "one-size-fits all" approach lacking the personalization of regular treatment and the engagement and effectiveness associated with a tailored regimen. This project brings together a multi-disciplinary team of leading researchers in trauma treatment, facial analysis, computer vision and machine learning to develop a scalable, adaptive person-centered approach that uses vision and sensing to improve web-based trauma treatment.  In particular, the effort measures specific personalized variables during treatment and then uses a model to adapt treatment to individuals in need. &lt;br/&gt;&lt;br/&gt;The core treatment design builds on well-established social-cognitive theory, where self-efficacy and physiological response are critical elements of recovery. The project measures these as well as engagement that is critical in self-directed web-based treatment. The modeling requires advances in computer vision and facial analysis to develop individualized models that can be computed with just a standard laptop. This project is the first effort to approximate changes in self-efficacy from sensory data. The effort uses and advances machine learning and domain adaption to support this approximation as well as to support the rapid personalization of models.  Building a smart system that empowers individuals by combining sensing and learning to improve web-based treatment offers a transformative approach to this national health need for cost-effective evidence-based treatment of trauma.</AbstractNarration>
<MinAmdLetterDate>08/13/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1418026</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Cohn</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey F Cohn</PI_FULL_NAME>
<EmailAddress>jeffcohn@pitt.edu</EmailAddress>
<PI_PHON>4126248825</PI_PHON>
<NSF_ID>000211710</NSF_ID>
<StartDate>08/13/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pittsburgh</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152133203</ZipCode>
<PhoneNumber>4126247400</PhoneNumber>
<StreetAddress>300 Murdoch Building</StreetAddress>
<StreetAddress2><![CDATA[3420 Forbes Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004514360</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF PITTSBURGH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004514360</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pittsburgh]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152132303</ZipCode>
<StreetAddress><![CDATA[123 University Place]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramElement>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8062</Code>
<Text>SCH Type II: INT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~238795</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This was a collaborative project with NSF Award&nbsp;1418520. The collaborative&nbsp;project&rsquo;s primary goal was to learn mappings from facial expression to self-report ratings of engagement, arousal, and self-efficacy in participants that vary in their exposure to trauma.&nbsp;&nbsp;Participants were 18 years or older and had experienced one or more traumatic events within the past 24 months.&nbsp;&nbsp;They were recorded from a webcam while viewing online psycho-educational modules about trauma and situations that trigger trauma.&nbsp;&nbsp;They completed self-report measures of severity of post-traumatic disorder and depression severity (CES-D).&nbsp;&nbsp;To learn mappings from facial expression to arousal, engagement, and self-efficacy, three studies were conducted by the University of Pittsburgh site under Award 1418026 using project data and publicly-available data sources&nbsp;(Zhang, Girard et al. 2016, Girard, Chu et al. 2017).</p> <p><strong>Observer ratings of engagement, arousal, and self-efficacy</strong>. Collaborators at UCCS developed self-instructional materials for online viewing, recruited participants that varied in their history of trauma, and developed a behavioral coding scheme.&nbsp;&nbsp;We provided software tools and training to accomplish continuous observer ratings of participant response to the modules.&nbsp;The results of the UCCS team with CARMA are reported in&nbsp;(Dhamija and Boult 2018).</p> <p><strong>Relation between facial expression and PTSD intensity.&nbsp;</strong>Emotional distress is a key component of PTSD. Previous work has been limited for most part to self-reported distress.&nbsp;&nbsp;We investigated whether emotional distress is communicated in facial actions.&nbsp;&nbsp;Fifty-four women with history of trauma were video-recorded while viewing the psycho-educational modules described above.&nbsp;&nbsp;Facial action units (AUs) were manually annotated using a comprehensive anatomically-based annotation system.</p> <p>AUs were strongly related both to global self-report indices of distress and specifically to symptoms of trauma.&nbsp;&nbsp;Both smiling and pressed lips were negatively associated with PTSD symptoms (&#119861;=&minus;16.21,&nbsp;&#119901;=.004 and&nbsp;&#119861;=&minus;14.94,&nbsp;&#119901;=.020, respectively).&nbsp;&nbsp;When depressive symptoms were statistically controlled, smiling remained significant (&#119861;=&minus;8.81,&nbsp;&#119901;=.042).&nbsp;&nbsp;These findings inform the relation between observable behavior and subjective emotion and have potential to inform clinical assessment to improve treatment outcomes.</p> <p><strong>Automatic detection of facial action unit occurrence and intensity</strong>. Given the sensitivity and specificity of facial actions to PTSD, automated ways of detecting facial actions in real time are needed.&nbsp;&nbsp;We developed state-of-the-art approaches that use the correlation among facial actions and their dynamics&nbsp;(Chu, De la Torre et al. 2019, Ertugrul, Jeni et al. 2019, Yang, Ertugrul et al. 2019)&nbsp;to automatically detect AUs. Because algorithms to detect AUs will be are intended for domains beyond which they have been trained, we 1) reviewed the literature on cross-domain generalizability; and 2) empirically evaluated performance of deep-learning and shall0w-learing approaches to AU detection within and between domains&nbsp;(Cohn, Ertugrul et al. 2018, Ertugrul, Cohn et al. 2019). So that others may use AU detection algorithms in research and clinical applications involving PTSD and other disorders, we made them available for non-commercial use&nbsp;(Erturgrul, Jeni et al. 2019).</p> <p><strong>Literature Cited</strong></p> <p class="EndNoteBibliography">Chu, W.-S., F. De la Torre and J. F. Cohn (2019). "Learning facial action units with spatiotemporal cues and multi-label sampling."&nbsp;<span style="text-decoration: underline;">Image and Vision Computing</span>&nbsp;<strong>81</strong>(x): 1-14.</p> <p class="EndNoteBibliography">Cohn, J. F., I. O. Ertugrul, C. Wen-Sheng, J. M. Girard, L. A. Jeni and Z. Hammal (2018). Affective facial computing: Generalizability across domains.&nbsp;<span style="text-decoration: underline;">Multimodal behavior analysis in the wild: Advances and challenges</span>. X. Alameda-Pineda, P. E. Ricci-Bitti and N. Sebe. London, UK, Elsevier.</p> <p class="EndNoteBibliography">Dhamija, S. and T. E. Boult (2018). Automated action units vs. expert raters: Face off.&nbsp;<span style="text-decoration: underline;">2018 IEEE Winter Conference on Applications of Computer Vision</span>.</p> <p class="EndNoteBibliography">Ertugrul, I. O., J. F. Cohn, L. Jeni, X. Zhang, Q. Ji and L. Yin (2019). Cross-domain AU detection: Domains, learning approaches, and measures.&nbsp;<span style="text-decoration: underline;">IEEE International Conference on Automatic Face and Gesture Recognition</span><strong>:&nbsp;</strong>1-8.</p> <p class="EndNoteBibliography">Ertugrul, I. O., L. A. Jeni and J. F. Cohn (2019). PAttNet: Patch-attentive deep network for action unit detection.&nbsp;<span style="text-decoration: underline;">British Machine Learning Conference</span><strong>:&nbsp;</strong>1-13.</p> <p class="EndNoteBibliography">Erturgrul, I. O., L. A. Jeni, W. Ding and J. F. Cohn (2019). AFAR: A Deep learning based tool for automated facial affect recognition.&nbsp;<span style="text-decoration: underline;">IEEE International Conference on Automatic Face and Gesture Recognition</span>. Lille, France, IEEE.</p> <p class="EndNoteBibliography">Girard, J. M., W.-S. Chu, L. A. Jeni, J. F. Cohn, F. De la Torre and M. A. Sayette (2017). Sayette Group Formation Task (GFT) spontaneous facial expression database.&nbsp;<span style="text-decoration: underline;">IEEE International Conference on Automatic Face and Gesture Recognition</span>.</p> <p class="EndNoteBibliography">Yang, L., I. O. Ertugrul, J. F. Cohn, Z. Hammal, D. Jiang and H. Sahli (2019). FACS3D-Net: 3D convolution based spatiotemporal representation for action unit detection.&nbsp;<span style="text-decoration: underline;">Conference on Affective Computing and Intelligent Interaction</span>. Cambridge, UK.</p> <p class="EndNoteBibliography">Zhang, X., J. M. Girard, Y. Wu, X. Zhang, P. Liu, U. Ciftci, S. Canavan, M. Reale, A. Horowitz, H. Yang, J. F. Cohn, Q. Ji and L. Yin (2016). Multimodal spontaneous human emotion corpus for human behavior analysis.&nbsp;<span style="text-decoration: underline;">IEEE International Conference on Computer Vision and Pattern Recognition</span>. Las Vegas, NV.</p> <p>&nbsp;</p> <p style="text-align: center;">&nbsp;</p><br> <p>            Last Modified: 10/13/2019<br>      Modified by: Jeffrey&nbsp;F&nbsp;Cohn</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This was a collaborative project with NSF Award 1418520. The collaborative project?s primary goal was to learn mappings from facial expression to self-report ratings of engagement, arousal, and self-efficacy in participants that vary in their exposure to trauma.  Participants were 18 years or older and had experienced one or more traumatic events within the past 24 months.  They were recorded from a webcam while viewing online psycho-educational modules about trauma and situations that trigger trauma.  They completed self-report measures of severity of post-traumatic disorder and depression severity (CES-D).  To learn mappings from facial expression to arousal, engagement, and self-efficacy, three studies were conducted by the University of Pittsburgh site under Award 1418026 using project data and publicly-available data sources (Zhang, Girard et al. 2016, Girard, Chu et al. 2017).  Observer ratings of engagement, arousal, and self-efficacy. Collaborators at UCCS developed self-instructional materials for online viewing, recruited participants that varied in their history of trauma, and developed a behavioral coding scheme.  We provided software tools and training to accomplish continuous observer ratings of participant response to the modules. The results of the UCCS team with CARMA are reported in (Dhamija and Boult 2018).  Relation between facial expression and PTSD intensity. Emotional distress is a key component of PTSD. Previous work has been limited for most part to self-reported distress.  We investigated whether emotional distress is communicated in facial actions.  Fifty-four women with history of trauma were video-recorded while viewing the psycho-educational modules described above.  Facial action units (AUs) were manually annotated using a comprehensive anatomically-based annotation system.  AUs were strongly related both to global self-report indices of distress and specifically to symptoms of trauma.  Both smiling and pressed lips were negatively associated with PTSD symptoms (&#119861;=&minus;16.21, &#119901;=.004 and &#119861;=&minus;14.94, &#119901;=.020, respectively).  When depressive symptoms were statistically controlled, smiling remained significant (&#119861;=&minus;8.81, &#119901;=.042).  These findings inform the relation between observable behavior and subjective emotion and have potential to inform clinical assessment to improve treatment outcomes.  Automatic detection of facial action unit occurrence and intensity. Given the sensitivity and specificity of facial actions to PTSD, automated ways of detecting facial actions in real time are needed.  We developed state-of-the-art approaches that use the correlation among facial actions and their dynamics (Chu, De la Torre et al. 2019, Ertugrul, Jeni et al. 2019, Yang, Ertugrul et al. 2019) to automatically detect AUs. Because algorithms to detect AUs will be are intended for domains beyond which they have been trained, we 1) reviewed the literature on cross-domain generalizability; and 2) empirically evaluated performance of deep-learning and shall0w-learing approaches to AU detection within and between domains (Cohn, Ertugrul et al. 2018, Ertugrul, Cohn et al. 2019). So that others may use AU detection algorithms in research and clinical applications involving PTSD and other disorders, we made them available for non-commercial use (Erturgrul, Jeni et al. 2019).  Literature Cited Chu, W.-S., F. De la Torre and J. F. Cohn (2019). "Learning facial action units with spatiotemporal cues and multi-label sampling." Image and Vision Computing 81(x): 1-14. Cohn, J. F., I. O. Ertugrul, C. Wen-Sheng, J. M. Girard, L. A. Jeni and Z. Hammal (2018). Affective facial computing: Generalizability across domains. Multimodal behavior analysis in the wild: Advances and challenges. X. Alameda-Pineda, P. E. Ricci-Bitti and N. Sebe. London, UK, Elsevier. Dhamija, S. and T. E. Boult (2018). Automated action units vs. expert raters: Face off. 2018 IEEE Winter Conference on Applications of Computer Vision. Ertugrul, I. O., J. F. Cohn, L. Jeni, X. Zhang, Q. Ji and L. Yin (2019). Cross-domain AU detection: Domains, learning approaches, and measures. IEEE International Conference on Automatic Face and Gesture Recognition: 1-8. Ertugrul, I. O., L. A. Jeni and J. F. Cohn (2019). PAttNet: Patch-attentive deep network for action unit detection. British Machine Learning Conference: 1-13. Erturgrul, I. O., L. A. Jeni, W. Ding and J. F. Cohn (2019). AFAR: A Deep learning based tool for automated facial affect recognition. IEEE International Conference on Automatic Face and Gesture Recognition. Lille, France, IEEE. Girard, J. M., W.-S. Chu, L. A. Jeni, J. F. Cohn, F. De la Torre and M. A. Sayette (2017). Sayette Group Formation Task (GFT) spontaneous facial expression database. IEEE International Conference on Automatic Face and Gesture Recognition. Yang, L., I. O. Ertugrul, J. F. Cohn, Z. Hammal, D. Jiang and H. Sahli (2019). FACS3D-Net: 3D convolution based spatiotemporal representation for action unit detection. Conference on Affective Computing and Intelligent Interaction. Cambridge, UK. Zhang, X., J. M. Girard, Y. Wu, X. Zhang, P. Liu, U. Ciftci, S. Canavan, M. Reale, A. Horowitz, H. Yang, J. F. Cohn, Q. Ji and L. Yin (2016). Multimodal spontaneous human emotion corpus for human behavior analysis. IEEE International Conference on Computer Vision and Pattern Recognition. Las Vegas, NV.            Last Modified: 10/13/2019       Submitted by: Jeffrey F Cohn]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
