<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Medium: Collaborative Research: Scalable Algorithms for Spatio-temporal Data Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>709342.00</AwardTotalIntnAmount>
<AwardAmount>709342</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Acceleration of computing power of supercomputers along with development and deployment of large instruments such as telescopes, colliders, sensors and devices raises one fundamental question. "Can the time to insight and knowledge discovery be reduced at the same exponential rate?" The answer currently is clearly "NO", because a critical step that combines analytics, mining and discovering knowledge from the massive datasets has lagged far behind advances in software, simulation and generation of data. Analysis of data requires "data-driven" computing and analytics. This entails scalable software for data reduction, approximations, analysis, statistics, and bottom-up discovery. Scalable and parallel analytics software for processing large amount of data is required in order to make a significant leap forward in scientific discoveries. &lt;br/&gt;This project develops innovative, scalable, and sustainable data analytics algorithms to enable analysis and mining of massive data on high-performance parallel computers, which include (1) bottom-up and unsupervised data clustering algorithms that are suitable for spatio-temporal data, massive graph analytics, community computations, and detection of patterns in time-varying graphs, different types of data, and different data characteristics; (2) change detection and anomaly detection in spatio-temporal data; and (3) tracking moving data and cluster dynamics within certain time and space constraints. These parallel algorithms use the massive amount of data generated from scientific applications, such as astrophysics, cosmology simulations, climate modeling, and social networking analysis, for result verification and performance evaluation on modern high-performance parallel computers.&lt;br/&gt;This project directly addresses the critical needs for spatio-temporal data analysis, performance scalability, and programming productivity of large-scale scientific discovery via parallel analytics software for big data. This work will impact applications of enormous societal benefits and scientific importance such as climate understanding, environmental sustainability, astrophysics, biology and medicine by accelerating scientific discoveries. Furthermore, the developed software infrastructure can be used and adopted in commercial applications, such as commerce, social, security, drug discovery, and so on. The source codes are open to the public for all community to adapt, build-upon, customize and contribute to, thereby multiplying its value and usage.</AbstractNarration>
<MinAmdLetterDate>05/21/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/21/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409601</AwardID>
<Investigator>
<FirstName>Alok</FirstName>
<LastName>Choudhary</LastName>
<PI_MID_INIT>N</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alok N Choudhary</PI_FULL_NAME>
<EmailAddress>choudhar@ece.northwestern.edu</EmailAddress>
<PI_PHON>8474674129</PI_PHON>
<NSF_ID>000101070</NSF_ID>
<StartDate>05/21/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Wei-keng</FirstName>
<LastName>Liao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wei-keng Liao</PI_FULL_NAME>
<EmailAddress>wkliao@eecs.northwestern.edu</EmailAddress>
<PI_PHON>8474912916</PI_PHON>
<NSF_ID>000164583</NSF_ID>
<StartDate>05/21/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ankit</FirstName>
<LastName>Agrawal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ankit Agrawal</PI_FULL_NAME>
<EmailAddress>ankitag@eecs.northwestern.edu</EmailAddress>
<PI_PHON>8474918163</PI_PHON>
<NSF_ID>000582160</NSF_ID>
<StartDate>05/21/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Evanston</CityName>
<StateCode>IL</StateCode>
<ZipCode>602083118</ZipCode>
<StreetAddress><![CDATA[2145 Sheridan Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~709342</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>The accelerating computing power of supercomputers, moving from <span>petascale</span> towards <span>exascale</span> performance, along with development and deployment of large instruments such as telescopes, <span>colliders</span>, satellites, sensors, etc. is producing a great volume, velocity, and variety of data to be analyzed for knowledge discovery. Existing data analytics software has not been designed to process such massive data in a scalable way. There is a growing need to create innovative, scalable <span>spatio</span>-temporal data analytics algorithms to enable analysis and mining of massive data on high-performance computing systems. In this project, we incorporated a co-design approach to the development of scalable, parallel algorithms and software, rather than simply considering and scaling existing sequential techniques. This is particularly important for big data analytics because the existing techniques developed on small data sets may be neither scalable, nor suitable for in-situ analytics on parallel computer systems. In addition, close collaborations between computer scientists and domain scientists as part of this project has helped advance knowledge in other scientific domains such as astrophysics.&nbsp;</span></p> <p>&nbsp;</p> <p><span>This project has supported the development of the following parallel data analysis software that run on state-of-the-art supercomputers. Parallel CNN is a distributed-memory based parallel implementation for Convolution Neural Network data training algorithms. <span>PMEP</span> is a parallel community detection algorithm based on the idea of maximizing equilibrium and purity of communities. <span>AGORAS</span> a hierarchical data clustering algorithm based on the well-known k-<span>medoids</span> algorithm. MACH (Maximum Clique Heuristic) computes disjoint cliques from a graph using a heuristic-based branch-and-bound technique. Spark-based <span>DBSCAN</span> is an implementation of <span>DBSACN</span>, a density-based data clustering algorithm, using Spark, a <span>MapReduce</span> framework. K-Means on <span>KNL</span> is the optimized implementation of K-means algorithm for Intel <span>Xeon</span> Phi Knights Landing MIC-based architecture. Merger tree algorithm is used in astrophysics to study the dynamics of evolution of dark matter halos over time, where the formation, merging, and evolution of halos are driven by large-scale N-body simulations. <span>Kriging</span> is a well-known <span>geostatistical</span> interpolation algorithm, which we enhanced in this project by adding a data filtering algorithm for enhancing prediction accuracy on <span>spatio</span>-temporal data. <span>DTFE</span> (<span>Delaunay</span> tessellation field estimator) is another algorithm with applications in astrophysics that reconstructs surface density field from a discrete point set, used in N-body gravitational lensing simulations. The&nbsp;</span>source codes are openly available to the public at <span>http</span>://<span>cucis</span>.<span>ece</span>.northwestern.<span>edu</span>.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/30/2019<br>      Modified by: Wei-Keng&nbsp;Liao</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The accelerating computing power of supercomputers, moving from petascale towards exascale performance, along with development and deployment of large instruments such as telescopes, colliders, satellites, sensors, etc. is producing a great volume, velocity, and variety of data to be analyzed for knowledge discovery. Existing data analytics software has not been designed to process such massive data in a scalable way. There is a growing need to create innovative, scalable spatio-temporal data analytics algorithms to enable analysis and mining of massive data on high-performance computing systems. In this project, we incorporated a co-design approach to the development of scalable, parallel algorithms and software, rather than simply considering and scaling existing sequential techniques. This is particularly important for big data analytics because the existing techniques developed on small data sets may be neither scalable, nor suitable for in-situ analytics on parallel computer systems. In addition, close collaborations between computer scientists and domain scientists as part of this project has helped advance knowledge in other scientific domains such as astrophysics.      This project has supported the development of the following parallel data analysis software that run on state-of-the-art supercomputers. Parallel CNN is a distributed-memory based parallel implementation for Convolution Neural Network data training algorithms. PMEP is a parallel community detection algorithm based on the idea of maximizing equilibrium and purity of communities. AGORAS a hierarchical data clustering algorithm based on the well-known k-medoids algorithm. MACH (Maximum Clique Heuristic) computes disjoint cliques from a graph using a heuristic-based branch-and-bound technique. Spark-based DBSCAN is an implementation of DBSACN, a density-based data clustering algorithm, using Spark, a MapReduce framework. K-Means on KNL is the optimized implementation of K-means algorithm for Intel Xeon Phi Knights Landing MIC-based architecture. Merger tree algorithm is used in astrophysics to study the dynamics of evolution of dark matter halos over time, where the formation, merging, and evolution of halos are driven by large-scale N-body simulations. Kriging is a well-known geostatistical interpolation algorithm, which we enhanced in this project by adding a data filtering algorithm for enhancing prediction accuracy on spatio-temporal data. DTFE (Delaunay tessellation field estimator) is another algorithm with applications in astrophysics that reconstructs surface density field from a discrete point set, used in N-body gravitational lensing simulations. The source codes are openly available to the public at http://cucis.ece.northwestern.edu.          Last Modified: 08/30/2019       Submitted by: Wei-Keng Liao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
