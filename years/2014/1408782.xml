<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Collaborative Research: FTFS: A Read/Write-Optimized Fractal Tree File System</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>240000.00</AwardTotalIntnAmount>
<AwardAmount>240000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern, general-purpose file systems offer poor performance on microdata operations, such as file creation and destruction, small writes to large files, and metadata updates, yet these operations are pervasive on today's computer systems.  Underlying this problem are fundamental limitations of the data structures used to organize data on disk.  This project will explore the practical efficacy of a recently-discovered category of data structures, called write-read-optimized (WRO) data structures, which have the potential to improve microdata performance dramatically without sacrificing good performance on other types of operations.  This project will bring together a team of experts from theory and systems who can bring cutting-edge algorithmic advances into operating system (OS) designs.  To this end, the team will build a general-purpose file system for Linux, called FTFS, that uses WRO data structures.&lt;br/&gt;&lt;br/&gt;Work of this nature has the potential to eliminate the current trade-off between data locality on disk and small-write performance.  This project observes that WRO data structures, such as B^epsilon trees and fractal tree indexes, can give comparable asymptotic behavior to a B-tree for queries and bulk updates, as well as support small updates with performance close to logging.  Preliminary work demonstrates that these asymptotic benefits translate to real performance improvements - up to two orders of magnitude faster than a traditional B-tree for some operations.  Modern operating systems have certain assumptions about how file systems are designed, such as inducing extra lookups during update operations (called cryptoreads). Cryptoreads cause update operations to block on lookups, thus throttling the faster updates that WRO data structures provide. The project will investigate OS support for WRO data structures, as well as redesigning WRO data structures to support the operations of a fully-featured file system.&lt;br/&gt;&lt;br/&gt;The ultimate goal is technology transfer and practical adoption. The effort will advance the current state of the art in file system and operating system design.  Computers are a fundamental part of our society, with desktops and laptops permeating schools and workplaces, individuals carrying at least one mobile device, and scientists driving new discovery with supercomputers. File systems are the backbone of these computing platforms, and improvements to the efficiency of a general-purpose file system can improve the efficiency of our national cyber-infrastructure, as well as reintroduce flexibility into the storage stack needed to adapt to rapidly evolving devices.</AbstractNarration>
<MinAmdLetterDate>08/26/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1408782</AwardID>
<Investigator>
<FirstName>Martin</FirstName>
<LastName>Farach-Colton</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martin Farach-Colton</PI_FULL_NAME>
<EmailAddress>farach@cs.rutgers.edu</EmailAddress>
<PI_PHON>8484458313</PI_PHON>
<NSF_ID>000255442</NSF_ID>
<StartDate>08/26/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548072</ZipCode>
<StreetAddress><![CDATA[110 Frelinghuysen Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~240000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>File systems are at the heart of all computers. &nbsp;They maintain the contents of files and folders/directories, as well as metadata such as the time of last modification. File-system performance bottlenecks can reduce the overall performance of computing platforms at every scale, from smart phones to supercomputers.</p> <p>Modern file systems are mostly built on top of data structures that have been around for decades. &nbsp;The main thrust of this proposal was to build a new file system based on the new data structures that are fast for many file-system-critical operations. &nbsp;The aim is twofold: to show that faster data structures translate into faster systems at the user level; and to show that many tradeoffs that are built into modern file systems can be overcome by radically redesigning the file system with the right data structures.</p> <p>The main results are summarized in four papers, in the last four FAST conferences. &nbsp;The USENIX FAST conference is the top conference for file systems.</p> <p>The FAST 15 paper established the basic premise that fast data structures can indeed translate into better performance at the application level. &nbsp;This paper introduced BetrFS, so named because it uses a B^e-tree to store both data and metadata. &nbsp;This paper showed that data could be modified at every scale, from micro- to macro-updates, while allowing the file and directory contents to be read quickly. &nbsp;Not every operation was fast in this version of BetrFS, and improving the performance of some key operations was left as future work. &nbsp;This paper was one of three finalists for Best Paper award.</p> <p>The FAST 16 paper showed that some of the performance problems in the FAST 15 paper were fixable. &nbsp;In particular, sequential writes -- that is, writing the contents of a large file, such as when a file is downloaded -- was slower than in other file systems. &nbsp;The problem was that such large files were being written twice: once as the content of a file, and once in a log file designed to make the file system secure against computer crashes. &nbsp;The FAST 16 paper showed how to write the file once while still allowing the file system to remain secure against computer crashes. &nbsp;Surprisingly, most file systems in use today can become corrupted if the computer crashes, so achieving both crash safety and high performance was an important advance. &nbsp;This paper received the Best Paper award.</p> <p>The FAST 17 paper studied the problem of file-system aging. &nbsp;Many users of older Windows machines will remember that these computer required defragmentation from time to time in order to maintain performance. &nbsp;Performance degradation with use is called aging. &nbsp;Advances in file systems suggested in the early 2000's that file systems no longer needed defragmentation, because they were able to stay ahead of aging by the use of various heuristics. &nbsp;The mathematical models used to design the data structures used in BetrFS suggested that as storage devices have more bandwidth, aging should become a worse problem. &nbsp;Therefore, 15 year old studies that showed no file-system aging would be out of date. &nbsp;These models also suggested that B^e-trees would not suffer from aging. &nbsp;This paper introduced a simple method to age a file system. &nbsp;It further showed that standard file systems can age by a factor of 20 or more -- that is, performance can drop to 5% of peak -- and that BetrFS, as predicted, does not age. &nbsp;These results were surprising enough that this paper was invited for repeat presentation at two other conferences and the authors were invited to present a less technical version of the work in USENIX's magazine, login.</p> <p>The upcoming FAST 18 paper fixes the seemingly intractable problem of making file systems fast to read and making directory renames fast. &nbsp;The problem is that of updating path names. &nbsp;The path name of a file is the string that encodes the nesting directories that contain it. &nbsp;So if a file a.txt is inside a directory called b, which is inside a directory called c, then the path name of the file is /c/b/a.txt. &nbsp;Keeping files in path-name order makes reading them fast, because files that are near each other in this order tend to be accessed at similar times. &nbsp;On the other hand, if a directory gets moved or renamed, the path name of every file below it in the directory tree gets a new path name, which makes rename slow. &nbsp;The standard solution is to introduce so-called inodes, which makes renames fast but reads slow. &nbsp;This paper shows how to make both fast, by modifying the B^e-tree where the data is stored.</p> <p>These four papers are only some of the work produced by this grant, but they illustrate the main arc of this research over the last few years.</p><br> <p>            Last Modified: 01/03/2018<br>      Modified by: Martin&nbsp;Farach-Colton</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ File systems are at the heart of all computers.  They maintain the contents of files and folders/directories, as well as metadata such as the time of last modification. File-system performance bottlenecks can reduce the overall performance of computing platforms at every scale, from smart phones to supercomputers.  Modern file systems are mostly built on top of data structures that have been around for decades.  The main thrust of this proposal was to build a new file system based on the new data structures that are fast for many file-system-critical operations.  The aim is twofold: to show that faster data structures translate into faster systems at the user level; and to show that many tradeoffs that are built into modern file systems can be overcome by radically redesigning the file system with the right data structures.  The main results are summarized in four papers, in the last four FAST conferences.  The USENIX FAST conference is the top conference for file systems.  The FAST 15 paper established the basic premise that fast data structures can indeed translate into better performance at the application level.  This paper introduced BetrFS, so named because it uses a B^e-tree to store both data and metadata.  This paper showed that data could be modified at every scale, from micro- to macro-updates, while allowing the file and directory contents to be read quickly.  Not every operation was fast in this version of BetrFS, and improving the performance of some key operations was left as future work.  This paper was one of three finalists for Best Paper award.  The FAST 16 paper showed that some of the performance problems in the FAST 15 paper were fixable.  In particular, sequential writes -- that is, writing the contents of a large file, such as when a file is downloaded -- was slower than in other file systems.  The problem was that such large files were being written twice: once as the content of a file, and once in a log file designed to make the file system secure against computer crashes.  The FAST 16 paper showed how to write the file once while still allowing the file system to remain secure against computer crashes.  Surprisingly, most file systems in use today can become corrupted if the computer crashes, so achieving both crash safety and high performance was an important advance.  This paper received the Best Paper award.  The FAST 17 paper studied the problem of file-system aging.  Many users of older Windows machines will remember that these computer required defragmentation from time to time in order to maintain performance.  Performance degradation with use is called aging.  Advances in file systems suggested in the early 2000's that file systems no longer needed defragmentation, because they were able to stay ahead of aging by the use of various heuristics.  The mathematical models used to design the data structures used in BetrFS suggested that as storage devices have more bandwidth, aging should become a worse problem.  Therefore, 15 year old studies that showed no file-system aging would be out of date.  These models also suggested that B^e-trees would not suffer from aging.  This paper introduced a simple method to age a file system.  It further showed that standard file systems can age by a factor of 20 or more -- that is, performance can drop to 5% of peak -- and that BetrFS, as predicted, does not age.  These results were surprising enough that this paper was invited for repeat presentation at two other conferences and the authors were invited to present a less technical version of the work in USENIX's magazine, login.  The upcoming FAST 18 paper fixes the seemingly intractable problem of making file systems fast to read and making directory renames fast.  The problem is that of updating path names.  The path name of a file is the string that encodes the nesting directories that contain it.  So if a file a.txt is inside a directory called b, which is inside a directory called c, then the path name of the file is /c/b/a.txt.  Keeping files in path-name order makes reading them fast, because files that are near each other in this order tend to be accessed at similar times.  On the other hand, if a directory gets moved or renamed, the path name of every file below it in the directory tree gets a new path name, which makes rename slow.  The standard solution is to introduce so-called inodes, which makes renames fast but reads slow.  This paper shows how to make both fast, by modifying the B^e-tree where the data is stored.  These four papers are only some of the work produced by this grant, but they illustrate the main arc of this research over the last few years.       Last Modified: 01/03/2018       Submitted by: Martin Farach-Colton]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
