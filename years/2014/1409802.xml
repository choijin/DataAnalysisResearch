<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR:  Medium:  Distributed Inference Algorithms for Machine Learning and Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>1200000.00</AwardTotalIntnAmount>
<AwardAmount>1200000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Aidong Zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Machine learning systems are operating at increasing scale in ways that benefit nearly all areas of human activity, from improved voice recognition, search and advertising, automatic language translation, and on the horizon, to activities such as self-driving cars. It is already extremely hard to implement on large, scalable clusters of computers the "inference algorithms" that enable these systems, and future trends of computer data centers will further exacerbate this difficulty: increasingly large numbers of nodes, heterogeneous clusters that mix conventional microprocessors, graphics processors, larger numbers of small and power-efficient microprocessors, and hardware changes such as the introduction of flash-based solid-state disks. The goal of this proposal is to design, analyse, and implement novel inference algorithms that not only take advantage of these trends for high performance , but that also enable future, even-larger-scale systems to be implemented.&lt;br/&gt;&lt;br/&gt;    The proposal specifically aims to achieve the following:&lt;br/&gt;&lt;br/&gt;1. Develop a broad family of novel optimization algorithms for machine learning;&lt;br/&gt;2. Analyse their convergence properties theoretically, as well as empirically;&lt;br/&gt;3. Release open-source code implementing them.&lt;br/&gt;&lt;br/&gt;    The research proposed is based upon four likely shifts in the design of data centers of the future:&lt;br/&gt;&lt;br/&gt;1. Small and power efficient microprocessors with a much improved CPU power to energy consumption ratio will become common in the data centers of the future.&lt;br/&gt;2. Architectures mixing different types of hardware, ranging from computer graphics processors to general purpose multi-core microprocessors are becoming the norm among all major semiconductor manufacturers. These changes will propagate to the data center.&lt;br/&gt;3. Hard disks are increasingly being supplemented and replaced by solid state memory which requires 10,000 to 100,000 times less time to access.&lt;br/&gt;4. Modern network architectures that replace traditional hierarchical tree structures (with inherent bottlenecks) by more balanced layouts are being enabled by software-defined networking and specialized network chips.&lt;br/&gt;&lt;br/&gt;    All four of these aspects offer considerable potential to design faster machine learning algorithms. Doing so requires tightly coupled algorithmic and systems design that successfully creates algorithms that work well on the kinds of systems that can be built, and systems to be built that provide the right support for machine learning algorithms.&lt;br/&gt;&lt;br/&gt;    The software developed for this project will be distributed as open source.&lt;br/&gt;&lt;br/&gt;For further information see the project web site at:  http://www.parameterserver.org</AbstractNarration>
<MinAmdLetterDate>08/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409802</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Andersen</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David G Andersen</PI_FULL_NAME>
<EmailAddress>dga@cs.cmu.edu</EmailAddress>
<PI_PHON>4122683064</PI_PHON>
<NSF_ID>000423704</NSF_ID>
<StartDate>08/25/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Andersen</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David G Andersen</PI_FULL_NAME>
<EmailAddress>dga@cs.cmu.edu</EmailAddress>
<PI_PHON>4122683064</PI_PHON>
<NSF_ID>000423704</NSF_ID>
<StartDate>08/10/2014</StartDate>
<EndDate>08/25/2017</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Smola</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander J Smola</PI_FULL_NAME>
<EmailAddress>smola@cmu.edu</EmailAddress>
<PI_PHON>4122681161</PI_PHON>
<NSF_ID>000622084</NSF_ID>
<StartDate>08/10/2014</StartDate>
<EndDate>08/25/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Suvrit</FirstName>
<LastName>Sra</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Suvrit Sra</PI_FULL_NAME>
<EmailAddress>suvrit@mit.edu</EmailAddress>
<PI_PHON>6172533816</PI_PHON>
<NSF_ID>000657401</NSF_ID>
<StartDate>08/10/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~424240</FUND_OBLG>
<FUND_OBLG>2015~382638</FUND_OBLG>
<FUND_OBLG>2016~393122</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main goals of this project were to design, analyze, and implement novel inference algorithms that take advantage of emerging hardware paradigms and enable highly scalable analysis of massive datasets. The project has numerous concrete outcomes that deliver on these goals. These are summarized below.</p> <p>&nbsp;</p> <ul> <li>&nbsp;<strong>Software:</strong> (i) One major output was mxnet (https://github.com/dmlc/mxnet), a modern, high-throughput deep-learning toolkit that subsequently went on to grow to a very large extent, and took on a life of its own, well-beyond this project; (ii) An open source parameter server implementation that makes it easy to perform large-scale distributed machine learning. Research papers describing these major pieces of software have been published at leading conferences in machine learning and systems.&nbsp;</li> <li>&nbsp;<strong>Systems+ML research:</strong> Design of efficient data structures and algorithms for (distributed) machine learning, e.g., Cuckoo Linear Algebra, a novel a novel data structure for sparse vectors based&nbsp;upon recent advances by PI Andersen's group on Cuckoo Hashing.&nbsp;DiFacto (https://github.com/dmlc/difacto ), recognized with a best paper honorable mention at WSDM'15</li> </ul> <ul> <li><strong>Algorithms and theory:</strong> This project contributed fundamentally towards developing the theory of modern, large-scale nonconvex optimization, in particular, in developing the theory of variance reduced stochastic methods (which are provably better than the widely used stochastic gradient method), as well as initial progress towards a far-reaching theory of non-Euclidean optimization. This progress has led to over 10 papers in the leading machine learning conferences (e.g., NeurIPS, ICML, among others)</li> <li><strong>Research training:</strong> Given the high demand nature of the topic studied by this project, a large number of graduate students got mentorship as part of this project. These students include: Mu Li, Sashank Reddi, Tianqi Chen, and &nbsp;Manzil Zaheer (CMU), both of whom now work for Google research; and Zelda Mariet, Hongyi Zhang, Chengtao Li, and Chulhee Yun (MIT).</li> <li><strong>Broader impact:</strong> This project involved a broader impact and outreach via several workshops on the topic of large-scale ML, organized by some of the PIs, as well as workshops, and invited talks as well as tutorials at machine learning summer schools across the globe, where the PIs delivered lectures. Moreover, a tutorial held by PI Sra at NIPS 2016 was built largely on the output from this project. Perhaps the widest practical impact, ultimately coming from this project, is that now one of the PIs (Alex Smola), has moved to Amazon, where he has been instrumental in translating mxnet (see "Software" section above) into an extremely widely available tool. And possibly, the broader theoretical impact lies in the new applications and techniques enabled by the research of this project (as seen by the diverse and large body of follow up work that cites the research output of this project).</li> </ul><br> <p>            Last Modified: 12/18/2018<br>      Modified by: Suvrit&nbsp;Sra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main goals of this project were to design, analyze, and implement novel inference algorithms that take advantage of emerging hardware paradigms and enable highly scalable analysis of massive datasets. The project has numerous concrete outcomes that deliver on these goals. These are summarized below.      Software: (i) One major output was mxnet (https://github.com/dmlc/mxnet), a modern, high-throughput deep-learning toolkit that subsequently went on to grow to a very large extent, and took on a life of its own, well-beyond this project; (ii) An open source parameter server implementation that makes it easy to perform large-scale distributed machine learning. Research papers describing these major pieces of software have been published at leading conferences in machine learning and systems.   Systems+ML research: Design of efficient data structures and algorithms for (distributed) machine learning, e.g., Cuckoo Linear Algebra, a novel a novel data structure for sparse vectors based upon recent advances by PI Andersen's group on Cuckoo Hashing. DiFacto (https://github.com/dmlc/difacto ), recognized with a best paper honorable mention at WSDM'15   Algorithms and theory: This project contributed fundamentally towards developing the theory of modern, large-scale nonconvex optimization, in particular, in developing the theory of variance reduced stochastic methods (which are provably better than the widely used stochastic gradient method), as well as initial progress towards a far-reaching theory of non-Euclidean optimization. This progress has led to over 10 papers in the leading machine learning conferences (e.g., NeurIPS, ICML, among others) Research training: Given the high demand nature of the topic studied by this project, a large number of graduate students got mentorship as part of this project. These students include: Mu Li, Sashank Reddi, Tianqi Chen, and  Manzil Zaheer (CMU), both of whom now work for Google research; and Zelda Mariet, Hongyi Zhang, Chengtao Li, and Chulhee Yun (MIT). Broader impact: This project involved a broader impact and outreach via several workshops on the topic of large-scale ML, organized by some of the PIs, as well as workshops, and invited talks as well as tutorials at machine learning summer schools across the globe, where the PIs delivered lectures. Moreover, a tutorial held by PI Sra at NIPS 2016 was built largely on the output from this project. Perhaps the widest practical impact, ultimately coming from this project, is that now one of the PIs (Alex Smola), has moved to Amazon, where he has been instrumental in translating mxnet (see "Software" section above) into an extremely widely available tool. And possibly, the broader theoretical impact lies in the new applications and techniques enabled by the research of this project (as seen by the diverse and large body of follow up work that cites the research output of this project).        Last Modified: 12/18/2018       Submitted by: Suvrit Sra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
