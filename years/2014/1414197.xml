<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Large: CrowdProgramming</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1403377.00</AwardTotalIntnAmount>
<AwardAmount>1403377</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Crowdsourcing has had extraordinary success in solving a diverse set of problems, ranging from digitization of libraries and translation of the Internet, to scientific challenges such as classifying elements in the galaxy or determining the 3D shape of an enzyme. By leveraging the power of the masses, it is feasible to complete tasks in mere days and sometimes even hours, and to take on tasks that were previously impossible because of their sheer scale. &lt;br/&gt;&lt;br/&gt;This proposal addresses the question of whether the same kinds of successes that crowdsourcing is having in revolutionizing other domains can be brought to software development. As compared to crowdsourcing in other domains, however, software is particularly challenging to crowdsource: it is inherently non-uniform, steeped with dependencies, difficult to describe in terms of the functionality desired, and can be implemented in any number of ways. The fundamental question that we address in this research, then, is how the nature of software impacts what may and may not be possible in terms of crowdsourcing. &lt;br/&gt;&lt;br/&gt;Specific outcomes from this research will include: (1) theoretical understandings of crowd programming in terms of whether it can be achieved, in which form(s), under what conditions, and with which benefits and drawbacks, (2) a publicly available crowdsourcing platform, CrowdCode, that offers a tool set specifically designed to address the intricacies of crowd programming, and (3) lessons learned from our project that could well influence crowdsourcing research in other domains, as the broader crowdsourcing community is actively seeking to understand how to support more complex, interdependent tasks.</AbstractNarration>
<MinAmdLetterDate>06/04/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/03/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1414197</AwardID>
<Investigator>
<FirstName>Adriaan</FirstName>
<LastName>van der Hoek</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adriaan W van der Hoek</PI_FULL_NAME>
<EmailAddress>andre@ics.uci.edu</EmailAddress>
<PI_PHON>9498246326</PI_PHON>
<NSF_ID>000418719</NSF_ID>
<StartDate>06/04/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>LaToza</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas LaToza</PI_FULL_NAME>
<EmailAddress>tlatoza@gmu.edu</EmailAddress>
<PI_PHON>7039931677</PI_PHON>
<NSF_ID>000637316</NSF_ID>
<StartDate>06/04/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926173067</ZipCode>
<StreetAddress><![CDATA[5171 California Avenue, Ste 150]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~800000</FUND_OBLG>
<FUND_OBLG>2016~300000</FUND_OBLG>
<FUND_OBLG>2017~303377</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Crowdsourcing has had extraordinary success in solving a diverse set of problems, ranging from digitization of libraries and translation of the Internet, to scientific challenges such as classifying elements in the galaxy or determining the 3D shape of an enzyme. By leveraging the power of the masses, it is feasible to complete tasks in mere days and sometimes even hours, and to take on tasks that were previously impossible because of their sheer scale. Underlying the success of crowdsourcing is a common theme - the microtask. By breaking down the overall task at hand into microtasks (short, self-contained pieces of work), work can be performed independently, quickly, and in parallel - enabling numerous and often untrained participants to chip in.</p> <p>Our project addressed the question of whether the same kinds of successes that microtask crowdsourcing is having in revolutionizing other domains can be brought to software programming. That is, we asked whether it is possible to push well beyond the open source paradigm, which still relies on traditional, coarse-grained tasks, to a model in which programming proceeds through microtasks performed by vast numbers of crowd developers.</p> <p>Our goal, then, was to seek answers to the following three key research questions:</p> <ol> <li>How can software be decomposed into microtasks?</li> <li>How can microtasks be coordinated at massive scale?</li> <li>How can quality be promoted?</li> </ol> <p>Towards answering these questions, the project pursued a research agenda that focused on three key aspects of software programming: coding/implementation, debugging, and design. Toward each of these aspects, we designed, implemented, and evaluated a variety of potential approaches. Each approach encompassed a particular workflow to orchestrate the multitude of microtask contributions made by individual crowdworkers, and was supported by a software tool that implemented that workflow and with which we performed detailed experiments.&nbsp;</p> <p>Throughout the project, we learned a broad variety of lessons about the feasibility of microtask crowdsourcing. Among them, we highlight the following findings contributing to the overall knowledge base of crowdsourcing in software engineering:</p> <ol> <li>Programming is complex work, for which existing crowdsourcing workflows are ill-suited. New challenges are arising because of the dynamic nature of programming tasks. </li> <li>It is possible to implement small-scale libraries using small crowds of developers, with a relatively high level of completeness and quality in a limited time.</li> <li>Developers are extremely tied to existing modes of work and particularly to engaging with coarse-grain tasks; they need to be encouraged strongly - and even required by the environment - to operate differently when programming with a crowd.</li> <li>Meaningful programming contributions can be made, in under 5 minutes and, in some cases, as little as 90 seconds.</li> <li>Crowds of contributors can be coordinated through a Q&amp;A mechanism in which contributors ask questions about crosscutting decisions and offer and discuss answers.</li> <li>It is possible for a crowd of workers, with a maximum replication factor of 20, to successfully identify a variety of bugs in single methods.</li> <li>Various subcrowds (e.g., all non-students, only the crowd that scored 100% on the qualifying test, only the crowd that considered the task the least difficult) perform better than the entire crowd, suggesting up-front filtering to obtain better results quicker.</li> <li>In terms of precision of method, the number of extra lines to inspect is at most 12 (in absolute terms) beyond the line(s) that contain the bug, with the two best performing aggregation methods incurring at most a 21% overhead in developers needing to inspect lines of code that do not contain the bug.</li> <li>Enabling developers to borrow and recombine design ideas from competing solutions leads to measurably better designs.</li> <li>It is possible for a crowd to generate a broad range of alternatives for individual decision points in a morphological chart, both for user interface design and internal code design.</li> <li>Overall diversity of ideas suffers when workers are exposed to previous work of other workers; it leads to periods of innovation stagnation during which similar types of solution alternatives are created by new workers.</li> <li>Overall quality also suffers when workers are exposed to previous work of other workers. It particularly appears that low quality examples lead to long streaks of further low-quality solution alternatives; the effect of high-quality examples has a much shorter duration of (positive) influence.</li> </ol> <p>Overall, our findings show the promise of microtask crowdsourcing, establishing that it is a viable method of &lsquo;getting work done&rsquo; at a small to medium scale.&nbsp; Our results equally show, however, that to scale up microtask crowdsourcing to significant systems, much further work is needed.&nbsp;</p> <p>Beyond software engineering, our results contribute a variety of workflows, models, and insights that may transfer to other domains where microtask crowdsourcing is being explored as a mechanism to create new ways of getting work done in a highly parallel manner.</p><br> <p>            Last Modified: 09/22/2019<br>      Modified by: Adriaan&nbsp;W&nbsp;Van Der Hoek</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Crowdsourcing has had extraordinary success in solving a diverse set of problems, ranging from digitization of libraries and translation of the Internet, to scientific challenges such as classifying elements in the galaxy or determining the 3D shape of an enzyme. By leveraging the power of the masses, it is feasible to complete tasks in mere days and sometimes even hours, and to take on tasks that were previously impossible because of their sheer scale. Underlying the success of crowdsourcing is a common theme - the microtask. By breaking down the overall task at hand into microtasks (short, self-contained pieces of work), work can be performed independently, quickly, and in parallel - enabling numerous and often untrained participants to chip in.  Our project addressed the question of whether the same kinds of successes that microtask crowdsourcing is having in revolutionizing other domains can be brought to software programming. That is, we asked whether it is possible to push well beyond the open source paradigm, which still relies on traditional, coarse-grained tasks, to a model in which programming proceeds through microtasks performed by vast numbers of crowd developers.  Our goal, then, was to seek answers to the following three key research questions:  How can software be decomposed into microtasks? How can microtasks be coordinated at massive scale? How can quality be promoted?   Towards answering these questions, the project pursued a research agenda that focused on three key aspects of software programming: coding/implementation, debugging, and design. Toward each of these aspects, we designed, implemented, and evaluated a variety of potential approaches. Each approach encompassed a particular workflow to orchestrate the multitude of microtask contributions made by individual crowdworkers, and was supported by a software tool that implemented that workflow and with which we performed detailed experiments.   Throughout the project, we learned a broad variety of lessons about the feasibility of microtask crowdsourcing. Among them, we highlight the following findings contributing to the overall knowledge base of crowdsourcing in software engineering:  Programming is complex work, for which existing crowdsourcing workflows are ill-suited. New challenges are arising because of the dynamic nature of programming tasks.  It is possible to implement small-scale libraries using small crowds of developers, with a relatively high level of completeness and quality in a limited time. Developers are extremely tied to existing modes of work and particularly to engaging with coarse-grain tasks; they need to be encouraged strongly - and even required by the environment - to operate differently when programming with a crowd. Meaningful programming contributions can be made, in under 5 minutes and, in some cases, as little as 90 seconds. Crowds of contributors can be coordinated through a Q&amp;A mechanism in which contributors ask questions about crosscutting decisions and offer and discuss answers. It is possible for a crowd of workers, with a maximum replication factor of 20, to successfully identify a variety of bugs in single methods. Various subcrowds (e.g., all non-students, only the crowd that scored 100% on the qualifying test, only the crowd that considered the task the least difficult) perform better than the entire crowd, suggesting up-front filtering to obtain better results quicker. In terms of precision of method, the number of extra lines to inspect is at most 12 (in absolute terms) beyond the line(s) that contain the bug, with the two best performing aggregation methods incurring at most a 21% overhead in developers needing to inspect lines of code that do not contain the bug. Enabling developers to borrow and recombine design ideas from competing solutions leads to measurably better designs. It is possible for a crowd to generate a broad range of alternatives for individual decision points in a morphological chart, both for user interface design and internal code design. Overall diversity of ideas suffers when workers are exposed to previous work of other workers; it leads to periods of innovation stagnation during which similar types of solution alternatives are created by new workers. Overall quality also suffers when workers are exposed to previous work of other workers. It particularly appears that low quality examples lead to long streaks of further low-quality solution alternatives; the effect of high-quality examples has a much shorter duration of (positive) influence.   Overall, our findings show the promise of microtask crowdsourcing, establishing that it is a viable method of ?getting work done? at a small to medium scale.  Our results equally show, however, that to scale up microtask crowdsourcing to significant systems, much further work is needed.   Beyond software engineering, our results contribute a variety of workflows, models, and insights that may transfer to other domains where microtask crowdsourcing is being explored as a mechanism to create new ways of getting work done in a highly parallel manner.       Last Modified: 09/22/2019       Submitted by: Adriaan W Van Der Hoek]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
