<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Small: Networking and Sensing Using Visible Light Communications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>499996.00</AwardTotalIntnAmount>
<AwardAmount>499996</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alexander Sprintson</SignBlockName>
<PO_EMAI>asprints@nsf.gov</PO_EMAI>
<PO_PHON>7032922170</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Mobile computing is accelerating beyond the smartphone era. Today, people wear smart glasses, smart watches, and fitness devices and carry smartphones, tablets and laptops. In a decade, the very same people are likely to wear or carry tens of wireless devices and interact with the Internet and computing infrastructure in markedly different ways. Two fundamental challenges emerge in a post-smartphone era: dealing with the impending spectrum crunch before it becomes a full-fledged crisis, and, advancing human-computer interaction (HCI) before people buckle under the demands of dealing with an ever increasing number of personal devices. This project seeks to address these two challenges by proposing a first of its kind integrated networking and sensing environment based on visible light communication (VLC). The proposed research advances VLC research in two new directions: VLC networking and VLC sensing.&lt;br/&gt;&lt;br/&gt;The project designs integrated VLC networking and sensing for smart spaces (iVLC) and a set of robust VLC networking and sensing algorithms. The VLC networking algorithms address the following challenges: 1) frequent connection disruptions caused by user mobility and the limited coverage of individual VLC links, 2) link interference from diffusive lights, and 3) determining the optimal placement and density of LED lights. Addressing these challenges requires efficient handoff schemes between adjacent LEDs, interference handling schemes for VLC networks, and optimization algorithms to determine the LED light deployment in smart spaces. The VLC sensing requires accurate inference of user gestures using shadows rendered on the floor. The combination of light from multiple light sources on the ceiling obfuscates and distorts these shadows. To tackle this challenge, the iVLC system separates shadow components by embedding identifiers into light signals from LEDs allowing light sensors on the floor to measure the light illuminance of individual light sources. Finally, a novel proof-of-concept iVLC testbed will be designed, built, and deployed, which comprises LEDs and light sensors. Results from the Bits in the Light Project will help accelerate the deployment of VLC as a new mobile networking and sensing technology.</AbstractNarration>
<MinAmdLetterDate>09/09/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1421528</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Campbell</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew T Campbell</PI_FULL_NAME>
<EmailAddress>campbell@cs.dartmouth.edu</EmailAddress>
<PI_PHON>6136468712</PI_PHON>
<NSF_ID>000103945</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Xia</FirstName>
<LastName>Zhou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xia Zhou</PI_FULL_NAME>
<EmailAddress>Xia.Zhou@Dartmouth.edu</EmailAddress>
<PI_PHON>6036468871</PI_PHON>
<NSF_ID>000659637</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Dartmouth College</Name>
<CityName>HANOVER</CityName>
<ZipCode>037551421</ZipCode>
<PhoneNumber>6036463007</PhoneNumber>
<StreetAddress>OFFICE OF SPONSORED PROJECTS</StreetAddress>
<StreetAddress2><![CDATA[11 ROPE FERRY RD #6210]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<StateCode>NH</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NH02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041027822</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF DARTMOUTH COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041027822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Department of Computer Science]]></Name>
<CityName>Hanover</CityName>
<StateCode>NH</StateCode>
<ZipCode>037553510</ZipCode>
<StreetAddress><![CDATA[6211 Sudikoff Lab, Rm. 208]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NH02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~499996</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project aims to turn the ubiquitous light around us into a medium that integrates data communication and behavioral sensing. It tackles systems challenges to advance the state of the art in visible light communication (VLC) and to integrate VLC with human sensing.&nbsp;&nbsp;</p> <p>== Intellectual merits&nbsp;<br />The team have made following intellectual contributions.&nbsp;<br />1. Visible light communication<br />1) The design and implementation of a new form of VLC between screens and cameras, where screens (e.g., screens of TVs, smartphones) communicate with devices equipped with cameras without any coded images on the screen. It supports any screen content, including dynamic content generated by user interactions (e.g., web browsing, gaming).&nbsp;</p> <p>2) Communication and networking designs to sustain VLC even when LEDs emit extremely-low illuminance. Efficient encoding and decoding schemes are designed to improve resulting data rates; link coexistence is analyzed; mechanisms are designed to decode data even when pulses from multiple LEDs are received.&nbsp;</p> <p>2. Visible light sensing<br />1) The design and implementation of a light sensing system that reconstructs skeleton poses in real time, without cameras or on-body sensors. It consists of VLC-enabled LED lights on the ceiling and photodiodes distributed in the environment. The system recovers shadows created by a human body and combines these shadows to infer skeleton poses.&nbsp;</p> <p>2) The optimization of photodiode placement to mitigate the impact of furniture blockage. Refinement on the pose reconstruction algorithm to deal with a mobile user with unknown orientation and location.&nbsp;&nbsp;</p> <p>3) Algorithmic design to enable the reconstruction of hand skeleton poses using a table lamp augmented by light sensing. It consists of an LED panel inside the lampshade and a grid of 16 photodiodes in the lampbase.&nbsp;</p> <p>4) The design and implementation of a self-powered module for gesture recognition. It consists of photodiodes operating in the photovoltaic mode, harvesting energy from ambient light while recognizing finger gestures based on instantaneously harvested power. Recognition algorithm was designed to enable robust recognition in diverse light conditions.&nbsp;<br /><br />== Broad impacts<br />Research outcomes have been disseminated via following publications and presented in these conferences/workshops: <br />- Xia Zhou and Andrew T. Campbell. Visible Light Networking and Sensing. ACM Workshop on Hot Topics in Wireless (HotWireless), 2014.</p> <p>- Tianxing Li, Chuankai An, Andrew T. Campbell, and Xia Zhou. HiLight: Hiding Bits in Pixel Translucency Changes. ACM Workshop on Visible Light Communication Systems (VLCS), 2014.&nbsp;</p> <p>- Tianxing Li, Chuankai An, Zhao Tian, Andrew T. Campbell, and Xia Zhou. Human Sensing Using Visible Light Communication. ACM Conference on Mobile Computing and Networking (MobiCom), 2015.</p> <p>- Chuankai An, Tianxing Li, Zhao Tian, Andrew T. Campbell, and Xia Zhou. Visible Light Knows Who You Are.&nbsp; ACM Workshop on Visible Light Communication Systems (VLCS), 2015.</p> <p>- Tianxing Li, Chuankai An, Xinran Xiao, Andrew T. Campbell, and Xia Zhou. Real-Time Screen-Camera Communication Behind Any Scene. ACM International Conference on Mobile Systems, Applications, and Services (MobiSys), 2015.&nbsp;</p> <p>- Zhao Tian, Kevin Wright, and Xia Zhou. Lighting Up the Internet of Things with DarkVLC. The Workshop on Mobile Computing Systems and Applications (HotMobile), 2016.&nbsp;</p> <p>- Tianxing Li, Qiang Liu, and Xia Zhou. Practical Human Sensing in the Light. ACM International Conference on Mobile Systems, Applications, and Services (MobiSys), 2016.</p> <p>- Zhao Tian, Kevin Wright, and Xia Zhou. The DarkLight Rises: Visible Light Communication in the Dark. ACM Conference on Mobile Computing and Networking (MobiCom), 2016.</p> <p>- Tianxing Li, Xi Xiong, Yifei Xie, George Hito, Xing-Dong Yang, and Xia Zhou. Reconstructing Hand Poses Using Visible Light. PACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 71, 2017. Presented at UbiComp 2017.&nbsp;</p> <p>- Yichen Li, Tianxing Li, Ruchir A. Patel, Xing-Dong Yang, and Xia Zhou. Self-Powered Gesture Recognition with Ambient Light. The ACM Symposium on User Interface Software and Technology (UIST), 2018.</p> <p>Dataset, source code, and demo video are available at project websites:&nbsp;<br />- http://dartnets.cs.dartmouth.edu/hilight<br />- http://dartnets.cs.dartmouth.edu/lisense<br />- http://dartnets.cs.dartmouth.edu/darklight<br />- http://dartnets.cs.dartmouth.edu/starlight<br />- http://dartnets.cs.dartmouth.edu/aili</p> <p>Project demo videos were disseminated in the lab YouTuble channel:&nbsp;<br />- https://youtu.be/VbG4VNA7RZs<br />- https://youtu.be/7wK-zo66GdY<br />- https://youtu.be/qwxLYC2z1C0<br />- https://youtu.be/DIDxR4zdrds<br />- https://youtu.be/Fl1vVc3UGLA<br />- https://youtu.be/GeSoc9CrvVY&nbsp;</p> <p>Results have also been disseminated in PI's 12 seminar talks, 1 keynote, and 6 invited talks.&nbsp;</p> <p>The project has provided opportunities to train 1 post-doc researcher, 4 PhD students, 6 Master students, 8 undergraduate students, and 1 high-school student. Research results are integrated into PI's courses, including CS69/169: All things wireless, and CS60: Computer Networking.&nbsp;</p> <p>The team have more broadly disseminated results in the local community and engaged the public via the following programs:&nbsp;<br />- 2015: Dartmouth Bound STEM Exploration Program (https://admissions.dartmouth.edu/ visits-programs/dartmouth-bound/stem-exploration-program), 30 - 40 high-school students;&nbsp;<br />- 2016: Workshop on Exploring Graduate Study in Computer Science (http://egs.cs.dartmouth.edu/), 20 undergraduate students;<br />- 2016: Dartmouth Information Expo (http://www.dartmouth.edu/~orientation/schedule/expo.html);&nbsp;<br />- 2016 &amp; 2017: the Hour of Code session (https://code.org/) at Hanover Ray Elementary School, 40 - 50 Grade-5 students and 87 Grade-5 students, respectively.&nbsp;</p> <p>The research has been covered in the media. Here are a few examples:<br />- http://www.theatlantic.com/technology/archive/2016/11/how-light-bulbs-watch-you-buy-groceries/508061/<br />- http://www.electronics-eetimes.com/news/communicating-invisibly-dimmed-down-visible-light<br />- http://cacm.acm.org/news/205978-smart-lighting-follows-people/<br />- http://www.gizmag.com/smart-light/38872/<br />- http://phys.org/news/2015-08-team-smart-shadows-track-human.html<br />- http://cacm.acm.org/news/180506-researchers-test-first-smart-spaces-using-light-to-send-data/fulltext</p><br> <p>            Last Modified: 11/22/2018<br>      Modified by: Xia&nbsp;Zhou</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project aims to turn the ubiquitous light around us into a medium that integrates data communication and behavioral sensing. It tackles systems challenges to advance the state of the art in visible light communication (VLC) and to integrate VLC with human sensing.    == Intellectual merits  The team have made following intellectual contributions.  1. Visible light communication 1) The design and implementation of a new form of VLC between screens and cameras, where screens (e.g., screens of TVs, smartphones) communicate with devices equipped with cameras without any coded images on the screen. It supports any screen content, including dynamic content generated by user interactions (e.g., web browsing, gaming).   2) Communication and networking designs to sustain VLC even when LEDs emit extremely-low illuminance. Efficient encoding and decoding schemes are designed to improve resulting data rates; link coexistence is analyzed; mechanisms are designed to decode data even when pulses from multiple LEDs are received.   2. Visible light sensing 1) The design and implementation of a light sensing system that reconstructs skeleton poses in real time, without cameras or on-body sensors. It consists of VLC-enabled LED lights on the ceiling and photodiodes distributed in the environment. The system recovers shadows created by a human body and combines these shadows to infer skeleton poses.   2) The optimization of photodiode placement to mitigate the impact of furniture blockage. Refinement on the pose reconstruction algorithm to deal with a mobile user with unknown orientation and location.    3) Algorithmic design to enable the reconstruction of hand skeleton poses using a table lamp augmented by light sensing. It consists of an LED panel inside the lampshade and a grid of 16 photodiodes in the lampbase.   4) The design and implementation of a self-powered module for gesture recognition. It consists of photodiodes operating in the photovoltaic mode, harvesting energy from ambient light while recognizing finger gestures based on instantaneously harvested power. Recognition algorithm was designed to enable robust recognition in diverse light conditions.   == Broad impacts Research outcomes have been disseminated via following publications and presented in these conferences/workshops:  - Xia Zhou and Andrew T. Campbell. Visible Light Networking and Sensing. ACM Workshop on Hot Topics in Wireless (HotWireless), 2014.  - Tianxing Li, Chuankai An, Andrew T. Campbell, and Xia Zhou. HiLight: Hiding Bits in Pixel Translucency Changes. ACM Workshop on Visible Light Communication Systems (VLCS), 2014.   - Tianxing Li, Chuankai An, Zhao Tian, Andrew T. Campbell, and Xia Zhou. Human Sensing Using Visible Light Communication. ACM Conference on Mobile Computing and Networking (MobiCom), 2015.  - Chuankai An, Tianxing Li, Zhao Tian, Andrew T. Campbell, and Xia Zhou. Visible Light Knows Who You Are.  ACM Workshop on Visible Light Communication Systems (VLCS), 2015.  - Tianxing Li, Chuankai An, Xinran Xiao, Andrew T. Campbell, and Xia Zhou. Real-Time Screen-Camera Communication Behind Any Scene. ACM International Conference on Mobile Systems, Applications, and Services (MobiSys), 2015.   - Zhao Tian, Kevin Wright, and Xia Zhou. Lighting Up the Internet of Things with DarkVLC. The Workshop on Mobile Computing Systems and Applications (HotMobile), 2016.   - Tianxing Li, Qiang Liu, and Xia Zhou. Practical Human Sensing in the Light. ACM International Conference on Mobile Systems, Applications, and Services (MobiSys), 2016.  - Zhao Tian, Kevin Wright, and Xia Zhou. The DarkLight Rises: Visible Light Communication in the Dark. ACM Conference on Mobile Computing and Networking (MobiCom), 2016.  - Tianxing Li, Xi Xiong, Yifei Xie, George Hito, Xing-Dong Yang, and Xia Zhou. Reconstructing Hand Poses Using Visible Light. PACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 71, 2017. Presented at UbiComp 2017.   - Yichen Li, Tianxing Li, Ruchir A. Patel, Xing-Dong Yang, and Xia Zhou. Self-Powered Gesture Recognition with Ambient Light. The ACM Symposium on User Interface Software and Technology (UIST), 2018.  Dataset, source code, and demo video are available at project websites:  - http://dartnets.cs.dartmouth.edu/hilight - http://dartnets.cs.dartmouth.edu/lisense - http://dartnets.cs.dartmouth.edu/darklight - http://dartnets.cs.dartmouth.edu/starlight - http://dartnets.cs.dartmouth.edu/aili  Project demo videos were disseminated in the lab YouTuble channel:  - https://youtu.be/VbG4VNA7RZs - https://youtu.be/7wK-zo66GdY - https://youtu.be/qwxLYC2z1C0 - https://youtu.be/DIDxR4zdrds - https://youtu.be/Fl1vVc3UGLA - https://youtu.be/GeSoc9CrvVY   Results have also been disseminated in PI's 12 seminar talks, 1 keynote, and 6 invited talks.   The project has provided opportunities to train 1 post-doc researcher, 4 PhD students, 6 Master students, 8 undergraduate students, and 1 high-school student. Research results are integrated into PI's courses, including CS69/169: All things wireless, and CS60: Computer Networking.   The team have more broadly disseminated results in the local community and engaged the public via the following programs:  - 2015: Dartmouth Bound STEM Exploration Program (https://admissions.dartmouth.edu/ visits-programs/dartmouth-bound/stem-exploration-program), 30 - 40 high-school students;  - 2016: Workshop on Exploring Graduate Study in Computer Science (http://egs.cs.dartmouth.edu/), 20 undergraduate students; - 2016: Dartmouth Information Expo (http://www.dartmouth.edu/~orientation/schedule/expo.html);  - 2016 &amp; 2017: the Hour of Code session (https://code.org/) at Hanover Ray Elementary School, 40 - 50 Grade-5 students and 87 Grade-5 students, respectively.   The research has been covered in the media. Here are a few examples: - http://www.theatlantic.com/technology/archive/2016/11/how-light-bulbs-watch-you-buy-groceries/508061/ - http://www.electronics-eetimes.com/news/communicating-invisibly-dimmed-down-visible-light - http://cacm.acm.org/news/205978-smart-lighting-follows-people/ - http://www.gizmag.com/smart-light/38872/ - http://phys.org/news/2015-08-team-smart-shadows-track-human.html - http://cacm.acm.org/news/180506-researchers-test-first-smart-spaces-using-light-to-send-data/fulltext       Last Modified: 11/22/2018       Submitted by: Xia Zhou]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
