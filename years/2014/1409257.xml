<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Write A Classifier: Learning Fine-Grained Visual Classifiers from Text and Images</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>463208.00</AwardTotalIntnAmount>
<AwardAmount>463208</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops the learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.  The research team investigates computational models for joint learning of visual concepts from images and textual descriptions of fine-grained categories, for example, discriminating between bird species.  The research activities have broader impact in three fields: computer vision, natural language processing, and machine learning. There is a huge need to develop algorithms to automatically understand the content of images and videos, with numerous potential applications in web searches, image and video archival and retrieval, surveillance applications, robot navigation and others. There are various applications for developing an intelligent system that can use narrative to define and recognize categories.&lt;br/&gt;&lt;br/&gt;This project addresses two research questions:  First, given a visual corpus and a textual corpus about a specific domain, how to jointly and effectively learn visual concepts? Second, given these two modalities how to facilitate learning novel visual concepts using only pure textual descriptions of novel categories in the domain? The research team approaches the problem on three integrated fronts: Learning, Natural Language Processing (NLP), and Computer Vision. On the learning front, the project investigates and develops algorithms suitable for learning and predicting visual classifiers with side textual information. On the NLP front, the project aims to develop novel methods for learning global and local discriminative category-level attributes and their values from text, with feedback from human computation and visual signal. The project investigates supervised and unsupervised methods for detecting visual text, and learning methods for deep language understanding to build such rich domain models from the noisy visual text. On the Vision front, the project addresses the tasks of detection and classification with side textual information. The project investigates models for the shape and appearance of a general category that can specialize to different subordinates, in a way that allows interpreting information from text within a proper geometric context, and handle variability in viewpoints and articulation.</AbstractNarration>
<MinAmdLetterDate>06/16/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409257</AwardID>
<Investigator>
<FirstName>Smaranda</FirstName>
<LastName>Muresan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Smaranda Muresan</PI_FULL_NAME>
<EmailAddress>smara@columbia.edu</EmailAddress>
<PI_PHON>2128701294</PI_PHON>
<NSF_ID>000542607</NSF_ID>
<StartDate>06/16/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~147708</FUND_OBLG>
<FUND_OBLG>2015~153081</FUND_OBLG>
<FUND_OBLG>2016~162419</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>When students learn from a teacher about different species, for example birds or flowers, the teacher will not just give you sample images of each species and their class labels; the teacher will tell you about discriminative visual or non-visual features for each species, similarity and differences between species, hierarchal relations between species, and many other aspects. The same learning experience takes place when you read a book or a web page to learn about the different species. Typically, the text will tell you about the taxonomy, highlights discriminative features and discusses similarities and differences between species, as well as within-species variations (male vs. female). This learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.</p> <p>The main aim of the proposal is to investigate models for joint learning of visual concepts from images and textual descriptions of fine-grained categories. One of the main challenges of zero-shot learning for fine-grained image classification is the detection of discriminative semantic attributes correlated with images. Using full-length articles such as Wikipedia pages as textual descriptions for an object has obtained limited success. Our team has developed a novel approach of automatically identifying visually relevant sentence with respect to an object from documents that may contain predominantly non-visual text detecting. Using such visually relevant text has proven to be more effective in zero-shot learning for fine-grained classification. However, an important aspect for fine- grained object classification in particular is the ability to capture cases where two objects differ by a slight variation in one of the properties, e.g., a small deviation in color. In textual descriptions, this differentiation is most often described using comparative adjectives (&ldquo;less orangish&rdquo;, &ldquo;lighter brown wings&rdquo;). To address this problem our team developed a new paradigm of grounding comparative adjectives for color description in the color space of an image. Given a reference color and a comparative term, the model learns to ground the comparative as a direction in the color space such that the colors along the vector, rooted at the reference color, satisfy the comparison. This approach opens the road of modeling subtle differences in objects, specifically when used together with part-based learning representations.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/19/2019<br>      Modified by: Smaranda&nbsp;Muresan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ When students learn from a teacher about different species, for example birds or flowers, the teacher will not just give you sample images of each species and their class labels; the teacher will tell you about discriminative visual or non-visual features for each species, similarity and differences between species, hierarchal relations between species, and many other aspects. The same learning experience takes place when you read a book or a web page to learn about the different species. Typically, the text will tell you about the taxonomy, highlights discriminative features and discusses similarities and differences between species, as well as within-species variations (male vs. female). This learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.  The main aim of the proposal is to investigate models for joint learning of visual concepts from images and textual descriptions of fine-grained categories. One of the main challenges of zero-shot learning for fine-grained image classification is the detection of discriminative semantic attributes correlated with images. Using full-length articles such as Wikipedia pages as textual descriptions for an object has obtained limited success. Our team has developed a novel approach of automatically identifying visually relevant sentence with respect to an object from documents that may contain predominantly non-visual text detecting. Using such visually relevant text has proven to be more effective in zero-shot learning for fine-grained classification. However, an important aspect for fine- grained object classification in particular is the ability to capture cases where two objects differ by a slight variation in one of the properties, e.g., a small deviation in color. In textual descriptions, this differentiation is most often described using comparative adjectives ("less orangish", "lighter brown wings"). To address this problem our team developed a new paradigm of grounding comparative adjectives for color description in the color space of an image. Given a reference color and a comparative term, the model learns to ground the comparative as a direction in the color space such that the colors along the vector, rooted at the reference color, satisfy the comparison. This approach opens the road of modeling subtle differences in objects, specifically when used together with part-based learning representations.          Last Modified: 12/19/2019       Submitted by: Smaranda Muresan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
