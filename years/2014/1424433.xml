<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Characterizing Nonresponse Error across General Population Survey Data Collection Modes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>308225.00</AwardTotalIntnAmount>
<AwardAmount>308225</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project will advance knowledge about nonresponse error introduced by different modes of data collection for a wide variety of survey measures.  The results of this research will contribute to the development of surveys employing multiple modes, thus enabling greater public participation and strengthening survey data quality.  Population-based surveys are used to measure everything from unemployment to the health of the nation, guiding policy decision making at national, state, and local levels.  But responses to surveys in general, and telephone surveys in particular, have declined significantly over the past two decades.  As a result, researchers are exploring multiple modes of survey data collection.  This project will provide additional information about the magnitude of nonresponse error among alternative modes of data collection.&lt;br/&gt;&lt;br/&gt;This project will estimate nonresponse error directly rather than inferring it from demographic differences between those who respond and do not respond to different survey protocols.  Nonresponse error will be measured for two modes of survey administration: computer-assisted telephone interviewing (CATI) and telephonic interactive voice response (IVR).  To minimize mode effects, questions for the test surveys will be selected among those least likely to be impacted by mode.  These questions will be chosen among questions included in several federal surveys, such as the Health Information National Trends Survey, National Household Travel Survey, the Behavioral Risk Factor Surveillance System survey, the National Crime Victimization Survey, the National Immunization Survey, and the Current Population Survey.  Using an address-based sample frame, residents in diverse neighborhoods in the Boston metropolitan area will be randomly assigned to one of two experimental survey modes: CATI or IVR.  At the end of survey field period, nonrespondents will be assigned to in-person interviewers.  The interviewers will contact household members who did not respond to the survey, inviting them to complete the survey.  Monetary incentives will be provided to encourage response.  In-person interviews typically have the highest response rates among any survey administration mode.  Data from CATI and IVR nonrespondents will be used to assess nonresponse error for key measures included in the survey.  The resulting combined samples will provide direct estimates of the variables included in the survey with a minimum of nonresponse error against which to compare estimates from the two survey administration modes.  The project is supported by the Methodology, Measurement, and Statistics Program and a consortium of federal statistical agencies as part of a joint activity to support research on survey and statistical methodology.</AbstractNarration>
<MinAmdLetterDate>08/28/2014</MinAmdLetterDate>
<MaxAmdLetterDate>02/26/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1424433</AwardID>
<Investigator>
<FirstName>J. Lee</FirstName>
<LastName>Hargraves</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>J. Lee Hargraves</PI_FULL_NAME>
<EmailAddress>lee.hargraves@umb.edu</EmailAddress>
<PI_PHON>6172877200</PI_PHON>
<NSF_ID>000639372</NSF_ID>
<StartDate>08/28/2014</StartDate>
<EndDate>02/26/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Philip</FirstName>
<LastName>Brenner</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Philip S Brenner</PI_FULL_NAME>
<EmailAddress>philip.brenner@umb.edu</EmailAddress>
<PI_PHON>6172875370</PI_PHON>
<NSF_ID>000649228</NSF_ID>
<StartDate>02/26/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Boston</Name>
<CityName>Dorchester</CityName>
<ZipCode>021253300</ZipCode>
<PhoneNumber>6172875370</PhoneNumber>
<StreetAddress>100 Morrissey Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>808008122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Center for Survey Research]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>021253300</ZipCode>
<StreetAddress><![CDATA[100 Morrissey Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<ProgramElement>
<Code>N170</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>N192</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>N583</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>N641</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>NX45</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>NX52</Code>
<Text/>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~308225</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This study estimates the effects of survey nonresponse on estimates based on address-based samples using interactive voice response (IVR) or telephone data collection methods.&nbsp; Two address based samples were drawn from three neighborhoods and two suburbs of Boston.&nbsp; One sample was assigned to an IVR protocol; an automated method of telephone data collection.&nbsp; A letter was sent to the household explaining the sponsorship and purposes of the survey was sent to each selected household.&nbsp; The letter included $2 cash and asked a randomly selected adult to call to complete a survey.</span></p> <p><span>For the second sample, wherever possible telephone numbers were matched to selected households.&nbsp; If a matching number was found, a letter was sent with a $2 incentive explaining that an interviewer would call soon.&nbsp; A minimum of six calls were made.&nbsp; Households for which telephone numbers could not be found received a letter saying that in-person interviewers would be visiting to do a survey interview and promising a $20 post-interview incentive.&nbsp; Following this initial wave of data collection, samples of nonrespondents from both the IVR and telephone protocols were also assigned to the in-person household interview protocol.&nbsp;</span></p> <p><span>The survey instrument included items drawn from federally administered and sponsored surveys across a variety of topics, including health and health care, experiences with crime, civic engagement and participation, housing, employment and labor force status, religious participation, technology use, and transportation.</span></p> <p><span>Subsequent analysis assesses the effect of survey nonresponse on estimates from the primary data collection protocols by comparing them to the follow-up in-person interviews.&nbsp; Findings vary between measures and protocols.&nbsp; Results suggest that estimates based on the telephone surveys do include bias for some of the measures, although other measures to appear to be unbiased by survey nonresponse.&nbsp; Results also suggest that most estimates based on the IVR surveys are not biased by survey nonresponse.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 12/16/2017<br>      Modified by: Philip&nbsp;S&nbsp;Brenner</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This study estimates the effects of survey nonresponse on estimates based on address-based samples using interactive voice response (IVR) or telephone data collection methods.  Two address based samples were drawn from three neighborhoods and two suburbs of Boston.  One sample was assigned to an IVR protocol; an automated method of telephone data collection.  A letter was sent to the household explaining the sponsorship and purposes of the survey was sent to each selected household.  The letter included $2 cash and asked a randomly selected adult to call to complete a survey.  For the second sample, wherever possible telephone numbers were matched to selected households.  If a matching number was found, a letter was sent with a $2 incentive explaining that an interviewer would call soon.  A minimum of six calls were made.  Households for which telephone numbers could not be found received a letter saying that in-person interviewers would be visiting to do a survey interview and promising a $20 post-interview incentive.  Following this initial wave of data collection, samples of nonrespondents from both the IVR and telephone protocols were also assigned to the in-person household interview protocol.   The survey instrument included items drawn from federally administered and sponsored surveys across a variety of topics, including health and health care, experiences with crime, civic engagement and participation, housing, employment and labor force status, religious participation, technology use, and transportation.  Subsequent analysis assesses the effect of survey nonresponse on estimates from the primary data collection protocols by comparing them to the follow-up in-person interviews.  Findings vary between measures and protocols.  Results suggest that estimates based on the telephone surveys do include bias for some of the measures, although other measures to appear to be unbiased by survey nonresponse.  Results also suggest that most estimates based on the IVR surveys are not biased by survey nonresponse.          Last Modified: 12/16/2017       Submitted by: Philip S Brenner]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
