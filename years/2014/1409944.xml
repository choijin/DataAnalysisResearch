<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Medium: Collaborative Research: Scalable Algorithms for Spatio-temporal Data Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Acceleration of computing power of supercomputers along with development and deployment of large instruments such as telescopes, colliders, sensors and devices raises one fundamental question. "Can the time to insight and knowledge discovery be reduced at the same exponential rate?" The answer currently is clearly "NO", because a critical step that combines analytics, mining and discovering knowledge from the massive datasets has lagged far behind advances in software, simulation and generation of data. Analysis of data requires "data-driven" computing and analytics. This entails scalable software for data reduction, approximations, analysis, statistics, and bottom-up discovery. Scalable and parallel analytics software for processing large amount of data is required in order to make a significant leap forward in scientific discoveries. &lt;br/&gt;This project develops innovative, scalable, and sustainable data analytics algorithms to enable analysis and mining of massive data on high-performance parallel computers, which include (1) bottom-up and unsupervised data clustering algorithms that are suitable for spatio-temporal data, massive graph analytics, community computations, and detection of patterns in time-varying graphs, different types of data, and different data characteristics; (2) change detection and anomaly detection in spatio-temporal data; and (3) tracking moving data and cluster dynamics within certain time and space constraints. These parallel algorithms use the massive amount of data generated from scientific applications, such as astrophysics, cosmology simulations, climate modeling, and social networking analysis, for result verification and performance evaluation on modern high-performance parallel computers.&lt;br/&gt;This project directly addresses the critical needs for spatio-temporal data analysis, performance scalability, and programming productivity of large-scale scientific discovery via parallel analytics software for big data. This work will impact applications of enormous societal benefits and scientific importance such as climate understanding, environmental sustainability, astrophysics, biology and medicine by accelerating scientific discoveries. Furthermore, the developed software infrastructure can be used and adopted in commercial applications, such as commerce, social, security, drug discovery, and so on. The source codes are open to the public for all community to adapt, build-upon, customize and contribute to, thereby multiplying its value and usage.</AbstractNarration>
<MinAmdLetterDate>05/21/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/21/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409944</AwardID>
<Investigator>
<FirstName>Salman</FirstName>
<LastName>Habib</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Salman Habib</PI_FULL_NAME>
<EmailAddress>habib@anl.gov</EmailAddress>
<PI_PHON>6302521110</PI_PHON>
<NSF_ID>000621940</NSF_ID>
<StartDate>05/21/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606375418</ZipCode>
<StreetAddress><![CDATA[5735 S. Ellis Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The work undertaken in this project was in the area of large-scale data analytics for massive simulations carried out on parallel supercomputers. In applications such as astrophysics and cosmology, where codes use simulation particles as mass tracers, one has to follow small localized structures (particle clusters, called halos) -- how they are born, evolve, and merge. Additionally, one is interested in knowing the density field as a relatively smooth function of space, rather than as a very large number of density spikes (the particles). Our project introduced new ways of following the small structures (a method called "core-tracking"), new ways of constructing the timeline of halo births, interactions, and mergers (so-called "merger trees") and a new, fast, method of density estimation.</p> <p>The algorithms are characterized by scalability, speed, and robustness to the inherent noise of particle-based datasets. Because the data is usually highly clustered, the workloads are highly inhomogeneous, and the algorithms must properly share the work across a large number of processors. Task-based load balancing approaches are used to solve this probem.</p> <p>These new algorithms are already playing an important role in cosmological applications. The density estimator is being used for calculating the effects of light-bending by gravity (gravitational lensing), a key cosmological probe. The core-tracking method, along with the new merger tree technique, are both being used to construct synthetic galaxy catalogs for ongoing and future sky surveys. The galaxies are placed on the core positions and their properties are based on empirical methods or on semi-analytic models of galaxy formation that use the halo properties on a given merger tree as input to the galaxy modeling scheme.</p> <p>Aside from the applications mentioned above, the algorithms are also of interest in general problems of network analysis and density estimation where the fundamental quantity is essentially particulate (e.g., atomic, molecular, or dust grains).</p><br> <p>            Last Modified: 01/29/2018<br>      Modified by: Salman&nbsp;Habib</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1409944/1409944_10304421_1517208307953_fig--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1409944/1409944_10304421_1517208307953_fig--rgov-800width.jpg" title="Density field of a galaxy cluster"><img src="/por/images/Reports/POR/2018/1409944/1409944_10304421_1517208307953_fig--rgov-66x44.jpg" alt="Density field of a galaxy cluster"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A high-resolution projected density field of the host halo of a galaxy cluster in a very large cosmological simulation; this image was made from 1.6 million particles using a new method developed in this project.</div> <div class="imageCredit">E. Rangel, N. Li, T. Petreka, S. Habib</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Salman&nbsp;Habib</div> <div class="imageTitle">Density field of a galaxy cluster</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The work undertaken in this project was in the area of large-scale data analytics for massive simulations carried out on parallel supercomputers. In applications such as astrophysics and cosmology, where codes use simulation particles as mass tracers, one has to follow small localized structures (particle clusters, called halos) -- how they are born, evolve, and merge. Additionally, one is interested in knowing the density field as a relatively smooth function of space, rather than as a very large number of density spikes (the particles). Our project introduced new ways of following the small structures (a method called "core-tracking"), new ways of constructing the timeline of halo births, interactions, and mergers (so-called "merger trees") and a new, fast, method of density estimation.  The algorithms are characterized by scalability, speed, and robustness to the inherent noise of particle-based datasets. Because the data is usually highly clustered, the workloads are highly inhomogeneous, and the algorithms must properly share the work across a large number of processors. Task-based load balancing approaches are used to solve this probem.  These new algorithms are already playing an important role in cosmological applications. The density estimator is being used for calculating the effects of light-bending by gravity (gravitational lensing), a key cosmological probe. The core-tracking method, along with the new merger tree technique, are both being used to construct synthetic galaxy catalogs for ongoing and future sky surveys. The galaxies are placed on the core positions and their properties are based on empirical methods or on semi-analytic models of galaxy formation that use the halo properties on a given merger tree as input to the galaxy modeling scheme.  Aside from the applications mentioned above, the algorithms are also of interest in general problems of network analysis and density estimation where the fundamental quantity is essentially particulate (e.g., atomic, molecular, or dust grains).       Last Modified: 01/29/2018       Submitted by: Salman Habib]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
