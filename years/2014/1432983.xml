<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I/UCRC FRP:  Collaborative Research: Autonomous Perception and Manipulation in Search and Rescue</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>80000.00</AwardTotalIntnAmount>
<AwardAmount>80000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Thyagarajan Nandagopal</SignBlockName>
<PO_EMAI>tnandago@nsf.gov</PO_EMAI>
<PO_PHON>7032924550</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project wants to capitalize on a team of robots that move in order to better sense the environment and then perform basic manipulation tasks. The vision of the project is to integrate robots easily, provide human-team interfaces, and develop manipulation algorithms. The research will involve the development of perception strategies and manipulation schemes that will allow operation of the robot teams in real-world environments during search and rescue missions. In addition, this research will involve working in cluttered scenes where the lighting conditions may not be ideal. The project addresses: (i) Research on the novel problem of robotic perception and manipulation of target objects that interact with other objects as an integral part of the environment, which cannot be fully isolated in views and in physical arrangements before being manipulated; (ii) An appearance-based approach for recognition and pose estimation of 3D objects in cluttered scenes from a single view; (iii) Development of a measure of scene recognizability from each viewpoint to evaluate how accurately partially-occluded objects are recognized and how well their poses are estimated; (iv) Creation of solutions for disassembly analysis of 3D structures, extending our preliminary analysis of 2D structures; (v) Development of grasping based on the results of perception and with the aid of stability analysis of the arrangement of the objects and their interaction with the environment and with one another; and (vi) Experimental validation of the system in real-world settings, in close consultation with our industrial partners.&lt;br/&gt;&lt;br/&gt;This project will allow the creation of manipulation capabilities along with perception schemes to facilitate the development of a multi-robot team for search and rescue missions. The impacts of the project include: (i) Expansion of the annual robot summer camp to include robot-team activities with the objective of attracting middle-schoolers from under-represented groups to computer science/electrical engineering; (ii) Integration of the activities with first responders through the UPenn and UNC Charlotte collaborations; (ii) Experimental validation in SAFL at UMN and the Disaster City in TX; (iii) Offering project themes to REU undergraduates or to the UROP program; (iv) Outreach programs that involve demonstrations to local K-12 institutions; and (v) Inclusion of the project theme to the regular curricula.</AbstractNarration>
<MinAmdLetterDate>08/07/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1432983</AwardID>
<Investigator>
<FirstName>Srinivas</FirstName>
<LastName>Akella</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Srinivas Akella</PI_FULL_NAME>
<EmailAddress>sakella@uncc.edu</EmailAddress>
<PI_PHON>7046878573</PI_PHON>
<NSF_ID>000169704</NSF_ID>
<StartDate>08/07/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jing</FirstName>
<LastName>Xiao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jing Xiao</PI_FULL_NAME>
<EmailAddress>jxiao2@wpi.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000327458</NSF_ID>
<StartDate>08/07/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Charlotte</Name>
<CityName>CHARLOTTE</CityName>
<ZipCode>282230001</ZipCode>
<PhoneNumber>7046871888</PhoneNumber>
<StreetAddress>9201 University City Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066300096</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Charlotte]]></Name>
<CityName/>
<StateCode>NC</StateCode>
<ZipCode>282230001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramReference>
<Code>5761</Code>
<Text>INDUSTRY/UNIV COOP RES CENTERS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~80000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project is to investigate how to enable autonomous perception of an environment and autonomous robotic manipulation to assist search and rescue missions, such as detecting and recognizing objects in cluttered and unstructured scenes in order to fetch them. One key idea is to interleave perception and manipulation so that perception enables manipulation, which in turn, enables further perception.</p> <p>The project outcomes include a number of algorithms and methodologies. An algorithm was developed to identify and estimate poses (i.e., positions and orientations) of partially occluded 3D objects in a cluttered scene based on features of surface segments of the objects, taking advantage of RGB-D data from a RGB-D camera, such as a Microsoft Kinect. This algorithm can reconstruct the scene, i.e., reconstruct the objects and their poses in the scene, from a single view point robustly and efficiently. &nbsp;A learning-based automatic evaluator was also developed for scene recognizability from each viewpoint based on how many and how accurately partially occluded objects could recognized and how well their poses could be estimated. This evaluator is useful for deciding the best viewpoints to perceive an environment. Next, an approach of interleaving RGB-D perception and robotic manipulation by an articulated manipulator arm was developed to enable automatic modeling of an unknown object based on the visual appearance of all the object surfaces. An approach was further developed to enable modeling of an unknown object situated in an unknown environment by interleaving RGB-D perception and motion of a continuum manipulator, like an elephant trunk, holding the RGB-D sensor around the target object. Finally, an approach was developed to enable object detection based on touch and shape detection from manipulation by a continuum manipulator.</p> <p>The project results are disseminated in top robotics journals and conferences as well as in demos for outreach activities. Through this project, two PhD students are trained, and one of them have graduated.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/29/2017<br>      Modified by: Jing&nbsp;Xiao</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project is to investigate how to enable autonomous perception of an environment and autonomous robotic manipulation to assist search and rescue missions, such as detecting and recognizing objects in cluttered and unstructured scenes in order to fetch them. One key idea is to interleave perception and manipulation so that perception enables manipulation, which in turn, enables further perception.  The project outcomes include a number of algorithms and methodologies. An algorithm was developed to identify and estimate poses (i.e., positions and orientations) of partially occluded 3D objects in a cluttered scene based on features of surface segments of the objects, taking advantage of RGB-D data from a RGB-D camera, such as a Microsoft Kinect. This algorithm can reconstruct the scene, i.e., reconstruct the objects and their poses in the scene, from a single view point robustly and efficiently.  A learning-based automatic evaluator was also developed for scene recognizability from each viewpoint based on how many and how accurately partially occluded objects could recognized and how well their poses could be estimated. This evaluator is useful for deciding the best viewpoints to perceive an environment. Next, an approach of interleaving RGB-D perception and robotic manipulation by an articulated manipulator arm was developed to enable automatic modeling of an unknown object based on the visual appearance of all the object surfaces. An approach was further developed to enable modeling of an unknown object situated in an unknown environment by interleaving RGB-D perception and motion of a continuum manipulator, like an elephant trunk, holding the RGB-D sensor around the target object. Finally, an approach was developed to enable object detection based on touch and shape detection from manipulation by a continuum manipulator.  The project results are disseminated in top robotics journals and conferences as well as in demos for outreach activities. Through this project, two PhD students are trained, and one of them have graduated.             Last Modified: 10/29/2017       Submitted by: Jing Xiao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
