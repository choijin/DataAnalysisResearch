<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>A Grammar-Based Approach to Visual-Haptic Object Perception</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>399049.00</AwardTotalIntnAmount>
<AwardAmount>399049</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>People can perceive the shape of objects accurately and reliably but how this occurs is not yet understood. This ability may stem, at least in part, from our use of both visual information and haptic information (information obtained when an object is touched or grasped). Moreover, if we learn to recognize an object based on visual information, we can often recognize the same object when our eyes are closed but we are allowed to grasp it. Similarly, if we learn to recognize an object based on haptic information, we can often recognize the object when we see it but cannot touch it. In other words, we exhibit cross-modal transfer of object shape information. How does information from the eyes and hands link up in the brain to yield a coherent representation of object shape? Insights obtained from this research can contribute both to our understanding of how humans perceive object shape using vision and/or touch and to development of improved robotic and other artificial intelligence systems operating in multi-modal settings in industrial, medical, military, and other applications.&lt;br/&gt;&lt;br/&gt;The present project develops a theory of visual-haptic object shape perception in which people's notions of object similarity are not based on sensory features but rather on latent or hidden variables that represent object parts and their spatial relations in an abstract, modality-independent format. Object representations are formalized using a probabilistic "shape grammar" with Bayesian inference used to infer grammar-based object representations when an object is viewed, when it is grasped, or both. The model is tested using data obtained from behavioral studies of visual, haptic, and visual-haptic object shape perception by humans. The investigators will explore the types of representational change that underlie the transition from perceptual novice to expert (e.g, radiologists) and will assess whether perceptual expertise is well characterized as category learning, grammar learning, both, or neither. The research program will also develop a large public database of code for re-creating both visual and haptic features of complex objects. This will allow other researchers to fabricate the objects using a 3D printer, enhancing complementarity and comparison across research sites. Finally, training undergraduate and graduate students in the emerging field of computational cognitive science will contribute to a new generation of multidisciplinary scientists working across traditional boundaries between cognitive science and computer science.</AbstractNarration>
<MinAmdLetterDate>08/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1400784</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Jacobs</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert A Jacobs</PI_FULL_NAME>
<EmailAddress>robbie@bcs.rochester.edu</EmailAddress>
<PI_PHON>5852750753</PI_PHON>
<NSF_ID>000172078</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041294109</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ROCHESTER</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041294109</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Rochester]]></Name>
<CityName>Rochester</CityName>
<StateCode>NY</StateCode>
<ZipCode>146270140</ZipCode>
<StreetAddress><![CDATA[518 Hylan]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~399049</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>People can learn about the shape of an object by viewing the object, grasping the object, or both. This raises the following questions: If people view an object, can they guess what hand shape would be appropriate if they were to grasp the object? If people grasp an object, can they guess what the object would look like if they were to view it?</p> <p><br />This project addressed these questions using both behavioral experiments and computer simulations. Our experiments demonstrated that, indeed, people are capable of "cross-modal transfer". That is, they can use visual information to guess the correct hand shape for grasping an object, and can use tactile information to guess the correct image that would be received if the object were viewed. Our computer simulations explored the hypothesis that when viewing or grasping an object, people form a mental representation of the object's shape that is "modality independent", meaning that it carries information about the object's shape in a way that is not strictly tied to its visual or tactile properties. Simulations used methods developed in the field of machine learning to explore how modality-independent representations could be acquired and used for the purposes of cross-modal transfer. This work has implications for our understanding of human perception, as well as our understanding of artificial intelligence and robotics.</p> <p><br />In addition, the project greatly expanded a previously existing database of visual and tactile properties of naturalistic objects. This expanded database should be useful to both cognitive scientists in scientists studying artificial intelligence. The project also trained undergraduate and graduate students to obtain a variety of skills, including skills needed to design and implement behavior experiments and skills needed to design and implement complex computer simulations.<br /><br /></p><br> <p>            Last Modified: 09/04/2019<br>      Modified by: Robert&nbsp;A&nbsp;Jacobs</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ People can learn about the shape of an object by viewing the object, grasping the object, or both. This raises the following questions: If people view an object, can they guess what hand shape would be appropriate if they were to grasp the object? If people grasp an object, can they guess what the object would look like if they were to view it?   This project addressed these questions using both behavioral experiments and computer simulations. Our experiments demonstrated that, indeed, people are capable of "cross-modal transfer". That is, they can use visual information to guess the correct hand shape for grasping an object, and can use tactile information to guess the correct image that would be received if the object were viewed. Our computer simulations explored the hypothesis that when viewing or grasping an object, people form a mental representation of the object's shape that is "modality independent", meaning that it carries information about the object's shape in a way that is not strictly tied to its visual or tactile properties. Simulations used methods developed in the field of machine learning to explore how modality-independent representations could be acquired and used for the purposes of cross-modal transfer. This work has implications for our understanding of human perception, as well as our understanding of artificial intelligence and robotics.   In addition, the project greatly expanded a previously existing database of visual and tactile properties of naturalistic objects. This expanded database should be useful to both cognitive scientists in scientists studying artificial intelligence. The project also trained undergraduate and graduate students to obtain a variety of skills, including skills needed to design and implement behavior experiments and skills needed to design and implement complex computer simulations.         Last Modified: 09/04/2019       Submitted by: Robert A Jacobs]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
