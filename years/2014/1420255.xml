<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Detecting Misinformation Flows in Social Media Spaces During Crisis Events</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>467629.00</AwardTotalIntnAmount>
<AwardAmount>515046</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research seeks both to understand the patterns and mechanisms of the diffusion of misinformation on social media and to develop algorithms to automatically detect misinformation as events unfold.  During natural disasters and other hazard events, individuals increasingly utilize social media to disseminate, search for and curate event-related information.  Eyewitness accounts of event impacts can now be shared by those on the scene in a matter of seconds.  There is great potential for this information to be used by affected communities and emergency responders to enhance situational awareness and improve decision-making, facilitating response activities and potentially saving lives.  Yet several challenges remain; one is the generation and propagation of misinformation.  Indeed, during recent disaster events, including Hurricane Sandy and the Boston Marathon bombings, the spread of misinformation via social media was noted as a significant problem; evidence suggests it spread both within and across social media sites as well as into the broader information space. &lt;br/&gt;&lt;br/&gt;Taking a novel and transformative approach, this project aims to utilize the collective intelligence of the crowd - the crowdwork of some social media users who challenge and correct questionable information - to distinguish misinformation and aid in its detection.  It will both characterize the dynamics of misinformation flow online during crisis events, and develop a machine learning strategy for automatically identifying misinformation by leveraging the collective intelligence of the crowd.  The project focuses on identifying distinctive behavioral patterns of social media users in both spreading and challenging or correcting misinformation.  It incorporates qualitative and quantitative methods, including manual and machine-based content analysis, to look comprehensively at the spread of misinformation.  The primary research site is Twitter, because it is public, it facilitates rapid information dissemination, and it has gained exposure as a highly used medium during disaster events.  This investigation expands beyond Twitter to study information flows across other social media and the surrounding Internet by tracing URL links to their original sources. &lt;br/&gt;&lt;br/&gt;This research offers empirical, theoretical and applied contributions to the field of human-computer interaction in the areas of social computing, crisis informatics, and crowdsourcing.  Empirically, it enhances our understanding of the flow of misinformation in online spaces.  It builds on previous studies to include a more nuanced view of misinformation by examining several types of behavioral actions, including correction, speculation and challenges to misinformation.  Moreover, the project maps information contagion within a particular social media network and across different platforms (using URL analysis) to identify patterns of information diffusion, or signatures, that can be used to detect and classify different types of misinformation. Theoretically, it contributes to a growing understanding of crowdwork, crowdsourcing, and collective intelligence within online social networks, specifically looking to understand and describe how the connected crowd performs as a massive sensor network that detects misinformation during crisis events.   Finally, it aims to leverage these empirical and theoretical contributions to develop solutions for the real-time detection of misinformation on social media.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/11/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1420255</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Mason</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert M Mason</PI_FULL_NAME>
<EmailAddress>rmmason@uw.edu</EmailAddress>
<PI_PHON>2062215623</PI_PHON>
<NSF_ID>000348364</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kate</FirstName>
<LastName>Starbird</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kate Starbird</PI_FULL_NAME>
<EmailAddress>kstarbi@uw.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000636015</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Emma</FirstName>
<LastName>Spiro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emma Spiro</PI_FULL_NAME>
<EmailAddress>espiro@uw.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000662081</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName/>
<StateCode>WA</StateCode>
<ZipCode>981952500</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~260272</FUND_OBLG>
<FUND_OBLG>2015~238807</FUND_OBLG>
<FUND_OBLG>2016~15967</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Social media are now an integrated component of the critical infrastructure of crisis response. Information shared through these channels can be valuable to affected citizens and emergency responders. However, these channels are vulnerable to rumors and misinformation, which threaten their utility. Addressing this challenge, this research examined online rumoring during crisis events, including natural disasters such as hurricanes and earthquakes and manmade emergencies such as terrorist attacks and shootings. The project had two related goals: to understand how and why rumors spread in this context, and to build and test machine learning solutions to automatically identify rumors. Specifically, this work explored the potential of using &ldquo;corrections&rdquo; by the online crowd as a signal for identifying rumors&mdash;with the idea that rumors can spread in myriad different forms, but that corrections of rumors may be more consistent.</p> <p>Towards the first goal of better understanding how rumors spread online, this project made several contributions. Most notably, we enhanced understandings of how and why some Twitter users correct themselves and others for sharing false rumors and how &ldquo;official&rdquo; accounts participate in and correct rumors. The latter findings led to specific recommendations for emergency response organizations to engage early and clearly in rumor correcting. We also explored how (and when) people express uncertainty in tweets&mdash;suggesting that rumor detecting algorithms could use expressions of uncertainty as early signals for real-time rumor detection. We conducted a comparative study across several rumors, comparing the engagement of journalists to that of non-journalists, finding that journalists engage earlier in rumors and are more likely to correct false rumors and to correct themselves for sharing a false rumor than other social media users. Finally, we made an unexpected discovery of the salience of conspiracy theory-type rumors during crisis events and their connection to online disinformation flows.</p> <p>Towards the second goal of identifying rumor-related content on social media, this project developed and tested machine learning models to predict rumor stance&mdash;in other words, whether a specific social media post passes along, corrects, or is neutral towards a rumor. Achieving strong performance in classifying social media messages as rumor affirming versus rumor denying, these models demonstrate notable features (e.g. language and structural elements) of rumor-related content and corrections that are consistent across different rumors and crisis events. Findings demonstrate the potential for building information systems to flag rumors and misinformation; they also provide insight into the language and textual elements commonly found in rumor-related content. Additionally, integrating machine learning methods into studies of rumoring allow researchers to scale studies beyond the limitations of human coding, relying on machine-coded datasets to explore patterns of rumoring behavior in large-scale social media datasets and cross-event analyses.</p> <p>This research also contributed to the training of dozens of students&mdash;incorporated more than 30 undergraduate and graduate students into research through research assistantships, REU opportunities, and for-credit research groups. More than half of these students have been women. Five undergraduates who participated in this research are now pursuing PhDs (either applying or already enrolled).</p> <p>Finally, this research has laid the foundations for two continued projects. The first looks at how and why rumors change over time. The second examines the purposeful spread of disinformation online.</p><br> <p>            Last Modified: 12/27/2017<br>      Modified by: Kate&nbsp;Starbird</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Social media are now an integrated component of the critical infrastructure of crisis response. Information shared through these channels can be valuable to affected citizens and emergency responders. However, these channels are vulnerable to rumors and misinformation, which threaten their utility. Addressing this challenge, this research examined online rumoring during crisis events, including natural disasters such as hurricanes and earthquakes and manmade emergencies such as terrorist attacks and shootings. The project had two related goals: to understand how and why rumors spread in this context, and to build and test machine learning solutions to automatically identify rumors. Specifically, this work explored the potential of using "corrections" by the online crowd as a signal for identifying rumors&mdash;with the idea that rumors can spread in myriad different forms, but that corrections of rumors may be more consistent.  Towards the first goal of better understanding how rumors spread online, this project made several contributions. Most notably, we enhanced understandings of how and why some Twitter users correct themselves and others for sharing false rumors and how "official" accounts participate in and correct rumors. The latter findings led to specific recommendations for emergency response organizations to engage early and clearly in rumor correcting. We also explored how (and when) people express uncertainty in tweets&mdash;suggesting that rumor detecting algorithms could use expressions of uncertainty as early signals for real-time rumor detection. We conducted a comparative study across several rumors, comparing the engagement of journalists to that of non-journalists, finding that journalists engage earlier in rumors and are more likely to correct false rumors and to correct themselves for sharing a false rumor than other social media users. Finally, we made an unexpected discovery of the salience of conspiracy theory-type rumors during crisis events and their connection to online disinformation flows.  Towards the second goal of identifying rumor-related content on social media, this project developed and tested machine learning models to predict rumor stance&mdash;in other words, whether a specific social media post passes along, corrects, or is neutral towards a rumor. Achieving strong performance in classifying social media messages as rumor affirming versus rumor denying, these models demonstrate notable features (e.g. language and structural elements) of rumor-related content and corrections that are consistent across different rumors and crisis events. Findings demonstrate the potential for building information systems to flag rumors and misinformation; they also provide insight into the language and textual elements commonly found in rumor-related content. Additionally, integrating machine learning methods into studies of rumoring allow researchers to scale studies beyond the limitations of human coding, relying on machine-coded datasets to explore patterns of rumoring behavior in large-scale social media datasets and cross-event analyses.  This research also contributed to the training of dozens of students&mdash;incorporated more than 30 undergraduate and graduate students into research through research assistantships, REU opportunities, and for-credit research groups. More than half of these students have been women. Five undergraduates who participated in this research are now pursuing PhDs (either applying or already enrolled).  Finally, this research has laid the foundations for two continued projects. The first looks at how and why rumors change over time. The second examines the purposeful spread of disinformation online.       Last Modified: 12/27/2017       Submitted by: Kate Starbird]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
