<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative Research: A Computational Model for Evaluating the Quality of Citizen Science Contributions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>37800.00</AwardTotalIntnAmount>
<AwardAmount>37800</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Citizen science is a form of research collaboration that involves members of the public in scientific projects, bringing multiple voices and ideas to problem solving and community participation. Citizen science can be powerful because while specific individuals may lack formal expertise and be limited in their ability to contribute high-quality data and new directions, a crowd of individuals may collectively possess the expertise and creativity necessary for identifying and solving difficult problems. However, a major concern for collecting scientific data from the crowd is the varied quality of the contributed data and their relevance to scientific hypotheses. The PIs will explore the potential for deriving metrics from research on computational creativity to automatically assess the quality of citizen science data as a complement to existing research on human assessment of data quality. The project will also explore how the automated assessment of quality can be incorporated into an agent that makes suggestions to individuals in the crowd about the quality of their data, resulting in a prototype for a computational agent that measures the novelty and value of a cizen science contribution. This project will inform future research in computational agents that learn from and contribute to the crowd in order to address challenges associated with the quality of the data and ideas from crowdsourcing in citizen science. &lt;br/&gt;&lt;br/&gt;More specifically, the project includes a) development of a model of citizen-science-data quality based on the notion that good contributions are not just reliable and accurate but also novel and surprising; b) an evaluation of the model against citizen-science data that has been labeled by humans for quality; and c) initial studies of the effect of computational quality feedback on the behavior and perception of members of the crowd. Extending quality assessment to include creativity and being able to make such assessments automatically is potentially tranformative for citizen science projects. The PIs will demonstrate their agent-based model of quality in citizen science projects including their own NatureNet project, which involves crowd participants in data collection in nature preserves and also in the design of scientific challenges and interaction experience that facilitate data collection.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1450942</AwardID>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Preece</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer J Preece</PI_FULL_NAME>
<EmailAddress>preece@umd.edu</EmailAddress>
<PI_PHON>3014052036</PI_PHON>
<NSF_ID>000233368</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~37800</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In many biodiversity citizen science projects, volunteers are asked to contribute observations of natural phenomena such as bird and animal sightings to advance research conducted by scientists. When large numbers of high-quality observations are pooled, it is possible to begin to identify important trends&mdash;for example, that a plant has begun flowering earlier in the season or that a bird species is now appearing in a different habitat. This approach produces very large amounts of data, however, and it can be time-consuming for scientists to examine every possible species and region for new or changing trends.</p> <p>Applying methods from the fields of computational creativity and collective intelligence, our research focused on computer-assisted ways to identify novel and unexpected data. Our goal was to help save scientists&rsquo; time by focusing their attention on potentially interesting parts of the data. &nbsp;At early stages in the research, we consulted with expert birders, managers of citizen science projects, and visualization experts to explore the value of identifying, displaying, and analyzing anomalous data and engaging citizen science participants in data analysis. We then used data from the eBird project (Cornell Lab of Ornithology and National Audubon Society) with a prototype of some visualizations that could demonstrate to birders how frequently particular birds were in different places. &nbsp;Birders were asked which birds&rsquo; distributions were surprising, and we then compared these ratings to our computer model that identified unusual patterns in bird data. This demonstrates two different ways of assessing what data is interesting without expert professional input &ndash; crowdsourcing via interested amateurs, and automatically calculating it with algorithms.</p> <p>We found both approaches to have their advantages and disadvantages, and hope that they can one day be combined to leverage the strengths of each. Figure 1 shows a screenshot from the software, with the inset map in the top right showing the Texas/Oklahoma region within the country, the y-axis showing the chance to observe the bird species in question (the loggerheaded shrike), and the x-axis showing the past decade. &nbsp;This visualization highlights a trend in the Fall observation chance for this species, which is shown to be decreasing.&nbsp;</p> <p>Through expert evaluation and informal testing among a small group of birders, we identified the following findings:</p> <ul> <li>Domain enthusiasts&mdash;people with some knowledge of birding&mdash;were not typically willing to simply identify anomalies, or unusual data appearing in the visualization, without more context to understand their cause. &nbsp;At this stage we are uncertain whether this behavior is specific to birdwatching, where anomalous seasonal counts are relatively commonplace, or if it is a more general trait.</li> <li>When they do judge trends based on data alone, people are much more likely to predict new data based only on data from the recent past (in our experiments, 2 to 5 yearly cycles, even though nearly 15 years of data were available). They appear to treat observations from the past few years as &ldquo;the new normal&rdquo; especially if they appear to be stable over that period. &nbsp;It is not clear if this result generalizes beyond the birding community.</li> </ul> <p>This project was an initial exploration of the possibility of combining crowdsourced and data-driven models of scientific interestingness. Accordingly, we have not been able to demonstrate with statistical significance that the combination of humans and machines performs better than either alone. Our findings have demonstrated that this kind of mixed-initiative approach to data quality and scientific interestingness merits further exploration. &nbsp;Early results show promise for streamlining the identification fo scientifically interesting data by pooling the reasoning skills of volunteers with our computational technique for detecting novel trends. We believe this could help engage and motivate citizen scientists to deepen their participation as well as aid scientists in discovery.</p><br> <p>            Last Modified: 11/28/2016<br>      Modified by: Jennifer&nbsp;J&nbsp;Preece</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1450942/1450942_10333872_1480361378088_birdsightingprobability--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1450942/1450942_10333872_1480361378088_birdsightingprobability--rgov-800width.jpg" title="Graph: Probability of seeing loggerheaded shrike between 2003 and 2012"><img src="/por/images/Reports/POR/2016/1450942/1450942_10333872_1480361378088_birdsightingprobability--rgov-66x44.jpg" alt="Graph: Probability of seeing loggerheaded shrike between 2003 and 2012"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Graph shows a decreasing probability of seeing the loggerheaded shrike in the Oklahoma/Texas vicinity between 2003 and 2012, from a high of approximately 45% to a low of less than 10%.</div> <div class="imageCredit">Kaz Grace</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Jennifer&nbsp;J&nbsp;Preece</div> <div class="imageTitle">Graph: Probability of seeing loggerheaded shrike between 2003 and 2012</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In many biodiversity citizen science projects, volunteers are asked to contribute observations of natural phenomena such as bird and animal sightings to advance research conducted by scientists. When large numbers of high-quality observations are pooled, it is possible to begin to identify important trends&mdash;for example, that a plant has begun flowering earlier in the season or that a bird species is now appearing in a different habitat. This approach produces very large amounts of data, however, and it can be time-consuming for scientists to examine every possible species and region for new or changing trends.  Applying methods from the fields of computational creativity and collective intelligence, our research focused on computer-assisted ways to identify novel and unexpected data. Our goal was to help save scientists? time by focusing their attention on potentially interesting parts of the data.  At early stages in the research, we consulted with expert birders, managers of citizen science projects, and visualization experts to explore the value of identifying, displaying, and analyzing anomalous data and engaging citizen science participants in data analysis. We then used data from the eBird project (Cornell Lab of Ornithology and National Audubon Society) with a prototype of some visualizations that could demonstrate to birders how frequently particular birds were in different places.  Birders were asked which birds? distributions were surprising, and we then compared these ratings to our computer model that identified unusual patterns in bird data. This demonstrates two different ways of assessing what data is interesting without expert professional input &ndash; crowdsourcing via interested amateurs, and automatically calculating it with algorithms.  We found both approaches to have their advantages and disadvantages, and hope that they can one day be combined to leverage the strengths of each. Figure 1 shows a screenshot from the software, with the inset map in the top right showing the Texas/Oklahoma region within the country, the y-axis showing the chance to observe the bird species in question (the loggerheaded shrike), and the x-axis showing the past decade.  This visualization highlights a trend in the Fall observation chance for this species, which is shown to be decreasing.   Through expert evaluation and informal testing among a small group of birders, we identified the following findings:  Domain enthusiasts&mdash;people with some knowledge of birding&mdash;were not typically willing to simply identify anomalies, or unusual data appearing in the visualization, without more context to understand their cause.  At this stage we are uncertain whether this behavior is specific to birdwatching, where anomalous seasonal counts are relatively commonplace, or if it is a more general trait. When they do judge trends based on data alone, people are much more likely to predict new data based only on data from the recent past (in our experiments, 2 to 5 yearly cycles, even though nearly 15 years of data were available). They appear to treat observations from the past few years as "the new normal" especially if they appear to be stable over that period.  It is not clear if this result generalizes beyond the birding community.   This project was an initial exploration of the possibility of combining crowdsourced and data-driven models of scientific interestingness. Accordingly, we have not been able to demonstrate with statistical significance that the combination of humans and machines performs better than either alone. Our findings have demonstrated that this kind of mixed-initiative approach to data quality and scientific interestingness merits further exploration.  Early results show promise for streamlining the identification fo scientifically interesting data by pooling the reasoning skills of volunteers with our computational technique for detecting novel trends. We believe this could help engage and motivate citizen scientists to deepen their participation as well as aid scientists in discovery.       Last Modified: 11/28/2016       Submitted by: Jennifer J Preece]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
