<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Effects of production variability on the acoustic consequences of coordinated articulatory gestures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>132362.00</AwardTotalIntnAmount>
<AwardAmount>132362</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The human voice, our oldest and most reliable communication tool, is now rapidly becoming the input interface of choice that we use everyday to interact with technologies such as car navigation systems, medical  and legal dictation systems, personal assistants like "Siri," automated financial systems, etc.  Thousands of 'apps' have been developed to help consumers use voice to get the information they are looking for.   Speech recognition is the backbone of all of these technologies.  As a result, the performance of speech recognizers is key for customer satisfaction.  Currently, many systems still need to be tuned for a particular speaker to perform well, and the recognition task has to be limited in other ways such as requiring (1) usage of a specific vocabulary,  (2) clear pronunciation of most of the words, especially the content words and (3) limited background noise. In this research, speech variability will be studied, and methods and models will be developed that will enable recognizers to be more speaker independent and capable of handling the full range of speech styles from clear articulation to very casually spoken speech. The results will also bear on linguistic models of speech planning and organization, providing evidence for how speakers trade off efficiencies in the production of speech against the need to be intelligible.&lt;br/&gt;&lt;br/&gt;In this project, point source tracking of the speech articulators will be collected concurrently with the corresponding acoustics.  Speakers will record speech at both a normal and rapid pace (the purpose of the latter is to increase significantly the degree of variability in the signal).  This data will allow for the investigation of whether speakers always move their speech articulators in the direction of a desired target (e.g. tongue tip to teeth in producing /t/) even when a rapid production pace occludes the relevant acoustic information (as in "perfect").  If confirmed, this finding will point the way towards making recognition systems more robust through the incorporation of articulatory information.  In addition, such data will support the development of a speech inversion system capable of 'uncovering' hidden articulatory movements potentially masked from the acoustics.</AbstractNarration>
<MinAmdLetterDate>08/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1436600</AwardID>
<Investigator>
<FirstName>Carol</FirstName>
<LastName>Espy-Wilson</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carol Y Espy-Wilson</PI_FULL_NAME>
<EmailAddress>espy@eng.umd.edu</EmailAddress>
<PI_PHON>3014057411</PI_PHON>
<NSF_ID>000258918</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~132362</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project resulted in the creation of a dataset consisting of acoustic signals and articulatory measurements from three males and three females speaking at normal and fast rates. &nbsp;These data were used in multiple ways.&nbsp; First, an acoustic study was conducted to look at the differences in the normal and fast rates of speech to determine the effects of coarticulation and lenition.&nbsp; Second, the data were used to train speaker-specific models of speech inversion. As expected, the more speaker-specific the model, the better the speech inversion system performed.&nbsp; This finding has led to various approaches to normalize the data to improve the accuracy of gender-dependent models and speaker-independent models.&nbsp; Results have shown that gestures that are not apparent from the acoustics are seen in the vocal tract variables of the speech inversion system.&nbsp; Thus, such information is useful in improving the performance of speech recognition systems.&nbsp;&nbsp;</p> <p>In addition to the research performed using natural speech, a large synthetic dataset (consisting of acoustic and tract variable trajectories (or, articulatory constriction-level information)) was also created that consisted of multiple speakers speaking at different rates. Several machine learning approaches (based on deep neural networks) were explored to train speech inversion models that would take acoustic features as input and produce articulatory information as outputs. Results from these explorations were shared with the research community through several publications. The speech inversion models trained on synthetic data were used for speech recognition tasks, where articulatory features were fed along with standard spectral features to train and test speech recognition models and a consistent gain in performance was observed compared to the baseline models trained with standard acoustic features only.</p> <p>This work supported the thesis work of a doctoral student, who received formal training in research and development on signal processing techniques, speech recognition tools and machine learning concepts. The outcomes of this research have been shared with the research community through conference/meeting presentation, posters and a book chapter. The data produced in this work will be shared with the research community through an online website. The work also resulted in novel acoustic modeling architectures, and such architectures have demonstrated reasonable performance gains with respect to strong baseline systems.</p><br> <p>            Last Modified: 11/29/2016<br>      Modified by: Carol&nbsp;Y&nbsp;Espy-Wilson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project resulted in the creation of a dataset consisting of acoustic signals and articulatory measurements from three males and three females speaking at normal and fast rates.  These data were used in multiple ways.  First, an acoustic study was conducted to look at the differences in the normal and fast rates of speech to determine the effects of coarticulation and lenition.  Second, the data were used to train speaker-specific models of speech inversion. As expected, the more speaker-specific the model, the better the speech inversion system performed.  This finding has led to various approaches to normalize the data to improve the accuracy of gender-dependent models and speaker-independent models.  Results have shown that gestures that are not apparent from the acoustics are seen in the vocal tract variables of the speech inversion system.  Thus, such information is useful in improving the performance of speech recognition systems.    In addition to the research performed using natural speech, a large synthetic dataset (consisting of acoustic and tract variable trajectories (or, articulatory constriction-level information)) was also created that consisted of multiple speakers speaking at different rates. Several machine learning approaches (based on deep neural networks) were explored to train speech inversion models that would take acoustic features as input and produce articulatory information as outputs. Results from these explorations were shared with the research community through several publications. The speech inversion models trained on synthetic data were used for speech recognition tasks, where articulatory features were fed along with standard spectral features to train and test speech recognition models and a consistent gain in performance was observed compared to the baseline models trained with standard acoustic features only.  This work supported the thesis work of a doctoral student, who received formal training in research and development on signal processing techniques, speech recognition tools and machine learning concepts. The outcomes of this research have been shared with the research community through conference/meeting presentation, posters and a book chapter. The data produced in this work will be shared with the research community through an online website. The work also resulted in novel acoustic modeling architectures, and such architectures have demonstrated reasonable performance gains with respect to strong baseline systems.       Last Modified: 11/29/2016       Submitted by: Carol Y Espy-Wilson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
