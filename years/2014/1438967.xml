<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: DSD: Collaborative Research: Moving the Abyss: Database Management on Future 1000-core Processors</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>350429.00</AwardTotalIntnAmount>
<AwardAmount>350429</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Aidong Zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>There are fundamental problems to speeding up CPUs beyond what is &lt;br/&gt;possible today. Because individual transistors are not getting any &lt;br/&gt;faster, manufacturers like Intel and AMD are no longer able to get &lt;br/&gt;massive performance improvements just by increasing clock speeds &lt;br/&gt;(e.g., going from 2GHz to 10GHz). To overcome this, future CPUs will &lt;br/&gt;contain hundreds to thousands of smaller computational cores on a &lt;br/&gt;single chip which will all run at the speed similar to current &lt;br/&gt;processors (e.g., 2GHz). This means that each single core will only be &lt;br/&gt;as powerful as current CPUs, but that the total aggregate power of all &lt;br/&gt;the cores will be significantly more than what is possible today. An &lt;br/&gt;important problem with the advent of these new CPUs is that the &lt;br/&gt;database systems that are used in all aspects of our society are &lt;br/&gt;ill-suited for this change. Such database systems are used to store &lt;br/&gt;and access data for a variety of applications, including on-line &lt;br/&gt;business (e.g., Google, Facebook), scientific instruments (e.g., &lt;br/&gt;astronomical telescopes), and medicine (e.g., MRI scanners). The &lt;br/&gt;reason that they are not ready to handle these new "many-core CPUs" is &lt;br/&gt;because most of them use ideas that were designed in the 1970s and &lt;br/&gt;1980s when processors only had a single core. Thus, the purpose of &lt;br/&gt;this project is to develop both software and hardware technologies &lt;br/&gt;that will allow database systems to utilize the full computational &lt;br/&gt;power of future CPU architectures. The results of this project will &lt;br/&gt;enable organizations to deploy future applications on fewer machines &lt;br/&gt;that use less energy than what is currently used today.&lt;br/&gt;&lt;br/&gt;Computer architectures are moving towards an era dominated by &lt;br/&gt;many-core machines with hundreds of cores on a single chip. This &lt;br/&gt;unprecedented level of on-chip parallelism introduces a new dimension &lt;br/&gt;to scalability that current database management systems (DBMSs) were &lt;br/&gt;not designed for. In particular, it becomes exceedingly difficult for &lt;br/&gt;the DBMS to perform concurrency control, logging, and indexing &lt;br/&gt;efficiently. With hundreds of threads running in parallel, the &lt;br/&gt;complexity of coordinating competing reads and writes to data &lt;br/&gt;diminishes the benefits of increased core counts. Thus, in this &lt;br/&gt;project the PIs propose to develop a software-hardware co-design approach &lt;br/&gt;for DBMSs in the many-core era. On the software side, rather than &lt;br/&gt;attempting to remove scalability bottlenecks of existing DBMS &lt;br/&gt;architectures through incremental improvements, the PIs seek a bottom-up &lt;br/&gt;approach where the architecture is designed to target many-core &lt;br/&gt;systems from inception. On the hardware side, instead of simply adding &lt;br/&gt;more cores to a single chip, the PIs will design new hardware components &lt;br/&gt;that can unburden the software system from computationally critical &lt;br/&gt;tasks.&lt;br/&gt;&lt;br/&gt;For further information see project web site at: &lt;br/&gt;http://db.cs.cmu.edu/projects/1000cores/</AbstractNarration>
<MinAmdLetterDate>08/06/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/06/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1438967</AwardID>
<Investigator>
<FirstName>Srini</FirstName>
<LastName>Devadas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Srini Devadas</PI_FULL_NAME>
<EmailAddress>devadas@mit.edu</EmailAddress>
<PI_PHON>6172530454</PI_PHON>
<NSF_ID>000451511</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~350429</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica} --> <p class="p1">Online transaction processing (OLTP) applications incur high performance penalties when executing transactions in a distributed database management system (DBMS). This is due to high network communication costs, which make distributed transaction coordination and remote data accesses expensive.</p> <p class="p1"><span>In this project, we have developed TicToc, a new optimistic concurrency control algorithm that avoids the scalability and concurrency bottlenecks of prior timestamp ordering (T/O) schemes. TicToc relies on a novel and provably correct data-driven timestamp management protocol. Instead of assigning timestamps to transactions, this protocol assigns read and write timestamps to data items and uses logical leases associated with transactions to lazily compute a valid commit timestamp for each transaction. TicToc removes the need for centralized timestamp allocation through its use of logical leases.</span></p> <p class="p1">In this project we have also developed Sundial, a new distributed concurrency control protocol based on the concept of logical leases pioneered by TicToc that improves the performance of distributed transactions. Sundial reduces coordination costs by eliminating the need for transactions to wait for locks and reducing the number of aborts due to conflicts. Sundial also reduces the overhead of remote data accesses by seamlessly caching remote data in a node&rsquo;s local main memory without requiring additional software or protocols.</p> <p class="p1">TicToc and Sundial can increase the throughput of distributed databases, which are used by many academic and industrial organizations. In this project, we have mentored high school students who have participated in the project's research after learning database management principles.</p><br> <p>            Last Modified: 11/17/2018<br>      Modified by: Srini&nbsp;Devadas</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Online transaction processing (OLTP) applications incur high performance penalties when executing transactions in a distributed database management system (DBMS). This is due to high network communication costs, which make distributed transaction coordination and remote data accesses expensive. In this project, we have developed TicToc, a new optimistic concurrency control algorithm that avoids the scalability and concurrency bottlenecks of prior timestamp ordering (T/O) schemes. TicToc relies on a novel and provably correct data-driven timestamp management protocol. Instead of assigning timestamps to transactions, this protocol assigns read and write timestamps to data items and uses logical leases associated with transactions to lazily compute a valid commit timestamp for each transaction. TicToc removes the need for centralized timestamp allocation through its use of logical leases. In this project we have also developed Sundial, a new distributed concurrency control protocol based on the concept of logical leases pioneered by TicToc that improves the performance of distributed transactions. Sundial reduces coordination costs by eliminating the need for transactions to wait for locks and reducing the number of aborts due to conflicts. Sundial also reduces the overhead of remote data accesses by seamlessly caching remote data in a node?s local main memory without requiring additional software or protocols. TicToc and Sundial can increase the throughput of distributed databases, which are used by many academic and industrial organizations. In this project, we have mentored high school students who have participated in the project's research after learning database management principles.       Last Modified: 11/17/2018       Submitted by: Srini Devadas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
