<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: DSD: End-to-end Acceleration of Genomic Workflows on Emerging Heterogeneous Supercomputers</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>02/29/2020</AwardExpirationDate>
<AwardTotalIntnAmount>849984.00</AwardTotalIntnAmount>
<AwardAmount>849984</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The proposed research harnesses parallelism to accelerate the&lt;br/&gt;pervasive bioinformatics workflow of detecting genetic variations.&lt;br/&gt;This workflow determines the genetic variants present in an&lt;br/&gt;individual, given DNA sequencing data. The variant detection workflow&lt;br/&gt;is an integral part of current genomic data analysis, and several&lt;br/&gt;studies have linked genetic variants to diseases. Typical instances&lt;br/&gt;of this workflow currently take several hours to multiple days to&lt;br/&gt;complete with state-of-the-art software, and current algorithms and&lt;br/&gt;software are unable to exploit and benefit from even modest levels of&lt;br/&gt;hardware parallelism. Most prior approaches to parallelization and&lt;br/&gt;performance tuning of genomic data analysis pipelines have targeted&lt;br/&gt;computation, I/O, or network data transfer bottlenecks in isolation,&lt;br/&gt;and consequently, are limited in the overall performance improvement&lt;br/&gt;they can achieve. This project targets end-to-end acceleration&lt;br/&gt;methodologies and uses emerging heterogeneous supercomputers to&lt;br/&gt;reduce workflow time-to-completion.&lt;br/&gt;&lt;br/&gt;The project focuses on holistic methodologies to accelerate multiple&lt;br/&gt;components within the genetic variant detection workflow. It explores&lt;br/&gt;lightweight data reorganizations at multiple granularities to enhance&lt;br/&gt;locality, investigates compute-, communication-, and I/O task&lt;br/&gt;cotuning, locality-aware load-balancing, and coordinated resource&lt;br/&gt;partitioning to exploit high-performance computing platforms. A key&lt;br/&gt;goal of the proposed research is to design domain-specific&lt;br/&gt;optimizations targeting the massive parallelism and scalability&lt;br/&gt;potential of current heterogeneous supercomputers, so that the&lt;br/&gt;developed techniques can be easily transferred and applied to &lt;br/&gt;dedicated academic cluster and commercial computational environments.&lt;br/&gt;Outreach efforts target undergraduate students through recruiting&lt;br/&gt;workshops and attract them to interdisciplinary graduate programs.&lt;br/&gt;Curriculum development activities emphasize cross-layer parallelism.&lt;br/&gt;&lt;br/&gt;For further information, see project web site at &lt;br/&gt;http://sites.psu.edu/XPSGenomics</AbstractNarration>
<MinAmdLetterDate>08/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>03/15/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439057</AwardID>
<Investigator>
<FirstName>Padma</FirstName>
<LastName>Raghavan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Padma Raghavan</PI_FULL_NAME>
<EmailAddress>padma.raghavan@vanderbilt.edu</EmailAddress>
<PI_PHON>6153226155</PI_PHON>
<NSF_ID>000097691</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate>03/15/2019</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mahmut</FirstName>
<LastName>Kandemir</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mahmut T Kandemir</PI_FULL_NAME>
<EmailAddress>mtk2@psu.edu</EmailAddress>
<PI_PHON>8148634888</PI_PHON>
<NSF_ID>000163936</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kamesh</FirstName>
<LastName>Madduri</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kamesh Madduri</PI_FULL_NAME>
<EmailAddress>madduri@cse.psu.edu</EmailAddress>
<PI_PHON>8148651372</PI_PHON>
<NSF_ID>000598267</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Medvedev</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Medvedev</PI_FULL_NAME>
<EmailAddress>pashadag@cse.psu.edu</EmailAddress>
<PI_PHON>8148631242</PI_PHON>
<NSF_ID>000636101</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Pennsylvania State Univ University Park</Name>
<CityName>University Park</CityName>
<ZipCode>168021503</ZipCode>
<PhoneNumber>8148651372</PhoneNumber>
<StreetAddress>201 Old Main</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003403953</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PENNSYLVANIA STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003403953</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Pennsylvania State Univ University Park]]></Name>
<CityName>University Park</CityName>
<StateCode>PA</StateCode>
<ZipCode>168021503</ZipCode>
<StreetAddress><![CDATA[201 Old Main]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~849984</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern genomic data analysis workflows are complex and resource-intensive. Typical instances of genomic workflows currently take several hours to complete with state-of-the-art software, and current algorithms and software are unable to exploit and benefit from even modest levels of hardware parallelism. Most prior approaches to parallelization and performance tuning of genomic workflows have targeted computation, I/O, or network data transfer bottlenecks in isolation, and consequently, are limited in the overall performance improvement they can achieve. This project focused on end-to-end acceleration methodologies and uses supercomputers to reduce workflow time-to-completion.</p> <p>The project has resulted in several technical contributions. We have developed new software tools (SPRITE4, MetaPartMin, MetaPrep) that exploit multi-node distributed memory parallelism to accelerate data-intensive genomic workflows. Some common optimizations underlying their design include lightweight preprocessing routines for static load balancing, parallel I/O, inter-node communication-reducing mechanisms, and fully utilizing shared-memory multicore resources.</p> <p>SPRITE4 is a FASTQ-to-VCF variant detection pipeline with highly scalable methods for sequence alignment, SAM to BAM file processing, and variant calling. Specifically, we parallelize Minimap2 and Strelka2 for multi-node environments and reduce intermediate file I/O. Using 32 compute nodes (1536 compute cores) of the Stampede2 supercomputer, SPRITE4 can process a 159 gigabase human genome in under six minutes, while retaining Strelka2's state-of-the-art variant detection quality. MetaPartMin is a read partitioning tool, with the goal of partitioning metagenomic reads into disjoint components that can be assembled independently. MetaPartMin is the successor to MetaPrep, and includes novel memory-reducing optimizations to drastically reduce aggregate main memory use, enabling the partitioning of massive datasets on a modest number of compute nodes. All steps of MetaPartMin exploit hybrid multicore and distributed-memory parallelism. On 32 compute nodes of Stampede2, MetaPartMin can partition a 1.25 terabase soil metagenome in six minutes.</p> <p>We also investigated new manycore-centric parallelization and optimization of a popular machine learning method with natural language processing applications, called Word2Vec Skip gram with negative sampling (SGNS). This tool can be used for unsupervised literature-based discovery of concepts and relationships. There also exist extensions to this method for bioinformatics (e.g, BioVectors). We introduce a new optimization called context combining to boost SGNS training performance on multicore and manycore systems. We also show that our accuracy on benchmark queries is comparable to state-of-the-art implementations. For processing the One Billion Word benchmark dataset on a 16-core platform, our approach SGNScc is 3.53X faster than the original multithreaded WordVec implementation and 1.35X faster than an Intel-optimized Word2Vec implementation.</p> <p>To complement the peer-reviewed research publications describing these new methods, we made the source code for the software programs freely available online.</p> <p>The project partially supported seven doctoral students and three Masters students in Computer Science and Engineering. Three of the seven doctoral students and the three Masters students have graduated. The dissertation of four PhD students is in progress. Three of the ten students are from groups traditionally underrepresented in computing. The project's research outcomes have been incorporated into undergraduate and graduate classes.</p><br> <p>            Last Modified: 07/13/2020<br>      Modified by: Kamesh&nbsp;Madduri</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern genomic data analysis workflows are complex and resource-intensive. Typical instances of genomic workflows currently take several hours to complete with state-of-the-art software, and current algorithms and software are unable to exploit and benefit from even modest levels of hardware parallelism. Most prior approaches to parallelization and performance tuning of genomic workflows have targeted computation, I/O, or network data transfer bottlenecks in isolation, and consequently, are limited in the overall performance improvement they can achieve. This project focused on end-to-end acceleration methodologies and uses supercomputers to reduce workflow time-to-completion.  The project has resulted in several technical contributions. We have developed new software tools (SPRITE4, MetaPartMin, MetaPrep) that exploit multi-node distributed memory parallelism to accelerate data-intensive genomic workflows. Some common optimizations underlying their design include lightweight preprocessing routines for static load balancing, parallel I/O, inter-node communication-reducing mechanisms, and fully utilizing shared-memory multicore resources.  SPRITE4 is a FASTQ-to-VCF variant detection pipeline with highly scalable methods for sequence alignment, SAM to BAM file processing, and variant calling. Specifically, we parallelize Minimap2 and Strelka2 for multi-node environments and reduce intermediate file I/O. Using 32 compute nodes (1536 compute cores) of the Stampede2 supercomputer, SPRITE4 can process a 159 gigabase human genome in under six minutes, while retaining Strelka2's state-of-the-art variant detection quality. MetaPartMin is a read partitioning tool, with the goal of partitioning metagenomic reads into disjoint components that can be assembled independently. MetaPartMin is the successor to MetaPrep, and includes novel memory-reducing optimizations to drastically reduce aggregate main memory use, enabling the partitioning of massive datasets on a modest number of compute nodes. All steps of MetaPartMin exploit hybrid multicore and distributed-memory parallelism. On 32 compute nodes of Stampede2, MetaPartMin can partition a 1.25 terabase soil metagenome in six minutes.  We also investigated new manycore-centric parallelization and optimization of a popular machine learning method with natural language processing applications, called Word2Vec Skip gram with negative sampling (SGNS). This tool can be used for unsupervised literature-based discovery of concepts and relationships. There also exist extensions to this method for bioinformatics (e.g, BioVectors). We introduce a new optimization called context combining to boost SGNS training performance on multicore and manycore systems. We also show that our accuracy on benchmark queries is comparable to state-of-the-art implementations. For processing the One Billion Word benchmark dataset on a 16-core platform, our approach SGNScc is 3.53X faster than the original multithreaded WordVec implementation and 1.35X faster than an Intel-optimized Word2Vec implementation.  To complement the peer-reviewed research publications describing these new methods, we made the source code for the software programs freely available online.  The project partially supported seven doctoral students and three Masters students in Computer Science and Engineering. Three of the seven doctoral students and the three Masters students have graduated. The dissertation of four PhD students is in progress. Three of the ten students are from groups traditionally underrepresented in computing. The project's research outcomes have been incorporated into undergraduate and graduate classes.       Last Modified: 07/13/2020       Submitted by: Kamesh Madduri]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
