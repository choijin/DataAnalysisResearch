<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Exploring the Feasibility of Deployable Crowd-Powered Real-Time Captioning Supplemented with Automatic Speech Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Glenn H. Larsen</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project will investigate the feasibility of a high-quality speech-to-text service that combines the input of multiple non-expert human workers with the input of automatic recognition. Real-time captioning converts speech to text quickly (in less than five seconds), and is a vital accommodation that allows deaf and hard of hearing students to participate in mainstream classrooms and other educational activities. The current accepted approach for real-time captioning is to use expert human captionists (stenographers) that are very expensive ($150-300 per hour) and difficult to schedule. Computers can also convert speech to text via automatic speech recognition, but this technology is still unreliable in realistic settings and is likely to remain unreliable in the near- and medium-term future. This award will advance a higher-quality and more affordable alternative systems for real-time captioning that uses computation to coordinate multiple workers who can be more readily drawn from the existing labor force than highly specialized typing experts. This project will allow for increased access for deaf and hard of hearing people, resulting greater opportunities to participate in science and engineering. This in turn may afford deaf and hard of hearing people greater employment opportunities. &lt;br/&gt;&lt;br/&gt;The approach advanced by this project combines the partial captions provided on-demand by human workers using computation to convert speech to text with very low latencies (less than five seconds). Advances in human-computer interaction will allow each constituent worker to be directed to type only part of what he or she hears via both aural and visual cues, and will optimally adjust the playback rate of the audio to each worker's current typing speed. Novel algorithms based on multiple sequence alignment (often used in gene sequencing) will merge the resulting partial captions into a final output stream that can be forwarded back to the user. The incorporation of automatic speech recognition will further reduce costs and increase the scalability of the approach. As the service is developed and automatic speech recognition improves, the service will rely less on humans and more on computation, providing a path toward full automation in the future. This award will investigate the appropriateness and feasibility of captioning systems based on this approach by deploying it in the field, measuring the quality of the captions generated, and collecting qualitative feedback from deaf and hard of hearing students in science and engineering fields.</AbstractNarration>
<MinAmdLetterDate>12/11/2014</MinAmdLetterDate>
<MaxAmdLetterDate>12/11/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1448616</AwardID>
<Investigator>
<FirstName>Walter</FirstName>
<LastName>Lasecki</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Walter S Lasecki</PI_FULL_NAME>
<EmailAddress>wlasecki@umich.edu</EmailAddress>
<PI_PHON>7347644259</PI_PHON>
<NSF_ID>000647212</NSF_ID>
<StartDate>12/11/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Legion Labs LLC</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152171326</ZipCode>
<PhoneNumber>4129450708</PhoneNumber>
<StreetAddress>1401 Beechwood Blvd</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079267131</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LEGION LABS LLC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Legion Labs LLC]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152171326</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8031</Code>
<Text>Education Products</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This work investigated a new <em>collaborative captioning approach </em>for providing access to high-quality real-time captioning accommodation for Deaf and Hard-of-Hearing (DHH) students in educational settings. Existing approaches to real-time captioning currently has three main limitations: it is expensive, difficult to schedule, and prone to errors. This is because, in order to caption at speaking rates of up to 250 words per minute (WPM), professional captionists (stenographers) must train for years to acquire the sensory-motor skills required, making them rare in the workforce. To overcome this, we have introduced a system, Scribe, which allows groups of ordinary people to collectively produce high-quality captions.</p> <p>Intelligently dividing the effort needed to keep up with real-time speech over multiple people makes it possible for anyone who can hear and type to contribute captions. This means that friends, colleagues, family members, and other volunteers can now help make speech more accessible. This award allowed us to build a deployable, scalable version of the Scribe system that can be released to universities and other organizations to help them provide more comprehensive access accommodations within their existing budget.</p> <p>Even when using a paid workforce, this dramatically reduces the cost of providing captioning accommodations. For example, five captionists with no special training, each earning $15/hour would cost $75/hour, versus $150/hour or more for a professional stenographer. To reduce this cost further, and help DHH people have access to captions anytime, anywhere, we explored methods for making this process more efficient, and leveraging automatic speech recognition (ASR) technology when possible. We found that supporting non-expert captionists with ASR was not simply a matter of providing an automated &lsquo;first guess&rsquo; that could be corrected by humans. Instead, we found that there was an error threshold beyond which people were faster at typing content from scratch than correct the machines output. A similar tradeoff exists when adding ASR to a Scribe captioning session as an additional contributor. These results inform future work that seeks to use a hybrid of human and machine intelligence to provide access technologies, and other intelligent tools.</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/03/2016<br>      Modified by: Walter&nbsp;S&nbsp;Lasecki</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This work investigated a new collaborative captioning approach for providing access to high-quality real-time captioning accommodation for Deaf and Hard-of-Hearing (DHH) students in educational settings. Existing approaches to real-time captioning currently has three main limitations: it is expensive, difficult to schedule, and prone to errors. This is because, in order to caption at speaking rates of up to 250 words per minute (WPM), professional captionists (stenographers) must train for years to acquire the sensory-motor skills required, making them rare in the workforce. To overcome this, we have introduced a system, Scribe, which allows groups of ordinary people to collectively produce high-quality captions.  Intelligently dividing the effort needed to keep up with real-time speech over multiple people makes it possible for anyone who can hear and type to contribute captions. This means that friends, colleagues, family members, and other volunteers can now help make speech more accessible. This award allowed us to build a deployable, scalable version of the Scribe system that can be released to universities and other organizations to help them provide more comprehensive access accommodations within their existing budget.  Even when using a paid workforce, this dramatically reduces the cost of providing captioning accommodations. For example, five captionists with no special training, each earning $15/hour would cost $75/hour, versus $150/hour or more for a professional stenographer. To reduce this cost further, and help DHH people have access to captions anytime, anywhere, we explored methods for making this process more efficient, and leveraging automatic speech recognition (ASR) technology when possible. We found that supporting non-expert captionists with ASR was not simply a matter of providing an automated æfirst guessÆ that could be corrected by humans. Instead, we found that there was an error threshold beyond which people were faster at typing content from scratch than correct the machines output. A similar tradeoff exists when adding ASR to a Scribe captioning session as an additional contributor. These results inform future work that seeks to use a hybrid of human and machine intelligence to provide access technologies, and other intelligent tools.          Last Modified: 02/03/2016       Submitted by: Walter S Lasecki]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
