<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Autonomous Synthesis of Haptic Languages</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>585217.00</AwardTotalIntnAmount>
<AwardAmount>585217</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops algorithms that enable a robot to physically explore its environment using touch and to construct a language that it can use to describe that environment.  The steps include exploring an environment while actively seeking information and then detecting potential elements of a language to describe what was touched.  A secondary phase involves taking the set of language elements and compressing the language itself so that sensing, storage, and communication are all more efficient and more robust.  The work will use a robot equipped with a robotic arm, hand, and fingertip sensors to describe objects and surfaces it encounters, all without any information about the objects provided beforehand. The importance of the work stems from the need for robots to operate in environments where touch is the only reliable sensory source.  For instance, underwater applications often have limited visibility and dexterous manipulation can suffer from visual occlusion due to the hand itself.  This research will enable robots to be more responsive to touch and more reliable in vision-impoverished environments.&lt;br/&gt;&lt;br/&gt;A key technical tool used in this work is ergodic control, a computational technique that finds exploration strategies matching desired statistics.  Symbol detection involves finding definitions of dynamic sensor evolution that minimize measures of variability.  Language minimization depends on computing the entropy of a language, and finding the minimal language that has the same level of expressiveness.  These three mathematical and algorithmic components need to be used in parallel during language creation, and they each have to respect physical limitations on the part of the robot (e.g., computational limitations and physical limitations).  Software will be shared through the Robot Operating System (ROS) and TREP (physical simulation and optimal control software).&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/04/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1426961</AwardID>
<Investigator>
<FirstName>J. Edward</FirstName>
<LastName>Colgate</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>J. Edward Colgate</PI_FULL_NAME>
<EmailAddress>colgate@northwestern.edu</EmailAddress>
<PI_PHON>8474914264</PI_PHON>
<NSF_ID>000267549</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Murphey</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd D Murphey</PI_FULL_NAME>
<EmailAddress>t-murphey@northwestern.edu</EmailAddress>
<PI_PHON>3034671041</PI_PHON>
<NSF_ID>000485210</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Evanston</CityName>
<StateCode>IL</StateCode>
<ZipCode>602080834</ZipCode>
<StreetAddress><![CDATA[2145 Sheridan Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~585217</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project &ldquo;NRI: Autonomous Synthesis of Haptic Languages&rdquo; (Award 1426961) developed algorithms that enable machines to learn to describe objects that they touch. People are remarkably good at this skill, picking up a wide variety of objects, manipulating them, and learning how to identify them in the future.&nbsp; Identifying objects by touch requires motion, so people and robotic systems have to choose how to move the physical world in order to learn.&nbsp; As a result of this need to synthesize, robotic systems are typically ineffective at learning about objects using touch sensation. This project created algorithms that enable a robot to explore an object, determine physical properties such as shape and texture, and use those properties to identify similar objects in the future.&nbsp; The work included development of real-time algorithms that the robot needs to use to physically explore an object as well as machine learning algorithms that allow it to learn about the physics of touch using data.&nbsp; These algorithms were then used in hardware experiments that included common situations for robotic systems, such as traveling across sand, uneven terrain, and other complex physical interactions that the robot might need to identify in order to avoid getting immobilized.&nbsp; Other experiments used robot arms with sensors to explore an object to determine its shape, a challenging task in many manufacturing settings since the shape might be anything from a simple cube to extremely complex geometries. &nbsp;Lastly, texture is an important part of how people and other animals discriminate objects, despite the complexity of how a surface interacts with the mechanoreceptors in a person&rsquo;s fingertips.&nbsp; The algorithms in this project have been used to build models of texture based on robot sensory data.&nbsp;</p> <p>&nbsp;</p> <p>The work is important partially because robots will increasingly be working in space that includes people.&nbsp; Human-machine collaboration, such as cooperatively manipulating an object or a smart surgical device interacting with tissue, depend on cohesive interaction between the machine and the human.&nbsp; This cohesive interaction also depends on both the machine and human interpreting each other and the environment in compatible ways.&nbsp; Since humans have extremely good touch-based perception, this work brings robots closer to working with humans in mechanically subtle situations where the human&rsquo;s sense of touch is essential to task success.&nbsp;</p> <p>&nbsp;</p> <p>Both simulated and physical systems were tested as part of the work.&nbsp; Simulated systems included three dimensional shape estimation using contact, texture determination using a variety of models of touch sensation, and distributed coverage of a surface.&nbsp; Physical experiments included robot arms interacting with objects on tables and mobile robots locomoting across sand.&nbsp; These simulated and physical experiments are only example applications, and all the software is implemented in a general manner, with most of it already available as open source code.</p> <p>&nbsp;</p> <p>During the period of the award, important broader impact goals were accomplished.&nbsp; Outreach at the Museum of Science and Industry, Chicago occurred in each year of the award, reaching thousands of students.&nbsp; &nbsp;All of the robots were used in the museum outreach and one of the robots used algorithms developed specifically to illustrate coverage algorithms.&nbsp; This robot, which draws portraits using the same coverage algorithm that we use to explore objects, was also featured in the National Science Foundation Science Nation magazine.&nbsp; Moreover, a class was developed at Northwestern University on active learning, the primary topic of this project. The class incorporates some of the results in lectures and in exercises. Algorithms developed as part of this effort have been applied at Northwestern University&rsquo;s Feinberg School of Medicine using physical assistance robots.&nbsp;</p> <p><em>&nbsp;</em></p><br> <p>            Last Modified: 10/15/2019<br>      Modified by: Todd&nbsp;D&nbsp;Murphey</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project "NRI: Autonomous Synthesis of Haptic Languages" (Award 1426961) developed algorithms that enable machines to learn to describe objects that they touch. People are remarkably good at this skill, picking up a wide variety of objects, manipulating them, and learning how to identify them in the future.  Identifying objects by touch requires motion, so people and robotic systems have to choose how to move the physical world in order to learn.  As a result of this need to synthesize, robotic systems are typically ineffective at learning about objects using touch sensation. This project created algorithms that enable a robot to explore an object, determine physical properties such as shape and texture, and use those properties to identify similar objects in the future.  The work included development of real-time algorithms that the robot needs to use to physically explore an object as well as machine learning algorithms that allow it to learn about the physics of touch using data.  These algorithms were then used in hardware experiments that included common situations for robotic systems, such as traveling across sand, uneven terrain, and other complex physical interactions that the robot might need to identify in order to avoid getting immobilized.  Other experiments used robot arms with sensors to explore an object to determine its shape, a challenging task in many manufacturing settings since the shape might be anything from a simple cube to extremely complex geometries.  Lastly, texture is an important part of how people and other animals discriminate objects, despite the complexity of how a surface interacts with the mechanoreceptors in a person?s fingertips.  The algorithms in this project have been used to build models of texture based on robot sensory data.      The work is important partially because robots will increasingly be working in space that includes people.  Human-machine collaboration, such as cooperatively manipulating an object or a smart surgical device interacting with tissue, depend on cohesive interaction between the machine and the human.  This cohesive interaction also depends on both the machine and human interpreting each other and the environment in compatible ways.  Since humans have extremely good touch-based perception, this work brings robots closer to working with humans in mechanically subtle situations where the human?s sense of touch is essential to task success.      Both simulated and physical systems were tested as part of the work.  Simulated systems included three dimensional shape estimation using contact, texture determination using a variety of models of touch sensation, and distributed coverage of a surface.  Physical experiments included robot arms interacting with objects on tables and mobile robots locomoting across sand.  These simulated and physical experiments are only example applications, and all the software is implemented in a general manner, with most of it already available as open source code.     During the period of the award, important broader impact goals were accomplished.  Outreach at the Museum of Science and Industry, Chicago occurred in each year of the award, reaching thousands of students.   All of the robots were used in the museum outreach and one of the robots used algorithms developed specifically to illustrate coverage algorithms.  This robot, which draws portraits using the same coverage algorithm that we use to explore objects, was also featured in the National Science Foundation Science Nation magazine.  Moreover, a class was developed at Northwestern University on active learning, the primary topic of this project. The class incorporates some of the results in lectures and in exercises. Algorithms developed as part of this effort have been applied at Northwestern University?s Feinberg School of Medicine using physical assistance robots.           Last Modified: 10/15/2019       Submitted by: Todd D Murphey]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
