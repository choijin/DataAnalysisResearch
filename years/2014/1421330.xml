<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Investigating an Interactive Computational Framework for Nonverbal Interpersonal Skills Training</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>499992.00</AwardTotalIntnAmount>
<AwardAmount>523992</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will advance social skill training by developing and evaluating a multimodal computational framework specifically targeted to improve public speaking performance through repeated training interactions with a virtual audience that perceives the speaker and produces meaningful nonverbal feedback.  Interpersonal skills such as public speaking are essential assets for a large variety of professions and in everyday life.  The ability to communicate in social environments often greatly influences a person's career development, can help build relationships and resolve conflict.  Public speaking is not a skill that is innate to everyone, but can be mastered through extensive training.  Nonverbal communication is an important aspect of successful public speaking and interpersonal communication, and at the same time difficult to train.  This research effort will create the computational foundations to automatically assess interpersonal skill expertise and help people improve their skills using an interactive simulated virtual human framework.&lt;br/&gt;&lt;br/&gt;There are three fundamental research goals: (1) Developing a probabilistic computational model to learn the temporal and multimodal dependencies and infer a speaker's public speaking performance from acoustic and visual nonverbal behavior; (2) Understanding the design challenges of developing a simulated audience that is interactive, believable, and most importantly providing meaningful and training-relevant feedback to the speaker; and (3) Understanding the impact of the virtual audience on speakers' performance and learning outcomes by performing a comparative study investigating alternative feedback and training approaches.  This work builds upon the promising results of a pilot research study and upon a prototype virtual human infrastructure allowing for the seamless integration of automatically modeled interpersonal skill expertise for flexible virtual human interaction and gesture control.&lt;br/&gt;&lt;br/&gt;Virtual audiences have the great advantage that their appearance and behavioral patterns can be precisely programmed and systematically presented to pace the interaction.  The algorithms developed as part of this research to model temporal and multimodal dependencies will have a broad applicability outside the domain of public speaking assessment, including healthcare applications.  The interactive virtual human technology may serve as the basis for novel teaching applications in a wide range of areas in the future, due to its extensibility and availability.   The programming code and data will be made available to the research community and students.</AbstractNarration>
<MinAmdLetterDate>07/16/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1421330</AwardID>
<Investigator>
<FirstName>Louis-Philippe</FirstName>
<LastName>Morency</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Louis-Philippe Morency</PI_FULL_NAME>
<EmailAddress>morency@cs.cmu.edu</EmailAddress>
<PI_PHON>3104485323</PI_PHON>
<NSF_ID>000519300</NSF_ID>
<StartDate>07/16/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ari</FirstName>
<LastName>Shapiro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ari Shapiro</PI_FULL_NAME>
<EmailAddress>shapiro@ict.usc.edu</EmailAddress>
<PI_PHON>2137407762</PI_PHON>
<NSF_ID>000629465</NSF_ID>
<StartDate>07/16/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stefan</FirstName>
<LastName>Scherer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stefan Scherer</PI_FULL_NAME>
<EmailAddress>scherer@ict.usc.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000657577</NSF_ID>
<StartDate>07/16/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Playa Vista</CityName>
<StateCode>CA</StateCode>
<ZipCode>900942536</ZipCode>
<StreetAddress><![CDATA[12015 Waterfront Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>43</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA43</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~499992</FUND_OBLG>
<FUND_OBLG>2016~24000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Good public speaking skills convey strong and effective communication, which is critical in many professions and used in everyday life. The ability to speak publicly requires a lot of training and practice. Recent technological developments enable new approaches for public speaking training that allow users to practice in a safe and engaging environment. We explored feedback strategies for public speaking training that are based on an interactive virtual audience paradigm. A virtual audience is comprised by interactive and reactive digital representations of humans. Within this project, we perform extensive evaluation based on self-assessment questionnaires, expert assessments, and objectively annotated measures, such as eye-contact and avoidance of pause fillers. Our experiments showed that the interactive virtual audience can be used to successfully train public speaking skills. Specifically, we showed that training with the audience increases engagement in the student as well as improved public speaking skills as judged by experts. We provide both realtime and actionable post-interaction feedback to the users to achieve optimal learning outcomes. In addition, to showing successful training outcomes we developed audiovisual machine learning methods to automatically assess speaker performances and their public speaking anxiety. Lastly, we expanded the technology to additionally enable training of Doctors' bedside manners and job interviewing skills (in collaboration with Drs. Talbot and Rizzo at the USC Institute for Creative Technologies).</p><br> <p>            Last Modified: 11/28/2018<br>      Modified by: Stefan&nbsp;Scherer</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1421330/1421330_10319604_1543430960864_ScreenShot2018-11-28at10.47.22AM--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1421330/1421330_10319604_1543430960864_ScreenShot2018-11-28at10.47.22AM--rgov-800width.jpg" title="Cicero - Virtual Audience"><img src="/por/images/Reports/POR/2018/1421330/1421330_10319604_1543430960864_ScreenShot2018-11-28at10.47.22AM--rgov-66x44.jpg" alt="Cicero - Virtual Audience"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Cicero - Virtual Audience Example. The virtual humans react automatically to the human presenter in realtime.</div> <div class="imageCredit">USC ICT</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Stefan&nbsp;Scherer</div> <div class="imageTitle">Cicero - Virtual Audience</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Good public speaking skills convey strong and effective communication, which is critical in many professions and used in everyday life. The ability to speak publicly requires a lot of training and practice. Recent technological developments enable new approaches for public speaking training that allow users to practice in a safe and engaging environment. We explored feedback strategies for public speaking training that are based on an interactive virtual audience paradigm. A virtual audience is comprised by interactive and reactive digital representations of humans. Within this project, we perform extensive evaluation based on self-assessment questionnaires, expert assessments, and objectively annotated measures, such as eye-contact and avoidance of pause fillers. Our experiments showed that the interactive virtual audience can be used to successfully train public speaking skills. Specifically, we showed that training with the audience increases engagement in the student as well as improved public speaking skills as judged by experts. We provide both realtime and actionable post-interaction feedback to the users to achieve optimal learning outcomes. In addition, to showing successful training outcomes we developed audiovisual machine learning methods to automatically assess speaker performances and their public speaking anxiety. Lastly, we expanded the technology to additionally enable training of Doctors' bedside manners and job interviewing skills (in collaboration with Drs. Talbot and Rizzo at the USC Institute for Creative Technologies).       Last Modified: 11/28/2018       Submitted by: Stefan Scherer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
