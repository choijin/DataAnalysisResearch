<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Multimodal Corpus for Vision-Based Meeting Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>11/19/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>9566.00</AwardTotalIntnAmount>
<AwardAmount>9566</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project explores a multimodal corpus for vision-based meeting analysis. The research team is working on: (1) extracting the data from tapes and  organizing them into multimedia databases; (2) developing a database visualization and analysis tool to support model development; and (3) developing an agent-based algorithm to extract hand and head tracking information so that higher level models may be built onto the data. &lt;br/&gt;&lt;br/&gt;The project provides datasets that are organized into a usable corpus with many unique properties, such as the ground truth at the psycholinguistic/psycho-social level of the social roles status, purpose of each meeting, and at the video level in the form of motion tracking data collected co-temporally with the video, for developing and testing new algorithms. The developed tools improve the access to the multimedia database of multi-view group human behavior. The agent-based approach provides a novel way in video annotation.  The developed tools and algorithms from this project can be applied to many other applications. For example, the tools may be applied to analyze classroom behavior and in learning scenarios. The project provides research opportunities for undergraduate and graduate students including women and individuals from underrepresented populations. The project outreaches to the user communities through publications, presentations, web presence, and broader collaborative interactions.</AbstractNarration>
<MinAmdLetterDate>06/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439613</AwardID>
<Investigator>
<FirstName>Francis</FirstName>
<LastName>Quek</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Francis Quek</PI_FULL_NAME>
<EmailAddress>quek@tamu.edu</EmailAddress>
<PI_PHON>9798453465</PI_PHON>
<NSF_ID>000324474</NSF_ID>
<StartDate>06/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M University</Name>
<CityName>College Station</CityName>
<ZipCode>778454375</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy South</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>020271826</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A &amp; M UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas AM University]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778431260</ZipCode>
<StreetAddress><![CDATA[400 Harvey Mitchell Pkwy South S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~9566</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The major goals of the project were to assemble the AFIT multimodal meeting room corpus that was scattered over multiple disks and tapes into a cogent dataset, and to perform research on interactive temporal event data exploration and analysis of multi-person multimodal temporal data that describes human behavior (both speech and gestures). In summary, We 1.&nbsp; Extracted all the data from the AFIT dataset and preserved them on a 4TB hard disk with backups. 2. We performed research on the discovery of multimodal temporal behavior patterns from video/audio data, developing algorithms for discovery-based exploration of temporal event data, and developing an interactive model by which researchers can construct multimodal event-based models that are grounded in empirical data.</p> <p>We&nbsp; investigated temporal and ordered relational aspects of multimodal data. The temporal aspects have lead to understanding how events in multimodal data depend on temporal constraints and how the different contexts events are found lead to a better understanding of their relevance. Our research outcomes support a situated analysis approach to meeting analysis in which theoretical modeling of behavior may be grounded in exemplars and patterns in real multimodal data. The ordered relational aspects have allowed us to provide a rank-order system to events that are otherwise difficult to compare as their meaning is dependent on the current analysis being conducted.</p> <p>With these two aspects, we have built a prototype that allows a user to explore a meeting room data-set beginning from an initial hypothesis. From this hypothesis, the user can mold and grow the hypothesis into a model reflective of their expert knowledge and connected to instance(s) in the data-set. This provides an informed view of their model in all contexts it is found.</p> <p>Given what we learned from the prototype system, we rebuilt it from the ground-up supporting flexible pattern identification within multimodal data. We expanded the ideas from the prototype and molded the approach as an interactive data-driven search and discovery approach that iteratively evolves a model based on user's online interaction. We have successfully tested the accuracy of pattern identification using this approach and results have been published accepted in two conferences (ACM Multimedia 12, ICMI 2011, 2012 and 2013, and ICMR 2014).</p> <p>Along with the above, we have been investigating different structural modeling representations that can be used to represent a user's hypothesis. We are currently developing a set of temporal relationship principles for defining the behavior models of interest. Such a representation is necessary as the interactions encapsulated in behavior models can be very rich and complex and current means of representation are insufficient.</p> <p>We have conducted a set of studies where domain experts in three different projects utilized our system to analyze their temporal data. This study is part of the research of Chreston Miller's doctoral thesis that was defended in 2013.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/15/2015<br>      Modified by: Francis&nbsp;Quek</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The major goals of the project were to assemble the AFIT multimodal meeting room corpus that was scattered over multiple disks and tapes into a cogent dataset, and to perform research on interactive temporal event data exploration and analysis of multi-person multimodal temporal data that describes human behavior (both speech and gestures). In summary, We 1.  Extracted all the data from the AFIT dataset and preserved them on a 4TB hard disk with backups. 2. We performed research on the discovery of multimodal temporal behavior patterns from video/audio data, developing algorithms for discovery-based exploration of temporal event data, and developing an interactive model by which researchers can construct multimodal event-based models that are grounded in empirical data.  We  investigated temporal and ordered relational aspects of multimodal data. The temporal aspects have lead to understanding how events in multimodal data depend on temporal constraints and how the different contexts events are found lead to a better understanding of their relevance. Our research outcomes support a situated analysis approach to meeting analysis in which theoretical modeling of behavior may be grounded in exemplars and patterns in real multimodal data. The ordered relational aspects have allowed us to provide a rank-order system to events that are otherwise difficult to compare as their meaning is dependent on the current analysis being conducted.  With these two aspects, we have built a prototype that allows a user to explore a meeting room data-set beginning from an initial hypothesis. From this hypothesis, the user can mold and grow the hypothesis into a model reflective of their expert knowledge and connected to instance(s) in the data-set. This provides an informed view of their model in all contexts it is found.  Given what we learned from the prototype system, we rebuilt it from the ground-up supporting flexible pattern identification within multimodal data. We expanded the ideas from the prototype and molded the approach as an interactive data-driven search and discovery approach that iteratively evolves a model based on user's online interaction. We have successfully tested the accuracy of pattern identification using this approach and results have been published accepted in two conferences (ACM Multimedia 12, ICMI 2011, 2012 and 2013, and ICMR 2014).  Along with the above, we have been investigating different structural modeling representations that can be used to represent a user's hypothesis. We are currently developing a set of temporal relationship principles for defining the behavior models of interest. Such a representation is necessary as the interactions encapsulated in behavior models can be very rich and complex and current means of representation are insufficient.  We have conducted a set of studies where domain experts in three different projects utilized our system to analyze their temporal data. This study is part of the research of Chreston Miller's doctoral thesis that was defended in 2013.          Last Modified: 09/15/2015       Submitted by: Francis Quek]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
