<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Development of the Sensor Environment Imaging (SENSEI) Instrument</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>2999999.00</AwardTotalIntnAmount>
<AwardAmount>3563999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project, developing SENSEI (SENSor Environment Imaging), an instrument that targets a broad range of big data science and engineering challenges, promises a unique sensor platform for 2D and 3D recording within dynamic environments. SENSEI contributes to a broad range of data-driven application domains. These range from fundamental research in instrument design and development to data processing, fusion and synthesis, enabling not only the creation of the instrument, but also its use as a resource for multi-domain science and engineering on the ground, in the air, and underwater.&lt;br/&gt;&lt;br/&gt;Specifically, SENSEI is a spherical, (ultra) high-resolution (9-times-IMAX resolution and ~terapixel/minute flood of imagery), vision-based capture system capable of video-rate data-acquisition. The instrument will address domain challenges in science, engineering, medicine, and beyond by enabling investigation of big data acquisition, streaming, processing, archiving and access, visualization, and analytics.&lt;br/&gt;&lt;br/&gt;The broader significance of this project will be felt in a variety of image-intensive scientific disciplines. The areas of environmental monitoring, remote sensing, situational awareness, homeland security, and mechanical and structural engineering can greatly benefit from the proposed instrument. The instrument will be designed for replication by the global community of researchers and the graduate students.  The technology will be communicated through classes, projects, theses, publications, as well as museum exhibits and conferences. Special attention has been paid to broadening participation through the Minority Serving Institutions Cyber-Infrastructure Empowerment Coalition (MSI-CIEC).</AbstractNarration>
<MinAmdLetterDate>09/16/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/05/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1456638</AwardID>
<Investigator>
<FirstName>Maxine</FirstName>
<LastName>Brown</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maxine D Brown</PI_FULL_NAME>
<EmailAddress>maxine@uic.edu</EmailAddress>
<PI_PHON>3129963002</PI_PHON>
<NSF_ID>000218524</NSF_ID>
<StartDate>09/16/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Kenyon</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert V Kenyon</PI_FULL_NAME>
<EmailAddress>kenyon@uic.edu</EmailAddress>
<PI_PHON>3129960450</PI_PHON>
<NSF_ID>000109769</NSF_ID>
<StartDate>09/16/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Johnson</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew E Johnson</PI_FULL_NAME>
<EmailAddress>ajohnson@uic.edu</EmailAddress>
<PI_PHON>3129963002</PI_PHON>
<NSF_ID>000332025</NSF_ID>
<StartDate>09/16/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tanya</FirstName>
<LastName>Berger-Wolf</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tanya Berger-Wolf</PI_FULL_NAME>
<EmailAddress>berger-wolf.1@osu.edu</EmailAddress>
<PI_PHON>6142926665</PI_PHON>
<NSF_ID>000296514</NSF_ID>
<StartDate>09/16/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606124305</ZipCode>
<PhoneNumber>3129962862</PhoneNumber>
<StreetAddress>809 S. Marshfield Avenue</StreetAddress>
<StreetAddress2><![CDATA[MB 502, M/C 551]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>098987217</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606077053</ZipCode>
<StreetAddress><![CDATA[842 West Taylor]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<ProgramReference>
<Code>1714</Code>
<Text>SPECIAL PROJECTS - CISE</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~2999999</FUND_OBLG>
<FUND_OBLG>2015~516000</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<FUND_OBLG>2018~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the SENSEI project was to develop a reconfigurable, ultra-high-resolution, spherical (4&pi; steradian), photometric, radiometric and photogrammetric, real-time data-acquisition, sensor-based camera system capable of capturing 3D stereo and still images for viewing in collaboration-enabled, nationally networked, virtual-reality systems. Another goal was that the camera system?s output could be used to extract objects? sizes, shapes and distances within a scene. And, yet another goal was that the final system includes versatile data ports to accommodate a variety of sensors for different applications, such as multi-channel audio, GPS, temperature, pressure, special illumination, SONAR, LIDAR, etc.</p> <p>The SENSEI project was motivated by early work partners at University of California, San Diego (UCSD) that developed CAVEcam. This was a still-photography, spherical, stereoscopic camera system, in which the stereo camera pairs were semi-automatically rotated to take 360-degree x 180-degree views, and whose output was manually stitched together. SENSEI was the next logical advancement on several fronts: a spherical, stereoscopic video camera with multiple camera sensors to capture scenes in all directions simultaneously, and then automatically post-processed to stitch individual frames together. A photograph of output from the original CAVEcam system is attached (Luxor, Egypt, shown in the CAVE2? system; camera developed by Dick Ainsworth, Dan Sandin, and Tom DeFanti.)</p> <p>The SENSEI team did not foresee the technical limitations and problems that would be encountered when this research started five years ago. However, the team has made considerable progress and has successes to report as the award comes to an end. On the hardware front, the team successfully developed a hardware camera prototype; a photo of the camera prototype is attached. On the software front, we developed prototype 3D-panorama stitching software that enhanced the processing pipeline to improve performance, alleviate vertical and horizontal disparities between stereo panoramas, and create better viewing experiences. On the networking front, bottlenecks to stream ultra-resolution 360? videos over high-performance networks to large-scale virtual-reality systems were alleviated.</p> <p>To our knowledge, there is still no commercially available system with SENSEI?s proposed capabilities. Cameras that address the low-end virtual-reality consumer ?edutainment? market ? i.e., cameras that work with virtual-reality head-mounted display systems by companies such as Oculus Rift ? are being sold.&nbsp;</p> <p><strong>Intellectual Merit:</strong> To achieve SENSEI?s design goals proved to be a stimulating research effort for computer science and electrical engineering research, focused on architectural designs for mounting, pointing, triggering and synchronizing high-resolution camera sensors, as well as the flexible computational capability needed to ramp up the necessary image processing stitching algorithms several orders of magnitude. SENSEI requirements were motivated by the needs of domain scientists in such fields as archaeology, architecture, astronomy, biology, cultural heritage, digital arts, earth science, emergency preparedness, engineering, geographic information systems, manufacturing, oceanography, planetary science, and science education.</p> <p><strong>Broader Impacts:</strong> SENSEI took advantage of next-generation layered manufacturing techniques (3D printing) to design, model, deploy and test its hardware scaffold, contributing to open source hardware and software packages so that SENSEI technologies can replicated and adopted by the research and education communities. Though currently a prototype, the final SENSEI camera system, as envisioned, with multi-spectral and variable focal options, will allow for deeper and higher-density information analysis in addition to high-resolution imagery well adapted to human visual acuity. As described above, SENSEI has application in a variety of image-intensive domain sciences, enabling researchers, students, and the general public to accurately experience places of scientific or engineering interest, or to share cultural heritage locations that are restricted or too difficult or expensive to visit. SENSEI is also a necessary tool to help address societal grand challenges, including environmental monitoring, remote sensing, situational awareness, homeland security, and mechanical and structural engineering.</p><br> <p>            Last Modified: 12/23/2019<br>      Modified by: Maxine&nbsp;D&nbsp;Brown</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1456638/1456638_10342135_1577138950504_EVL-Calit2-Luxor_LJL2805--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1456638/1456638_10342135_1577138950504_EVL-Calit2-Luxor_LJL2805--rgov-800width.jpg" title="Luxor"><img src="/por/images/Reports/POR/2019/1456638/1456638_10342135_1577138950504_EVL-Calit2-Luxor_LJL2805--rgov-66x44.jpg" alt="Luxor"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Panoramic images of Luxor in Egypt taken with the CAVEcam camera and displayed in the CAVE2 at UIC. Special thanks to Dick Ainsworth, Dan Sandin, and Tom DeFanti.</div> <div class="imageCredit">Calit2 - Qualcomm Institute at UC San Diego and the Electronic Visualization Laboratory (EVL) at the University of Illinois at Chicago</div> <div class="imageSubmitted">Maxine&nbsp;D&nbsp;Brown</div> <div class="imageTitle">Luxor</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1456638/1456638_10342135_1577139549509_SENSEIcamera091619--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1456638/1456638_10342135_1577139549509_SENSEIcamera091619--rgov-800width.jpg" title="StarCAM: SENSEI Camera prototype"><img src="/por/images/Reports/POR/2019/1456638/1456638_10342135_1577139549509_SENSEIcamera091619--rgov-66x44.jpg" alt="StarCAM: SENSEI Camera prototype"></a> <div class="imageCaptionContainer"> <div class="imageCaption">StarCAM is a one-ring prototype camera that is potentially higher resolution and lower cost than what is commercially available.</div> <div class="imageCredit">Calit2 - Qualcomm Institute at UC San Diego and the Electronic Visualization Laboratory at the University of Illinois at Chicago</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Maxine&nbsp;D&nbsp;Brown</div> <div class="imageTitle">StarCAM: SENSEI Camera prototype</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the SENSEI project was to develop a reconfigurable, ultra-high-resolution, spherical (4&pi; steradian), photometric, radiometric and photogrammetric, real-time data-acquisition, sensor-based camera system capable of capturing 3D stereo and still images for viewing in collaboration-enabled, nationally networked, virtual-reality systems. Another goal was that the camera system?s output could be used to extract objects? sizes, shapes and distances within a scene. And, yet another goal was that the final system includes versatile data ports to accommodate a variety of sensors for different applications, such as multi-channel audio, GPS, temperature, pressure, special illumination, SONAR, LIDAR, etc.  The SENSEI project was motivated by early work partners at University of California, San Diego (UCSD) that developed CAVEcam. This was a still-photography, spherical, stereoscopic camera system, in which the stereo camera pairs were semi-automatically rotated to take 360-degree x 180-degree views, and whose output was manually stitched together. SENSEI was the next logical advancement on several fronts: a spherical, stereoscopic video camera with multiple camera sensors to capture scenes in all directions simultaneously, and then automatically post-processed to stitch individual frames together. A photograph of output from the original CAVEcam system is attached (Luxor, Egypt, shown in the CAVE2? system; camera developed by Dick Ainsworth, Dan Sandin, and Tom DeFanti.)  The SENSEI team did not foresee the technical limitations and problems that would be encountered when this research started five years ago. However, the team has made considerable progress and has successes to report as the award comes to an end. On the hardware front, the team successfully developed a hardware camera prototype; a photo of the camera prototype is attached. On the software front, we developed prototype 3D-panorama stitching software that enhanced the processing pipeline to improve performance, alleviate vertical and horizontal disparities between stereo panoramas, and create better viewing experiences. On the networking front, bottlenecks to stream ultra-resolution 360? videos over high-performance networks to large-scale virtual-reality systems were alleviated.  To our knowledge, there is still no commercially available system with SENSEI?s proposed capabilities. Cameras that address the low-end virtual-reality consumer ?edutainment? market ? i.e., cameras that work with virtual-reality head-mounted display systems by companies such as Oculus Rift ? are being sold.   Intellectual Merit: To achieve SENSEI?s design goals proved to be a stimulating research effort for computer science and electrical engineering research, focused on architectural designs for mounting, pointing, triggering and synchronizing high-resolution camera sensors, as well as the flexible computational capability needed to ramp up the necessary image processing stitching algorithms several orders of magnitude. SENSEI requirements were motivated by the needs of domain scientists in such fields as archaeology, architecture, astronomy, biology, cultural heritage, digital arts, earth science, emergency preparedness, engineering, geographic information systems, manufacturing, oceanography, planetary science, and science education.  Broader Impacts: SENSEI took advantage of next-generation layered manufacturing techniques (3D printing) to design, model, deploy and test its hardware scaffold, contributing to open source hardware and software packages so that SENSEI technologies can replicated and adopted by the research and education communities. Though currently a prototype, the final SENSEI camera system, as envisioned, with multi-spectral and variable focal options, will allow for deeper and higher-density information analysis in addition to high-resolution imagery well adapted to human visual acuity. As described above, SENSEI has application in a variety of image-intensive domain sciences, enabling researchers, students, and the general public to accurately experience places of scientific or engineering interest, or to share cultural heritage locations that are restricted or too difficult or expensive to visit. SENSEI is also a necessary tool to help address societal grand challenges, including environmental monitoring, remote sensing, situational awareness, homeland security, and mechanical and structural engineering.       Last Modified: 12/23/2019       Submitted by: Maxine D Brown]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
