<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: DSD: Collaborative Research: Rapid Prototyping HPC Environment for Deep Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>202500.00</AwardTotalIntnAmount>
<AwardAmount>202500</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tevfik Kosar</SignBlockName>
<PO_EMAI>tkosar@nsf.gov</PO_EMAI>
<PO_PHON>7032927992</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The impact of Big Data is all around us and is enabling a plethora of commercial services. Further it is establishing the fourth paradigm of scientific investigation where discovery is based on mining data rather than from theories verified by observation. Big Data has established a new discipline (Data Science) with vibrant research activities across several areas of computer science. This ?Rapid Python Deep Learning Infrastructure? (RaPyDLI) project advances Deep Learning (DL) which is a novel exciting artificial intelligence approach to Big Data problems, which also involves a sophisticated model and a corresponding ?big compute? needing high end supercomputer architectures. DL has already seen success in areas like speech recognition, drug discovery and computer vision where self-driving cars are an early target. DL uses a very general unbiased way of analyzing large data sets inspired by the brain as a set of connected neurons. As with the brain, the artificial neurons learn from experience corresponding to a ?training dataset? and the ?trained network? can be used to make decisions. Trained on voices, the DL network can enhance voice recognition and trained on images, the DL network can recognize objects in the image. A recent study by the Stanford participants in this project trained 10 billion connections on 10 million images to recognize objects in an image. This study involved a dataset that was approximately 0.1% the size of data ?learnt? by an adult human in their lifetime and one billionth of the total digital data stored in the world today. Note the 1.5 billion images uploaded to social media sites every day emphasize the staggering size of big data. The project aims to enhance by DL by allowing it to use large supercomputers efficiently and by providing a convenient DL computing environment that enables rapid prototyping i.e. interactive experimentation with new algorithms. This will enable DL to be applied to much larger datasets such as those ?seen? by a human in their lifetime. The RaPyDLI partnership of Indiana University, University of Tennessee, and Stanford enables this with expertise in parallel computing algorithms and run times, big data, clouds, and DL itself.&lt;br/&gt;RaPyDLI will reach out to DL practitioners with workshops both to gather requirements for and feedback on its software. Further it will proactively reach out to under-represented communities with summer experiences and DL curriculum modules that include demonstrations built as ?Deep Learning as a Service?.&lt;br/&gt;RaPyDLI will be built as a set of open source modules that can be accessed from a Python user interface but executed interoperably in a C/C++ or Java environment on the largest supercomputers or clouds with interactive analysis and visualization. RaPyDLI will support GPU accelerators and Intel Phi coprocessors and a broad range of storage approaches including files, NoSQL, HDFS and databases. RaPyDLI will include benchmarks as well as software and will offer a repository so users can contribute the high level code for a range of neural networks with benefits to research and education.</AbstractNarration>
<MinAmdLetterDate>08/05/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439005</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Ng</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew Y Ng</PI_FULL_NAME>
<EmailAddress>ang@cs.stanford.edu</EmailAddress>
<PI_PHON>6507232300</PI_PHON>
<NSF_ID>000215465</NSF_ID>
<StartDate>08/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[450 Jane Stanford Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~202500</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>In recent years, Deep Neural Networks have become a hotly contested technology in academia and commercially. Caffe from UC Berkeley has become the de facto standard (</span><a rel="nofollow" href="http://demo.caffe.berkeleyvision.org/">http://demo.caffe.berkeleyvision.org/</a><span>), while some other projects are now obsolete (e.g., ConvNet and ConvNet2). The Python-only Theano gets some traction due to its purely mathematical approach to defining machine learning algorithms. We use Stanford&rsquo;s system called FastLab (formerly DeepSail), and it includes both multi-GPU and MPI support, but lacks some features present in Caffe. Berkeley Caffe achieves performance by calling cuBlas, but the matrix shapes passed to cuBlas are not the most favorable for good performance (they&rsquo;re driven by the shape of DNN). In RaPyDLI, we tried autotuning these oddly-shaped calls and some improvements can be achieved. NVIDIA makes available a closed source library called cuDNN (CUDA-based DNN) and much better performance is achieved (10% over standard Caffe as advertised by NVIDIA) due to a more DNN-optimized interface. Other DNN libraries include Facebook&rsquo;s approach, which is based on phase space computation and use of FFT for conversion through highly optimized fbfft (superior to NVIDIA&rsquo;s cuFFT). Ebay also developed a DNN library specifically for NVIDIA&rsquo;s Maxwell architecture; their code is called maxDNN (Maxwell DNN), which is based on reverse engineering work that produced knowledge of binary code and made an assembler for Maxwell provided by Nervana on GitHub. maxDNN achieves over 90% of the peak performance on single precision matrix-matrix multiply. The published code uses almost all available GPU optimization tricks: Instruction scheduling and constant cache to remove overhead of integer instructions. With these efforts from the Big Data and Social Network industry, the performance gains left on the table reach the point of diminishing returns.</span>We attended the XPS PI meeting presenting a talk and Poster. We attended two workshops with a strong deep learning focus: a) Deep Learning and Representation Learning Workshop co-located with NIPS 2014, December 12, 2014. We had a meeting with the NVIDIA team at the GPU Technology Conference (GTC), April 4-8, 2015, mainly updating progress of the teams involved and community interest from industry, universities and national labs. We have presented at the DOE Machine Learning and Understanding for Intelligent Extreme Scale Scientific Computing and Discovery workshop.</p><br> <p>            Last Modified: 12/20/2017<br>      Modified by: Andrew&nbsp;Y&nbsp;Ng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In recent years, Deep Neural Networks have become a hotly contested technology in academia and commercially. Caffe from UC Berkeley has become the de facto standard (http://demo.caffe.berkeleyvision.org/), while some other projects are now obsolete (e.g., ConvNet and ConvNet2). The Python-only Theano gets some traction due to its purely mathematical approach to defining machine learning algorithms. We use Stanford?s system called FastLab (formerly DeepSail), and it includes both multi-GPU and MPI support, but lacks some features present in Caffe. Berkeley Caffe achieves performance by calling cuBlas, but the matrix shapes passed to cuBlas are not the most favorable for good performance (they?re driven by the shape of DNN). In RaPyDLI, we tried autotuning these oddly-shaped calls and some improvements can be achieved. NVIDIA makes available a closed source library called cuDNN (CUDA-based DNN) and much better performance is achieved (10% over standard Caffe as advertised by NVIDIA) due to a more DNN-optimized interface. Other DNN libraries include Facebook?s approach, which is based on phase space computation and use of FFT for conversion through highly optimized fbfft (superior to NVIDIA?s cuFFT). Ebay also developed a DNN library specifically for NVIDIA?s Maxwell architecture; their code is called maxDNN (Maxwell DNN), which is based on reverse engineering work that produced knowledge of binary code and made an assembler for Maxwell provided by Nervana on GitHub. maxDNN achieves over 90% of the peak performance on single precision matrix-matrix multiply. The published code uses almost all available GPU optimization tricks: Instruction scheduling and constant cache to remove overhead of integer instructions. With these efforts from the Big Data and Social Network industry, the performance gains left on the table reach the point of diminishing returns.We attended the XPS PI meeting presenting a talk and Poster. We attended two workshops with a strong deep learning focus: a) Deep Learning and Representation Learning Workshop co-located with NIPS 2014, December 12, 2014. We had a meeting with the NVIDIA team at the GPU Technology Conference (GTC), April 4-8, 2015, mainly updating progress of the teams involved and community interest from industry, universities and national labs. We have presented at the DOE Machine Learning and Understanding for Intelligent Extreme Scale Scientific Computing and Discovery workshop.       Last Modified: 12/20/2017       Submitted by: Andrew Y Ng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
