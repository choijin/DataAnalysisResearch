<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Functional Object-Oriented Network for Manipulation Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>398529.00</AwardTotalIntnAmount>
<AwardAmount>398529</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project constructs a functional object-oriented network (FOON) to model the connectivity of the functional-related objects and their functional motions in manipulation tasks. The graphical model FOON are developed and learned by observing state change of objects and human manipulations with the objects. A well-trained FOON enables robots to decipher a task goal, seek the correct objects at the desired states, and generate an optimal motion, which eventually allows robots to perform daily living tasks and provide needed help to people with disabilities and seniors. The project builds a kitchen environment with representative kitchen objects and a virtual environment to collect training data and learn a representative FOON from the observations of self-generated manipulation tasks performed in both environments. The data collected in the proposed work enables researchers to fully explore daily living tasks from the object state and object motion point of view and provide excellent training and testing data sets for other related research. The FOON provides a powerful tool for discovering new insights in neuroscience, cognitive science, and psychology. It is also useful in training to break down skills for teaching and training evaluation. This project draws additional minority students through the exceptional diversity programs that exist at USF. The project supports a summer camp to attract young students to STEM careers and the research results are widely disseminated through publications and inclusion in courses.</AbstractNarration>
<MinAmdLetterDate>08/13/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1421418</AwardID>
<Investigator>
<FirstName>Yu</FirstName>
<LastName>Sun</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yu Sun</PI_FULL_NAME>
<EmailAddress>yusun@mail.usf.edu</EmailAddress>
<PI_PHON>8139745465</PI_PHON>
<NSF_ID>000552980</NSF_ID>
<StartDate>08/13/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of South Florida</Name>
<CityName>Tampa</CityName>
<ZipCode>336172008</ZipCode>
<PhoneNumber>8139742897</PhoneNumber>
<StreetAddress>4019 E. Fowler Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 100]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL14</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>069687242</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTH FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>069687242</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of South Florida]]></Name>
<CityName>Tampa</CityName>
<StateCode>FL</StateCode>
<ZipCode>336129446</ZipCode>
<StreetAddress><![CDATA[4202 E. Fowler Ave., ENB 118]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL14</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~103441</FUND_OBLG>
<FUND_OBLG>2015~295088</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Outcomes&nbsp;With the support, the research team at USF has created a new knowledge representation called a functional object-oriented network (FOON). It advanced the knowledge representation by providing the relationships between objects and motions in the network. The basic structure and algorithms have been developed at the beginning of the project. Then we labeled 100 cooking instructional videos to make a universal FOON for cooking. The goal is to make a FOON that has comprehensive knowledge about cooking so that a robot could follow a task tree generated from FOON. After we have the first version FOON, we have evaluated it with many cooking requests and demonstrated that FOON is a valid way of storing and conveying cooking knowledge to robots.&nbsp;&nbsp;</p> <p>We have computed a histogram of all the motion labels and found that 20 motions account for 85% of all motions since they happen very frequently. The most performed motion in our cooking videos is pouring. Therefore, we have developed a recurrent neural network that learns pouring skills from human demonstration. Our pouring motion generator has been evaluated in different conditions, and it can pour as precisely as humans with a human-like speed.&nbsp;</p> <p>Cooking knowledge can be retrieved from FOON as a task tree. However, the current state-of-the-art robot and motion planners cannot perform all cooking motions correctly. We have demonstrated several robotic cooking using FOON. However, quickly, we realized that a robot could not finish a cooking task alone even with a perfect clear task tree because of the limitation of the robot. Motivated by that, we have developed a weighted FOON that takes the robot&rsquo;s capability into consideration. &nbsp;A task tree retrieved from the weighted FOON automatically takes the capability of a robot and the handicap of a human into consideration. It assigns robot-infeasible motions to a human collaborator and a human-handicap motion to a robot.&nbsp;</p> <p>The labels of the 100 videos have ingredient names, tool names, motion names, and their relationship and timing in the video. The videos and their labels are released. The universal FOON has 1,921 motion nodes, 3,407 object nodes. It has been released as well.&nbsp;&nbsp;</p> <p>Labeling cooking videos cost a lot of time. It is unrealistic to label all cooking videos. To generalize FOON to unseen recipes, we have developed abstraction of FOON using levels of hierarchy provide in WordNet and created a FOON-GEN using object catalogs. We have also expanded FOON to FOON-EXP using object similarities derived from WordNet.&nbsp;</p> <p>Since FOON connects ingredients, motions, and provide cooking process sequence information, it provides a good prior knowledge in cooking. We have developed a cooking activity video understanding algorithm using FOON as a prior to estimate recipe names from cooking videos and extract step by step content in cooking videos with better than state-of-the-art accuracy.&nbsp;</p> <p>We have found states (food ingredient states such as sliced, peeled) are crucial to cooking. Therefore, we have defined cooking state taxonomy and developed a visual-based object state classifier that can recognize ingredients and their states at the same time.&nbsp;&nbsp;</p> <p>The research outcomes have been used in the PI&rsquo;s classes as teaching material. Object state recognition has been introduced in the PI&rsquo;s Neural Network and Deep Learning class and converted into a class project. Knowledge representation and FOON were taught to the undergraduate students in the PI&rsquo;s Intro to Robotics class. The universal FOON was used in the class&rsquo;s project.&nbsp;&nbsp;</p> <p>The project has trained three Ph.D. students and five undergraduate students (REUs). One Ph.D. student has graduated, and the second Ph.D. student will graduate in Spring 2020. All five undergraduate students have graduated. &nbsp;Two of them went on to graduate schools.&nbsp;</p><br> <p>            Last Modified: 10/30/2019<br>      Modified by: Yu&nbsp;Sun</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Outcomes With the support, the research team at USF has created a new knowledge representation called a functional object-oriented network (FOON). It advanced the knowledge representation by providing the relationships between objects and motions in the network. The basic structure and algorithms have been developed at the beginning of the project. Then we labeled 100 cooking instructional videos to make a universal FOON for cooking. The goal is to make a FOON that has comprehensive knowledge about cooking so that a robot could follow a task tree generated from FOON. After we have the first version FOON, we have evaluated it with many cooking requests and demonstrated that FOON is a valid way of storing and conveying cooking knowledge to robots.    We have computed a histogram of all the motion labels and found that 20 motions account for 85% of all motions since they happen very frequently. The most performed motion in our cooking videos is pouring. Therefore, we have developed a recurrent neural network that learns pouring skills from human demonstration. Our pouring motion generator has been evaluated in different conditions, and it can pour as precisely as humans with a human-like speed.   Cooking knowledge can be retrieved from FOON as a task tree. However, the current state-of-the-art robot and motion planners cannot perform all cooking motions correctly. We have demonstrated several robotic cooking using FOON. However, quickly, we realized that a robot could not finish a cooking task alone even with a perfect clear task tree because of the limitation of the robot. Motivated by that, we have developed a weighted FOON that takes the robot?s capability into consideration.  A task tree retrieved from the weighted FOON automatically takes the capability of a robot and the handicap of a human into consideration. It assigns robot-infeasible motions to a human collaborator and a human-handicap motion to a robot.   The labels of the 100 videos have ingredient names, tool names, motion names, and their relationship and timing in the video. The videos and their labels are released. The universal FOON has 1,921 motion nodes, 3,407 object nodes. It has been released as well.    Labeling cooking videos cost a lot of time. It is unrealistic to label all cooking videos. To generalize FOON to unseen recipes, we have developed abstraction of FOON using levels of hierarchy provide in WordNet and created a FOON-GEN using object catalogs. We have also expanded FOON to FOON-EXP using object similarities derived from WordNet.   Since FOON connects ingredients, motions, and provide cooking process sequence information, it provides a good prior knowledge in cooking. We have developed a cooking activity video understanding algorithm using FOON as a prior to estimate recipe names from cooking videos and extract step by step content in cooking videos with better than state-of-the-art accuracy.   We have found states (food ingredient states such as sliced, peeled) are crucial to cooking. Therefore, we have defined cooking state taxonomy and developed a visual-based object state classifier that can recognize ingredients and their states at the same time.    The research outcomes have been used in the PI?s classes as teaching material. Object state recognition has been introduced in the PI?s Neural Network and Deep Learning class and converted into a class project. Knowledge representation and FOON were taught to the undergraduate students in the PI?s Intro to Robotics class. The universal FOON was used in the class?s project.    The project has trained three Ph.D. students and five undergraduate students (REUs). One Ph.D. student has graduated, and the second Ph.D. student will graduate in Spring 2020. All five undergraduate students have graduated.  Two of them went on to graduate schools.        Last Modified: 10/30/2019       Submitted by: Yu Sun]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
