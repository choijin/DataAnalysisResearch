<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>695525.00</AwardTotalIntnAmount>
<AwardAmount>695525</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of "nuclei," which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.</AbstractNarration>
<MinAmdLetterDate>08/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>10/04/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1450488</AwardID>
<Investigator>
<FirstName>Carl</FirstName>
<LastName>Maltzahn</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carl G Maltzahn</PI_FULL_NAME>
<EmailAddress>carlosm@ucsc.edu</EmailAddress>
<PI_PHON>8314591627</PI_PHON>
<NSF_ID>000419997</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Cruz</Name>
<CityName>Santa Cruz</CityName>
<ZipCode>950641077</ZipCode>
<PhoneNumber>8314595278</PhoneNumber>
<StreetAddress>1156 High Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>125084723</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA CRUZ</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Cruz]]></Name>
<CityName>Santa Cruz</CityName>
<StateCode>CA</StateCode>
<ZipCode>950641077</ZipCode>
<StreetAddress><![CDATA[1156 High St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1525</Code>
<Text>Physical &amp; Dynamic Meteorology</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8009</Code>
<Text>Scientifc Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~695525</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Big Weather Web was a very successful project to make big data infrastructure affordable and adequate for university members of the numerical weather prediction community. The key insight of the project was that the highly evolved strategies and techniques of software delivery in the commercial sector -- and widely used in the public cloud big data infrastructures&nbsp; -- can be beneficially adapted to the delivery of research in numerical weather prediction and&nbsp; other scientific domains. <br /><br />Ivo Jimenez created Popper, a container-native workflow execution engine that ensures easy reuse and practical reproducibility of scientific workflows and is an abstraction over multiple software container runtimes, including Docker, Singularity, and Podman. The support of multiple container runtimes makes Popper useful for many scientific communities, including the high performance computing and machine learning communities. For this work he was awarded one of four 2018 Better Scientific Software (BSSw) fellowships as recognition as a leader and advocate of high-quality scientific software. Jimenez completed his Ph.D. thesis on "Agile Research Delivery" in 2019 and continues his work on Popper as Incubator Fellow at the UC Santa Cruz Center for Research in Open Source Software (CROSS). He founded the International Workshop on Practical Reproducible Evaluation of Computer Systems (P-RECS) that was held in conjunction with HPDC'18, HPDC'19, and is scheduled for HPDC'20 and was able to assemble excellent program committees for peer review of workshop contributions. <br /><br />Dr. Joshua Hacker, John Exby, and Kate Fossell at the National Center for Atmospheric Research (NCAR) transformed research and education in Numerical Weather Prediction with NCAR Docker-WRF, a set of software containers for running the Weather Research and Forecasting (WRF) and to analyze and share results with collaborators. Docker-WRF greatly facilitates the reproducibility of published results and shortened the time for students to learn how to productively run WRF from months to minutes. Hacker and Exby later left NCAR to join Jupiter Technology Systems where they continued supporting and enhancing Docker software containers for Popper workflows for end-to-end numerical weather prediction research and education, and helped transfer technology developed in the Big Weather Web project to the private sector and established a&nbsp; software infrastructure for collaborations between education, research, and the private sector.<br /><br />Led by Prof. Gretchen Mullendore (University of North Dakota), Profs. Brian Ancell (Texas Tech University), William Capehart (South Dakota School of Mines and Technology), Clark Evans (University of Wisconsin, Milwaukee), Robert Fovell (University at Albany, State University of New York), Steven Greybush (Pennsylvania State University), and Russ Schumacher (Colorado State University) created the first non-operational numerical weather prediction ensemble dataset shared via the cloud consisting of 100s of Terabytes. At first the dataset was hosted at Amazon Web Services (AWS). However, because of AWS' egress cost of moving data to supercomputers allocations available to university members of the numerical weather prediction community, the dataset was also uploaded to XSEDE Wrangler resources where it is productively used for peer-reviewed science using NCAR Docker-WRF and Popper.</p><br> <p>            Last Modified: 09/08/2020<br>      Modified by: Carl&nbsp;G&nbsp;Maltzahn</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1450488/1450488_10383291_1599542209048_Logo-1000x500--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1450488/1450488_10383291_1599542209048_Logo-1000x500--rgov-800width.jpg" title="Big Weather Web logo"><img src="/por/images/Reports/POR/2020/1450488/1450488_10383291_1599542209048_Logo-1000x500--rgov-66x44.jpg" alt="Big Weather Web logo"></a> <div class="imageCaptionContainer"> <div class="imageCaption">bigweatherweb.org</div> <div class="imageCredit">Carlos Maltzahn</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Carl&nbsp;G&nbsp;Maltzahn</div> <div class="imageTitle">Big Weather Web logo</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1450488/1450488_10383291_1599542342046_popper_logo--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1450488/1450488_10383291_1599542342046_popper_logo--rgov-800width.jpg" title="Popper logo"><img src="/por/images/Reports/POR/2020/1450488/1450488_10383291_1599542342046_popper_logo--rgov-66x44.jpg" alt="Popper logo"></a> <div class="imageCaptionContainer"> <div class="imageCaption">getpopper.io</div> <div class="imageCredit">Ivo Jimenez</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Carl&nbsp;G&nbsp;Maltzahn</div> <div class="imageTitle">Popper logo</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Big Weather Web was a very successful project to make big data infrastructure affordable and adequate for university members of the numerical weather prediction community. The key insight of the project was that the highly evolved strategies and techniques of software delivery in the commercial sector -- and widely used in the public cloud big data infrastructures  -- can be beneficially adapted to the delivery of research in numerical weather prediction and  other scientific domains.   Ivo Jimenez created Popper, a container-native workflow execution engine that ensures easy reuse and practical reproducibility of scientific workflows and is an abstraction over multiple software container runtimes, including Docker, Singularity, and Podman. The support of multiple container runtimes makes Popper useful for many scientific communities, including the high performance computing and machine learning communities. For this work he was awarded one of four 2018 Better Scientific Software (BSSw) fellowships as recognition as a leader and advocate of high-quality scientific software. Jimenez completed his Ph.D. thesis on "Agile Research Delivery" in 2019 and continues his work on Popper as Incubator Fellow at the UC Santa Cruz Center for Research in Open Source Software (CROSS). He founded the International Workshop on Practical Reproducible Evaluation of Computer Systems (P-RECS) that was held in conjunction with HPDC'18, HPDC'19, and is scheduled for HPDC'20 and was able to assemble excellent program committees for peer review of workshop contributions.   Dr. Joshua Hacker, John Exby, and Kate Fossell at the National Center for Atmospheric Research (NCAR) transformed research and education in Numerical Weather Prediction with NCAR Docker-WRF, a set of software containers for running the Weather Research and Forecasting (WRF) and to analyze and share results with collaborators. Docker-WRF greatly facilitates the reproducibility of published results and shortened the time for students to learn how to productively run WRF from months to minutes. Hacker and Exby later left NCAR to join Jupiter Technology Systems where they continued supporting and enhancing Docker software containers for Popper workflows for end-to-end numerical weather prediction research and education, and helped transfer technology developed in the Big Weather Web project to the private sector and established a  software infrastructure for collaborations between education, research, and the private sector.  Led by Prof. Gretchen Mullendore (University of North Dakota), Profs. Brian Ancell (Texas Tech University), William Capehart (South Dakota School of Mines and Technology), Clark Evans (University of Wisconsin, Milwaukee), Robert Fovell (University at Albany, State University of New York), Steven Greybush (Pennsylvania State University), and Russ Schumacher (Colorado State University) created the first non-operational numerical weather prediction ensemble dataset shared via the cloud consisting of 100s of Terabytes. At first the dataset was hosted at Amazon Web Services (AWS). However, because of AWS' egress cost of moving data to supercomputers allocations available to university members of the numerical weather prediction community, the dataset was also uploaded to XSEDE Wrangler resources where it is productively used for peer-reviewed science using NCAR Docker-WRF and Popper.       Last Modified: 09/08/2020       Submitted by: Carl G Maltzahn]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
