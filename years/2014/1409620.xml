<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDS&amp;E:  Fast, scalable GPU-enabled software for predictive materials design &amp; discovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>595880.00</AwardTotalIntnAmount>
<AwardAmount>595880</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03070000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMR</Abbreviation>
<LongName>Division Of Materials Research</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Daryl Hess</SignBlockName>
<PO_EMAI>dhess@nsf.gov</PO_EMAI>
<PO_PHON>7032924942</PO_PHON>
</ProgramOfficer>
<AbstractNarration>NONTECHNICAL SUMMARY&lt;br/&gt;&lt;br/&gt;This Computational and Data-Enabled Science and Engineering award supports computational materials research and the development of computational tools for materials research. Designing novel materials requires new computational tools capable of performing simulations that reveal unexpected insights that, in turn, inform the way we think about materials and materials processes. These tools must be scientifically valid, robust, accessible and easy to use, and should exploit the fastest available hardware. Today this hardware involves graphics processing units, known as GPUS, whose architecture exploits massive parallelism, allowing many more calculations to be done simultaneously per second on a single chip than on current, more traditional CPUs.  To use GPUs for the study of materials systems and chemical processes, the workhorse algorithms and codes used by that community of researchers must be redesigned and rewritten specifically for that architecture. This project will develop those tools, share them broadly with an existing and rapidly growing user base, and apply them, as an exemplar area, to the outstanding and computationally demanding problem of colloidal crystallization. In colloidal crystallization, micron-sized particles suspended in solution self-assemble into ordered structures, giving rise to properties and behavior with wide-ranging application. Predicting these structures requires considerable computation, especially for complex crystal structures.  This project will also train students in software engineering, algorithm design, and open source software development for materials simulation. The enhancements to HOOMD-Blue, DEM-HOOMD-Blue and HPMC developed under this award will be made available to the broader community through the HOOMD-Blue website on University of Michigan Codeblue.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;TECHNICAL SUMMARY&lt;br/&gt;&lt;br/&gt;This Computational and Data-Enabled Science and Engineering award supports computational materials research and the development of computational tools for materials research. This project will develop simulation software for materials and chemical systems.  Building on the open source software platform known as highly optimized object oriented many particle dynamics-blue, the PI will expand the capabilities of HOOMD-Blue to include discrete-element molecular dynamics and Monte Carlo algorithms optimized for Graphics Processor Units. A workhorse tool for granular matter, DEM will be adopted for hard particle collisions in the absence of friction, allowing high fidelity studies of the dynamics and thermodynamics of colloidal systems. Monte Carlo - a traditionally serial algorithm for sampling phase space stochastically - will leverage the HOOMD infrastructure to achieve a high degree of parallelism using a checkerboarding strategy. Both additions, DEM-HOOMD-blue and HPMC, will allow the simulation of particle-based systems and materials processes of considerable complexity. The PI will demonstrate the efficacy of the codes by applying them to the problem of crystal nucleation and growth in hard particle systems driven to order by entropy maximization. Combined with rare event sampling tools, DEM-HOOMD-blue and HPMC will enable the study of thermodynamic and kinetic pathways by which hard-particle fluids assemble into quasicrystals and open, chiral, or hierarchical crystals characterized by large or complex unit cells. Given that the current state of the art in nucleation and growth simulation studies of nanoparticles and colloids is limited to simple Bravais lattices, this project will expand the knowledge base needed to design new crystalline materials.  Following current HOOMD-blue strategy, the new computational tools will run efficiently on laptops, desktops, and massive GPU clusters, thereby serving multiple user types. The PI's findings will be of immediate interest to the nanoparticle and colloidal assembly communities. The PI's approaches and tools are transferable and will be of immediate and even broader interest to the materials, engineering, and chemistry communities interested in crystallization of appropriate atomic, molecular, or nanoscale building blocks. This project will also train students in software engineering, algorithm design, and open source software development for materials simulation. The enhancements to HOOMD-Blue, DEM-HOOMD-Blue and HPMC developed under this award will be made available to the broader community through the HOOMD-Blue website on University of Michigan Codeblue.</AbstractNarration>
<MinAmdLetterDate>06/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409620</AwardID>
<Investigator>
<FirstName>Sharon</FirstName>
<LastName>Glotzer</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sharon C Glotzer</PI_FULL_NAME>
<EmailAddress>sglotzer@umich.edu</EmailAddress>
<PI_PHON>7346156296</PI_PHON>
<NSF_ID>000472028</NSF_ID>
<StartDate>06/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName>Ann Arbor</CityName>
<StateCode>MI</StateCode>
<ZipCode>481091274</ZipCode>
<StreetAddress><![CDATA[3003 S. State St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1765</Code>
<Text>CONDENSED MATTER &amp; MAT THEORY</Text>
</ProgramElement>
<ProgramElement>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramElement>
<ProgramReference>
<Code>7237</Code>
<Text>NANO NON-SOLIC SCI &amp; ENG AWD</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7569</Code>
<Text>CYBERINFRASTRUCTURE/SCIENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~595880</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This CDS&amp;E project involves the sustained development of the <strong>HOOMD-blue </strong>particle simulation toolkit to add capabilities to the suite, and optimize for new generations of GPU architectures. HOOMD-blue's primary focus is on soft matter, especially nanoparticles, colloids, and other building blocks on dimensions from nanometers to microns.<strong><em> &nbsp;</em></strong>The software was designed for GPUs since its inception in 2008, and for 10 years has been the fastest publicly available molecular simulation code in existence (for soft-matter simulations), with benchmarks reported by NVIDIA (http://www.nvidia.com/object/tesla-p100.html). &nbsp;Written in CUDA, C++ and Python, HOOMD-blue is highly extensible and is now the only software to provide not only GPU-enabled traditional Molecular Dynamics (MD), but also GPU implementations of discrete element method molecular dynamics (DEM-MD) and hard particle Monte Carlo (HPMC) simulations of particles of arbitrary shape. Developing, implementing and releasing to the public DEM-MD and HPMC capabilities in HOOMD-blue were the primary goals of this CDS&amp;E grant.</p> <p>We implemented DEM-MD on GPUs and added it to HOOMD-blue. DEM-MD enables molecular dynamics simulations of rounded polyhedral particles, such as those created at the nano- and colloidal scale by experimentalists. Our implementation lacks the frictional contact force present in traditional DEM implementations by choice -- we have redesigned the algorithm to be appropriate for the kinds of forces present in colloidal and nanoscale materials, rather than those typical in macroscopic granular matter. To our knowledge, our implementation is the first to bring DEM into the domain of classical MD. The method is intended for capturing steric repulsive forces between particles -- such as those that have been ligated, charge-screened, or otherwise functionalized to&nbsp; interact only via short-ranged forces, and is most directly applicable to studying the behavior of coarse-grained faceted colloidal- and nanoscale particles with higher shape fidelity than can be achieved with rigid bodies of spheres.</p> <p>HPMC allows HOOMD-blue to perform hard particle Monte Carlo simulations of many different shape classes, including spheres / disks, unions of spheres, convex polygons, convex spheropolygons, concave polygons, ellipsoids / ellipses, convex polyhedra, convex spheropolyhedra, spheres cut by planes, concave polyhedra, and unions of convex polyhedra. HPMC is the first open source general purpose hard particle Monte Carlo code and the first with efficient parallel execution on many CPUs and GPUs. Parallel execution on the CPU is effective, even for small systems of 4096 dodecahedra: &nbsp;a simulation that used to take more than 20 hours can now be completed in 45 minutes on 96 CPU cores. Our GPU parallel implementation enables simulations that were not possible before. GPU simulations are best suited for large systems (&gt; 100,000 particles). &nbsp;</p> <p>We developed a highly optimized implementation of MPI domain decomposition in HOOMD-blue, and achieved optimal GPU performance by packing/unpacking communication buffers on the GPU and using an autotuning algorithm to choose kernel launch parameters. For a representative polymer physics application, we showed that these improvements provided an effective GPU vs. CPU node speed-up of 12.5<strong>x</strong>. Modern architectures execute this same benchmark 4x faster, and GPUs have outpaced CPUs, now delivering 14x the performance of a CPU node. The publication of this work in <em>Computer Physics Communications</em> in 2015 is identified by Elsevier Web of Science as a <strong>highly cited paper</strong> for its field.</p> <p>This award has resulted in eleven peer-reviewed publications, including publications in Nature Communications, PRX, Soft Matter, Langmuir, the Journal of Computational Physics, and Computer Physics Communications. Five more publications are in preparation. HOOMD-blue is publicly available on Bitbucket under a BSD-3 clause license. It is used by research groups worldwide and is growing steadily in popularity, with 446 active members on the mailing list, and 251+ (known) research articles using HOOMD-blue to date (31 of these papers are from the Glotzer Group). &nbsp;During this 4-yr CDS&amp;E award, students and postdocs gave 22 contributed talks at national and international conferences; 29 invited talks and colloquia were given by either the PI or by lead HOOMD-blue developer Dr. Joshua Anderson. Anderson received the 2015 CoMSEF Young Investigator Award <em>'For contributions to the development and dissemination of open source, GPU-enabled molecular simulation software, HOOMD-blue, which enables scientific computations with unprecedented speed,' </em>by the Computational Molecular Science and Engineering Forum of the American Institute for Chemical Engineers (AIChE). PI Sharon C. Glotzer has been selected for the 2018 Nanoscale Science and Engineering Forum Award in AIChE, among numerous awards and fellowships conferred during the award period. We presented six invited tutorial-type workshops at which we taught new users how to use HOOMD-blue: the APS March meeting (2017), AIChE annual meeting (in 2016, 2017 and 2018), FOMMS (Foundations of Molecular Modeling and Simulation) Conference (2015), and an NVIDIA-hosted webinar (2014). We estimate that as many as 400 students, postdocs, and senior researchers participated in these workshops; many of them learned to do simulations on GPUs for the first time at these workshops.</p><br> <p>            Last Modified: 10/31/2018<br>      Modified by: Sharon&nbsp;C&nbsp;Glotzer</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1409620/1409620_10312142_1540840836737_DEM_assemblies--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1409620/1409620_10312142_1540840836737_DEM_assemblies--rgov-800width.jpg" title="Examples of shape assembly with DEM-MD."><img src="/por/images/Reports/POR/2018/1409620/1409620_10312142_1540840836737_DEM_assemblies--rgov-66x44.jpg" alt="Examples of shape assembly with DEM-MD."></a> <div class="imageCaptionContainer"> <div class="imageCaption">A simulation of 4,096 truncated octahedra can take 25 days to simulate 10 million steps, a typical time to observe crystallization. With DEM-MD's inherent parallelism, the same simulation scales very well up to 16 GPUs where can complete 10 million steps in only 40 hours. 10.1016/j.jcp.2017.01.014</div> <div class="imageCredit">M. Spellings, Glotzer Group</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sharon&nbsp;C&nbsp;Glotzer</div> <div class="imageTitle">Examples of shape assembly with DEM-MD.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1409620/1409620_10312142_1540839943902_binarycrystals--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1409620/1409620_10312142_1540839943902_binarycrystals--rgov-800width.jpg" title="HPMC showed for the first time that binary crystal structures self-assemble"><img src="/por/images/Reports/POR/2018/1409620/1409620_10312142_1540839943902_binarycrystals--rgov-66x44.jpg" alt="HPMC showed for the first time that binary crystal structures self-assemble"></a> <div class="imageCaptionContainer"> <div class="imageCaption">HPMC showed for the first time that binary crystal structures can self-assemble from hard, non-interacting polyhedra due solely to entropic forces. The alternating arrangement of octahedra and tetrahedra is a known space-tessellation, but had not previously been observed in self-assembly simulations</div> <div class="imageCredit">J. Dshemuchadse, Glotzer Group</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sharon&nbsp;C&nbsp;Glotzer</div> <div class="imageTitle">HPMC showed for the first time that binary crystal structures self-assemble</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541021167562_smaller_spheres--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541021167562_smaller_spheres--rgov-800width.jpg" title="DEM reveals clathrate structures discovered in laboratory"><img src="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541021167562_smaller_spheres--rgov-66x44.jpg" alt="DEM reveals clathrate structures discovered in laboratory"></a> <div class="imageCaptionContainer"> <div class="imageCaption">To resolve the complex structure of clathrates formed in the lab from DNA-modified triangular bipyramids, we used DEM with implicit solvent to model the assemblies. Shown: Particles forming a clathrate I (cP54-K4Si23) structure, colored by their local density. 10.1126/science.aal3919</div> <div class="imageCredit">M. Spellings, Glotzer Group</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Sharon&nbsp;C&nbsp;Glotzer</div> <div class="imageTitle">DEM reveals clathrate structures discovered in laboratory</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541022763714_2Dmelt--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541022763714_2Dmelt--rgov-800width.jpg" title="HPMC reveals two-dimensional melting transition of hard polygons"><img src="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541022763714_2Dmelt--rgov-66x44.jpg" alt="HPMC reveals two-dimensional melting transition of hard polygons"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The melting transition of  2D systems is a fundamental problem in condensed matter and statistical physics.  We show that the melting transition depends upon both particle shape and symmetry, which together can predict which of three different melting scenarios will occur. 10.1103/PhysRevX.7.021001</div> <div class="imageCredit">J. Anderson, Glotzer Group</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sharon&nbsp;C&nbsp;Glotzer</div> <div class="imageTitle">HPMC reveals two-dimensional melting transition of hard polygons</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541025850059_allophiles-SM--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541025850059_allophiles-SM--rgov-800width.jpg" title="Shape allophiles improve entropic assembly"><img src="/por/images/Reports/POR/2018/1409620/1409620_10312142_1541025850059_allophiles-SM--rgov-66x44.jpg" alt="Shape allophiles improve entropic assembly"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Using HPMC we investigated a class of 'shape allophiles' that fit together like puzzle pieces, as a method to access and stabilize desired structures by controlling directional entropic forces.</div> <div class="imageCredit">E. Harper, Glotzer Group</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sharon&nbsp;C&nbsp;Glotzer</div> <div class="imageTitle">Shape allophiles improve entropic assembly</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This CDS&amp;E project involves the sustained development of the HOOMD-blue particle simulation toolkit to add capabilities to the suite, and optimize for new generations of GPU architectures. HOOMD-blue's primary focus is on soft matter, especially nanoparticles, colloids, and other building blocks on dimensions from nanometers to microns.  The software was designed for GPUs since its inception in 2008, and for 10 years has been the fastest publicly available molecular simulation code in existence (for soft-matter simulations), with benchmarks reported by NVIDIA (http://www.nvidia.com/object/tesla-p100.html).  Written in CUDA, C++ and Python, HOOMD-blue is highly extensible and is now the only software to provide not only GPU-enabled traditional Molecular Dynamics (MD), but also GPU implementations of discrete element method molecular dynamics (DEM-MD) and hard particle Monte Carlo (HPMC) simulations of particles of arbitrary shape. Developing, implementing and releasing to the public DEM-MD and HPMC capabilities in HOOMD-blue were the primary goals of this CDS&amp;E grant.  We implemented DEM-MD on GPUs and added it to HOOMD-blue. DEM-MD enables molecular dynamics simulations of rounded polyhedral particles, such as those created at the nano- and colloidal scale by experimentalists. Our implementation lacks the frictional contact force present in traditional DEM implementations by choice -- we have redesigned the algorithm to be appropriate for the kinds of forces present in colloidal and nanoscale materials, rather than those typical in macroscopic granular matter. To our knowledge, our implementation is the first to bring DEM into the domain of classical MD. The method is intended for capturing steric repulsive forces between particles -- such as those that have been ligated, charge-screened, or otherwise functionalized to  interact only via short-ranged forces, and is most directly applicable to studying the behavior of coarse-grained faceted colloidal- and nanoscale particles with higher shape fidelity than can be achieved with rigid bodies of spheres.  HPMC allows HOOMD-blue to perform hard particle Monte Carlo simulations of many different shape classes, including spheres / disks, unions of spheres, convex polygons, convex spheropolygons, concave polygons, ellipsoids / ellipses, convex polyhedra, convex spheropolyhedra, spheres cut by planes, concave polyhedra, and unions of convex polyhedra. HPMC is the first open source general purpose hard particle Monte Carlo code and the first with efficient parallel execution on many CPUs and GPUs. Parallel execution on the CPU is effective, even for small systems of 4096 dodecahedra:  a simulation that used to take more than 20 hours can now be completed in 45 minutes on 96 CPU cores. Our GPU parallel implementation enables simulations that were not possible before. GPU simulations are best suited for large systems (&gt; 100,000 particles).    We developed a highly optimized implementation of MPI domain decomposition in HOOMD-blue, and achieved optimal GPU performance by packing/unpacking communication buffers on the GPU and using an autotuning algorithm to choose kernel launch parameters. For a representative polymer physics application, we showed that these improvements provided an effective GPU vs. CPU node speed-up of 12.5x. Modern architectures execute this same benchmark 4x faster, and GPUs have outpaced CPUs, now delivering 14x the performance of a CPU node. The publication of this work in Computer Physics Communications in 2015 is identified by Elsevier Web of Science as a highly cited paper for its field.  This award has resulted in eleven peer-reviewed publications, including publications in Nature Communications, PRX, Soft Matter, Langmuir, the Journal of Computational Physics, and Computer Physics Communications. Five more publications are in preparation. HOOMD-blue is publicly available on Bitbucket under a BSD-3 clause license. It is used by research groups worldwide and is growing steadily in popularity, with 446 active members on the mailing list, and 251+ (known) research articles using HOOMD-blue to date (31 of these papers are from the Glotzer Group).  During this 4-yr CDS&amp;E award, students and postdocs gave 22 contributed talks at national and international conferences; 29 invited talks and colloquia were given by either the PI or by lead HOOMD-blue developer Dr. Joshua Anderson. Anderson received the 2015 CoMSEF Young Investigator Award 'For contributions to the development and dissemination of open source, GPU-enabled molecular simulation software, HOOMD-blue, which enables scientific computations with unprecedented speed,' by the Computational Molecular Science and Engineering Forum of the American Institute for Chemical Engineers (AIChE). PI Sharon C. Glotzer has been selected for the 2018 Nanoscale Science and Engineering Forum Award in AIChE, among numerous awards and fellowships conferred during the award period. We presented six invited tutorial-type workshops at which we taught new users how to use HOOMD-blue: the APS March meeting (2017), AIChE annual meeting (in 2016, 2017 and 2018), FOMMS (Foundations of Molecular Modeling and Simulation) Conference (2015), and an NVIDIA-hosted webinar (2014). We estimate that as many as 400 students, postdocs, and senior researchers participated in these workshops; many of them learned to do simulations on GPUs for the first time at these workshops.       Last Modified: 10/31/2018       Submitted by: Sharon C Glotzer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
