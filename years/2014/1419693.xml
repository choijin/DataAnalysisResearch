<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Understanding the Relevance of Text Passages</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James French</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Some information retrieval (IR) queries can be best answered with a web page, others can be answered with a single fact or named entity. Many other queries could best be answered with a text passage and we propose to develop new techniques for this task that will be a significant improvement on the current state-of-the-art. Developing effective passage retrieval would have a major effect on search tools by greatly extending the range of queries that could be answered directly using text passages retrieved from the web. This is particularly important for mobile search applications with limited output bandwidth based on using either a small screen or speech output. In this case, the ability to use passages to reduce the amount of output while maintaining high relevance will be critical. &lt;br/&gt;&lt;br/&gt;We will study research issues that have either been ignored, or only partially addressed, in prior research, such as showing whether passages be better answers than documents for some queries, predicting which queries have good answers at the passage level, ranking passages to retrieve the best answers, and evaluating the effectiveness of passages as answers. To address these issues, we will develop new retrieval models that can define and rank "answers" for different text granularities such as sentences and passages, models of query properties that are associated with good passage-level answers, and models that differentiate between topicality and information content.  Understanding the relevance of text passages will also involve obtaining new types of relevance assessments at passage granularity, and developing new evaluation metrics that combine relevance with the size of the result output. For further information see the project web site at: http://ciir.cs.umass.edu/research/answer_passages.</AbstractNarration>
<MinAmdLetterDate>08/06/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/06/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1419693</AwardID>
<Investigator>
<FirstName>W. Bruce</FirstName>
<LastName>Croft</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>W. Bruce Croft</PI_FULL_NAME>
<EmailAddress>croft@cs.umass.edu</EmailAddress>
<PI_PHON>4135450463</PI_PHON>
<NSF_ID>000095962</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Amherst</CityName>
<StateCode>MA</StateCode>
<ZipCode>010039264</ZipCode>
<StreetAddress><![CDATA[140 Governors Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has focused on studying the problems involved with providing focused answers to specific questions in a retrieval environment. This is in contrast to much of the previous research in information retrieval (IR) that has developed models for ranked retrieval of text documents in response to relatively short, &ldquo;keyword&rdquo; queries. Although the emphasis on retrieving short answers in the form of a small number of sentences was novel when we started the project, it is now one of the major research themes in our field. This is particularly true of conversational IR, which aims to develop techniques and methods to support IR in conversational setting, such as with devices such as the Amazon Echo or Google Home. Our project has been able to lead the way on some important aspects of this research, and consequently has had significant impact on academia and industry.<br /><br />The project has produced 15 publications in major research conferences and five technical reports. These publications cover a range of research.<br /><br />In earlier work, we studied feature-based models for retrieving answer passages and sentences from answer passages. We analyzed two types of features, namely semantic and context features in addition to traditional text matching features. We compared learning to rank methods with multiple baseline methods and showed that previous models and word-based features are not sufficient for retrieving answer sentences for non-factoid queries. With additional semantic and context features, however, we could significantly outperform the baseline methods.<br /><br />As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In one paper from this project, we proposed one of the first attention based neural matching models for ranking short answer texts and showed that it was particularly effective. We also studied embedding representations that were effective for answer retrieval and developed a specific form of the paragraph vector model for this task.<br /><br />Our most recent research in this project has developed models for finding responsive text responses or answers in a conversational setting. In this work, we proposed a learning framework on the top of deep neural matching networks that leverages external knowledge for response ranking in information-seeking conversation systems. Our models and research findings provide new insights on how to use external knowledge with deep neural models for response selection and have implications for the design of the next generation of information-seeking conversation systems.<br /><br />To understand the types of text responses and the answers that are used in conversational IR, we introduced a new dataset designed for research in conversational IR and used it to analyze information-seeking conversations. Our dataset and related datasets we have used for this research are based on open domain technical and problem-solving discussions.<br /><br />Another contribution we have made to the research community is to introduce and formalize the task of predicting questions in conversations, where the goal is to predict the new question that the user will ask, given the past conversational context. We developed neural models to address this task and evaluated it using technical chat log data.<br /><br />Given the importance of this area for the future development of search engines, we expect to see substantial future research based on the work done on this project.</p><br> <p>            Last Modified: 08/01/2018<br>      Modified by: W. Bruce&nbsp;Croft</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has focused on studying the problems involved with providing focused answers to specific questions in a retrieval environment. This is in contrast to much of the previous research in information retrieval (IR) that has developed models for ranked retrieval of text documents in response to relatively short, "keyword" queries. Although the emphasis on retrieving short answers in the form of a small number of sentences was novel when we started the project, it is now one of the major research themes in our field. This is particularly true of conversational IR, which aims to develop techniques and methods to support IR in conversational setting, such as with devices such as the Amazon Echo or Google Home. Our project has been able to lead the way on some important aspects of this research, and consequently has had significant impact on academia and industry.  The project has produced 15 publications in major research conferences and five technical reports. These publications cover a range of research.  In earlier work, we studied feature-based models for retrieving answer passages and sentences from answer passages. We analyzed two types of features, namely semantic and context features in addition to traditional text matching features. We compared learning to rank methods with multiple baseline methods and showed that previous models and word-based features are not sufficient for retrieving answer sentences for non-factoid queries. With additional semantic and context features, however, we could significantly outperform the baseline methods.  As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In one paper from this project, we proposed one of the first attention based neural matching models for ranking short answer texts and showed that it was particularly effective. We also studied embedding representations that were effective for answer retrieval and developed a specific form of the paragraph vector model for this task.  Our most recent research in this project has developed models for finding responsive text responses or answers in a conversational setting. In this work, we proposed a learning framework on the top of deep neural matching networks that leverages external knowledge for response ranking in information-seeking conversation systems. Our models and research findings provide new insights on how to use external knowledge with deep neural models for response selection and have implications for the design of the next generation of information-seeking conversation systems.  To understand the types of text responses and the answers that are used in conversational IR, we introduced a new dataset designed for research in conversational IR and used it to analyze information-seeking conversations. Our dataset and related datasets we have used for this research are based on open domain technical and problem-solving discussions.  Another contribution we have made to the research community is to introduce and formalize the task of predicting questions in conversations, where the goal is to predict the new question that the user will ask, given the past conversational context. We developed neural models to address this task and evaluated it using technical chat log data.  Given the importance of this area for the future development of search engines, we expect to see substantial future research based on the work done on this project.       Last Modified: 08/01/2018       Submitted by: W. Bruce Croft]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
