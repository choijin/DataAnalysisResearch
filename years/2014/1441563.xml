<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DIP: Developing Crosscutting Concepts in STEM with Simulation and Embodied Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1349504.00</AwardTotalIntnAmount>
<AwardAmount>1349504</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Amy Baylor</SignBlockName>
<PO_EMAI>abaylor@nsf.gov</PO_EMAI>
<PO_PHON>7032925126</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Cyberlearning and Future Learning Technologies Program funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Development and Implementation (DIP) Projects build on proof-of-concept work that showed the possibilities of the proposed new type of learning technology, and project teams build and refine a minimally-viable example of their proposed innovation that allows them to understand how such technology should be designed and used in the future and that allows them to answer questions about how people learn, how to foster or assess learning, and/or how to design for learning. This proposal uses advances in multimodal immersive interfaces, such as sensors that allow motion detection (like the Microsoft Kinect) to examine questions about how learners think with their bodies as they make sense of science concepts like 'scale' or 'rates of change'. The project will help create "simulation theatres for embodied learning," or rooms with immersive technology that allow students to interact with science simulations and simultaneously express ideas by moving their bodies. Research studies will examine whether gestures students use carry over from one science discipline to the next, and whether this type of interaction helps them transfer what they know in one science domain to others. At the end of the project, we should have 1. a technology platform that can be used to help research how students use gesture to understand science concepts, 2. information about how well this tool supports learning across disciplines, and 3. novel psychology research about how people think with their bodies.&lt;br/&gt;&lt;br/&gt;This project seeks to extend and refine emerging theories of embodied learning and embodied design. Embodied interactions have shown promise for increasing learning in specific STEM concepts, but there is less known about how body movement and gesture promote understanding abstract and crosscutting ideas that may facilitate learning transfer. This project examines explicitly whether persistent schemes of embodied interactions with computer simulations make it easier for learners to engage with, and learn from, new simulations of novel STEM topics. This project will also make intellectual advances in computational gesture recognition and processing, for instance through single-instance machine learning algorithms, real-time training, and modeling of paraterized gestures to capture full-body gestures to create a highly flexible gesture-learning environment that will enable training based on individual subjects without having to build a large database of gestures in order to achieve reliable recognition. By developing (1) an easy to use, low cost, and highly reconfigurable system for recognizing learning gestures and (2) an integrated set of learning simulations that rely on embodied interactions to investigate a broad range of STEM topics using consistent interface schemes, the project will be able to investigate how gestural congruency can be used to support learners' conceptions of STEM disciplines. Research studies will use 12-15 middle-school students in the initial phases to help identify candidate gestures for cross-disciplinary gestural metaphors. Three later iterations will use approximately 50 students per iteration to examine whether interacting with the system can engage embodied metaphors that support transfer of learning from the domain of a STEM simulation to other domains, including development of instruments for assessing transfer.</AbstractNarration>
<MinAmdLetterDate>08/06/2014</MinAmdLetterDate>
<MaxAmdLetterDate>11/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1441563</AwardID>
<Investigator>
<FirstName>Jose</FirstName>
<LastName>Mestre</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jose P Mestre</PI_FULL_NAME>
<EmailAddress>mestre@illinois.edu</EmailAddress>
<PI_PHON>2173330098</PI_PHON>
<NSF_ID>000091976</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Craig</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan B Craig</PI_FULL_NAME>
<EmailAddress>acraig@ncsa.uiuc.edu</EmailAddress>
<PI_PHON>2172441988</PI_PHON>
<NSF_ID>000296088</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate>06/22/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Guy</FirstName>
<LastName>Garnett</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guy Garnett</PI_FULL_NAME>
<EmailAddress>garnett@uiuc.edu</EmailAddress>
<PI_PHON>2178983396</PI_PHON>
<NSF_ID>000169569</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate>11/07/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Wai-Tat</FirstName>
<LastName>Fu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wai-Tat Fu</PI_FULL_NAME>
<EmailAddress>wfu@illinois.edu</EmailAddress>
<PI_PHON>2172448617</PI_PHON>
<NSF_ID>000243175</NSF_ID>
<StartDate>11/07/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robb</FirstName>
<LastName>Lindgren</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robb W Lindgren</PI_FULL_NAME>
<EmailAddress>robblind@illinois.edu</EmailAddress>
<PI_PHON>2175509470</PI_PHON>
<NSF_ID>000549185</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress><![CDATA[1901 S. First Street, Suite A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>7645</Code>
<Text>Discovery Research K-12</Text>
</ProgramElement>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8842</Code>
<Text>Design and Implementation Projects</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~1349504</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The aim of this project was to investigate the potential to use motion tracking sensors and machine learning methods to create full-body interactive STEM learning technologies. Specifically, the project examined whether big ideas in STEM such as "scale" or "rates of change" could be transferred from one science content area to another when learned through body movement such as gestures. In order to test these ideas, an educational simulation platform was created named ELASTIC<sup>3</sup>S (Embodied Learning Augmented through Simulation Theaters for Interacting with Cross-Cutting Concepts in Science). The ELASTIC<sup>3</sup>S platform is a semi-immersive learning environment comprised of three projection screens and a Microsoft Kinect sensor that can track the skeletal position of one or multiple users. Gesture recognition software was developed that could interpret the Kinect data and classify student gestures in "one shot", meaning that a student only had to perform a gesture once and the same gesture would be recognized again the next time the student performed it. This meant that a student could associate their own gesture for an operation such as "multiply a quantity times 10" and every time they performed that gesture it would execute that operation with a high degree of accuracy.</p> <p>&nbsp;</p> <p>Multiple "simulation theaters" were developed that use the custom gesture recognition system that was developed for this project. Each theater is comprised of multiple simulations built around a shared big idea so that students can use the same gestures for all the simulations in a theater. For example, the first simulation theater that was built is focused on the crosscutting concept of "scale, proportion, and magnitude" defined in the Next Generation Science Standards. Specifically, this theater looked at exponential growth and ways that different science domains used logarithmic scales. One of the simulations was a dynamic visualization of earthquakes and the Richter Scale. Another simulation was a dynamic visualization of acids/ bases and the pH scale. Students could use the same gesture to perform operations on both scales. For example, a gesture that is used to move from a magnitude 3.0 to a magnitude 4.0 earthquake can also be used to make a solution more basic, moving from a pH of 3 to pH of 4. The rationale was that allowing students to use their own gestures as a means of understanding critical ideas that cut across multiple science domains would make it easier for them to transfer their learning from one context to another. A second simulation theater was built around the crosscutting idea of "rates of change" and in this theater two students worked together, each controlling their own component of a simulation using their own gestures.</p> <p>&nbsp;</p> <p>The research findings from multiple studies that were conducted on the ELASTIC<sup>3</sup>S platform supported the conjecture that using gestures to interact with simulations would boost learning and transfer. Compared to learning these same concepts with traditional educational media (e.g., textbooks and videos) students who used the full-body simulations had greater knowledge gains and showed a higher tendency to transfer the crosscutting concepts from one science domain to another. The research also supported the idea that students could work in pairs and coordinate their gestures while using a simulation such that learning for both students improved.</p> <p>&nbsp;</p> <p>The intellectual merit of the ELASTIC<sup>3</sup>S project is a clear demonstration that "embodied learning" with new technologies has educational benefits not only for specific science topics but for crosscutting concepts and the ability to transfer learning. The work on this project also informs our general understanding of how gestures and other forms of body movement are related to how people learn, and suggests new approaches to helping people learn challenging STEM concepts. The broader impact of this project includes several design principles that emerged from the iterative work on the ELASTIC<sup>3</sup>S simulations that can potentially be applied to future learning technologies. It also includes tangible technology development such as a robust and adaptive gesture recognition system that can be used for other applications, and a set of tested science simulations on topics ranging from earthquakes to chemical equilibrium to population dynamics (predator-prey systems).</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2019<br>      Modified by: Robb&nbsp;W&nbsp;Lindgren</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091531306_FigureLab--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091531306_FigureLab--rgov-800width.jpg" title="Research working with an ELASTICS sim user"><img src="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091531306_FigureLab--rgov-66x44.jpg" alt="Research working with an ELASTICS sim user"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A researcher is helping a student use the ELASTICS earthquake simulation</div> <div class="imageCredit">The ELASTICS team</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robb&nbsp;W&nbsp;Lindgren</div> <div class="imageTitle">Research working with an ELASTICS sim user</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091643759_FigureEarthquakeSim--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091643759_FigureEarthquakeSim--rgov-800width.jpg" title="ELASTICS Earthquake Simulation"><img src="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091643759_FigureEarthquakeSim--rgov-66x44.jpg" alt="ELASTICS Earthquake Simulation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Several screens of the ELASTICS Earthquake simulation. A students attends to multiple screens simultaneously as they attempt to create an earthquake of a target magnitude.</div> <div class="imageCredit">The ELASTICS team</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robb&nbsp;W&nbsp;Lindgren</div> <div class="imageTitle">ELASTICS Earthquake Simulation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091792267_FigureAcidsBasesSim--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091792267_FigureAcidsBasesSim--rgov-800width.jpg" title="ELASTICS Acids/Bases Simulation"><img src="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091792267_FigureAcidsBasesSim--rgov-66x44.jpg" alt="ELASTICS Acids/Bases Simulation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Multiple screens from the ELASTICS Acids and Bases Simulation. Students are manipulating the pH of the water in a pond and can observe the effects on the organisms that live there.</div> <div class="imageCredit">The ELASTICS Team</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robb&nbsp;W&nbsp;Lindgren</div> <div class="imageTitle">ELASTICS Acids/Bases Simulation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091956737_FigureLabRates--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091956737_FigureLabRates--rgov-800width.jpg" title="Two students using ELASTICS Rates Sim"><img src="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575091956737_FigureLabRates--rgov-66x44.jpg" alt="Two students using ELASTICS Rates Sim"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Two students working together and using their own gestures to use the ELASTICS rates of changes sim. One student is controlling the rate of a food resource and another student is controlling the rate of predation in a population dynamics simulation.</div> <div class="imageCredit">The ELASTICS team</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robb&nbsp;W&nbsp;Lindgren</div> <div class="imageTitle">Two students using ELASTICS Rates Sim</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575092104200_FigureRatesofChangeSims--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575092104200_FigureRatesofChangeSims--rgov-800width.jpg" title="ELASTICS Rates of Change Sims"><img src="/por/images/Reports/POR/2019/1441563/1441563_10329610_1575092104200_FigureRatesofChangeSims--rgov-66x44.jpg" alt="ELASTICS Rates of Change Sims"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Two ELASTICS rates of change simulations (3 screens each). The top simulation is a simulation of climate change and factors that control C02 in the atmosphere. The bottom simulation is a simulation of population dynamics and predator/prey.</div> <div class="imageCredit">The ELASTICS Team</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robb&nbsp;W&nbsp;Lindgren</div> <div class="imageTitle">ELASTICS Rates of Change Sims</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The aim of this project was to investigate the potential to use motion tracking sensors and machine learning methods to create full-body interactive STEM learning technologies. Specifically, the project examined whether big ideas in STEM such as "scale" or "rates of change" could be transferred from one science content area to another when learned through body movement such as gestures. In order to test these ideas, an educational simulation platform was created named ELASTIC3S (Embodied Learning Augmented through Simulation Theaters for Interacting with Cross-Cutting Concepts in Science). The ELASTIC3S platform is a semi-immersive learning environment comprised of three projection screens and a Microsoft Kinect sensor that can track the skeletal position of one or multiple users. Gesture recognition software was developed that could interpret the Kinect data and classify student gestures in "one shot", meaning that a student only had to perform a gesture once and the same gesture would be recognized again the next time the student performed it. This meant that a student could associate their own gesture for an operation such as "multiply a quantity times 10" and every time they performed that gesture it would execute that operation with a high degree of accuracy.     Multiple "simulation theaters" were developed that use the custom gesture recognition system that was developed for this project. Each theater is comprised of multiple simulations built around a shared big idea so that students can use the same gestures for all the simulations in a theater. For example, the first simulation theater that was built is focused on the crosscutting concept of "scale, proportion, and magnitude" defined in the Next Generation Science Standards. Specifically, this theater looked at exponential growth and ways that different science domains used logarithmic scales. One of the simulations was a dynamic visualization of earthquakes and the Richter Scale. Another simulation was a dynamic visualization of acids/ bases and the pH scale. Students could use the same gesture to perform operations on both scales. For example, a gesture that is used to move from a magnitude 3.0 to a magnitude 4.0 earthquake can also be used to make a solution more basic, moving from a pH of 3 to pH of 4. The rationale was that allowing students to use their own gestures as a means of understanding critical ideas that cut across multiple science domains would make it easier for them to transfer their learning from one context to another. A second simulation theater was built around the crosscutting idea of "rates of change" and in this theater two students worked together, each controlling their own component of a simulation using their own gestures.     The research findings from multiple studies that were conducted on the ELASTIC3S platform supported the conjecture that using gestures to interact with simulations would boost learning and transfer. Compared to learning these same concepts with traditional educational media (e.g., textbooks and videos) students who used the full-body simulations had greater knowledge gains and showed a higher tendency to transfer the crosscutting concepts from one science domain to another. The research also supported the idea that students could work in pairs and coordinate their gestures while using a simulation such that learning for both students improved.     The intellectual merit of the ELASTIC3S project is a clear demonstration that "embodied learning" with new technologies has educational benefits not only for specific science topics but for crosscutting concepts and the ability to transfer learning. The work on this project also informs our general understanding of how gestures and other forms of body movement are related to how people learn, and suggests new approaches to helping people learn challenging STEM concepts. The broader impact of this project includes several design principles that emerged from the iterative work on the ELASTIC3S simulations that can potentially be applied to future learning technologies. It also includes tangible technology development such as a robust and adaptive gesture recognition system that can be used for other applications, and a set of tested science simulations on topics ranging from earthquakes to chemical equilibrium to population dynamics (predator-prey systems).          Last Modified: 11/30/2019       Submitted by: Robb W Lindgren]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
