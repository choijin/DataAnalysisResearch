<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: CCA:  Collaborative Research: SPARTA: a Stream-based Processor And Run-Time Architecture</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>271000.00</AwardTotalIntnAmount>
<AwardAmount>313371</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computer systems have undergone a fundamental transformation recently, from single&amp;#8208;core processors to devices with increasingly higher core counts within a single chip. The semi&amp;#8208;conductor industry now faces the infamous power and utilization walls, that is, physical constraints such as levels of power and energy consumption, but also reliability of the various components, must be taken into account not only during the chip fabrication process, but also when generating machine code and during program execution. To meet these challenges, heterogeneity in design, both at the architecture and technology levels, will be the prevailing approach for energy efficient computing as specialized cores, accelerators, and graphical processing units (GPUs) can eliminate the energy overheads of general&amp;#8208;purpose homogeneous cores. However, with future technological challenges pointing in the direction of on&amp;#8208;chip heterogeneity, and because of the traditional difficulty of parallel programming, it becomes imperative to produce new system software stacks that can take advantage of the heterogeneous hardware. &lt;br/&gt;&lt;br/&gt;This project proposes to rethink the whole hardware&amp;#8208;software interface, by researching novel ways to design many&amp;#8208;core chip architectures and weaving heterogeneous components together and binding them by a fast and energy efficient on&amp;#8208;chip interconnection network. On top of it will lay a system software layer to efficiently drive applications and map them onto the best suited components of the chip. Both the hardware and software layer are encompassed by a novel execution model, which describes how to orchestrate the various parts of a program in the most efficient way (be it with respect to power and energy, performance, or reliability). To achieve these goals, the development of a new model of computation called SPARTA (Stream-based Processor And RunTime Architecture) is proposed. The proposed model combines a new runtime and compiler technology with a hierarchical heterogeneous many&amp;#8208;core chip and features hardware mechanisms for stream&amp;#8208;based fine&amp;#8208;grain program execution models to be reflected in different new software/hardware systems. Many issues are be envisioned, including programmability, scalability, performance evaluation, and power efficiency. Specifically, the goal is to identify the major challenges and obstacles toward an efficient exploitation of parallelism and scalability. To do so, traditional approaches will be re-evaluated by studying a collection of representative programs. A vertical design methodology is then proposed to effectively address the above challenges through the SPARTA approach and its implementation. In particular, the proposed cross-layer methodology consists of (a) a programming/execution model that will combine the Codelet model (leveraging our past research in dataflow models and extensions) with generalized streams: the Streaming Codelets, (b) an architecture model that will efficiently support the Streaming Codelets in heterogeneous hardware, and (c) a system software Stack that will be capable of effectively mapping Streaming Codelets to the proposed architecture. Finally, a qualitative and quantitative study of SPARTA will be performed via selected benchmarks and a consolidated methodology based on experimentation and analysis. The holistic cross-layer design methodology spanning the hardware/software stack and the reliability techniques developed from this research will significantly impact next generation multi&amp;#8208;core and System&amp;#8208;on&amp;#8208;Chip (SoC) architectures with improvements in energy efficiency, programmability, performance and robustness.</AbstractNarration>
<MinAmdLetterDate>07/24/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439097</AwardID>
<Investigator>
<FirstName>Guang</FirstName>
<LastName>Gao</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guang R Gao</PI_FULL_NAME>
<EmailAddress>ggao@udel.edu</EmailAddress>
<PI_PHON>3028318218</PI_PHON>
<NSF_ID>000090087</NSF_ID>
<StartDate>07/24/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Delaware</Name>
<CityName>Newark</CityName>
<ZipCode>197160099</ZipCode>
<PhoneNumber>3028312136</PhoneNumber>
<StreetAddress>210 Hullihen Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<StateCode>DE</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DE00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>059007500</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF DELAWARE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>059007500</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Delaware]]></Name>
<CityName>Newark</CityName>
<StateCode>DE</StateCode>
<ZipCode>197163130</ZipCode>
<StreetAddress><![CDATA[200 Evans Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DE00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~271000</FUND_OBLG>
<FUND_OBLG>2017~42371</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>System software: Compiler and runtime system (Delaware):&nbsp;</strong>our&nbsp;omp2cd&nbsp;compiler gets results that are at least competitive with regular OpenMP implementations (such as GCC&rsquo;s): we have run both NWCHEM-SCF and Graph500 with OpenMP/GCC and OpenMP/Codelets. In the case of the SCF kernel, results show that we are on-par: speedups vary between 0.9x and 1.1x, depending on how many atoms we wish to simulate. We reach a speedup of 1.7x compared to the regular GCC/OpenMP version of Graph500. Two different Intel-based compute nodes were used to verify our results: a 4-socket Intel Sandy Bridge machine, and a 2-socket Intel Ivy Bridge one. The results follow the same trend.</p> <p><strong>Fine-Grain Synchronization on General Purpose Many-Core Compute Nodes (UDel and UCI):&nbsp;</strong>In the case of the 2D stencil kernel we implemented for DARTS and OpenMP, we reached up to 1.75x compared to the regular coarse-grain synchronization version we implemented in OpenMP. We tested our implementation on three different types of nodes: a 4-socket Intel Sandy Bridge compute node, a 2-socket Intel Sandy Bridge node, and a 4-socket AMD Bulldozer node. Our best performance improvements happened on the Intel platforms; for&nbsp;all&nbsp;variants, our fine- grain implementations outperform the original coarse-grain version (from 1.2x on AMD to 1.7x on Intel).</p> <p><strong>Hilbert-Inspired Tile Scheduling for Matrix Multiplication (Delaware):&nbsp;</strong>In the case of the Hilbert-inspired curves for tiled matrix multiplication on a manycore chip, our simulation results show a significant reduction in energy expenditure compared to a classical hierarchical tiling scheme.</p> <p><strong>Performance evaluation of multicore chips (UCI):&nbsp;</strong>We extended Amdahl&rsquo;s law by considering the overhead of Data Preparation (ODP) for multicore systems, and demonstrated that the overhead of data preparation had become an unavoidable key parameter. Our analysis clearly shows that the higher parallelism gained from either computation or data preparation brings greater energy-efficiency. Improving the performance-energy efficiency of data preparation is another promising approach to affect power consumption. Therefore, more informed tradeoffs should be taken when we design a modern heterogeneous processor system within limited budget of energy (power). In addition, models for efficient resource scheduling have been demonstrated.</p> <p><strong>Profile-based Dynamic Adaptive Workload Balance on Heterogeneous Architectures (UCI):</strong>&nbsp;we have extended our homogeneous DARTS to support heterogeneous hardware resources (CPUs, GPUs). We have also developped a workload balance scheduler which combines offline machine algorithm and online dynamic scheduling methodology. Experiments show our heterogeneous resources scheduler is either on-par or far outperforms whichever yields the best results (CPU or GPU), depending on the memory footprint required.</p> <p><strong>Key outcomes at Arizona/GWU:</strong></p> <p>CogNoC is able to quickly overcome the initial learning&rsquo;s large overhead and achieve better performance over time. On a large 16x16 mesh network with very high percentage of faulty links, the simulation results show that CogNoC, with a simple learning algorithm, is able to shorten 8% of all routes with an average reduction of 15% in hop counts on those paths, compared to the conventional design. Overall dynamic power is also reduced.</p> <p>OWN provided much higher scalability as it provides a much higher sturation throughput when compared to other networks including wired mettalic interconnects,&nbsp;and photonic interconnects.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/12/2019<br>      Modified by: Guang&nbsp;R&nbsp;Gao</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ System software: Compiler and runtime system (Delaware): our omp2cd compiler gets results that are at least competitive with regular OpenMP implementations (such as GCC’s): we have run both NWCHEM-SCF and Graph500 with OpenMP/GCC and OpenMP/Codelets. In the case of the SCF kernel, results show that we are on-par: speedups vary between 0.9x and 1.1x, depending on how many atoms we wish to simulate. We reach a speedup of 1.7x compared to the regular GCC/OpenMP version of Graph500. Two different Intel-based compute nodes were used to verify our results: a 4-socket Intel Sandy Bridge machine, and a 2-socket Intel Ivy Bridge one. The results follow the same trend.  Fine-Grain Synchronization on General Purpose Many-Core Compute Nodes (UDel and UCI): In the case of the 2D stencil kernel we implemented for DARTS and OpenMP, we reached up to 1.75x compared to the regular coarse-grain synchronization version we implemented in OpenMP. We tested our implementation on three different types of nodes: a 4-socket Intel Sandy Bridge compute node, a 2-socket Intel Sandy Bridge node, and a 4-socket AMD Bulldozer node. Our best performance improvements happened on the Intel platforms; for all variants, our fine- grain implementations outperform the original coarse-grain version (from 1.2x on AMD to 1.7x on Intel).  Hilbert-Inspired Tile Scheduling for Matrix Multiplication (Delaware): In the case of the Hilbert-inspired curves for tiled matrix multiplication on a manycore chip, our simulation results show a significant reduction in energy expenditure compared to a classical hierarchical tiling scheme.  Performance evaluation of multicore chips (UCI): We extended Amdahl’s law by considering the overhead of Data Preparation (ODP) for multicore systems, and demonstrated that the overhead of data preparation had become an unavoidable key parameter. Our analysis clearly shows that the higher parallelism gained from either computation or data preparation brings greater energy-efficiency. Improving the performance-energy efficiency of data preparation is another promising approach to affect power consumption. Therefore, more informed tradeoffs should be taken when we design a modern heterogeneous processor system within limited budget of energy (power). In addition, models for efficient resource scheduling have been demonstrated.  Profile-based Dynamic Adaptive Workload Balance on Heterogeneous Architectures (UCI): we have extended our homogeneous DARTS to support heterogeneous hardware resources (CPUs, GPUs). We have also developped a workload balance scheduler which combines offline machine algorithm and online dynamic scheduling methodology. Experiments show our heterogeneous resources scheduler is either on-par or far outperforms whichever yields the best results (CPU or GPU), depending on the memory footprint required.  Key outcomes at Arizona/GWU:  CogNoC is able to quickly overcome the initial learning’s large overhead and achieve better performance over time. On a large 16x16 mesh network with very high percentage of faulty links, the simulation results show that CogNoC, with a simple learning algorithm, is able to shorten 8% of all routes with an average reduction of 15% in hop counts on those paths, compared to the conventional design. Overall dynamic power is also reduced.  OWN provided much higher scalability as it provides a much higher sturation throughput when compared to other networks including wired mettalic interconnects, and photonic interconnects.             Last Modified: 12/12/2019       Submitted by: Guang R Gao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
