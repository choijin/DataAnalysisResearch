<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Bridging Efficiency and Low Latency in Warehouse-scale Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>466783.00</AwardTotalIntnAmount>
<AwardAmount>466783</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computing is now an essential tool and a catalyst for innovation for all aspects of human endeavor, including healthcare, education, science, commerce, government, and entertainment. An increasing amount of computing is performed on private and public clouds, primarily due to the cost and scalability benefits for both the end-users and operators of the warehouse-scale systems that host clouds.  We have come to expect that these systems provide millions of users with instantaneous, personalized, and contextual access to petabytes of data. The goal of this project is to improve the capabilities and efficiency of warehouse-scale systems. Specifically, we aim to reconcile the presumed incompatibility between low-latency processing at massive scales and efficiency in terms of energy consumption and resource usage. We aim to improve energy and resource efficiency in warehouse-scale systems by factors of 2x-5x while allowing for low-latency processing at massive scales. Equally important, we aim to improve our understanding of the tradeoffs between scalability, low latency, and energy or resource efficiency in modern computing systems.&lt;br/&gt;&lt;br/&gt;The project focuses on on-line, data-intensive workloads, such as search, social networking, real-time analytics, and machine learning analysis, that occupy thousands of servers in warehouse-scale systems and pose significant scaling challenges. Their strict latency constraints, large state requirements, and high communication fan-out makes it difficult to apply known techniques for power reduction and resource sharing across applications. Hence, they typically consume a significant percentage of peak power and use non-shared servers even during the frequent periods of medium or low user traffic. To a large extent, low latency and high efficiency are considered incompatible for these workloads. To bridge this gap, the project uses a cross-layer approach that monitors end-to-end workload performance and quality-of-service to guide system-wide power management and resource management. The first step is to develop a power management system that improves the energy proportionality of warehouse-scale systems during periods of low or medium load without compromising latency guarantees. The second step is to develop a system-wide resource management system that allows aggressive server sharing between latency-critical workloads and other workloads during periods of low or medium load without compromising latency guarantees. The third step is to design operating system policies for performance isolation between co-located workloads within a server. The final step is to use the insights from the previous steps to evaluate the efficacy of existing and proposed server architectures with respect to energy and resource efficiency for on-line, data-intensive workloads.</AbstractNarration>
<MinAmdLetterDate>08/04/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422088</AwardID>
<Investigator>
<FirstName>Christoforos</FirstName>
<LastName>Kozyrakis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christoforos Kozyrakis</PI_FULL_NAME>
<EmailAddress>kozyraki@stanford.edu</EmailAddress>
<PI_PHON>6507253716</PI_PHON>
<NSF_ID>000486618</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>943054100</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~466783</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Information technology (IT) is now a major catalyst for innovation across all aspects of human endeavor. An increasing amount of IT is hosted in the cloud, primarily due to the cost and scalability benefits for both the end-users and the operators of the warehouse-scale datacenters (DC) that host cloud services. Hence, it is vital to continuously improve the capabilities and efficiency of datacenters. In the past ten years, capability has improved by increasing the number servers in a DC and the bandwidth of the network that connects them. Cost and energy efficiency improved by eliminating the high overheads of the power delivery and cooling infrastructure. To achieve further improvements, we must now examine how well we are utilizing the servers themselves, which are the primary determinant for DC performance, cost, and energy efficiency. This is particularly important since the semiconductor chips used in servers are now energy limited and their efficiency does not scale as fast as in the past.</p> <p>The goal of this proposal is to increase DC resource efficiency, hence improving both capability and cost effectiveness. Sharing compute servers, memory, and storage devices across mul- tiple services is the prevailing approach for resource efficient DCs. However, sharing resources introduces interference, which can be detrimental to performance especially for latency-critical services with strict quality-of-service guarantees (QoS). The difficulty of identifying and reducing interference keeps some DC operators from sharing resources and discourages others from aggressive sharing in the presence of latency- critical services. Consequently, DC server utilization is typically at 10%-20% or even lower.<br />This project followed a cross-layer approach towards improving resource efficiency in datacenters that host latency- critical services. The specific outcomes of this project were the following:</p> <p><br />- A feedback-based controller (Heracles) that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks.&nbsp;</p> <p><br />- The set of OS mechanisms and dynamic control needed to adjust core allocation and voltage/frequency settings based on the measured delays for latency-critical workloads. This allows for energy proportionality and frees the maximum amount of resources per server for other background applications, while respecting service-level objectives. Monitoring hardware queue depths allows us to detect increases in queuing latencies.</p> <p><br />- A distributed scheduler (Trail) that targets both scheduling speed and quality. Tarcil uses an analyticallyderived sampling framework that adjusts the sample size based on load, and provides statistical guarantees on thequality of allocated resources. It also implements admission control when sampling is unlikely to find suitable resources. This makes it appropriate for large, shared clusters hosting short- and long-running jobs.</p> <p><br />- In in-depth examination of Flash disaggregation as a way to deal with Flash over-provisioning. We tune remote access to Flash over commodity networks and analyze its impact on workloads sampled from real datacenter applications. We show that, while remote Flash access introduces a 20% throughput drop at the application level, disaggregation allows us to make up for these overheads through resource-efficient scale-out. Hence, we show that Flash disaggregation allows scaling CPU and Flash resources independently in a cost effective manner.&nbsp;</p> <p><br />- A software-based system for remote Flash access (ReFlex), that provides nearly identical performance to accessing local Flash. ReFlex uses a dataplane kernel to closely integrate networking and storage processing to achieve low latency and high throughput at low resource requirements. Specifically, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, while adding 21&micro;s over direct access to local Flash. ReFlex uses a QoS scheduler that can enforce tail latency and throughput service-level objectives (SLOs) for thousands of remote clients.&nbsp;</p> <p><br />- An in depth exploration of the suitability of different cloud storage services (e.g., object stores and distributed caches) as remote storage for serverless analytics. Our analysis leads to key insights to guide the design of an ephemeral cloud storage system, including the performance and cost efficiency of Flash storage for serverless application requirements and the need for a pay what-you-use storage service that can support the highthroughput demands of highly parallel applications.</p> <p><br />- A tool (Selecta) that recommends near0optimal configurations of cloud compute and storage resources for data analytics workloads. Selecta uses latent factor collaborative filtering to predict how an application will perform across different configurations, based on sparse data collected by profiling training workloads. We evaluate Selecta with over one hundred Spark SQL and ML applications, showing that Selecta chooses a near-optimal performance configuration (within 10% of optimal) with 94% probability and a near-optimal cost configuration with 80% probability.&nbsp;</p> <p><br />We have open-sourced the key systems we developed in this project in order to help the broader community understand the basic tradeoffs between efficiency and low latency and develop further techniques for cross-layer energy optimization.</p><br> <p>            Last Modified: 08/14/2018<br>      Modified by: Christoforos&nbsp;Kozyrakis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Information technology (IT) is now a major catalyst for innovation across all aspects of human endeavor. An increasing amount of IT is hosted in the cloud, primarily due to the cost and scalability benefits for both the end-users and the operators of the warehouse-scale datacenters (DC) that host cloud services. Hence, it is vital to continuously improve the capabilities and efficiency of datacenters. In the past ten years, capability has improved by increasing the number servers in a DC and the bandwidth of the network that connects them. Cost and energy efficiency improved by eliminating the high overheads of the power delivery and cooling infrastructure. To achieve further improvements, we must now examine how well we are utilizing the servers themselves, which are the primary determinant for DC performance, cost, and energy efficiency. This is particularly important since the semiconductor chips used in servers are now energy limited and their efficiency does not scale as fast as in the past.  The goal of this proposal is to increase DC resource efficiency, hence improving both capability and cost effectiveness. Sharing compute servers, memory, and storage devices across mul- tiple services is the prevailing approach for resource efficient DCs. However, sharing resources introduces interference, which can be detrimental to performance especially for latency-critical services with strict quality-of-service guarantees (QoS). The difficulty of identifying and reducing interference keeps some DC operators from sharing resources and discourages others from aggressive sharing in the presence of latency- critical services. Consequently, DC server utilization is typically at 10%-20% or even lower. This project followed a cross-layer approach towards improving resource efficiency in datacenters that host latency- critical services. The specific outcomes of this project were the following:   - A feedback-based controller (Heracles) that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks.    - The set of OS mechanisms and dynamic control needed to adjust core allocation and voltage/frequency settings based on the measured delays for latency-critical workloads. This allows for energy proportionality and frees the maximum amount of resources per server for other background applications, while respecting service-level objectives. Monitoring hardware queue depths allows us to detect increases in queuing latencies.   - A distributed scheduler (Trail) that targets both scheduling speed and quality. Tarcil uses an analyticallyderived sampling framework that adjusts the sample size based on load, and provides statistical guarantees on thequality of allocated resources. It also implements admission control when sampling is unlikely to find suitable resources. This makes it appropriate for large, shared clusters hosting short- and long-running jobs.   - In in-depth examination of Flash disaggregation as a way to deal with Flash over-provisioning. We tune remote access to Flash over commodity networks and analyze its impact on workloads sampled from real datacenter applications. We show that, while remote Flash access introduces a 20% throughput drop at the application level, disaggregation allows us to make up for these overheads through resource-efficient scale-out. Hence, we show that Flash disaggregation allows scaling CPU and Flash resources independently in a cost effective manner.    - A software-based system for remote Flash access (ReFlex), that provides nearly identical performance to accessing local Flash. ReFlex uses a dataplane kernel to closely integrate networking and storage processing to achieve low latency and high throughput at low resource requirements. Specifically, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, while adding 21&micro;s over direct access to local Flash. ReFlex uses a QoS scheduler that can enforce tail latency and throughput service-level objectives (SLOs) for thousands of remote clients.    - An in depth exploration of the suitability of different cloud storage services (e.g., object stores and distributed caches) as remote storage for serverless analytics. Our analysis leads to key insights to guide the design of an ephemeral cloud storage system, including the performance and cost efficiency of Flash storage for serverless application requirements and the need for a pay what-you-use storage service that can support the highthroughput demands of highly parallel applications.   - A tool (Selecta) that recommends near0optimal configurations of cloud compute and storage resources for data analytics workloads. Selecta uses latent factor collaborative filtering to predict how an application will perform across different configurations, based on sparse data collected by profiling training workloads. We evaluate Selecta with over one hundred Spark SQL and ML applications, showing that Selecta chooses a near-optimal performance configuration (within 10% of optimal) with 94% probability and a near-optimal cost configuration with 80% probability.    We have open-sourced the key systems we developed in this project in order to help the broader community understand the basic tradeoffs between efficiency and low latency and develop further techniques for cross-layer energy optimization.       Last Modified: 08/14/2018       Submitted by: Christoforos Kozyrakis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
