<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Using Prediction to Build a Compact Visual Memex Memory for Rapid Analysis and Understanding of Egocentric Video Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project develops new data-driven techniques for egocentric (first-person) video stream analysis that exploit the structure and redundancy in streams captured over days, months, and even years, to significantly reduce the size of these datasets without losing the most useful visual information. Simultaneously, the research team is developing parallel programming frameworks that simplify expression and acceleration of these video analysis algorithms at scale.  While the focus of this research is the design of core algorithms and systems, success stands to enable the development of new classes of applications (in domains such as navigation, personal assistance, health/behavior monitoring) that use the extensive visual history of a camera to intelligently interpret continuous visual data sources and immediately respond to the observed input.  A further output of this research is the collection and organization of a large egocentric video database from the life of a single individual.&lt;br/&gt;&lt;br/&gt;The core idea of this research is to identify and exploit redundancy in everyday life. While it is not tractable to maintain an easily analyzable representation of all video ever seen by a camera, it is likely possible to identify and provide future applications fast access to the most important visual information. The challenge is to determine what visual data is the most important.  This work explores the use of video stream predictability as a notion of importance. Specifically, the vast visual history of the camera (e.g., life experiences captured by a head-mounted camera) is used to make predictions about what the camera will see next, and the accuracy of these predictions dictates what data is retained. (Highly predictable occurrences are judged to be less valuable to retain in the database.) In addition, this research is characterizing the structure of always-on egocentric video streams (What is the "working set" of a person's day? How much novel information is collected from day to day?), leveraging this structure to inform the design of new algorithms for video corpus analysis (data compression, accelerated retrieval), and exploring the design of specialized programming abstractions for authoring visual data understanding applications at scale.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;URL: http://graphics.cs.cmu.edu/projects/egocentricPrediction</AbstractNarration>
<MinAmdLetterDate>08/25/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/17/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422767</AwardID>
<Investigator>
<FirstName>Kayvon</FirstName>
<LastName>Fatahalian</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kayvon Fatahalian</PI_FULL_NAME>
<EmailAddress>kayvonf@cs.stanford.edu</EmailAddress>
<PI_PHON>6504976043</PI_PHON>
<NSF_ID>000624440</NSF_ID>
<StartDate>08/25/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~368696</FUND_OBLG>
<FUND_OBLG>2016~81304</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Helvetica; min-height: 16.0px} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Helvetica} --> <p class="p1">The project was based on the idea that large-scale continuous capture of large video datasets presented unique opportunities to develop new video understanding algorithms and to better understand the systems requirements of applications that process always-on video.&nbsp; Our work on the project was organized into three main themes:</p> <p class="p2">(1) Long-Running Video Dataset Capture</p> <p class="p2">The project results in the creation of two large video datasets for the computer vision community.<span>&nbsp; </span>These datasets differ from most current video datasets in that they focused on a small number of long-running video streams, rather than containing a small number of short video clips.<span>&nbsp; </span>The first dataset is the KrishnaCam dataset, which is a 70-hour outdoor egocentric dataset spanning the life of a single computer vision graduate student (He literally dedicated his life to science!). This dataset captures a diverse range of outdoor scenes in the greater Pittsburgh area, ranging from urban cityscapes to city parks, and spans multiple seasons and times of day.<span>&nbsp; </span>The second is called the Long Video Streams Dataset (LVS), a dataset of 30-minute clips from 30 HD video streams.<span>&nbsp; </span>LVS consists of streams featuring a diverse array of challenges: from fixed-viewpoint cameras, to constantly moving and zooming television cameras, to hand-held and egocentric video.<span>&nbsp;</span></p> <p class="p2">(2) New Algorithms for Increasing the Efficiency of Image/Video Understanding<span>&nbsp;</span></p> <p class="p2">We developed a number of techniques that aimed to improve the efficiency of DNN inference on images and video (accurate per unit cost).<span>&nbsp; </span>The first was a DNN-design methodology that we called HydraNets, which enables state-of-the-art architectures for image classification to be transformed into dynamic architectures which exploit conditional execution for efficient inference. HydraNets are wide networks containing distinct components each specialized to compute features for visually similar classes, but they retain efficiency by dynamically selecting only a small number of components to evaluate for any one input image.&nbsp; In other words, the main idea of HydraNets is that a large amount of DNN capacity is necessary to accurately classify all images a network might encounter, only a small fraction of that capacity is necessary to correctly classify any one image. <span>&nbsp;</span></p> <p class="p2">Building upon the idea of dynamic DNN execution, we developed the approach of online model distillation for training efficient models that are specialized for the content of specific video streams.<span>&nbsp; </span>The central motivation for this work was that most video cameras observe a very small fraction of the visual world. A fraction that, in many cases, evolves over time.<span>&nbsp; </span>For example, stationary cameras observe scenes that evolve with time of day or changing weather conditions, TV cameras pan and zoom, smartphone videos are hand-held, and egocentric cameras on vehicles or robots move through dynamic scenes. <span>&nbsp;</span></p> <p class="p2">Therefore, we moved away from pre-training specialized models using camera-specific datasets curated in advance, and instead train models online on a live video stream as new video frames arrive.<span>&nbsp; </span>Specifically, we employed the well-known technique of model distillation, training a lightweight "student" model to output the predictions of a larger, reliable high-capacity "teacher'', but did so in an online fashion, intermittently running the teacher on a live stream to provide a target for student learning. We found that simple DNN models can be accurate, provided they are continuously adapted to the specific contents of a video stream as new frames arrive. (i.e. models can learn to cheat---segmenting people sitting on a park lawn might be as easy as looking for shades of green!)<span>&nbsp; </span>Specifically, we applied this methodology to the task of realizing high-accuracy and low-cost semantic segmentation models that continuously adapt to the contents of a video stream.<span>&nbsp; </span>We were able to create models that were 20-30X more efficient than state-of-the-art DNN architectures without much loss in accuracy on the target video stream.<span>&nbsp; </span>Although our initial efforts focused on segmentation efficiency, we believe there are many future opportunities where continuous online training will provide valuable for improving inference efficiency or even accuracy.</p> <p class="p2">(3) Systems Infrastructure for Analysis Large Video Datasets</p> <p class="p1">&nbsp;Of course, analyzing large video datasets requires significant compute and storage costs, as well as significant parallel computing knowledge to develop applications that scale to large video datasets or large machines.&nbsp; To make it both more productive, and more efficient, to develop large-scale video processing applications, we developed Scanner, a platform for video processing at cloud scale.&nbsp; Scanner was developed in part in this project, as well as in IIS-1539069, and is now an open source project available to the public at (http://scanner.run).&nbsp; We are aware of use of Scanner at a number of academic institutions, as well as in industry.</p> <p class="p2">In addition to these three research thrusts, early work on this project catalyzed broader interest in system-support for video processing at the PI's home institution, shaping technical direction for new research programs such as the CMU-hosted Intel Science and Technology Center for Visual Cloud Computing.</p> <p class="p1">&nbsp;</p><br> <p>            Last Modified: 05/13/2019<br>      Modified by: Kayvon&nbsp;Fatahalian</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project was based on the idea that large-scale continuous capture of large video datasets presented unique opportunities to develop new video understanding algorithms and to better understand the systems requirements of applications that process always-on video.  Our work on the project was organized into three main themes: (1) Long-Running Video Dataset Capture The project results in the creation of two large video datasets for the computer vision community.  These datasets differ from most current video datasets in that they focused on a small number of long-running video streams, rather than containing a small number of short video clips.  The first dataset is the KrishnaCam dataset, which is a 70-hour outdoor egocentric dataset spanning the life of a single computer vision graduate student (He literally dedicated his life to science!). This dataset captures a diverse range of outdoor scenes in the greater Pittsburgh area, ranging from urban cityscapes to city parks, and spans multiple seasons and times of day.  The second is called the Long Video Streams Dataset (LVS), a dataset of 30-minute clips from 30 HD video streams.  LVS consists of streams featuring a diverse array of challenges: from fixed-viewpoint cameras, to constantly moving and zooming television cameras, to hand-held and egocentric video.  (2) New Algorithms for Increasing the Efficiency of Image/Video Understanding  We developed a number of techniques that aimed to improve the efficiency of DNN inference on images and video (accurate per unit cost).  The first was a DNN-design methodology that we called HydraNets, which enables state-of-the-art architectures for image classification to be transformed into dynamic architectures which exploit conditional execution for efficient inference. HydraNets are wide networks containing distinct components each specialized to compute features for visually similar classes, but they retain efficiency by dynamically selecting only a small number of components to evaluate for any one input image.  In other words, the main idea of HydraNets is that a large amount of DNN capacity is necessary to accurately classify all images a network might encounter, only a small fraction of that capacity is necessary to correctly classify any one image.   Building upon the idea of dynamic DNN execution, we developed the approach of online model distillation for training efficient models that are specialized for the content of specific video streams.  The central motivation for this work was that most video cameras observe a very small fraction of the visual world. A fraction that, in many cases, evolves over time.  For example, stationary cameras observe scenes that evolve with time of day or changing weather conditions, TV cameras pan and zoom, smartphone videos are hand-held, and egocentric cameras on vehicles or robots move through dynamic scenes.   Therefore, we moved away from pre-training specialized models using camera-specific datasets curated in advance, and instead train models online on a live video stream as new video frames arrive.  Specifically, we employed the well-known technique of model distillation, training a lightweight "student" model to output the predictions of a larger, reliable high-capacity "teacher'', but did so in an online fashion, intermittently running the teacher on a live stream to provide a target for student learning. We found that simple DNN models can be accurate, provided they are continuously adapted to the specific contents of a video stream as new frames arrive. (i.e. models can learn to cheat---segmenting people sitting on a park lawn might be as easy as looking for shades of green!)  Specifically, we applied this methodology to the task of realizing high-accuracy and low-cost semantic segmentation models that continuously adapt to the contents of a video stream.  We were able to create models that were 20-30X more efficient than state-of-the-art DNN architectures without much loss in accuracy on the target video stream.  Although our initial efforts focused on segmentation efficiency, we believe there are many future opportunities where continuous online training will provide valuable for improving inference efficiency or even accuracy. (3) Systems Infrastructure for Analysis Large Video Datasets  Of course, analyzing large video datasets requires significant compute and storage costs, as well as significant parallel computing knowledge to develop applications that scale to large video datasets or large machines.  To make it both more productive, and more efficient, to develop large-scale video processing applications, we developed Scanner, a platform for video processing at cloud scale.  Scanner was developed in part in this project, as well as in IIS-1539069, and is now an open source project available to the public at (http://scanner.run).  We are aware of use of Scanner at a number of academic institutions, as well as in industry. In addition to these three research thrusts, early work on this project catalyzed broader interest in system-support for video processing at the PI's home institution, shaping technical direction for new research programs such as the CMU-hosted Intel Science and Technology Center for Visual Cloud Computing.         Last Modified: 05/13/2019       Submitted by: Kayvon Fatahalian]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
