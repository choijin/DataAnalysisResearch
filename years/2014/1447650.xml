<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: F: DKM: Collaborative Research: PXFS: ParalleX Based Transformative I/O System for Big Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recent decades have seen the development of computational science where modeling and data analysis are critical to exploration, discovery, and refinement of new innovations in science and engineering. More recently the techniques have been applied to arts, social, political and other fields less traditionally reliant on high performance computing. This innovation has grown out of realization some 20 years ago that I/O (input/output) support for high performance parallel and distributed architectures had lagged behind that of pure computational speed, and further that bring I/O up to speed was both critical, and a rather difficult problem. The core hurdle of contemporary I/O on large HPC machines relates to issues of latency in large parts caused by the deficiencies of the historical I/O model that was relevant when computers were exclusively large, centralized, single processor systems shared by many time-sharing programs. In order to improve I/O on scalability on future hardware architectures novel approaches are required.&lt;br/&gt;&lt;br/&gt;This project is conducting research on an extension of ParalleX, a new highly innovative parallel execution model. The extension provides a powerful I/O interface that allows researchers to create highly efficient data management, discovery, and analysis codes for Big Data applications.  This new extension, known as PXFS, is based on HPX, an implementation of ParalleX based on C++, and OrangeFS, a high performance parallel file system.  The research goal driving PXFS is to extend HPX objects into I/O space so that the objects become persistent and storage becomes another class of memory, all accessed as a single virtual address space and managed by an event driven dynamic adaptive computation environment.  Critical aspects of this approach include futures-based synchronization, dynamic locality management, dynamic resource management, hierarchical name space, and an active global address space (AGAS).  The overall goals of PXFS are to eliminate the division of programming imposed by conventional file system through the unification of name spaces and their management, and to minimize global synchronization in order to support asynchronous concurrency.  The research methodology is to implement a Map/Reduce application framework using PXFS and evaluate its effectiveness in both performance and ease of use.&lt;br/&gt;&lt;br/&gt;This project is conducted at three major research universities involving undergraduate and graduate students, post-docs, and high-school teachers and their students.  The project includes a PI from the functional genomics field acting as domain science expert in order to focus the development efforts on real world problems. Graduate students and post-docs involved in the project are trained in these areas to promote scientists who understanding both aspects of Big Data problems.  The project engages under represented minorities with the goal to inspire them to pursue a career in computer science or genomics. The software developed by the project is available open-source and archived using an integrated source code revision repository, wiki, and bug tracking software system in addition to code releases with accompanying documentation.</AbstractNarration>
<MinAmdLetterDate>08/22/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/22/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1447650</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Sterling</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Sterling</PI_FULL_NAME>
<EmailAddress>tron@indiana.edu</EmailAddress>
<PI_PHON>8128564597</PI_PHON>
<NSF_ID>000119080</NSF_ID>
<StartDate>08/22/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Maciej</FirstName>
<LastName>Brodowicz</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maciej Brodowicz</PI_FULL_NAME>
<EmailAddress>simultac@gmail.com</EmailAddress>
<PI_PHON>8122878110</PI_PHON>
<NSF_ID>000492001</NSF_ID>
<StartDate>08/22/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Center for Research in Extreme Scale Technologies]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474082671</ZipCode>
<StreetAddress><![CDATA[2719 E. 10th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p { margin-bottom: 0.1in; border: medium none; padding: 0in; line-height: 120%; text-align: left; page-break-inside: auto; page-break-after: auto; }a:link {  } --> <p style="margin-bottom: 0in; line-height: 115%;">Computational genomics has many applications from cancer research to evolutionary biology and from genetics diseases to targeted drug delivery. The field is growing rapidly due to the advancement machinery which has revolutionized genomics. A high-end sequencer today in a single run can sequence as many as a trillion nucleotides of DNA producing hundreds of gigabytes of data. These new technologies open up a world of new experiments that were not feasible before due to time and cost involved. However, these experiments are being slowed down by the time it takes to process and interpret the vast amount of data produced. In this project, our team defines the challenges facing a particular suite of genomic software and has laid the groundwork for a solution which can portably address the challenges posed by vast amounts of I/O.</p> <p style="margin-bottom: 0in; line-height: 115%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 115%;">The framework we have designed is made of three main components which together have promise to change the way domain scientist perform I/O. First we have a parallel file system, OrangeFS. This library provides a server process that includes networking modules, storage modules, object management routines, file system routines, management features, and security components. In this project, we have worked to re-implement OrangeFS to have a completely distributed design that would allow only relevant components to be activated on storage and management nodes when needed.  Not only is this approach leaner, it would allow us to manage FS resources in a way that has never been done before, resulting is less overhead and better performance. Under this project, we were able to prototype at least some of the client side elements of this design.</p> <p style="margin-bottom: 0in; line-height: 115%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 115%;">The second component which we developed was the runtime system, HPX. HPX provided the project with a global address system, a platform to gather and respond to system information, and a distributed execution engine. Combined with OrangeFS, the team was able to open and manage distributed files as well as work with local files on the machine. This ability to use different I/O backends is critical to the flexibility and portability of our solution.</p> <p style="margin-bottom: 0in; line-height: 115%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 115%;">The final component is the parallel workflow manager. This piece of software launches the individual applications which make up the genomics workflow and is implemented on top of HPX. The workflow manager keeps track of which applications need to be launch and schedules them when appropriate. This tool allows us to launch each application in the workflow with an &ldquo;interposition library&rdquo; which redirects I/O calls to our runtime system. In this way, the applications are unaware of our runtime environment. This allows domain scientists to make changes to their code without needing to consider the PXFS technology underneath. The behaviour of the manager can be influenced, however, through the use of policies. In the future we hope to develop different policies to fine-tune actions such as data distribution, pre-fetching, and migration.</p> <p style="margin-bottom: 0in; line-height: 115%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 115%;">Taken together, these components allow scientists to have a data centric view of their applications. Large files can be broken up and run concurrently, and once loaded into memory these virtual files do not have to be continuously read and written to disk for every step in the workflow. In the future, the team hopes to continue to improve the implementation and extend runtime adaptive techniques available to the project. This could enable more sophisticated policies which could save unused parts of a file to disk to free up memory and then return them to memory just before the application needs the data.</p> <p style="margin-bottom: 0in; line-height: 115%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 115%;">The broader impact of this project lies in the pioneering attempt to address the challenges of parallel runtime and storage system of the next Exascale computing decade. We have provided insights and directions on how to target the challenges of efficient I/O in the context of future Exascale systems. These conclusions will help scientists efficiently process an unprecedented amount of data in a science computation and influence the next generation storage system for massively parallel runtime systems.</p> <p style="margin-bottom: 0in; line-height: 115%;">&nbsp;</p><br> <p>            Last Modified: 11/30/2017<br>      Modified by: Maciej&nbsp;Brodowicz</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Computational genomics has many applications from cancer research to evolutionary biology and from genetics diseases to targeted drug delivery. The field is growing rapidly due to the advancement machinery which has revolutionized genomics. A high-end sequencer today in a single run can sequence as many as a trillion nucleotides of DNA producing hundreds of gigabytes of data. These new technologies open up a world of new experiments that were not feasible before due to time and cost involved. However, these experiments are being slowed down by the time it takes to process and interpret the vast amount of data produced. In this project, our team defines the challenges facing a particular suite of genomic software and has laid the groundwork for a solution which can portably address the challenges posed by vast amounts of I/O.   The framework we have designed is made of three main components which together have promise to change the way domain scientist perform I/O. First we have a parallel file system, OrangeFS. This library provides a server process that includes networking modules, storage modules, object management routines, file system routines, management features, and security components. In this project, we have worked to re-implement OrangeFS to have a completely distributed design that would allow only relevant components to be activated on storage and management nodes when needed.  Not only is this approach leaner, it would allow us to manage FS resources in a way that has never been done before, resulting is less overhead and better performance. Under this project, we were able to prototype at least some of the client side elements of this design.   The second component which we developed was the runtime system, HPX. HPX provided the project with a global address system, a platform to gather and respond to system information, and a distributed execution engine. Combined with OrangeFS, the team was able to open and manage distributed files as well as work with local files on the machine. This ability to use different I/O backends is critical to the flexibility and portability of our solution.   The final component is the parallel workflow manager. This piece of software launches the individual applications which make up the genomics workflow and is implemented on top of HPX. The workflow manager keeps track of which applications need to be launch and schedules them when appropriate. This tool allows us to launch each application in the workflow with an "interposition library" which redirects I/O calls to our runtime system. In this way, the applications are unaware of our runtime environment. This allows domain scientists to make changes to their code without needing to consider the PXFS technology underneath. The behaviour of the manager can be influenced, however, through the use of policies. In the future we hope to develop different policies to fine-tune actions such as data distribution, pre-fetching, and migration.   Taken together, these components allow scientists to have a data centric view of their applications. Large files can be broken up and run concurrently, and once loaded into memory these virtual files do not have to be continuously read and written to disk for every step in the workflow. In the future, the team hopes to continue to improve the implementation and extend runtime adaptive techniques available to the project. This could enable more sophisticated policies which could save unused parts of a file to disk to free up memory and then return them to memory just before the application needs the data.   The broader impact of this project lies in the pioneering attempt to address the challenges of parallel runtime and storage system of the next Exascale computing decade. We have provided insights and directions on how to target the challenges of efficient I/O in the context of future Exascale systems. These conclusions will help scientists efficiently process an unprecedented amount of data in a science computation and influence the next generation storage system for massively parallel runtime systems.         Last Modified: 11/30/2017       Submitted by: Maciej Brodowicz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
