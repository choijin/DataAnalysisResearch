<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:  Scalable Algorithms for Extreme Computing on Heterogeneous Hardware, with Applications in Fluids and Biology</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>02/28/2018</AwardExpirationDate>
<AwardTotalIntnAmount>405396.00</AwardTotalIntnAmount>
<AwardAmount>405396</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alan Sussman</SignBlockName>
<PO_EMAI>alasussm@nsf.gov</PO_EMAI>
<PO_PHON>7032927563</PO_PHON>
</ProgramOfficer>
<AbstractNarration>There is currently a world-wide quest to achieve exascale computing by the end of the decade, with vigorous efforts in the US as well as China and Japan, in particular. In the US, the President's Strategy for American Innovation (2009) explicitly lists among its goals to dramatically increase our simulations capacity via an exascale computer. The challenges to achieve this goal are unprecedented: power constraints, microchip fabrication reaching physical limits, the growing imbalance between compute capacity and interconnect bandwidth, and the ever increasing number of cores in a system.&lt;br/&gt;&lt;br/&gt;Among the matters of highest priority are development of scalable algorithms that can exploit the enormous parallelism of new systems, and educating the next generation of computational  scientists. The first of these is at the center of the scientific part of this CAREER project. A potentially transformative combination is emerging where a class of hierarchical algorithms, offering ideal scaling linear with problem size, maps with excellent performance to many-core hardware (such as GPUs). Algorithmic improvements will be undertaken, such as combining elements of treecodes and fast multipole methods, communication and synchronization avoidance, dynamic error control and auto-tuning of the computation. The research program is vertically integrated across disciplines, including applications at extreme scales in fluid dynamics and biological systems.&lt;br/&gt;&lt;br/&gt;This project will produce highly scalable scientific software, reformulating the algorithms to achieve maximum performance in many-core hardware. Disseminated and curated via the open-source model, the computational infrastructure delivered will offer maximum impact, beyond the application areas of focus. Community software that is able to scale to millions of processors will be crucial to exploit post-petascale systems, and this project aims to provide that. The educational part of this program, on the other hand, builds on the PI's track record of success both in the use of technology to support learning, and in catalyzing international collaboration and outreach. The program includes enhancing educational environments using technology for both curricular instruction and contributing to the nation's science literacy (via open courseware). The goals of fostering the next generation of computational scientists will be pursued via extra-mural advanced training events, and online learning media.</AbstractNarration>
<MinAmdLetterDate>09/09/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1460035</AwardID>
<Investigator>
<FirstName>Lorena</FirstName>
<LastName>Barba</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lorena A Barba</PI_FULL_NAME>
<EmailAddress>labarba@gwu.edu</EmailAddress>
<PI_PHON>2029943715</PI_PHON>
<NSF_ID>000517032</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043990498</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName/>
<StateCode>DC</StateCode>
<ZipCode>200521000</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1045</Code>
<Text>CAREER: FACULTY EARLY CAR DEV</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~405396</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As the 21st century roared in, the heavy duty use of computers in science faced a turning point. First came multi-core processors, then graphics processing units (GPUs), and later other types of accelerators. Programming scientific applications became increasingly difficult, at a fast pace, as computer hardware turned predominantly heterogeneous. At the start of this project, three of the Top500 supercomputers in the world were powered by GPUs; in the 2019 list, <a href="https://blogs.nvidia.com/blog/2019/11/19/record-gpu-accelerated-supercomputers-top500/" target="_blank">136 systems are GPU-accelerated</a>. This project developed high-performance scientific software combining heterogenous computing with GPUs, and fast hierarchical algorithms. The algorithmic core technology is the fast multipole method, a mathematical advance highlighted as one of the <a href="https://doi.ieeecomputersociety.org/10.1109/MCISE.2000.814652" target="_blank">top-10 algorithms of the 20th century</a>. We amassed more than a decade of experience with this algorithm, and have refined and optimized its implementation in software. Our main software product, exaFMM, is an open-source library written in C++ with a Python interface, the result of years of accumulated knowledge and the work of several trainees. We also developed PyGBe, a software library using multipole methods within a solver for protein electrostatics. It allowed us to study the impact on computing binding energies of including solvent cavities in detailed models of proteins. We later extended PyGBe to treat the interaction of proteins with engineered surfaces of prescribed charge or electric potential. This set-up models a type of biosensors where a sensor element is represented as a charged surface, interacting with a biomolecule through electrostatics. Since performance of such biosensors is strongly dependent on the orientation of the molecules near the bioactive surface, we studied the preferred orientation of certain proteins under various conditions of surface charge and ambient salt concentration. In yet another extension of PyGBe, we added the capacity of simulating an optical effect, called localized surface plasmon resonance, via an electrostatics approximation. This new functionality allowed us to study sensitivity of a certain class of nano sensors. Overall, we published six journal articles about or applying PyGBe.&nbsp;</p> <div>&nbsp; &nbsp;This project targeted applications in biophysics, as just described, and also in fluid dynamics. For the latter, we developed the PetIBM library for simulating flow around immersed bodies, exploiting GPU hardware and parallel systems. PetIBM exploits the solvers available in the PETSc library (a toolkit for parallel scientific computing developed at Argonne National Laboratory), and the AmgX library (developed by NVIDIA for solving linear systems on GPUs). We wrote AmgXWrappter to couple PETSc and AmgX within PetIBM. This framework can be used for adding GPU capability to other open-source fluids simulation software, such as OpenFoam, offering opportunities for future impact of our work. A consistent area of focus for this project was to facilitate greater impact by making all research products open and freely available, and to enhance transparency in the research process by way of reproducibility.&nbsp;In a new effort to make our research transparent and reproducible by others, we developed a workflow to run and share computational studies on the public cloud.&nbsp;We adopted modern tools that facilitate creating and managing virtual machines on compute nodes and submitting jobs to these nodes.&nbsp;The configuration files for these tools are part of expanded "reproducibility packages" including workflow definitions for cloud computing, in addition to input files and instructions. We published this work supplemented with rigorous open data and methods.&nbsp;We also showed that cloud offerings are now suitable for computational fluid dynamics studies via research software that uses parallel computing with GPUs.</div> <div>&nbsp; &nbsp;A major component of this project was its educational program, building on the principal investigator's track record of success in the use of technology to support learning. Its aims: enhancing educational environments using technology for both curricular instruction and contributing to increase the nation's science literacy, and fostering the next generation of computational scientists. Teaching a course in computational fluid dynamics in a flipped classroom modality, we produced a set of YouTube videos that have gathered almost a million collective views. The principal investigator was an early adopter of the flipped classroom model, scarcely known at the start of this project. She disseminated at professional development events, conferences, and invited talks.&nbsp;She was an invited&nbsp;panelist to the 2013 National Academy of Engineering Frontiers of Engineering Education Symposium, speaking on online and blended learning. She developed a set of open educational resources using Jupyter (IPython) notebooks: the "CFD Python" series is available on GitHub under open licenses, and has inspired many others (it was translated to Spanish by volunteers). These materials were used as instructional scaffolding for an intensive two-day course at the event "HPC-LatAm, High-performance Computing Latin-America Symposium" in Argentina, July 2013. The following year, she developed a new set of open educational materials on the topic of aerodynamics ("AeroPython"), and a massive open online course in numerical methods ("NumericalMOOC") that registered more than 8,000 people from around the world.</div><br> <p>            Last Modified: 06/11/2020<br>      Modified by: Lorena&nbsp;A&nbsp;Barba</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890498183_OutcomesReport_figs.002--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890498183_OutcomesReport_figs.002--rgov-800width.jpg" title="PyGBe, open-source code for protein electrostatics: applications"><img src="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890498183_OutcomesReport_figs.002--rgov-66x44.jpg" alt="PyGBe, open-source code for protein electrostatics: applications"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Applications of PyGBe include protein electrostatics, protein-surface interactions, analysis of nanobiosensors.</div> <div class="imageCredit">Lorena A. Barba</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lorena&nbsp;A&nbsp;Barba</div> <div class="imageTitle">PyGBe, open-source code for protein electrostatics: applications</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890345312_OutcomesReport_figs.001--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890345312_OutcomesReport_figs.001--rgov-800width.jpg" title="Software products"><img src="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890345312_OutcomesReport_figs.001--rgov-66x44.jpg" alt="Software products"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This project developed high-performance scientific software combining heterogenous computing with GPUs, and fast hierarchical algorithms. All software is open source under a permissive license.</div> <div class="imageCredit">Lorena A. Barba</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lorena&nbsp;A&nbsp;Barba</div> <div class="imageTitle">Software products</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890583216_OutcomesReport_figs.003--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890583216_OutcomesReport_figs.003--rgov-800width.jpg" title="ExaFMM: a high-performance open-source library for fast multipole methods"><img src="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890583216_OutcomesReport_figs.003--rgov-66x44.jpg" alt="ExaFMM: a high-performance open-source library for fast multipole methods"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Performance comparison between the new exaFMM with its previous version and another open-source library.</div> <div class="imageCredit">Lorena A. Barba</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lorena&nbsp;A&nbsp;Barba</div> <div class="imageTitle">ExaFMM: a high-performance open-source library for fast multipole methods</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890765005_OutcomesReport_figs.004--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890765005_OutcomesReport_figs.004--rgov-800width.jpg" title="Reproducible workflow on a public cloud for computational fluid dynamics"><img src="/por/images/Reports/POR/2020/1460035/1460035_10156697_1591890765005_OutcomesReport_figs.004--rgov-66x44.jpg" alt="Reproducible workflow on a public cloud for computational fluid dynamics"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Reproducibility of computational science extended to the configuration of virtual computing resources on the cloud. https://doi.org/10.6084/m9.figshare.9636722.v1</div> <div class="imageCredit">Olivier Mesnard, Lorena A. Barba</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lorena&nbsp;A&nbsp;Barba</div> <div class="imageTitle">Reproducible workflow on a public cloud for computational fluid dynamics</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As the 21st century roared in, the heavy duty use of computers in science faced a turning point. First came multi-core processors, then graphics processing units (GPUs), and later other types of accelerators. Programming scientific applications became increasingly difficult, at a fast pace, as computer hardware turned predominantly heterogeneous. At the start of this project, three of the Top500 supercomputers in the world were powered by GPUs; in the 2019 list, 136 systems are GPU-accelerated. This project developed high-performance scientific software combining heterogenous computing with GPUs, and fast hierarchical algorithms. The algorithmic core technology is the fast multipole method, a mathematical advance highlighted as one of the top-10 algorithms of the 20th century. We amassed more than a decade of experience with this algorithm, and have refined and optimized its implementation in software. Our main software product, exaFMM, is an open-source library written in C++ with a Python interface, the result of years of accumulated knowledge and the work of several trainees. We also developed PyGBe, a software library using multipole methods within a solver for protein electrostatics. It allowed us to study the impact on computing binding energies of including solvent cavities in detailed models of proteins. We later extended PyGBe to treat the interaction of proteins with engineered surfaces of prescribed charge or electric potential. This set-up models a type of biosensors where a sensor element is represented as a charged surface, interacting with a biomolecule through electrostatics. Since performance of such biosensors is strongly dependent on the orientation of the molecules near the bioactive surface, we studied the preferred orientation of certain proteins under various conditions of surface charge and ambient salt concentration. In yet another extension of PyGBe, we added the capacity of simulating an optical effect, called localized surface plasmon resonance, via an electrostatics approximation. This new functionality allowed us to study sensitivity of a certain class of nano sensors. Overall, we published six journal articles about or applying PyGBe.     This project targeted applications in biophysics, as just described, and also in fluid dynamics. For the latter, we developed the PetIBM library for simulating flow around immersed bodies, exploiting GPU hardware and parallel systems. PetIBM exploits the solvers available in the PETSc library (a toolkit for parallel scientific computing developed at Argonne National Laboratory), and the AmgX library (developed by NVIDIA for solving linear systems on GPUs). We wrote AmgXWrappter to couple PETSc and AmgX within PetIBM. This framework can be used for adding GPU capability to other open-source fluids simulation software, such as OpenFoam, offering opportunities for future impact of our work. A consistent area of focus for this project was to facilitate greater impact by making all research products open and freely available, and to enhance transparency in the research process by way of reproducibility. In a new effort to make our research transparent and reproducible by others, we developed a workflow to run and share computational studies on the public cloud. We adopted modern tools that facilitate creating and managing virtual machines on compute nodes and submitting jobs to these nodes. The configuration files for these tools are part of expanded "reproducibility packages" including workflow definitions for cloud computing, in addition to input files and instructions. We published this work supplemented with rigorous open data and methods. We also showed that cloud offerings are now suitable for computational fluid dynamics studies via research software that uses parallel computing with GPUs.    A major component of this project was its educational program, building on the principal investigator's track record of success in the use of technology to support learning. Its aims: enhancing educational environments using technology for both curricular instruction and contributing to increase the nation's science literacy, and fostering the next generation of computational scientists. Teaching a course in computational fluid dynamics in a flipped classroom modality, we produced a set of YouTube videos that have gathered almost a million collective views. The principal investigator was an early adopter of the flipped classroom model, scarcely known at the start of this project. She disseminated at professional development events, conferences, and invited talks. She was an invited panelist to the 2013 National Academy of Engineering Frontiers of Engineering Education Symposium, speaking on online and blended learning. She developed a set of open educational resources using Jupyter (IPython) notebooks: the "CFD Python" series is available on GitHub under open licenses, and has inspired many others (it was translated to Spanish by volunteers). These materials were used as instructional scaffolding for an intensive two-day course at the event "HPC-LatAm, High-performance Computing Latin-America Symposium" in Argentina, July 2013. The following year, she developed a new set of open educational materials on the topic of aerodynamics ("AeroPython"), and a massive open online course in numerical methods ("NumericalMOOC") that registered more than 8,000 people from around the world.       Last Modified: 06/11/2020       Submitted by: Lorena A Barba]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
