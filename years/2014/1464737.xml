<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Human Cognition Assisted Control of Industrial Robots for Manufacturing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>558527.00</AwardTotalIntnAmount>
<AwardAmount>558527</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bruce Kramer</SignBlockName>
<PO_EMAI>bkramer@nsf.gov</PO_EMAI>
<PO_PHON>7032925348</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Advanced manufacturing, driven by industrial robots, is playing an increasing role in US economy. Robots are being used to carry out assembly, welding, material handling and fabrication. Even as such interactions are becoming more common in every phase of manufacturing, a perfect symbiotic relationship between machines and human beings is still very far away. Because of this, a majority of the robotic applications in manufacturing are currently limited to areas where a relatively low level of skill is required. This has restricted the full potential of robotics to augment human operators and improve productivity and quality of life. With recent advances in cognitive neuroscience and brain interface technologies, connecting the human cognitive thought process directly to robots and machines is possible, resulting in direct control of real world applications. By collecting the brain signals using sensors and analyzing the thought processes, many activities that take place inside the brain when humans take specific actions or think of actions can be identified and matched to known signals using fast computation. This new human-robot communication paradigm will be demonstrated by developing three manufacturing scenarios. The project will also have broad applicability in the design of robotic systems in fields outside manufacturing, including telesurgery, rehabilitation and space exploration. Results from this multidisciplinary research, which combines manufacturing, computer science and robotics, have the potential to improve the productivity of future manufacturing plants and can lead to new commercial ventures, which will help the US maintain global leadership in robotics and manufacturing, broaden participation of underrepresented groups in research, and positively impact engineering education.&lt;br/&gt;&lt;br/&gt;Significant future challenges in the development of a new human-robot communication system, which allows operators to perform complex high skilled tasks, will be addressed. The postulated paradigm will be explored by meeting the following intellectual challenges: (i) researching a novel methodology for communicating motion commands to a robot by imagining simple actions using a grammar called "actemes," (ii) new brain-computer mode and algorithms to classify these actemes and, (iii) an intent-based system that auto-completes robotic actions based on most likely sequence of events that human operators are planning to complete. Three robotic manufacturing scenarios will be explored to demonstrate the human cognition based interactions in manufacturing environment: assembly, direct control, and quality control through object recognition. Finally, by using a non-invasive brain-computer interface a wide range of day-to-day applications of robotics will be demonstrated.</AbstractNarration>
<MinAmdLetterDate>11/03/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/22/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1464737</AwardID>
<Investigator>
<FirstName>T.</FirstName>
<LastName>Kesavadas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>T. Kesavadas</PI_FULL_NAME>
<EmailAddress>kesh@illinois.edu</EmailAddress>
<PI_PHON>2172449341</PI_PHON>
<NSF_ID>000208163</NSF_ID>
<StartDate>11/03/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>087E</Code>
<Text>SENSING &amp; CONTROL</Text>
</ProgramReference>
<ProgramReference>
<Code>092E</Code>
<Text>Control systems &amp; applications</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<ProgramReference>
<Code>MANU</Code>
<Text>MANUFACTURING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~558527</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The next generation of industrial robots must be ready to work with their operators in a more collaborative manner. It would be extremely valuable if, in addition to taking commands from their operators, robots could also understand the tacit intentions of the operators. A novel way to achieve that goal would be to enable interaction of robots and operators through brain-computer interfaces (BCIs). We can provide a vital new channel of communication if we can enable robots to analyze a human?s neural activities directly, and thereby perceive the person?s intentions.</p> <p>This exploratory project aimed to develop non-invasive (EEG-based) BCIs for human-robot collaboration in industrial applications. Our novel approach allows robots to analyze and perform actions based on the operator?s intent, while being cognizant of the operator?s mental status. We demonstrated the new paradigm through three example BCI systems.</p> <p>First, we constructed a prototype robotic welding system. Using a BCI system, the robot was able to understand a welding plan and automatically generate a manufacturing path. Based on the joint edges identified, the robot can plan the welding, simulate the weld seam, and conduct welding on the selected weld bead.</p> <p>In a second example, we developed a BCI system for a robot whose job is to remove any defective parts from a series of industrial parts moving past it on a conveyor belt. The system allows the robot to recognize faulty parts by analyzing the mental reactions of an operator who is examining the parts on the conveyor. If the operator recognizes a defective part visually, the robot successfully identifies the part and removes it from the production line.</p> <p>In a third example BCI, we demonstrated how robot-human interactions can be made safer if the robot can tell when its actions during a collaborative task are making its human partner feel unsafe. When the human is startled by an unexpected motion of the robot, the BCI senses the human?s reaction and sends a command to the robot, telling it to perform an immediate shutdown. The brain response is detected via a regression model.</p> <p>We evaluated all three BCI examples through human-subject studies. The welding robot and the part-picking robot communicated with their operators through the brain response steady-state visual evoked potential (SSVEP) methodology. Although many deep-learning-based SSVEP classification models have been studied, they still perform much worse than the traditional template-correlation-based methods. We constructed a combined convolutional neural network and correlation analysis model that significantly improved the deep learning performance of the SSVEP classification technique. Our new approach also outperformed the template-correlation-based methods, and has potential to improve many other classification applications in which noisy signals are encountered.</p><br> <p>            Last Modified: 06/17/2020<br>      Modified by: T.&nbsp;Kesavadas</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1464737/1464737_10322158_1592419147382_exp--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1464737/1464737_10322158_1592419147382_exp--rgov-800width.jpg" title="A simulated welding experiment with Brain Computer Interface"><img src="/por/images/Reports/POR/2020/1464737/1464737_10322158_1592419147382_exp--rgov-66x44.jpg" alt="A simulated welding experiment with Brain Computer Interface"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A simulated welding experiment with Brain Computer Interface</div> <div class="imageCredit">Yao Li and T. Kesavadas, UIUC</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">T.&nbsp;Kesavadas</div> <div class="imageTitle">A simulated welding experiment with Brain Computer Interface</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1464737/1464737_10322158_1592419466712_Onlineexp--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1464737/1464737_10322158_1592419466712_Onlineexp--rgov-800width.jpg" title="Defective part picker"><img src="/por/images/Reports/POR/2020/1464737/1464737_10322158_1592419466712_Onlineexp--rgov-66x44.jpg" alt="Defective part picker"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An operator identifies a defective part on the conveyor. A brain computer interface recognizes this decision and instructs a robot to remove the part from the conveyor.</div> <div class="imageCredit">Yao Li and T. Kesavadas, UIUC</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">T.&nbsp;Kesavadas</div> <div class="imageTitle">Defective part picker</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The next generation of industrial robots must be ready to work with their operators in a more collaborative manner. It would be extremely valuable if, in addition to taking commands from their operators, robots could also understand the tacit intentions of the operators. A novel way to achieve that goal would be to enable interaction of robots and operators through brain-computer interfaces (BCIs). We can provide a vital new channel of communication if we can enable robots to analyze a human?s neural activities directly, and thereby perceive the person?s intentions.  This exploratory project aimed to develop non-invasive (EEG-based) BCIs for human-robot collaboration in industrial applications. Our novel approach allows robots to analyze and perform actions based on the operator?s intent, while being cognizant of the operator?s mental status. We demonstrated the new paradigm through three example BCI systems.  First, we constructed a prototype robotic welding system. Using a BCI system, the robot was able to understand a welding plan and automatically generate a manufacturing path. Based on the joint edges identified, the robot can plan the welding, simulate the weld seam, and conduct welding on the selected weld bead.  In a second example, we developed a BCI system for a robot whose job is to remove any defective parts from a series of industrial parts moving past it on a conveyor belt. The system allows the robot to recognize faulty parts by analyzing the mental reactions of an operator who is examining the parts on the conveyor. If the operator recognizes a defective part visually, the robot successfully identifies the part and removes it from the production line.  In a third example BCI, we demonstrated how robot-human interactions can be made safer if the robot can tell when its actions during a collaborative task are making its human partner feel unsafe. When the human is startled by an unexpected motion of the robot, the BCI senses the human?s reaction and sends a command to the robot, telling it to perform an immediate shutdown. The brain response is detected via a regression model.  We evaluated all three BCI examples through human-subject studies. The welding robot and the part-picking robot communicated with their operators through the brain response steady-state visual evoked potential (SSVEP) methodology. Although many deep-learning-based SSVEP classification models have been studied, they still perform much worse than the traditional template-correlation-based methods. We constructed a combined convolutional neural network and correlation analysis model that significantly improved the deep learning performance of the SSVEP classification technique. Our new approach also outperformed the template-correlation-based methods, and has potential to improve many other classification applications in which noisy signals are encountered.       Last Modified: 06/17/2020       Submitted by: T. Kesavadas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
