<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Human-Supervised Perception and Grasping in  Clutter</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>807978</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>One of the basic building blocks in semi-autonomous manipulation is the ability for a robot to grasp an object that a human operator indicates.  There are many tasks where the natural way for a human and robot to work together is for the human to point out the approximate locations of objects to be grasped and for the robot to generate the precise motions necessary to achieve the grasp.  This core "auto-grasp" functionality is critical to providing assistive manipulation for the disabled and elderly, as well as for a variety of military, police, space, or underwater applications. But implementing auto-grasp capability can be challenging in situations where the environment is cluttered, or when it is difficult to determine the grasp intention of the human.  In this collaborative project that combines expertise from two institutions, the PIs will tackle situations where it is necessary for the robot to actively explore or "interrogate" the environment in order to figure out what the human intends to grasp and how the robot should do it.  To these ends, the PIs will investigate a modified approach to planning under uncertainty known as belief space planning.  Belief space planning is well-suited to active localization for grasping, because it is a single framework in which the algorithm can reason about perception-oriented and goal-orientation parts of the task.  The PIs will use belief space planning to localize graspable geometries in the environment, known as grasp affordances, in a region indicated by the user.  They will also explore different ways in which a human can interact with the system in order to control the grasping.  The application focus of the work will be in assistive manipulation, where a person who is elderly or disabled operates an assistive robot arm mounted on an electric wheelchair or scooter.  User studies will determine the best methods for the target population to operate the system.  The project will contribute to the opportunities available for undergraduates and high school students in the PIs' institutions, and it will also be integrated as appropriate into the curricula of the courses they teach.&lt;br/&gt;&lt;br/&gt;This research contains two key innovations that the PIs expect will make robot grasping more robust.  The first is to incorporate ideas from belief space planning into the reach and grasp planning process.  Because belief space planning can reason about how the robot's own "state of information" is expected to change in the future, it is capable of producing plans that acquire task-relevant information in the course of performing a task.  The second innovation is a new approach to perception-for-grasping that localizes grasp affordance geometries in the neighborhoods of objects of potential interest.  Not only is this grasp affordance approach helpful to the belief space planner, but the PIs' preliminary work indicates that this approach can be accurate and very fast (10Hz).  Finally, the connection between the user interface and uncertainty in the location of the grasp target will also be explored, the plan being to model human behavior as an uncertain system where hidden variables describe user intention.</AbstractNarration>
<MinAmdLetterDate>08/11/2014</MinAmdLetterDate>
<MaxAmdLetterDate>04/03/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1427081</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Platt</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Platt</PI_FULL_NAME>
<EmailAddress>r.platt@neu.edu</EmailAddress>
<PI_PHON>6173737681</PI_PHON>
<NSF_ID>000601686</NSF_ID>
<StartDate>08/11/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northeastern University</Name>
<CityName>BOSTON</CityName>
<ZipCode>021155005</ZipCode>
<PhoneNumber>6173733004</PhoneNumber>
<StreetAddress>360 HUNTINGTON AVE</StreetAddress>
<StreetAddress2><![CDATA[177-500]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001423631</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHEASTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001423631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northeastern University]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>021155005</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7298</Code>
<Text>International Research Collab</Text>
</ProgramElement>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>5947</Code>
<Text>BELGIUM</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~750000</FUND_OBLG>
<FUND_OBLG>2015~57978</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><br />The goal of this project was to develop a robotic mobility scooter that could be used by people with disabilities to improve their ability to perform the activities of daily life in the home. Our robotic system combined an off the shelf mobility scooter with a robotic arm mounted on the front of it. This arm could be controlled by the user (i.e. the driver of the scooter) to pick up or place objects around the home. Most assistive robotic arms are hard to control because their low level motions are controlled directly by joystick. Our system is easier to use because it can reach toward and grasp an arbitrary object identified by the user. All the user must do is point at the object to grasp using a laser pointer and the robot does the rest. It automatically picks up the robot and either hands it to the user or holds it so that it can be placed elsewhere in the home. This project focused on the low level hardware development of the robot, the grasping capabilities, and on the user interface needed to make the system usable by our target user group. Our system was evaluated by expert users and in a limited way by users from our target user group.<br /><br />Probably our most important project outcome was to demonstrate that the high level concept of a robotic mobility scooter to perform manipulation tasks (grasping and placing) in a home environment is feasible. We demonstrated that a user could use our system to drive to a target object in a room, indicate the object with the laser, and grasp it in approximately one minute. In another 50 seconds, the user could drive elsewhere in the room, indicate a place location using the laser, and place the object on a flat surface. While this is slower than an able bodied human could perform the task, it is competitive for situations where the user involved has a disability.&nbsp;<br /><br />Other important outcomes of the project included the development of GPD, one of the first robotic grasp detection systems. In grasp detection, the robot looks at the world through a camera and identifies potential grasp locations that could be used to pick up novel objects in its environment. Our system has been used widely in the robotic grasping research community.<br /><br />Finally, our project had an important impact on the development of user interfaces. First, we demonstrated that the idea of using a laser pointer to identify objects to grasp was viable in our setting. Second, we introduced the idea of a projection-based user interface where our system conveyed intent by projecting light onto the real world in front of it.<br /><br /></p><br> <p>            Last Modified: 04/14/2021<br>      Modified by: Robert&nbsp;Platt</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The goal of this project was to develop a robotic mobility scooter that could be used by people with disabilities to improve their ability to perform the activities of daily life in the home. Our robotic system combined an off the shelf mobility scooter with a robotic arm mounted on the front of it. This arm could be controlled by the user (i.e. the driver of the scooter) to pick up or place objects around the home. Most assistive robotic arms are hard to control because their low level motions are controlled directly by joystick. Our system is easier to use because it can reach toward and grasp an arbitrary object identified by the user. All the user must do is point at the object to grasp using a laser pointer and the robot does the rest. It automatically picks up the robot and either hands it to the user or holds it so that it can be placed elsewhere in the home. This project focused on the low level hardware development of the robot, the grasping capabilities, and on the user interface needed to make the system usable by our target user group. Our system was evaluated by expert users and in a limited way by users from our target user group.  Probably our most important project outcome was to demonstrate that the high level concept of a robotic mobility scooter to perform manipulation tasks (grasping and placing) in a home environment is feasible. We demonstrated that a user could use our system to drive to a target object in a room, indicate the object with the laser, and grasp it in approximately one minute. In another 50 seconds, the user could drive elsewhere in the room, indicate a place location using the laser, and place the object on a flat surface. While this is slower than an able bodied human could perform the task, it is competitive for situations where the user involved has a disability.   Other important outcomes of the project included the development of GPD, one of the first robotic grasp detection systems. In grasp detection, the robot looks at the world through a camera and identifies potential grasp locations that could be used to pick up novel objects in its environment. Our system has been used widely in the robotic grasping research community.  Finally, our project had an important impact on the development of user interfaces. First, we demonstrated that the idea of using a laser pointer to identify objects to grasp was viable in our setting. Second, we introduced the idea of a projection-based user interface where our system conveyed intent by projecting light onto the real world in front of it.         Last Modified: 04/14/2021       Submitted by: Robert Platt]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
