<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Integrating Humans and Computers for Image and Video Understanding</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/21/2013</AwardEffectiveDate>
<AwardExpirationDate>04/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>764930.00</AwardTotalIntnAmount>
<AwardAmount>764930</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In this project, the research team explores several research challenges to exploit the relationship between images, video, and the people viewing this visual imagery.  Areas of exploration include: 1) behavioral experiments to better understand the relationship between human viewers and imagery, 2) development of human-computer collaborative systems for image and video understanding that utilize automatic computer vision algorithms in conjunction with active and passive cues from human viewers, and 3) implementing retrieval and collection organization applications using our collaborative models.&lt;br/&gt;&lt;br/&gt;Billions of images and millions of videos are now available online via the infrastructure of amazingly successful companies from Google to Microsoft to Facebook. This wealth of visual data is creating considerable opportunities for communication and community, and tightening the social fabric of our world.  In parallel to this explosion in online imagery, there is also an increasing proliferation of cameras viewing the user, from the ever present webcams peering out at us from our laptops, to cell phone cameras carried in our pockets wherever we go. This record of a user's viewing behavior, particularly of their eye, body movements, or descriptions, can provide enormous insight into how people interact with images or video, and can inform construction of more effective visual applications such as image or video retrieval.  In addition, understanding what people recognize, attend to, or describe about an image or video is a necessary step toward high level goals of human centric image understanding that will have research benefits to many diverse fields, including computer vision and behavioral science.</AbstractNarration>
<MinAmdLetterDate>06/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1445409</AwardID>
<Investigator>
<FirstName>Tamara</FirstName>
<LastName>Berg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tamara Berg</PI_FULL_NAME>
<EmailAddress>tlberg@cs.unc.edu</EmailAddress>
<PI_PHON>6465093361</PI_PHON>
<NSF_ID>000519059</NSF_ID>
<StartDate>06/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S Columbia St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~25238</FUND_OBLG>
<FUND_OBLG>2013~504144</FUND_OBLG>
<FUND_OBLG>2015~235548</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Billions of images and millions of videos are now available online via the infrastructure of amazingly successful companies from Google to Microsoft and Facebook. This wealth of visual data is creating unparalleled opportunities for communication and community, tightening the social fabric of our world. Generally people are the end consumers of this vast visual imagery. Therefore, understanding what people recognize, attend to, or describe about an image is a necessary goal for computational image understanding, as it can help focus efforts on what to comprehend about an image toward those aspects that people find important.&nbsp;</span><span class="im"><br /><br />Toward this goal, the first part of our grant addressed how people view and narrate images through a series of behavioral experiments aimed at discovering and probing the relationships between gaze, description, and imagery. The second part of our project implemented methods to integrate human cues and models of human perception into computational recognition tasks such as object recognition, detection, and natural language description.<br /><br />Outcomes of our research efforts include a better understanding and modeling of the relationship between viewers and the imagery they are viewing, improved applications to computer vision technologies for object recognition and localization, as well as systems that can generate and comprehend natural language describing objects in the real world. Impacts of these technologies will have and have had benefits to disciplines such as psychology, computer vision, natural language processing, robotics, and human-computer interaction. Additionally, results were disseminated to the public through conferences, journals, publicly released datasets, code, and educational seminars, as well as demonstrated to high school and middle school students at public outreach events.</span></p><br> <p>            Last Modified: 07/14/2017<br>      Modified by: Tamara&nbsp;Berg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Billions of images and millions of videos are now available online via the infrastructure of amazingly successful companies from Google to Microsoft and Facebook. This wealth of visual data is creating unparalleled opportunities for communication and community, tightening the social fabric of our world. Generally people are the end consumers of this vast visual imagery. Therefore, understanding what people recognize, attend to, or describe about an image is a necessary goal for computational image understanding, as it can help focus efforts on what to comprehend about an image toward those aspects that people find important.   Toward this goal, the first part of our grant addressed how people view and narrate images through a series of behavioral experiments aimed at discovering and probing the relationships between gaze, description, and imagery. The second part of our project implemented methods to integrate human cues and models of human perception into computational recognition tasks such as object recognition, detection, and natural language description.  Outcomes of our research efforts include a better understanding and modeling of the relationship between viewers and the imagery they are viewing, improved applications to computer vision technologies for object recognition and localization, as well as systems that can generate and comprehend natural language describing objects in the real world. Impacts of these technologies will have and have had benefits to disciplines such as psychology, computer vision, natural language processing, robotics, and human-computer interaction. Additionally, results were disseminated to the public through conferences, journals, publicly released datasets, code, and educational seminars, as well as demonstrated to high school and middle school students at public outreach events.       Last Modified: 07/14/2017       Submitted by: Tamara Berg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
