<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: New Optimization Methods for Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This proposal explores the optimization of complicated nonlinear equations that underlie machine learning problems by reducing them to simpler easy-to-solve update rules. The learning problems include classification, regression, unsupervised learning and more. Through a method known as majorization, complicated optimization problems are handled by iteratively solving simpler problems like least-squares and traditional linear algebra operations. The proposal focuses on how to parallelize this method so that it can efficiently leverage many CPUs/GPUs simultaneously and in a distributed manner. Furthermore, by making the method stochastic, faster convergence on large or streaming data-sets becomes possible. Other variations are explored such as sparse learning where the recovered solution is forced to be compact which also leads to further efficiency.  &lt;br/&gt;&lt;br/&gt;Increasingly, the vast majority of machine learning problems in the literature are optimized by using generic first- and second-order methods.  The approach in this proposal is designed specifically for machine learning optimization problems and uses majorization and bounding to guarantee monotonic convergence.  In preliminary work, majorization has produced faster convergence in practice as well as novel theoretical guarantees. To make the method truly viable in practice, this proposal puts forward distributed, parallel, stochastic and sparse extensions. Since such extensions may violate monotonic convergence guarantees, the proposal explores additional algorithmic and theoretical efforts to preserve guarantees while also obtaining fast algorithms. In particular, parallelization and distributed computation is performed by wrapping current state-of-the-art least squares solvers with bound majorization steps. Stochastic computation is explored using singleton, small-batch and variable-sized batch methods. Sparsity is achieved by iterating current large-scale sparse solvers like FISTA and QUIC within the bound majorization technique.  In terms of broader impact, one graduate student will be supported and will help produce downloadable tools for machine learning experts as well as practitioners.  Modules will be developed to add to the PI's existing courses in machine learning. The PI will organize a one-day workshop on majorization methods.  The proposal also provides a public project website with access to research publications, software/data downloads and schedules of upcoming events.</AbstractNarration>
<MinAmdLetterDate>08/22/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1451500</AwardID>
<Investigator>
<FirstName>Tony</FirstName>
<LastName>Jebara</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tony Jebara</PI_FULL_NAME>
<EmailAddress>jebara@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397079</PI_PHON>
<NSF_ID>000093679</NSF_ID>
<StartDate>08/22/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In most machine learning systems, parameter optimization is needed to learn models from training data. The learning process is frequently accomplished via gradient descent: tweaking the parameters of the model by sliding downhill on a given cost surface. This project considered alternative approaches called majorization and variational methods.<br /><br />Through majorization, we replace the optimization at hand with a sequence of simpler problems (such as quadratic or linear sub-problems) which admit analytic solutions. This sequence of simpler problems makes guaranteed progress on the original cost function that we wanted to minimize. We applied majorization to logistic regression and more general probabilistic models by reducing the learning problem to a sequence of quadratic optimizations. This ultimately gave better models more quickly than classical gradient descent. We were also able to extend this concept to intractable probabilistic models (e.g. dense networks of variables) through a related technique called Frank-Wolfe.<br /><br />Applications of these methods included learning probabilistic graphical models that describe networks of interconnected variables such as financial variables and stocks or networks of interacting neurons (given neuronal recordings from in vivo mice). We were able to recover such networks from data and migrated them towards applications in finance and neuroscience.<br /><br />We also explored stochastic and parallel extensions of majorization methods where the training data is only available in an online/streaming setting or is distributed in files across several machines. Finally, we developed majorization for saddle-point optimization where we need to minimize cost over some variables while maximizing profit over other variables. This resulted in a novel extension of the Frank-Wolfe method to saddle-point minimax optimization. We demonstrated this approach on real data problems and provided new theoretical results to prove the convergence of Saddle-Point Frank-Wolfe.<br /><br />&nbsp;<br /></p><br> <p>            Last Modified: 07/28/2018<br>      Modified by: Tony&nbsp;Jebara</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In most machine learning systems, parameter optimization is needed to learn models from training data. The learning process is frequently accomplished via gradient descent: tweaking the parameters of the model by sliding downhill on a given cost surface. This project considered alternative approaches called majorization and variational methods.  Through majorization, we replace the optimization at hand with a sequence of simpler problems (such as quadratic or linear sub-problems) which admit analytic solutions. This sequence of simpler problems makes guaranteed progress on the original cost function that we wanted to minimize. We applied majorization to logistic regression and more general probabilistic models by reducing the learning problem to a sequence of quadratic optimizations. This ultimately gave better models more quickly than classical gradient descent. We were also able to extend this concept to intractable probabilistic models (e.g. dense networks of variables) through a related technique called Frank-Wolfe.  Applications of these methods included learning probabilistic graphical models that describe networks of interconnected variables such as financial variables and stocks or networks of interacting neurons (given neuronal recordings from in vivo mice). We were able to recover such networks from data and migrated them towards applications in finance and neuroscience.  We also explored stochastic and parallel extensions of majorization methods where the training data is only available in an online/streaming setting or is distributed in files across several machines. Finally, we developed majorization for saddle-point optimization where we need to minimize cost over some variables while maximizing profit over other variables. This resulted in a novel extension of the Frank-Wolfe method to saddle-point minimax optimization. We demonstrated this approach on real data problems and provided new theoretical results to prove the convergence of Saddle-Point Frank-Wolfe.           Last Modified: 07/28/2018       Submitted by: Tony Jebara]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
