<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Tracing the Use of Research Resources Using Persistent Citable Identifiers</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>281718.00</AwardTotalIntnAmount>
<AwardAmount>281718</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Research organizations, data repositories, and universities are beginning to assign persistent web-accessible identifiers to scientific resources. Assigning these persistent identifiers serves two crucial purposes. First, they are assumed to increase the traceability and reusability of scientific resources by allowing data sets, software, and other resources to be cited within the scientific literature. Second, without such traceability, it is difficult, if not impossible, to develop tools to measure the impact such resources have within the communities they belong to, or to understand the spread of that impact. Significant progress has been made in the past few years in the development of recommendations, policies, and procedures for creating and promoting these citable identifiers. While these efforts have raised the awareness of citation principles, which primarily address solutions to this issue moving forward, little progress has been made in tracking how data sets and other digital resources have actually been identified and cited in the past. This project focuses on developing efficient and extensible computational approaches to such attribution tracking. This effort will inform broader efforts to understand how scientific communities benefit from the use of scientific resources, facilities, and services. &lt;br/&gt; &lt;br/&gt;Using a multifaceted experimental design, the project examines whether resources with persistent identifiers receive more attribution (through citations or acknowledgements) than those that do not. The research questions focus on the benefits of assigning persistent identifiers to scientific resources, namely, whether the overall citations of such resources increase, and whether the traceability of such citations of resources increased with resource persistent identifiers. The outcomes and contribution of this project center on providing insight into 1) whether human or computational approaches compare favorably in terms of their accuracy and efficiency in finding relevant references to scientific resources, 2) whether persistent identifiers provide quantifiable benefit in terms of increasing the amount a resource is referenced by users, and 3) whether persistent identifiers are being included in significant numbers in references to scientific resources.</AbstractNarration>
<MinAmdLetterDate>08/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1448480</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Mayernik</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Mayernik</PI_FULL_NAME>
<EmailAddress>mayernik@ucar.edu</EmailAddress>
<PI_PHON>3034971183</PI_PHON>
<NSF_ID>000621619</NSF_ID>
<StartDate>08/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Keith</FirstName>
<LastName>Maull</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Keith Maull</PI_FULL_NAME>
<EmailAddress>kmaull@ucar.edu</EmailAddress>
<PI_PHON>3034971187</PI_PHON>
<NSF_ID>000647994</NSF_ID>
<StartDate>08/10/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University Corporation For Atmospheric Res</Name>
<CityName>Boulder</CityName>
<ZipCode>803012252</ZipCode>
<PhoneNumber>3034971000</PhoneNumber>
<StreetAddress>3090 Center Green Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078339587</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY CORPORATION FOR ATMOSPHERIC RESEARCH</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>078339587</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University Corporation For Atmospheric Res]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803055602</ZipCode>
<StreetAddress><![CDATA[1850 Table Mesa Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7626</Code>
<Text>SciSIP-Sci of Sci Innov Policy</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramElement>
<Code>8022</Code>
<Text>STAR Metrics</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~281718</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-4394ef20-da4d-4653-551f-1d709e6e68e3" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">This project, &ldquo;EAGER: Tracing the Use of Research Resources Using Persistent Citable Identifiers,&rdquo; developed and evaluated tools for automating the tracing of how research infrastructures are identified and referenced in the research literature via persistent citable identifiers. Our project goals were to make advances toward (1) research frameworks for understanding how to methodically and consistently analyze the scientific impact of datasets, facilities, software, instruments, etc., and (2) techniques, software, and algorithms to effectively and automatically collect, analyze, and share the application of (1). Since assigning and using persistent identifiers for scientific research infrastructures like data collections, software packages, and research facilities is a relatively new development, very few assessments have been conducted that systematically examine the effects of such identifiers assigned to these infrastructures. Our project explored the approaches and techniques necessary to more easily track attribution to scientific research infrastructures. </span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">To investigate the question of whether citations and references to research infrastructures are increasing or changing with the increased assignment of persistent digital identifiers, citations and references for four NCAR-managed resources were collected and analyzed. The analysis focused on characterizing the extent to which these resources receive references and citations via the associated persistent ID for each resource. Relevant documents were analyzed to identify where/how in the structure of the document the reference took place. This assessment found that papers that reference research infrastructures such as data collections, software packages, and supercomputers via the persistent IDs were growing in number, but that those references are spread throughout articles in inconsistent ways. Formal references in the reference lists at the end of papers are becoming more prevalent in all three cases, but in-text references and mentions in the acknowledgements sections of papers are still common. Shifts in citation/reference practices over time are occurring, but also vary from resource to resource.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">The second focus of the project developed computational tracing algorithms and methods to detect when academic papers have used or cited NCAR supercomputing facilities. These algorithms centered on three axes: (1) collection of candidate publications for classification, (2) development of an experimental methodology for classification, and (3) an automation framework for executing document classification and analysis. We used a case study to focus our work, applying computational tools to try to predict whether a paper is likely to have utilized the NCAR supercomputer. The intuition guiding this effort is that documents citing the supercomputer&rsquo;s persistent identifier can be used to develop machine learning training features that can help identify relevant documents that did not use the identifier, but were likely to have used the supercomputer. &nbsp;We explored two methodologies to develop tools to understand how a classifier would be built to find documents that used the supercomputer.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">Our initial automated classification experiments in identifying documents that used the NCAR supercomputer used a Naive Bayes classifier. The result was an initial precision at less than 0.65. This provided a preliminary indication that our initial set of training documents (e.g. documents that are known to have used the supercomputer) will be a sufficient starting point for expanding the classification work on documents of unknown relevance. Experiments were expanded further, using a larger dataset and more refined extraction methodology that included processing PDF document text through the open source text and metadata extraction tool GROBID. Once text extraction and post-processed was complete, three document classification algorithms (Support Vector Machines, Multinomial Naive Bayes, and Random Forest classifiers) were used to compare the performance of these methods. The Random Forest classifier performed the best on the training data with an accuracy of 0.80469, beating all other classifiers with the test data. The code and experimental methodology for this project are open and available on Github repository.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">The need to understand the impact of investments in research infrastructure and validate the benefits of such services is common to every organization, regardless of the type of infrastructure. This project informs how those benefits can be assessed, both conceptually and methodologically. The challenges that exist in tracing attribution to scientific infrastructures through the research literature are considerable, and no one tool or method is likely to provide an overarching solution. The better the problems impeding progress in this area can be bounded, the better solutions can be evaluated, tools can be shared, and generalizable solutions can emerge.</span></p><br> <p>            Last Modified: 11/20/2017<br>      Modified by: Matthew&nbsp;Mayernik</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[This project, "EAGER: Tracing the Use of Research Resources Using Persistent Citable Identifiers," developed and evaluated tools for automating the tracing of how research infrastructures are identified and referenced in the research literature via persistent citable identifiers. Our project goals were to make advances toward (1) research frameworks for understanding how to methodically and consistently analyze the scientific impact of datasets, facilities, software, instruments, etc., and (2) techniques, software, and algorithms to effectively and automatically collect, analyze, and share the application of (1). Since assigning and using persistent identifiers for scientific research infrastructures like data collections, software packages, and research facilities is a relatively new development, very few assessments have been conducted that systematically examine the effects of such identifiers assigned to these infrastructures. Our project explored the approaches and techniques necessary to more easily track attribution to scientific research infrastructures.     To investigate the question of whether citations and references to research infrastructures are increasing or changing with the increased assignment of persistent digital identifiers, citations and references for four NCAR-managed resources were collected and analyzed. The analysis focused on characterizing the extent to which these resources receive references and citations via the associated persistent ID for each resource. Relevant documents were analyzed to identify where/how in the structure of the document the reference took place. This assessment found that papers that reference research infrastructures such as data collections, software packages, and supercomputers via the persistent IDs were growing in number, but that those references are spread throughout articles in inconsistent ways. Formal references in the reference lists at the end of papers are becoming more prevalent in all three cases, but in-text references and mentions in the acknowledgements sections of papers are still common. Shifts in citation/reference practices over time are occurring, but also vary from resource to resource.    The second focus of the project developed computational tracing algorithms and methods to detect when academic papers have used or cited NCAR supercomputing facilities. These algorithms centered on three axes: (1) collection of candidate publications for classification, (2) development of an experimental methodology for classification, and (3) an automation framework for executing document classification and analysis. We used a case study to focus our work, applying computational tools to try to predict whether a paper is likely to have utilized the NCAR supercomputer. The intuition guiding this effort is that documents citing the supercomputer?s persistent identifier can be used to develop machine learning training features that can help identify relevant documents that did not use the identifier, but were likely to have used the supercomputer.  We explored two methodologies to develop tools to understand how a classifier would be built to find documents that used the supercomputer.    Our initial automated classification experiments in identifying documents that used the NCAR supercomputer used a Naive Bayes classifier. The result was an initial precision at less than 0.65. This provided a preliminary indication that our initial set of training documents (e.g. documents that are known to have used the supercomputer) will be a sufficient starting point for expanding the classification work on documents of unknown relevance. Experiments were expanded further, using a larger dataset and more refined extraction methodology that included processing PDF document text through the open source text and metadata extraction tool GROBID. Once text extraction and post-processed was complete, three document classification algorithms (Support Vector Machines, Multinomial Naive Bayes, and Random Forest classifiers) were used to compare the performance of these methods. The Random Forest classifier performed the best on the training data with an accuracy of 0.80469, beating all other classifiers with the test data. The code and experimental methodology for this project are open and available on Github repository.    The need to understand the impact of investments in research infrastructure and validate the benefits of such services is common to every organization, regardless of the type of infrastructure. This project informs how those benefits can be assessed, both conceptually and methodologically. The challenges that exist in tracing attribution to scientific infrastructures through the research literature are considerable, and no one tool or method is likely to provide an overarching solution. The better the problems impeding progress in this area can be bounded, the better solutions can be evaluated, tools can be shared, and generalizable solutions can emerge.       Last Modified: 11/20/2017       Submitted by: Matthew Mayernik]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
