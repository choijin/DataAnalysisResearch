<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative proposal: A multimodal tactile sensor skin designed to reduce the cognitive burden on the user of a prosthetic hand</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>175015.00</AwardTotalIntnAmount>
<AwardAmount>175015</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07020000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CBET</Abbreviation>
<LongName>Div Of Chem, Bioeng, Env, &amp; Transp Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Aleksandr Simonian</SignBlockName>
<PO_EMAI>asimonia@nsf.gov</PO_EMAI>
<PO_PHON>7032922191</PO_PHON>
</ProgramOfficer>
<AbstractNarration>PI: Santos, Veronica J. and Posner, Jonathan D.&lt;br/&gt;Proposal Number: 1264444 &amp; 1264046&lt;br/&gt;&lt;br/&gt;Intellectual Merit: Whether a prosthetic hand is a simple body-powered hook or an advanced anthropomorphic device, it will only be useful and desirable to an amputee if it improves quality of life and is intuitive to control. A prosthesis will be rejected if it poses too great of a cognitive burden on the user. One way to simultaneously reduce the cognitive burden on the user and enhance the functionality of the user to focus on high-level commands as opposed to low-level details that may be frustrating to control or even impossible to control given the "language barrier" between human and machine because of different timescales and resolutions of control. Amputees could be empowered with prostheses having autonomous, local reflex algorithms akin to short latency grip reflexes observed in humans, and even suites of basic behavioral building blocks that are critical for activities of daily living. The only way for a semi-autonomous system to gain the trust of its operator is through reliable, context-dependent performance. Such context-aware performance will require information about forceful interactions between the prosthetic hand and everyday objects in unstructured environments that can only be obtained through touch. The great number and dynamic range of tactile mechanoreceptors in the human hand (17000 tactile sensors total, 2000 in each fingertip) highlight the importance of rich multimodal tactile feedback for grasp and dexterous manipulation. Unfortunately, many tactile sensor designs have focused on detection of normal forces alone, which are necessary but not sufficient for reliable artificial grasp. What is sorely needed is a multimodal tactile sensor that can detect additional important features of finger-object interactions such as shear force, vibration, and slip direction.  This proposal aims to strengthen the ability of an artificial hand to perform automated behaviors reliably by detecting, processing, and utilizing rich, real-time information about finger-object interactions with an innovative multimodal tactile sensor skin. This sensor system is transformative because it will reduce the cognitive burden on an amputee and will provide a foundation for paradigm-shifting advancements for automating complex behaviors by artificial hands and providing a conscious perception of touch through sensory feedback to the user. The long-term research objective of this proposal is to reduce the cognitive burden on the user of an upper extremity prosthesis. The following contributions to artificial hand systems are proposed: Research Goal 1) Design, model, fabricate, and test a flexible, multimodal tactile sensor skin system for artificial fingertips using a multilayer microfluidic architecture; Research Goal 2) Establish functional relationships between finger-object interactions and tactile sensor skin data for use in autonomous grip control algorithms; and Research Goal 3) Integrate the tactile sensor skin data into grip control algorithms and evaluate effectiveness for reducing the cognitive burden on prosthesis users.&lt;br/&gt;&lt;br/&gt;Broader Impacts: The proposed translational research could enhance the functional capabilities of artificial, robotic manipulators intended for unstructured, unsafe, or limited-access environments (prosthetic, rehabilitative, assistive, space, underwater, military, rescue, surgery). The proposed work could play a critical role in improving the quality of life for end-users of prosthetic and assistive devices. Specific benefits to end-users of prosthetic devices include: automation of complex prosthesis behaviors, rich artificial sensory feedback, and "smart socket liners" for monitoring user safety and comfort.&lt;br/&gt;&lt;br/&gt;Contributions to elementary school, undergraduate, and graduate-level education are proposed: Education Goal 1) Develop hands-on instructional modules for teaching elementary school students about sensors using low-cost materials, and deploy them locally for the benefit of students underrepresented in science, technology, engineering, and mathematics fields; Education Goal 2) Enhance undergraduate-level course titled Sensors and Controls and graduate-level course titled "Robotics" with a sensors module; and Education Goal 3) Promote interdisciplinary undergraduate research opportunities via internships related to the development, testing, and application of sensors.</AbstractNarration>
<MinAmdLetterDate>09/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/10/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1461630</AwardID>
<Investigator>
<FirstName>Veronica</FirstName>
<LastName>Santos</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Veronica J Santos</PI_FULL_NAME>
<EmailAddress>vjsantos@ucla.edu</EmailAddress>
<PI_PHON>3108252125</PI_PHON>
<NSF_ID>000521933</NSF_ID>
<StartDate>09/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>900952000</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>5342</Code>
<Text>RESEARCH TO AID THE DISABLED</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~175015</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Intellectual Merit:&nbsp; </strong>Long-term research objective: Reduce the cognitive burden on the user of an upper extremity prosthesis. Research Goals: 1) Design, model, fabricate, and test a flexible, multimodal tactile sensor skin system for artificial fingertips using a multilayer microfluidic architecture, 2) Establish functional relationships between finger-object interactions and tactile sensor skin data for use in autonomous grip control algorithms, 3) Integrate the tactile sensor skin data into grip control algorithms and evaluate effectiveness for reducing the cognitive burden on prosthesis users.</p> <p><span style="text-decoration: underline;">Toward Goal #1:</span> Collaborating with experts in microfluidics and soft lithography at the University of Washington, we developed a deformable, bioinspired, resistive microfluidic shear force sensor skin that can be conformally wrapped around curved surfaces, such as fingertips. When subjected to shear force, the skin continuum bulges like a human fingerpad, with one region experiencing tension and an opposing region experiencing compression. Tension and compression are sensed through differential measurements across pairs of liquid metal strain gauges embedded in polydimethylsiloxane that are strategically placed in opposition to one another and adjacent to a fixed nailbed. Placing shear force taxels (individual tactile sensing units) away from regions of direct finger-object contact leaves room for normal force taxels. Unaffected by delays associated with capacitive taxels, resistive taxels are highly sensitive to vibrations and transient events. The shear sensor skin has a dynamic range (up to 5N) of more than 10-bits and can quantify vibration up to 800Hz, which exceeds human capabilities. The skin was demonstrated to be sensitive to tactile events during robot manipulation tasks. The microfluidic skin is intrinsically flexible and immune to fatigue when subjected to repeated, large strains.</p> <p><span style="text-decoration: underline;">Toward Goal #2:</span> We developed a model for the haptic perception of tactile directionality (direction of applied shear force), which is useful for perceiving movements of grasped objects relative to a hand-centric reference frame. Commercially available sensors were used to collect tactile data (fingerpad deformation, internal fluid pressure, vibration, temperature) during perturbations of an object in a two-digit grasp. Fingerpad deformation data were the most useful for encoding tactile directionality. After transforming fingerpad deformation data into grayscale tactile 'images,' we developed a regression model based on a convolutional neural network to estimate tactile directionality. Using the trained model, we demonstrated real-time haptic perception and context-appropriate decision-making on a robot testbed.</p> <p><span style="text-decoration: underline;">Toward Goal #3:</span> Previously, we developed a highly sensorized, tendon-driven, anthropomorphic robot testbed called the 'BairClaw' to study proprioceptive and tactile sensor feedback during finger-object interactions. We began expanding this testbed to a three-digit hand to enable the testing of tactile sensor skins and grip control algorithms. Using the BairClaw, we conducted a single-digit study on isometric force production. Tendon tension was a function of baseline tension, joint torque ratios associated with the multi-joint digit design, and contact forces at the fingertip. We also completed the development of a low-cost, two-channel surface electromyography controller for the myoelectric control of artificial hands.</p> <p><strong>Broader Impacts:</strong>&nbsp; Our work contributes to the development of technology and methods that address communication challenges between humans and machines. Advances in tactile sensing could improve quality of life for prosthesis and assistive device users through the provision of rich artificial sensory feedback. Conformal, multimodal tactile sensor skins could enhance the functional capabilities of robotic manipulators intended for unstructured and/or limited-access environments (space, underwater, military, rescue, nuclear decommissioning, telesurgery). Tactile sensors that provide rich, dynamic information can complement computer vision-based approaches and are indispensable when line-of-sight is unavailable. Artificial haptic perception can be used to advance semi-autonomous robots and/or provide actionable information to remote teleoperators.</p> <p>Our work at Arizona State University and the University of California, Los Angeles strengthened collaborations across countries, universities, disciplines, and between academia and industry. The project produced five journal articles, one conference proceeding article, one doctoral dissertation, and two U.S. patents. Lab tours, exhibits, and school visits benefited hundreds of students from elementary to graduate levels, teachers, and members of the public. We developed low-cost, hands-on instructional robotics modules for elementary school students. Free slides, videos, and activities are provided at <a href="https://uclabiomechatronics.wordpress.com/outreach/">https://uclabiomechatronics.wordpress.com/outreach/</a>. A graduate-level course on the 'Control of Robotic Systems' was developed, which featured research supported by this grant.</p> <p>This project provided opportunities to educate the public on scientific approaches and the benefits of publicly funded research. The multimodal tactile sensor skin was featured in two international middle and high school textbooks. Our work was featured in articles by Science News, PCMag, TechCrunch, Engadget, Huffington Post UK, Newsweek, and Forbes, among others. Our work was featured in video segments for PBS NewsHour hosted by Miles O'Brien, an educational show called 'STEM Journals' hosted by Geoff Notkin, Voice of America (bilingual), NSF Science Nation (with podcast), an NSF robotics video, a documentary/visual art piece on humanity and technology called 'NEO,' and an episode of History Channel's longest-running nonfiction series 'Ancient Aliens.'</p><br> <p>            Last Modified: 11/26/2018<br>      Modified by: Veronica&nbsp;J&nbsp;Santos</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218167960_Figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218167960_Figure1--rgov-800width.jpg" title="Multimodal tactile sensor skin"><img src="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218167960_Figure1--rgov-66x44.jpg" alt="Multimodal tactile sensor skin"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Developed by Univ. of Washington and UCLA, the deformable, bioinspired, multimodal tactile sensor skin can be wrapped around a robotic or prosthetic device to provide a rich, dynamic sense of touch. All circular normal force taxels and rectangular shear force taxels can measure vibration.</div> <div class="imageCredit">UCLA Engineering, republished with credit in Langston, J. 'Flexible 'skin' can help robots, prosthetics perform everyday tasks by sensing shear force,' Univ. of Washington News, Oct. 17, 2017.</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Veronica&nbsp;J&nbsp;Santos</div> <div class="imageTitle">Multimodal tactile sensor skin</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218259592_Figure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218259592_Figure2--rgov-800width.jpg" title="Shear sensing skin concept of operation"><img src="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218259592_Figure2--rgov-66x44.jpg" alt="Shear sensing skin concept of operation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">(A) Shear sensing skin wrapped around a rigid finger and affixed by nailbed-like brackets. Shear force is measured by the difference in resistance between two opposing strain gauges. (B) Resistive liquid metal strain gauge. (C) Skin mounted on a fingertip. (D) Robot gripper with sensorized finger.</div> <div class="imageCredit">Yin, J., Aspinall, P., Santos, V.J., and Posner, J.D. 'Measuring dynamic shear force and vibration with a bioinspired tactile sensor skin,' IEEE Sensors. 2018:18(9):3544-3553. doi.org/10.1109/JSEN.2018.2811407.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Veronica&nbsp;J&nbsp;Santos</div> <div class="imageTitle">Shear sensing skin concept of operation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218322532_Figure3--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218322532_Figure3--rgov-800width.jpg" title="Shear sensing skin"><img src="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218322532_Figure3--rgov-66x44.jpg" alt="Shear sensing skin"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Resistive liquid metal strain gauges embedded in an elastomeric artificial skin. (A) An assembled shear sensor skin shows a pair of eutectic gallium-indium microfluidic channels embedded in a polydimethylsiloxane elastomer. (B) Microscopic view of the serpentine microchannel strain gauge.</div> <div class="imageCredit">Yin, J., Santos, V.J., and Posner, J.D. 'Bioinspired flexible microfluidic shear force sensor skin,' Sensors and Actuators A: Physical 2017:264:289-297. doi.org/10.1016/j.sna.2017.08.001.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Veronica&nbsp;J&nbsp;Santos</div> <div class="imageTitle">Shear sensing skin</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218411234_Figure4--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218411234_Figure4--rgov-800width.jpg" title="Digit coordination during a pouring task"><img src="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218411234_Figure4--rgov-66x44.jpg" alt="Digit coordination during a pouring task"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We investigated individual digit contributions to a three-digit task in which an object was rotationally manipulated against gravity. Subjects poured fluid from containers having different shapes and center of mass locations into a narrow-necked receptacle to the left of the container.</div> <div class="imageCredit">Manis, R. and Santos, V.J. 'Independent digit contributions to rotational manipulation in a three-digit pouring task requiring dynamic stability,' Exp Brain Res 2015:233(7):2195?2204. doi.org/10.1007/s00221-015-4289-6.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Veronica&nbsp;J&nbsp;Santos</div> <div class="imageTitle">Digit coordination during a pouring task</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218519452_Figure5--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218519452_Figure5--rgov-800width.jpg" title="'BairClaw' sensorized robot hand testbed"><img src="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218519452_Figure5--rgov-66x44.jpg" alt="'BairClaw' sensorized robot hand testbed"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We developed a highly sensorized robot hand testbed called the 'BairClaw.' (A) Tap-and-hold experiment against an instrumented plate. (B) Joint angles, (C) tactile sensor internal fluid pressure and microvibration, (D) tactile sensor skin deformation, and (E) normal contact force data are shown.</div> <div class="imageCredit">Hellman, R.B., Chang, E., Tanner, J., Helms Tillery, S.I., and Santos, V.J., 'A robot hand testbed designed for enhancing embodiment and functional neurorehabilitation of body schema in subjects with upper limb impairment or loss.' Front Hum Neurosci 2015:9(26):1-10. doi: 10.3389/fnhum.2015.00026.</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Veronica&nbsp;J&nbsp;Santos</div> <div class="imageTitle">'BairClaw' sensorized robot hand testbed</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218621054_Figure6--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218621054_Figure6--rgov-800width.jpg" title="Artificial haptic perception of edges"><img src="/por/images/Reports/POR/2018/1461630/1461630_10270088_1543218621054_Figure6--rgov-66x44.jpg" alt="Artificial haptic perception of edges"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Using a commercially available tactile sensor, we developed a method for the haptic perception of edges. Human-inspired, 'exploratory procedures' (EPs) were performed. (a) A constant fingertip contact angle was used. (b) static contact, (c, d) distal to proximal strokes, (e) radial to ulnar stroke.</div> <div class="imageCredit">Ponce Wong, R.D., Hellman, R.B., and Santos, V.J. 'Spatial asymmetry in tactile sensor skin deformation aids perception of edge orientation during haptic exploration,' IEEE Trans on Haptics, Special Issue on 'Haptics in Rehabilitation and Neural Engineering,' 2014:7(2):191?202.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Veronica&nbsp;J&nbsp;Santos</div> <div class="imageTitle">Artificial haptic perception of edges</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit:  Long-term research objective: Reduce the cognitive burden on the user of an upper extremity prosthesis. Research Goals: 1) Design, model, fabricate, and test a flexible, multimodal tactile sensor skin system for artificial fingertips using a multilayer microfluidic architecture, 2) Establish functional relationships between finger-object interactions and tactile sensor skin data for use in autonomous grip control algorithms, 3) Integrate the tactile sensor skin data into grip control algorithms and evaluate effectiveness for reducing the cognitive burden on prosthesis users.  Toward Goal #1: Collaborating with experts in microfluidics and soft lithography at the University of Washington, we developed a deformable, bioinspired, resistive microfluidic shear force sensor skin that can be conformally wrapped around curved surfaces, such as fingertips. When subjected to shear force, the skin continuum bulges like a human fingerpad, with one region experiencing tension and an opposing region experiencing compression. Tension and compression are sensed through differential measurements across pairs of liquid metal strain gauges embedded in polydimethylsiloxane that are strategically placed in opposition to one another and adjacent to a fixed nailbed. Placing shear force taxels (individual tactile sensing units) away from regions of direct finger-object contact leaves room for normal force taxels. Unaffected by delays associated with capacitive taxels, resistive taxels are highly sensitive to vibrations and transient events. The shear sensor skin has a dynamic range (up to 5N) of more than 10-bits and can quantify vibration up to 800Hz, which exceeds human capabilities. The skin was demonstrated to be sensitive to tactile events during robot manipulation tasks. The microfluidic skin is intrinsically flexible and immune to fatigue when subjected to repeated, large strains.  Toward Goal #2: We developed a model for the haptic perception of tactile directionality (direction of applied shear force), which is useful for perceiving movements of grasped objects relative to a hand-centric reference frame. Commercially available sensors were used to collect tactile data (fingerpad deformation, internal fluid pressure, vibration, temperature) during perturbations of an object in a two-digit grasp. Fingerpad deformation data were the most useful for encoding tactile directionality. After transforming fingerpad deformation data into grayscale tactile 'images,' we developed a regression model based on a convolutional neural network to estimate tactile directionality. Using the trained model, we demonstrated real-time haptic perception and context-appropriate decision-making on a robot testbed.  Toward Goal #3: Previously, we developed a highly sensorized, tendon-driven, anthropomorphic robot testbed called the 'BairClaw' to study proprioceptive and tactile sensor feedback during finger-object interactions. We began expanding this testbed to a three-digit hand to enable the testing of tactile sensor skins and grip control algorithms. Using the BairClaw, we conducted a single-digit study on isometric force production. Tendon tension was a function of baseline tension, joint torque ratios associated with the multi-joint digit design, and contact forces at the fingertip. We also completed the development of a low-cost, two-channel surface electromyography controller for the myoelectric control of artificial hands.  Broader Impacts:  Our work contributes to the development of technology and methods that address communication challenges between humans and machines. Advances in tactile sensing could improve quality of life for prosthesis and assistive device users through the provision of rich artificial sensory feedback. Conformal, multimodal tactile sensor skins could enhance the functional capabilities of robotic manipulators intended for unstructured and/or limited-access environments (space, underwater, military, rescue, nuclear decommissioning, telesurgery). Tactile sensors that provide rich, dynamic information can complement computer vision-based approaches and are indispensable when line-of-sight is unavailable. Artificial haptic perception can be used to advance semi-autonomous robots and/or provide actionable information to remote teleoperators.  Our work at Arizona State University and the University of California, Los Angeles strengthened collaborations across countries, universities, disciplines, and between academia and industry. The project produced five journal articles, one conference proceeding article, one doctoral dissertation, and two U.S. patents. Lab tours, exhibits, and school visits benefited hundreds of students from elementary to graduate levels, teachers, and members of the public. We developed low-cost, hands-on instructional robotics modules for elementary school students. Free slides, videos, and activities are provided at https://uclabiomechatronics.wordpress.com/outreach/. A graduate-level course on the 'Control of Robotic Systems' was developed, which featured research supported by this grant.  This project provided opportunities to educate the public on scientific approaches and the benefits of publicly funded research. The multimodal tactile sensor skin was featured in two international middle and high school textbooks. Our work was featured in articles by Science News, PCMag, TechCrunch, Engadget, Huffington Post UK, Newsweek, and Forbes, among others. Our work was featured in video segments for PBS NewsHour hosted by Miles O'Brien, an educational show called 'STEM Journals' hosted by Geoff Notkin, Voice of America (bilingual), NSF Science Nation (with podcast), an NSF robotics video, a documentary/visual art piece on humanity and technology called 'NEO,' and an episode of History Channel's longest-running nonfiction series 'Ancient Aliens.'       Last Modified: 11/26/2018       Submitted by: Veronica J Santos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
