<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: F: DKM: Collaborative Research: Scalable Middleware for Managing and Processing Big Data on Next Generation HPC Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>359999.00</AwardTotalIntnAmount>
<AwardAmount>359999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Managing and processing large volumes of data and gaining meaningful insights is a significant challenge facing the Big Data community.  Thus, it is critical that data-intensive computing middleware (such as Hadoop, HBase and Spark) to process such data are diligently designed, with high performance and scalability, in order to meet the growing demands of such Big Data applications.  While Hadoop, Spark and HBase are gaining popularity for processing Big Data applications, these middleware and the associated Big Data applications are not able to take advantage of the advanced features on modern High Performance Computing (HPC) systems widely deployed all over the world, including many of of the multi-Petaflop systems in the XSEDE environment.  Modern HPC systems and the associated middleware (such as MPI and Parallel File systems) have been exploiting the advances in HPC technologies (multi/many-core architectures, RDMA-enabled networking, NVRAMs and SSDs) during the last decade. However, Big Data middleware (such as Hadoop, HBase and Spark) have not embraced such technologies. These disparities are taking HPC and Big Data processing into "divergent trajectories." &lt;br/&gt;&lt;br/&gt;The proposed research, undertaken by a team of computer and application scientists from OSU and SDSC, aim to bring HPC and Big Data processing into a "convergent trajectory." The investigators will specifically address the following challenges: 1) designing novel communication and I/O runtime for Big Data processing while exploiting the features of modern multi-/many-core, networking and storage technologies; 2) redesigning Big Data middleware (such as Hadoop, HBase and Spark) to deliver performance and scalability on modern and next-generation HPC systems; and 3) demonstrating the benefits of the proposed approach for a set of driving Big Data applications on HPC system.  The proposed work targets four major workloads and applications in the Big Data community (namely data analytics, query, interactive, and iterative) using the popular Big Data middleware (Hadoop, HBase and Spark).  The proposed framework will be validated on a variety of Big Data benchmarks and applications.  The proposed middleware and runtimes will be made publicly available to the community.  The research enables curricular advancements via research in pedagogy for key courses in the new data analytics program at Ohio State and SDSC -- among the first of its kind nationwide.</AbstractNarration>
<MinAmdLetterDate>08/25/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1447861</AwardID>
<Investigator>
<FirstName>Amitava</FirstName>
<LastName>Majumdar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amitava Majumdar</PI_FULL_NAME>
<EmailAddress>majumdar@sdsc.edu</EmailAddress>
<PI_PHON>8585348356</PI_PHON>
<NSF_ID>000413199</NSF_ID>
<StartDate>08/25/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mahidhar</FirstName>
<LastName>Tatineni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mahidhar Tatineni</PI_FULL_NAME>
<EmailAddress>Mahidhar@sdsc.edu</EmailAddress>
<PI_PHON>8588223676</PI_PHON>
<NSF_ID>000643983</NSF_ID>
<StartDate>08/25/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~359999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Advances in technology have enabled us to collect large amounts of<br /> data from all walks of life.&nbsp; Managing and processing such large<br /> volumes of data, or "Big Data", and gaining meaningful insights is a<br /> significant challenge facing the Big Data community. This has<br /> significant impact in a wide range of domains including health care,<br /> bio-medical research, Internet search, finance and business<br /> informatics, and scientific computing. As data-gathering technologies<br /> and data-sources witness an explosion in the amount of input data, it<br /> is expected that in the future massive quantities of data in the order<br /> of hundreds or thousands of petabytes will need to be processed. Thus,<br /> it is critical that data-intensive computing middleware (such as<br /> Hadoop, Spark, and HBase) to process such data are diligently<br /> designed, with high performance and scalability, in order to meet the<br /> growing demands of such Big Data applications.<br /> <br /> While Hadoop, Spark, and HBase are gaining popularity for processing<br /> Big Data applications, these middleware and the associated Big Data<br /> applications are not able to fully take advantage of the advanced<br /> features on modern High Performance Computing (HPC) systems widely<br /> deployed all over the world, including many of of the multi-Petaflop<br /> systems in the XSEDE environment. Modern HPC systems and the<br /> associated middleware (such as MPI and Parallel File systems) have<br /> been exploiting the advances in HPC technologies (multi/many-core<br /> architectures, RDMA-enabled networking, NVRAMs and SSDs) during the<br /> last decade.&nbsp; However, Big Data middleware (such as Hadoop, Spark, and<br /> HBase) have not embraced such technologies. These disparities are<br /> taking HPC and Big Data processing into "divergent trajectories".<br /> This leads to the following broad challenges: "Can novel runtimes be<br /> used to redesign Big Data middleware (such as Hadoop, Spark, and<br /> HBase) to deliver performance and scalability on modern and<br /> next-generation HPC systems?"<br /> <br /> In this project, we have proposed new runtimes and re-designed the Big<br /> Data middleware stacks to take advantage of modern HPC technologies<br /> and systems. Challenges have been addressed in the following three<br /> directions: 1) Designed novel communication and I/O runtime for Big<br /> Data processing while exploiting the features of modern<br /> multi-/many-core, networking and storage technologies; 2) Redesigned<br /> Big Data middleware (such as Hadoop, Spark, and HBase) to deliver<br /> performance and scalability on modern and next-generation HPC systems;<br /> and 3) Demonstrated the benefits of the proposed approach for a set of<br /> driving Big Data benchmarks and applications on HPC systems.&nbsp; The<br /> proposed designs have brought HPC and Big Data processing into a<br /> "convergent trajectory".<br /> <br /> Contributions were made in all three areas and evaluated with a range<br /> of Big Data benchmarks and applications including - TeraGen, Sort,<br /> TeraSort, TestDFSIO, GroupBy, PUMA, YCSB, CCIndex, HiBench,<br /> CloudBurst, Deep Learning, and Astronomy.&nbsp; Some highlights of these<br /> results are:<br /> <br /> * The new RDMA-HDFS design delivers a speedup of 2.3x for TeraGen of<br /> &nbsp; 200 GBytes of dataset<br /> <br /> * There is an improvement of 25% for 120 GB Sort using the new<br /> &nbsp; MapReduce over Luster<br /> <br /> * The RDMA-Spark design over 1536 cores improves the HiBench PageRank<br /> &nbsp; time by 43%.<br /> <br /> * An astronomy application using 65GB dataset is accelerated by 21%<br /> &nbsp;&nbsp;using the proposed RDMA-Spark design.<br /> <br /> * Up to 2.4x speedup for the YCSB workload A with RDMA-based design<br /> &nbsp; for HBase.<br /> <br /> The results of this research (new designs, performance results,<br /> benchmarks, etc.) have been made available to the community through<br /> RDMA-Hadoop, RDMA-Spark, and RDMA-HBase libraries and OSU HiBD<br /> (High-Performance Big Data) benchmark suite (multiple versions and<br /> libraries).&nbsp; The latest versions of these libraries are currently<br /> running on many large-scale InfiniBand and RoCE systems including SDSC<br /> Comet. Currently, the HiBD libraries are being used by more than 275<br /> organizations in 34 countries. The HiBD libraries and the associated<br /> enhancements are being used by a large number of users of these<br /> systems to accelerate Big Data applications.<br /> &nbsp; <br /> In each of these releases, information about the tuned designs for<br /> various components (HDFS, MapReduce, Spark, HBase, etc.) has been<br /> shared with the HiBD user community through mailing lists. The<br /> applications-based results have been made available to the community<br /> through the "Performance" link of the HiBD project web page.&nbsp; In<br /> addition to the software distribution, the results have been presented<br /> at various conferences and events through talks, tutorials, and<br /> hands-on sessions.&nbsp; Multiple Ph.D and Masters students have performed<br /> research work and received their Ph.D and M.S. degrees as a part of<br /> this project.</p><br> <p>            Last Modified: 12/26/2017<br>      Modified by: Amitava&nbsp;Majumdar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Advances in technology have enabled us to collect large amounts of  data from all walks of life.  Managing and processing such large  volumes of data, or "Big Data", and gaining meaningful insights is a  significant challenge facing the Big Data community. This has  significant impact in a wide range of domains including health care,  bio-medical research, Internet search, finance and business  informatics, and scientific computing. As data-gathering technologies  and data-sources witness an explosion in the amount of input data, it  is expected that in the future massive quantities of data in the order  of hundreds or thousands of petabytes will need to be processed. Thus,  it is critical that data-intensive computing middleware (such as  Hadoop, Spark, and HBase) to process such data are diligently  designed, with high performance and scalability, in order to meet the  growing demands of such Big Data applications.    While Hadoop, Spark, and HBase are gaining popularity for processing  Big Data applications, these middleware and the associated Big Data  applications are not able to fully take advantage of the advanced  features on modern High Performance Computing (HPC) systems widely  deployed all over the world, including many of of the multi-Petaflop  systems in the XSEDE environment. Modern HPC systems and the  associated middleware (such as MPI and Parallel File systems) have  been exploiting the advances in HPC technologies (multi/many-core  architectures, RDMA-enabled networking, NVRAMs and SSDs) during the  last decade.  However, Big Data middleware (such as Hadoop, Spark, and  HBase) have not embraced such technologies. These disparities are  taking HPC and Big Data processing into "divergent trajectories".  This leads to the following broad challenges: "Can novel runtimes be  used to redesign Big Data middleware (such as Hadoop, Spark, and  HBase) to deliver performance and scalability on modern and  next-generation HPC systems?"    In this project, we have proposed new runtimes and re-designed the Big  Data middleware stacks to take advantage of modern HPC technologies  and systems. Challenges have been addressed in the following three  directions: 1) Designed novel communication and I/O runtime for Big  Data processing while exploiting the features of modern  multi-/many-core, networking and storage technologies; 2) Redesigned  Big Data middleware (such as Hadoop, Spark, and HBase) to deliver  performance and scalability on modern and next-generation HPC systems;  and 3) Demonstrated the benefits of the proposed approach for a set of  driving Big Data benchmarks and applications on HPC systems.  The  proposed designs have brought HPC and Big Data processing into a  "convergent trajectory".    Contributions were made in all three areas and evaluated with a range  of Big Data benchmarks and applications including - TeraGen, Sort,  TeraSort, TestDFSIO, GroupBy, PUMA, YCSB, CCIndex, HiBench,  CloudBurst, Deep Learning, and Astronomy.  Some highlights of these  results are:    * The new RDMA-HDFS design delivers a speedup of 2.3x for TeraGen of    200 GBytes of dataset    * There is an improvement of 25% for 120 GB Sort using the new    MapReduce over Luster    * The RDMA-Spark design over 1536 cores improves the HiBench PageRank    time by 43%.    * An astronomy application using 65GB dataset is accelerated by 21%    using the proposed RDMA-Spark design.    * Up to 2.4x speedup for the YCSB workload A with RDMA-based design    for HBase.    The results of this research (new designs, performance results,  benchmarks, etc.) have been made available to the community through  RDMA-Hadoop, RDMA-Spark, and RDMA-HBase libraries and OSU HiBD  (High-Performance Big Data) benchmark suite (multiple versions and  libraries).  The latest versions of these libraries are currently  running on many large-scale InfiniBand and RoCE systems including SDSC  Comet. Currently, the HiBD libraries are being used by more than 275  organizations in 34 countries. The HiBD libraries and the associated  enhancements are being used by a large number of users of these  systems to accelerate Big Data applications.      In each of these releases, information about the tuned designs for  various components (HDFS, MapReduce, Spark, HBase, etc.) has been  shared with the HiBD user community through mailing lists. The  applications-based results have been made available to the community  through the "Performance" link of the HiBD project web page.  In  addition to the software distribution, the results have been presented  at various conferences and events through talks, tutorials, and  hands-on sessions.  Multiple Ph.D and Masters students have performed  research work and received their Ph.D and M.S. degrees as a part of  this project.       Last Modified: 12/26/2017       Submitted by: Amitava Majumdar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
