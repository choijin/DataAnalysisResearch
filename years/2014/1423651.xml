<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Visual Situation Recognition: An Integration of Deep Networks and Analogy-Making</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>449754.00</AwardTotalIntnAmount>
<AwardAmount>475754</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates a novel approach to building computer systems that can recognize visual situations.  While much effort in computer vision has focused on identifying isolated objects in images, what people actually do is recognize coherent situations--collections of objects and their interrelations that, taken together, correspond to a known concept, such as "a child's birthday party," or "a man walking a dog on the beach," or "two people about to fight," or "a blind person crossing the street."  Situation recognition by humans may appear on the surface to be effortless, but it relies on a complex dynamic interplay among human abilities to perceive objects, systems of relationships among objects, and analogies with stored knowledge and memories.  No computer vision system yet comes close to capturing these human abilities.  Enabling computers to flexibly recognize visual situations would create a flood of important applications in fields as diverse as medical diagnosis, interpretation of scientific imagery, enhanced human-computer interaction, and personal information organization.&lt;br/&gt;&lt;br/&gt;The approach explored in this project integrates two previously studied approaches:  brain-inspired neural networks for lower-level vision and cognitive-level models of concepts and analogy-making.  In this integrated architecture, recognizing situations--via analogies with stored conceptual structures--will be a dynamic process in which bottom-up (perceptual) and top-down (conceptual) influences affect one another as perception unfolds.  If successful, this system will be able to recognize visual situations in a way that scales well with the complexity of the scene and the abstract concept being recognized.  As part of this project, a number of benchmark image datasets--reflecting different abstract visual situations--will be collected to evaluate the recognition system.  In addition, the PI will design and run a public competition on automated recognition of visual situations, using the collected datasets.  This competition will spur research on this topic, and help researchers working in this area evaluate the success of various methods and gauge the current state of the art on abstract visual recognition.  All source code and benchmarking databases developed in this project will be made publicly available via the web.</AbstractNarration>
<MinAmdLetterDate>08/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/15/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423651</AwardID>
<Investigator>
<FirstName>Melanie</FirstName>
<LastName>Mitchell</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Melanie Mitchell</PI_FULL_NAME>
<EmailAddress>mm@santafe.edu</EmailAddress>
<PI_PHON>9712055307</PI_PHON>
<NSF_ID>000461779</NSF_ID>
<StartDate>08/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Portland State University</Name>
<CityName>Portland</CityName>
<ZipCode>972070751</ZipCode>
<PhoneNumber>5037259900</PhoneNumber>
<StreetAddress>1600 SW 4th Ave</StreetAddress>
<StreetAddress2><![CDATA[Attn: Sponsored Projects Admin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052226800</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PORTLAND STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052226800</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Portland State University]]></Name>
<CityName/>
<StateCode>OR</StateCode>
<ZipCode>972070751</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~449754</FUND_OBLG>
<FUND_OBLG>2016~10000</FUND_OBLG>
<FUND_OBLG>2018~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>While much of computer vision has focused on object recognition, humans don't simply recognize objects; they recognize coherent situations whose categories are familiar concepts, such as ?Walking a Dog?; "Shaking Hands"; "A Ping Pong Match"; or "A Protest March".&nbsp; Each of these situations involves various entities and actions, and stereotypical relationships among the entities and actions. The relationships can be spatial, temporal, or more abstract semantic connections.&nbsp;</p> <p>In this project, we constructed a computer program, called "Situate", aimed at recognizing a given visual situation in photographs.&nbsp; In describing how Situate works, here we use the example of the "Walking a Dog" situation, in which the relevant object categories are "dog-walker", "leash", and "dog".<br /> <br /> As illustrated in the accompanying figure, in its training phase Situate is given a training set of photographs containing instances of the situation, each of which has been manually annotated to identify the relevant objects with bounding boxes.&nbsp; The program learns two types of models from these training examples: (1) Object appearance models: for each relevant object category, a deep neural network that, given an image region, output the confidence that that region contains an object of that category (e.g., "dog"). (2) A situation model: a model that expresses the joint probability of the location, size, and aspect ratio of the relevant objects.<br /> <br /> In its running (or "test") phase Situate is given a new image that may or may not contain an instance of the given situation (e.g., "Walking a Dog"). The program's output is a "grounding" of the situation in the image---that is, bounding boxes for each of the relevant object categories, if they are detected in the image---and a numerical score indicating how well this grounding fits the learned situation model. The higher the score, the more confident the program is that it has detected an instance of the given situation.<br /> <br /> To construct groundings like the one in the accompanying figure, Situate uses an dynamic, active perception method.&nbsp; At each time step of a run, the program chooses one of the relevant object categories, and uses its learned situation model to sample a region in the input image. That region is then scored based on the learned object-appearance model and the situation model.&nbsp; If its score is above a learned threshold, the region is marked as "detected" as the chosen object category.&nbsp; At that point, the probabilistic situation model is updated to be conditioned on that detected object.&nbsp; In this way, the program uses both its expectations (according to its situation model) and what it has perceived in the given image to influence where it looks next.&nbsp; Our hypothesis is that this active approach, which continually incorporates prior knowledge and perceived context, will be more successful in detecting and grounding visual situations than approaches that lack such an active perception method. <br /> <br /> To test this hypothesis, we performed a set of experiments comparing Situate's performance with a baseline object-detection method, as well as with an alternative, previously published visual grounding method. &nbsp;The experiments involved four visual situations, each with a different set of objects and relationships: "Walking a Dog", "Playing Ping Pong", "Shaking Hands", and "Leading a Horse".&nbsp; The training and testing images containing instances of each of these situations were collected and annotated by our group.&nbsp; For each situation, we ran each method on the "positive" images---those that contained instances of the situation---and a much larger set of negative images.&nbsp; Each of the methods input an image and output a score indicating the confidence that the image contained an instance of that situation.&nbsp; We measured the performance of each method in terms of the ranking of positive vs. negative images based on the output scores.&nbsp; An ideal method would rank all positive images higher than negative images.&nbsp; We found that our method, though far from this perfect ideal, performed significantly better than the baseline and alternative method on three out of four of our test situations, thus supporting our hypothesis. We also analyzed the errors each of the methods made in order to understand what factors gave rise to the differences in performance.&nbsp; We found that Situate?s active perception method indeed was what gave it an advantage over the other methods. <br /> <br /> The intellectual merit of this project is a novel effort to integrate a neural-network-based low-level vision system with a cognitively inspired architecture for high-level perception---a hybrid architecture that is a step towards recognizing complex visual concepts.&nbsp; This work has had broader impact as well: the project has produced new ideas and methods for computer vision, an area with large impacts in many areas of society, and has resulted in the training of graduate and undergraduate students, the availability of all source code and image datasets, and the public dissemination of new ideas via articles and lectures.</p><br> <p>            Last Modified: 12/30/2019<br>      Modified by: Melanie&nbsp;Mitchell</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1423651/1423651_10330983_1577737242726_SituatePipeline--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1423651/1423651_10330983_1577737242726_SituatePipeline--rgov-800width.jpg" title="Situate's Training and Testing Process"><img src="/por/images/Reports/POR/2019/1423651/1423651_10330983_1577737242726_SituatePipeline--rgov-66x44.jpg" alt="Situate's Training and Testing Process"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An illustration of Situate's training and testing processes.</div> <div class="imageCredit">Melanie Mitchell</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Melanie&nbsp;Mitchell</div> <div class="imageTitle">Situate's Training and Testing Process</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ While much of computer vision has focused on object recognition, humans don't simply recognize objects; they recognize coherent situations whose categories are familiar concepts, such as ?Walking a Dog?; "Shaking Hands"; "A Ping Pong Match"; or "A Protest March".  Each of these situations involves various entities and actions, and stereotypical relationships among the entities and actions. The relationships can be spatial, temporal, or more abstract semantic connections.   In this project, we constructed a computer program, called "Situate", aimed at recognizing a given visual situation in photographs.  In describing how Situate works, here we use the example of the "Walking a Dog" situation, in which the relevant object categories are "dog-walker", "leash", and "dog".    As illustrated in the accompanying figure, in its training phase Situate is given a training set of photographs containing instances of the situation, each of which has been manually annotated to identify the relevant objects with bounding boxes.  The program learns two types of models from these training examples: (1) Object appearance models: for each relevant object category, a deep neural network that, given an image region, output the confidence that that region contains an object of that category (e.g., "dog"). (2) A situation model: a model that expresses the joint probability of the location, size, and aspect ratio of the relevant objects.    In its running (or "test") phase Situate is given a new image that may or may not contain an instance of the given situation (e.g., "Walking a Dog"). The program's output is a "grounding" of the situation in the image---that is, bounding boxes for each of the relevant object categories, if they are detected in the image---and a numerical score indicating how well this grounding fits the learned situation model. The higher the score, the more confident the program is that it has detected an instance of the given situation.    To construct groundings like the one in the accompanying figure, Situate uses an dynamic, active perception method.  At each time step of a run, the program chooses one of the relevant object categories, and uses its learned situation model to sample a region in the input image. That region is then scored based on the learned object-appearance model and the situation model.  If its score is above a learned threshold, the region is marked as "detected" as the chosen object category.  At that point, the probabilistic situation model is updated to be conditioned on that detected object.  In this way, the program uses both its expectations (according to its situation model) and what it has perceived in the given image to influence where it looks next.  Our hypothesis is that this active approach, which continually incorporates prior knowledge and perceived context, will be more successful in detecting and grounding visual situations than approaches that lack such an active perception method.     To test this hypothesis, we performed a set of experiments comparing Situate's performance with a baseline object-detection method, as well as with an alternative, previously published visual grounding method.  The experiments involved four visual situations, each with a different set of objects and relationships: "Walking a Dog", "Playing Ping Pong", "Shaking Hands", and "Leading a Horse".  The training and testing images containing instances of each of these situations were collected and annotated by our group.  For each situation, we ran each method on the "positive" images---those that contained instances of the situation---and a much larger set of negative images.  Each of the methods input an image and output a score indicating the confidence that the image contained an instance of that situation.  We measured the performance of each method in terms of the ranking of positive vs. negative images based on the output scores.  An ideal method would rank all positive images higher than negative images.  We found that our method, though far from this perfect ideal, performed significantly better than the baseline and alternative method on three out of four of our test situations, thus supporting our hypothesis. We also analyzed the errors each of the methods made in order to understand what factors gave rise to the differences in performance.  We found that Situate?s active perception method indeed was what gave it an advantage over the other methods.     The intellectual merit of this project is a novel effort to integrate a neural-network-based low-level vision system with a cognitively inspired architecture for high-level perception---a hybrid architecture that is a step towards recognizing complex visual concepts.  This work has had broader impact as well: the project has produced new ideas and methods for computer vision, an area with large impacts in many areas of society, and has resulted in the training of graduate and undergraduate students, the availability of all source code and image datasets, and the public dissemination of new ideas via articles and lectures.       Last Modified: 12/30/2019       Submitted by: Melanie Mitchell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
