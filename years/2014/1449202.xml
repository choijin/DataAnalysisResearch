<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: A Corpus of Aligned Speech and ANS Sensor Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>49990.00</AwardTotalIntnAmount>
<AwardAmount>49990</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Despite a sizeable literature on emotional speech and speech under stress, little is understood about how features in continuous speech vary with subtle and real-world-relevant changes in physiological state within any particular speaker. This EArly Grant for Exploratory Research relates speech features to direct measures of physiological activation, rather than to categorical hand-annotated labels of emotion or state. The study collects and analyzes a corpus of speech and autonomic nervous system (ANS) sensor data to discover what changes occur in speech features when a person is exposed to different activation-relevant emotional, cognitive, stress-related conditions. The broader significance and impact is discovery of cues in speech that can be used to estimate changes in a speaker's physiological activation level when no sensors are available. Applications include health care (monitoring physical, mental, cognitive states), education and learning (monitoring engagement), social interaction (monitoring activation level), and law enforcement/intelligence (monitoring behavioral changes of high interest individuals).&lt;br/&gt;&lt;br/&gt;In Phase 1 (Corpus Collection), the project creates a 40-subject corpus of time-aligned speech and physiological signals. Activation is measured using state-of-the-art methods to extract cardiovascular (ECG), blood pressure, respiration rate, and skin conductance signals. Each subject participates in five conditions: (1) neutral baseline; (2) emotional (description of emotionally salient pictures); (3) stressed (speaking task incentivized for accuracy and completion time); (4) cognitive load (speaking task with a visual distractor, incentivized for task completion and distractor task accuracy); and (5) computer-directed speech (task requiring perfect recognition from a speech recognizer). In Phase 2 (Analysis), sensor output is post-processed to calibrate the signals and look for changes. These changes are then compared to a range of automatically extracted features (based on acoustics, prosody, discourse patterns, and disfluency patterns) from the time-aligned speech. Analyses and machine learning experiments then examine which speech feature changes correlate with changes in sensor output, both within and across speakers. Results shed light on how information from natural continuous speech can be used to estimate changes in a speaker?s physiological activation level in ongoing, subtle and everyday contexts.</AbstractNarration>
<MinAmdLetterDate>07/22/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/22/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1449202</AwardID>
<Investigator>
<FirstName>Elizabeth</FirstName>
<LastName>Shriberg</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elizabeth E Shriberg</PI_FULL_NAME>
<EmailAddress>elizabeth.shriberg@sri.com</EmailAddress>
<PI_PHON>6508595490</PI_PHON>
<NSF_ID>000338871</NSF_ID>
<StartDate>07/22/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andreas</FirstName>
<LastName>Kathol</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andreas Kathol</PI_FULL_NAME>
<EmailAddress>kathol@speech.sri.com</EmailAddress>
<PI_PHON>6508592915</PI_PHON>
<NSF_ID>000653094</NSF_ID>
<StartDate>07/22/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Massimiliano</FirstName>
<LastName>de Zambotti</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Massimiliano de Zambotti</PI_FULL_NAME>
<EmailAddress>massimiliano.dezambotti@sri.com</EmailAddress>
<PI_PHON>6508592651</PI_PHON>
<NSF_ID>000672764</NSF_ID>
<StartDate>07/22/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SRI International</Name>
<CityName>Menlo Park</CityName>
<ZipCode>940253493</ZipCode>
<PhoneNumber>7032478529</PhoneNumber>
<StreetAddress>333 RAVENSWOOD AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009232752</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SRI INTERNATIONAL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009232752</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SRI International]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>940253453</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~49990</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="tinyMCEContent"> <p>The outcome of the project is a unique, time-aligned corpus that contains speech, video, and biological sensor data for 43 adult subjects&nbsp; (25 female, 18 male) representing a range of ages.&nbsp; Each subject sessoin lasted over an hour and covered a fixed set of different speaking conditions.&nbsp;&nbsp; Conditions include neutral speech, speech in different emotional conditions,  speech under stress, speech under cognitive load, and computer-directed  speech. &nbsp; Biological sensors included high-quality electrocardiogram, blood pressure, respiration, and skin conductance data. Heart beat data  was also independently recorded using fitbit wearable devices for comparison purposes.&nbsp; Participants also filled out detailed surveys about their health and habits.</p> <p>Data were collected in a laboratory setting.&nbsp; Because of high degree of experimental control in the design and stimuli, and well as high quality microphones and sensors, the corpus offers a unique opportunity to study a variety of questions about the relationship between emotion and cognitive state, and speech/video/biological signals.</p> <p>The project funded corpus collection only, but the plan is to find additional funding for corpus post-processing and analysis, the outcomes of which would be shared with the community via publications.&nbsp;&nbsp; An additional proposed outcome of the project, dependent on obtaining further funding,&nbsp; is the sharing of signal outputs with collaborators in the community for joint research.</p> </div> <dl class="clearing"><dd> </dd></dl><br> <p>            Last Modified: 09/28/2015<br>      Modified by: Elizabeth&nbsp;E&nbsp;Shriberg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The outcome of the project is a unique, time-aligned corpus that contains speech, video, and biological sensor data for 43 adult subjects  (25 female, 18 male) representing a range of ages.  Each subject sessoin lasted over an hour and covered a fixed set of different speaking conditions.   Conditions include neutral speech, speech in different emotional conditions,  speech under stress, speech under cognitive load, and computer-directed  speech.   Biological sensors included high-quality electrocardiogram, blood pressure, respiration, and skin conductance data. Heart beat data  was also independently recorded using fitbit wearable devices for comparison purposes.  Participants also filled out detailed surveys about their health and habits.  Data were collected in a laboratory setting.  Because of high degree of experimental control in the design and stimuli, and well as high quality microphones and sensors, the corpus offers a unique opportunity to study a variety of questions about the relationship between emotion and cognitive state, and speech/video/biological signals.  The project funded corpus collection only, but the plan is to find additional funding for corpus post-processing and analysis, the outcomes of which would be shared with the community via publications.   An additional proposed outcome of the project, dependent on obtaining further funding,  is the sharing of signal outputs with collaborators in the community for joint research.          Last Modified: 09/28/2015       Submitted by: Elizabeth E Shriberg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
