<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Scalable Statistical Validation and Uncertainty Quantification for Large Spatio-Temporal Datasets</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>75090.00</AwardTotalIntnAmount>
<AwardAmount>75090</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computer simulations, satellites and various other technological advances have paved the way into unprecedented scientific territory by generating volumes of previously uncollectable datasets.  Properly utilizing these new data products to promote scientific discovery requires that they, first, be (i) validated and (ii) paired with an appropriate measure of uncertainty.  Validating a data product entails comparing "synthetic" data (e.g., computer model simulations, remote sensing measurements, or statistical predictions) with observational counterparts to substantiate the digital data for its use in scientific discovery.  Uncertainty quantification (UQ) is a necessary component to validation and entails accounting for and stating the uncertainties associated with scientific conclusions derived from digital or observational data.  The purpose of this research is to promote scientific discovery using digital data products by developing statistical methods to perform validation and uncertainty quantification.&lt;br/&gt;&lt;br/&gt;Given the strong need to perform, and the substantial challenges facing, statistical validation and UQ, this research will (i) develop new validation strategies for simulated and digital datasets based on scientifically motivated features; (ii) develop multivariate spatio-temporal statistical models that can be used to implement, and perform UQ for spatio-temporal data products; and, (iii) develop scalable computation techniques for fitting the developed spatio-temporal statistical models.  This research will implement these techniques on data products in atmospheric, agricultural and environmental sciences to facilitate their use in scientific inquiry.</AbstractNarration>
<MinAmdLetterDate>07/25/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/06/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1417857</AwardID>
<Investigator>
<FirstName>Douglas</FirstName>
<LastName>Nychka</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Douglas W Nychka</PI_FULL_NAME>
<EmailAddress>nychka@mines.edu</EmailAddress>
<PI_PHON>3037253199</PI_PHON>
<NSF_ID>000191118</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Douglas</FirstName>
<LastName>Nychka</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Douglas W Nychka</PI_FULL_NAME>
<EmailAddress>nychka@mines.edu</EmailAddress>
<PI_PHON>3037253199</PI_PHON>
<NSF_ID>000191118</NSF_ID>
<StartDate>07/25/2014</StartDate>
<EndDate>08/06/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stephan</FirstName>
<LastName>Sain</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephan R Sain</PI_FULL_NAME>
<EmailAddress>ssain@ucar.edu</EmailAddress>
<PI_PHON>3034971709</PI_PHON>
<NSF_ID>000408700</NSF_ID>
<StartDate>07/25/2014</StartDate>
<EndDate>08/06/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University Corporation For Atmospheric Res</Name>
<CityName>Boulder</CityName>
<ZipCode>803012252</ZipCode>
<PhoneNumber>3034971000</PhoneNumber>
<StreetAddress>3090 Center Green Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078339587</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY CORPORATION FOR ATMOSPHERIC RESEARCH</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>078339587</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[National Center for Atmospheric Research]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803055602</ZipCode>
<StreetAddress><![CDATA[1850 Table Mesa Dr.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~75090</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Given the vast amount of digital and observational data produced to study atmospheric, environmental and agricultural processes, the research funded by this award seeks to develop statistical methodology for the validation and the uncertainty quantification for these kind of data products. This analysis is important for problems where data is located at irregular locations but one needs estimates at locations on a regular grid or at locations that have not been observed. A contribution of a statistical method is not only to provide good estimates of the spatial variables but also quantify the uncertainty in the estimate as a "plus or minus" error bound. This research supports the statistical analysis of spatial datasets by increasing the size that can be handled or dramatically speeding up the analysis when data is of a moderate size.<br />A spatial model (<em>LatticeKrig</em>) was extended to spherical geometry and also modified to handle heteroegneous (nonstationary) behavior in spatial data. Part of LatticeKrig's design is&nbsp; based on dividing up the spatial region with regular grids of points (a lattice) and Figure 1 is an example of how this was done for the sphere (i.e. global data).&nbsp; The LatticeKrig model is designed to handle larger data sets and in this work was tested on more that 50,000 spatial locations. This sample size is at least a&nbsp; factor of 10 larger than the capability for most other statistical methods and allows for the analysis of improtant climate data sets and climate model simulations.&nbsp; Moreover these innovations were implemented in <em>LatticeKrig</em>, an open source, extensively documented, package in the R language. Thus these methods are easy to use and available to a broad community.<br />A second part of this research is to speed the computation of more standard spatial statistics methods. One strategy extended the software package <em>fields</em>, also an open source R package, by harnessing graphics processing unit (GPU) hardware for speeding the computation. For example using a GPU decreased the time for a spatial analysis by a factor of 10 compared to the standard implementation. A completely different strategy also explored was using many R programs in parallel and on a supercomputer (Cheyenne and Yellowstone at the National Center for Atmospheric Research) to compute independent parts of a spatial data analysis simultaneously. Here we showed timing where 1000 parallel R sessions complete their tasks in about same time as a single R session assigned one task. This is effectively a speedup by a factor of 1000 and Figure 2 summarizes these results.&nbsp; Completing a spatial&nbsp; analysis quickly has two impacts. The first is to complete a computation that might otherwise not be possible using current algorithms and software. The second is by&nbsp; decreasing the time in order for a scientist to interact with the data in a more flexible and fluid manner. For example, from the analysis summarized in Figure 2 a computation that would take roughly 90 hours ( more than 3 days)&nbsp; with a single R session is now completed in under 8 minutes.&nbsp; Getting back results this quickly often changes the entire way one thinks about exploring a data set, checking for data errors, and gaining insight.<br />The third impact of this research is on training graduate students and creating easy to use software. As an example of this acitivity two statistics graduate students from the University of Colorado,&nbsp; A. Weins and M. Krock, created atutorial for the&nbsp;<em> fields </em>R package that explains the concepts of interpreting spatial data and also gives many examples in R for analysis. Figure 3 is an example of a diagram that walks the reader through the key parts of a variogram plot, an often confusing, but also valuable technique that quantifies how spatial correlation between two&nbsp; variables decreases with the distance of&nbsp; their separation. Overall the impact is to make the fields software accessible to a broader audience by giving many examples and illustrations of using spatial statistics. In the process the graduate students as authors also increased their expertise in this area.</p><br> <p>            Last Modified: 11/05/2018<br>      Modified by: Douglas&nbsp;W&nbsp;Nychka</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445056634_balls--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445056634_balls--rgov-800width.jpg" title="Figure 1 Three levels from a  multi-resolution node distribution for the sphere"><img src="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445056634_balls--rgov-66x44.jpg" alt="Figure 1 Three levels from a  multi-resolution node distribution for the sphere"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The first three levels of nodes  (green points) used to locate basis functions for the LatticeKrig model on the sphere.</div> <div class="imageCredit">D. Nychka</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Douglas&nbsp;W&nbsp;Nychka</div> <div class="imageTitle">Figure 1 Three levels from a  multi-resolution node distribution for the sphere</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445151218_Timing--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445151218_Timing--rgov-800width.jpg" title="Timing for a spatial data analysis using the HPC4Stats framework."><img src="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445151218_Timing--rgov-66x44.jpg" alt="Timing for a spatial data analysis using the HPC4Stats framework."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Timing results for fitting local stationary covariances to 1000 grid boxes as a function of the number of cores. In this case the number of cores is equal to the number of worker R sessions.</div> <div class="imageCredit">D. Nychka</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Douglas&nbsp;W&nbsp;Nychka</div> <div class="imageTitle">Timing for a spatial data analysis using the HPC4Stats framework.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445257485_tutorial--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445257485_tutorial--rgov-800width.jpg" title="Important features  of a variogram function: a figure from the fields vignette."><img src="/por/images/Reports/POR/2018/1417857/1417857_10323312_1541445257485_tutorial--rgov-66x44.jpg" alt="Important features  of a variogram function: a figure from the fields vignette."></a> <div class="imageCaptionContainer"> <div class="imageCaption">This figure is a pictorial explanation of the variogram function, ubiquitous in spatial modeling, and how its features are related to the covariance parameters standard in the fields package. This figure is part of the fields tutorial ( vignette) developed under this award.</div> <div class="imageCredit">D. Nychka</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Douglas&nbsp;W&nbsp;Nychka</div> <div class="imageTitle">Important features  of a variogram function: a figure from the fields vignette.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Given the vast amount of digital and observational data produced to study atmospheric, environmental and agricultural processes, the research funded by this award seeks to develop statistical methodology for the validation and the uncertainty quantification for these kind of data products. This analysis is important for problems where data is located at irregular locations but one needs estimates at locations on a regular grid or at locations that have not been observed. A contribution of a statistical method is not only to provide good estimates of the spatial variables but also quantify the uncertainty in the estimate as a "plus or minus" error bound. This research supports the statistical analysis of spatial datasets by increasing the size that can be handled or dramatically speeding up the analysis when data is of a moderate size. A spatial model (LatticeKrig) was extended to spherical geometry and also modified to handle heteroegneous (nonstationary) behavior in spatial data. Part of LatticeKrig's design is  based on dividing up the spatial region with regular grids of points (a lattice) and Figure 1 is an example of how this was done for the sphere (i.e. global data).  The LatticeKrig model is designed to handle larger data sets and in this work was tested on more that 50,000 spatial locations. This sample size is at least a  factor of 10 larger than the capability for most other statistical methods and allows for the analysis of improtant climate data sets and climate model simulations.  Moreover these innovations were implemented in LatticeKrig, an open source, extensively documented, package in the R language. Thus these methods are easy to use and available to a broad community. A second part of this research is to speed the computation of more standard spatial statistics methods. One strategy extended the software package fields, also an open source R package, by harnessing graphics processing unit (GPU) hardware for speeding the computation. For example using a GPU decreased the time for a spatial analysis by a factor of 10 compared to the standard implementation. A completely different strategy also explored was using many R programs in parallel and on a supercomputer (Cheyenne and Yellowstone at the National Center for Atmospheric Research) to compute independent parts of a spatial data analysis simultaneously. Here we showed timing where 1000 parallel R sessions complete their tasks in about same time as a single R session assigned one task. This is effectively a speedup by a factor of 1000 and Figure 2 summarizes these results.  Completing a spatial  analysis quickly has two impacts. The first is to complete a computation that might otherwise not be possible using current algorithms and software. The second is by  decreasing the time in order for a scientist to interact with the data in a more flexible and fluid manner. For example, from the analysis summarized in Figure 2 a computation that would take roughly 90 hours ( more than 3 days)  with a single R session is now completed in under 8 minutes.  Getting back results this quickly often changes the entire way one thinks about exploring a data set, checking for data errors, and gaining insight. The third impact of this research is on training graduate students and creating easy to use software. As an example of this acitivity two statistics graduate students from the University of Colorado,  A. Weins and M. Krock, created atutorial for the  fields R package that explains the concepts of interpreting spatial data and also gives many examples in R for analysis. Figure 3 is an example of a diagram that walks the reader through the key parts of a variogram plot, an often confusing, but also valuable technique that quantifies how spatial correlation between two  variables decreases with the distance of  their separation. Overall the impact is to make the fields software accessible to a broader audience by giving many examples and illustrations of using spatial statistics. In the process the graduate students as authors also increased their expertise in this area.       Last Modified: 11/05/2018       Submitted by: Douglas W Nychka]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
