<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Enabling robust visual intelligence using propagators to model human competence</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The investigators approach the question of robust intelligence by asking what it is that makes humans both intelligent and robustly intelligent.  Part of the answer is that humans are uniquely able to see, to report on what they see, and to use visual events--- both real and imagined---to answer questions on demand and to develop a common sense understanding of the physical world.  If robust human-level intelligence is to be understood and engineered, then it is necessary to understand human visual competence.  To understand human visual competence, it is necessary to understand how the architecture of the brain enables fragmentary and ambiguous perceptions to be brought into alignment with expectations so as to produce an understanding of the visual world.&lt;br/&gt;&lt;br/&gt;To take understanding of human vision to the next level, the investigators model the human visual system using the propagator paradigm, a label for a collection of ideas suited to the computational problems faced by vision systems.  Propagators themselves are stateless, which makes them appropriate for operation on retinotopic arrays.  Also, propagators connect cells in which information monotonically increases, assuring convergence.  Most importantly, bi-directonal information flow lies at the core of the propagator paradigm, so when augmented with new capabilities tailored specifically to vision processing, visual information flows not only from the bottom up but also from the top down and, in general, from any module to any other module, just as information flows to and from the many brain centers devoted to vision in the human brain.  The investigators note, for example, that the lateral geniculate, a relay station for information flowing from the retina to primary visual cortex, receives most of its input from the primary visual cortex itself.&lt;br/&gt;&lt;br/&gt;The investigators are motivated not only by a desire to understand human vision, but also by a desire to build vision applications now far beyond the state of the art.  To drive their work, they concentrate on a dynamic scene understanding problem: given an urban scene and one or more stationary cameras, recognize actions such as walk, run, stop, put down, pick up, drop, give, take, follow, enter, and leave.</AbstractNarration>
<MinAmdLetterDate>06/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1421065</AwardID>
<Investigator>
<FirstName>Patrick</FirstName>
<LastName>Winston</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patrick H Winston</PI_FULL_NAME>
<EmailAddress>phw@mit.edu</EmailAddress>
<PI_PHON>6172536754</PI_PHON>
<NSF_ID>000308018</NSF_ID>
<StartDate>06/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Human visual intelligence is robust. Our visual intelligence is versatile in the variety of problems it is able to solve and in the variety of its operating conditions, such as recognizing objects at varying distances in bright and dim light. Human vision is flexible, adapting facilely to new tasks like driving a car, even tasks that have had little significance in our recent evolutionary development.&nbsp; Human vision is also provides richly compositional explanations for its findings. We understand what we see not just in terms of captions or labels, but in terms of parts, material properties, support relations and many other mutually-reinforcing descriptions.<br /><br />In this project, we addressed the problem of understanding and modeling these robust properties of natural vision in AI vision systems. We believe that in order to understand and model robust visual intelligence, we must make principled choices about the mechanisms we apply to the problem, guided by natural vision&rsquo;s computational imperatives. We considered several key operating constraints that natural vision systems are subject to, and conjectured that the core computational process of natural vision is a process of alignment: spreading information between computational specialists by aligning internal state with other specialists, through shared representations. We further conjecture that this process of alignment occurs bidirectionally between specialists, and that the alignment process shares information among all sensory modes in the perception system. This necessarily blurs the boundary between vision and other sensory apparatus, allowing the specialized computational machinery of many senses may be brought to bear on problems with strictly-visual inputs.<br /><br />We divided our work on this project into four phases. In the first phase, we developed a propagator architecture based on the work of Sussman and Radul (2009) and implemented a system using that architecture that can perform scene-understanding tasks. The system can propagate constraints to infer locations and heights of pedestrians and locations of occluding objects in an outdoor urban scene. <br /><br />We then began investigating how to improve our propagator architecture through the application of neural networks. Specifically, in the second phase of our work, we performed an experiment to investigate and characterize the ways that high-performing neural-network classifiers fall short of robust perception, and evaluated whether our alignment-based approach can address the shortcomings. We focused on understanding the underlying causes of certain idiosyncratic deficiencies in neural-network classification performance known collectively as fooling phenomena. The experiment, which relied on a procedure to remove signal energy from natural images while preserving high classification confidence by a neural network, revealed that multiple-choice classification tasks can cause neural networks to take the path of least resistance, classifying some object categories via features that are visually robust by human standards, but relying on obscure features, that are perceptually irrelevant to humans, once the visually-robust categories have been ruled out.<br /><br />In the third phase of our work, we applied the intuition from our neural-network characterization experiment to the challenge of building robust neural networks. The intuition behind the fooling phenomena is that feature learning in deep networks can lead to shortcuts that allow the learned models to become proficient at the training task without developing visually robust models. Our goal to address this problem was to learn models as densely connected networks that are trained with multimodal inputs. In contrast with most deep neural network architectures in which a deep hierarchy of representations maps from inputs to output classes, our architecture is structured as multi-task training on a network that maps several input modalities onto several output types, such as semantic segmentation along with depth maps. Importantly, the many paths through this network enforce sharing of internal representations that may prevent overspecialized intermediate representations from developing. <br /><br />In the final phase of the project we developed specific neural-network mechanisms that are inspired by propagation and that facilitate alignment of partial descriptions held by computational specialists. Our prior work on this project indicated that requiring networks to learn common internal representations from multiple sensory modalities would lead to less error-prone performance than that afforded by representations learned from vision alone. Such representations, in turn, allow the networks to bring machinery from, for example, auditory and tactile sensory modes to bear on vision tasks even when the inputs are presented as purely visual stimuli. In this reporting period we solidified this intuition into a learning technique which we label <em>ally networks</em>. Ally networks accomplish two goals: they encourage networks to learn internal representations that have meaning in more than one sensory modality, and they permit networks to cooperate and solve problems that the networks were not specifically trained to solve.</p><br> <p>            Last Modified: 09/25/2018<br>      Modified by: Patrick&nbsp;H&nbsp;Winston</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537849607969_splash--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537849607969_splash--rgov-800width.jpg" title="Visual Propagator Example"><img src="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537849607969_splash--rgov-66x44.jpg" alt="Visual Propagator Example"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our visual propagator architecture can track pedestrians and infer their 3D locations and heights, and the 3D locations of occluding background objects. The system is based on work by Sussman and Radul (2009)</div> <div class="imageCredit">Adam Kraft</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Patrick&nbsp;H&nbsp;Winston</div> <div class="imageTitle">Visual Propagator Example</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537850740235_hare--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537850740235_hare--rgov-800width.jpg" title="Example of Minimal Image for Confident Classification by Neural Net"><img src="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537850740235_hare--rgov-66x44.jpg" alt="Example of Minimal Image for Confident Classification by Neural Net"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The image on the left is classified correctly with high confidence by a neural network. The minimal-feature image on the right is generated by selectively driving pixels toward the mean without reducing classification confidence, until no additional signal energy can be removed.</div> <div class="imageCredit">Adam Kraft</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Patrick&nbsp;H&nbsp;Winston</div> <div class="imageTitle">Example of Minimal Image for Confident Classification by Neural Net</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537851756625_ally--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537851756625_ally--rgov-800width.jpg" title="Example Simple Ally Network"><img src="/por/images/Reports/POR/2018/1421065/1421065_10311889_1537851756625_ally--rgov-66x44.jpg" alt="Example Simple Ally Network"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Ally Networks are our multimodal learning technique that enable networks to learn robust visual features from multimodal inputs. This example exhibits the architecture of two ally networks that cooperate to perform image-guided depth super resolution, though neither network was trained on this task.</div> <div class="imageCredit">Adam Kraft</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Patrick&nbsp;H&nbsp;Winston</div> <div class="imageTitle">Example Simple Ally Network</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Human visual intelligence is robust. Our visual intelligence is versatile in the variety of problems it is able to solve and in the variety of its operating conditions, such as recognizing objects at varying distances in bright and dim light. Human vision is flexible, adapting facilely to new tasks like driving a car, even tasks that have had little significance in our recent evolutionary development.  Human vision is also provides richly compositional explanations for its findings. We understand what we see not just in terms of captions or labels, but in terms of parts, material properties, support relations and many other mutually-reinforcing descriptions.  In this project, we addressed the problem of understanding and modeling these robust properties of natural vision in AI vision systems. We believe that in order to understand and model robust visual intelligence, we must make principled choices about the mechanisms we apply to the problem, guided by natural vision?s computational imperatives. We considered several key operating constraints that natural vision systems are subject to, and conjectured that the core computational process of natural vision is a process of alignment: spreading information between computational specialists by aligning internal state with other specialists, through shared representations. We further conjecture that this process of alignment occurs bidirectionally between specialists, and that the alignment process shares information among all sensory modes in the perception system. This necessarily blurs the boundary between vision and other sensory apparatus, allowing the specialized computational machinery of many senses may be brought to bear on problems with strictly-visual inputs.  We divided our work on this project into four phases. In the first phase, we developed a propagator architecture based on the work of Sussman and Radul (2009) and implemented a system using that architecture that can perform scene-understanding tasks. The system can propagate constraints to infer locations and heights of pedestrians and locations of occluding objects in an outdoor urban scene.   We then began investigating how to improve our propagator architecture through the application of neural networks. Specifically, in the second phase of our work, we performed an experiment to investigate and characterize the ways that high-performing neural-network classifiers fall short of robust perception, and evaluated whether our alignment-based approach can address the shortcomings. We focused on understanding the underlying causes of certain idiosyncratic deficiencies in neural-network classification performance known collectively as fooling phenomena. The experiment, which relied on a procedure to remove signal energy from natural images while preserving high classification confidence by a neural network, revealed that multiple-choice classification tasks can cause neural networks to take the path of least resistance, classifying some object categories via features that are visually robust by human standards, but relying on obscure features, that are perceptually irrelevant to humans, once the visually-robust categories have been ruled out.  In the third phase of our work, we applied the intuition from our neural-network characterization experiment to the challenge of building robust neural networks. The intuition behind the fooling phenomena is that feature learning in deep networks can lead to shortcuts that allow the learned models to become proficient at the training task without developing visually robust models. Our goal to address this problem was to learn models as densely connected networks that are trained with multimodal inputs. In contrast with most deep neural network architectures in which a deep hierarchy of representations maps from inputs to output classes, our architecture is structured as multi-task training on a network that maps several input modalities onto several output types, such as semantic segmentation along with depth maps. Importantly, the many paths through this network enforce sharing of internal representations that may prevent overspecialized intermediate representations from developing.   In the final phase of the project we developed specific neural-network mechanisms that are inspired by propagation and that facilitate alignment of partial descriptions held by computational specialists. Our prior work on this project indicated that requiring networks to learn common internal representations from multiple sensory modalities would lead to less error-prone performance than that afforded by representations learned from vision alone. Such representations, in turn, allow the networks to bring machinery from, for example, auditory and tactile sensory modes to bear on vision tasks even when the inputs are presented as purely visual stimuli. In this reporting period we solidified this intuition into a learning technique which we label ally networks. Ally networks accomplish two goals: they encourage networks to learn internal representations that have meaning in more than one sensory modality, and they permit networks to cooperate and solve problems that the networks were not specifically trained to solve.       Last Modified: 09/25/2018       Submitted by: Patrick H Winston]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
