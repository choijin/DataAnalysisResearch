<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Algorithms for accelerating optimization in deep learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>449999.00</AwardTotalIntnAmount>
<AwardAmount>457999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Intelligent processing of complex signals such as images or sound is often performed by a parameterized, nested hierarchy of simple nonlinear processing layers, as in a deep neural net, an object recognition cascade or a speech front-end. Joint estimation of the parameters of all the layers and selection of an optimal architecture is a difficult nonconvex optimization problem, difficult to parallelize, and requiring significant human expert effort, which leads to suboptimal systems in practice. This research will develop a new, general mathematical strategy to learn the parameters and, to some extent, the architecture of nested systems, called the method of auxiliary coordinates (MAC). MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, applies even when parameter derivatives are not available, easily makes use of parallel architectures, and often provides reasonable models within a few iterations.  The PI's research and teaching will introduce undergraduate students to the design of nested machine learning systems, and provide computer science graduate students with skills in optimization, in support of specific areas (machine learning, computer vision/speech, etc.). The PI will broadly disseminate the results of the research by publishing papers in optimization, machine learning, computer vision and speech. The PI will make Matlab and C/C++ code available for the algorithms developed and use it as teaching aid in his courses on optimization and machine learning. The PI will strive to involve a diverse population of students in the research.&lt;br/&gt;&lt;br/&gt;MAC could drastically facilitate, by reducing runtime and human effort, the practical design and estimation of complex models currently being developed in data-rich disciplines such as machine learning, computer vision and speech, but also in other areas of engineering and science. It could also obviate the construction of hand-crafted features in trees and other classifiers. It may bring a wide-ranging and timely benefit to society given that serial computation is reaching a plateau and cloud computing is becoming a commodity, and intelligent data processing is finding its way into mainstream devices (phones, cameras, etc.), thanks to increases in computational power and data availability.  The research will develop MAC along two aims. Optimization: The PI will explore different ways to define auxiliary coordinates; different algorithms to solve the MAC-constrained problem; and efficient W and Z steps. The PI will also investigate the convergence properties of these approaches and their ability to be parallelized. Machine learning: The PI will apply MAC-based optimization to several existing and new nested models: deep neural nets; best-subset feature selection; learning features for decision trees; dictionary and classifier learning; parametric embeddings; learning hash functions; distal learning; model adaptation; and others. The evaluation plan includes standard benchmarks and articulatory speech modeling tasks. The research is interdisciplinary and potentially transformative: it opens many opportunities for research in optimization and machine learning, promoting thinking in terms of MAC formulations when designing learning systems, and could replace or complement the backpropagation algorithm in learning nested systems both in the serial and parallel settings. The models developed for articulatory speech will also improve our understanding of speech production.</AbstractNarration>
<MinAmdLetterDate>08/01/2014</MinAmdLetterDate>
<MaxAmdLetterDate>04/14/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423515</AwardID>
<Investigator>
<FirstName>Miguel</FirstName>
<LastName>Carreira-Perpinan</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Miguel A Carreira-Perpinan</PI_FULL_NAME>
<EmailAddress>mcarreira-perpinan@ucmerced.edu</EmailAddress>
<PI_PHON>2092284545</PI_PHON>
<NSF_ID>000233489</NSF_ID>
<StartDate>08/01/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California - Merced</Name>
<CityName>Merced</CityName>
<ZipCode>953435001</ZipCode>
<PhoneNumber>2092012039</PhoneNumber>
<StreetAddress>5200 North Lake Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA16</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>113645084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, MERCED</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California - Merced]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>953435001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~449999</FUND_OBLG>
<FUND_OBLG>2015~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A machine learning model works by taking an input vector, such as an image, and applying a mathematical function to it to predict an output, such as classifying the image as containing a car, or transforming the image into a short sequence of bits which is useful for fast image search. Machine learning models are learned from training data, such as images and their corresponding class labels, by optimizing a desirable measure, such as the classification error, over the model parameters.<br /><br />The mathematical function defining the model often takes the form of sequentially applying or composing a series of simpler stages each with its own parameters. For example, it could first compute some low-level features from an image and then apply a simple classifier (such as a decision tree) to these features. Optimizing such models almost always results in difficult, nonconvex problems, even non-differentiable.<br /><br />This project identified such <em>nested functions</em> as an important pattern in building complex machine learning models and developed a generic optimization algorithm to learn all the parameters jointly, that is, the parameters of each individual stage. This algorithm, called <em>method of auxiliary coordinates (MAC)</em>, is schematically shown in the figure. The idea involves 1) introducing new variables in the problem (the auxiliary coordinates), thus turning it into a constrained optimization problem; and 2) solving this by defining a penalty function (such as the augmented Lagrangian) and applying to it alternating optimization over the original parameters and the auxiliary coordinates. Intuitively, the resulting algorithm can be seen as training each stage on its own given the current values of the auxiliary coordinates, as if there were no other stages, and then updating the coordinates in order to coordinate the stages with each other; this process is iterated till convergence. MAC has multiple advantages over algorithms based on gradients (the chain rule): the stages need not be differentiable, parallelism is introduced, and the optimization of individual stages can be done by reusing existing code. Thus MAC can apply when gradients do not exist, and if gradients do exist it can be faster.<br /><br />The project developed MAC in its theoretical and computational aspects, for a number of stage functions (support vector machines, logistic regression, neural nets, decision trees, nonlinear embeddings, etc.) and machine learning applications (classification, dimensionality reduction, binary hashing, etc.). It trained 8 graduate students and 4 undergraduate students, and resulted in a number of technical publications, software products and contributions to the PI's graduate courses' lecture notes on machine learning and optimization, all of which are available for free to the public in the PI's web site.</p><br> <p>            Last Modified: 07/15/2020<br>      Modified by: Miguel&nbsp;A&nbsp;Carreira-Perpinan</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1423515/1423515_10327253_1594809521399_1423515-QuadChart-pic--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1423515/1423515_10327253_1594809521399_1423515-QuadChart-pic--rgov-800width.jpg" title="Illustration of the method of auxiliary coordinates"><img src="/por/images/Reports/POR/2020/1423515/1423515_10327253_1594809521399_1423515-QuadChart-pic--rgov-66x44.jpg" alt="Illustration of the method of auxiliary coordinates"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Illustration of the method of auxiliary coordinates for a two-stage model function.</div> <div class="imageCredit">Miguel �. Carreira-Perpi��n</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Miguel&nbsp;A&nbsp;Carreira-Perpinan</div> <div class="imageTitle">Illustration of the method of auxiliary coordinates</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A machine learning model works by taking an input vector, such as an image, and applying a mathematical function to it to predict an output, such as classifying the image as containing a car, or transforming the image into a short sequence of bits which is useful for fast image search. Machine learning models are learned from training data, such as images and their corresponding class labels, by optimizing a desirable measure, such as the classification error, over the model parameters.  The mathematical function defining the model often takes the form of sequentially applying or composing a series of simpler stages each with its own parameters. For example, it could first compute some low-level features from an image and then apply a simple classifier (such as a decision tree) to these features. Optimizing such models almost always results in difficult, nonconvex problems, even non-differentiable.  This project identified such nested functions as an important pattern in building complex machine learning models and developed a generic optimization algorithm to learn all the parameters jointly, that is, the parameters of each individual stage. This algorithm, called method of auxiliary coordinates (MAC), is schematically shown in the figure. The idea involves 1) introducing new variables in the problem (the auxiliary coordinates), thus turning it into a constrained optimization problem; and 2) solving this by defining a penalty function (such as the augmented Lagrangian) and applying to it alternating optimization over the original parameters and the auxiliary coordinates. Intuitively, the resulting algorithm can be seen as training each stage on its own given the current values of the auxiliary coordinates, as if there were no other stages, and then updating the coordinates in order to coordinate the stages with each other; this process is iterated till convergence. MAC has multiple advantages over algorithms based on gradients (the chain rule): the stages need not be differentiable, parallelism is introduced, and the optimization of individual stages can be done by reusing existing code. Thus MAC can apply when gradients do not exist, and if gradients do exist it can be faster.  The project developed MAC in its theoretical and computational aspects, for a number of stage functions (support vector machines, logistic regression, neural nets, decision trees, nonlinear embeddings, etc.) and machine learning applications (classification, dimensionality reduction, binary hashing, etc.). It trained 8 graduate students and 4 undergraduate students, and resulted in a number of technical publications, software products and contributions to the PI's graduate courses' lecture notes on machine learning and optimization, all of which are available for free to the public in the PI's web site.       Last Modified: 07/15/2020       Submitted by: Miguel A Carreira-Perpinan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
