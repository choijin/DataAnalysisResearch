<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Development and Application of a Multilevel Evaluation Procedure for Examining State and School Educational Contexts with NAEP</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>339916.00</AwardTotalIntnAmount>
<AwardAmount>339916</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The National Assessment of Educational Progress (NAEP) mathematics exam renders aggregate scores based on the assumption that each test item measures a single underlying proficiency (e.g., an area of mathematics). Prior work of the investigators suggests that there may be more information that can be mined from the NAEP exam that may be used for policymaking at the instructional level, such as policymaking for schools). &lt;br/&gt;&lt;br/&gt;Using the NAEP item response data, the investigators propose a two-fold goal: (1) to understand school and state influences on student mathematics achievement, and (2) to improve measurement and statistical analysis techniques for the evaluation of mathematics education.  A considerable portion of the research will focus on the creation of measurement and statistical techniques (goal two) that will be demonstrated in the advancement of goal one analyses.&lt;br/&gt;&lt;br/&gt;The investigators will use Item Response Theory (IRT) and Hierarchical Linear Modeling (HLM) analyses to produce their own statistically-based classification of NAEP items that, when contrasted with the existing (official) NAEP conceptually-based classification system, should reveal information that can be attributed at the school and state levels simultaneously.  This will allow the researchers to use the data and their (error) variance--both in IRT and HLM--to examine how schools differ on any number of preselected, relevant policy characteristics.  To do this, however, requires that the investigators build new statistical tools and will require that this very premise be tested in the form of a hypothesis.  &lt;br/&gt;&lt;br/&gt;Thus, the results of this feasibility study will determine if a test designed for assessing national (only) progress can be used to explain the relative mathematical strengths of schools. This approach could be used to evaluate policy initiatives focused at the school level using NAEP and potentially other large-scale assessment data.</AbstractNarration>
<MinAmdLetterDate>03/19/2014</MinAmdLetterDate>
<MaxAmdLetterDate>03/19/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1433760</AwardID>
<Investigator>
<FirstName>Gregory</FirstName>
<LastName>Camilli</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregory Camilli</PI_FULL_NAME>
<EmailAddress>greg.camilli@gmail.com</EmailAddress>
<PI_PHON>8489320831</PI_PHON>
<NSF_ID>000415665</NSF_ID>
<StartDate>03/19/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName/>
<StateCode>NJ</StateCode>
<ZipCode>089011183</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7625</Code>
<Text>REAL</Text>
</ProgramElement>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0411</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~339916</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>My work to date supported by NSF funding has focused on extracting more information from two large-scale assessments: the National Assessment of Educational Progress (NAEP), and the Trends in Mathematics and Science Study (TIMSS). These assessments consist of different types of test items in format, content, and complexity. Both assessment programs provide subscores to provide diagnostic information at the state or country level. However, these subscores are too highly correlated to discriminate between different mathematical proficiencies. My research is focused on producing more distinct subscores for these assessments in order to study educational policies regarding mathematics achievement. Subscores reported by sponsoring organizations are highly correlated at  r&nbsp;&gt;&nbsp;.9, while correlations between the empirical factors I develop  in my work are typically modest at about r = .5.</p> <p>This work has led to the discovery of multifactor solutions to the 2009 and 2011 NAEP mathematics data at grades four and eight. A two-factor solution is readily interpretable across 2009-2011. For both factors, state-level correlations are high across years. Multifactor solutions were also found for the TIMSS 2007 and 2011 mathematics assessments: three factors emerged at fourth grade (number competencies, algebra and fractions) and two at eighth grade. Obtaining factor solutions allows the examination of the empirical structure of test items, as opposed to theoretical structure, and computation of subscores consistent with those empirical structures. Factor analysis approaches demonstrate there are subscores that can be reported in a way to better reflect upon educational practices.</p> <p>Countries participating in TIMSS had profiles of achievement that provide clues to the effects of educational systems. For instance, in 2007 Singapore was ranked first in terms of fractions but former Russian republics were ranked highly in terms of algebra. For the fourth grade NAEP assessment, two highly interpretable factors were found on the NAEP assessment that generalized across 2009-2011. The first general factor was defined by cognitive behaviors best described as geometric or graphical reasoning containing a strong element of visual interpretation. This factor is highly correlated with the NAEP total score. This demonstrates that high NAEP scores reflect complex and abstract competency in mathematics. &nbsp;The second factor was defined by items requiring multidigit operations. States that scored highly on this factor appear to have stressed (at least until 2011) more traditional mathematics.</p><br> <p>            Last Modified: 11/29/2016<br>      Modified by: Gregory&nbsp;Camilli</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ My work to date supported by NSF funding has focused on extracting more information from two large-scale assessments: the National Assessment of Educational Progress (NAEP), and the Trends in Mathematics and Science Study (TIMSS). These assessments consist of different types of test items in format, content, and complexity. Both assessment programs provide subscores to provide diagnostic information at the state or country level. However, these subscores are too highly correlated to discriminate between different mathematical proficiencies. My research is focused on producing more distinct subscores for these assessments in order to study educational policies regarding mathematics achievement. Subscores reported by sponsoring organizations are highly correlated at  r &gt; .9, while correlations between the empirical factors I develop  in my work are typically modest at about r = .5.  This work has led to the discovery of multifactor solutions to the 2009 and 2011 NAEP mathematics data at grades four and eight. A two-factor solution is readily interpretable across 2009-2011. For both factors, state-level correlations are high across years. Multifactor solutions were also found for the TIMSS 2007 and 2011 mathematics assessments: three factors emerged at fourth grade (number competencies, algebra and fractions) and two at eighth grade. Obtaining factor solutions allows the examination of the empirical structure of test items, as opposed to theoretical structure, and computation of subscores consistent with those empirical structures. Factor analysis approaches demonstrate there are subscores that can be reported in a way to better reflect upon educational practices.  Countries participating in TIMSS had profiles of achievement that provide clues to the effects of educational systems. For instance, in 2007 Singapore was ranked first in terms of fractions but former Russian republics were ranked highly in terms of algebra. For the fourth grade NAEP assessment, two highly interpretable factors were found on the NAEP assessment that generalized across 2009-2011. The first general factor was defined by cognitive behaviors best described as geometric or graphical reasoning containing a strong element of visual interpretation. This factor is highly correlated with the NAEP total score. This demonstrates that high NAEP scores reflect complex and abstract competency in mathematics.  The second factor was defined by items requiring multidigit operations. States that scored highly on this factor appear to have stressed (at least until 2011) more traditional mathematics.       Last Modified: 11/29/2016       Submitted by: Gregory Camilli]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
