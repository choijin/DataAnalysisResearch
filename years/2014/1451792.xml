<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: The acquisition of phoneme categories</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2015</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>6080.00</AwardTotalIntnAmount>
<AwardAmount>6080</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Language comes effortlessly to children, and only adults learning a second language get a glimpse of the numerous intricacies involved in learning a language. These range from correlation of word to meaning, to something as deceptively simple as the categorization of speech sounds. To best illustrate this, we can look at two examples: differences in perception between English and Hindi speakers, and differences in perception between English and Japanese speakers. English speakers have trouble hearing the difference between the "p" in "pear" and the "p" in "spare," even though they are in reality pronounced quite differently. A speaker of Hindi on the other hand would have no trouble hearing this difference. To take another example, while English speakers could easily tell you that "light" and "right" begin with two different sounds, Japanese speakers find it very difficult to hear the difference between these two words. All of these difficulties arise as a byproduct of the human infant's very efficient learning mechanism when (s)he is first learning a language. The human brain unconsciously and automatically sorts the large amount of information we are constantly receiving into ordered data, even before we make conscious decisions. This study is concerned with determining what critical pieces of information were received by the Hindi infant to lead him or her to (unconsciously) determine that those were two different "p" sounds, while the English infant determined that they were the same sound; or what information led the Japanese infant to ignore the difference between "l" and "r" when acquiring Japanese, but did not lead the English infant to ignore the same difference. This study both contributes to broad theoretical issues within cognitive science by addressing how categories are learned by humans, and also leads to practical applications, such as more efficient second language learning techniques and advancements in speech pathology.&lt;br/&gt;&lt;br/&gt;Previous work has identified at least two main cues that humans use when categorizing sounds: the learner's growing vocabulary ("vocabulary cue") and acoustic properties found within a stream of speech ("acoustic cue"). The vocabulary cue groups together sounds occurring in similar word environments ("light" and "right") into a single sound category. The acoustic cue maps sounds into some acoustic map and picks out regions of high frequencies on this map. This study makes use of past observations that adults can unconsciously simulate, at least to some degree, aspects of the infant's learning experience within a laboratory setting. This experiment will test how much adults rely on each of these two cues by presenting participants with an artificial language where the vocabulary cue and the acoustic cue give the participant conflicting information. In this way, the research will contribute to a better understanding of what unconscious decisions the human brain makes when categorizing speech sounds. This study addresses two fundamental issues within linguistics and within cognitive science: (1) how the human brain forms complex categories, and (2) how much of language is acquired through specialized language-specific modules and how much of language can be attributed to general cognitive mechanisms.</AbstractNarration>
<MinAmdLetterDate>05/07/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/07/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1451792</AwardID>
<Investigator>
<FirstName>Elliott</FirstName>
<LastName>Moreton</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elliott Moreton</PI_FULL_NAME>
<EmailAddress>moreton@unc.edu</EmailAddress>
<PI_PHON>9199663411</PI_PHON>
<NSF_ID>000354303</NSF_ID>
<StartDate>05/07/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Smith</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer L Smith</PI_FULL_NAME>
<EmailAddress>jlsmith@email.unc.edu</EmailAddress>
<PI_PHON>9199621192</PI_PHON>
<NSF_ID>000636524</NSF_ID>
<StartDate>05/07/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Emily</FirstName>
<LastName>Moeng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emily Moeng</PI_FULL_NAME>
<EmailAddress>moeng@live.unc.edu</EmailAddress>
<PI_PHON>9199663411</PI_PHON>
<NSF_ID>000671260</NSF_ID>
<StartDate>05/07/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275991350</ZipCode>
<StreetAddress><![CDATA[104 AIRPORT DR STE 2200]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8374</Code>
<Text>DDRI Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~6080</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Language comes effortlessly to children, and only adults learning a second language get a glimpse of the numerous intricacies involved in learning a language. This study looks at one of these tasks, the categorization of speech sounds. To best illustrate this, we can look at one example: differences in perception between English and Japanese speakers. English speakers could easily tell you that &ldquo;light&rdquo; and &ldquo;right&rdquo; begin with two different sounds, but Japanese speakers find it very difficult to hear the difference between these two words. These difficulties arise as a byproduct of the human infant&rsquo;s very efficient learning mechanism when (s)he is first learning a language. The human brain unconsciously and automatically sorts the large amount of information we are constantly receiving into ordered data, even before we make conscious decisions. This study is concerned with determining what critical pieces of information led the Japanese infant to ignore the difference between &ldquo;l&rdquo; and &ldquo;r&rdquo; when acquiring Japanese, but did not lead the English infant to ignore the same difference.</p> <p>Previous work has identified at least two main cues that humans use when categorizing sounds: the learner&rsquo;s growing vocabulary (&ldquo;vocabulary cue&rdquo;) and acoustic properties found within a stream of speech (&ldquo;acoustic cue&rdquo;). The vocabulary cue groups together sounds occurring in similar word environments (&ldquo;light&rdquo; and &ldquo;right&rdquo;) into a single sound category, which is what happens for the learner of Japanese, but not the learner of English. The acoustic cue maps sounds into some acoustic map and picks out regions of high frequencies on this map. This goal of this study was to build on past observations that adults can unconsciously simulate, at least to some degree, aspects of the infant&rsquo;s learning experience within a laboratory setting in order to study the vocabulary cue and the acoustic cue.</p> <p><strong><span style="text-decoration: underline;">The Vocabulary Cue</span></strong>: In a set of three experiments, this study was not able to replicate effects of the vocabulary cue. It is unclear what differences existed between my study and the study conducted by Feldman et al. (2011), who found evidence of an effect of the vocabulary cue in adults, that resulted in this non-replication, but we believe it had something to do with dialectal differences between my participants and those tested by Feldman et al. Specifically, my study was conducted in North Carolina, whereas Feldman et al.&rsquo;s study was conducted in Maryland. Regional dialectal differences in the pronunciation of the vowel sound found in words like <em>caught</em> and <em>bought </em>may have resulted in this difference in participant responses.</p> <p><strong><span style="text-decoration: underline;">The Acoustic Cue</span></strong>: Unlike the vocabulary cue, effects of the acoustic cue have already been replicated many times. In a set of six experiments, several details regarding the effect of the acoustic cue were found which previously have not been reported on. These details are as follows:</p> <p>1) When attention is shifted to some other unrelated acoustic task, effects of the acoustic cue are not found. This suggests that attention level is a relevant factor for this cue. This is a new observation that has not been made in past studies of the acoustic cue.</p> <p>2) Previously, it was unclear if this cue was caused by an increased sensitivity to a previously difficult-to-detect sound contrast. That is, it could be the case that the acoustic cue causes &ldquo;l&rdquo;<em> </em>and &ldquo;r&rdquo; to sound more different to a language learner, and this increased sensitivity results in learners assuming that the difference must be significant to differences in word meaning. In other words, an increased sensitivity to the differences between &ldquo;l&rdquo; and &ldquo;r&rdquo; might make a learner think that &ldquo;light&rdquo; and &ldquo;right&rdquo; must be two different words. However, this study finds evidence for the acoustic cue being used <em>without</em> an accompanying increase in sensitivity, suggesting that the acoustic cue is not directly caused by increased sensitivity. Instead, it is believed that the acoustic cue makes learners more likely to think that there must be two meaningfully-different sounds in the speech stream rather than one, whether or not they can actually hear that difference any better than someone who was not trained on this distinction.</p> <p>3) Regarding methodology, it was also found that evidence for the acoustic cue could be found even in an online experiment. Previous experiments regarding the acoustic cue have all been conducted in a lab setting, but this set of experiments was conducted through Amazon&rsquo;s Mechanical Turk, which recruits participants online. Now that we know that the acoustic cue can be tested online, we have the potential of reaching a wider variety of potential participants via the web, which can lead to more aspects of the acoustic cue being studied.</p> <p>This study makes a methodological contribution (even effects requiring slight distinctions in sound can be studied through online participant pools), and two theoretical contributions regarding the details of a widely-cited phenomenon, the acoustic cue.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/13/2017<br>      Modified by: Emily&nbsp;Moeng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Language comes effortlessly to children, and only adults learning a second language get a glimpse of the numerous intricacies involved in learning a language. This study looks at one of these tasks, the categorization of speech sounds. To best illustrate this, we can look at one example: differences in perception between English and Japanese speakers. English speakers could easily tell you that "light" and "right" begin with two different sounds, but Japanese speakers find it very difficult to hear the difference between these two words. These difficulties arise as a byproduct of the human infant?s very efficient learning mechanism when (s)he is first learning a language. The human brain unconsciously and automatically sorts the large amount of information we are constantly receiving into ordered data, even before we make conscious decisions. This study is concerned with determining what critical pieces of information led the Japanese infant to ignore the difference between "l" and "r" when acquiring Japanese, but did not lead the English infant to ignore the same difference.  Previous work has identified at least two main cues that humans use when categorizing sounds: the learner?s growing vocabulary ("vocabulary cue") and acoustic properties found within a stream of speech ("acoustic cue"). The vocabulary cue groups together sounds occurring in similar word environments ("light" and "right") into a single sound category, which is what happens for the learner of Japanese, but not the learner of English. The acoustic cue maps sounds into some acoustic map and picks out regions of high frequencies on this map. This goal of this study was to build on past observations that adults can unconsciously simulate, at least to some degree, aspects of the infant?s learning experience within a laboratory setting in order to study the vocabulary cue and the acoustic cue.  The Vocabulary Cue: In a set of three experiments, this study was not able to replicate effects of the vocabulary cue. It is unclear what differences existed between my study and the study conducted by Feldman et al. (2011), who found evidence of an effect of the vocabulary cue in adults, that resulted in this non-replication, but we believe it had something to do with dialectal differences between my participants and those tested by Feldman et al. Specifically, my study was conducted in North Carolina, whereas Feldman et al.?s study was conducted in Maryland. Regional dialectal differences in the pronunciation of the vowel sound found in words like caught and bought may have resulted in this difference in participant responses.  The Acoustic Cue: Unlike the vocabulary cue, effects of the acoustic cue have already been replicated many times. In a set of six experiments, several details regarding the effect of the acoustic cue were found which previously have not been reported on. These details are as follows:  1) When attention is shifted to some other unrelated acoustic task, effects of the acoustic cue are not found. This suggests that attention level is a relevant factor for this cue. This is a new observation that has not been made in past studies of the acoustic cue.  2) Previously, it was unclear if this cue was caused by an increased sensitivity to a previously difficult-to-detect sound contrast. That is, it could be the case that the acoustic cue causes "l" and "r" to sound more different to a language learner, and this increased sensitivity results in learners assuming that the difference must be significant to differences in word meaning. In other words, an increased sensitivity to the differences between "l" and "r" might make a learner think that "light" and "right" must be two different words. However, this study finds evidence for the acoustic cue being used without an accompanying increase in sensitivity, suggesting that the acoustic cue is not directly caused by increased sensitivity. Instead, it is believed that the acoustic cue makes learners more likely to think that there must be two meaningfully-different sounds in the speech stream rather than one, whether or not they can actually hear that difference any better than someone who was not trained on this distinction.  3) Regarding methodology, it was also found that evidence for the acoustic cue could be found even in an online experiment. Previous experiments regarding the acoustic cue have all been conducted in a lab setting, but this set of experiments was conducted through Amazon?s Mechanical Turk, which recruits participants online. Now that we know that the acoustic cue can be tested online, we have the potential of reaching a wider variety of potential participants via the web, which can lead to more aspects of the acoustic cue being studied.  This study makes a methodological contribution (even effects requiring slight distinctions in sound can be studied through online participant pools), and two theoretical contributions regarding the details of a widely-cited phenomenon, the acoustic cue.          Last Modified: 10/13/2017       Submitted by: Emily Moeng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
