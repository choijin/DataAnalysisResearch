<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Chameleon: A Large-Scale, Reconfigurable Experimental Environment for Cloud Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>599775.00</AwardTotalIntnAmount>
<AwardAmount>599775</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Deepankar Medhi</SignBlockName>
<PO_EMAI>dmedhi@nsf.gov</PO_EMAI>
<PO_PHON>7032922935</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A persistent problem facing academic cloud research is the lack of infrastructure and data to perform experimental research: large-scale hardware is needed to investigate the scalability of cloud infrastructure and applications, heterogeneous hardware is needed to investigate algorithmic and implementation tradeoffs, fully-configurable software environments are needed to investigate the performance of virtualization techniques and the differences between cloud software stacks, and data about how clouds are used is needed to evaluate virtual machine scheduling and data placement algorithms. &lt;br/&gt;&lt;br/&gt;The Chameleon project will addresses these needs by providing a large-scale, fully configurable experimental testbed driven by the needs of the cloud research and education communities. The testbed, and the ecosystem associated with it, will enable researchers to explore a range of cloud research challenges, from large scale to small scale, including exploring low-level problems in hardware architecture, systems research, network configuration, and software design, or at higher levels of abstraction looking at cloud scheduling, cloud platforms, and cloud applications. &lt;br/&gt;&lt;br/&gt;Chameleon will significantly enhance the ability of the computing research community to understand the behavior of Internet scale cloud systems, and to develop new software, ideas and algorithms for the cloud environment. As the tremendous shift to cloud as the primary means of providing computing infrastructure continues, a large-scale testbed tailored to researchers' needs is essential to the continued relevance of a large fraction of computing research. &lt;br/&gt;&lt;br/&gt;The project is led by the University of Chicago and includes partners from the Texas Advanced Computing Center (TACC), Northwestern University, the Ohio State University, and the University of Texas at San Antonio, comprising a highly qualified and experienced team, with research leaders from the cloud and networking world blended with providers of production quality cyberinfrastructure.  The team includes members from the NSF-supported FutureGrid project and from the GENI community, both forerunners of the NSFCloud solicitation under which this project is funded. &lt;br/&gt;&lt;br/&gt;The Chameleon testbed, will be deployed at the University of Chicago (UC) and the Texas Advanced Computing Center (TACC) and will consist of 650 multi-core cloud nodes, 5PB of total disk space, and leverage 100 Gbps connection between the sites. While a large part of the testbed will consist of homogenous hardware to support large-scale experiments, a portion of it will support heterogeneous units allowing experimentation with high-memory, large-disk, low-power, GPU, and co-processor units. The project will also leverage existing FutureGrid hardware at UC and TACC in its first year to provide a transition period for the existing FutureGrid community of experimental users. &lt;br/&gt;&lt;br/&gt;To support a broad range of experiments emphasizing a range of requirements ranging from a high degree of control to ease of use the project will support a graduated configuration system allowing full user configurability of the stack, from provisioning of bare metal and network interconnects to delivery of fully functioning cloud environments. In addition, to facilitate experiments, Chameleon will support a set of services designed to meet researchers needs, including support for experimental management, reproducibility, and repositories of trace and workload data of production cloud workloads. &lt;br/&gt;&lt;br/&gt;To facilitate the latter, the project will form a set of partnerships with commercial as well as academic clouds, such as Rackspace and Open Science Data Cloud (OSDC). It will also partner with other testbeds, notably GENI and INRIA's Grid5000 testbed, and reach out to the user community to shape the policy an direction of the testbed. &lt;br/&gt;&lt;br/&gt;The Chameleon project will bring a new dimension and scale of resources to the CS community who wish to educate their students about design, implementation, operation and applications of cloud computing, a critical skillset for future computing professionals. It will enhance the understanding and application of experimental methodology in computer science and generate new educational materials and resources, with the participation of, and for, Minority Serving Institution (MSI) students.</AbstractNarration>
<MinAmdLetterDate>08/19/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/05/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1419165</AwardID>
<Investigator>
<FirstName>Rajendra</FirstName>
<LastName>Boppana</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rajendra V Boppana</PI_FULL_NAME>
<EmailAddress>RAJENDRA.BOPPANA@UTSA.EDU</EmailAddress>
<PI_PHON>2104585692</PI_PHON>
<NSF_ID>000471500</NSF_ID>
<StartDate>08/19/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Peyman</FirstName>
<LastName>Najafirad</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peyman Najafirad</PI_FULL_NAME>
<EmailAddress>paul.rad@utsa.edu</EmailAddress>
<PI_PHON>2108729147</PI_PHON>
<NSF_ID>000662068</NSF_ID>
<StartDate>08/19/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at San Antonio</Name>
<CityName>San Antonio</CityName>
<ZipCode>782491644</ZipCode>
<PhoneNumber>2104584340</PhoneNumber>
<StreetAddress>One UTSA Circle</StreetAddress>
<StreetAddress2><![CDATA[Grants, Contracts & Ind. Agr.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>800189185</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT SAN ANTONIO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The University of Texas at San Antonio]]></Name>
<CityName>San Antonio</CityName>
<StateCode>TX</StateCode>
<ZipCode>782491944</ZipCode>
<StreetAddress><![CDATA[One UTSA Circle]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>2890</Code>
<Text>CISE Research Resources</Text>
</ProgramElement>
<ProgramReference>
<Code>8002</Code>
<Text>CISE Research Resources</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~199775</FUND_OBLG>
<FUND_OBLG>2015~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="WordSection1"> <p>The Chameleon project deployed a successful large-scale distributed and configurable testbed&nbsp;to support cloud computing research. Chameleon differs substantially from other NSF computing resources in that it is directly focused on supporting&nbsp;Computer Science experimental research&nbsp;ranging from operating systems and virtualization to networking and security. The Chameleon testbed provides deep reconfigurability (bare metal, BIOS reconfiguration, console access, etc.).&nbsp;These capabilities are delivered using primarily OpenStack, a free and open-source cloud computing software platform that originated as a joint Rackspace and NASA initiative and is now run by the OpenStack Foundation. The testbed is distributed over University of Chicago and TACC and consists of ~400 nodes/10,000 cores and 5PB of total storage to support HPC and ML Big Data experiments, with high-memory, large-disk, and GPU subsystems.&nbsp;Chameleon provides tremendous flexibility to computing researchers by allowing them to deploy alternate software stacks; users could start with "bare metal" allocations or select from a library of pre-configured virtual machine images and appliances.&nbsp;</p> <p>The team has significantly broadened experimental capabilities of the testbed through the addition of new hardware and testbed capabilities and improving the ease of use of the testbed via such capabilities as support for complex appliances.&nbsp; To-date, the testbed has 26 appliances in the appliance catalog, 13 of which are developed and supported by the Chameleon team. Based on those Chameleon-supported appliances, our users have configured 550 appliances, 96 of which have been made public (i.e., shareable with the user community), and 13 of those have been accepted into the Chameleon catalog (appliances including additional documentation and meta-data such as support information).</p> <p>We have seen strong growth in the number of projects and researchers served on Chameleon. To-date, the Chameleon environment has supported over 2,000 researchers, hundreds of individual projects, and multiple education activities among over 30 disciplines with 210 distinct PIs, 116 institutions spanning 31 states which 11 are Minority Serving Universities. The Chameleon infrastructure lives on and continues to support the community through a renewal grant.</p> <p>In addition to the education workshop training and GitHub content, results have been presented at various conferences and events through Keynote talks, invited talks, tutorials, and hands-on sessions.&nbsp;Multiple Ph.D. and masters&rsquo; students have performed research work and received their<br />degrees as a part of this project.</p> <p>&nbsp;</p> <p>&nbsp;</p> </div><br> <p>            Last Modified: 12/29/2019<br>      Modified by: Peyman&nbsp;Najafirad</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The Chameleon project deployed a successful large-scale distributed and configurable testbed to support cloud computing research. Chameleon differs substantially from other NSF computing resources in that it is directly focused on supporting Computer Science experimental research ranging from operating systems and virtualization to networking and security. The Chameleon testbed provides deep reconfigurability (bare metal, BIOS reconfiguration, console access, etc.). These capabilities are delivered using primarily OpenStack, a free and open-source cloud computing software platform that originated as a joint Rackspace and NASA initiative and is now run by the OpenStack Foundation. The testbed is distributed over University of Chicago and TACC and consists of ~400 nodes/10,000 cores and 5PB of total storage to support HPC and ML Big Data experiments, with high-memory, large-disk, and GPU subsystems. Chameleon provides tremendous flexibility to computing researchers by allowing them to deploy alternate software stacks; users could start with "bare metal" allocations or select from a library of pre-configured virtual machine images and appliances.   The team has significantly broadened experimental capabilities of the testbed through the addition of new hardware and testbed capabilities and improving the ease of use of the testbed via such capabilities as support for complex appliances.  To-date, the testbed has 26 appliances in the appliance catalog, 13 of which are developed and supported by the Chameleon team. Based on those Chameleon-supported appliances, our users have configured 550 appliances, 96 of which have been made public (i.e., shareable with the user community), and 13 of those have been accepted into the Chameleon catalog (appliances including additional documentation and meta-data such as support information).  We have seen strong growth in the number of projects and researchers served on Chameleon. To-date, the Chameleon environment has supported over 2,000 researchers, hundreds of individual projects, and multiple education activities among over 30 disciplines with 210 distinct PIs, 116 institutions spanning 31 states which 11 are Minority Serving Universities. The Chameleon infrastructure lives on and continues to support the community through a renewal grant.  In addition to the education workshop training and GitHub content, results have been presented at various conferences and events through Keynote talks, invited talks, tutorials, and hands-on sessions. Multiple Ph.D. and masters’ students have performed research work and received their degrees as a part of this project.              Last Modified: 12/29/2019       Submitted by: Peyman Najafirad]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
