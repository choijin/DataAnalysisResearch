<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Research in Student Peer Review: A Cooperative Web-Services Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>293432.00</AwardTotalIntnAmount>
<AwardAmount>293432</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Talitha Washington</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Hundreds of thousands of students have used online peer review applications to review their classmates' work. While learning gains from peer review have been documented repeatedly, current systems do not always produce accurate scores and often give inadequate guidance to students about what constitutes a good review, resulting in haphazard feedback. This research addresses these issues with a common set of web services that can be used by any peer review system, as well as new visualizations that identify students' strengths and weaknesses, and gauge improvement over time.&lt;br/&gt;&lt;br/&gt;This project differs from previous research that typically involves a single peer review system. It will develop a set of web services that will be usable by any peer review system in the same way that Google Maps is available to any website that wants to display location data. This common implementation will allow the project team to gather data from large numbers of students in a wide variety of contexts, thereby giving us the statistical power to produce more convincing, highly generalizable results&lt;br/&gt;&lt;br/&gt;Peer review during the writing process is an example of formative assessment, feedback that is received while the recipient still has a chance to improve his/her work. Several studies have found that the students who benefit most from formative assessment are those who typically underperform as measured by exams and standardized tests. Formative assessment tends to level the playing field for underrepresented minorities by allowing these students to receive input from their peers when they are not stressed about how their grade is being affected.</AbstractNarration>
<MinAmdLetterDate>08/05/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1432690</AwardID>
<Investigator>
<FirstName>Luca</FirstName>
<LastName>De Alfaro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Luca De Alfaro</PI_FULL_NAME>
<EmailAddress>luca@soe.ucsc.edu</EmailAddress>
<PI_PHON>6502482856</PI_PHON>
<NSF_ID>000490752</NSF_ID>
<StartDate>08/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Cruz</Name>
<CityName>Santa Cruz</CityName>
<ZipCode>950641077</ZipCode>
<PhoneNumber>8314595278</PhoneNumber>
<StreetAddress>1156 High Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>125084723</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA CRUZ</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Cruz]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>950641077</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>1998</Code>
<Text>IUSE</Text>
</ProgramElement>
<ProgramReference>
<Code>8209</Code>
<Text>Improv Undergrad STEM Ed(IUSE)</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~293432</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-2d40e8a8-7fff-91a6-c864-330ef1cd63a9" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In online peer grading, students evaluate each other&rsquo;s work, according to guidelines provided by the instructor. The peer grading system then aggregates all evaluations, assigning a grade to each student. &nbsp;The overall goal of the project consisted in devising incentives for students to provide precise evaluations, and to devise aggregation schemes that can construct an accurate evaluation of a student&rsquo;s work from the input provided by the peers. </span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">We first took an in-depth look at the dynamics of online peer learning, and we performed the first broad study of the causes affecting the precision of peer grades. The results paint a complex picture. &nbsp;We established that of the two elements that can affect grade precision: homework submission, and the student grader, the greater part of the grading error can be attributed to the submission, rather than to the grader. &nbsp;In other words: imprecisions in the evaluation can be better explained in terms of difficult-to-grade submissions rather than imprecise students.  We also were able to rule out as minor many other factors, such as the amount of time dedicated to reviewing the homework submission, or the timeliness with which the reviews were performed. </span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">This understanding led us to improve the tools used for peer review, adding means by which the submission authors can communicate anonymously with the peer reviewers and clarify as required aspects of the submitted work, leading to more accurate evaluations. &nbsp;On the other hand, this also meant that our efforts to improve the methods for aggregating the student evaluations into a single evaluation were not very successful.  We planned to experiment with machine learning methods for understanding which students were more accurate in their reviews, and correct automatically any systematic evaluation biases. &nbsp;However, our results meant that systematic biases among graders were small, and there was little that could be gained by following the approach. </span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Our research in providing incentives to accurate peer reviews resulted in a hierarchical incentive system that provides a provably unbiased incentive for students to be accurate in their evaluations, and that regardless of the number of students, it requires only a small and bounded work on the part of the instructor. </span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Our research into improving the accuracy of peer grading also resulted in foundational work in crowdsourcing. &nbsp;Out of the algorithms we use for aggregating grades, we obtained efficient and highly accurate algorithms for aggregating Yes/No feedback from users. &nbsp;This type of binary feedback is very common on the web, and is used, for instance, whenever sites need to flag inappropriate content.  Contrary to previous methods, our algorithm is well suited to cases where the amount of crowdsourced information is very non-uniform, with some items receiving large amounts of feedback, while others receiving little. </span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Recently, we have had success in applying our algorithms for aggregating binary feedback to the problem of detecting fake news. &nbsp;The algorithms are applied to the bipartite graph consisting of news items on one side, and people (or bots) who have tweeted them on the other side. &nbsp;The algorithms are able to propagate a limited ground truth consisting of known fake news and fake sites, into a classification encompassing a large percentage of the news being published every day.</span></p><br> <p>            Last Modified: 10/12/2018<br>      Modified by: Luca&nbsp;De Alfaro</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[In online peer grading, students evaluate each other?s work, according to guidelines provided by the instructor. The peer grading system then aggregates all evaluations, assigning a grade to each student.  The overall goal of the project consisted in devising incentives for students to provide precise evaluations, and to devise aggregation schemes that can construct an accurate evaluation of a student?s work from the input provided by the peers.     We first took an in-depth look at the dynamics of online peer learning, and we performed the first broad study of the causes affecting the precision of peer grades. The results paint a complex picture.  We established that of the two elements that can affect grade precision: homework submission, and the student grader, the greater part of the grading error can be attributed to the submission, rather than to the grader.  In other words: imprecisions in the evaluation can be better explained in terms of difficult-to-grade submissions rather than imprecise students.  We also were able to rule out as minor many other factors, such as the amount of time dedicated to reviewing the homework submission, or the timeliness with which the reviews were performed.     This understanding led us to improve the tools used for peer review, adding means by which the submission authors can communicate anonymously with the peer reviewers and clarify as required aspects of the submitted work, leading to more accurate evaluations.  On the other hand, this also meant that our efforts to improve the methods for aggregating the student evaluations into a single evaluation were not very successful.  We planned to experiment with machine learning methods for understanding which students were more accurate in their reviews, and correct automatically any systematic evaluation biases.  However, our results meant that systematic biases among graders were small, and there was little that could be gained by following the approach.     Our research in providing incentives to accurate peer reviews resulted in a hierarchical incentive system that provides a provably unbiased incentive for students to be accurate in their evaluations, and that regardless of the number of students, it requires only a small and bounded work on the part of the instructor.     Our research into improving the accuracy of peer grading also resulted in foundational work in crowdsourcing.  Out of the algorithms we use for aggregating grades, we obtained efficient and highly accurate algorithms for aggregating Yes/No feedback from users.  This type of binary feedback is very common on the web, and is used, for instance, whenever sites need to flag inappropriate content.  Contrary to previous methods, our algorithm is well suited to cases where the amount of crowdsourced information is very non-uniform, with some items receiving large amounts of feedback, while others receiving little.     Recently, we have had success in applying our algorithms for aggregating binary feedback to the problem of detecting fake news.  The algorithms are applied to the bipartite graph consisting of news items on one side, and people (or bots) who have tweeted them on the other side.  The algorithms are able to propagate a limited ground truth consisting of known fake news and fake sites, into a classification encompassing a large percentage of the news being published every day.       Last Modified: 10/12/2018       Submitted by: Luca De Alfaro]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
