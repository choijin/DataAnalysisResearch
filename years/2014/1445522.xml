<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>An Exploration of the Alignment of SLDS Infrastructure and Data Highway To Relevant Success Indicators in Mathematics and Science</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>270982.00</AwardTotalIntnAmount>
<AwardAmount>299694</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Karen King</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project will examine the potential of data collected by the State Longitudinal Data Systems (SLDS) as a way to reflect national trends-and yet still address state information needs for which each system was designed. This project will examine the feasibility of how the SLDS infrastructure might be used to collect some of the data elements related to the STEM indicators proposed by the National Research Council in the report on Monitoring Progress Toward Successful K-12 Education: A Nation Advancing? The project explores the leverage points for action like the NCES Forum that can stimulate policy change in the data collections. Revisions designed to improve the data for national use will be weighed in terms of data quality as well as usefulness, timeliness, and burden forcing trade-offs and definition of needed supports. In addition, should an outcome be that the SLDS infrastructure cannot support some of the indicators, policymakers and other stakeholders will have clearer information about the need to establish other means. The outcome of the project will be a report, both across and within states, on the extent to which the SLDS infrastructure and data highway have the capacity to address the STEM indictors, focusing on a subset of 6 indicators that we think have the most potential. A second outcome will be a small convening that will generate action steps of what needs to be done and what might be done to plan for and implement future data collections that will address the STEM indicators.&lt;br/&gt;&lt;br/&gt;The project will focus on the intersection of STEM indicators and the data infrastructure at the state level to address those indicators. Specifically, the focus is the extent to which the infrastructure surrounding the SLDSs data highway can support the data needed to address the STEM indicators. State data systems are considerably more robust than 10 or even five years ago. Each state has now developed a SLDS containing hundreds, if not thousands of data elements disaggregated to the school level. Some of these data are required by the federal government and appear in the federal EDFacts system. Other data elements are unique to the specific states. The project will obtain data dictionaries from states and EDFacts and use them as a foundation with National Center for Education Statistics, STEM experts, and SLDS officials to determine how existing state data collections could address any of indicators, beginning with a subset that experts say a best aligned to SLDS current and future data collections and what revisions to the state systems would be necessary. The project has identified Indicators 1, 2, 3, 6, 7 and 8 as having the most potential. An outcome will be a set of recommendations and action steps as well as a statement, from states and others, of what role the NCES Forum and other supports might play in leveraging for the needed modifications to state data collections.</AbstractNarration>
<MinAmdLetterDate>07/30/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1445522</AwardID>
<Investigator>
<FirstName>Shandy</FirstName>
<LastName>Hauk</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shandy Hauk</PI_FULL_NAME>
<EmailAddress>shauk@wested.org</EmailAddress>
<PI_PHON>6503816445</PI_PHON>
<NSF_ID>000447420</NSF_ID>
<StartDate>08/10/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ellen</FirstName>
<LastName>Mandinach</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ellen B Mandinach</PI_FULL_NAME>
<EmailAddress>emandin@wested.org</EmailAddress>
<PI_PHON>2026749300</PI_PHON>
<NSF_ID>000628102</NSF_ID>
<StartDate>07/30/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Martin</FirstName>
<LastName>Orland</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martin Orland</PI_FULL_NAME>
<EmailAddress>martin.orland@gmail.com</EmailAddress>
<PI_PHON>2024712464</PI_PHON>
<NSF_ID>000658366</NSF_ID>
<StartDate>07/30/2014</StartDate>
<EndDate>08/10/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>WestEd</Name>
<CityName>San Francisco</CityName>
<ZipCode>941071242</ZipCode>
<PhoneNumber>4156153136</PhoneNumber>
<StreetAddress>730 Harrison Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>074653882</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WESTED</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>074653882</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[WestEd]]></Name>
<CityName>San Francisco</CityName>
<StateCode>CA</StateCode>
<ZipCode>941071242</ZipCode>
<StreetAddress><![CDATA[730 Harrison Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>7261</Code>
<Text>PROGRAM EVALUATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~270982</FUND_OBLG>
<FUND_OBLG>2015~28712</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><h1>Purpose</h1> <p><em>Monitoring Progress Toward Successful K-12 STEM Education</em> (National Research Council [NRC], 2013) described 14 indicators that could be used as common touchstones, across states, to monitor local and national progress in science, technology, engineering, and mathematics (STEM) education. The indicators serve the dual purpose of providing a common language for states to communicate about STEM education efforts and as foundation for national-level reporting to support advancement towards the NRC&rsquo;s goals to: inclusively increase the number of students who pursue advanced degrees/careers in STEM, expand and broaden the participation of traditionally under-served groups in the STEM workforce, and increase STEM literacy for all students.</p> <p><strong>The focal STEM Indicators for the project </strong>(as numbered in <em>Monitoring Progress</em>):</p> <p>1. Number of, and enrollment in, different types of STEM schools and programs, by district;</p> <p>2. Time allocated to teach science in grades K-5;</p> <p>3. Science-related learning opportunities in elementary schools;</p> <p>6. Teachers&rsquo; science and mathematics content knowledge for teaching;</p> <p>7. Teachers&rsquo; participation in STEM-specific professional development (PD) activities; and</p> <p>8. Instructional leaders&rsquo; participation in PD on creating conditions that support STEM learning.</p> <p>Measuring progress in each Indicator requires data. Begun as compliance-based warehousing for federal reporting, Statewide Longitudinal Data Systems (SLDSs) for education are now complex systems that shape diverse kinds of local and regional decision-making. The last decade has seen several federally funded efforts to support development of SLDSs and to create a shared dictionary, the U.S. Department of Education&rsquo;s <em>Common Education Data Standards</em> (CEDS).</p> <p>The driving question for the project was whether current SLDSs for education collect data sufficiently robust, comprehensive, and aligned to the Indicators to allow monitoring of progress.</p> <h1>Intellectual Merit</h1> <p>Informed by the latest research on educational structures, such as what makes something a &ldquo;STEM program,&rdquo; the project combined qualitative (document analysis and interview) with quantitative (descriptive statistics) methods. Project STEM experts identified classes of data needed to address the focal indicators and then examined 15 states&rsquo; SLDS data dictionaries and the national CEDS dictionary of data elements (more than 5000 entries in all). Additionally, project staff talked with staff in SLDS and STEM programs in six states (a total of 10 individual and group interviews) about current use and future viability of SLDSs as an indicator-supportive repository.</p> <p>The presence of indicator-aligned data elements in CEDS and state SLDS dictionaries varied widely. The most data elements pertained to Indicator 1, some for Indicators 6 and 7, a few for Indicator 8, and the fewest for Indicators 2 and 3. As a result, an ancillary question arose: To what extent do any standardized, national data repositories address the STEM indicators? The federal ED<em>Facts</em> and Civil Rights Data Collection (CRDC) repositories are limited and the CEDS database has not (yet) gained the state and local attention and &ldquo;buy-in&rdquo; needed (for more detail, see Hauk &amp; Mandinach, 2017)</p> <p>Interviews indicated that SLDS cultures differed in their communication structures and degree of cross-talk with STEM-specific state-level staff. For most state SLDS offices there was turnover in staffing on an annual, quarterly, even monthly basis. With the exception of one western state, the SLDS offices had in common a compliance-based approach, with little room for attention to improvement-based efforts in data curating or communication with STEM program staff. As a result, SLDS data generally did not systematically include process details that might be available from other state-level offices (e.g., from the office of teacher certification: an audit trail for teacher development of an online professional portfolio; whether or not a professional course for leaders has <em>Quality Matters </em>certification).</p> <p>&nbsp;</p> <h1>Broader Impacts</h1> <p>One impact of the project was to shape part of a workshop, on access to quality STEM programs, for staff in the U.S. Department of Education's Office for Civil Rights. Other impacts included the conversations the project started at regional meetings among district administrators and data staff, and during project interviews among state SLDS and STEM staff (most reported they had never talked with each other before). Certainly, the project's work indicates that if states mapped their SLDS data dictionaries to the CEDS dictionary, a national picture, if sparse, would be possible. The detail would, necessarily, be meager &ndash; particularly for Indicators 2 and 3. No professional structures (much less data elements) exist that allow local or state-level staff to gauge where and how there are time and opportunity to learn science in K-5 (much less how this is situated in the context of the other goals for each grade). And, there is no articulation between elementary, middle, and high school in science (e.g., processes like the teacher- and data-driven placement decisions in mathematics and reading). Thus, the future of the use of SLDSs is intimately linked to Indicator 9: Inclusion of science in federal and state accountability systems.</p><br> <p>            Last Modified: 10/30/2017<br>      Modified by: Shandy&nbsp;Hauk</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Purpose  Monitoring Progress Toward Successful K-12 STEM Education (National Research Council [NRC], 2013) described 14 indicators that could be used as common touchstones, across states, to monitor local and national progress in science, technology, engineering, and mathematics (STEM) education. The indicators serve the dual purpose of providing a common language for states to communicate about STEM education efforts and as foundation for national-level reporting to support advancement towards the NRC?s goals to: inclusively increase the number of students who pursue advanced degrees/careers in STEM, expand and broaden the participation of traditionally under-served groups in the STEM workforce, and increase STEM literacy for all students.  The focal STEM Indicators for the project (as numbered in Monitoring Progress):  1. Number of, and enrollment in, different types of STEM schools and programs, by district;  2. Time allocated to teach science in grades K-5;  3. Science-related learning opportunities in elementary schools;  6. Teachers? science and mathematics content knowledge for teaching;  7. Teachers? participation in STEM-specific professional development (PD) activities; and  8. Instructional leaders? participation in PD on creating conditions that support STEM learning.  Measuring progress in each Indicator requires data. Begun as compliance-based warehousing for federal reporting, Statewide Longitudinal Data Systems (SLDSs) for education are now complex systems that shape diverse kinds of local and regional decision-making. The last decade has seen several federally funded efforts to support development of SLDSs and to create a shared dictionary, the U.S. Department of Education?s Common Education Data Standards (CEDS).  The driving question for the project was whether current SLDSs for education collect data sufficiently robust, comprehensive, and aligned to the Indicators to allow monitoring of progress. Intellectual Merit  Informed by the latest research on educational structures, such as what makes something a "STEM program," the project combined qualitative (document analysis and interview) with quantitative (descriptive statistics) methods. Project STEM experts identified classes of data needed to address the focal indicators and then examined 15 states? SLDS data dictionaries and the national CEDS dictionary of data elements (more than 5000 entries in all). Additionally, project staff talked with staff in SLDS and STEM programs in six states (a total of 10 individual and group interviews) about current use and future viability of SLDSs as an indicator-supportive repository.  The presence of indicator-aligned data elements in CEDS and state SLDS dictionaries varied widely. The most data elements pertained to Indicator 1, some for Indicators 6 and 7, a few for Indicator 8, and the fewest for Indicators 2 and 3. As a result, an ancillary question arose: To what extent do any standardized, national data repositories address the STEM indicators? The federal EDFacts and Civil Rights Data Collection (CRDC) repositories are limited and the CEDS database has not (yet) gained the state and local attention and "buy-in" needed (for more detail, see Hauk &amp; Mandinach, 2017)  Interviews indicated that SLDS cultures differed in their communication structures and degree of cross-talk with STEM-specific state-level staff. For most state SLDS offices there was turnover in staffing on an annual, quarterly, even monthly basis. With the exception of one western state, the SLDS offices had in common a compliance-based approach, with little room for attention to improvement-based efforts in data curating or communication with STEM program staff. As a result, SLDS data generally did not systematically include process details that might be available from other state-level offices (e.g., from the office of teacher certification: an audit trail for teacher development of an online professional portfolio; whether or not a professional course for leaders has Quality Matters certification).    Broader Impacts  One impact of the project was to shape part of a workshop, on access to quality STEM programs, for staff in the U.S. Department of Education's Office for Civil Rights. Other impacts included the conversations the project started at regional meetings among district administrators and data staff, and during project interviews among state SLDS and STEM staff (most reported they had never talked with each other before). Certainly, the project's work indicates that if states mapped their SLDS data dictionaries to the CEDS dictionary, a national picture, if sparse, would be possible. The detail would, necessarily, be meager &ndash; particularly for Indicators 2 and 3. No professional structures (much less data elements) exist that allow local or state-level staff to gauge where and how there are time and opportunity to learn science in K-5 (much less how this is situated in the context of the other goals for each grade). And, there is no articulation between elementary, middle, and high school in science (e.g., processes like the teacher- and data-driven placement decisions in mathematics and reading). Thus, the future of the use of SLDSs is intimately linked to Indicator 9: Inclusion of science in federal and state accountability systems.       Last Modified: 10/30/2017       Submitted by: Shandy Hauk]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
