<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Understanding the Performance of Distributed Systems through Causal Tracing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2015</AwardEffectiveDate>
<AwardExpirationDate>02/29/2020</AwardExpirationDate>
<AwardTotalIntnAmount>576344.00</AwardTotalIntnAmount>
<AwardAmount>576344</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>CAREER: Understanding the Performance of Distributed Systems through Causal Tracing&lt;br/&gt; &lt;br/&gt;Society increasingly depends on shared software systems that are large, decentralized, with many components that interact in complex and subtle ways. These systems include financial and banking services, Web and cloud resources, airline reservations, and big data and scientific computing, to name a few. Despite their unquestionable reach and success, in these systems it is very hard to answer questions about the causes of failures, to uncover dependency issues among their components, to determine the impact of one operation on the rest of the system, or to provide guarantees about their performance to users. By developing and applying techniques to enable deep and real-time understanding of the performance characteristics of large-scale distributed systems, this project?s goal is to develop techniques that will enable users and providers of these systems to better express their needs and their guarantees in terms of performance, and better plan for and mitigate the effects of failures.  &lt;br/&gt;&lt;br/&gt;The main insight in this project is that because of the many components in distributed systems, the context of an operation initiated in one component gets lost as the operation involves other components. This makes it hard for a component deep in the system to discern with which client it is working, making it also hard to apply consistent policies or account for the cost of operations across component boundaries. This research will create the abstraction of a Tracing Plane that preserves this context throughout the entire execution of the system, allowing for debugging and diagnosis of performance problems, and for real-time provisioning of performance guarantees. This Tracing Plane will be a pervasive infrastructure to collect causal information from the execution of a distributed system and facilitate the efficient deployment of analytics and diagnostic tasks. Further, by aggregating information about tasks in the system across all components in a coherent way, the Tracing Plane enables the implementation of resource management policies that can act locally, in real-time,  and with global knowledge - which is presently not possible. &lt;br/&gt;&lt;br/&gt;We are better today at building large-scale distributed systems than we are at understanding precisely how they work, and how they fail and this will provide a core educational aspect, as the Tracing Plane is a strong pedagogical tool for the understanding of distributed systems structure and execution. This work will engage undergraduate and graduate students, as well as industry partners that operate such large-scale distributed systems.  By starting from increased visibility into these systems, the ultimate goal of this project is to provide tools and methods to allow building, operation, and management of large-scale, shared distributed systems that are efficient, reliable, and predictable. As society increasingly depends on systems of this kind, this research has a large and long lasting potential impact.</AbstractNarration>
<MinAmdLetterDate>03/03/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/06/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1452712</AwardID>
<Investigator>
<FirstName>Rodrigo</FirstName>
<LastName>Fonseca</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rodrigo Fonseca</PI_FULL_NAME>
<EmailAddress>rfonseca@cs.brown.edu</EmailAddress>
<PI_PHON>4018632777</PI_PHON>
<NSF_ID>000550621</NSF_ID>
<StartDate>03/03/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brown University</Name>
<CityName>Providence</CityName>
<ZipCode>029129002</ZipCode>
<PhoneNumber>4018632777</PhoneNumber>
<StreetAddress>BOX 1929</StreetAddress>
<StreetAddress2><![CDATA[350 Eddy Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<StateCode>RI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>RI01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001785542</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BROWN UNIVERSITY IN PROVIDENCE IN THE STATE OF RHODE ISLAND AND PROVIDENCE PLANTATIONS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001785542</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>Providence</CityName>
<StateCode>RI</StateCode>
<ZipCode>029129093</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Projects]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>RI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~220834</FUND_OBLG>
<FUND_OBLG>2017~232938</FUND_OBLG>
<FUND_OBLG>2019~122572</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The proposal for this project stated that society was depending ever more on complex, distributed computing systems, and this is even more true today than it was 5 years ago. As an example, when a large fraction of the workforce had to start working from home, due to the 2020 COVID pandemic, this made it patent how much of our economy depends on cloud computing, and on distributed communication and collaboration platforms. One common feature of all these systems is that they are very complex, have many components that are written and operated by different groups, and we are better at understanding how they work, than how they fail.</p> <p>Distributed tracing is a set of techniques that provide what is now called 'observability' into these distributed systems, enabling developers and operators to understand their behavior, better provision resources, perform security auditing, and find the root causes of problems when they do occur. When this project was proposed, it was not common practice to use distributed tracing in a systematized way: rather, it usually evolved in an ad-hoc fashion when operators would be at a loss to understand the behavior of their increasingly complex systems.</p> <p>The main goal of this project, and its main intellectual merit, was to advance the state of the art in distributed tracing, by proposing a systematic way to organize different uses of tracing in a layered architecture. To that effect, we proposed the Tracing Plane as a structured way to instrument systems such that (i) it would be modular, and (ii) support multiple uses of tracing. This means that components created independently could work together for tracing when integrated, and that this tracing substract would support multiple applications without needing to be changed.&nbsp;</p> <p>Earlier work in the project demonstrated different uses of causal tracing that go beyond examining records of past executions.&nbsp; For example, the project used causal tracing to apportion resources dynamically, in Retro (NSDI'2015), and to provide dynamic, global, and real-time queries about the performance of a system with Pivot Tracing (SOSP'15).&nbsp;</p> <p>These diverse uses proved valuable, but also showed commonalities and differences in their structure and use.&nbsp; This led to a natural decomposition of tracing systems into layers, with a separation of concerns: the lowest layer being concerned with following execution and propagating generic metadata; the middle layer with representing different data types with different behaviors when computation branches and merges; and the top layer with the semantics of the data: traces, metrics, counters, etc, that form a tracing application. To the developers of an infrastructure component, it becomes simple to instrument their system in a way that will later integrate with other instrumented components. On the other hand, high-level application developers get to use an abstraction of 'execution-scoped variables', which makes it easy to build tracing-like meta applications. This structuring is called Tracing Plane, and is described in detail in a 2018 publication, "Universal Context Propagation for Distributed System Instrumentation" (Eurosys'18).&nbsp;</p> <p>The last set of contributions concern what to do with detailed traces of many executions of a system. Vast amounts of tracing information can overload the cognitive capacity of human operators, and it becomes difficult to extract meaningful information from the data. Thus, the project started examining ways to concisely represent the different behaviors of the system with as few representative traces as possible. This was published in 'Weighted Sampling of Execution Traces' (SoCC'18), and has opened the path to new research.&nbsp; Beyond the publications, the project also produced a concrete implementation of the Tracing Plane stack, and demonstrated the reuse and multiplexing of several tracing applications on multiple underlying systems.&nbsp;</p> <p>The project had significant broader impacts, and directly contributed to the education of 2 PhD students, 3 Master's students, and 3 undergraduate students. The project&nbsp; directly influenced the specification of the emerging OpenTelemetry standard with the concept of Baggage for distributed context propagation. Members of the project gave talks and interacted with industry and open source developers to disseminate the ideas. Concepts and software from the project were used in three editions of the undergraduate Distributed Systems course at Brown, where almost 200 students could instrument their class projects and gain a much better understanding of the intricacies of writing and debugging these kinds of systems.</p> <p>Finally, the project had a number of expressive publications and awards: SOSP'15 (best paper award), NSDI'15, SoCC'16 and '18, and Eurosys. The PI gave several talks, including an invited keynote at the 17th International Conference on Runtime Verification, RV&rsquo;17, and Jonathan Mace's PhD dissertation, which includes much of the core research sponsored by the project, received an Honorable Mention for the 2018 ACM SIGOPS Dennis M. Ritchie Doctoral Dissertation Award.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/14/2020<br>      Modified by: Rodrigo&nbsp;Fonseca</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The proposal for this project stated that society was depending ever more on complex, distributed computing systems, and this is even more true today than it was 5 years ago. As an example, when a large fraction of the workforce had to start working from home, due to the 2020 COVID pandemic, this made it patent how much of our economy depends on cloud computing, and on distributed communication and collaboration platforms. One common feature of all these systems is that they are very complex, have many components that are written and operated by different groups, and we are better at understanding how they work, than how they fail.  Distributed tracing is a set of techniques that provide what is now called 'observability' into these distributed systems, enabling developers and operators to understand their behavior, better provision resources, perform security auditing, and find the root causes of problems when they do occur. When this project was proposed, it was not common practice to use distributed tracing in a systematized way: rather, it usually evolved in an ad-hoc fashion when operators would be at a loss to understand the behavior of their increasingly complex systems.  The main goal of this project, and its main intellectual merit, was to advance the state of the art in distributed tracing, by proposing a systematic way to organize different uses of tracing in a layered architecture. To that effect, we proposed the Tracing Plane as a structured way to instrument systems such that (i) it would be modular, and (ii) support multiple uses of tracing. This means that components created independently could work together for tracing when integrated, and that this tracing substract would support multiple applications without needing to be changed.   Earlier work in the project demonstrated different uses of causal tracing that go beyond examining records of past executions.  For example, the project used causal tracing to apportion resources dynamically, in Retro (NSDI'2015), and to provide dynamic, global, and real-time queries about the performance of a system with Pivot Tracing (SOSP'15).   These diverse uses proved valuable, but also showed commonalities and differences in their structure and use.  This led to a natural decomposition of tracing systems into layers, with a separation of concerns: the lowest layer being concerned with following execution and propagating generic metadata; the middle layer with representing different data types with different behaviors when computation branches and merges; and the top layer with the semantics of the data: traces, metrics, counters, etc, that form a tracing application. To the developers of an infrastructure component, it becomes simple to instrument their system in a way that will later integrate with other instrumented components. On the other hand, high-level application developers get to use an abstraction of 'execution-scoped variables', which makes it easy to build tracing-like meta applications. This structuring is called Tracing Plane, and is described in detail in a 2018 publication, "Universal Context Propagation for Distributed System Instrumentation" (Eurosys'18).   The last set of contributions concern what to do with detailed traces of many executions of a system. Vast amounts of tracing information can overload the cognitive capacity of human operators, and it becomes difficult to extract meaningful information from the data. Thus, the project started examining ways to concisely represent the different behaviors of the system with as few representative traces as possible. This was published in 'Weighted Sampling of Execution Traces' (SoCC'18), and has opened the path to new research.  Beyond the publications, the project also produced a concrete implementation of the Tracing Plane stack, and demonstrated the reuse and multiplexing of several tracing applications on multiple underlying systems.   The project had significant broader impacts, and directly contributed to the education of 2 PhD students, 3 Master's students, and 3 undergraduate students. The project  directly influenced the specification of the emerging OpenTelemetry standard with the concept of Baggage for distributed context propagation. Members of the project gave talks and interacted with industry and open source developers to disseminate the ideas. Concepts and software from the project were used in three editions of the undergraduate Distributed Systems course at Brown, where almost 200 students could instrument their class projects and gain a much better understanding of the intricacies of writing and debugging these kinds of systems.  Finally, the project had a number of expressive publications and awards: SOSP'15 (best paper award), NSDI'15, SoCC'16 and '18, and Eurosys. The PI gave several talks, including an invited keynote at the 17th International Conference on Runtime Verification, RV’17, and Jonathan Mace's PhD dissertation, which includes much of the core research sponsored by the project, received an Honorable Mention for the 2018 ACM SIGOPS Dennis M. Ritchie Doctoral Dissertation Award.          Last Modified: 09/14/2020       Submitted by: Rodrigo Fonseca]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
