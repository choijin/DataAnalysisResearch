<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>II-New: Seeing the Future: Ubiquitous Computing in EyeGlasses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>613623.00</AwardTotalIntnAmount>
<AwardAmount>613623</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is building a prototype ecosystem in which a collection of people can interact in an environment with the envisioned personal augmented reality (AR) device of the future research. The provided hardware serves as a platform for understanding and building of the ecosystem for personal AR devices that a user can ask "Where did I put my keys?" and get an instantaneous response from his/her eyeglasses. The envisioned system can also remind people to take medicine, create a collaborative reconstruction of an environment incorporating the perspectives of multiple people, or an app that enables one to shop for any item seen while walking down the street. All of these applications and more could be enabled by personal ubiquitous hands-free visual devices that have total awareness of the environment. &lt;br/&gt;&lt;br/&gt;This project provides hardware to enable research in developing technologies for personal AR systems that are unencumbered, ubiquitous, and can be used hands-free, with a wide field-of-view display overlaid to the user's view of the surroundings. Such devices would enable the development of societally and commercially beneficial applications such as assistance tools for physicians. The prototyped hardware and software components are supporting the research to reach these goals. The collaborative projects enabled by the prototypes are also studying unique research questions, including enhancing everyday life activities, health care management and training, as well as computer science research questions such as distributed interactive computer vision, distributed computing, energy/power management, security, and privacy. Broader impacts include developing applications for the disabled, tools for healthcare management, and elderly assistance.</AbstractNarration>
<MinAmdLetterDate>08/11/2014</MinAmdLetterDate>
<MaxAmdLetterDate>12/12/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1405847</AwardID>
<Investigator>
<FirstName>Henry</FirstName>
<LastName>Fuchs</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Henry Fuchs</PI_FULL_NAME>
<EmailAddress>fuchs@cs.unc.edu</EmailAddress>
<PI_PHON>9199714951</PI_PHON>
<NSF_ID>000451367</NSF_ID>
<StartDate>12/12/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Henry</FirstName>
<LastName>Fuchs</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Henry Fuchs</PI_FULL_NAME>
<EmailAddress>fuchs@cs.unc.edu</EmailAddress>
<PI_PHON>9199714951</PI_PHON>
<NSF_ID>000451367</NSF_ID>
<StartDate>08/11/2014</StartDate>
<EndDate>12/12/2019</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jan-Michael</FirstName>
<LastName>Frahm</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jan-Michael Frahm</PI_FULL_NAME>
<EmailAddress>jmf@cs.unc.edu</EmailAddress>
<PI_PHON>9195906003</PI_PHON>
<NSF_ID>000427356</NSF_ID>
<StartDate>08/11/2014</StartDate>
<EndDate>12/12/2019</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tamara</FirstName>
<LastName>Berg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tamara Berg</PI_FULL_NAME>
<EmailAddress>tlberg@cs.unc.edu</EmailAddress>
<PI_PHON>6465093361</PI_PHON>
<NSF_ID>000519059</NSF_ID>
<StartDate>08/11/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jay</FirstName>
<LastName>Aikat</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jay Aikat</PI_FULL_NAME>
<EmailAddress>ja@unc.edu</EmailAddress>
<PI_PHON>9195906178</PI_PHON>
<NSF_ID>000561232</NSF_ID>
<StartDate>08/11/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Cynthia</FirstName>
<LastName>Sturton</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cynthia Sturton</PI_FULL_NAME>
<EmailAddress>csturton@cs.unc.edu</EmailAddress>
<PI_PHON>9199663411</PI_PHON>
<NSF_ID>000657162</NSF_ID>
<StartDate>08/11/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S. Columbia St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~613623</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1"><span class="s1">In this project we aimed to research, design, and build prototypes of head-worn computer display systems, like &ldquo;smart&rdquo; eyeglasses, with which people can see a combination of both their ordinary surroundings and also computer-generated objects, for example 3D furniture or realistic life-size depictions of distant people. The tiny computers inside these &ldquo;smart&rdquo; eyeglasses, often called &ldquo;Augmented Reality&rdquo; devices, sense the precise location and orientation of the wearer in the room, so that as the wearer moves, even a tiny bit, the images of the computer-generated objects are quickly redrawn from the wearer&rsquo;s new point of view so that these computer-generated objects appear to be stationary in the room. Thus the user can walk around a computer-generated sofa sitting next to real furniture, or can walk around and interact with computer-generated images of remote participants as if they were colocated with them. Many people, including ourselves, believe that these kinds of displays and the computer networks and systems that connect them, will form a next-generation of teleconferencing, more comfortable and natural than today&rsquo;s 2D video-based systems such as Zoom and Skype.</span></p> <p class="p1">1. We developed several designs for more capable AR displays:</p> <p class="p1"><span class="s1"><span> </span>a) We built and tested a volumetric display that simultaneously displayed computer generated objects at all distances between 15 cm. and 400 cm. This capability is in contrast with most of today&rsquo;s commercial AR displays which put the virtual objects at a fixed location or at most two location distances from the user, causing visual strain and often headaches when users try to pay attention to both real and virtual objects at different locations of focus.</span></p> <p class="p1"><span class="s1"><span> </span>b) We built and tested a more compact display that presented the computer-generated only at a single depth, but a depth that could be rapidly changed when the system sensed the depth of the object of the viewer&rsquo;s current attention, sensed by cameras in the headset.</span></p> <p class="p1"><span class="s1"><span> </span>c) We also built and tested an advanced display that generates holographic images. Although it takes much more computations to generate than images to a conventional display, the holographic design may allow in the future more compact AR glasses with wider field of view.</span></p> <p class="p1"><span class="s1">2) We developed faster methods for determining the position and orientation of the headset in the local environment, called &ldquo;head tracking.&rdquo;<span>&nbsp; </span>In our approach, we use cameras on the headset, looking out into the local environment, like most current headsets. We however, don&rsquo;t wait for a complete image to arrive from the camera; we process each horizontal line (&ldquo;scan line&rdquo;) of the image as it comes from the camera sensor. In this way, we can determine the user&rsquo;s movement much faster than current systems, so display system received more up-to-date head-tracking information and generates more up-to-date the virtual objects that appear more stable than with current, slower commercial approaches.</span></p> <p class="p1"><span class="s1">3) We developed new approach to compute the 3D description of each user&rsquo;s body to send to the distant other(s) with whom they are interacting. In contrast to the 2D video images in today&rsquo;s teleconferencing systems, 3D models can be much more realistic, but require multiple cameras around each participant from which to construct the 3D model representation to send to the others &mdash; a major impediment to 3D telepresence-style teleconferencing. Our approach is to build these camera systems from miniature cameras mounted directly on the AR headset itself, pointing in all different direction to capture different parts of the user&rsquo;s body and the surrounding environment. The major difficulty with this approach is that all the cameras get very poor points of view of the user. We overcome this by training an artificial intelligence<span>&nbsp; </span>program (a deep-learning neural network) with thousands of video images taken in a room in which the user wears the AR headgear with the many miniature cameras, and the room also contains conventional cameras surrounding the user. In this way, the AI program &ldquo;learns&rdquo; which camera images from the AR headset correspond to which 3D shapes of the body, and after training, the user can go anywhere the headset-based system can construct the user&rsquo;s 3D body shape from only the miniature cameras, and, for now, from a few motion sensing devices worn on the wrists and ankles. These devices are the same as those in smart watches and exercise monitor wrist bands and motion sensors in sport shoes &mdash; sensors that in the future many people may wear daily anyway.</span></p> <p class="p1"><span class="s1">This project enabled advances in AR displays, in AR tracking, and in mobile, self-contained 3D construction of humans and environments. The project also made possible training in research for more than a dozen graduate and undergraduate students and at least one high school student.</span></p><br> <p>            Last Modified: 11/16/2020<br>      Modified by: Henry&nbsp;Fuchs</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[In this project we aimed to research, design, and build prototypes of head-worn computer display systems, like "smart" eyeglasses, with which people can see a combination of both their ordinary surroundings and also computer-generated objects, for example 3D furniture or realistic life-size depictions of distant people. The tiny computers inside these "smart" eyeglasses, often called "Augmented Reality" devices, sense the precise location and orientation of the wearer in the room, so that as the wearer moves, even a tiny bit, the images of the computer-generated objects are quickly redrawn from the wearer’s new point of view so that these computer-generated objects appear to be stationary in the room. Thus the user can walk around a computer-generated sofa sitting next to real furniture, or can walk around and interact with computer-generated images of remote participants as if they were colocated with them. Many people, including ourselves, believe that these kinds of displays and the computer networks and systems that connect them, will form a next-generation of teleconferencing, more comfortable and natural than today’s 2D video-based systems such as Zoom and Skype. 1. We developed several designs for more capable AR displays:  a) We built and tested a volumetric display that simultaneously displayed computer generated objects at all distances between 15 cm. and 400 cm. This capability is in contrast with most of today’s commercial AR displays which put the virtual objects at a fixed location or at most two location distances from the user, causing visual strain and often headaches when users try to pay attention to both real and virtual objects at different locations of focus.  b) We built and tested a more compact display that presented the computer-generated only at a single depth, but a depth that could be rapidly changed when the system sensed the depth of the object of the viewer’s current attention, sensed by cameras in the headset.  c) We also built and tested an advanced display that generates holographic images. Although it takes much more computations to generate than images to a conventional display, the holographic design may allow in the future more compact AR glasses with wider field of view. 2) We developed faster methods for determining the position and orientation of the headset in the local environment, called "head tracking."  In our approach, we use cameras on the headset, looking out into the local environment, like most current headsets. We however, don’t wait for a complete image to arrive from the camera; we process each horizontal line ("scan line") of the image as it comes from the camera sensor. In this way, we can determine the user’s movement much faster than current systems, so display system received more up-to-date head-tracking information and generates more up-to-date the virtual objects that appear more stable than with current, slower commercial approaches. 3) We developed new approach to compute the 3D description of each user’s body to send to the distant other(s) with whom they are interacting. In contrast to the 2D video images in today’s teleconferencing systems, 3D models can be much more realistic, but require multiple cameras around each participant from which to construct the 3D model representation to send to the others &mdash; a major impediment to 3D telepresence-style teleconferencing. Our approach is to build these camera systems from miniature cameras mounted directly on the AR headset itself, pointing in all different direction to capture different parts of the user’s body and the surrounding environment. The major difficulty with this approach is that all the cameras get very poor points of view of the user. We overcome this by training an artificial intelligence  program (a deep-learning neural network) with thousands of video images taken in a room in which the user wears the AR headgear with the many miniature cameras, and the room also contains conventional cameras surrounding the user. In this way, the AI program "learns" which camera images from the AR headset correspond to which 3D shapes of the body, and after training, the user can go anywhere the headset-based system can construct the user’s 3D body shape from only the miniature cameras, and, for now, from a few motion sensing devices worn on the wrists and ankles. These devices are the same as those in smart watches and exercise monitor wrist bands and motion sensors in sport shoes &mdash; sensors that in the future many people may wear daily anyway. This project enabled advances in AR displays, in AR tracking, and in mobile, self-contained 3D construction of humans and environments. The project also made possible training in research for more than a dozen graduate and undergraduate students and at least one high school student.       Last Modified: 11/16/2020       Submitted by: Henry Fuchs]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
