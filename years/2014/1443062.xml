<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Beyond Data Discovery: Shared Services for Community Metadata Improvement</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2015</AwardEffectiveDate>
<AwardExpirationDate>04/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1498604.00</AwardTotalIntnAmount>
<AwardAmount>1498604</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Amy Walton</SignBlockName>
<PO_EMAI>awalton@nsf.gov</PO_EMAI>
<PO_PHON>7032924538</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Science data and results must be well documented in order to be reproducible and re-usable.  Metadata -- ancillary contextual information such as science objectives, data provenance, and uncertainty estimates at each step -- is a fundamental part of the research documentation, reuse, and collaboration process.&lt;br/&gt;&lt;br/&gt;This project develops flexible tools for evaluating metadata, using consistent measurement systems that encourage community engagement, integrate guidance for improvement, and are a critical element in cross-community metadata improvement efforts.  Provision of these new metadata and data evaluation services across communities will improve the ability to integrate and reuse trustworthy data for crosscutting synthesis and analysis across science communities.  The focus on use metadata rather than discovery metadata is a significant shift in focus.  Use metadata is a fundamental building block needed to allow effective scientific analysis workflows.  The team builds a significant collaboration with several interdisciplinary partner organizations that provide guidance to this project.</AbstractNarration>
<MinAmdLetterDate>04/27/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/27/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1443062</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Jones</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew B Jones</PI_FULL_NAME>
<EmailAddress>jones@nceas.ucsb.edu</EmailAddress>
<PI_PHON>8058932500</PI_PHON>
<NSF_ID>000087236</NSF_ID>
<StartDate>04/27/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ray</FirstName>
<LastName>Habermann</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ray Habermann</PI_FULL_NAME>
<EmailAddress>ted@metadatagamechangers.com</EmailAddress>
<PI_PHON>3038191795</PI_PHON>
<NSF_ID>000644045</NSF_ID>
<StartDate>04/27/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>The HDF Group</Name>
<CityName>Champaign</CityName>
<ZipCode>618203871</ZipCode>
<PhoneNumber>2175316100</PhoneNumber>
<StreetAddress>410 E University Ave</StreetAddress>
<StreetAddress2><![CDATA[Suite 200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>614660236</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>HDF GROUP, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[The HDF Group]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207059</ZipCode>
<StreetAddress><![CDATA[1800 S. Oak Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7726</Code>
<Text>Data Cyberinfrastructure</Text>
</ProgramElement>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8048</Code>
<Text>Data Infrstr Bldg Blocks-DIBBs</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~1498604</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-2fdfcaf9-7fff-125b-ca51-b4a1cfc7aef5"> <p dir="ltr"><span>Scientific research increasingly focuses on challenges that span large regions and long time periods and involve data from many disciplines. &nbsp;These studies depend on our ability to reuse existing data that has been shared in data repositories along with high-quality metadata, i.e. documentation that describes the content, structure, and research context of the data in sufficient detail to enable discovery and correct interpretation. &nbsp;In this project, we developed approaches and systems for measuring the completeness and effectiveness of the metadata that is used to preserve, discover, access, and understand data from the Earth, environmental, and life sciences.</span></p> <p dir="ltr"><span>Communities that create and use data provide recommendations for metadata needed to address use cases for that are important to them. Rather than making new recommendations, we focused on facilitating evaluations of metadata collections using existing and emerging community recommendations. Communities use many dialects for naming metadata elements and many formats for storing these elements. Rather than making new dialects and formats, we focused on identifying concepts shared across communities and mapped between them to facilitate evaluations in many native dialects, i.e., using evaluation to encourage convergence among these communities of practice.</span></p> <p dir="ltr"><span>In addition to a diverse set of existing recommendations and dialects, the metadata environment continues to evolve as new requirements and capabilities emerge. Tracking the response of actual metadata to those changes provides insight into how effectively we as a community are documenting and preserving data for reuse in cross-disciplinary, synthetic studies, and for supporting reproducible scientific results.</span></p> <p dir="ltr"><span>We initially described and demonstrated this approach by examining the influence of community recommendations on metadata in several dialects from the NSF Long-Term Ecological Research (LTER) Program. We continued our LTER collaboration with a detailed &nbsp;examination of changes in metadata from all LTER sites through time. In this case, a network-wide migration to a single software environment had the most significant effect on metadata completeness and consistency.</span></p> <p dir="ltr"><span>Developing tools that implement these metadata evaluation capabilities and integrating evaluations into repository workflows was an important goal of this project. We achieved this goal with the development of the Metadata Quality Engine implemented for the NSF Arctic Data Center, the KNB Data Repository, and the DataONE network of over forty member data repositories. These DataONE repositories span many Earth Science disciplines, academic research institutions, and government agencies around the world. This is the first systematic metadata evaluation capability available for DataONE members and the impact of these tools will grow over time.</span></p> <p dir="ltr"><span>We also extended this evaluation work to metadata repositories at the center of international academic publishing and permanent identifier (PID) creation and management, CrossRef and DataCite. These repositories include metadata for tens of millions of research articles and datasets and are expanding into metadata for software, scientific instruments, samples, and other kinds of research objects. As these repositories grow, they can play important roles in identifying and connecting published papers to the people, institutions, data and software that contributed to the research behind the paper. Those connections depend on identifiers and links in the metadata. We found that kind of information is missing from many of the metadata records and that, in many cases, content is limited to the minimal required fields. The services and capabilities these repositories provide are changing rapidly. Consistent evaluation across providers in many disciplines can provide information about metadata required to support these new capabilities and good examples that demonstrate usage and benefits.</span></p> <p dir="ltr"><span>We found similar limitations in over twenty metadata dialects that had been mapped to the Codemeta vocabulary for metadata for software. The codemeta vocabulary included over 60 items that covered discovery, access, use, and understanding use cases. The dialects mapped to codemeta included only eleven of these items on average. Strong focus on discovery in many systems, tools, and recommendations needs to be overcome if these metadata are going to support interoperability and data reuse.</span></p> <p dir="ltr"><span>The tools we developed are now part of the DataONE infrastructure, and will continue to provide a systematic and quantitative means for data repository managers and researchers from around the US and the world to evaluate and improve the extent to which their data are Findable, Accessible, Interoperable, and Reusable (FAIR), which in turn will increase the long-term impact of research data by accelerating cross-disciplinary, synthetic research with existing data.</span></p> <br /></span></p> <p>&nbsp;</p><br> <p>            Last Modified: 06/13/2019<br>      Modified by: Ray&nbsp;Habermann</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Scientific research increasingly focuses on challenges that span large regions and long time periods and involve data from many disciplines.  These studies depend on our ability to reuse existing data that has been shared in data repositories along with high-quality metadata, i.e. documentation that describes the content, structure, and research context of the data in sufficient detail to enable discovery and correct interpretation.  In this project, we developed approaches and systems for measuring the completeness and effectiveness of the metadata that is used to preserve, discover, access, and understand data from the Earth, environmental, and life sciences. Communities that create and use data provide recommendations for metadata needed to address use cases for that are important to them. Rather than making new recommendations, we focused on facilitating evaluations of metadata collections using existing and emerging community recommendations. Communities use many dialects for naming metadata elements and many formats for storing these elements. Rather than making new dialects and formats, we focused on identifying concepts shared across communities and mapped between them to facilitate evaluations in many native dialects, i.e., using evaluation to encourage convergence among these communities of practice. In addition to a diverse set of existing recommendations and dialects, the metadata environment continues to evolve as new requirements and capabilities emerge. Tracking the response of actual metadata to those changes provides insight into how effectively we as a community are documenting and preserving data for reuse in cross-disciplinary, synthetic studies, and for supporting reproducible scientific results. We initially described and demonstrated this approach by examining the influence of community recommendations on metadata in several dialects from the NSF Long-Term Ecological Research (LTER) Program. We continued our LTER collaboration with a detailed  examination of changes in metadata from all LTER sites through time. In this case, a network-wide migration to a single software environment had the most significant effect on metadata completeness and consistency. Developing tools that implement these metadata evaluation capabilities and integrating evaluations into repository workflows was an important goal of this project. We achieved this goal with the development of the Metadata Quality Engine implemented for the NSF Arctic Data Center, the KNB Data Repository, and the DataONE network of over forty member data repositories. These DataONE repositories span many Earth Science disciplines, academic research institutions, and government agencies around the world. This is the first systematic metadata evaluation capability available for DataONE members and the impact of these tools will grow over time. We also extended this evaluation work to metadata repositories at the center of international academic publishing and permanent identifier (PID) creation and management, CrossRef and DataCite. These repositories include metadata for tens of millions of research articles and datasets and are expanding into metadata for software, scientific instruments, samples, and other kinds of research objects. As these repositories grow, they can play important roles in identifying and connecting published papers to the people, institutions, data and software that contributed to the research behind the paper. Those connections depend on identifiers and links in the metadata. We found that kind of information is missing from many of the metadata records and that, in many cases, content is limited to the minimal required fields. The services and capabilities these repositories provide are changing rapidly. Consistent evaluation across providers in many disciplines can provide information about metadata required to support these new capabilities and good examples that demonstrate usage and benefits. We found similar limitations in over twenty metadata dialects that had been mapped to the Codemeta vocabulary for metadata for software. The codemeta vocabulary included over 60 items that covered discovery, access, use, and understanding use cases. The dialects mapped to codemeta included only eleven of these items on average. Strong focus on discovery in many systems, tools, and recommendations needs to be overcome if these metadata are going to support interoperability and data reuse. The tools we developed are now part of the DataONE infrastructure, and will continue to provide a systematic and quantitative means for data repository managers and researchers from around the US and the world to evaluate and improve the extent to which their data are Findable, Accessible, Interoperable, and Reusable (FAIR), which in turn will increase the long-term impact of research data by accelerating cross-disciplinary, synthetic research with existing data.            Last Modified: 06/13/2019       Submitted by: Ray Habermann]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
