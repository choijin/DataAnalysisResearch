<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: Foundations for Learning in the Age of Big Data---New Frameworks and Algorithms for Interactive, Distributed, and Multi-Task Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Machine learning is a broad discipline with important application domains including computer vision, robotics, sustainability, and bio-surveillance. Its past successful evolution was heavily influenced by mathematical foundations developed for core problems of generalizing from labeled data. However, with the variety of applications of machine learning across science, engineering, and computing in the age of Big Data, re-examining the underlying foundations of the field has become imperative. This project aims to substantially advance the field of machine learning by developing foundations and algorithms for a number of important modern learning paradigms. These include interactive learning, where the algorithm and the domain expert engage in a two-way dialogue to facilitate more accurate learning from less data compared to the classic approach of passively observing labeled data; distributed learning, where a large dataset is distributed across multiple servers and the challenge lies in learning with limited communication; and multi-task learning, where the goal is to solve multiple related learning problems from less data by taking advantage of relationship among the learning tasks. The project also aims to develop new connections between machine learning and property testing, a flourishing area of theoretical computer science. In addition to solving fundamental questions in each of these directions, the project will highlight and leverage synergies between these topics.&lt;br/&gt;&lt;br/&gt;More specifically, the key research directions of this project are: (1) Developing mathematical foundations for interactive learning by analyzing new forms of interactions between the learning algorithm and the domain expert that could lead to fast and efficient learning of difficult tasks by wisely exploiting the capabilities of domain experts. (2) Developing new algorithms for distributed learning, an important modern scenario where data is distributed among several locations. This project will develop protocols that trade off the various types of resources involved in such settings (computation, communication, and domain expertise). (3) Developing new algorithms with provable guarantees for learning multiple related tasks from limited amounts of labeled data and massive amounts of unlabeled data by wisely exploiting explicitly known or latent relationships between the given tasks. (4) Developing mathematical foundations for property testing, where the question is to quickly determine whether there exists a low-error rule of a desired form by using significantly less data than needed to actually find the rule itself. This project will specifically focus on active and distributed scenarios, with the goal of using testing as a way to improve learning efficiency itself.&lt;br/&gt;&lt;br/&gt;Broader impacts include mentoring women in CS and actively organizing workshops and seminars in the interdisciplinary area.</AbstractNarration>
<MinAmdLetterDate>07/24/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422910</AwardID>
<Investigator>
<FirstName>Maria-Florina</FirstName>
<LastName>Balcan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maria-Florina Balcan</PI_FULL_NAME>
<EmailAddress>ninamf@cs.cmu.edu</EmailAddress>
<PI_PHON>4122685295</PI_PHON>
<NSF_ID>000537870</NSF_ID>
<StartDate>07/24/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Machine Learning studies the design of automatic methods for extracting information from data and for learning to make accurate predictions or useful decisions based on past observations and experience. Over the past decades, machine learning has evolved into a broad discipline with numerous and important application domains including computer vision, robotics, and computational biology. Its past successful development was heavily influenced by foundational work on passive supervised learning. However, new problems in science and computing, as well as the ever present explosion of data (the &ldquo;Big Data&rdquo; phenomenon) have rendered this classic paradigm insufficient. &nbsp;For example, the amounts of data available in many modern machine learning applications are now growing much faster than our ability to label it for training. This project has advanced machine learning by developing mathematical foundations and algorithms for new paradigms developed in recent years that go beyond the classic passive supervised learning paradigm for dealing with massive amounts of possibly very noisy data. &nbsp;</p> <p>More specifically, this project has developed new theory and algorithmic foundations for techniques that can leverage large amounts of readily available unlabeled or unannotated data, both in the classic single task setting, and in multi-task and transfer learning scenarios (where the goal is to transfer insights and representations from past&nbsp;learning tasks or experiences to future ones). Additionally, this project has provided new computationally efficient, noise tolerant algorithms for active learning, a modern learning paradigm where the learning algorithms only receive the classifications of examples when they ask for them. This project has also provided new techniques for distributed machine learning techniques, both in cases where the data is inherently distributed (such as distributed sensors or scientific experiments), and in cases where massive amounts of data are collected centrally, and for space and efficiency reasons this data must be dispatched to distributed machines in order to perform the processing needed.</p> <p>The research developed in this project was published in top machine learning and theory of computing conferences and journals, and it has contributed to training of several graduate students.</p><br> <p>            Last Modified: 10/29/2017<br>      Modified by: Maria-Florina&nbsp;Balcan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Machine Learning studies the design of automatic methods for extracting information from data and for learning to make accurate predictions or useful decisions based on past observations and experience. Over the past decades, machine learning has evolved into a broad discipline with numerous and important application domains including computer vision, robotics, and computational biology. Its past successful development was heavily influenced by foundational work on passive supervised learning. However, new problems in science and computing, as well as the ever present explosion of data (the "Big Data" phenomenon) have rendered this classic paradigm insufficient.  For example, the amounts of data available in many modern machine learning applications are now growing much faster than our ability to label it for training. This project has advanced machine learning by developing mathematical foundations and algorithms for new paradigms developed in recent years that go beyond the classic passive supervised learning paradigm for dealing with massive amounts of possibly very noisy data.    More specifically, this project has developed new theory and algorithmic foundations for techniques that can leverage large amounts of readily available unlabeled or unannotated data, both in the classic single task setting, and in multi-task and transfer learning scenarios (where the goal is to transfer insights and representations from past learning tasks or experiences to future ones). Additionally, this project has provided new computationally efficient, noise tolerant algorithms for active learning, a modern learning paradigm where the learning algorithms only receive the classifications of examples when they ask for them. This project has also provided new techniques for distributed machine learning techniques, both in cases where the data is inherently distributed (such as distributed sensors or scientific experiments), and in cases where massive amounts of data are collected centrally, and for space and efficiency reasons this data must be dispatched to distributed machines in order to perform the processing needed.  The research developed in this project was published in top machine learning and theory of computing conferences and journals, and it has contributed to training of several graduate students.       Last Modified: 10/29/2017       Submitted by: Maria-Florina Balcan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
