<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Effective Data Compression for Modern Memory Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>497618.00</AwardTotalIntnAmount>
<AwardAmount>497618</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many areas of science and technology have experienced transformational changes in recent years due to the increased ability to collect and analyze extremely large data sets.  As a result, computers are often limited by their ability to move data in and out of their processors (or CPUs).   One attractive approach to reducing this performance bottleneck would be to compress the data throughout the computer's memory system.  Although data compression has been successful in certain areas of computing (e.g., shrinking the size of files), it has not been widely adopted within modern memory systems.  This research is to develop a new memory compression technique that is fast enough, simple enough, and effective enough (in saving space) to make memory compression practical and attractive.&lt;br/&gt;&lt;br/&gt;The key insight to the proposed approach is that decompression latency and simplicity of design are far more critical than compression ratio when designing a compression scheme that is effective for modern memory. Preliminary work has identified simple and effective mechanisms for compressing data in on-chip caches and in main memory that achieve significant compression ratios while adding minimal access latency overhead. The simplicity of the proposed mechanisms also enables elegant solutions for dealing with the practical challenges of how on-chip caches and main memories are organized in modern systems.  The anticipated data compression framework will help improve the performance, cost, and energy efficiency of a wide spectrum of computer systems, ranging from high-end servers (including Cloud Computing, high-performance computing, and other distributed systems) down to mobile and embedded devices.&lt;br/&gt;&lt;br/&gt;The planned compression framework should relieve pressure on both the capacity of the various layers of the memory hierarchy (including caches, DRAM, non-volatile memory technologies, etc.) as well as the bandwidth of the interconnects that transfer data between these layers. This in turn would allow systems designers to avoid over-provisioning these resources. As a result, future computer systems should be better suited to the increasingly data-intensive workloads of the future.</AbstractNarration>
<MinAmdLetterDate>09/05/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423172</AwardID>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Mowry</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd Mowry</PI_FULL_NAME>
<EmailAddress>tcm@cs.cmu.edu</EmailAddress>
<PI_PHON>4122683725</PI_PHON>
<NSF_ID>000216388</NSF_ID>
<StartDate>09/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Onur</FirstName>
<LastName>Mutlu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Onur Mutlu</PI_FULL_NAME>
<EmailAddress>onur@cmu.edu</EmailAddress>
<PI_PHON>4122681186</PI_PHON>
<NSF_ID>000512629</NSF_ID>
<StartDate>09/05/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~497618</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As computation has become increasingly data-intensive, it has become increasingly difficult for modern memory systems to supply that data in a way that is fast, cost-effective, and energy-efficient.&nbsp; Given these challenges, it may be surprising that data compression has not been widely adopted in the memory hierarchy to date: by effectively reducing the amount of information that needs to be stored and transferred throughout the memory hierarchy, data compression offers the promise of improving performance, increasing cost-effectiveness, and reducing energy consumption (by essentially making a more modest memory system perform as though it were a more expensive memory system).&nbsp; In this project, we have developed new data compression techniques that overcome key stumbling blocks throughout the memory hierarchy (including the processor caches, main memory, and the memory system interconnects).</p> <p>Our first major outcome was the development of a set of new size-aware replacement policies for compressed on-chip caches (which we call CAMP: Compression-Aware Management Policies).&nbsp; CAMP improves system performance and energy efficiency compared to three state-of-the-art cache replacement mechanisms. Our policies are based on two key observations. First, we show that direct incorporation of the compressed cache block size into replacement decisions can be a basis for a more efficient replacement policy. Second, we find that the compressed block size can be used as an indicator of a block&rsquo;s future reuse in some applications. Our extensive evaluations show that CAMP, applied to modern last-level-caches (LLC), improves performance by 4.9%/9.0%/10.2% (on average for memory-intensive workloads) for single-core/two-/four-core workloads over the best state-of-the-art replacement mechanisms we evaluated.</p> <p>Our second major outcome was the development of a new, flexible framework (called CABA: Core-Assisted Bottleneck Acceleration) for accelerating GPUs by generating special &ldquo;assist warps&rdquo; that can execute code to speed up the performance of the main GPU computation.&nbsp; We illustrated this framework by using it specifically to enable a flexible form of data compression for GPU memory hierarchies (where cache blocks are compressed before they are written out to memory and decompressed before they are placed in the cache).&nbsp; CABA-based compression/decompression provides several benefits over a purely hardware-based implementation of data compression for GPU memory hierarchies. First, CABA primarily employs hardware that is already available on-chip but is otherwise underutilized. Second, different applications tend to have distinct data patterns that are more efficiently compressed with different compression algorithms, and CABA supports this flexibility in compression algorithm choice.&nbsp; Third, for GPU applications that do not benefit from compression, CABA can easily disable compression, thereby avoiding performance and energy overheads (compared with hardware compression).&nbsp; Our evaluations across a wide variety of applications show that on average, CABA-based compression reduces memory bandwidth by 2.1X, improves performance by 41.7%, and reduces overall system energy by 22.2%.</p> <p>Our third major outcome focused on the energy-efficiency of communication when compression is applied. While compression reduces the amount of transferred data, it leads to a substantial increase in the number of bit toggles (i.e., when communication channels switch from 0 to 1 or from 1 to 0). This increased toggle count increases the dynamic energy consumed by on-chip and off-chip memory buses due to more frequent charging and discharging of the wires. (Our results show that the total bit toggle count can increase from 20% to 2.2&times; when compression is applied for some compression algorithms, averaged across different application suites.) We characterize and demonstrate this new problem across 242 GPU applications and six different compression algorithms. To mitigate the problem, we proposed and evaluated two new toggle-aware compression techniques: Energy Control and Metadata Consolidation. Energy Control decides whether to send data in compressed or uncompressed form based upon a model that accounts for the compression ratio, the increase in bit toggles, and current bandwidth utilization.&nbsp; Metadata Consolidation reduces the negative effects of scattering the metadata across a compressed cache line by consolidating compression-related metadata in a contiguous fashion.&nbsp; These techniques greatly reduce the bit toggle count impact of the data compression algorithms we examine, while keeping most of their bandwidth reduction benefits.<br /><br /></p><br> <p>            Last Modified: 03/28/2018<br>      Modified by: Todd&nbsp;Mowry</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As computation has become increasingly data-intensive, it has become increasingly difficult for modern memory systems to supply that data in a way that is fast, cost-effective, and energy-efficient.  Given these challenges, it may be surprising that data compression has not been widely adopted in the memory hierarchy to date: by effectively reducing the amount of information that needs to be stored and transferred throughout the memory hierarchy, data compression offers the promise of improving performance, increasing cost-effectiveness, and reducing energy consumption (by essentially making a more modest memory system perform as though it were a more expensive memory system).  In this project, we have developed new data compression techniques that overcome key stumbling blocks throughout the memory hierarchy (including the processor caches, main memory, and the memory system interconnects).  Our first major outcome was the development of a set of new size-aware replacement policies for compressed on-chip caches (which we call CAMP: Compression-Aware Management Policies).  CAMP improves system performance and energy efficiency compared to three state-of-the-art cache replacement mechanisms. Our policies are based on two key observations. First, we show that direct incorporation of the compressed cache block size into replacement decisions can be a basis for a more efficient replacement policy. Second, we find that the compressed block size can be used as an indicator of a block?s future reuse in some applications. Our extensive evaluations show that CAMP, applied to modern last-level-caches (LLC), improves performance by 4.9%/9.0%/10.2% (on average for memory-intensive workloads) for single-core/two-/four-core workloads over the best state-of-the-art replacement mechanisms we evaluated.  Our second major outcome was the development of a new, flexible framework (called CABA: Core-Assisted Bottleneck Acceleration) for accelerating GPUs by generating special "assist warps" that can execute code to speed up the performance of the main GPU computation.  We illustrated this framework by using it specifically to enable a flexible form of data compression for GPU memory hierarchies (where cache blocks are compressed before they are written out to memory and decompressed before they are placed in the cache).  CABA-based compression/decompression provides several benefits over a purely hardware-based implementation of data compression for GPU memory hierarchies. First, CABA primarily employs hardware that is already available on-chip but is otherwise underutilized. Second, different applications tend to have distinct data patterns that are more efficiently compressed with different compression algorithms, and CABA supports this flexibility in compression algorithm choice.  Third, for GPU applications that do not benefit from compression, CABA can easily disable compression, thereby avoiding performance and energy overheads (compared with hardware compression).  Our evaluations across a wide variety of applications show that on average, CABA-based compression reduces memory bandwidth by 2.1X, improves performance by 41.7%, and reduces overall system energy by 22.2%.  Our third major outcome focused on the energy-efficiency of communication when compression is applied. While compression reduces the amount of transferred data, it leads to a substantial increase in the number of bit toggles (i.e., when communication channels switch from 0 to 1 or from 1 to 0). This increased toggle count increases the dynamic energy consumed by on-chip and off-chip memory buses due to more frequent charging and discharging of the wires. (Our results show that the total bit toggle count can increase from 20% to 2.2&times; when compression is applied for some compression algorithms, averaged across different application suites.) We characterize and demonstrate this new problem across 242 GPU applications and six different compression algorithms. To mitigate the problem, we proposed and evaluated two new toggle-aware compression techniques: Energy Control and Metadata Consolidation. Energy Control decides whether to send data in compressed or uncompressed form based upon a model that accounts for the compression ratio, the increase in bit toggles, and current bandwidth utilization.  Metadata Consolidation reduces the negative effects of scattering the metadata across a compressed cache line by consolidating compression-related metadata in a contiguous fashion.  These techniques greatly reduce the bit toggle count impact of the data compression algorithms we examine, while keeping most of their bandwidth reduction benefits.         Last Modified: 03/28/2018       Submitted by: Todd Mowry]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
