<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Human-Supervised Perception and Grasping in Clutter</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>01/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>496213.00</AwardTotalIntnAmount>
<AwardAmount>577607</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>One of the basic building blocks in semi-autonomous manipulation is the ability for a robot to grasp an object that a human operator indicates.  There are many tasks where the natural way for a human and robot to work together is for the human to point out the approximate locations of objects to be grasped and for the robot to generate the precise motions necessary to achieve the grasp.  This core "auto-grasp" functionality is critical to providing assistive manipulation for the disabled and elderly, as well as for a variety of military, police, space, or underwater applications. But implementing auto-grasp capability can be challenging in situations where the environment is cluttered, or when it is difficult to determine the grasp intention of the human.  In this collaborative project that combines expertise from two institutions, the PIs will tackle situations where it is necessary for the robot to actively explore or "interrogate" the environment in order to figure out what the human intends to grasp and how the robot should do it.  To these ends, the PIs will investigate a modified approach to planning under uncertainty known as belief space planning.  Belief space planning is well-suited to active localization for grasping, because it is a single framework in which the algorithm can reason about perception-oriented and goal-orientation parts of the task.  The PIs will use belief space planning to localize graspable geometries in the environment, known as grasp affordances, in a region indicated by the user.  They will also explore different ways in which a human can interact with the system in order to control the grasping.  The application focus of the work will be in assistive manipulation, where a person who is elderly or disabled operates an assistive robot arm mounted on an electric wheelchair or scooter.  User studies will determine the best methods for the target population to operate the system.  The project will contribute to the opportunities available for undergraduates and high school students in the PIs' institutions, and it will also be integrated as appropriate into the curricula of the courses they teach.&lt;br/&gt;&lt;br/&gt;This research contains two key innovations that the PIs expect will make robot grasping more robust.  The first is to incorporate ideas from belief space planning into the reach and grasp planning process.  Because belief space planning can reason about how the robot's own "state of information" is expected to change in the future, it is capable of producing plans that acquire task-relevant information in the course of performing a task.  The second innovation is a new approach to perception-for-grasping that localizes grasp affordance geometries in the neighborhoods of objects of potential interest.  Not only is this grasp affordance approach helpful to the belief space planner, but the PIs' preliminary work indicates that this approach can be accurate and very fast (10Hz).  Finally, the connection between the user interface and uncertainty in the location of the grasp target will also be explored, the plan being to model human behavior as an uncertain system where hidden variables describe user intention.</AbstractNarration>
<MinAmdLetterDate>08/11/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/28/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1426968</AwardID>
<Investigator>
<FirstName>Holly</FirstName>
<LastName>Yanco</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Holly A Yanco</PI_FULL_NAME>
<EmailAddress>holly@cs.uml.edu</EmailAddress>
<PI_PHON>9789343642</PI_PHON>
<NSF_ID>000278965</NSF_ID>
<StartDate>08/11/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Lowell</Name>
<CityName>Lowell</CityName>
<ZipCode>018543692</ZipCode>
<PhoneNumber>9789344170</PhoneNumber>
<StreetAddress>Office of Research Admin.</StreetAddress>
<StreetAddress2><![CDATA[600 Suffolk Street - Suite 212]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>956072490</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Lowell]]></Name>
<CityName>Lowell</CityName>
<StateCode>MA</StateCode>
<ZipCode>018543643</ZipCode>
<StreetAddress><![CDATA[One University Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7298</Code>
<Text>International Research Collab</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>5948</Code>
<Text>NETHERLANDS</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~496213</FUND_OBLG>
<FUND_OBLG>2015~57394</FUND_OBLG>
<FUND_OBLG>2016~8000</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project resulted in the creation of an assistive manipulation system, with an arm from a Rethink Robotics Baxter robot mounted to a mobility scooter. We developed an easy to use human-robot interaction process, where a person would aim a laser pointer at the object that he or she wanted the robot to pick up. In conjunction with an occupational therapist at our partner institution, Crotched Mountain Rehabiitation Center, we developed the laser pointer selection system for several access methods. Through our collaborative partner Northeastern University, our system was able to compute how to grasp objects that it had not previously seen before from cluttered environments, unlike prior research in the area.</p> <p>We also conducted studies to learn the best methods for handing an object to a person from a robot's gripper. We found that the best handovers occurred when the robot had a force sensor on one side of its gripper to allow it to sense when a person was touching the object in the gripper and when the person started to pull on the object in order to get it from the robot.</p> <p>We have also been investigating the best methods for a robot to indicate that it will be moving its arm before it does so, in order to prepare the person near the system. We are conducting user studies in which we are trying two different methods (small movements of the arm to get the attention of the user or methods using lights to get the attention of the user). A person's experience with an assistive device -- or any robot system -- will be shaped by understanding what the robot intends to do. This part of the research will allow us to learn what types of feedback would be best to use for human-robot interaction.</p> <p>Over the course of the project, three doctoral students, one masters student, and five undergraduate students have been trained to do research in robotics and assistive technology.</p><br> <p>            Last Modified: 06/09/2019<br>      Modified by: Holly&nbsp;A&nbsp;Yanco</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1426968/1426968_10331055_1560108189314_scooter_bigger_labels--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1426968/1426968_10331055_1560108189314_scooter_bigger_labels--rgov-800width.jpg" title="Scooter with robot arm"><img src="/por/images/Reports/POR/2019/1426968/1426968_10331055_1560108189314_scooter_bigger_labels--rgov-66x44.jpg" alt="Scooter with robot arm"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The assistive manipulator scooter developed for this project. The arm mounted on the scooter is one of the arms from a Rethink Robotics Baxter robot.</div> <div class="imageCredit">UMass Lowell and Northeastern University</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Holly&nbsp;A&nbsp;Yanco</div> <div class="imageTitle">Scooter with robot arm</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project resulted in the creation of an assistive manipulation system, with an arm from a Rethink Robotics Baxter robot mounted to a mobility scooter. We developed an easy to use human-robot interaction process, where a person would aim a laser pointer at the object that he or she wanted the robot to pick up. In conjunction with an occupational therapist at our partner institution, Crotched Mountain Rehabiitation Center, we developed the laser pointer selection system for several access methods. Through our collaborative partner Northeastern University, our system was able to compute how to grasp objects that it had not previously seen before from cluttered environments, unlike prior research in the area.  We also conducted studies to learn the best methods for handing an object to a person from a robot's gripper. We found that the best handovers occurred when the robot had a force sensor on one side of its gripper to allow it to sense when a person was touching the object in the gripper and when the person started to pull on the object in order to get it from the robot.  We have also been investigating the best methods for a robot to indicate that it will be moving its arm before it does so, in order to prepare the person near the system. We are conducting user studies in which we are trying two different methods (small movements of the arm to get the attention of the user or methods using lights to get the attention of the user). A person's experience with an assistive device -- or any robot system -- will be shaped by understanding what the robot intends to do. This part of the research will allow us to learn what types of feedback would be best to use for human-robot interaction.  Over the course of the project, three doctoral students, one masters student, and five undergraduate students have been trained to do research in robotics and assistive technology.       Last Modified: 06/09/2019       Submitted by: Holly A Yanco]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
