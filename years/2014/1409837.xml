<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Models of Handshape Articulatory Phonology for Recognition and Analysis of American Sign Language</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>854131.00</AwardTotalIntnAmount>
<AwardAmount>854131</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Sign languages are the primary means of communication for millions of Deaf people in the world, including about 350,000-500,000 American Sign Language (ASL) users in the US.  While the hearing population has benefited from advances in speech technologies such as speech recognition and spoken web search, much less progress has been made for sign language interfaces.  Advances depend on improved technology for analyzing sign language from video.  In addition, the linguistics of sign language is less well-understood than that of spoken language.  This project addresses both of these needs, with an interdisciplinary approach that will contribute to research in linguistics, language processing, computer vision, and machine learning.  Applications of the work include better access to ASL social media video archives, interactive recognition and search applications for Deaf individuals, and ASL-English interpretation assistance.&lt;br/&gt;&lt;br/&gt;This project focuses on handshape in ASL, in particular on one constrained but very practical component:  fingerspelling, or the spelling out of a word as a sequence of handshapes and trajectories between them.  Fingerspelling comprises up to 35% of ASL, depending on the context, and includes 72% of ASL handshapes, making it an excellent testing ground.  The project addresses gaps in existing work by focusing on handshape in various conditions, including fast, highly coarticulated signing.  The main project activities include development of (1) robust automatic detection and recognition of fingerspelled words using new handshape models, including segmental and "multi-segmental" graphical models of ASL phonological features; (2) techniques for generalizing across signers, styles, and recording conditions; (3) improved phonetics and phonology of handshape, in particular contributing to an articulatory phonology of sign; and (4) publicly released multi-speaker, multi-style fingerspelling data and associated semi-automatic annotation.</AbstractNarration>
<MinAmdLetterDate>06/09/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/09/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409837</AwardID>
<Investigator>
<FirstName>Karen</FirstName>
<LastName>Livescu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karen Livescu</PI_FULL_NAME>
<EmailAddress>klivescu@ttic.edu</EmailAddress>
<PI_PHON>7738342549</PI_PHON>
<NSF_ID>000512036</NSF_ID>
<StartDate>06/09/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gregory</FirstName>
<LastName>Shakhnarovich</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregory Shakhnarovich</PI_FULL_NAME>
<EmailAddress>greg@ttic.edu</EmailAddress>
<PI_PHON>7738342572</PI_PHON>
<NSF_ID>000554614</NSF_ID>
<StartDate>06/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Toyota Technological Institute at Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372803</ZipCode>
<PhoneNumber>7738340409</PhoneNumber>
<StreetAddress>6045 S Kenwood Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>127228927</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Toyota Technological Institute at Chicago]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>606372902</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~854131</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project focuses on automatic recognition of fingerspelling in American Sign Language (ASL) videos, as well as developing an improved understanding of the linguistic role and properties of fingerspelling within ASL.&nbsp; This is one part of a collaborative project focusing on development of both new computational methods and new linguistic understanding.&nbsp; The long-term goal is for computational innovation and analysis to help sign language users in their everyday lives.</p> <p>&nbsp;</p> <p>Fingerspelling is&nbsp;a part of ASL in which words are spelled out letter by letter (using the English alphabet), where each letter is represented by&nbsp;a distinct handshape.&nbsp; Fingerspelled words are typically names, technical words, or words borrowed from another language.&nbsp; Fingerspelling can also be used for emphasis.&nbsp; Recognizing fingerspelling has great practical importance because fingerspelled words are often some of the most important content words.&nbsp; At the same time, recognizing fingerspelling in video is very challenging because it involves quick, highly coarticuated motions of the fingers and hands.</p> <p>&nbsp;</p> <p>This project addresses the challenges of fingerspelling recognition with a combination of data collection and new research ideas in computer vision, sequence modeling, machine learning, and linguistics.&nbsp; The project also develops an improved linguistic understanding of fingerspelling through targeted data collection.&nbsp; The project has focused on two data sets of ASL fingerspelling:&nbsp; A pre-existing smaller studio data set, consisting of four native signers producing individual fingerspelled words from a list in a controlled lab setting; and a larger data set of fingerspelling "in the wild," collected and annotated as part of the project from naturally occurring videos on the web.&nbsp;&nbsp;</p> <p>&nbsp;</p> <p>The main outcomes of the project can be divided into (1) collection and annotation of the new data set, (2) contributions to the main task of ASL fingerspelling recognition from video, (3) contributions to related research areas (computer vision, machine learning), (4) contributions to the linguistics of fingerspelling, and (5) broader impact on other fields and on education.&nbsp; Each of these is described further below.</p> <p>&nbsp;</p> <p>(1) Data collection and annotation.&nbsp; This project has produced the largest fingerspelling video data set, and the only one including a large proportion of spontaneous ASL produced by signers for signers.&nbsp; We have found that our annotators can label such data with very high agreement, but the data is very challenging for automatic recognizers.&nbsp; We have also developed an approach for crowdsourcing the annotation, which allows us to collect data much more efficiently.&nbsp; This data set, and initial recognizers we have trained on the data, will be made publicly available to enable further study and improved recognizers.</p> <p>(2) The project has produced and studied a variety of models that advance the state of the art of fingerspelling recognition from video.&nbsp; These include models for recognizing fingerspelling by a known signer (that the model was trained for), an unknown signer, or a new signer for which a small amount of adaptation data is available.&nbsp; We have developed end-to-end neural models that do not rely on frame-by-frame annotation or a predefined lexicon of known words.&nbsp; Since sign language training data is scarce, we have also developed ways of leveraging unannotated hand images to improve recognition.</p> <p>(3) Along the way, this project has produced some methods that are more broadly applicable to computer vision tasks and other aritificial intelligence problems.&nbsp; In particular, we have developed methods for handling image blur and low resolution, building very deep neural networks, and learning from unlabeled sequential data.</p> <p>(4) The project has produced new linguistic understanding of fingerspelling.&nbsp; One outcome is an articulatory model of handshape, which parameterizes handshape and allows for direct measures of handshape similarity.&nbsp; Our analysis has also found that fingerspelling functions similarly to codeswitching.&nbsp; Finally, we have studied how fingerspelled forms sometimes evolve to form new lexical items.&nbsp; These insights will inform both future linguistic research and future work on automatically detecting fingerpselling within ASL video.</p> <p>(5) The project has had broader impact through the PIs' courses, which have used some of the materials produced by the project and have involved a broad range of students.&nbsp; In the longer term, the linguistic results are likely to have an impact on ASL teaching and interpreter training.&nbsp; Ultimately, the project will lead to improvements in interaction between Deaf and hearing individuals, as well as on increased usability and searchability of Deaf community video media.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/12/2018<br>      Modified by: Karen&nbsp;Livescu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project focuses on automatic recognition of fingerspelling in American Sign Language (ASL) videos, as well as developing an improved understanding of the linguistic role and properties of fingerspelling within ASL.  This is one part of a collaborative project focusing on development of both new computational methods and new linguistic understanding.  The long-term goal is for computational innovation and analysis to help sign language users in their everyday lives.     Fingerspelling is a part of ASL in which words are spelled out letter by letter (using the English alphabet), where each letter is represented by a distinct handshape.  Fingerspelled words are typically names, technical words, or words borrowed from another language.  Fingerspelling can also be used for emphasis.  Recognizing fingerspelling has great practical importance because fingerspelled words are often some of the most important content words.  At the same time, recognizing fingerspelling in video is very challenging because it involves quick, highly coarticuated motions of the fingers and hands.     This project addresses the challenges of fingerspelling recognition with a combination of data collection and new research ideas in computer vision, sequence modeling, machine learning, and linguistics.  The project also develops an improved linguistic understanding of fingerspelling through targeted data collection.  The project has focused on two data sets of ASL fingerspelling:  A pre-existing smaller studio data set, consisting of four native signers producing individual fingerspelled words from a list in a controlled lab setting; and a larger data set of fingerspelling "in the wild," collected and annotated as part of the project from naturally occurring videos on the web.       The main outcomes of the project can be divided into (1) collection and annotation of the new data set, (2) contributions to the main task of ASL fingerspelling recognition from video, (3) contributions to related research areas (computer vision, machine learning), (4) contributions to the linguistics of fingerspelling, and (5) broader impact on other fields and on education.  Each of these is described further below.     (1) Data collection and annotation.  This project has produced the largest fingerspelling video data set, and the only one including a large proportion of spontaneous ASL produced by signers for signers.  We have found that our annotators can label such data with very high agreement, but the data is very challenging for automatic recognizers.  We have also developed an approach for crowdsourcing the annotation, which allows us to collect data much more efficiently.  This data set, and initial recognizers we have trained on the data, will be made publicly available to enable further study and improved recognizers.  (2) The project has produced and studied a variety of models that advance the state of the art of fingerspelling recognition from video.  These include models for recognizing fingerspelling by a known signer (that the model was trained for), an unknown signer, or a new signer for which a small amount of adaptation data is available.  We have developed end-to-end neural models that do not rely on frame-by-frame annotation or a predefined lexicon of known words.  Since sign language training data is scarce, we have also developed ways of leveraging unannotated hand images to improve recognition.  (3) Along the way, this project has produced some methods that are more broadly applicable to computer vision tasks and other aritificial intelligence problems.  In particular, we have developed methods for handling image blur and low resolution, building very deep neural networks, and learning from unlabeled sequential data.  (4) The project has produced new linguistic understanding of fingerspelling.  One outcome is an articulatory model of handshape, which parameterizes handshape and allows for direct measures of handshape similarity.  Our analysis has also found that fingerspelling functions similarly to codeswitching.  Finally, we have studied how fingerspelled forms sometimes evolve to form new lexical items.  These insights will inform both future linguistic research and future work on automatically detecting fingerpselling within ASL video.  (5) The project has had broader impact through the PIs' courses, which have used some of the materials produced by the project and have involved a broad range of students.  In the longer term, the linguistic results are likely to have an impact on ASL teaching and interpreter training.  Ultimately, the project will lead to improvements in interaction between Deaf and hearing individuals, as well as on increased usability and searchability of Deaf community video media.          Last Modified: 08/12/2018       Submitted by: Karen Livescu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
