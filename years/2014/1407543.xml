<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative research:   Statistical and computational efficiency for massive data sets via approximation-regularization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>79996.00</AwardTotalIntnAmount>
<AwardAmount>79996</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project integrates approximation methodology from computer science with modern statistical theory to improve analysis of large data sets. Modern statistical analysis requires methods that are computationally feasible on large datasets while at the same time preserving statistical efficiency. Frequently, these two concerns are seen as contradictory: approximation methods that enable computation are assumed to degrade statistical performance relative to exact methods. The statistical perspective is that the exact solution is undesirable, and a regularized solution is preferred. Regularization can be thought of as formalizing a trade-off between fidelity to the data and adherence to prior knowledge about the data-generating process such as smoothness or sparsity. The resulting estimator tends to be more useful, interpretable, and suitable as an input to other methods. Conversely, in computer science applications, where much of the current work on approximation methods resides, the inputs are generally considered to be observed exactly. The prevailing philosophy is that while the exact problem is, regrettably, unsolvable, any approximate solution should be as close as possible to the exact one. We make a crucial realization: that the approximation methods themselves naturally lead to regularization, suggesting the intriguing possibility that some computational approximations can simultaneously enable the analysis of massive data while enhancing statistical performance. &lt;br/&gt;&lt;br/&gt;Our research develops new methods that leverage this phenomenon, which we have dubbed 'approximation-regularization.' The first method uses a matrix pre-conditioner to stabilize the least-squares criterion. If properly calibrated, this approach provides computational and storage advantages over regularized least squares while providing a statistically superior solution. A second innovation addresses principal components analysis (PCA) for regression on large data sets where PCA is both computationally infeasible and known to be statistically inconsistent. By employing randomized approximations, we can address both of these issues, while improving predictions at the same time. Lastly, we introduce new methods for unsupervised dimension reduction, whereby approximation algorithms that leverage sparsity, and statistical methods that induce it, enable the use of spectral techniques on very large matrices. In each of these cases, approximation-regularization yields both computational and statistical gains relative to existing methodologies. This research recognizes that approximation is regularization and can thereby increase statistical accuracy while enabling computation. It will result in new statistical methods for large datasets, which are computationally and statistically preferable to existing approaches, while also bringing attention to this important area in statistics. Additionally, these methods will permit scientists in other fields, such as astronomy, genetics, text and image processing, climate science, and forecasting, to make ready use of available data.</AbstractNarration>
<MinAmdLetterDate>08/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/22/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1407543</AwardID>
<Investigator>
<FirstName>Donald</FirstName>
<LastName>Estep</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Donald J Estep</PI_FULL_NAME>
<EmailAddress>donald.estep@colostate.edu</EmailAddress>
<PI_PHON>9704916722</PI_PHON>
<NSF_ID>000281960</NSF_ID>
<StartDate>05/22/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Darren</FirstName>
<LastName>Homrighausen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Darren Homrighausen</PI_FULL_NAME>
<EmailAddress>darrenho@stat.colostate.edu</EmailAddress>
<PI_PHON>9704916355</PI_PHON>
<NSF_ID>000634782</NSF_ID>
<StartDate>05/22/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Darren</FirstName>
<LastName>Homrighausen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Darren Homrighausen</PI_FULL_NAME>
<EmailAddress>darrenho@stat.colostate.edu</EmailAddress>
<PI_PHON>9704916355</PI_PHON>
<NSF_ID>000634782</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate>05/22/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Colorado State University</Name>
<CityName>Fort Collins</CityName>
<ZipCode>805232002</ZipCode>
<PhoneNumber>9704916355</PhoneNumber>
<StreetAddress>601 S Howes St</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>785979618</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLORADO STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>948905492</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Colorado State University]]></Name>
<CityName>Fort Collins</CityName>
<StateCode>CO</StateCode>
<ZipCode>805214593</ZipCode>
<StreetAddress><![CDATA[200 W. Lake St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~79996</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The rate at which computers can generate data has far outpaced the growth in computational and storage resources.&nbsp; Hence, a major and increasing obstacle is to generalize or adapt existing statistical procedures so that they are usable under this newer regime of very large data sets. Generally, statistical methods are implemented via the introduction of various necessary computational approximations. The intellectual merit of this project is to investigate and publicize the idea that these computational approximations, if properly done, can actually improve statistical performance relative to the unapproximated version of a method.&nbsp;</p> <p>This idea, which we dub approximation&shy;-regularization, is very important as the prevailing wisdom has been that approximations involve a trade&shy;-off of computational/storage complexity versus statistical accuracy. Our research has applied this idea to some of the most commonly used statistical techniques available today. We show that one can improve on these procedures by simultaneously making them more computationally tractable and more accurate. This has the potential to open the door to for a nearly unlimited amount of future insights due to facilitating the analysis of ever larger and more complex data sets.</p> <p>This research broadly impacts other areas,&nbsp;such as astronomy, genetics, text and image processing, climate science, and forecasting by enabling computations on much larger data sets while simultaneously improving the accuracy of some workhorse statistical procedures.&nbsp; Additionally, even practitioners&nbsp;outside academia can use and explore these ideas via freely available software that has been developed. This will allow even non-experts in industry and the American economy at large to incorporate these methods and ideas into their business practices.</p> <p>More specifically, this research has produced two peered reviewed manuscripts as well as two more manuscripts that are in review and a fifth manuscript that is still being developed.&nbsp; These papers deeply explore the idea of approximation-regularization and apply it to two common statistical methods: regularized regression and principal components. Also, publicly available software has been developed in the form of an R package: cplr.&nbsp; Furthermore, these projects have simultaneously been used to train the next generation of statisticians by providing financial and intellectual&nbsp;support to developing researchers.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/04/2017<br>      Modified by: Darren&nbsp;Homrighausen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The rate at which computers can generate data has far outpaced the growth in computational and storage resources.  Hence, a major and increasing obstacle is to generalize or adapt existing statistical procedures so that they are usable under this newer regime of very large data sets. Generally, statistical methods are implemented via the introduction of various necessary computational approximations. The intellectual merit of this project is to investigate and publicize the idea that these computational approximations, if properly done, can actually improve statistical performance relative to the unapproximated version of a method.   This idea, which we dub approximation&shy;-regularization, is very important as the prevailing wisdom has been that approximations involve a trade&shy;-off of computational/storage complexity versus statistical accuracy. Our research has applied this idea to some of the most commonly used statistical techniques available today. We show that one can improve on these procedures by simultaneously making them more computationally tractable and more accurate. This has the potential to open the door to for a nearly unlimited amount of future insights due to facilitating the analysis of ever larger and more complex data sets.  This research broadly impacts other areas, such as astronomy, genetics, text and image processing, climate science, and forecasting by enabling computations on much larger data sets while simultaneously improving the accuracy of some workhorse statistical procedures.  Additionally, even practitioners outside academia can use and explore these ideas via freely available software that has been developed. This will allow even non-experts in industry and the American economy at large to incorporate these methods and ideas into their business practices.  More specifically, this research has produced two peered reviewed manuscripts as well as two more manuscripts that are in review and a fifth manuscript that is still being developed.  These papers deeply explore the idea of approximation-regularization and apply it to two common statistical methods: regularized regression and principal components. Also, publicly available software has been developed in the form of an R package: cplr.  Furthermore, these projects have simultaneously been used to train the next generation of statisticians by providing financial and intellectual support to developing researchers.                Last Modified: 10/04/2017       Submitted by: Darren Homrighausen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
