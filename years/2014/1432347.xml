<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Research in Student Peer Review: A Cooperative Web-Services Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1034166.00</AwardTotalIntnAmount>
<AwardAmount>1074166</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Talitha Washington</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Hundreds of thousands of students have used online peer review applications to review their classmates' work. While learning gains from peer review have been documented repeatedly, current systems do not always produce accurate scores and often give inadequate guidance to students about what constitutes a good review, resulting in haphazard feedback. This research addresses these issues with a common set of web services that can be used by any peer review system, as well as new visualizations that identify students' strengths and weaknesses, and gauge improvement over time.&lt;br/&gt;&lt;br/&gt;This project differs from previous research that typically involves a single peer review system. It will develop a set of web services that will be usable by any peer review system in the same way that Google Maps is available to any website that wants to display location data. This common implementation will allow the project team to gather data from large numbers of students in a wide variety of contexts, thereby giving us the statistical power to produce more convincing, highly generalizable results&lt;br/&gt;&lt;br/&gt;Peer review during the writing process is an example of formative assessment, feedback that is received while the recipient still has a chance to improve his/her work. Several studies have found that the students who benefit most from formative assessment are those who typically underperform as measured by exams and standardized tests. Formative assessment tends to level the playing field for underrepresented minorities by allowing these students to receive input from their peers when they are not stressed about how their grade is being affected.</AbstractNarration>
<MinAmdLetterDate>08/05/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/19/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1432347</AwardID>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Gehringer</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward F Gehringer</PI_FULL_NAME>
<EmailAddress>efg@ncsu.edu</EmailAddress>
<PI_PHON>9195152066</PI_PHON>
<NSF_ID>000328919</NSF_ID>
<StartDate>08/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>North Carolina State University</Name>
<CityName>Raleigh</CityName>
<ZipCode>276957514</ZipCode>
<PhoneNumber>9195152444</PhoneNumber>
<StreetAddress>2601 Wolf Village Way</StreetAddress>
<StreetAddress2><![CDATA[Admin. III, STE 240]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042092122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTH CAROLINA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[North Carolina State University]]></Name>
<CityName>Raleigh</CityName>
<StateCode>NC</StateCode>
<ZipCode>276958206</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>1998</Code>
<Text>IUSE</Text>
</ProgramElement>
<ProgramReference>
<Code>8209</Code>
<Text>Improv Undergrad STEM Ed(IUSE)</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~1034166</FUND_OBLG>
<FUND_OBLG>2015~40000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-168b2017-7fff-dbaa-ab8f-1567ffcc25e3"> </span></p> <p dir="ltr"><span>When we were in school most of us were familiar with the teacher, or maybe the teaching assistant, giving feedback and assigning grades. Research shows that there are major benefits in involving students in the grading process itself.&nbsp; When students review each other's work they have to think more carefully about the requirements for the assignment, and that extra effort leads to deeper learning.  The students themselves may pick up tips or suggestions from their classmates that help them improve their work.</span></p> <p dir="ltr"><span>Peer assessment is the process of students reviewing each other's work. Initially, assessment was done by having the students exchange papers, but since the mid-1990s the process has increasingly migrated online.&nbsp; Many online applications have been developed for peer assessment. We were aware of about 20 of them when we started this work and now we know of at least 60.  These systems have been developed independently, but they all have to perform any of the same functions, such as allowing students and instructors to visualize the results in a concise format, and helping the instructor decide who the effective reviewers are.</span></p> <p dir="ltr"><span>The Peerlogic project&nbsp; had two ideas: Rather than having each system use its own way of visualizing and deciding on the good reviewers, we would implement the functionality once as a web service and allow any peer-assessment system to make use of it. &nbsp; (A web service is a component that an application running on the web can call to perform some service for it.  A good example is Google Maps, which is called by many different web sites.)  Second, the Peerlogic project wanted to gather together data from a number of peer-assessment systems so the researchers could use the data in testing hypotheses about the process of peer assessment.&nbsp; By performing these two services, we would go a long way toward making the peer-assessment community, which had been sets of isolated researchers, each with their own system, into an integrated research field.</span></p> <p dir="ltr"><span>One of the visualisations that we developed under the project was called the rainbow graph.&nbsp; It's a way for students who are being ranked by their peer reviewers to see where they fare in the class.&nbsp; An example is shown at the right.  Students review five other students' work. and rank their peers from 1st to 5th place compared to each other. A student who gets several first-place ranks has a taller overall bar than the student who gets lesser evaluations.&nbsp; If students are asked to rank themselves along with their classmates, then the graph shows who rated themselves better or worse than their peers.</span></p> <p dir="ltr"><span>As a way to find good reviewers, we developed several metrics for peer assessments.&nbsp; The first is sentiment.  A peer assessment should not be too negative, because the advice in a negative-sounding review is likely to be disregarded.&nbsp; So our first web service  measured sentiment.   A good review detects problems in the reviewed work, so we trained a machine-learning model to recognize when problems were being pointed out.&nbsp; A good review contains suggestions for how to improve.   We used natural language processing and machine learning tools to recognize suggestions, and achieved accuracies of over 90%.</span></p> <p dir="ltr"><span>What this means is that competent reviewers can be identified automatically.&nbsp; Instructors can use these metrics to give credit to students for reviewing, and also to know whose scores to trust when assigning grades for submitted work. As research progresses, we might even be able to assign peer grades automatically in most cases, allowing the instructor to concentrate on grading those students who have not been competently assessed by their peers.&nbsp; The result would be that students are graded accurately receive better feedback along with their grades, at the same time they are learning from assessing others.</span></p><br> <p>            Last Modified: 11/30/2019<br>      Modified by: Edward&nbsp;F&nbsp;Gehringer</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1432347/1432347_10328838_1575166905897_rainbow--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1432347/1432347_10328838_1575166905897_rainbow--rgov-800width.jpg" title="A &quot;rainbow graph&quot; to visualize student rankings"><img src="/por/images/Reports/POR/2019/1432347/1432347_10328838_1575166905897_rainbow--rgov-66x44.jpg" alt="A &quot;rainbow graph&quot; to visualize student rankings"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The graph shows one column for each student in the class, about 50 columns overall.  The leftmost column is the student with the whole lot with the highest overall rank.  That student got six 1st-place scores and 1 second.  Students with lower ranks have shorter overall bars.</div> <div class="imageCredit">David Tinapple</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Edward&nbsp;F&nbsp;Gehringer</div> <div class="imageTitle">A "rainbow graph" to visualize student rankings</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   When we were in school most of us were familiar with the teacher, or maybe the teaching assistant, giving feedback and assigning grades. Research shows that there are major benefits in involving students in the grading process itself.  When students review each other's work they have to think more carefully about the requirements for the assignment, and that extra effort leads to deeper learning.  The students themselves may pick up tips or suggestions from their classmates that help them improve their work. Peer assessment is the process of students reviewing each other's work. Initially, assessment was done by having the students exchange papers, but since the mid-1990s the process has increasingly migrated online.  Many online applications have been developed for peer assessment. We were aware of about 20 of them when we started this work and now we know of at least 60.  These systems have been developed independently, but they all have to perform any of the same functions, such as allowing students and instructors to visualize the results in a concise format, and helping the instructor decide who the effective reviewers are. The Peerlogic project  had two ideas: Rather than having each system use its own way of visualizing and deciding on the good reviewers, we would implement the functionality once as a web service and allow any peer-assessment system to make use of it.   (A web service is a component that an application running on the web can call to perform some service for it.  A good example is Google Maps, which is called by many different web sites.)  Second, the Peerlogic project wanted to gather together data from a number of peer-assessment systems so the researchers could use the data in testing hypotheses about the process of peer assessment.  By performing these two services, we would go a long way toward making the peer-assessment community, which had been sets of isolated researchers, each with their own system, into an integrated research field. One of the visualisations that we developed under the project was called the rainbow graph.  It's a way for students who are being ranked by their peer reviewers to see where they fare in the class.  An example is shown at the right.  Students review five other students' work. and rank their peers from 1st to 5th place compared to each other. A student who gets several first-place ranks has a taller overall bar than the student who gets lesser evaluations.  If students are asked to rank themselves along with their classmates, then the graph shows who rated themselves better or worse than their peers. As a way to find good reviewers, we developed several metrics for peer assessments.  The first is sentiment.  A peer assessment should not be too negative, because the advice in a negative-sounding review is likely to be disregarded.  So our first web service  measured sentiment.   A good review detects problems in the reviewed work, so we trained a machine-learning model to recognize when problems were being pointed out.  A good review contains suggestions for how to improve.   We used natural language processing and machine learning tools to recognize suggestions, and achieved accuracies of over 90%. What this means is that competent reviewers can be identified automatically.  Instructors can use these metrics to give credit to students for reviewing, and also to know whose scores to trust when assigning grades for submitted work. As research progresses, we might even be able to assign peer grades automatically in most cases, allowing the instructor to concentrate on grading those students who have not been competently assessed by their peers.  The result would be that students are graded accurately receive better feedback along with their grades, at the same time they are learning from assessing others.       Last Modified: 11/30/2019       Submitted by: Edward F Gehringer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
