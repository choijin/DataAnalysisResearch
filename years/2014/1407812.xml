<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Better efficiency, better forecasting, better accuracy: A new light on the dependence structure in high frequency data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>196149.00</AwardTotalIntnAmount>
<AwardAmount>196149</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recent years have seen an explosion in the availability and size of data in many areas of endeavor; the phenomenon is often referred to as big data. This project is concerned with a particular form of such data, namely high frequency data (HFD), where series of observations can see new data to arrive in fractions of milliseconds. HFD occurs in medicine, in finance and economics, in certain recordings relating to the environment, and perhaps in other areas. Research is often concerned with how to turn this data into knowledge, and this is where the current project will help. Specifically, the project has discovered a new way to look at the dependence relationships between the parameters governing the state of the HFD system. The new dependence structure permits the borrowing of information from adjacent time periods, and also from other series if one has a panel of data. The consequences of this new approach are being explored by the project. The research produces transformational improvements in the statistical handling of high frequency data.&lt;br/&gt;&lt;br/&gt;The new way to look at dependence involves the representation of series of ordinary integrals with the help of stochastic integrals. This permits the use of high frequency regression techniques to connect the information in adjacent time intervals. It is achieved without altering current models. This has far-reaching consequences, leading to more efficient estimators, better prediction, and, in terms of accuracy, a more systematic treatment of the estimation of standard errors. Model selection will also be greatly facilitated. The methodology does not depend on either time or panel size being large; neither does it depend on assumptions such as stationarity of the data series. All the new dependence relationships can be consistently estimated from high frequency data inside the relevant time periods. Efficiency gains are at the very least close to 50%, and thus existing efficiency bounds will become irrelevant. It is expected that this approach will form a new paradigm for high frequency data. In addition to developing a general theory, the project is concerned with applications to financial data. Applied quantities of interest include realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with improved standard errors, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain. The dependence structure also has application in other areas of research that have high frequency data, including medicine, neural science, and turbulence.</AbstractNarration>
<MinAmdLetterDate>08/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1407812</AwardID>
<Investigator>
<FirstName>Per</FirstName>
<LastName>Mykland</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Per Mykland</PI_FULL_NAME>
<EmailAddress>mykland@galton.uchicago.edu</EmailAddress>
<PI_PHON>7737028044</PI_PHON>
<NSF_ID>000450400</NSF_ID>
<StartDate>08/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606375418</ZipCode>
<StreetAddress><![CDATA[5734 S. University Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~196149</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Background: Recent years have seen an explosion in the availability of high frequency financial data. The data is usually called tick-by-tick data: every quote and every transaction is recorded. The data arrive at high frequency: for some stocks, there are more than a hundred thousand transactions per day. However, this vast amount of rapidly arriving data is not free of noise and its information content is often masked. An important question which faces us, as a society, is how to turn this data into knowledge and uncover its informational architecture. This project has made progress in this direction.<br /><br />Big data often lead to more uncertainty, which makes it hard to reveal the underlying structure. In the first part of this project ("Assessment of Uncertainty in High Frequency Data: The Observed Asymptotic Variance"), the PIs has made contributions on three fronts:<br /><br />1) the PIs have discovered a new way to look at the dependence relationships governing the latent structures in the data. Each latent element is estimated from high frequency data series. Then the dependence structure between latent elements in adjacent time periods is established. The new structure will permit the "borrowing" of information from adjacent time periods, and also from other data series if one has a panel of data. The methodology does not depend on either time or panel size being large, neither does it depend on mathematical assumptions such as stable structure. All the new dependence relationships can be consistently estimated from high frequency data inside the relevant time periods. Efficiency gains are at the very least close to 50%, and thus existing efficiency bounds may become irrelevant. It is expected that this approach will form a new paradigm for high frequency data.<br /><br />2) the PIs have developed a more systematic treatment of the uncertainty measure, called "observed AVAR". This uncertainty measure is not restricted by any specific econometric model, and can be implemented daily or intra-day. The new measure is off the shelf, and can be applied to various economic and financial quantities.<br /><br />3) One application of this project produces an intra-day real-time risk measure of the market volatility. The recent ten years have witnessed many episodes of "risk on" and "risk off". It is during the period of highly volatile market that old-time or in-house measures all break down and investors lose oversight on how to evaluate risk. Based on the real-time equity market behavior our approach gives a quantitative evaluation on market greed and fear, which complements the Market Volatility Index created by the Chicago Board Options Exchange. The latter is about market expectations for future volatility.<br /><br />In another part of the project, the PIs analyzed behavior of pre-averaged data. Data averaging is a practice that combines the data that arrives in a nearby window, for example, within the same minute. While dealing with intra-day data, this is often the first step that many researchers and practitioners do, since it reduces the noise and total data size. The PIs documented the situations when this data reduction could cause problems, in terms of under-reporting the risk in market data and obscuring the information in the original trade times. The paper "Between data cleaning and inference: Pre-averaging and robust estimators of the efficient price" shows how to conduct data reduction in an appropriate way.<br /><br />In a third part of the project, the PIs reported how to assess market quantity when data arrives in an irregular fashion. The randomness in how information arrives intra-day is often overlooked in the literature. In "The Algebra of Two Scales Estimation - High Frequency Estimation that is Robust to Sampling Times", the PIs found that ignoring the irregular arrival times of the information leads to further understatement of the market risk. The PIs proceeds to device a bias-corrected measure of market risk. This new measure is particularly useful when information arrives in a time that is endogenous to the system, as in the financial market.<br /><br />Broader Impacts: The data are mostly financial, and interest focuses on quantities like realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with more precise standard errors, will be useful in all these areas of finance. In particular, as we have seen in the recent financial crisis, markets can change abruptly. Better measurements of market behavior will make early detection possible when market conditions change, greatly aiding the management of risk. The results will be of interest to investors, regulators and policymakers. The dependence structure also has use in other areas of research that use high frequency data, including neural science, and turbulence.</p><br> <p>            Last Modified: 12/08/2018<br>      Modified by: Per&nbsp;Mykland</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Background: Recent years have seen an explosion in the availability of high frequency financial data. The data is usually called tick-by-tick data: every quote and every transaction is recorded. The data arrive at high frequency: for some stocks, there are more than a hundred thousand transactions per day. However, this vast amount of rapidly arriving data is not free of noise and its information content is often masked. An important question which faces us, as a society, is how to turn this data into knowledge and uncover its informational architecture. This project has made progress in this direction.  Big data often lead to more uncertainty, which makes it hard to reveal the underlying structure. In the first part of this project ("Assessment of Uncertainty in High Frequency Data: The Observed Asymptotic Variance"), the PIs has made contributions on three fronts:  1) the PIs have discovered a new way to look at the dependence relationships governing the latent structures in the data. Each latent element is estimated from high frequency data series. Then the dependence structure between latent elements in adjacent time periods is established. The new structure will permit the "borrowing" of information from adjacent time periods, and also from other data series if one has a panel of data. The methodology does not depend on either time or panel size being large, neither does it depend on mathematical assumptions such as stable structure. All the new dependence relationships can be consistently estimated from high frequency data inside the relevant time periods. Efficiency gains are at the very least close to 50%, and thus existing efficiency bounds may become irrelevant. It is expected that this approach will form a new paradigm for high frequency data.  2) the PIs have developed a more systematic treatment of the uncertainty measure, called "observed AVAR". This uncertainty measure is not restricted by any specific econometric model, and can be implemented daily or intra-day. The new measure is off the shelf, and can be applied to various economic and financial quantities.  3) One application of this project produces an intra-day real-time risk measure of the market volatility. The recent ten years have witnessed many episodes of "risk on" and "risk off". It is during the period of highly volatile market that old-time or in-house measures all break down and investors lose oversight on how to evaluate risk. Based on the real-time equity market behavior our approach gives a quantitative evaluation on market greed and fear, which complements the Market Volatility Index created by the Chicago Board Options Exchange. The latter is about market expectations for future volatility.  In another part of the project, the PIs analyzed behavior of pre-averaged data. Data averaging is a practice that combines the data that arrives in a nearby window, for example, within the same minute. While dealing with intra-day data, this is often the first step that many researchers and practitioners do, since it reduces the noise and total data size. The PIs documented the situations when this data reduction could cause problems, in terms of under-reporting the risk in market data and obscuring the information in the original trade times. The paper "Between data cleaning and inference: Pre-averaging and robust estimators of the efficient price" shows how to conduct data reduction in an appropriate way.  In a third part of the project, the PIs reported how to assess market quantity when data arrives in an irregular fashion. The randomness in how information arrives intra-day is often overlooked in the literature. In "The Algebra of Two Scales Estimation - High Frequency Estimation that is Robust to Sampling Times", the PIs found that ignoring the irregular arrival times of the information leads to further understatement of the market risk. The PIs proceeds to device a bias-corrected measure of market risk. This new measure is particularly useful when information arrives in a time that is endogenous to the system, as in the financial market.  Broader Impacts: The data are mostly financial, and interest focuses on quantities like realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with more precise standard errors, will be useful in all these areas of finance. In particular, as we have seen in the recent financial crisis, markets can change abruptly. Better measurements of market behavior will make early detection possible when market conditions change, greatly aiding the management of risk. The results will be of interest to investors, regulators and policymakers. The dependence structure also has use in other areas of research that use high frequency data, including neural science, and turbulence.       Last Modified: 12/08/2018       Submitted by: Per Mykland]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
