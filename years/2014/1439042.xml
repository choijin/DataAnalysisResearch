<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: SDA: Collaborative Research: RUI:  SCORE: Scalability-Oriented Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>252000.00</AwardTotalIntnAmount>
<AwardAmount>252000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Title: XPS: FULL: SDA: Collaborative Research: SCORE: Scalability-Oriented Optimization&lt;br/&gt;&lt;br/&gt;Modern CPUs, which contain an increasingly large number of processing units or "cores", offer the promise of continued increases in performance as the number of cores increases. Unfortunately, it is notoriously difficult for programmers to fully take advantage of this processing power. Computations can be viewed as cars on a network of highways: we want traffic to flow as fast as possible without any crashes. Programming languages offer "synchronization operations"---the programmer equivalent of traffic lights---which improve safety but reduce speed. For large programs, managing the tension between the twin goals of safety (more lights) and performance (fewer lights) can be out of reach for all but expert programmers. This project, SCORE (scalability-oriented optimization), lifts this burden by automatically maximizing program performance while maintaining correctness. The intellectual merits of this work are the development of a suite of techniques to identify bottlenecks in programs, and transform their code or execution environment to eliminate those bottlenecks. The project's broader significance and importance are to enable non-expert programmers to achieve high performance on modern, multicore platforms, and thus dramatically increase the performance and efficiency of existing and new software; contributing to the national software research infrastructure; and increasing access to science research opportunities and training for students.&lt;br/&gt;&lt;br/&gt;As with optimizing compilation for sequential code, SCORE lifts the burden of concurrency optimization from programmers, letting them focus exclusively on getting the logic of their program right. By handling architectural and synchronization optimizations without programmer involvement, SCORE lets programmers deliver applications that portably and effectively harness a wide range of multicore architectures. SCORE comprises a suite of new dynamic analyses, static analyses, and runtime systems to enable scalability-oriented optimization. It uncovers bottlenecks and ranks them by the performance impact of removing them. This information guides a bottleneck-remediation dynamic analysis to identify a range of opportunities for concurrency optimizations. Finally, a code robustification phase augments the optimized code with lightweight checking and recovery code to ensure correct execution.</AbstractNarration>
<MinAmdLetterDate>07/17/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/17/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439042</AwardID>
<Investigator>
<FirstName>Stephen</FirstName>
<LastName>Freund</LastName>
<PI_MID_INIT>N</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephen N Freund</PI_FULL_NAME>
<EmailAddress>freund@cs.williams.edu</EmailAddress>
<PI_PHON>4135974260</PI_PHON>
<NSF_ID>000110974</NSF_ID>
<StartDate>07/17/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Williams College</Name>
<CityName>Williamstown</CityName>
<ZipCode>012672600</ZipCode>
<PhoneNumber>4135974352</PhoneNumber>
<StreetAddress>880 Main St.</StreetAddress>
<StreetAddress2><![CDATA[Hopkins Hall]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>020665972</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT &amp; TRUSTEES OF WILLIAMS COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020665972</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Williams College]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>012672600</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~252000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-26ab75a1-7fff-3188-0618-e683e6021bd2"> </span></p> <p dir="ltr"><span>The nation's computing infrastructure plays a central role in all aspects of our society. Most computing systems now use multicore or multiprocessor architectures capable of performing multiple tasks at the same time. This ability to run concurrent threads of control has enabled computations at scales previously unimaginable.&nbsp;&nbsp;</span></p> <p dir="ltr"><span>Despite the prominence and importance of our software infrastructure infrastructure, however, constructing reliable and efficient concurrent software systems remains extremely challenging. For example, threads may interfere with each other in subtle ways if the programmer does not properly coordinate their behavior. Errors caused by such unanticipated interactions are a particularly pernicious type of defect. Synchronization mechanisms, such as mutual exclusion locks, can prevent such problems, but failing to follow a strict locking discipline can lead to deadlocks in which no threads are able to continue running. Debugging these errors is notoriously difficult, but getting their applications to run correctly is just one of the obstacles that programmers face.</span></p> <p>For many software engineers, the primary motivation for writing a concurrent application is to deliver increased performance. Multithreaded applications can, in theory, exploit multicore architectures to achieve performance benefits proportional to the number of cores. Unfortunately, for all but the most trivial "embarrassingly parallel'' applications, achieving the scaling that multicore CPUs promise often proves to be even more difficult than avoiding concurrency errors. In fact, the twin goals of correctness and performance are often in tension.</p> <p dir="ltr"><span>In more detail, synchronization operations like locks must be judiciously placed to protect shared data against race conditions and atomicity violations, but their use can lead to sequential bottlenecks when threads must wait for another thread to release a lock before continuing. Other obstacles to scalability exist as well. For example, false sharing occurs when two threads update logically distinct objects that happen to reside on the same cache line. This situation is both extremely hard to recognize and results in significant performance degradation.</span></p> <p>This grant explored techniques for automatically increasing the scalability of multithreaded applications without programmer intervention. Our approach involves a collection to tools and practices to automatically locate and mitigate scalability bottlenecks in multithreaded applications.</p> <p dir="ltr"><span>As part of this work, we developed causal profiling, a profiling technique that measures optimization potential via run-time performance experiments.&nbsp; These experiments predict the effect of optimizations, including scalability optimizations to program's synchronization code.  A key benefit of Coz is that it can predict the benefits of optimization efforts while requiring no modifications to the original program.&nbsp; We also developed a suite of synchronization primitives that modify their behavior at run time to provide the greatest scalability given the current workload of the system.  The modifications are similarly driven by online performance experiments.  Other work has targeted the design of Functional Pure Threads that are isolated from each other and have no side effects, thus enabling many performance optimizations without the risk of thread interference, and the application of our optimizations to the implementation of scalable, high-performance dynamic data race detector.</span></p> <p dir="ltr"><span>The intellectual merit and scientific contributions of this work include the development of tools facilitating the construction of more reliable and more scalable. It also pushes forward the state-of-the-art in profiling, program analysis, and optimization techniques for concurrent systems.&nbsp; This work has supported numerous undergraduate and graduate research students who received valuable training in performing scientific research. Some of our researchers are now pursuing advanced degrees.  One is now a faculty member continuing to perform research in this area.</span></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/21/2019<br>      Modified by: Stephen&nbsp;N&nbsp;Freund</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The nation's computing infrastructure plays a central role in all aspects of our society. Most computing systems now use multicore or multiprocessor architectures capable of performing multiple tasks at the same time. This ability to run concurrent threads of control has enabled computations at scales previously unimaginable.   Despite the prominence and importance of our software infrastructure infrastructure, however, constructing reliable and efficient concurrent software systems remains extremely challenging. For example, threads may interfere with each other in subtle ways if the programmer does not properly coordinate their behavior. Errors caused by such unanticipated interactions are a particularly pernicious type of defect. Synchronization mechanisms, such as mutual exclusion locks, can prevent such problems, but failing to follow a strict locking discipline can lead to deadlocks in which no threads are able to continue running. Debugging these errors is notoriously difficult, but getting their applications to run correctly is just one of the obstacles that programmers face.  For many software engineers, the primary motivation for writing a concurrent application is to deliver increased performance. Multithreaded applications can, in theory, exploit multicore architectures to achieve performance benefits proportional to the number of cores. Unfortunately, for all but the most trivial "embarrassingly parallel'' applications, achieving the scaling that multicore CPUs promise often proves to be even more difficult than avoiding concurrency errors. In fact, the twin goals of correctness and performance are often in tension. In more detail, synchronization operations like locks must be judiciously placed to protect shared data against race conditions and atomicity violations, but their use can lead to sequential bottlenecks when threads must wait for another thread to release a lock before continuing. Other obstacles to scalability exist as well. For example, false sharing occurs when two threads update logically distinct objects that happen to reside on the same cache line. This situation is both extremely hard to recognize and results in significant performance degradation.  This grant explored techniques for automatically increasing the scalability of multithreaded applications without programmer intervention. Our approach involves a collection to tools and practices to automatically locate and mitigate scalability bottlenecks in multithreaded applications. As part of this work, we developed causal profiling, a profiling technique that measures optimization potential via run-time performance experiments.  These experiments predict the effect of optimizations, including scalability optimizations to program's synchronization code.  A key benefit of Coz is that it can predict the benefits of optimization efforts while requiring no modifications to the original program.  We also developed a suite of synchronization primitives that modify their behavior at run time to provide the greatest scalability given the current workload of the system.  The modifications are similarly driven by online performance experiments.  Other work has targeted the design of Functional Pure Threads that are isolated from each other and have no side effects, thus enabling many performance optimizations without the risk of thread interference, and the application of our optimizations to the implementation of scalable, high-performance dynamic data race detector. The intellectual merit and scientific contributions of this work include the development of tools facilitating the construction of more reliable and more scalable. It also pushes forward the state-of-the-art in profiling, program analysis, and optimization techniques for concurrent systems.  This work has supported numerous undergraduate and graduate research students who received valuable training in performing scientific research. Some of our researchers are now pursuing advanced degrees.  One is now a faculty member continuing to perform research in this area.             Last Modified: 09/21/2019       Submitted by: Stephen N Freund]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
