<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Inferring the "Dark Matter" and "Dark Energy" from Image and Video</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>454400.00</AwardTotalIntnAmount>
<AwardAmount>454400</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops core techniques for improving the performance of key tasks in computer vision, such as recognizing objects, understanding scenes and events. Improving the performance of these tasks is able to generate broader impacts to the following applications: (1) video surveillance for security and timely intelligence; (2) intelligent robots for rescue in disaster areas; and (3) aerial scene and activity understanding from videos taken by unmanned aerial vehicles. In these applications, a significant portion of the contents in images, including i) entities such as objects, stuff like liquid, human actions, and scenes; and ii) relations, such as intents of humans, causal effects of actions, physical fields and attractions in a scene, cannot be recognized by the geometry and appearance features that are commonly used in current computer vision research. These entities and relations are referred as the "dark matter" and "dark energy," by analogy to cosmology models in physics, and plans to develop a unified representation that integrate the "visible" and the "dark" in a common model where the visible can be used to infer the dark, and the dark pose constraints for the inference of the visible in return. The research team is collaborating with industrial partner for technology transfer.&lt;br/&gt;&lt;br/&gt;More specifically, the project studies the following topics: i) Representing causal knowledge to go beyond associational knowledge in computer vision. Casual models are a large part of human knowledge and crucial for answering deeper questions on why, why not, what if (counterfactual). This research is the first formal study of causality (learning, modeling, and reasoning) in the vision literature.  ii) Reasoning the dark entities and relations to go beyond the current geometry and appearance-based paradigm. Perceptual causality, human intents and physics are generally applicable to all categories of object, scene, action and events, i.e., transportable across datasets. These entities and relations are deeper, and more invariant, than geometry and appearance - the dominating features used in visual recognition.   iii) Developing joint representation and joint inference algorithm. The rich contextual and causal links in this joint representation are essential for building robust vision systems where each visual entity can be inferred through multi-routes, but are not systematically studied and integrated in the existing paradigm.</AbstractNarration>
<MinAmdLetterDate>07/15/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/15/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423305</AwardID>
<Investigator>
<FirstName>Song-Chun</FirstName>
<LastName>Zhu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Song-Chun Zhu</PI_FULL_NAME>
<EmailAddress>sczhu@stat.ucla.edu</EmailAddress>
<PI_PHON>3102068693</PI_PHON>
<NSF_ID>000096074</NSF_ID>
<StartDate>07/15/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951554</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~454400</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This NSF project partially supported 5 Ph.D students and their dissertation over the 4 year project period, with 7 journal papers and more than a dozen peer-reviewed conference papers published in the areas of computer vision, cognitive science, robotics, and artificial intelligence. </span></p> <p>This research project has developed core techniques for improving the performance of key tasks in computer vision, such as object recognition, scene understanding, and action and event recognition which are used for i) Video surveillance for security and timely intelligence; ii) Intelligent robots for rescue in disaster areas; and iii)&nbsp;Aerial scene and activity understanding using unmanned aerial vehicles (UAV or Drones).</p> <p>The core techniques have been developed in five aspects.</p> <p>&nbsp;1) Reasoning the "dark" entities and relations to go beyond the current geometry and appearance-based paradigms. Here "dark" entities and relations refer to representations that are not directly visible in images, and have to be reasoned and imagined through human cognition in a top-down process, such as perceptual causality, human intents and physics. These entities and relations are deeper, and more invariant, than geometry and appearance --- the dominating features used in visual recognition.</p> <p>&nbsp; 2) Representing causal knowledge to go beyond associational knowledge in vision. Graphical models have been widely used in computer vision as the backbones for representing objects, scenes, actions and events. These models are associational or contextual in space and time, but not causal, and they are good at answering&nbsp;<em>what</em>,&nbsp;<em>who,</em>&nbsp;<em>where</em>&nbsp;and&nbsp;<em>when</em>. Casual models are a large part of human knowledge and allow us to answer deeper questions on&nbsp;<em>why</em>,&nbsp;<em>why not</em>,&nbsp;<em>what if</em>&nbsp;(counterfactual). The project studied, for the first time, causality in the video activities, i.e. learning, modeling, and reasoning about how human actions affect the fluents (states) of objects in daily scenes.</p> <p>&nbsp; 3) Modeling hidden human attention, joint attention and intention, and predicting human action in the context of executing tasks.&nbsp; In current computer vision research, visual attention is mainly studied in the process of object recognition when a human subject is looking at an image on screen. We studied algorithms for inferring hidden human intents, and predicting human actions in a 3D scene (from video) in the middle of executing daily tasks, such as getting water, mopping floor, etc. &nbsp; &nbsp;&nbsp;</p> <p>&nbsp;4) &nbsp;Modeling the interactions of humans by fields and forces. By analogy to physical models, we treat humans as attributed particles (this assumption is valid for aerial videos captured by drones), and thus their interactions are governed by invisible potential functions. &nbsp; We first learned these postential functions for different types of interactions: chasing, gathering etc. in a non-parametric form from video observations. then we derive the fields and forces from the potential function. &nbsp;Thus we obtained an explainable model for human interactions, in a way similar to physics. &nbsp;For example, we can derive the motion equations and trajectories of people in a scene driven by these potential functions.</p> <p>5) Modeling and reasoning hidden objects and containment relations. In scene understanding and video analysis, many objects are severely occluded or contained by other objects and thus become completely invisible (or dark). For example, when a person packs a lunch box, he/she put various food items&nbsp;and utensil &nbsp;in the box and then put the box into a backpack. Detecting such hidden objects and their containment relations have been discussed in AI and robotics, but have never been studied in realistic scenes. We developed a joint probabilistic representation for explicitly modeling the containment relations between objects in 3D scenes and human actions (taking out, putting in). Given an input RGB-D video, our algorithm quantizes the perceptual space of a 3D scene by reasoning about containment relations over time. At each frame, we represent the containment relations in space by a containment graph, where each vertex represents an object and each edge represents a containment relation. We assume that human actions are the only cause that leads to containment relation changes over time, we collected a new dataset with 1,326 video clips taken in 9 indoor scenes, including some challenging cases, such as heavy occlusions and diverse changes of containment relations. The experimental results demonstrate satisfactory results that outperforms comparable baselines.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/12/2018<br>      Modified by: Song-Chun&nbsp;Zhu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This NSF project partially supported 5 Ph.D students and their dissertation over the 4 year project period, with 7 journal papers and more than a dozen peer-reviewed conference papers published in the areas of computer vision, cognitive science, robotics, and artificial intelligence.   This research project has developed core techniques for improving the performance of key tasks in computer vision, such as object recognition, scene understanding, and action and event recognition which are used for i) Video surveillance for security and timely intelligence; ii) Intelligent robots for rescue in disaster areas; and iii) Aerial scene and activity understanding using unmanned aerial vehicles (UAV or Drones).  The core techniques have been developed in five aspects.   1) Reasoning the "dark" entities and relations to go beyond the current geometry and appearance-based paradigms. Here "dark" entities and relations refer to representations that are not directly visible in images, and have to be reasoned and imagined through human cognition in a top-down process, such as perceptual causality, human intents and physics. These entities and relations are deeper, and more invariant, than geometry and appearance --- the dominating features used in visual recognition.    2) Representing causal knowledge to go beyond associational knowledge in vision. Graphical models have been widely used in computer vision as the backbones for representing objects, scenes, actions and events. These models are associational or contextual in space and time, but not causal, and they are good at answering what, who, where and when. Casual models are a large part of human knowledge and allow us to answer deeper questions on why, why not, what if (counterfactual). The project studied, for the first time, causality in the video activities, i.e. learning, modeling, and reasoning about how human actions affect the fluents (states) of objects in daily scenes.    3) Modeling hidden human attention, joint attention and intention, and predicting human action in the context of executing tasks.  In current computer vision research, visual attention is mainly studied in the process of object recognition when a human subject is looking at an image on screen. We studied algorithms for inferring hidden human intents, and predicting human actions in a 3D scene (from video) in the middle of executing daily tasks, such as getting water, mopping floor, etc.        4)  Modeling the interactions of humans by fields and forces. By analogy to physical models, we treat humans as attributed particles (this assumption is valid for aerial videos captured by drones), and thus their interactions are governed by invisible potential functions.   We first learned these postential functions for different types of interactions: chasing, gathering etc. in a non-parametric form from video observations. then we derive the fields and forces from the potential function.  Thus we obtained an explainable model for human interactions, in a way similar to physics.  For example, we can derive the motion equations and trajectories of people in a scene driven by these potential functions.  5) Modeling and reasoning hidden objects and containment relations. In scene understanding and video analysis, many objects are severely occluded or contained by other objects and thus become completely invisible (or dark). For example, when a person packs a lunch box, he/she put various food items and utensil  in the box and then put the box into a backpack. Detecting such hidden objects and their containment relations have been discussed in AI and robotics, but have never been studied in realistic scenes. We developed a joint probabilistic representation for explicitly modeling the containment relations between objects in 3D scenes and human actions (taking out, putting in). Given an input RGB-D video, our algorithm quantizes the perceptual space of a 3D scene by reasoning about containment relations over time. At each frame, we represent the containment relations in space by a containment graph, where each vertex represents an object and each edge represents a containment relation. We assume that human actions are the only cause that leads to containment relation changes over time, we collected a new dataset with 1,326 video clips taken in 9 indoor scenes, including some challenging cases, such as heavy occlusions and diverse changes of containment relations. The experimental results demonstrate satisfactory results that outperforms comparable baselines.             Last Modified: 11/12/2018       Submitted by: Song-Chun Zhu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
