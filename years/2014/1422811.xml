<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Computational Support for Time Domain Continuous Imaging</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>358058.00</AwardTotalIntnAmount>
<AwardAmount>358058</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Conventional digital still and video cameras mimic film cameras: the sensor integrates light during an exposure interval and then the latent image is processed, creating a snapshot or sequence of video frames.  In contrast, time domain continuous imaging (TDCI) is a transformative new computational approach to imaging based on recording, for each pixel, a continuous waveform describing how the light level changes over time.  The time interval to be represented by a still image or video frame can be specified after capture and the image rendered by computationally integrating the portion of the recorded pixel waveforms corresponding to that period.  Further, the exposure parameters are no longer directly constrained by sensor ISO sensitivity:  the waveforms provide low noise and high dynamic range (HDR) independent of the range of brightness in the scene or apparent shutter speed used. In effect, a TDCI stream is a "raw" imaging representation that allows post-processing of temporal properties in addition to the usual image characteristics.  By specifying, experimentally evaluating, and disseminating the basic computational support needed for TDCI, this project lays the foundation for future development of sensor systems and new applications employing this model. Success could spawn an entirely new generation of technology that would revolutionize the fields of digital photography, video recording, and remote sensing.&lt;br/&gt;&lt;br/&gt;TDCI sensor systems have the potential to redefine the concept of a camera, but are extremely compute-intensive.  Substantial computation must be done in the camera to meet tight real-time constraints for control of the sensor, capture, and compressed encoding of a waveform per pixel to create a TDCI stream.  Manipulation of TDCI streams also requires new computational methods for other tasks ranging from efficient synthesis of conventional images from TDCI streams to implementation of algorithms directly transforming TDCI streams.  For example, the accuracy of each waveform within a TDCI stream can be improved using analysis of waveforms for nearby pixels.&lt;br/&gt;The proposed work centers on exploring and experimentally evaluating all aspects of computation needed to support TDCI, both in-camera and post-processing.  Recent advances in microchip technology suggest that these computations can be done economically in real time.  While full exploitation of TDCI may ultimately make use of custom sensors integrating massively-parallel nanocontroller arrays, the exploratory research in this project avoids dependence on such new sensors by using TDCI streams synthesized and processed using conventional sensors and computer hardware.  The goal of this project is to better understand the computational challenges and issues associated with TDCI, while producing reference implementations of the basic computing support and "project kits" as artifacts facilitating dissemination and further investigation.</AbstractNarration>
<MinAmdLetterDate>08/04/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422811</AwardID>
<Investigator>
<FirstName>Henry</FirstName>
<LastName>Dietz</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Henry G Dietz</PI_FULL_NAME>
<EmailAddress>hankd@engr.uky.edu</EmailAddress>
<PI_PHON>8592574701</PI_PHON>
<NSF_ID>000182891</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Kentucky Research Foundation</Name>
<CityName>Lexington</CityName>
<ZipCode>405260001</ZipCode>
<PhoneNumber>8592579420</PhoneNumber>
<StreetAddress>109 Kinkead Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<StateCode>KY</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>KY06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>939017877</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF KENTUCKY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007400724</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Kentucky Research Foundation]]></Name>
<CityName>Lexington</CityName>
<StateCode>KY</StateCode>
<ZipCode>405260001</ZipCode>
<StreetAddress><![CDATA[500 S Limestone 109 Kinkead Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>KY06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~358058</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Most image sensors, and digital cameras, mimic how film was used: the sensor integrates light during an exposure interval and then the "latent" image is read-out. This concept of capturing and processing an image, or movie frame, at a time is shared by nearly all imaging devices. This proposal suggested that a new and superior approach is now becoming viable: time domain continuous imaging (TDCI) in which image data is represented as a continuous waveform for each pixel describing how the light level varies over time. From TDCI streams, virtual exposures can be computationally extracted to represent any time interval, allowing shutter speed and exposure start time, or video framrate and shutter angle, to be freely adjusted after capture. These virtual exposures also benefit from lower noise and increased dynamic range as compared to conventional exposures representing the same time intervals.</p> <p>Although the project did not build any new sensors capable of directly delivering TDCI streams, it proved the approach is viable using conventional sensors and cameras. An early prototype system, FourSee, was built using reprogrammed Canon PowerShot N cameras to capture deliberately temporally-skewed exposures from which both higher dynamic range and finer temporal accuracy could be obtained. Later work proved that good quality TDCI streams could be synthesized from ordinary video or sequences of still images, and it was even possible to reprogram a single sub-$100 Canon PowerShot camera to internally record a TDCI stream.</p> <p>For storing TDCI streams, a file format was created called TIK: Temporal Image format from Kentucky. A software tool, also called TIK, was created to automatically generate a pixel value error model, to encode a TDCI stream from a video or still sequence according to a given pixel value error model, and to computationally render virtual exposures for any desired time intervals. Additional work was done in multispectral TDCI, temporal super-resolution processing, etc.</p> <p>Intellectual Merit:&nbsp;Imaging sensors are a very active area for research and development, but the concept of "frameless capture" enabled by smart processing of pixel data compellingly changes the most fundamental concepts underlying both still and video cameras.</p> <p>Broader Impacts:&nbsp;This work went far towards redefining the concept of a camera, allowing inexpensive sensors to capture complete environments with qualitatively finer spatial, temporal, Ev dynamic range, and accuracy. This allows complex environments to be surveilled, accurately modeled for preservation or virtual reality, etc.</p> <p>Public domain implementations of hardware (e.g., FourSee) and software (e.g., TIK: Temporal Imaging from Kentucky) related to this systems research were created and released, and also were used to facilitate replication of our results and to enhance outreach activites. Further, research results were broadly disseminated through both peer-reviewed publications and more accessible articles, as well as presentation in conference research exhibits, and distribution through the Aggregate.Org research website. Within the University of Kentucky, we involved students, including students from underrepresented groups, through various research experiences and integration of the new technology in specific courses.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/15/2018<br>      Modified by: Henry&nbsp;G&nbsp;Dietz</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542259376638_ei2017CHDKposter--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542259376638_ei2017CHDKposter--rgov-800width.jpg" title="Virtual Exposures From A SIngle TDCI Capture"><img src="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542259376638_ei2017CHDKposter--rgov-66x44.jpg" alt="Virtual Exposures From A SIngle TDCI Capture"></a> <div class="imageCaptionContainer"> <div class="imageCaption">These six images were all computationally rendered from the same single TDCI stream, which was captured within a reprogrammed Canon PowerShot camera.</div> <div class="imageCredit">H. Dietz</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Henry&nbsp;G&nbsp;Dietz</div> <div class="imageTitle">Virtual Exposures From A SIngle TDCI Capture</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542258376299_tictdci1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542258376299_tictdci1--rgov-800width.jpg" title="Conventional Image Capture"><img src="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542258376299_tictdci1--rgov-66x44.jpg" alt="Conventional Image Capture"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Conventional imaging records the average value of each pixel during a fixed time interval which is determined at capture.</div> <div class="imageCredit">H. Dietz</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Henry&nbsp;G&nbsp;Dietz</div> <div class="imageTitle">Conventional Image Capture</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542258590393_tictdci2--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542258590393_tictdci2--rgov-800width.jpg" title="TDCI Capture And Processing"><img src="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542258590393_tictdci2--rgov-66x44.jpg" alt="TDCI Capture And Processing"></a> <div class="imageCaptionContainer"> <div class="imageCaption">In TDCI, a continuous waveform is recorded for how the light level changes over time at each pixel. Images can be rendered to represent any desired time interval by integrating the area under the curve during that interval.</div> <div class="imageCredit">H. Dietz</div> <div class="imageSubmitted">Henry&nbsp;G&nbsp;Dietz</div> <div class="imageTitle">TDCI Capture And Processing</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542256206382_DSC09423sm--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542256206382_DSC09423sm--rgov-800width.jpg" title="The FourSee TDCI Prototype Camera"><img src="/por/images/Reports/POR/2018/1422811/1422811_10327991_1542256206382_DSC09423sm--rgov-66x44.jpg" alt="The FourSee TDCI Prototype Camera"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Using four  reprogrammed Canon PowerShot N to capture temporally-skewed images, FourSee image data could be fused into TDCI streams with high dynamic range and 1/480s temporal accuracy.</div> <div class="imageCredit">H. Dietz</div> <div class="imageSubmitted">Henry&nbsp;G&nbsp;Dietz</div> <div class="imageTitle">The FourSee TDCI Prototype Camera</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Most image sensors, and digital cameras, mimic how film was used: the sensor integrates light during an exposure interval and then the "latent" image is read-out. This concept of capturing and processing an image, or movie frame, at a time is shared by nearly all imaging devices. This proposal suggested that a new and superior approach is now becoming viable: time domain continuous imaging (TDCI) in which image data is represented as a continuous waveform for each pixel describing how the light level varies over time. From TDCI streams, virtual exposures can be computationally extracted to represent any time interval, allowing shutter speed and exposure start time, or video framrate and shutter angle, to be freely adjusted after capture. These virtual exposures also benefit from lower noise and increased dynamic range as compared to conventional exposures representing the same time intervals.  Although the project did not build any new sensors capable of directly delivering TDCI streams, it proved the approach is viable using conventional sensors and cameras. An early prototype system, FourSee, was built using reprogrammed Canon PowerShot N cameras to capture deliberately temporally-skewed exposures from which both higher dynamic range and finer temporal accuracy could be obtained. Later work proved that good quality TDCI streams could be synthesized from ordinary video or sequences of still images, and it was even possible to reprogram a single sub-$100 Canon PowerShot camera to internally record a TDCI stream.  For storing TDCI streams, a file format was created called TIK: Temporal Image format from Kentucky. A software tool, also called TIK, was created to automatically generate a pixel value error model, to encode a TDCI stream from a video or still sequence according to a given pixel value error model, and to computationally render virtual exposures for any desired time intervals. Additional work was done in multispectral TDCI, temporal super-resolution processing, etc.  Intellectual Merit: Imaging sensors are a very active area for research and development, but the concept of "frameless capture" enabled by smart processing of pixel data compellingly changes the most fundamental concepts underlying both still and video cameras.  Broader Impacts: This work went far towards redefining the concept of a camera, allowing inexpensive sensors to capture complete environments with qualitatively finer spatial, temporal, Ev dynamic range, and accuracy. This allows complex environments to be surveilled, accurately modeled for preservation or virtual reality, etc.  Public domain implementations of hardware (e.g., FourSee) and software (e.g., TIK: Temporal Imaging from Kentucky) related to this systems research were created and released, and also were used to facilitate replication of our results and to enhance outreach activites. Further, research results were broadly disseminated through both peer-reviewed publications and more accessible articles, as well as presentation in conference research exhibits, and distribution through the Aggregate.Org research website. Within the University of Kentucky, we involved students, including students from underrepresented groups, through various research experiences and integration of the new technology in specific courses.          Last Modified: 11/15/2018       Submitted by: Henry G Dietz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
