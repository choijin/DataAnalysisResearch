<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>166428.00</AwardTotalIntnAmount>
<AwardAmount>166428</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of "nuclei," which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.</AbstractNarration>
<MinAmdLetterDate>08/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1450177</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Ancell</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian C Ancell</PI_FULL_NAME>
<EmailAddress>brian.ancell@ttu.edu</EmailAddress>
<PI_PHON>8067423884</PI_PHON>
<NSF_ID>000591855</NSF_ID>
<StartDate>08/04/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas Tech University</Name>
<CityName>Lubbock</CityName>
<ZipCode>794091035</ZipCode>
<PhoneNumber>8067423884</PhoneNumber>
<StreetAddress>349 Administration Bldg</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX19</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041367053</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS TECH UNIVERSITY SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041367053</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas Tech University]]></Name>
<CityName/>
<StateCode>TX</StateCode>
<ZipCode>794091035</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX19</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1525</Code>
<Text>Physical &amp; Dynamic Meteorology</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>4444</Code>
<Text>INTERDISCIPLINARY PROPOSALS</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8009</Code>
<Text>Scientifc Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~166428</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>The Big Weather Web (<span>BWW</span>) project was proposed to address the challenge of exponential growth of data volumes in atmospheric modeling.&nbsp; This is especially a problem in ensemble forecasting.&nbsp; Ensemble forecasting is a broadly accepted method to mitigate the issue of known errors in the forecast models caused by imperfect observations and approximations of atmospheric processes combined with the chaotic nature of weather. &nbsp;Chaos, where small changes in initial conditions amplify and result in large changes in forecasts, has profound implications for numerical weather prediction as well as reproducibility of computer model simulations of the atmosphere.&nbsp; Ensemble forecasting utilizes information from many, many simulations, which requires large amounts of compute power and data storage.&nbsp; In fact, the data volumes in ensemble forecasting have already grown to a size that many small to mid-size research groups and especially primarily undergraduate institutions (<span>PUIs</span>) do not have the resources to manage the large amounts of data locally and share their data products globally with high availability. This lack of resources has a number of consequences in education and research, including: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations.&nbsp; Additionally, scientific journals and funding agencies are both prioritizing scientific reproducibility, more easily achieved in the age of big data with intelligent use of data science tools such as software containers and cloud computing.</span></p> <p>&nbsp;</p> <p><span>The <span>BWW</span> project teamed up 8 universities of varying sizes and two atmospheric science-serving institutions.&nbsp; The <span>BWW</span> team successfully piloted a method to run a large ensemble across multiple universities and share that data efficiently by storing all ensemble members in shared cloud storage.&nbsp; Additionally, much of the analysis of the large ensemble was done using data-proximate cloud computing. &nbsp;The findings of this project have been disseminated widely via publications, conference presentations, and tutorials, and have provided the atmospheric sciences community with an initial <span>roadmap</span> for scientific use of cloud services, training the next generation of physical scientists in data science and reproducibility, and sharing of very large model datasets across many research groups.&nbsp; Some of the highlights of the <span>BWW</span> findings are as follows:</span></p> <ul> <li><span><span>workflow</span> creation using shared storage and workspace on Amazon Web Services (<span>AWS</span>) and NSF <span>Jetstream</span>/Wrangler</span></li> <li><span>creation of a new reproducible <span>workflow</span> approach, Popper, grown from software development ideas reformulated for applied scientists</span></li> <li><span>creation of <span>WRF</span> In a Box for reproducible and easily deployable model simulations (<span>WRF</span> = ?Weather Research and Forecasting? Model)</span></li> <li><span>implementation of new shareable module for undergraduate classrooms that teach about ensemble modeling and data science using <span>AWS</span> and <span>WRF</span> In a Box</span></li> <li><span>many ensemble modeling and synoptic forecasting scientific studies utilizing the <span>BWW</span> Ensemble; at Penn State in particular, the <span>BWW</span> ensemble contributed to a study of ensemble design for and the predictability of East Coast winter storms</span><strong>&nbsp;</strong><em>&nbsp;</em></li> </ul> <p>&nbsp;</p><br> <p>            Last Modified: 08/16/2019<br>      Modified by: Brian&nbsp;C&nbsp;Ancell</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Big Weather Web (BWW) project was proposed to address the challenge of exponential growth of data volumes in atmospheric modeling.  This is especially a problem in ensemble forecasting.  Ensemble forecasting is a broadly accepted method to mitigate the issue of known errors in the forecast models caused by imperfect observations and approximations of atmospheric processes combined with the chaotic nature of weather.  Chaos, where small changes in initial conditions amplify and result in large changes in forecasts, has profound implications for numerical weather prediction as well as reproducibility of computer model simulations of the atmosphere.  Ensemble forecasting utilizes information from many, many simulations, which requires large amounts of compute power and data storage.  In fact, the data volumes in ensemble forecasting have already grown to a size that many small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage the large amounts of data locally and share their data products globally with high availability. This lack of resources has a number of consequences in education and research, including: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations.  Additionally, scientific journals and funding agencies are both prioritizing scientific reproducibility, more easily achieved in the age of big data with intelligent use of data science tools such as software containers and cloud computing.     The BWW project teamed up 8 universities of varying sizes and two atmospheric science-serving institutions.  The BWW team successfully piloted a method to run a large ensemble across multiple universities and share that data efficiently by storing all ensemble members in shared cloud storage.  Additionally, much of the analysis of the large ensemble was done using data-proximate cloud computing.  The findings of this project have been disseminated widely via publications, conference presentations, and tutorials, and have provided the atmospheric sciences community with an initial roadmap for scientific use of cloud services, training the next generation of physical scientists in data science and reproducibility, and sharing of very large model datasets across many research groups.  Some of the highlights of the BWW findings are as follows:  workflow creation using shared storage and workspace on Amazon Web Services (AWS) and NSF Jetstream/Wrangler creation of a new reproducible workflow approach, Popper, grown from software development ideas reformulated for applied scientists creation of WRF In a Box for reproducible and easily deployable model simulations (WRF = ?Weather Research and Forecasting? Model) implementation of new shareable module for undergraduate classrooms that teach about ensemble modeling and data science using AWS and WRF In a Box many ensemble modeling and synoptic forecasting scientific studies utilizing the BWW Ensemble; at Penn State in particular, the BWW ensemble contributed to a study of ensemble design for and the predictability of East Coast winter storms             Last Modified: 08/16/2019       Submitted by: Brian C Ancell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
