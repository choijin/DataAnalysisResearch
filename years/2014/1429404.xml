<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Development of Augmentarium: High Performance Visual Computing Infrastructure with Adaptive Displays</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>600000.00</AwardTotalIntnAmount>
<AwardAmount>699999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project, developing Augmentarium, enables visual understanding of a number of applications in science, engineering, and medicine. Once completed, this instrument is expected to be a stable shared-used cyberfacility that uses the emerging technologies of programmable-robot-mounted projector-camera pairs for high-performance computing and scientific applications with extensive visualization needs. The developed instrument provides unprecedented capabilities to visualize locally high-resolution images with lower-resolution context, new interaction modalities, and projection-based augmented reality. The Augmentarium development will proceed with close involvement of domain experts in a number of areas including Astronomy, Atmospheric and Oceanic Sciences, Biology, Materials Science, Computational Fluid Dynamics, Aerospace Engineering, Electrical Engineering, and Computer Science. It is also anticipated that this will attract and involve a number of international collaborations. &lt;br/&gt;&lt;br/&gt;The projects that can directly benefit from the proposed instrument include:&lt;br/&gt;- Understanding Large-Scale Astronomy Data&lt;br/&gt;- Multiscale Instabilties and Mixing Processes in the Atmosphere and the Oceans&lt;br/&gt;- Large Scale Characterization of Stem Cell Colonies and Material Properties&lt;br/&gt;- Large Scale Simulations in Fluid Dynamics and Macromolecular Reconstruction via Fast Multipole Accelerated Algorithms&lt;br/&gt;- Real-Time Data Visualization for Cyber Security&lt;br/&gt;- Analysis and Visualization of Large Streaming Data&lt;br/&gt;&lt;br/&gt;The proposed instrumentation is expected to advance the state of the art in a broad range of scientific and engineering disciplines, such as astrophysics, modeling of cosmological processes, modeling and simulation of biological systems, climate change, computational engineering, materials science, and high performance computing. The proposed infrastructure will be used for education and research training by the different research groups affiliated with the main research areas. The facility will also be used to operate a summer program designed to interest students in computing, particularly those from groups under-represented in computing. This cyberfacility will also reach out to the Women in Engineering Society, the Association of Women in Computing, and the Society of Black Engineers at the University of Maryland to encourage participation in computational science projects that will make use of the new facility.</AbstractNarration>
<MinAmdLetterDate>08/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/15/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1429404</AwardID>
<Investigator>
<FirstName>Amitabh</FirstName>
<LastName>Varshney</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amitabh Varshney</PI_FULL_NAME>
<EmailAddress>varshney@cs.umd.edu</EmailAddress>
<PI_PHON>3014056761</PI_PHON>
<NSF_ID>000253793</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~600000</FUND_OBLG>
<FUND_OBLG>2016~99999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Overview:</strong>&nbsp;Our state-of-the-art UMD Augmentarium (featuring an application-driven adaptive visualization infrastructure, incorporating a unique assembly of projection displays and CPU-GPU clusters, along with human vision and human-computer interaction technologies) allowed us to study the visual augmentation of human intelligence, in support of an array of cross-disciplinary, cross-institutional research projects.</p> <p>While affordable consumer-quality virtual and augmented reality (VR and AR) hardware is becoming available, significant work is necessary to adopt VR and AR for important and difficult scientific and societal applications. Essential research in scene generation and capture, tracking and registration, displays, multimodal rendering, and interfaces and usability will produce necessary advances in VR and AR only through long-term investments in major instrumentation projects like this one. Foundational advances in precise mathematical modeling, high-precision rendering, low-latency tracking, effective user interfaces and other responses to current system limitations are needed before VR and AR can revolutionize a diverse set of real-world applications like manufacturing, health care, augmented navigation, and disaster recovery.</p> <p>The long-term strategic objectives of the Augmentarium have spanned research, education, broadening participation, and knowledge transfer towards ubiquitous VR and AR.&nbsp;</p> <p><strong>Intellectual Merit Outcomes:</strong>&nbsp;This project has resulted in significant early findings on human function in immersive environments.&nbsp;We have <em>interactively characterized &ldquo;cybersickness&rdquo; in virtual environments using EEG</em>. During immersion, up to 40% of the population experience symptoms like nausea, headache and dizziness. Establishing a strong correlation between cybersickness and EEG-measured brain activity is a first step toward mitigating these effects in virtual environments. We have <em>studied peripheral vision in VR and AR</em>, characterizing how it differs from central vision. This will allow for building environments that reduce visual clutter, allow for greater information transfer and enable visual attention management. We also <em>quantified that immersion aids in recall</em>. We found that user study participants performed 8.8% better at memory tasks executed in an immersive environment, as opposed to on a 2D desktop. This has exciting implications for the future of immersive training and educational environments.</p> <p>This project has fostered multiple developments on the medical front. We have <em>developed diagnostic tools for patient care and surgery</em>. An AR-ultrasound prototype allows doctors to overlay ultrasound diagnostic images directly onto a patient. An intubation training prototype is being field-tested to determine if resident are able to learn this procedure better. An AR high-precision tracking and visualization system for external ventricular drainage will allow neurosurgeons to see the exact path of the catheter they use to relieve cranial pressure in patients with traumatic brain injury. We have <em>developed a data visualization tool for detecting subtypes in Parkinson&rsquo;s Disease</em>. This tool allows users to interrogate complex patient data sets, providing the understanding that is so crucial for research into the underlying disease mechanisms. We have further <em>created immersive environments for non-opioid pain management studies</em>. Early indications are that these environments reduce patient experience of pain, thus have the potential to reduce demand for opioid therapy. This could be of vital importance to combatting the current nation-wide opioid crisis.</p> <p>Additional projects include virtual environments for language acquisition, weather forecasting in VR, and immersive environments for combatting implicit bias and increasing empathy.</p> <p><strong>Broader Impact Outcomes: </strong>We supported <em>Computer Science Connect (CompSciConnect), a series of summer day camps that train middle-school girls in coding in Unity for virtual environments</em>. These students see the many ways that VR and AR can be applied, gain hands-on experience with problem-solving using these technologies, and create their own applications with the help of undergraduate and graduate student mentors.</p> <p>In addition to attracting underrepresented groups to STEM fields, we have supported novel interdisciplinary programs. <em>The First-year Innovation and Research Experiences (FIRE) program brings together undergraduate computer science majors and art majors to explore emerging technologies for museums</em>. In this collaboration between the University of Maryland and the Phillips Collection in Washington, D.C., students explore VR to customize virtual exhibitions and galleries, and create experiences that can be saved and shared. This has the potential not only to improve museum-goers&rsquo; experiences, but to help diverse audiences more deeply understand and appreciate works of art.</p> <p>We have also directly engaged the public extensively via extensive lab tours and technology demonstrations. Our tours have included media outlets, industry and health care, government agencies and labs, law enforcement officers and first responders, leaders from local, state and national government, and students ranging from elementary school to college levels. We have also utilized tours and demos in response to enthusiastic interest from several news agencies, including Washington Post, Chronicle of Higher Education, Yahoo! Tech, Discovery News, Associated Press Television, Voice of America, Baltimore Business Journal, and others. Many who visited the Augmentarium had their first experience with VR and AR there. It has been rewarding to help open the door onto this new world for so many.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/02/2019<br>      Modified by: Amitabh&nbsp;Varshney</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1429404/1429404_10335460_1574375429046_Augmentariumimage--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1429404/1429404_10335460_1574375429046_Augmentariumimage--rgov-800width.jpg" title="The Augmentarium at the University of Maryland"><img src="/por/images/Reports/POR/2019/1429404/1429404_10335460_1574375429046_Augmentariumimage--rgov-66x44.jpg" alt="The Augmentarium at the University of Maryland"></a> <div class="imageCaptionContainer"> <div class="imageCaption">For the Augmentarium's 24'x8' curved screen, we merged 15 projectors into a single immersive stereoscopic display.</div> <div class="imageCredit">John Consoli</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Amitabh&nbsp;Varshney</div> <div class="imageTitle">The Augmentarium at the University of Maryland</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Overview: Our state-of-the-art UMD Augmentarium (featuring an application-driven adaptive visualization infrastructure, incorporating a unique assembly of projection displays and CPU-GPU clusters, along with human vision and human-computer interaction technologies) allowed us to study the visual augmentation of human intelligence, in support of an array of cross-disciplinary, cross-institutional research projects.  While affordable consumer-quality virtual and augmented reality (VR and AR) hardware is becoming available, significant work is necessary to adopt VR and AR for important and difficult scientific and societal applications. Essential research in scene generation and capture, tracking and registration, displays, multimodal rendering, and interfaces and usability will produce necessary advances in VR and AR only through long-term investments in major instrumentation projects like this one. Foundational advances in precise mathematical modeling, high-precision rendering, low-latency tracking, effective user interfaces and other responses to current system limitations are needed before VR and AR can revolutionize a diverse set of real-world applications like manufacturing, health care, augmented navigation, and disaster recovery.  The long-term strategic objectives of the Augmentarium have spanned research, education, broadening participation, and knowledge transfer towards ubiquitous VR and AR.   Intellectual Merit Outcomes: This project has resulted in significant early findings on human function in immersive environments. We have interactively characterized "cybersickness" in virtual environments using EEG. During immersion, up to 40% of the population experience symptoms like nausea, headache and dizziness. Establishing a strong correlation between cybersickness and EEG-measured brain activity is a first step toward mitigating these effects in virtual environments. We have studied peripheral vision in VR and AR, characterizing how it differs from central vision. This will allow for building environments that reduce visual clutter, allow for greater information transfer and enable visual attention management. We also quantified that immersion aids in recall. We found that user study participants performed 8.8% better at memory tasks executed in an immersive environment, as opposed to on a 2D desktop. This has exciting implications for the future of immersive training and educational environments.  This project has fostered multiple developments on the medical front. We have developed diagnostic tools for patient care and surgery. An AR-ultrasound prototype allows doctors to overlay ultrasound diagnostic images directly onto a patient. An intubation training prototype is being field-tested to determine if resident are able to learn this procedure better. An AR high-precision tracking and visualization system for external ventricular drainage will allow neurosurgeons to see the exact path of the catheter they use to relieve cranial pressure in patients with traumatic brain injury. We have developed a data visualization tool for detecting subtypes in Parkinson’s Disease. This tool allows users to interrogate complex patient data sets, providing the understanding that is so crucial for research into the underlying disease mechanisms. We have further created immersive environments for non-opioid pain management studies. Early indications are that these environments reduce patient experience of pain, thus have the potential to reduce demand for opioid therapy. This could be of vital importance to combatting the current nation-wide opioid crisis.  Additional projects include virtual environments for language acquisition, weather forecasting in VR, and immersive environments for combatting implicit bias and increasing empathy.  Broader Impact Outcomes: We supported Computer Science Connect (CompSciConnect), a series of summer day camps that train middle-school girls in coding in Unity for virtual environments. These students see the many ways that VR and AR can be applied, gain hands-on experience with problem-solving using these technologies, and create their own applications with the help of undergraduate and graduate student mentors.  In addition to attracting underrepresented groups to STEM fields, we have supported novel interdisciplinary programs. The First-year Innovation and Research Experiences (FIRE) program brings together undergraduate computer science majors and art majors to explore emerging technologies for museums. In this collaboration between the University of Maryland and the Phillips Collection in Washington, D.C., students explore VR to customize virtual exhibitions and galleries, and create experiences that can be saved and shared. This has the potential not only to improve museum-goers’ experiences, but to help diverse audiences more deeply understand and appreciate works of art.  We have also directly engaged the public extensively via extensive lab tours and technology demonstrations. Our tours have included media outlets, industry and health care, government agencies and labs, law enforcement officers and first responders, leaders from local, state and national government, and students ranging from elementary school to college levels. We have also utilized tours and demos in response to enthusiastic interest from several news agencies, including Washington Post, Chronicle of Higher Education, Yahoo! Tech, Discovery News, Associated Press Television, Voice of America, Baltimore Business Journal, and others. Many who visited the Augmentarium had their first experience with VR and AR there. It has been rewarding to help open the door onto this new world for so many.             Last Modified: 12/02/2019       Submitted by: Amitabh Varshney]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
