<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Understanding Real-World Auditory Scene Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2015</AwardEffectiveDate>
<AwardExpirationDate>10/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>474146.00</AwardTotalIntnAmount>
<AwardAmount>181477</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A fundamental question in auditory science concerns how people can recognize speech and other sounds in the presence of competing sound sources, as when conversing with a dinner partner at a crowded restaurant. The process of hearing a sound of interest when it is embedded in a mixture of other sounds is known as "sound segregation" and human listeners vastly outperform machine systems for segregating sounds. However, the process is frequently effortful,  is highly vulnerable to hearing impairment, including hearing impairment that typically accompanies  normal aging. Understanding the basis of sound segregation in human listeners, and the factors that limit human segregation abilities, would enhance efforts to develop assistive listening devices and machine systems for robust speech recognition and sound recognition. The project will be complemented by an educational effort to stimulate interest in audition in the general public and in middle- and high-school students through a series of publicly available online video presentations describing auditory research with associated sound demonstrations.&lt;br/&gt;&lt;br/&gt;This CAREER award is aimed at enriching the understanding of human auditory perception by exploring the basis of sound segregation with natural sounds. The experiments will leverage recent advances in speech analysis and synthesis methods to 1) manipulate grouping cues in natural speech and test their effect on sound segregation in human listeners; 2) manipulate voice and speech structure to probe their role in segregation; and 3) test the ability of human listeners to attend to and track target sound sources. The long-term goals are to inspire signal-processing algorithms that facilitate segregation by human listeners and replicate their competence in machine systems.</AbstractNarration>
<MinAmdLetterDate>03/25/2015</MinAmdLetterDate>
<MaxAmdLetterDate>04/26/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1454094</AwardID>
<Investigator>
<FirstName>Joshua</FirstName>
<LastName>McDermott</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joshua McDermott</PI_FULL_NAME>
<EmailAddress>jhm@MIT.EDU</EmailAddress>
<PI_PHON>6172537437</PI_PHON>
<NSF_ID>000632976</NSF_ID>
<StartDate>03/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394307</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~181477</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main goals of the project were to clarify 1) the sound properties that contribute to our ability to hear individual speech signals embedded in mixtures of talkers, and 2) the process by which human listeners attend to particular sound sources as they evolve over time.</p> <p>To address the first goal, we conducted psychophysical experiments utilizing resynthesized speech signals whose frequencies were manipulated to be inharmonic. This was accomplished by perturbing the frequencies up or down so that they were no longer multiples of a common fundamental frequency. We found that harmonic and inharmonic speech signals were equally intelligible when presented in quiet, but that inharmonic speech was more difficult to understand when presented concurrently with other speech signals. This finding suggests that harmonic frequency relations help human listeners to separate talkers in a speech mixture.</p> <p>To address the second goal, we developed a paradigm with which to study &ldquo;attentive tracking&rdquo; of sound. Listeners were presented with two intertwined voices and had to track one of them as the voices evolved over time. We found that listeners could perform this task provided the voices maintained separation in a voice feature space. We subsequently began a set of experiments to test whether listeners can learn regularities in the sound sources they hear, and whether these regularities are used to aid sound segregation. We found that when listeners perform a series of trials of our attentive tracking paradigm, trials on which the target source is drawn from a set of similar variants produce higher performance over time. The regularity of the set of sources appears to be rapidly learned over the course of a few trials.</p> <p>In the course of this project we also developed methodology to facilitate web-based experiments with sound. Experiments conducted online, in which listeners hear sounds via a web site and make judgments about them, have the advantage of drawing from large numbers of participants, which is useful for experiments consisting of a small number of trials. The drawback is that the experimenter has little control over sound presentation. We developed a method to help ensure that participants are wearing headphones or earphones, partially mitigating concerns over sound presentation quality.</p> <p>All aspects of the project have resulted in educational demonstrations that are available online, as well as software packages that are available for download.</p><br> <p>            Last Modified: 08/16/2017<br>      Modified by: Joshua&nbsp;Mcdermott</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main goals of the project were to clarify 1) the sound properties that contribute to our ability to hear individual speech signals embedded in mixtures of talkers, and 2) the process by which human listeners attend to particular sound sources as they evolve over time.  To address the first goal, we conducted psychophysical experiments utilizing resynthesized speech signals whose frequencies were manipulated to be inharmonic. This was accomplished by perturbing the frequencies up or down so that they were no longer multiples of a common fundamental frequency. We found that harmonic and inharmonic speech signals were equally intelligible when presented in quiet, but that inharmonic speech was more difficult to understand when presented concurrently with other speech signals. This finding suggests that harmonic frequency relations help human listeners to separate talkers in a speech mixture.  To address the second goal, we developed a paradigm with which to study "attentive tracking" of sound. Listeners were presented with two intertwined voices and had to track one of them as the voices evolved over time. We found that listeners could perform this task provided the voices maintained separation in a voice feature space. We subsequently began a set of experiments to test whether listeners can learn regularities in the sound sources they hear, and whether these regularities are used to aid sound segregation. We found that when listeners perform a series of trials of our attentive tracking paradigm, trials on which the target source is drawn from a set of similar variants produce higher performance over time. The regularity of the set of sources appears to be rapidly learned over the course of a few trials.  In the course of this project we also developed methodology to facilitate web-based experiments with sound. Experiments conducted online, in which listeners hear sounds via a web site and make judgments about them, have the advantage of drawing from large numbers of participants, which is useful for experiments consisting of a small number of trials. The drawback is that the experimenter has little control over sound presentation. We developed a method to help ensure that participants are wearing headphones or earphones, partially mitigating concerns over sound presentation quality.  All aspects of the project have resulted in educational demonstrations that are available online, as well as software packages that are available for download.       Last Modified: 08/16/2017       Submitted by: Joshua Mcdermott]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
