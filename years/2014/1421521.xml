<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Recovering Object 3D Shape and Material from Isolated Images</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>476569.00</AwardTotalIntnAmount>
<AwardAmount>476569</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project improves a computer's ability to interpret the shape and material of objects from visual sensors.  The research hypothesizes that full 3D object shape can be estimated by matching visual features from an observed object to an object of known shape from a dataset, transferring the known shape, and deforming the 3D shape to better account for spatial correspondences of matched features.  The research represents materials at multiple scales, separately encoding little bumps and grooves from the patterns of material categories.  Because image properties arise from the combination of shape, material, and illumination, the research also involves developing algorithms to jointly estimate. The developed technologies can be applied to automated systems, personal and industrial robotics, surveillance and security, transportation, image retrieval, image editing and manipulation, and content creation. The project contributes to education through student projects, course development, and workshops and tutorials involving a broader audience. &lt;br/&gt;&lt;br/&gt;The research investigates improved representations of 3D shape and material and methods to recover them from one image. Rather than aiming for veridical models, such as precise surface normals or BRDF parameters, the research team recovers approximate models that are useful for object recognition, content creation, and other tasks.  The work on 3D object shape focuses on labeling object boundaries as occlusions, folds, or texture/albedo and using these boundaries as part of a data-driven approach to recover full 3D models of the objects.  The research involves studying methods to recover rich, multiscale representations of the materials that compose objects.  These methods exploit approximate shape representations and approximate representations of the illumination to recover estimates of radiometric properties of the object at a point.  The algorithms build maps of these material properties to model spatial variation in albedo and complex phenomena like veins in marble.  The research also involves extending these methods to report spatially varying normal maps that capture shape textures like the bark of trees.  Finally, the research investigates how to incorporate image-centered maps to capture more random, spatially localized phenomena like the pits in orange peel.</AbstractNarration>
<MinAmdLetterDate>08/06/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1421521</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Forsyth</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David A Forsyth</PI_FULL_NAME>
<EmailAddress>daf@cs.uiuc.edu</EmailAddress>
<PI_PHON>2172656851</PI_PHON>
<NSF_ID>000391155</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Derek</FirstName>
<LastName>Hoiem</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Derek Hoiem</PI_FULL_NAME>
<EmailAddress>dhoiem@illinois.edu</EmailAddress>
<PI_PHON>4129526964</PI_PHON>
<NSF_ID>000516153</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~147637</FUND_OBLG>
<FUND_OBLG>2015~328932</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The major goals of this project were to produce methods that could: estimate 3D object shape from an image or a depth map and estimate material properties and layout from an image. &nbsp;Using these methods, the project intended to demonstrate improvements in prediction of robotic grasp points and media creation, resulting from better shape and materal interpretation.</p> <p>&nbsp;Work under this project showed how to recover a representation of a 3D objects as a collection of parts from a single depth image. &nbsp;For example, recover a model of a chair as a base, back, sides and legs from a depth image. &nbsp;This work will have broader impact, because it enables methods to recognize unfamiliar chairs (say) by reasoning about their parts. &nbsp;Further work showed how to complete a depth map of an object. &nbsp;For example, a depth sensor observes the front of an object, but not the back; this work showed how to estimate the back. &nbsp;This work will have broader impact, because it will make it possible to produce methods that can make object models from one depth map, for use in (say) the entertainment industry. &nbsp;Yet further work showed how to recover an estimate of free space in an image that could not be seen. &nbsp;For example, the method could tell that there was room for movement behind an object in an image. &nbsp;This work will have broader impact because it will allow mobile robots to plan to move more effectively, by knowing that there might be free space they cannot see.</p> <p><br />Work on materials produced novel methods to colorize grey level images automatically. &nbsp;Further work on materials is showing how to take an image and recover a representation of (a) how much light each surface reflects; (b) how rough the surface is; (c) how much light is falling on the surface and (d) the extent to which the surface is glossy or shiny. &nbsp;Yet further work on materials showed how to build representationsof garments that could tell whether they were compatible. &nbsp;In particular, the method demonstrated that respecting the type of the garment (hat, shoes, blouse, etc.) provided strong improvements inthe compatibility measure. &nbsp;This work will have broader impact by improving the accuracy of fashion search methods.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/08/2019<br>      Modified by: David&nbsp;A&nbsp;Forsyth</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The major goals of this project were to produce methods that could: estimate 3D object shape from an image or a depth map and estimate material properties and layout from an image.  Using these methods, the project intended to demonstrate improvements in prediction of robotic grasp points and media creation, resulting from better shape and materal interpretation.   Work under this project showed how to recover a representation of a 3D objects as a collection of parts from a single depth image.  For example, recover a model of a chair as a base, back, sides and legs from a depth image.  This work will have broader impact, because it enables methods to recognize unfamiliar chairs (say) by reasoning about their parts.  Further work showed how to complete a depth map of an object.  For example, a depth sensor observes the front of an object, but not the back; this work showed how to estimate the back.  This work will have broader impact, because it will make it possible to produce methods that can make object models from one depth map, for use in (say) the entertainment industry.  Yet further work showed how to recover an estimate of free space in an image that could not be seen.  For example, the method could tell that there was room for movement behind an object in an image.  This work will have broader impact because it will allow mobile robots to plan to move more effectively, by knowing that there might be free space they cannot see.   Work on materials produced novel methods to colorize grey level images automatically.  Further work on materials is showing how to take an image and recover a representation of (a) how much light each surface reflects; (b) how rough the surface is; (c) how much light is falling on the surface and (d) the extent to which the surface is glossy or shiny.  Yet further work on materials showed how to build representationsof garments that could tell whether they were compatible.  In particular, the method demonstrated that respecting the type of the garment (hat, shoes, blouse, etc.) provided strong improvements inthe compatibility measure.  This work will have broader impact by improving the accuracy of fashion search methods.                Last Modified: 11/08/2019       Submitted by: David A Forsyth]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
