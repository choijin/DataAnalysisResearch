<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI:  Small:  Understanding Value-based Multiagent Learning and Its Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>01/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>157019.00</AwardTotalIntnAmount>
<AwardAmount>157019</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hector Munoz-Avila</SignBlockName>
<PO_EMAI>hmunoz@nsf.gov</PO_EMAI>
<PO_PHON>7032924481</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project explores the behavior of value-based learning methods in multi-agent environments. Value-based methods make decisions by using experience to estimate the utility impact of alternatives and choosing those with high predicted value. Because they evaluate components of behavior instead of treating behaviors as atomic units, they are computationally and statistically efficient. While these methods have been used in computational experiments for many years, only recently have researchers begun to formally characterize their behavior. Our own preliminary work is finding that some value-based methods exhibit super-Nash behavior, making them particularly worthy of study.&lt;br/&gt;&lt;br/&gt;More specifically, we are analyzing, mathematically and experimentally, how value-based algorithms perform in several classes of simulated games of varying complexity from the artificial intelligence community, multi-agent engineering applications drawn from the wireless networking area, and as models of human and animal decision making in collaboration with cognitive neuroscientists. Where possible, we are refining existing value-based algorithms to work more efficiently, robustly, and generally than existing algorithms. We are also designing educational outreach activities, including creating entertaining instructional videos on how to promote cooperative behavior in real-life social dilemmas.</AbstractNarration>
<MinAmdLetterDate>02/19/2014</MinAmdLetterDate>
<MaxAmdLetterDate>02/19/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1414935</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Littman</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael L Littman</PI_FULL_NAME>
<EmailAddress>mlittman@cs.brown.edu</EmailAddress>
<PI_PHON>9085148763</PI_PHON>
<NSF_ID>000210482</NSF_ID>
<StartDate>02/19/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brown University</Name>
<CityName>Providence</CityName>
<ZipCode>029129002</ZipCode>
<PhoneNumber>4018632777</PhoneNumber>
<StreetAddress>BOX 1929</StreetAddress>
<StreetAddress2><![CDATA[350 Eddy Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<StateCode>RI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>RI01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001785542</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BROWN UNIVERSITY IN PROVIDENCE IN THE STATE OF RHODE ISLAND AND PROVIDENCE PLANTATIONS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001785542</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>Providence</CityName>
<StateCode>RI</StateCode>
<ZipCode>029129002</ZipCode>
<StreetAddress><![CDATA[BOX 1929]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>RI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~157019</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was motivated by understanding how independently behaving agents can come to optimize their individual behavior (relative to some concrete reward function or numeric objective). The work included multiple kinds of explorations that bear on this topic. It provided formal algorithms for learning and proofs of their behavior. It provided computational experimental work in which algorithms with different properties were given the chance to interact and learn. It included behavioral studies in which people were asked to make similar decisions to our algorithms providing a basis for comparing the similarities and differences between them.</p> <p>The outcome of the project in terms of intellectual merit was the first formal analysis proving the convergence of the popular Q-learning algorithm in simple 2-player 2-action games. It also led to an understanding of human behavior in complex games varying depending on the participants' perceptions of whether they should cooperate or compete with each other. Finally, it showed that the "cooperative-competitve" value, introduced in economics by Kalai and Kalai, could be applied to sequential games and maximized using an efficient learning procedure.</p> <p>The outcomes of the project in terms of broader impacts included roughly ten published papers and reports sharing our findings, educational experiences for graduate and undergraduate students, and collaborations with scientific colleagues in neuroscience, cognitive science, engineering, and economics.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/26/2016<br>      Modified by: Michael&nbsp;L&nbsp;Littman</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1414935/1414935_10019501_1464301173510_threebyfive--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1414935/1414935_10019501_1464301173510_threebyfive--rgov-800width.jpg" title="3 x 5 grid game"><img src="/por/images/Reports/POR/2016/1414935/1414935_10019501_1464301173510_threebyfive--rgov-66x44.jpg" alt="3 x 5 grid game"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A simple two-player grid game with surprising strategic depth</div> <div class="imageCredit">Littman</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Michael&nbsp;L&nbsp;Littman</div> <div class="imageTitle">3 x 5 grid game</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was motivated by understanding how independently behaving agents can come to optimize their individual behavior (relative to some concrete reward function or numeric objective). The work included multiple kinds of explorations that bear on this topic. It provided formal algorithms for learning and proofs of their behavior. It provided computational experimental work in which algorithms with different properties were given the chance to interact and learn. It included behavioral studies in which people were asked to make similar decisions to our algorithms providing a basis for comparing the similarities and differences between them.  The outcome of the project in terms of intellectual merit was the first formal analysis proving the convergence of the popular Q-learning algorithm in simple 2-player 2-action games. It also led to an understanding of human behavior in complex games varying depending on the participants' perceptions of whether they should cooperate or compete with each other. Finally, it showed that the "cooperative-competitve" value, introduced in economics by Kalai and Kalai, could be applied to sequential games and maximized using an efficient learning procedure.  The outcomes of the project in terms of broader impacts included roughly ten published papers and reports sharing our findings, educational experiences for graduate and undergraduate students, and collaborations with scientific colleagues in neuroscience, cognitive science, engineering, and economics.          Last Modified: 05/26/2016       Submitted by: Michael L Littman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
