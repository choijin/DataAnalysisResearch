<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>C1F21 DIBBS: Porting Practical Natural Language Processing (NLP) and Machine Learning (ML) Semantics from Biomedicine to the Earth, Ice and Life Sciences</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>11/01/2014</AwardEffectiveDate>
<AwardExpirationDate>10/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>1497785.00</AwardTotalIntnAmount>
<AwardAmount>1497785</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Amy Walton</SignBlockName>
<PO_EMAI>awalton@nsf.gov</PO_EMAI>
<PO_PHON>7032924538</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Semantics is the study of word-based information. The sciences are filled with word-based descriptive data: field observations, materials and habitat identifications, parameter names and units, events and processes. Semantics are also important in medicine, where the human body and illnesses have to be described.  To enhance interoperability among these word-based (semantic) systems, and to more readily explore the rapidly growing quantities of semantic data, there has been a movement towards organizing word-based data in ways that allow machine-assisted, automated analysis.  Biomedicine has made great progress in organizing and using semantic information because of substantial funding investments. This project builds upon extensive investments in the biomedical field, providing an opportunity to rapidly develop the organization of semantic concepts for other domain sciences. &lt;br/&gt;&lt;br/&gt;A toolkit developed by the Center for Computational Language and Education Research (CLEAR TK) will be used to build semantic resources (taxonomies, ontologies, and semantic networks) for three science domains (geology, cryology, and biology).  CLEAR TK is a state-of-the-art natural language processing (NLP) and machine learning (ML) system that also has essential tools for machine-assisted annotation, validation, document tagging, and event extraction.  The CLEAR TK system has been used operationally for biomedical semantic applications, including in high-profile hospitals.  In this project, developments are focused upon the science fields of geology, ice and snow, and biology.  In these fields, accurate extraction of semantic information from the word-based data is required so users can quickly find the data they really need. This project provides a valuable opportunity to expand and evaluate semantic capabilities in conjunction with several scientific domain experts.</AbstractNarration>
<MinAmdLetterDate>08/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/17/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1443085</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Martin</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James H Martin</PI_FULL_NAME>
<EmailAddress>martin@cs.colorado.edu</EmailAddress>
<PI_PHON>3034923552</PI_PHON>
<NSF_ID>000142893</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Martha</FirstName>
<LastName>Palmer</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martha S Palmer</PI_FULL_NAME>
<EmailAddress>mpalmer@colorado.edu</EmailAddress>
<PI_PHON>3034921300</PI_PHON>
<NSF_ID>000377893</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Jenkins</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher J Jenkins</PI_FULL_NAME>
<EmailAddress>chris.jenkins@colorado.edu</EmailAddress>
<PI_PHON>3037355250</PI_PHON>
<NSF_ID>000493812</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ruth</FirstName>
<LastName>Duerr</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ruth E Duerr</PI_FULL_NAME>
<EmailAddress>ruth.duerr@ronininstitute.org</EmailAddress>
<PI_PHON>3039464842</PI_PHON>
<NSF_ID>000401600</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090572</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, room 479]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1165</Code>
<Text>ADVANCES IN BIO INFORMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>5407</Code>
<Text>Polar Cyberinfrastructure</Text>
</ProgramElement>
<ProgramElement>
<Code>7726</Code>
<Text>Data Cyberinfrastructure</Text>
</ProgramElement>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8048</Code>
<Text>Data Infrstr Bldg Blocks-DIBBs</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>02XX</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~1497785</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-d222efaf-7fff-2c12-bf17-f0c9fcdc0f38"> <p dir="ltr"><span>Some fields of business and technology have advanced greatly with machine comprehension of natural texts as a result of heavy investment, especially Biomedicine. On the other hand science fields such as the earth sciences have lagged due to several factors. </span></p> <p dir="ltr"><span>This project formed a collaboration between computational linguists (machine reading experts) and earth scientists (geologists, ice experts, ecologists) so that some of the technologies of biomedicine could be ported over into the earth sciences. Especially because the linguists on the team had already been leaders in forming those biomedical technologies, this promised to accelerate the uptake of natural language processing (NLP) in the science fields. </span></p> <p dir="ltr"><span>Our work proceeded on several fronts. First: a collection of training data needed to be assembled to &lsquo;teach&rsquo; the machine software about words, phrases and their syntaxes and meanings. This is the part which is the greatest workload, and which has typically held back NLP in science fields because of the labor and costs. We invented some software which works immediately on texts and pre-annotates it with syntax (such as parts of speech) and meanings (object names, relationships). This new tool greatly reduces workloads. It reduces the cost of entry for researchers into NLP in their subject of interest.</span></p> <p dir="ltr"><span>We also made discoveries about texts which clarified the research problems. Such as how the software reacts to different types of text: e.g., popular magazine articles or blogs, versus full scientific technical works. The languages they use are different. (The main source of training data in earth sciences was newspapers - not written at all like science !) We also discovered that too little or too much expertise is a bad thing in the annotation: the undergraduate level of knowledge suits the processing best. A team of student annotators tackled texts about: (i) ecology and species interactions, (ii) sea ice and all its shapes and forms, and (iii) earthquakes and how they occur. Along the way we had to decide how to make suitable &lsquo;bins&rsquo; for meanings such as events and processes - both in the bin &lsquo;eventualities&rsquo;. The annotations are now released through Github for use by any other NLP system at any university or other place.</span></p> <p dir="ltr"><span>As this work was proceeding a second team concentrated on improving the processing &lsquo;pipeline&rsquo;. We started from one which already was in use in biomedicine - &lsquo;CLEAR TK&rsquo;. New modules were added to take advantage of the new and improved annotations. Generally the modules employed machine-learning methods like neural networks &ldquo;under the hood&rdquo;. The result allowed the ecologists, for example, to extract detailed information about species from data collections, in this case, &lsquo;Encyclopedia of Life&rsquo; (EOL) from the Smithsonian. Automatically, the processing of that text could determine which species was eating whom. Also which species were physically associated in habitats. It is important that we get machines to do this work because there are millions of species, texts and observations to mine to understand the ecologic network, which by the way, includes agriculture. The size of this task is beyond humans.</span></p> <p dir="ltr"><span>The first impact of the project will likely be the reading across the community, of the Guideline and Documentation that is served through the online portals ResearchGate and Github. Those products have already attracted good readship in a short time there. More practically, the CLEAR TK Pipeline and the Pre-annotation tool at Github should be taken up by laboratories as they become aware of them. We expect that the Annotated Texts will attract considerable interest (as they have already, at live demonstrations) as examples of what is required for NLP to get started in a discipline. The Annotation tool 'Anafora' which was used heavily and developed further during the project, is already used widely.</span></p> <p dir="ltr"><span>At the end of the project time, we can say that have brought biomedical software into the earth sciences, and provided it with training data on a large scale so that scientists interested in big-scale data extraction can now experiment, learn and develop their own capabilities.</span></p> </span></p><br> <p>            Last Modified: 02/15/2019<br>      Modified by: Christopher&nbsp;J&nbsp;Jenkins</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Some fields of business and technology have advanced greatly with machine comprehension of natural texts as a result of heavy investment, especially Biomedicine. On the other hand science fields such as the earth sciences have lagged due to several factors.  This project formed a collaboration between computational linguists (machine reading experts) and earth scientists (geologists, ice experts, ecologists) so that some of the technologies of biomedicine could be ported over into the earth sciences. Especially because the linguists on the team had already been leaders in forming those biomedical technologies, this promised to accelerate the uptake of natural language processing (NLP) in the science fields.  Our work proceeded on several fronts. First: a collection of training data needed to be assembled to ?teach? the machine software about words, phrases and their syntaxes and meanings. This is the part which is the greatest workload, and which has typically held back NLP in science fields because of the labor and costs. We invented some software which works immediately on texts and pre-annotates it with syntax (such as parts of speech) and meanings (object names, relationships). This new tool greatly reduces workloads. It reduces the cost of entry for researchers into NLP in their subject of interest. We also made discoveries about texts which clarified the research problems. Such as how the software reacts to different types of text: e.g., popular magazine articles or blogs, versus full scientific technical works. The languages they use are different. (The main source of training data in earth sciences was newspapers - not written at all like science !) We also discovered that too little or too much expertise is a bad thing in the annotation: the undergraduate level of knowledge suits the processing best. A team of student annotators tackled texts about: (i) ecology and species interactions, (ii) sea ice and all its shapes and forms, and (iii) earthquakes and how they occur. Along the way we had to decide how to make suitable ?bins? for meanings such as events and processes - both in the bin ?eventualities?. The annotations are now released through Github for use by any other NLP system at any university or other place. As this work was proceeding a second team concentrated on improving the processing ?pipeline?. We started from one which already was in use in biomedicine - ?CLEAR TK?. New modules were added to take advantage of the new and improved annotations. Generally the modules employed machine-learning methods like neural networks "under the hood". The result allowed the ecologists, for example, to extract detailed information about species from data collections, in this case, ?Encyclopedia of Life? (EOL) from the Smithsonian. Automatically, the processing of that text could determine which species was eating whom. Also which species were physically associated in habitats. It is important that we get machines to do this work because there are millions of species, texts and observations to mine to understand the ecologic network, which by the way, includes agriculture. The size of this task is beyond humans. The first impact of the project will likely be the reading across the community, of the Guideline and Documentation that is served through the online portals ResearchGate and Github. Those products have already attracted good readship in a short time there. More practically, the CLEAR TK Pipeline and the Pre-annotation tool at Github should be taken up by laboratories as they become aware of them. We expect that the Annotated Texts will attract considerable interest (as they have already, at live demonstrations) as examples of what is required for NLP to get started in a discipline. The Annotation tool 'Anafora' which was used heavily and developed further during the project, is already used widely. At the end of the project time, we can say that have brought biomedical software into the earth sciences, and provided it with training data on a large scale so that scientists interested in big-scale data extraction can now experiment, learn and develop their own capabilities.        Last Modified: 02/15/2019       Submitted by: Christopher J Jenkins]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
