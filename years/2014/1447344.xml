<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: IA: DKA: Collaborative Research: High-Throughput Connectomics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>935000.00</AwardTotalIntnAmount>
<AwardAmount>935000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration>High-Throughput Connectomics &lt;br/&gt;&lt;br/&gt;Connectomics is the science of mapping the connectivity between neuronal structures to help us understand how brains work. Using the analogy of astronomy, connectomics researchers wish to build 'telescopes' that will allow scientists to accurately view the brain. However, as in astronomy, the raw data collected by microtomes and electron microscopes, the instruments of connectomics, is too large to store effectively, and must be analyzed at very high computation rates. Our goal is to research, develop, and deploy a software architecture that enables high-throughput analysis of connectomics data at the speed at which it is being acquired. We will develop the first computational infrastructure to support high-throughput connectomics without human intervention. If successful, this system will allow for the first time the mapping of a cortical column of a small mammalian brain (1 cubic millimeter), and hopefully within a few years the mapping of significant sections of a mammalian cortex. &lt;br/&gt;&lt;br/&gt;The solution to the big data problem of connectomics is a new high-throughput connectomics software architecture that we call MapRecurse. MapRecurse, named so because it bears some resemblance to the widely used MapReduce framework, will provide a unified way of specifying sequences of computational steps and validation tests to be applied to the collected data. Key to MapRecurse will be the ability to layout data and computation in a structured way that preserves locality. Using it, programmers will be able to apply fast, less accurate segmentation algorithms to low resolutions of the data in order to quickly compute a first version of the output neural network graph. Domain-specific graph theoretical methods will then check for correctness of the graph and identify areas of inconsistencies that are in need of further refinement. MapRecurse will then apply bottom-up, local processing with slower, more accurate segmentation and reconstruction algorithms to higher resolutions of the data, verifying and correcting any errors. The iterations progress recursively and in parallel across multiple cores, giving the approach its name. We believe that MapRecurse and the data structures and algorithms developed here will find applications in other high-throughput applications, such as, in astronomy, biology, social media applications, or economics.</AbstractNarration>
<MinAmdLetterDate>08/25/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1447344</AwardID>
<Investigator>
<FirstName>Hanspeter</FirstName>
<LastName>Pfister</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hanspeter Pfister</PI_FULL_NAME>
<EmailAddress>pfister@seas.harvard.edu</EmailAddress>
<PI_PHON>6174968269</PI_PHON>
<NSF_ID>000185558</NSF_ID>
<StartDate>08/25/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jeff</FirstName>
<LastName>Lichtman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeff Lichtman</PI_FULL_NAME>
<EmailAddress>jeff@mcb.harvard.edu</EmailAddress>
<PI_PHON>6174968943</PI_PHON>
<NSF_ID>000058932</NSF_ID>
<StartDate>08/25/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard SEAS]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021382933</ZipCode>
<StreetAddress><![CDATA[33 Oxford Street, 227MD]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramElement>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~935000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <div> <div><span>High-Throughput Connectomics</span></div> <div><span>Hanspeter P&#64257;ster (PI) and Jeff Lichtman, Harvard University; Nir Shavit, MIT</span></div> <br /> <div><span>Connectomics is the science of mapping the connectivity between neuronal structures to help us understand how brains work. Using the analogy of astronomy, connectomics researchers wish to build &ldquo;telescopes&rdquo; that will allow scientists to view the brain accurately. However, as in astronomy, the raw data collected by microtomes and electron microscopes, the instruments of connectomics, is too large to store effectively and must be analyzed at very high computation rates. In this research, we have developed and deployed a software architecture that enables high-throughput analysis of connectomics data from electron microscopy images. While such a system must tackle many of the problems that occur in big data analysis and engineering, tremendous differences in data size, computational complexity, and the problem domain, require novel computational frameworks and algorithms. Our solution is a new high-throughput connectomics software architecture for highly parallel data-allocation, control-&#64258;ow, and scheduling, and a collection of novel reconstruction algorithms for connectomics data. </span></div> <br /> <div><span>Our optimized software uses a deep learning approach for highly accurate cell segmentation and synapse detection in electron microscopy images. Our methods achieved the 3rd place on the competitive SNEMI3D EM segmentation and CREMI EM synapse detection benchmarks, respectively, while being much faster than competing approaches. We developed methods for semi-automatic segmentation error correction, called proof-reading, that flags candidate errors to the user, thereby reducing the overall error 7.5x faster than previous approaches. And we developed a new deep learning method to resolve 3D segmentation errors using biological constraints based on a skeleton representation of neurites. </span></div> <br /><br /><br /></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/18/2019<br>      Modified by: Hanspeter&nbsp;Pfister</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    High-Throughput Connectomics Hanspeter P&#64257;ster (PI) and Jeff Lichtman, Harvard University; Nir Shavit, MIT   Connectomics is the science of mapping the connectivity between neuronal structures to help us understand how brains work. Using the analogy of astronomy, connectomics researchers wish to build "telescopes" that will allow scientists to view the brain accurately. However, as in astronomy, the raw data collected by microtomes and electron microscopes, the instruments of connectomics, is too large to store effectively and must be analyzed at very high computation rates. In this research, we have developed and deployed a software architecture that enables high-throughput analysis of connectomics data from electron microscopy images. While such a system must tackle many of the problems that occur in big data analysis and engineering, tremendous differences in data size, computational complexity, and the problem domain, require novel computational frameworks and algorithms. Our solution is a new high-throughput connectomics software architecture for highly parallel data-allocation, control-&#64258;ow, and scheduling, and a collection of novel reconstruction algorithms for connectomics data.    Our optimized software uses a deep learning approach for highly accurate cell segmentation and synapse detection in electron microscopy images. Our methods achieved the 3rd place on the competitive SNEMI3D EM segmentation and CREMI EM synapse detection benchmarks, respectively, while being much faster than competing approaches. We developed methods for semi-automatic segmentation error correction, called proof-reading, that flags candidate errors to the user, thereby reducing the overall error 7.5x faster than previous approaches. And we developed a new deep learning method to resolve 3D segmentation errors using biological constraints based on a skeleton representation of neurites.                  Last Modified: 01/18/2019       Submitted by: Hanspeter Pfister]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
