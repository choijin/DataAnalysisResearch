<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Non-visual Access to Graphical Information Using a Vibro-Audio Display</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>499853.00</AwardTotalIntnAmount>
<AwardAmount>530573</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Vision impairment is estimated by the World Health Organization as effecting 12 million people in the United States and as many as 285 million people worldwide, and these numbers are projected to double by 2030 due to the aging of our population.  Lack of access to non-textual material such as graphs and maps is a major impediment for the blind, because the ability to apprehend and accurately interpret such information is critical for success in the classroom and in the workplace, as well as for independent travel.  The inability to exploit this information helps explain why only about 11% of blind or low-vision persons have a bachelor's degree, why only 25% of blind people are employed, and why almost 70% of blind individuals do not navigate independently outside of their home.  A major step toward improving these numbers, as well as the overall quality of life for members of the blind and low-vision community, would be to solve the longstanding challenge of affording low-cost and effective access to key graphical material.&lt;br/&gt;&lt;br/&gt;The PI's goal in this project is to develop and evaluate a highly intuitive tool for doing just that.  To this end, he will explore a multimodal combination of vibro-tactile, audio, and kinesthetic cues that can be generated by modern touchscreen tablets (especially smartphones), to convey useful visual information in real-time.  Benefits of this approach include portability, affordability, and flexibility of use for multiple critical and common applications such as those enumerated above.  The PI argues that considering these design factors from the outset, in conjunction with principled empirical investigations to evaluate and enhance information perceptibility interleaved with frequent prototype testing and iterative refinement, with tight involvement of members of the target population in all phases of the research, will ensure that project outcomes significantly reduce the graphical information gap between blind persons and their sighted peers.  The research will make important contributions to our understanding of how blind and low-vision individuals process non-visual information, which is essential for rendering perceptually salient multimodal graphics, and will also help establish best practices both for rendering these graphics using a vibro-audio interface implemented on touchscreen enabled devices, as well as for similar future specialized interface development efforts.</AbstractNarration>
<MinAmdLetterDate>07/31/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/09/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1425337</AwardID>
<Investigator>
<FirstName>Nicholas</FirstName>
<LastName>Giudice</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicholas Giudice</PI_FULL_NAME>
<EmailAddress>nicholas.giudice@maine.edu</EmailAddress>
<PI_PHON>2075812187</PI_PHON>
<NSF_ID>000503809</NSF_ID>
<StartDate>07/31/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maine</Name>
<CityName>ORONO</CityName>
<ZipCode>044695717</ZipCode>
<PhoneNumber>2075811484</PhoneNumber>
<StreetAddress>5717 Corbett Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maine</StateName>
<StateCode>ME</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>ME02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>186875787</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MAINE SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071750426</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maine]]></Name>
<CityName>Orono</CityName>
<StateCode>ME</StateCode>
<ZipCode>044695717</ZipCode>
<StreetAddress><![CDATA[5717 Corbett Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maine</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>ME02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~177568</FUND_OBLG>
<FUND_OBLG>2015~337645</FUND_OBLG>
<FUND_OBLG>2017~15360</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Outcomes Report.</p> <p>This project designed, developed, and experimentally evaluated a new low-cost, highly intuitive tool for providing non-visual access and interpretation of graphical information for blind and visually impaired (BVI) people. The ability to effectively use and accurately understand graphs, figures and other visual representations of numeric data is increasingly critical for success in the classroom and at work. Similarly, effective learning and navigation of new environments also frequently depends on the use of maps and other graphical aids. For BVI people, gaining access to this graphical information and using it to support accurate learning or independent travel, as is so readily done by their sighted peers, is often extremely difficult (if not impossible). While access to printed material has largely been solved via screen reading software using text-to-speech, these programs do not have the ability to convey meaningful information about graphic and non-text-based material.</p> <p>Our vision in this project was to develop a new method for solving the &lsquo;graphics access problem&rsquo; that would bridge this information gap by providing nonvisual access to graphical information by using the touchscreen of a smartphone or tablet. With the system, users move their finger around the screen and whenever they touch a graphical onscreen element, the vibration motor is activated, and they can feel that attribute in real-time based on vibrotactile feedback at that location. This vibrotactile finger stimulation is perceived as points, lines, or regions, similar to what would be perceived by touching traditional hardcopy graphics. The difference is that where traditional tactile graphics are static representations, based only on touch, and require expensive technologies to produce, the solution developed here is dynamic, is based on multiple senses, and is built on a low-cost commercial platform. In other words, using this system, the graphical information can be produced using a combination of vibrotactile and auditory cues, as well as speech-based labels that are triggered at key locations on the graphic as needed. In this way, the combination of vibrotactile, auditory, and kinesthetic information from hand movements around the touchscreen provides the user with real-time access to spatial and semantically-relevant information about the graphical material. The power of this system is that it is refreshable, meaning that any graphical information can be presented and dynamically changed. In addition, it is based on a low-cost, portable device (smart phone or tablet) that is already owned by many blind people as it is multi-purpose and includes many built-in universal design parameters.</p> <p>To accomplish this goal, we developed a prototype system and conducted a series of studies to show its efficacy and to obtain user feedback to improve the system, which we called a vibro-audio interface (VAI). We then conducted a second series of experiments with BVI participants to maximize its usability and to obtain scientifically-validated parameters governing best practices for rendering graphical content using the VAI. In a third series of studies, the system was extended to support new nonvisual techniques for zooming and panning of large format graphics, such as maps.</p> <p>The scientific and intellectual contribution of this research was a new system, rigorously tested in over a dozen behavioral experiments with more than 250 blind and blindfolded-sighted participants. Results demonstrated that our system was as effective for nonvisual learning/understanding of graphical information as the current gold standard paper-based techniques, but does so using a multimodal system that is more robust, real-time, and less expensive than is possible from current approaches.</p> <p>The work has been presented at many leading international research conferences, published in top academic journals, and supported multiple graduate and undergraduate students. It is increasingly being used by a growing number of researchers around the country and has motivated at least two subsequent grants and one spin-off company to extend the initial research started in this project. Further development of the VAI will have significant broader impacts on the educational, vocational, and social opportunities for blind individuals. It may also have important new application as an eyes-free interface for sighted people, such as for nonvisual control of vehicle-based infotainment systems during driving.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/31/2019<br>      Modified by: Nicholas&nbsp;Giudice</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Outcomes Report.  This project designed, developed, and experimentally evaluated a new low-cost, highly intuitive tool for providing non-visual access and interpretation of graphical information for blind and visually impaired (BVI) people. The ability to effectively use and accurately understand graphs, figures and other visual representations of numeric data is increasingly critical for success in the classroom and at work. Similarly, effective learning and navigation of new environments also frequently depends on the use of maps and other graphical aids. For BVI people, gaining access to this graphical information and using it to support accurate learning or independent travel, as is so readily done by their sighted peers, is often extremely difficult (if not impossible). While access to printed material has largely been solved via screen reading software using text-to-speech, these programs do not have the ability to convey meaningful information about graphic and non-text-based material.  Our vision in this project was to develop a new method for solving the ‘graphics access problem’ that would bridge this information gap by providing nonvisual access to graphical information by using the touchscreen of a smartphone or tablet. With the system, users move their finger around the screen and whenever they touch a graphical onscreen element, the vibration motor is activated, and they can feel that attribute in real-time based on vibrotactile feedback at that location. This vibrotactile finger stimulation is perceived as points, lines, or regions, similar to what would be perceived by touching traditional hardcopy graphics. The difference is that where traditional tactile graphics are static representations, based only on touch, and require expensive technologies to produce, the solution developed here is dynamic, is based on multiple senses, and is built on a low-cost commercial platform. In other words, using this system, the graphical information can be produced using a combination of vibrotactile and auditory cues, as well as speech-based labels that are triggered at key locations on the graphic as needed. In this way, the combination of vibrotactile, auditory, and kinesthetic information from hand movements around the touchscreen provides the user with real-time access to spatial and semantically-relevant information about the graphical material. The power of this system is that it is refreshable, meaning that any graphical information can be presented and dynamically changed. In addition, it is based on a low-cost, portable device (smart phone or tablet) that is already owned by many blind people as it is multi-purpose and includes many built-in universal design parameters.  To accomplish this goal, we developed a prototype system and conducted a series of studies to show its efficacy and to obtain user feedback to improve the system, which we called a vibro-audio interface (VAI). We then conducted a second series of experiments with BVI participants to maximize its usability and to obtain scientifically-validated parameters governing best practices for rendering graphical content using the VAI. In a third series of studies, the system was extended to support new nonvisual techniques for zooming and panning of large format graphics, such as maps.  The scientific and intellectual contribution of this research was a new system, rigorously tested in over a dozen behavioral experiments with more than 250 blind and blindfolded-sighted participants. Results demonstrated that our system was as effective for nonvisual learning/understanding of graphical information as the current gold standard paper-based techniques, but does so using a multimodal system that is more robust, real-time, and less expensive than is possible from current approaches.  The work has been presented at many leading international research conferences, published in top academic journals, and supported multiple graduate and undergraduate students. It is increasingly being used by a growing number of researchers around the country and has motivated at least two subsequent grants and one spin-off company to extend the initial research started in this project. Further development of the VAI will have significant broader impacts on the educational, vocational, and social opportunities for blind individuals. It may also have important new application as an eyes-free interface for sighted people, such as for nonvisual control of vehicle-based infotainment systems during driving.          Last Modified: 12/31/2019       Submitted by: Nicholas Giudice]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
