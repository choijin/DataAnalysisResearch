<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Acquisition of an Extreme GPU Cluster for Interdisciplinary Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>3500000.00</AwardTotalIntnAmount>
<AwardAmount>3500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Edward Walker</SignBlockName>
<PO_EMAI>edwalker@nsf.gov</PO_EMAI>
<PO_PHON>7032924863</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Stanford University requests $3,500,000 over 36 months to acquire an extreme GPU HPC cluster, called X-GPU, comprising 54 compute nodes built using the Cray Hydra technology with FDR Infiniband. Each node has Intel Haswell 12-cores; 8 NVIDIA Kepler cards; 128 GB of DDR4 memory; a 120 GB SSD and two 1 TB hard drives. energy-efficient, computational facility providing almost a petaflop of computational power. It will be used by 1) at least 25 research groups representing more than 100 students and postdoctorals at Stanford across 15 departments and 4 schools, 2) at least 8 collaborators from at least 7 other institutions across the nation, and 3) by as many as hundreds of national researchers through the NSF-sponsored XSEDE allocation system. The PIs plan to offer 25% of X-GPU to XSEDE to offset the impacts from the planned retiring of Keeneland, the current XSEDE resource providing heterogeneous parallel computing with CPUs and GPUs to the national community.&lt;br/&gt;Identified scientific outcomes enabled by this instrument include, but not limited to: astrophysics and cosmology, bioinformatics and biology, materials modeling, and climate modeling. The researchers have already invested significant efforts to develop modeling and simulation codes that can demonstrate high performance on GPU-accelerated clusters. The PIs plan develop software infrastructure and educational materials to help the national community in the transition to fine-grained parallel thinking and algorithm design, which is critical to effectively use this novel high-performance, low-cost, energy-efficient architecture.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1429830</AwardID>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Martinez</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd J Martinez</PI_FULL_NAME>
<EmailAddress>Todd.Martinez@stanford.edu</EmailAddress>
<PI_PHON>6507368860</PI_PHON>
<NSF_ID>000261066</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Vijay</FirstName>
<LastName>Pande</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vijay S Pande</PI_FULL_NAME>
<EmailAddress>pande@stanford.edu</EmailAddress>
<PI_PHON>6507233660</PI_PHON>
<NSF_ID>000155807</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tom</FirstName>
<LastName>Abel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tom Abel</PI_FULL_NAME>
<EmailAddress>tabel@stanford.edu</EmailAddress>
<PI_PHON>6509262421</PI_PHON>
<NSF_ID>000311631</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Margot</FirstName>
<LastName>Gerritsen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Margot Gerritsen</PI_FULL_NAME>
<EmailAddress>margot.gerritsen@stanford.edu</EmailAddress>
<PI_PHON>6507252727</PI_PHON>
<NSF_ID>000258294</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943054401</ZipCode>
<StreetAddress><![CDATA[333 Campus Drive, Rm 389A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~3500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This grant funded the acquisition of a unique computer cluster (called Xstream) based on a dense arrangement of graphical processing units (GPUs). GPUs are commodity cards that are often used for videogames but have been shown to also be effective at solving complex mathematical problems in the sciences. The Xstream cluster was installed at Stanford and a portion of the time was made available to researchers nationwide through the XSEDE allocation scheme. The successful installation and operation of Xstream proved the advantages of a dense GPU approach. Previous computational resources including GPUs generally only used one or two GPUs per computational node. In contrast, Xstream has 16 GPUs per computational node and thus encourages the use of the GPU as a primary computational resource for all aspects of the computation. Previous approaches used the GPU only to accelerate a few parts of the computation.</p> <p>Research carried out so far on the Xstream cluster has led to publications in fields as diverse as astrophysics, molecular biology and medicine, materials chemistry for renewable energy, machine learning for computer vision, and fundamental quantum mechanics. The availability of Xstream has allowed many graduate students and postdoctoral scholars to experiment with dense GPU based computing and to exploit massive parallelism in their research. Furthermore, it has provided computational resources that are vital to the research progress of groups across the country, including Stanford, CCNY, UC Merced, UC Davis, and many others.</p><br> <p>            Last Modified: 01/09/2019<br>      Modified by: Todd&nbsp;J&nbsp;Martinez</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This grant funded the acquisition of a unique computer cluster (called Xstream) based on a dense arrangement of graphical processing units (GPUs). GPUs are commodity cards that are often used for videogames but have been shown to also be effective at solving complex mathematical problems in the sciences. The Xstream cluster was installed at Stanford and a portion of the time was made available to researchers nationwide through the XSEDE allocation scheme. The successful installation and operation of Xstream proved the advantages of a dense GPU approach. Previous computational resources including GPUs generally only used one or two GPUs per computational node. In contrast, Xstream has 16 GPUs per computational node and thus encourages the use of the GPU as a primary computational resource for all aspects of the computation. Previous approaches used the GPU only to accelerate a few parts of the computation.  Research carried out so far on the Xstream cluster has led to publications in fields as diverse as astrophysics, molecular biology and medicine, materials chemistry for renewable energy, machine learning for computer vision, and fundamental quantum mechanics. The availability of Xstream has allowed many graduate students and postdoctoral scholars to experiment with dense GPU based computing and to exploit massive parallelism in their research. Furthermore, it has provided computational resources that are vital to the research progress of groups across the country, including Stanford, CCNY, UC Merced, UC Davis, and many others.       Last Modified: 01/09/2019       Submitted by: Todd J Martinez]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
