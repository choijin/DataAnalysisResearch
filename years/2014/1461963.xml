<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BDD: Human-Centered Situational Awareness Platform for Disaster Response and Recovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2015</AwardEffectiveDate>
<AwardExpirationDate>03/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>299976.00</AwardTotalIntnAmount>
<AwardAmount>299976</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Efficient and thorough data collection and its timely analysis are critical to any disaster response and recovery system in order to save people's lives during disasters. However, access to comprehensive data in disaster areas and their quick analysis to transform the data to actionable knowledge are major data science challenges. Moreover, the effective presentation of the collected knowledge to human decision-makers is an open problem.  In this project, these challenges of data collection, analysis and presentation are collectively referred to as 'Situational Awareness', and are studied by a collaborative team of data scientists from the University of Southern California (USC) in United States and the National Institute of Informatics (NII) in Japan.&lt;br/&gt;&lt;br/&gt;The first objective of this project is to devise effective techniques for comprehensive data collection. The challenge is that during disaster the availability of information is spatially biased and some areas are not well covered by available information. Towards this end, USC's spatial crowdsourcing platform, dubbed MediaQ, is utilized to collect pictures and videos on-demand from mobile devices of people in the vicinity of the disaster areas to facilitate effective collection, orchestration and aggregation of information. The second objective is to develop efficient methods for organizing and analyzing incoming data streams for human decision makers, given the challenges of volume and variety of unstructured data whose veracity is unknown. The collaborative US-Japan team takes advantage of their combined expertise in developing Spatial-Temporal-Thematic analytics engines and Geospatial Image Filtering Tools to meet this objective. Finally, for an effective presentation of the knowledge to human stakeholders, NII's DiVE virtual environment engine is used to facilitate seamless presentation and communication for both onsite (humans in the field) and offsite (disaster managers at the center) stakeholders.</AbstractNarration>
<MinAmdLetterDate>03/30/2015</MinAmdLetterDate>
<MaxAmdLetterDate>03/30/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1461963</AwardID>
<Investigator>
<FirstName>Cyrus</FirstName>
<LastName>Shahabi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cyrus Shahabi</PI_FULL_NAME>
<EmailAddress>shahabi@usc.edu</EmailAddress>
<PI_PHON>2137408162</PI_PHON>
<NSF_ID>000346776</NSF_ID>
<StartDate>03/30/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Seon</FirstName>
<LastName>Kim</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Seon H Kim</PI_FULL_NAME>
<EmailAddress>seonkim@usc.edu</EmailAddress>
<PI_PHON>2138210876</PI_PHON>
<NSF_ID>000491206</NSF_ID>
<StartDate>03/30/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[3720 S. Flower St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramReference>
<Code>5921</Code>
<Text>JAPAN</Text>
</ProgramReference>
<ProgramReference>
<Code>7363</Code>
<Text>RES IN NETWORKING TECH &amp; SYS</Text>
</ProgramReference>
<ProgramReference>
<Code>8230</Code>
<Text>Big Data and Disasters</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~299976</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Big data has the potential to enhance situational awareness for disaster response and recovery. When properly collected and analyzed, visual data such as photos and videos provide comprehensive information to help understand disasters. However, due to the nature of large volume of unstructured visual data, both accessing, analyzing, and transforming big disaster data into actionable knowledge in a timely fashion is challenging, which limits its potential and utilization. With the rapid advances in mobile technology and machine learning (ML), the goal of this project is to enhance the utilization of big disaster data by utilizing these emerging paradigms in data collection and analysis.</p> <p>This research project focused on two major objectives in big disaster data. The first objective of the project is to devise effective techniques for comprehensive data collection, addressing the problem of effective visual data collection by mobile devices such as smartphones for disasters by identifying a unique challenge of prioritizing data collection and transmission under bandwidth scarcity caused by damaged communication networks. This problem is critical when the availability of information is spatially biased with potentially some uncovered areas. We addressed this problem by utilizing spatial crowdsourcing of geo-tagged pictures and videos collected from mobile devices to facilitate effective collection, orchestration, and aggregation of information. The final outcome is an integration of the crowdsourcing solution with our media data management platform, MediaQ, which acquires mobile images and videos utilizing fine granularity spatial metadata for a rapidly changing disaster situation. Specifically, for the data collection, we devised the Visual Awareness Maximization algorithm to collect the most meaningful data first, assuming bandwidth scarcity.</p> <p>The second objective of the project is to develop efficient methods for organizing and analyzing incoming data for human decision makers, given the challenges of volume and variety of unstructured data. Even though a large quantity of data can be collected using growing number of sensors and mobile phones, they are hardly useful unless they are efficiently organized to be searchable for timely access. Data access for a big visual dataset faces two major challenges: <em>search performance </em>due to the large volume of visual data and <em>inaccuracy of search</em> <em>results </em>due to imprecision in image matching. To overcome these challenges, we focused on designing index structures that expedite the evaluation of spatial-visual queries enabling efficient query processing mechanisms on both spatial and visual features of the images. The final outcome is a class of hybrid index structures that evaluate both spatial and visual features in tandem, which produces more accurate and relevant search output faster than conventional indexes.</p> <p>Initially, our research was to establish a reference model for a human-in-the-loop disaster response platform to a challenge in a disaster situation: human decision makers and stakeholders suffer from both too little and too much information. Thus, we focused on human involvement in defining crowdsourcing tasks and data analysis during the first two years. However, we witnessed a rapid advance in machine learning and visual analysis techniques such as image deep learning, with significant potential to replace human intelligence. Moreover, the overwhelming amount of visual data can be hardly searched and verified by humans in many real-world applications. Thus, we investigated the automation of spatial crowdsourcing task generation based on the quantification of visual coverage by the collected images for a certain area of interest. This implies an automatic identification of missing visual information for an area and issuing crowdsourcing tasks to collect such missing data. Once enough visual data was collected, we analyzed the images using machine learning to automatically understand the visual content.</p> <p>Besides devising comprehensive solutions for big disaster data and our aforementioned results, which were published in more than 20 research publications, another main outcome of this project was the integration of the solution in a prototype system, MediaQ, and various real-world experiments that demonstrate the usefulness of our solutions. We have collaborated with several government agencies (e.g., US National Geospatial Intelligence Agency, NGA, Japan National Institute of Informatics, NII, Los Angeles Sanitation Department, LASAN) and industry partners (e.g., MITRE, Oracle) to integrate our solutions with their systems (e.g., NGA's GeoQ) and to apply image machine learning for real use cases (e.g., street cleanliness classification to automatically identify trash objects from geo-tagged images, surface material identification and classification for graffiti removal, and road damage detection and classification). Another outcome of our research includes the open datasets collected via our MediaQ system and the open source code of our solutions.</p> <p>Lastly, four Ph.D. students were involved in this project. One is now working at Didi AI Lab, another joined Amazon, and the other is working at Google. The remaining student will complete his Ph.D. by December 2019 and is interested in joining academia.</p><br> <p>            Last Modified: 05/30/2019<br>      Modified by: Cyrus&nbsp;Shahabi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Big data has the potential to enhance situational awareness for disaster response and recovery. When properly collected and analyzed, visual data such as photos and videos provide comprehensive information to help understand disasters. However, due to the nature of large volume of unstructured visual data, both accessing, analyzing, and transforming big disaster data into actionable knowledge in a timely fashion is challenging, which limits its potential and utilization. With the rapid advances in mobile technology and machine learning (ML), the goal of this project is to enhance the utilization of big disaster data by utilizing these emerging paradigms in data collection and analysis.  This research project focused on two major objectives in big disaster data. The first objective of the project is to devise effective techniques for comprehensive data collection, addressing the problem of effective visual data collection by mobile devices such as smartphones for disasters by identifying a unique challenge of prioritizing data collection and transmission under bandwidth scarcity caused by damaged communication networks. This problem is critical when the availability of information is spatially biased with potentially some uncovered areas. We addressed this problem by utilizing spatial crowdsourcing of geo-tagged pictures and videos collected from mobile devices to facilitate effective collection, orchestration, and aggregation of information. The final outcome is an integration of the crowdsourcing solution with our media data management platform, MediaQ, which acquires mobile images and videos utilizing fine granularity spatial metadata for a rapidly changing disaster situation. Specifically, for the data collection, we devised the Visual Awareness Maximization algorithm to collect the most meaningful data first, assuming bandwidth scarcity.  The second objective of the project is to develop efficient methods for organizing and analyzing incoming data for human decision makers, given the challenges of volume and variety of unstructured data. Even though a large quantity of data can be collected using growing number of sensors and mobile phones, they are hardly useful unless they are efficiently organized to be searchable for timely access. Data access for a big visual dataset faces two major challenges: search performance due to the large volume of visual data and inaccuracy of search results due to imprecision in image matching. To overcome these challenges, we focused on designing index structures that expedite the evaluation of spatial-visual queries enabling efficient query processing mechanisms on both spatial and visual features of the images. The final outcome is a class of hybrid index structures that evaluate both spatial and visual features in tandem, which produces more accurate and relevant search output faster than conventional indexes.  Initially, our research was to establish a reference model for a human-in-the-loop disaster response platform to a challenge in a disaster situation: human decision makers and stakeholders suffer from both too little and too much information. Thus, we focused on human involvement in defining crowdsourcing tasks and data analysis during the first two years. However, we witnessed a rapid advance in machine learning and visual analysis techniques such as image deep learning, with significant potential to replace human intelligence. Moreover, the overwhelming amount of visual data can be hardly searched and verified by humans in many real-world applications. Thus, we investigated the automation of spatial crowdsourcing task generation based on the quantification of visual coverage by the collected images for a certain area of interest. This implies an automatic identification of missing visual information for an area and issuing crowdsourcing tasks to collect such missing data. Once enough visual data was collected, we analyzed the images using machine learning to automatically understand the visual content.  Besides devising comprehensive solutions for big disaster data and our aforementioned results, which were published in more than 20 research publications, another main outcome of this project was the integration of the solution in a prototype system, MediaQ, and various real-world experiments that demonstrate the usefulness of our solutions. We have collaborated with several government agencies (e.g., US National Geospatial Intelligence Agency, NGA, Japan National Institute of Informatics, NII, Los Angeles Sanitation Department, LASAN) and industry partners (e.g., MITRE, Oracle) to integrate our solutions with their systems (e.g., NGA's GeoQ) and to apply image machine learning for real use cases (e.g., street cleanliness classification to automatically identify trash objects from geo-tagged images, surface material identification and classification for graffiti removal, and road damage detection and classification). Another outcome of our research includes the open datasets collected via our MediaQ system and the open source code of our solutions.  Lastly, four Ph.D. students were involved in this project. One is now working at Didi AI Lab, another joined Amazon, and the other is working at Google. The remaining student will complete his Ph.D. by December 2019 and is interested in joining academia.       Last Modified: 05/30/2019       Submitted by: Cyrus Shahabi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
