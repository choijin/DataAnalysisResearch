<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Small:  Designing Networks for High Throughput</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The combination of two inexorable trends - increasing parallelism and increasing big data analytics - means computing systems urgently need efficient high-capacity networks, capable of very rapidly delivery of data among servers. Modern warehouse-scale data centers, operated by large Internet services like Google, Amazon, Facebook, and many others, require networks connecting tens of thousands of servers. High capacity is useful to support data intensive applications such as Map-Reduce and scientific simulations. In cloud computing, a high-capacity network gives operators the freedom to place virtual machines on any physical server, without needing to worry about capacity constraints between hosts. This freedom translates into higher server utilization, lower management overhead, and thus lower operating costs.&lt;br/&gt;&lt;br/&gt;As cloud applications have demanded greater communication among servers, numerous data center network architectures have recently been proposed.  In these networks, both the routing and the topology of the network - that is, the pattern of links among routers and servers - are critical in working together to obtain high capacity. However, we lack a fundamental understanding about which network topologies achieve high capacity and what the resulting systems design tradeoffs are, as well as how to build real-time routing algorithms that achieve near-optimal capacity in arbitrary topologies.&lt;br/&gt;&lt;br/&gt;The project spans theory and systems design.  The principal investigators (PIs) will develop high-capacity network architectures, including routing and topologies that are efficient and flexible.  They will develop a suite of software to efficiently compute capacity of network architectures in a variety of network traffic patterns, topologies, and routing protocols, as well as build a hardware testbed to test the designs.  They will then tackle an ambitious goal -- developing a system which achieves near-optimal multicommodity flow in real time (e.g., tens or hundreds of milliseconds) in arbitrary network topologies.  Finally, the PIs will apply these routing advances to enable new more efficient and flexibile data center network topologies.&lt;br/&gt;&lt;br/&gt;The broader impact of this project lies in expected research impact and educational impact.  The data center network designs produced in this project are expected to provide significant efficiency and operational flexibility compared to today's data centers, potentially achieving 25-40% higher capacity with the same equipment as current large-scape data center networks.  The PIs will also develop a course module and e-book surveying the fundamental topic of how to achieve high capacity in networks, including past work and that of this project.  Finally, the project will include a free release of a software suite to facilitate open and reproducible research.</AbstractNarration>
<MinAmdLetterDate>09/02/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/02/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423452</AwardID>
<Investigator>
<FirstName>Philip Brighten</FirstName>
<LastName>Godfrey</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Philip Brighten Godfrey</PI_FULL_NAME>
<EmailAddress>pbg@illinois.edu</EmailAddress>
<PI_PHON>2173334862</PI_PHON>
<NSF_ID>000537388</NSF_ID>
<StartDate>09/02/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexandra</FirstName>
<LastName>Kolla</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexandra Kolla</PI_FULL_NAME>
<EmailAddress>alexkolla@gmail.com</EmailAddress>
<PI_PHON>3034927514</PI_PHON>
<NSF_ID>000636865</NSF_ID>
<StartDate>09/02/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Data centers today are an integral part of IT infrastructure for essentially every large and small company -- whether directly through private data centers, or indirectly through use of services running on hyperscale public data centers.&nbsp; To support big data and cloud virtualization applications in these data centers, a high-throughput network connecting servers is essential.</span></p> <p><span>The goal of this project was to design more efficient and flexible high-throughput network architectures for data centers.&nbsp; The work studied both the underlying physical network topology &ndash;&nbsp;that is, the pattern of how network routers are connected to each other &ndash; and methods to route and load balance data transmission through the network.</span></p> <p><span>The project's starting point was a rigorous understanding of how to measure throughput.&nbsp; The project showed that cut-based metrics (like bisection bandwidth), traditionally used as a measure of worst-case topology performance, are actually poor measures of throughput. The project instead developed a heuristic to approximate the worst case throughput of a topology.&nbsp; With these basic tools in hand, the project performed the largest benchmarking study of network topologies to date, in both typical workloads and worst-case workloads.&nbsp; This study concluded that a particular class of network topologies, called expander graphs, achieve highest performance at scale.&nbsp; The study also showed weaknesses of other topologies with difficult traffic patterns &ndash; in particular, the Clos topologies in common use today perform relatively poorly on skewed workloads, which are common.</span></p> <p><span>Once a topology is selected, how do you transmit data through it?&nbsp; Data center topologies have a large number paths between any source and destination, and it is important to be able to load-balance traffic across these paths to achieve high performance.&nbsp; The project developed two approaches to use all paths efficiently.&nbsp; First, to assist centralized control of traffic, the project designed a clean source-routing abstraction to enable use of every available path, compactly encoded using standard software-defined networking protocols.&nbsp; Second, the project developed an orthogonal approach using only local decisions without a central controller, but with a small amount of extra intelligence in network switches. </span>This approach, referred to as <span>"micro load balancing", was shown to substantially reduce micro-bursts of congestion, thus allowing data center networks to have higher throughput utilization.</span></p> <p><span>The above techniques are effective, but they utilize novel controller or hardware designs.&nbsp; Is it possible to realize the benefits of the highest-throughput topologies (i.e., expanders) using today's network hardware and protocols? To answer this, we designed a practical method of configuring routing and performed a realistic evaluation in terms of data center scale, traffic patterns, oversubscription ratios, and other factors.&nbsp; The results of these experiments showed that expanders can realize 3x more throughput than an equivalent 3-tier Clos network (common for large data centers today), and 1.5x more throughput than an equivalent leaf-spine topology (common for small to medium size data centers today), for a wide range of scenarios, with only traditional protocols. Expanders also achieved lower flow completion times, more resilience to bursty load conditions like incast and outcast, and degraded more gracefully with increasing load. These results are based on extensive simulations and hardware testbed experiments.</span></p> <p><span>In addition to these research results, the project produced broader impact in the form of experimental infrastructure.&nbsp; Originally used in support of the above results, this infrastructure has already been employed by other research groups.&nbsp; </span><span>First, a topology benchmarking suite includes a library of network topologies and traffic matrices (TMs, i.e., workloads), and flow optimization software for evaluating the performance of topologies with these TMs under optimal routing. This provides an open way to benchmark new topology designs.&nbsp; Second, the OCEAN testbed allows experimenters to map virtual topologies directly onto physical hardware (rather than software switches), thus providing higher performance fidelity than other emulation platforms. </span><span>In addition, the project supported professional development of several undergraduate and PhD researchers, and a number of publications in top computer networking venues.</span></p><br> <p>            Last Modified: 05/29/2019<br>      Modified by: Philip Brighten&nbsp;Godfrey</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Data centers today are an integral part of IT infrastructure for essentially every large and small company -- whether directly through private data centers, or indirectly through use of services running on hyperscale public data centers.  To support big data and cloud virtualization applications in these data centers, a high-throughput network connecting servers is essential.  The goal of this project was to design more efficient and flexible high-throughput network architectures for data centers.  The work studied both the underlying physical network topology &ndash; that is, the pattern of how network routers are connected to each other &ndash; and methods to route and load balance data transmission through the network.  The project's starting point was a rigorous understanding of how to measure throughput.  The project showed that cut-based metrics (like bisection bandwidth), traditionally used as a measure of worst-case topology performance, are actually poor measures of throughput. The project instead developed a heuristic to approximate the worst case throughput of a topology.  With these basic tools in hand, the project performed the largest benchmarking study of network topologies to date, in both typical workloads and worst-case workloads.  This study concluded that a particular class of network topologies, called expander graphs, achieve highest performance at scale.  The study also showed weaknesses of other topologies with difficult traffic patterns &ndash; in particular, the Clos topologies in common use today perform relatively poorly on skewed workloads, which are common.  Once a topology is selected, how do you transmit data through it?  Data center topologies have a large number paths between any source and destination, and it is important to be able to load-balance traffic across these paths to achieve high performance.  The project developed two approaches to use all paths efficiently.  First, to assist centralized control of traffic, the project designed a clean source-routing abstraction to enable use of every available path, compactly encoded using standard software-defined networking protocols.  Second, the project developed an orthogonal approach using only local decisions without a central controller, but with a small amount of extra intelligence in network switches. This approach, referred to as "micro load balancing", was shown to substantially reduce micro-bursts of congestion, thus allowing data center networks to have higher throughput utilization.  The above techniques are effective, but they utilize novel controller or hardware designs.  Is it possible to realize the benefits of the highest-throughput topologies (i.e., expanders) using today's network hardware and protocols? To answer this, we designed a practical method of configuring routing and performed a realistic evaluation in terms of data center scale, traffic patterns, oversubscription ratios, and other factors.  The results of these experiments showed that expanders can realize 3x more throughput than an equivalent 3-tier Clos network (common for large data centers today), and 1.5x more throughput than an equivalent leaf-spine topology (common for small to medium size data centers today), for a wide range of scenarios, with only traditional protocols. Expanders also achieved lower flow completion times, more resilience to bursty load conditions like incast and outcast, and degraded more gracefully with increasing load. These results are based on extensive simulations and hardware testbed experiments.  In addition to these research results, the project produced broader impact in the form of experimental infrastructure.  Originally used in support of the above results, this infrastructure has already been employed by other research groups.  First, a topology benchmarking suite includes a library of network topologies and traffic matrices (TMs, i.e., workloads), and flow optimization software for evaluating the performance of topologies with these TMs under optimal routing. This provides an open way to benchmark new topology designs.  Second, the OCEAN testbed allows experimenters to map virtual topologies directly onto physical hardware (rather than software switches), thus providing higher performance fidelity than other emulation platforms. In addition, the project supported professional development of several undergraduate and PhD researchers, and a number of publications in top computer networking venues.       Last Modified: 05/29/2019       Submitted by: Philip Brighten Godfrey]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
