<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Machine Learning to Combat Adversarial Attacks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>104997.00</AwardTotalIntnAmount>
<AwardAmount>104997</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>In domains such as spam and fraud, adversaries actively modify their behavior to avoid being detected by a system constructed to identify attacks. For example, spammers add and remove words from their email messages in order to bypass filters, and web spammers try to deceive search engines by creating "link farms" to make a web site seem important. This project analyzes how adversaries evade classifiers and develops new algorithms, based on a novel combination of machine learning and the theory of designing actions in the face of adversaries, that are more robust. In particular, it will focus on classifiers that use data compression statistics and graph structure to make predictions. These classifiers are popular and effective in a growing number of domains, but also challenging to analyze. The analyses and algorithms developed in this project will be useful for building machine learning systems more accurate in the face of evasive adversaries. This will improve our ability to fight web spam, social network spam, online auction fraud, credit card fraud, and more. Given the high worldwide cost of cybercrime, even a small increase in accuracy could save millions of dollars per year.&lt;br/&gt;&lt;br/&gt;Most previous work on modeling adversaries in machine learning has been limited to linear classifiers with features that can be manipulated independently. These results tell us little about the vulnerabilities of widely used non-linear classifiers. Such models also ignore the relational structure necessary to identify social network spam, comment spam, fake reviews, and more. This project takes the first steps towards a much broader understanding of the weaknesses present in machine learning methods, and how best to eliminate them. Specifically, the project explores how an intelligent adversary can evade non-linear compression-based classifiers in the face of features that are highly interdependent. System designers can use these results to understand possible system vulnerabilities and intelligently choose classifiers that are less vulnerable. To learn more robust models, this project integrates a model of the adversary into learning algorithms, optimizing its performance against the adversary's best response. The work focuses on structured prediction, which can model relationships among objects, such as users in a social network, and complex adversarial actions, such as creating new accounts or buying followers. These new methods are evaluated on synthetic and real-world adversarial domains, including Twitter spam.</AbstractNarration>
<MinAmdLetterDate>08/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1451453</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Lowd</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Lowd</PI_FULL_NAME>
<EmailAddress>lowd@cs.uoregon.edu</EmailAddress>
<PI_PHON>5413465131</PI_PHON>
<NSF_ID>000553100</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Oregon Eugene</Name>
<CityName>Eugene</CityName>
<ZipCode>974035219</ZipCode>
<PhoneNumber>5413465131</PhoneNumber>
<StreetAddress>5219 UNIVERSITY OF OREGON</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079289626</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF OREGON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049793995</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Oregon Eugene]]></Name>
<CityName/>
<StateCode>OR</StateCode>
<ZipCode>974031202</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~104997</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div> <p class="vert-spacing">In many domains such as spam and fraud, adversaries actively modify their behavior to avoid being detected by a machine learning system.&nbsp; For example, spammers add and remove words from their email messages in order to bypass spam, and web spammers try to deceive search engines by creating "link farms" to make a web site seem more important.&nbsp; These attacks can drastically reduce the performance of traditional machine learning systems, which are built on the assumption that test instances will resemble training data.&nbsp; In these adversarial domains, the test instances may be maliciously constructed to defeat the very classifier designed to detect them. This project developed new methods for analyzing the vulnerability of classifiers to adaptive adversaries and laid the foundation for building more robust models from data.</p> <p class="vert-spacing">For analyzing vulnerabilities, it was already known that linear classifiers such as na&iuml;ve Bayes and logistic regression are easy to evade. This project demonstrated that compression-based classifiers, which label their inputs based on sequential patterns, are also vulnerable. In experiments with three new types of attacks, the median spam email could be disguised with just 11% additional text.</p> <p class="vert-spacing">For learning robust models, this project developed new methods for learning tractable probabilistic models and more effective filters for real-world comment spam data. Reasoning in probabilistic models is typically intractable, which makes it difficult to ensure accurate predictions, especially in the case of adversarial manipulation. This project developed new methods for learning conditional probability distributions where reasoning can always be done efficiently, and demonstrated their effectiveness on a variety of benchmark domains. As a real-world application, this project developed new models for detecting comment spam that exploit relational structure among comments with similar text or written by the same user. In future work, combining tractable models with real-world applications will provide the foundation for developing adversarially robust machine learning systems and evaluating them in practice.</p> <p class="vert-spacing">The analyses and algorithms developed in this project will be useful for making many machine learning systems more accurate against adaptive adversaries. This can improve our ability to fight web spam, social network spam, online auction fraud, credit card fraud, and more. Given the high worldwide cost of cybercrime, even a small increase in accuracy could save millions of dollars per year.</p> </div><br> <p>            Last Modified: 12/01/2016<br>      Modified by: Daniel&nbsp;Lowd</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In many domains such as spam and fraud, adversaries actively modify their behavior to avoid being detected by a machine learning system.  For example, spammers add and remove words from their email messages in order to bypass spam, and web spammers try to deceive search engines by creating "link farms" to make a web site seem more important.  These attacks can drastically reduce the performance of traditional machine learning systems, which are built on the assumption that test instances will resemble training data.  In these adversarial domains, the test instances may be maliciously constructed to defeat the very classifier designed to detect them. This project developed new methods for analyzing the vulnerability of classifiers to adaptive adversaries and laid the foundation for building more robust models from data. For analyzing vulnerabilities, it was already known that linear classifiers such as na&iuml;ve Bayes and logistic regression are easy to evade. This project demonstrated that compression-based classifiers, which label their inputs based on sequential patterns, are also vulnerable. In experiments with three new types of attacks, the median spam email could be disguised with just 11% additional text. For learning robust models, this project developed new methods for learning tractable probabilistic models and more effective filters for real-world comment spam data. Reasoning in probabilistic models is typically intractable, which makes it difficult to ensure accurate predictions, especially in the case of adversarial manipulation. This project developed new methods for learning conditional probability distributions where reasoning can always be done efficiently, and demonstrated their effectiveness on a variety of benchmark domains. As a real-world application, this project developed new models for detecting comment spam that exploit relational structure among comments with similar text or written by the same user. In future work, combining tractable models with real-world applications will provide the foundation for developing adversarially robust machine learning systems and evaluating them in practice. The analyses and algorithms developed in this project will be useful for making many machine learning systems more accurate against adaptive adversaries. This can improve our ability to fight web spam, social network spam, online auction fraud, credit card fraud, and more. Given the high worldwide cost of cybercrime, even a small increase in accuracy could save millions of dollars per year.        Last Modified: 12/01/2016       Submitted by: Daniel Lowd]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
