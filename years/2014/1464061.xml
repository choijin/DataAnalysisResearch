<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Scalable Webcam Eyetracking by Learning from User Interactions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2015</AwardEffectiveDate>
<AwardExpirationDate>03/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Eye tracking technology is very useful for a variety of applications including human-computer interaction studies, usability testing, medical research, and experiments in psychology, to name just a few.  But the devices are highly specialized and costly equipment that is difficult to calibrate and use, so they remain available for the most part in the lab only.  In this project the PI will build on his prior work to establish a research program to investigate a new approach to eye tracking based on the webcams commonly present in today's laptops and mobile devices, with the goal of making the technology viable for a broader range of applications as part of the natural experience of everyday users and so no longer restricted to laboratories and highly controlled studies.  Of course, webcams are less accurate than specialized eye tracking equipment for estimating where a user is looking on the screen.  The PI's approach to overcoming this drawback is to improve the accuracy by exploiting user interactions to continuously calibrate the webcam-as-eye tracker during regular usage, and to do this online without the need to install additional software.  Project outcomes will ultimately include a real-time online eye tracking system using the typical webcam available in laptops and mobile devices, along with an evaluation of its performance.  The PI will also conduct research into how user interactions such as cursor clicks and text entry and touches can be used to automatically train the eye tracking algorithms.  The new technology will democratize eye tracking, releasing it from the confines of the lab; the PI will disseminate source code along with eye tracking demos to allow other researchers and developers to apply his technology in their work.&lt;br/&gt;&lt;br/&gt;The PI's prior work has shown that when a user clicks on a web page, they will first look where they intend to click.  Furthermore, psychology studies have shown that the eye is likely to be 2-4 characters to the right of the last typed character on the screen.  Webcam images during these user interactions can be collected by the website to use as cues to what a given user's pupil looks like when s/he is interacting with a particular location.  Future observations of the pupil can then be matched to past instances with similar-looking pupils as the system collects mappings of pupil features to eye-gaze locations on the page, allowing the model to infer the eye-gaze location even when the user is not interacting.  The pupil data can be collected during the entire time that a user interacts with a website and without disrupting the user experience, including at the beginning of a computer usage session to provide model training data that better matches the local environment in terms of ambient lighting, user sitting position, and background environment.  By enabling eye tracking to be accessible from a typical web browser and by continuously improving the tracking accuracy as a user visits a website, eye tracking becomes a reality for many potential applications such as large-scale naturalistic user studies, online gaming, or enabling people to perform hands-free navigation of websites.  This eye tracking procedure is opt-in as browsers request access to the webcam, and the website is able to capture this data if the user agrees.</AbstractNarration>
<MinAmdLetterDate>03/06/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/18/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1464061</AwardID>
<Investigator>
<FirstName>Jeff</FirstName>
<LastName>Huang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeff Huang</PI_FULL_NAME>
<EmailAddress>Jeff_Huang@brown.edu</EmailAddress>
<PI_PHON>4018632777</PI_PHON>
<NSF_ID>000651625</NSF_ID>
<StartDate>03/06/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brown University</Name>
<CityName>Providence</CityName>
<ZipCode>029129002</ZipCode>
<PhoneNumber>4018632777</PhoneNumber>
<StreetAddress>BOX 1929</StreetAddress>
<StreetAddress2><![CDATA[350 Eddy Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<StateCode>RI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>RI01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001785542</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BROWN UNIVERSITY IN PROVIDENCE IN THE STATE OF RHODE ISLAND AND PROVIDENCE PLANTATIONS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001785542</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>Providence</CityName>
<StateCode>RI</StateCode>
<ZipCode>029129093</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Projects]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>RI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This research investigates how to infer user attention using subtle forms of interaction, especially applied to remote usability testing to improve interfaces. While eye tracking is a nearly ubiquitous form of usability testing, typical equipment costs tens of thousands of dollars, requires scheduled time and travel for both the experimenter and participant, and sessions can only occur one at a time. My students and I developed WebGazer.js [https://webgazer.cs.brown.edu/], an eye tracking library that uses common webcams (like those already on a laptop) to infer the eye-gaze locations of web visitors on a web page in real time. The eye tracking model we developed will self-calibrate as users who are web visitors interact with the web page by training a mapping between the features of the eye and positions on the screen.</p> <p>WebGazer observes user interactions on web pages to continuously calibrate the eye tracker during regular activity: for example, when a user clicks on a page they first look where they intend to click, or they look a bit ahead of the caret during touch typing. Webcam images during these user interactions are collected to use as cues for what the user's pupil looks like when that user interacts with a particular location. So future observations of the pupil can be matched to past instances with similar-looking pupils as WebGazer collects mappings of pupil features to eye-gaze locations on the page, allowing it to infer the eye-gaze location even when the user is not interacting. We built WebGazer purely in JavaScript without any need to install any software to easily scale and democratize eye tracking; anyone can then use their eyes to interact with web pages without having to use their hands. Meanwhile, usability testers and web designers can identify which items on the page are attended, for how long, and in what order, to improve the user experience. The library runs entirely in the client browser, so to preserve user privacy, no video data needs to be sent to a server.</p> <p>The website and GitHub repository for the WebGazer software has had over 250,000 unique visitors; over 2,200 people bookmarked the source code, 265 forks of the software exist (people who build on top of the released software). We have open sourced the software and there are startup companies who use it as their core technology to enable remote usability testing; for example, RealEye is a startup we recently discovered using WebGazer claiming "Identify exactly what people focus on with the power of webcam eye-tracking. Test your design hypothesis and make decisions based on numbers not on what looks better" and reports having 500 registered companies and 15,000 testers. Additionally, we have shown that WebGazer can be used to replicate seminal papers in information retrieval, showing similar results but through online remote studies instead of in-person eye tracking studies. Because eye tracking happens differently each time a user sits down, we have released video, screen, and interaction data for over 50 participants for future researchers to use.</p> <div class="page" title="Page 2"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>We have conducted multiple evaluations of our research, which is published in peer-reviewed computer science conferences (IJCAI 2016, CHIIR 2017, and ETRA 2018).&nbsp;</span>The first publication at IJCAI already has 65 citations, many of which are people who have tested WebGazer for themselves for their own applications.&nbsp;The&nbsp;paper at CHIIR was nominated for a best paper award. Our research has been reported in the popular press, such as The Register and PC World.</p> <p>Other disciplines like psychology and cognitive science have started assessing WebGazer for their experimental paradigms. In total, 4 graduate students and 2 undergraduate students worked on this research at Brown University. Three of these students were women, and underrepresented group in computer science. Two of the students working on this project are now Ph.D. students at computer science programs elsewhere.</p> </div> </div> </div> </div><br> <p>            Last Modified: 06/30/2019<br>      Modified by: Jeff&nbsp;Huang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This research investigates how to infer user attention using subtle forms of interaction, especially applied to remote usability testing to improve interfaces. While eye tracking is a nearly ubiquitous form of usability testing, typical equipment costs tens of thousands of dollars, requires scheduled time and travel for both the experimenter and participant, and sessions can only occur one at a time. My students and I developed WebGazer.js [https://webgazer.cs.brown.edu/], an eye tracking library that uses common webcams (like those already on a laptop) to infer the eye-gaze locations of web visitors on a web page in real time. The eye tracking model we developed will self-calibrate as users who are web visitors interact with the web page by training a mapping between the features of the eye and positions on the screen.  WebGazer observes user interactions on web pages to continuously calibrate the eye tracker during regular activity: for example, when a user clicks on a page they first look where they intend to click, or they look a bit ahead of the caret during touch typing. Webcam images during these user interactions are collected to use as cues for what the user's pupil looks like when that user interacts with a particular location. So future observations of the pupil can be matched to past instances with similar-looking pupils as WebGazer collects mappings of pupil features to eye-gaze locations on the page, allowing it to infer the eye-gaze location even when the user is not interacting. We built WebGazer purely in JavaScript without any need to install any software to easily scale and democratize eye tracking; anyone can then use their eyes to interact with web pages without having to use their hands. Meanwhile, usability testers and web designers can identify which items on the page are attended, for how long, and in what order, to improve the user experience. The library runs entirely in the client browser, so to preserve user privacy, no video data needs to be sent to a server.  The website and GitHub repository for the WebGazer software has had over 250,000 unique visitors; over 2,200 people bookmarked the source code, 265 forks of the software exist (people who build on top of the released software). We have open sourced the software and there are startup companies who use it as their core technology to enable remote usability testing; for example, RealEye is a startup we recently discovered using WebGazer claiming "Identify exactly what people focus on with the power of webcam eye-tracking. Test your design hypothesis and make decisions based on numbers not on what looks better" and reports having 500 registered companies and 15,000 testers. Additionally, we have shown that WebGazer can be used to replicate seminal papers in information retrieval, showing similar results but through online remote studies instead of in-person eye tracking studies. Because eye tracking happens differently each time a user sits down, we have released video, screen, and interaction data for over 50 participants for future researchers to use.      We have conducted multiple evaluations of our research, which is published in peer-reviewed computer science conferences (IJCAI 2016, CHIIR 2017, and ETRA 2018). The first publication at IJCAI already has 65 citations, many of which are people who have tested WebGazer for themselves for their own applications. The paper at CHIIR was nominated for a best paper award. Our research has been reported in the popular press, such as The Register and PC World.  Other disciplines like psychology and cognitive science have started assessing WebGazer for their experimental paradigms. In total, 4 graduate students and 2 undergraduate students worked on this research at Brown University. Three of these students were women, and underrepresented group in computer science. Two of the students working on this project are now Ph.D. students at computer science programs elsewhere.           Last Modified: 06/30/2019       Submitted by: Jeff Huang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
