<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I/UCRC FRP:  Collaborative Research: Autonomous Perception and Manipulation in Search and Rescue</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>69961.00</AwardTotalIntnAmount>
<AwardAmount>69961</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Thyagarajan Nandagopal</SignBlockName>
<PO_EMAI>tnandago@nsf.gov</PO_EMAI>
<PO_PHON>7032924550</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project wants to capitalize on a team of robots that move in order to better sense the environment and then perform basic manipulation tasks. The vision of the project is to integrate robots easily, provide human-team interfaces, and develop manipulation algorithms. The research will involve the development of perception strategies and manipulation schemes that will allow operation of the robot teams in real-world environments during search and rescue missions. In addition, this research will involve working in cluttered scenes where the lighting conditions may not be ideal. The project addresses: (i) Research on the novel problem of robotic perception and manipulation of target objects that interact with other objects as an integral part of the environment, which cannot be fully isolated in views and in physical arrangements before being manipulated; (ii) An appearance-based approach for recognition and pose estimation of 3D objects in cluttered scenes from a single view; (iii) Development of a measure of scene recognizability from each viewpoint to evaluate how accurately partially-occluded objects are recognized and how well their poses are estimated; (iv) Creation of solutions for disassembly analysis of 3D structures, extending our preliminary analysis of 2D structures; (v) Development of grasping based on the results of perception and with the aid of stability analysis of the arrangement of the objects and their interaction with the environment and with one another; and (vi) Experimental validation of the system in real-world settings, in close consultation with our industrial partners.&lt;br/&gt;&lt;br/&gt;This project will allow the creation of manipulation capabilities along with perception schemes to facilitate the development of a multi-robot team for search and rescue missions. The impacts of the project include: (i) Expansion of the annual robot summer camp to include robot-team activities with the objective of attracting middle-schoolers from under-represented groups to computer science/electrical engineering; (ii) Integration of the activities with first responders through the UPenn and UNC Charlotte collaborations; (ii) Experimental validation in SAFL at UMN and the Disaster City in TX; (iii) Offering project themes to REU undergraduates or to the UROP program; (iv) Outreach programs that involve demonstrations to local K-12 institutions; and (v) Inclusion of the project theme to the regular curricula.</AbstractNarration>
<MinAmdLetterDate>08/07/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1432957</AwardID>
<Investigator>
<FirstName>Nikolaos</FirstName>
<LastName>Papanikolopoulos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nikolaos Papanikolopoulos</PI_FULL_NAME>
<EmailAddress>npapas@cs.umn.edu</EmailAddress>
<PI_PHON>6126250163</PI_PHON>
<NSF_ID>000205563</NSF_ID>
<StartDate>08/07/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 Union Street SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramReference>
<Code>5761</Code>
<Text>INDUSTRY/UNIV COOP RES CENTERS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~69961</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>The role of the UMN team was to provide a robot with many sensors that will help in the grasping and manipulation tasks (done by UNCC and UPenn). The name of the robot is the MicroVision and our goal was to&nbsp;create a user friendly robotic research platform. The platform must be easy to operate, quick to disassemble and assemble, and allow for plug and play sensors. Furthermore, the platform has to provide a software framework which allows researchers access to the open source community for developing robotic applications such as grasping. The intended use of a robot necessarily determines the mechanical design. We wanted the frame of the robot to be as strong as possible yet light in weight. The supporting structure of the robot must be able to carry a modest payload of sensor equipment, manipulators, and their respective payloads. The MicroVision robot consists of a one-piece transparent cylindrical frame constructed of polycarbonate, a durable plastic resistant to breakage. The design of the MicroVision chassis was done with convenient accessibility in mind. Several access holes in the frame allow for unobstructed sensor placement and USB connectivity. The frame not only serves as a supporting structure, but also houses the electronic hardware of the robot. Differential steering is employed to move the MicroVision robot about its environment. A tail at the rear of the robot provides structural support as well as allowing the robot to change its pitch angle. The tail is constructed of aluminum with a spherical skid ball at the ground contact end. Movement of the robot along its lateral axis is accomplished by a linear actuator. The hardware components of the MicroVision are located inside its cylindrical shell frame. Power is provided by an 11.1 V Lithium Ion battery with up to 45 minutes of operational time. A power management board performs the duty of providing a steady 5 V source to the robot&rsquo;s CPU, microcontroller, and sensors. The main processing component of the robot is the PC104. The sensors of a robot allow it to make decisions based on feedback from the surrounding environment. Robotic sensors are classified into two types: proprioceptive and exteroceptive. Proprioceptive sensors measure the internal state of the robot which may include the voltage of key components, the value of a wheel encoder, etc. Exteroceptive sensors generate information about the external environment in terms of distance to an object, interaction forces, etc. The MicroVision uses both of the above classes of sensors. The on-board sensors include wheel encoders, a laser range finder, RGB cameras, and a depth camera (used to provide the grasping info). The wheel encoders are built into the motors and the counts per wheel revolution can be controlled from the microcontroller. This allows the operator to specify exact wheel counts which is useful for having the robot move to a predetermined position and orientation. The robot is equipped with a Hokuyo URG-04LX laser range finder. A uEye camera developed by IDS Imaging Development Systems is the primary camera on the MicroVision. The camera is powered by the USB port, has a resolution of 640x480, and uses a C-mount lens. The uEye camera interfaces with a full featured API for developing computer vision applications.</span></p><br> <p>            Last Modified: 10/06/2016<br>      Modified by: Nikolaos&nbsp;Papanikolopoulos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The role of the UMN team was to provide a robot with many sensors that will help in the grasping and manipulation tasks (done by UNCC and UPenn). The name of the robot is the MicroVision and our goal was to create a user friendly robotic research platform. The platform must be easy to operate, quick to disassemble and assemble, and allow for plug and play sensors. Furthermore, the platform has to provide a software framework which allows researchers access to the open source community for developing robotic applications such as grasping. The intended use of a robot necessarily determines the mechanical design. We wanted the frame of the robot to be as strong as possible yet light in weight. The supporting structure of the robot must be able to carry a modest payload of sensor equipment, manipulators, and their respective payloads. The MicroVision robot consists of a one-piece transparent cylindrical frame constructed of polycarbonate, a durable plastic resistant to breakage. The design of the MicroVision chassis was done with convenient accessibility in mind. Several access holes in the frame allow for unobstructed sensor placement and USB connectivity. The frame not only serves as a supporting structure, but also houses the electronic hardware of the robot. Differential steering is employed to move the MicroVision robot about its environment. A tail at the rear of the robot provides structural support as well as allowing the robot to change its pitch angle. The tail is constructed of aluminum with a spherical skid ball at the ground contact end. Movement of the robot along its lateral axis is accomplished by a linear actuator. The hardware components of the MicroVision are located inside its cylindrical shell frame. Power is provided by an 11.1 V Lithium Ion battery with up to 45 minutes of operational time. A power management board performs the duty of providing a steady 5 V source to the robot?s CPU, microcontroller, and sensors. The main processing component of the robot is the PC104. The sensors of a robot allow it to make decisions based on feedback from the surrounding environment. Robotic sensors are classified into two types: proprioceptive and exteroceptive. Proprioceptive sensors measure the internal state of the robot which may include the voltage of key components, the value of a wheel encoder, etc. Exteroceptive sensors generate information about the external environment in terms of distance to an object, interaction forces, etc. The MicroVision uses both of the above classes of sensors. The on-board sensors include wheel encoders, a laser range finder, RGB cameras, and a depth camera (used to provide the grasping info). The wheel encoders are built into the motors and the counts per wheel revolution can be controlled from the microcontroller. This allows the operator to specify exact wheel counts which is useful for having the robot move to a predetermined position and orientation. The robot is equipped with a Hokuyo URG-04LX laser range finder. A uEye camera developed by IDS Imaging Development Systems is the primary camera on the MicroVision. The camera is powered by the USB port, has a resolution of 640x480, and uses a C-mount lens. The uEye camera interfaces with a full featured API for developing computer vision applications.       Last Modified: 10/06/2016       Submitted by: Nikolaos Papanikolopoulos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
