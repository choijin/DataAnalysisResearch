<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Collaborative Research: Real-Time Captioning by Groups of Non-Experts for Deaf and Hard of Hearing Students</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/31/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>317194.00</AwardTotalIntnAmount>
<AwardAmount>317194</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many deaf and hard of hearing students use real-time captioning to participate in education.  Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute.   But professional captionists are expensive and must be arranged in advance in blocks of at least an hour.  Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms.  In this collaborative effort involving the University of Rochester and Rochester Institute of Technology, the PIs will address these issues by blending human- and machine-powered captioning to produce captions on demand, in real time, for low cost.  The PIs' approach is for multiple non-experts and ASR to collectively caption speech in under 5 seconds, with the help of interfaces which encourage quick, incomplete captioning of live audio.  Because non-experts cannot keep up with natural speaking rates, new algorithms will merge incomplete captions in real time. (While the sequence alignment problem can be solved exactly with dynamic programming, existing approaches are too slow, are not robust to input error, and do not incorporate natural language semantics.)  Systematically varying audio saliency will encourage complete coverage of speech.  Non-expert captions will train ASR engines in real time, so that ASR may improve during a lecture. (Traditional approaches for ASR training assume that training occurs offline.)  The quikCaption mobile application will embody these ideas and will be iteratively designed with deaf and hard of hearing students at the National Technical Institute of the Deaf (NTID) via design sessions, lab studies and in-class deployments.  Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers.  They may be local (in the classroom) or remote.  Captionists may have experience from prior quikCaption sessions, or novice crowd workers recruited on demand from existing marketplaces (e.g., Mechanical Turk).  A flexible worker pool will allow real-time captions to be available on demand at low cost and for only as long as needed.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This research will dramatically improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged.  Real-time captioning will also be useful in other settings such as school programs, artistic performances, and political events.  Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population; hearing people may benefit because captioning is a first step in automatic translation of aural speech.  The algorithms developed as part of this project for real-time merging of incomplete natural language will likely be adaptable for other applications such as collaborative translation or communication over noisy mediums.</AbstractNarration>
<MinAmdLetterDate>08/04/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/04/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1446129</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Bigham</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey Bigham</PI_FULL_NAME>
<EmailAddress>jbigham@cmu.edu</EmailAddress>
<PI_PHON>5852754031</PI_PHON>
<NSF_ID>000541549</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~39732</FUND_OBLG>
<FUND_OBLG>2013~277462</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many deaf and hard of hearing students use real-time captioning to participate in education. Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute. Professional captionists are expensive (up to $200 USD per hour) and must be arranged in advance in blocks of at least an hour. Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms. This project developed techniques that blend human- and machine-powered captioning to produce captions in real-time, on-demand, for low cost.<br /><br />Our approach is for multiple non-experts and ASR to collectively caption speech in real-time (under 5 seconds). Interfaces encourage quick, incomplete captioning of live audio. Because non-experts cannot keep up with natural speaking rates, new algorithms merge incomplete captions in real-time. Systematically varying audio saliency encourages complete coverage of speech. Non-expert captions train ASR engines in real-time, so that ASR may improve during a lecture. Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers. They may be local (in the classroom) or remote. Captionists may have experience from prior sessions, or novice crowd workers recruited on-demand from existing marketplaces, e.g., Mechanical Turk. A flexible worker pool will allow real-time captions to be available on-demand for low cost for only as long as needed. <br /><br />Intellectual Merit: Real-time captioning previously involved either a single professional captionist or automatic speech recognition, which means that real-time captions are costly, unavailable, or poor quality. Our Scribe system was made possible through innovations in several complementary areas: (i) designing interfaces that encourage incomplete but real-time input and global coverage across workers, (ii) developing algorithms able to merge incomplete captions in real-time and train ASR in an online fashion, and (iii) understanding how deaf and hard of hearing people want to engage with on-demand real-time captioning in the classroom and their everyday lives. While the sequence alignment problem can be solved exactly with dynamic programming, prior approaches did not work in real-time, are not robust to input error, and did not incorporate natural language semantics. Prior approaches for training automatic speech recognition assumed that training occurs offline; this project developed methods for the real-time input of humans to immediately improve caption quality.<br /><br />Broader Impacts: This project may improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged. Real-time captioning may also be used in other settings as well, e.g. political events, school programs, artistic performances. Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population. Hearing people may benefit because captioning is a first step in automatic translation of aural speech. Algorithms for real-time merging of incomplete natural language may be adapted for other applications, e.g. for collaborative translation or communication over noisy mediums. Educational activities have engaged undergraduate and graduate students.</p><br> <p>            Last Modified: 11/27/2016<br>      Modified by: Jeffrey&nbsp;Bigham</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many deaf and hard of hearing students use real-time captioning to participate in education. Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute. Professional captionists are expensive (up to $200 USD per hour) and must be arranged in advance in blocks of at least an hour. Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms. This project developed techniques that blend human- and machine-powered captioning to produce captions in real-time, on-demand, for low cost.  Our approach is for multiple non-experts and ASR to collectively caption speech in real-time (under 5 seconds). Interfaces encourage quick, incomplete captioning of live audio. Because non-experts cannot keep up with natural speaking rates, new algorithms merge incomplete captions in real-time. Systematically varying audio saliency encourages complete coverage of speech. Non-expert captions train ASR engines in real-time, so that ASR may improve during a lecture. Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers. They may be local (in the classroom) or remote. Captionists may have experience from prior sessions, or novice crowd workers recruited on-demand from existing marketplaces, e.g., Mechanical Turk. A flexible worker pool will allow real-time captions to be available on-demand for low cost for only as long as needed.   Intellectual Merit: Real-time captioning previously involved either a single professional captionist or automatic speech recognition, which means that real-time captions are costly, unavailable, or poor quality. Our Scribe system was made possible through innovations in several complementary areas: (i) designing interfaces that encourage incomplete but real-time input and global coverage across workers, (ii) developing algorithms able to merge incomplete captions in real-time and train ASR in an online fashion, and (iii) understanding how deaf and hard of hearing people want to engage with on-demand real-time captioning in the classroom and their everyday lives. While the sequence alignment problem can be solved exactly with dynamic programming, prior approaches did not work in real-time, are not robust to input error, and did not incorporate natural language semantics. Prior approaches for training automatic speech recognition assumed that training occurs offline; this project developed methods for the real-time input of humans to immediately improve caption quality.  Broader Impacts: This project may improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged. Real-time captioning may also be used in other settings as well, e.g. political events, school programs, artistic performances. Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population. Hearing people may benefit because captioning is a first step in automatic translation of aural speech. Algorithms for real-time merging of incomplete natural language may be adapted for other applications, e.g. for collaborative translation or communication over noisy mediums. Educational activities have engaged undergraduate and graduate students.       Last Modified: 11/27/2016       Submitted by: Jeffrey Bigham]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
