<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Medium: Stochastic Program Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>798159.00</AwardTotalIntnAmount>
<AwardAmount>798159</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Title: SHF: Medium: Stochastic Program Optimization&lt;br/&gt;&lt;br/&gt;Virtually all software today is written by a combination of man and machine.  Programmers write programs in programming languages that cannot be executed directly---these languages are designed to make the task of programming easier but cannot actually be run on a computer without a second step, where the human-written program is translated into a language that the machine can execute.  Many changes to the program are made during this translation and especially important are optimizations that improve the performance of the final program on the hardware.&lt;br/&gt;&lt;br/&gt;Optimizing programs is a very complex process as there are many tradeoffs to be made in how to best use the limited resources of the target architecture.  Traditionally this large problem is solved by breaking it into many small sub-problems, each of which focuses on a particular class of optimizations.  In this project a very different alternative will be explored, using randomized search techniques to simultaneously consider all of the factors that go into producing the fastest possible code.  Preliminary results have shown this approach can match or outperform production compilers for important short kernels; a focus of this research will be to try to scale these results up to full programs.  The intellectual merit of this work is to reconsider how to perform compiler optimizations in light of the dramatic advances in computational power over the last several decades, a period in which the basic structure of compilers has hardly changed. The broader impact will be to demonstrate novel methods based on stochastic search, for producing, with a significant degree of automation, consistently much faster low-level code than today?s compilers. Thus, instead of compiler projects requiring years to produce reliable and high performance code for new architectures, stochastic optimizers that perform even better and with higher assurance can be built much more quickly by smaller teams of people. The techniques that will be developed will also be widely disseminated through massively online course portals.</AbstractNarration>
<MinAmdLetterDate>07/09/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409813</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Aiken</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander Aiken</PI_FULL_NAME>
<EmailAddress>aiken@cs.stanford.edu</EmailAddress>
<PI_PHON>6507253359</PI_PHON>
<NSF_ID>000281933</NSF_ID>
<StartDate>07/09/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>943054100</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~798159</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>Programs are notoriously difficult to manipulate and transform; it is easy to turn a working program into one that doesn't work by introducing errors that are difficult to detect, and so systems that seek to automatically improve programs (by, for example, making them run faster) are generally very conservative, limited to a set of well-understood changes.</p> <p>In this project we explored using very aggressive techniques to transform programs, in particular by making arbitrary random changes and then checking after the fact whether there was any improvement or not.&nbsp; Most random changes will result in a worse or simply non-working program, but very rarely a change will be beneficial and make the program run faster without sacrificing correctness.&nbsp; By repeatedly making random changes up to hundreds of millions of times and retaining the changes that were improvements, much higher quality programs can be found than by using traditional methods.&nbsp; It is important to understand that this approach only scales to relatively small amounts of code (it can improve a program a few hundred lines long), but even at these small scales the technique has potential utility in real applications.</p> <p>Two key things are needed to make this approach work, however.&nbsp; The first is be able to actually make enough proposals to find improvements.&nbsp; This requires keeping the rate of exploration high, and techniques for achieving that were part of the research effort.&nbsp; The second essential component is the ability to verify correctness.&nbsp; We developed "black box" techniques that assumed nothing except that it was given two programs to be proven equal (or not).&nbsp; The core issue faced was identifying which parts of the programs corresponded and then deriving from that a set of smaller proofs that, if succesfully carried out automatically, could prove the two programs were equal.</p> <p>Applications of these techniques are numerous.&nbsp; For example, our verification techniques are immediately applicable to existing compilers to increase confidence in their output (by, for example, checking optimized and unoptimized versions of the same program for equivalence).&nbsp; The search techniques for optimization have also been applied successfully in deep learning, with very different kinds of programs but a similar need to obtain the best performance.&nbsp; Finally, we also used the techniques for synthesis where we were given only a description of the program's behavior in the form of input-output examples; for example, we used these techniques to synthesize the most complete formal specification of the x86 instruction set to that point.</p><br> <p>            Last Modified: 07/08/2019<br>      Modified by: Alexander&nbsp;Aiken</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Programs are notoriously difficult to manipulate and transform; it is easy to turn a working program into one that doesn't work by introducing errors that are difficult to detect, and so systems that seek to automatically improve programs (by, for example, making them run faster) are generally very conservative, limited to a set of well-understood changes.  In this project we explored using very aggressive techniques to transform programs, in particular by making arbitrary random changes and then checking after the fact whether there was any improvement or not.  Most random changes will result in a worse or simply non-working program, but very rarely a change will be beneficial and make the program run faster without sacrificing correctness.  By repeatedly making random changes up to hundreds of millions of times and retaining the changes that were improvements, much higher quality programs can be found than by using traditional methods.  It is important to understand that this approach only scales to relatively small amounts of code (it can improve a program a few hundred lines long), but even at these small scales the technique has potential utility in real applications.  Two key things are needed to make this approach work, however.  The first is be able to actually make enough proposals to find improvements.  This requires keeping the rate of exploration high, and techniques for achieving that were part of the research effort.  The second essential component is the ability to verify correctness.  We developed "black box" techniques that assumed nothing except that it was given two programs to be proven equal (or not).  The core issue faced was identifying which parts of the programs corresponded and then deriving from that a set of smaller proofs that, if succesfully carried out automatically, could prove the two programs were equal.  Applications of these techniques are numerous.  For example, our verification techniques are immediately applicable to existing compilers to increase confidence in their output (by, for example, checking optimized and unoptimized versions of the same program for equivalence).  The search techniques for optimization have also been applied successfully in deep learning, with very different kinds of programs but a similar need to obtain the best performance.  Finally, we also used the techniques for synthesis where we were given only a description of the program's behavior in the form of input-output examples; for example, we used these techniques to synthesize the most complete formal specification of the x86 instruction set to that point.       Last Modified: 07/08/2019       Submitted by: Alexander Aiken]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
