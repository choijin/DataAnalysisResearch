<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Enabling Behavior Sensing via the Cloud and its Application to Public Speaking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2015</AwardEffectiveDate>
<AwardExpirationDate>03/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>173501.00</AwardTotalIntnAmount>
<AwardAmount>205501</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Public speaking is a task that people often rank as their top fear; one consequence is that even after repeatedly practicing a presentation many find they end up speaking too hastily when standing before the audience.  People often desire to improve their public speaking skills, but lack of resources and social stigma may impede their ability to obtain the personalized training they seek.  The PI's objective in this project is to build on his prior work and establish a research program to develop a ubiquitously available (Cloud based) automated social sensing framework that can recognize and interpret human nonverbal data (including facial expressions, tone of voice, body language, etc.), and then present constructive feedback to its users where they want and when they want.  Modeling of the full range of human nonverbal behavior remains a challenging endeavor.  Using the 43 muscles in our face, we can produce 10,000 unique combinations of facial expressions; modalities such as vocal tone, body language, and elements of physiology add to the complexity.  While computers can now recognize basic expressions such as smiling and frowning, the automated interpretation of an individual's intent remains an active area of exploration (e.g., a smiling customer does not necessarily indicate that s/he is satisfied).  This research represents a step towards developing algorithms and implementing a practical framework that can capture and interpret nonverbal data while providing meaningful feedback in the context of public speaking.  Project outcomes ultimately will transform the way social skills are adapted and learned, which will have a broad impact on people with social difficulties (e.g., those with Asperger's syndrome).  &lt;br/&gt;&lt;br/&gt;Human nonverbal behaviors can be subtle, are often confusing, and may even appear contradictory.  While computer algorithms are more reliable than people at sensing subtle human behavior objectively and consistently, human intelligence is currently far superior at interpreting contextual behavior.  This research adopts an approach that couples computer algorithms with human intelligence towards automated sensing and interpretation of nonverbal behavior in nearly real time.  The PI's approach is to develop a robust and scalable Web-based sensing framework that will automatically capture and analyze an individual?s behavior by exploiting the Cloud infrastructure, without requiring any major computational resources from the end-user.  The work will include three phases: development of a Cloud-enabled sensing platform for automated recognition of nonverbal behavior; development of algorithms for combining the behavioral data with human judgment using the so-called wisdom of the crowd to generate meaningful insights, interpretations, and social recommendations; and running user centric iterative studies to validate the framework for the general public as well as practitioners.  The work will also lead to core contributions in designing computer interfaces.  And since behavioral modeling methods typically assume a large amount of naturalistic data, preferably collected in the wild; it is therefore noteworthy that the PI's sensing framework has the potential to collect one of the largest naturalistic nonverbal datasets.</AbstractNarration>
<MinAmdLetterDate>03/06/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1464162</AwardID>
<Investigator>
<FirstName>Mohammed</FirstName>
<LastName>Hoque</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mohammed E Hoque</PI_FULL_NAME>
<EmailAddress>mehoque@cs.rochester.edu</EmailAddress>
<PI_PHON>5852754031</PI_PHON>
<NSF_ID>000654070</NSF_ID>
<StartDate>03/06/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041294109</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ROCHESTER</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041294109</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Rochester]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>146270140</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~101559</FUND_OBLG>
<FUND_OBLG>2016~103942</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To develop a ubiquitously available social sensing framework that can recognize and interpret human nonverbal data (e.g., facial expressions, tone of voice, body language) and then present constructive feedback to its users in the context of public speaking.&nbsp;</p> <p>To do this we developed a platform called ROC Speak, a browser-based system that allows people to start improving their social skills without stigma, wherever they want, whenever they want. The system has been deployed online at <a href="https://rocspeak.com">https://rocspeak.com</a>. More than 30,000 people have used it and donated their data, allowing us to collect one of the largest spontaneous datasets involving public speaking. We ran three independent, longitudinal experiments with online workers, providing quantitative evidence that people are indeed able to improve and retain their interview skills by practicing with the ROC Speak system.</p> <p>In addition to automated feedback on the quality of smile characteristics, intonation, filler word usage, and gestures, ROC Speak allows participants to provide constructive and positive feedback to one another&mdash;in effect, creating a human-machine symbiosis. If each participant is a node, and their interactions with one another are edges, we can view the entire framework as a network. As a result, we reframed the interaction among the participants with written comments as a network where the participants are edges and their interaction with other participants at edges. To further untangle why some people are improving their skills via the ROC Speak framework, we used tools from the emerging field of graph signal processing (gsp).</p> <p>We studied the performance ratings of the participants a graph signals atop underlying interaction topologies.&nbsp;GSP enjoys a distinction from Social Network Analysis in that the latter is concerned primarily with the connection structures of graphs, while the former studies signals on top of graphs. We study the performance ratings of the participants as graph signals atop underlying interaction topologies. Total variation analysis of the graph signals show that the participants&rsquo; rating differences decrease with time (slope = &minus;0.04, p &lt; 0.01), while average ratings increase (slope = 0.07, p &lt; 0.05)&mdash;thereby gradually building up the ratings towards community-wide homogeneity. We provide evidence for peer-influence through a prediction formulation. Our consensus-based prediction model outperforms baseline network-agnostic regression models by about 23% in predicting performance ratings. This in turn shows that participants&rsquo; ratings are affected by their peers&rsquo; ratings and the associated interaction patterns, corroborating previous findings. Then, we formulate a consensus-based diffusion model that captures these observations of peer influence from our analyses.</p> <p>We anticipate that the findings generated through this grant will open up future avenues for a broader exploration of peer-influenced skill development mechanisms, and potentially help design innovative interventions in small-groups to maximize peer-effects.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/23/2018<br>      Modified by: Mohammed&nbsp;E&nbsp;Hoque</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1464162/1464162_10352901_1537744629713_FeedbackPage--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1464162/1464162_10352901_1537744629713_FeedbackPage--rgov-800width.jpg" title="ROC Speak framework"><img src="/por/images/Reports/POR/2018/1464162/1464162_10352901_1537744629713_FeedbackPage--rgov-66x44.jpg" alt="ROC Speak framework"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our feedback page allows users to review their speech and read a variety of feedback.</div> <div class="imageCredit">R Zhao, V. Li, H. Barabosa, G. Ghoshal, M. E. Hoque,  A Semi-Automated, Collaborative, & Online Training Module for Communication Skills, PACM on Interactive, Mobile, Warble, and Ubiquitous Computing (IMWUT) (to be presented at UbiComp 2017).</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Mohammed&nbsp;E&nbsp;Hoque</div> <div class="imageTitle">ROC Speak framework</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To develop a ubiquitously available social sensing framework that can recognize and interpret human nonverbal data (e.g., facial expressions, tone of voice, body language) and then present constructive feedback to its users in the context of public speaking.   To do this we developed a platform called ROC Speak, a browser-based system that allows people to start improving their social skills without stigma, wherever they want, whenever they want. The system has been deployed online at https://rocspeak.com. More than 30,000 people have used it and donated their data, allowing us to collect one of the largest spontaneous datasets involving public speaking. We ran three independent, longitudinal experiments with online workers, providing quantitative evidence that people are indeed able to improve and retain their interview skills by practicing with the ROC Speak system.  In addition to automated feedback on the quality of smile characteristics, intonation, filler word usage, and gestures, ROC Speak allows participants to provide constructive and positive feedback to one another&mdash;in effect, creating a human-machine symbiosis. If each participant is a node, and their interactions with one another are edges, we can view the entire framework as a network. As a result, we reframed the interaction among the participants with written comments as a network where the participants are edges and their interaction with other participants at edges. To further untangle why some people are improving their skills via the ROC Speak framework, we used tools from the emerging field of graph signal processing (gsp).  We studied the performance ratings of the participants a graph signals atop underlying interaction topologies. GSP enjoys a distinction from Social Network Analysis in that the latter is concerned primarily with the connection structures of graphs, while the former studies signals on top of graphs. We study the performance ratings of the participants as graph signals atop underlying interaction topologies. Total variation analysis of the graph signals show that the participants? rating differences decrease with time (slope = &minus;0.04, p &lt; 0.01), while average ratings increase (slope = 0.07, p &lt; 0.05)&mdash;thereby gradually building up the ratings towards community-wide homogeneity. We provide evidence for peer-influence through a prediction formulation. Our consensus-based prediction model outperforms baseline network-agnostic regression models by about 23% in predicting performance ratings. This in turn shows that participants? ratings are affected by their peers? ratings and the associated interaction patterns, corroborating previous findings. Then, we formulate a consensus-based diffusion model that captures these observations of peer influence from our analyses.  We anticipate that the findings generated through this grant will open up future avenues for a broader exploration of peer-influenced skill development mechanisms, and potentially help design innovative interventions in small-groups to maximize peer-effects.          Last Modified: 09/23/2018       Submitted by: Mohammed E Hoque]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
