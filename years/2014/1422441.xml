<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Generative models of shapes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>499997.00</AwardTotalIntnAmount>
<AwardAmount>499997</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research will advance the state of the art in three-dimensional (3D) shape synthesis by developing generative probabilistic models that will enable the automatic understanding of semantics from shape geometry, and which will lead to the development of new computational modeling algorithms that allow anybody to easily create compelling and highly detailed 3D content.  Users will be able to create shapes by simply providing high-level specifications, shape types, parts, semantic shape attributes, landmark points, or sketches based on simple and intuitive user interfaces.  These models will also enable the computer to infer complete geometry from partial geometric data acquired by range cameras and to fill in any missing shape parts, or to robustly recognize objects in a scene acquired by 3D sensors.  Project outcomes will advance the state of the art in 3D modeling, providing users with intuitive tools that significantly lower the barrier of rapid and easy creation of detailed shapes.  Such tools are becoming increasingly important, since there is a growing interest in 3D models in scientific and engineering fields such as collaborative virtual environments, augmented reality, simulation, computer-aided design, and architecture.  In particular, this work will significantly benefit 3D printing; where despite hardware advances, the main bottleneck remains the creation of shapes to be supplied to the printer.  The research will also advance the state of the art in shape understanding and object recognition, which are important for computer vision and robotics applications.&lt;br/&gt;&lt;br/&gt;The key idea behind these generative models is that they represent complex hierarchical compositions, correlations and variations of detailed geometric shape features, as well as their relationships with high-level semantic shape attributes.  The models will be automatically learned from large shape repositories available on the Web, after the input shapes are pre-processed by new algorithms the Principal Investigator will develop for simultaneous shape segmentation and landmark localization so that their parts and points are consistently labeled.  Existing shape synthesis algorithms are limited to re-use and re-combine shape parts from a repository, or synthesize shapes in specific classes (such as human bodies or faces), with limited geometric variability and no structural or semantic variability.  The Principal Investigator's generative models, on the other hand, will instead learn how to densely place points and patches to create new plausible shapes in complex domains, such as furniture, vehicles, tools, creatures, etc.  Inference algorithms built upon the generative models will be able to synthesize shapes given linguistic terms or sparse geometric input.  As a result, the research will lead to the development of new 3D content creation tools that will transform the field of computational modeling: instead of executing a series of painstaking low-level geometric editing and manipulation commands, users will perform simple, easy, and intuitive interactions to achieve their design goals.</AbstractNarration>
<MinAmdLetterDate>06/26/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/26/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422441</AwardID>
<Investigator>
<FirstName>Evangelos</FirstName>
<LastName>Kalogerakis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Evangelos Kalogerakis</PI_FULL_NAME>
<EmailAddress>kalo@cs.umass.edu</EmailAddress>
<PI_PHON>4135450698</PI_PHON>
<NSF_ID>000630222</NSF_ID>
<StartDate>06/26/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>010039242</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~499997</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Automatic shape understanding is a fundamental problem in computer graphics and computer vision. In computer graphics, algorithms that intelligently process 3D shapes and learn how they are generated are bringing computational modeling into a new era, where any human capable of using a computer will be able to create compelling 3D content. 3D content creation has become increasingly important in several emerging applications, such as 3D printing, rapid prototyping, collaborative virtual environments, augmented reality, computer-aided design, simulations, digital entertainment and architecture, to name a few. One of the biggest challenges in the computer graphics field has been the development of modeling tools that are simple to use, easy to learn, enable rapid shape prototyping, generate detailed outputs, and provide useful suggestions to users, such as shape parts, deformation handles, and metadata to accelerate the design process. In computer vision, shape understanding algorithms are useful for object recognition and reconstruction. With the emergence of modern 3D geometry acquisition devices, automatic large-scale processing of the acquired 3D geometry has become a significant challenge.</p> <p>Our project made significant advances towards addressing these challenges, as summarized below:</p> <p>(a) It introduced deep learning techniques for 3D shape understanding: our deep architectures improved the state-of-the-art in shape retrieval by a very large margin. In 2015 we introduced a new deep neural network (Multi-View CNN) that improved mean average precision in shape retrieval by about 30% in large-scale shape benchmarks (ModelNet40) compared to prior methods. The method was presented at the Computer Vision and Pattern Recognition 2015 conference, and spurred significant research in this new area of &ldquo;3D deep learning&rdquo; - the paper has received more than 400 citations within 3 years of its publication.</p> <p>(b) It introduced a new generative model that synthesizes 3D shapes by considering both style and functionality specifications. Considering style and aesthetics in 3D modeling is fundamental in computer graphics. Style coordination across heterogeneous object arrangements greatly contributes to their overall aesthetics, and significantly improves the believability of virtual scenes. The project introduced the first structure-transcending style similarity measure for 3D shapes and validated it to be well aligned with human perception of stylistic similarity. This measure was then used to transfer style from exemplar shapes and create new shapes across different classes with both desired style and functionality. Our method has several applications: it can be used to generate scenes made of stylistically coherent objects, enables exploration of shape databases based on style attributes and tags, and provides stylistically compatible shape suggestions to designers during scene creation. Our work led to 2 publications presented at the SIGGRAPH 2015 and SIGGRAPH ASIA 2016 conferences.</p> <p>(c) While previous generative models in computer graphics and vision were only able to generate shapes in specific domains and scenarios (e.g.,&nbsp; assembling shapes only from pre-existing parts), we introduced much more general generative models that learn to automatically generate detailed shape geometry in a broad spectrum of classes (e.g., vehicles, furniture, organic models). Specifically, we introduced the first neural network able to generate point-based representations of surface geometry. The work was published in the Computer Graphics Forum journal in 2015. We introduced the first generative neural network that can produce 3D models directly from hand-drawn 2D sketches. The work was presented at the International Conference on 3D Vision 2017 conference. Finally, we introduced a neural network that is able to auto-complete shapes from partial, incomplete observations (e.g., scanned shapes, RGBD data) offering significantly better reconstruction accuracy compared to the state-of-the-art. The work was presented at the International Conference on Computer Vision 2017 conference.</p> <p>In terms of broader impacts, the project significantly automated the generation of 3D models as well as their printing into real-world objects, which finds applications in several disciplines, including computer-aided design, manufacturing, augmented and virtual reality. The research led to entirely new representations for 3D shape recognition, classification, segmentation, and correspondences based on deep learning, which are also useful for computer vision and robotics applications. With the increasing usage of 3D sensors in autonomous platforms, such as robots or self-driving cars, our 3D recognition algorithms can be very useful for autonomous agents that aim to correctly parse and interact with objects in real-world settings.</p> <p>In terms of educational impacts, the PI and collaborators created tutorials on 3D deep learning presented at several international conferences, including the Computer Vision and Pattern Recognition 2017, SIGGRAPH ASIA 2016, and Eurographics 2016 conferences. Students from several universities around the world learned how to incorporate machine learning techniques in geometry processing and 3D modeling pipelines. In addition, the PI and collaborators prepared a 30-page state-of-the-art-report and tutorial (published in Computer Graphics Forum journal in 2017) teaching students and researchers about advances in data-driven techniques for geometry processing and modeling. Finally, new courses have been developed to train UMass graduate and undergraduate students in advanced graphics and vision techniques.</p><br> <p>            Last Modified: 10/08/2018<br>      Modified by: Evangelos&nbsp;Kalogerakis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052215692_siggraph2015--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052215692_siggraph2015--rgov-800width.jpg" title="A suggestive interface for creating 3D scenes with stylistically coherent objects"><img src="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052215692_siggraph2015--rgov-66x44.jpg" alt="A suggestive interface for creating 3D scenes with stylistically coherent objects"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A scene created using our algorithm that provides suggestions of objects that are stylistically similar to the rest of the scene. For more information, see:Lun, Z., Kalogerakis, E., Sheffer, A., ?Elements of Style: Learning Structure-Transcending Perceptual Shape Style Similarity", SIGGRAPH 2015</div> <div class="imageCredit">?Elements of Style: Learning Structure-Transcending Perceptual Shape Style Similarity", SIGGRAPH 2015</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">A suggestive interface for creating 3D scenes with stylistically coherent objects</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052414912_CVPR1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052414912_CVPR1--rgov-800width.jpg" title="3D Shape Segmentation with Projective Convolutional Networks"><img src="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052414912_CVPR1--rgov-66x44.jpg" alt="3D Shape Segmentation with Projective Convolutional Networks"></a> <div class="imageCaptionContainer"> <div class="imageCaption">State-of-the-art results were achieved in the task of automatic 3D shape segmentation and part labeling based on ShapePFCN, a new algorithm developed during this project. Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri, S., "3D Shape Segmentation with Projective Convolutional Networks",CVPR2017</div> <div class="imageCredit">3D Shape Segmentation with Projective Convolutional Networks, CVPR2017</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">3D Shape Segmentation with Projective Convolutional Networks</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052628192_BSM1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052628192_BSM1--rgov-800width.jpg" title="Synthesis of 3D shape families via deep-learned generative models of surfaces"><img src="/por/images/Reports/POR/2018/1422441/1422441_10313276_1539052628192_BSM1--rgov-66x44.jpg" alt="Synthesis of 3D shape families via deep-learned generative models of surfaces"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We developed a probabilistic model that performs 3D joint shape analysis and synthesis. Here we show new shapes synthesized by our generative model. Huang, H., Kalogerakis, E., Marlin, B., ?Analysis and synthesis of 3D shape families via deep-learned generative models of surfaces?, CGF 2015</div> <div class="imageCredit">?Analysis and synthesis of 3D shape families via deep-learned generative models of surfaces?, CGF 2015</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">Synthesis of 3D shape families via deep-learned generative models of surfaces</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Automatic shape understanding is a fundamental problem in computer graphics and computer vision. In computer graphics, algorithms that intelligently process 3D shapes and learn how they are generated are bringing computational modeling into a new era, where any human capable of using a computer will be able to create compelling 3D content. 3D content creation has become increasingly important in several emerging applications, such as 3D printing, rapid prototyping, collaborative virtual environments, augmented reality, computer-aided design, simulations, digital entertainment and architecture, to name a few. One of the biggest challenges in the computer graphics field has been the development of modeling tools that are simple to use, easy to learn, enable rapid shape prototyping, generate detailed outputs, and provide useful suggestions to users, such as shape parts, deformation handles, and metadata to accelerate the design process. In computer vision, shape understanding algorithms are useful for object recognition and reconstruction. With the emergence of modern 3D geometry acquisition devices, automatic large-scale processing of the acquired 3D geometry has become a significant challenge.  Our project made significant advances towards addressing these challenges, as summarized below:  (a) It introduced deep learning techniques for 3D shape understanding: our deep architectures improved the state-of-the-art in shape retrieval by a very large margin. In 2015 we introduced a new deep neural network (Multi-View CNN) that improved mean average precision in shape retrieval by about 30% in large-scale shape benchmarks (ModelNet40) compared to prior methods. The method was presented at the Computer Vision and Pattern Recognition 2015 conference, and spurred significant research in this new area of "3D deep learning" - the paper has received more than 400 citations within 3 years of its publication.  (b) It introduced a new generative model that synthesizes 3D shapes by considering both style and functionality specifications. Considering style and aesthetics in 3D modeling is fundamental in computer graphics. Style coordination across heterogeneous object arrangements greatly contributes to their overall aesthetics, and significantly improves the believability of virtual scenes. The project introduced the first structure-transcending style similarity measure for 3D shapes and validated it to be well aligned with human perception of stylistic similarity. This measure was then used to transfer style from exemplar shapes and create new shapes across different classes with both desired style and functionality. Our method has several applications: it can be used to generate scenes made of stylistically coherent objects, enables exploration of shape databases based on style attributes and tags, and provides stylistically compatible shape suggestions to designers during scene creation. Our work led to 2 publications presented at the SIGGRAPH 2015 and SIGGRAPH ASIA 2016 conferences.  (c) While previous generative models in computer graphics and vision were only able to generate shapes in specific domains and scenarios (e.g.,  assembling shapes only from pre-existing parts), we introduced much more general generative models that learn to automatically generate detailed shape geometry in a broad spectrum of classes (e.g., vehicles, furniture, organic models). Specifically, we introduced the first neural network able to generate point-based representations of surface geometry. The work was published in the Computer Graphics Forum journal in 2015. We introduced the first generative neural network that can produce 3D models directly from hand-drawn 2D sketches. The work was presented at the International Conference on 3D Vision 2017 conference. Finally, we introduced a neural network that is able to auto-complete shapes from partial, incomplete observations (e.g., scanned shapes, RGBD data) offering significantly better reconstruction accuracy compared to the state-of-the-art. The work was presented at the International Conference on Computer Vision 2017 conference.  In terms of broader impacts, the project significantly automated the generation of 3D models as well as their printing into real-world objects, which finds applications in several disciplines, including computer-aided design, manufacturing, augmented and virtual reality. The research led to entirely new representations for 3D shape recognition, classification, segmentation, and correspondences based on deep learning, which are also useful for computer vision and robotics applications. With the increasing usage of 3D sensors in autonomous platforms, such as robots or self-driving cars, our 3D recognition algorithms can be very useful for autonomous agents that aim to correctly parse and interact with objects in real-world settings.  In terms of educational impacts, the PI and collaborators created tutorials on 3D deep learning presented at several international conferences, including the Computer Vision and Pattern Recognition 2017, SIGGRAPH ASIA 2016, and Eurographics 2016 conferences. Students from several universities around the world learned how to incorporate machine learning techniques in geometry processing and 3D modeling pipelines. In addition, the PI and collaborators prepared a 30-page state-of-the-art-report and tutorial (published in Computer Graphics Forum journal in 2017) teaching students and researchers about advances in data-driven techniques for geometry processing and modeling. Finally, new courses have been developed to train UMass graduate and undergraduate students in advanced graphics and vision techniques.       Last Modified: 10/08/2018       Submitted by: Evangelos Kalogerakis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
