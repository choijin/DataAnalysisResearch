<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Proposal:  Effects of production variability on the acoustic consequences of coordinated articulatory gestures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>105308.00</AwardTotalIntnAmount>
<AwardAmount>105308</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The human voice, our oldest and most reliable communication tool, is now rapidly becoming the input interface of choice that we use everyday to interact with technologies such as car navigation systems, medical  and legal dictation systems, personal assistants like "Siri," automated financial systems, etc.  Thousands of 'apps' have been developed to help consumers use voice to get the information they are looking for.   Speech recognition is the backbone of all of these technologies.  As a result, the performance of speech recognizers is key for customer satisfaction.  Currently, many systems still need to be tuned for a particular speaker to perform well, and the recognition task has be limited in other ways such as requiring (1) usage of a specific vocabulary,  (2) clear pronunciation of most of the words, especially the content words and (3) limited background noise. In this research, speech variability will be studied, and methods and models will be developed that will enable recognizers to be more speaker independent and capable of handling the full range of speech styles from clear articulation to very casually spoken speech. The results will also bear on linguistic models of speech planning and organization, providing evidence for how speakers trade off efficiencies in the production of speech against the need to be intelligible.&lt;br/&gt;&lt;br/&gt;In this project, point source tracking of the speech articulators will be collected concurrently with the corresponding acoustics.  Speakers will record speech at both a normal and rapid pace (the purpose of the latter is to increase significantly the degree of variability in the signal).  This data will allow for the investigation of whether speakers always move their speech articulators in the direction of a desired target (e.g. tongue tip to teeth in producing /t/) even when a rapid production pace occludes the relevant acoustic information (as in "perfect").  If confirmed, this finding will point the way towards making recognition systems more robust through the incorporation of articulatory information.  In addition, such data will support the development of a speech inversion system capable of 'uncovering' hidden articulatory movements potentially masked from the acoustics.</AbstractNarration>
<MinAmdLetterDate>08/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1435592</AwardID>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Tiede</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Tiede</PI_FULL_NAME>
<EmailAddress>tiede@haskins.yale.edu</EmailAddress>
<PI_PHON>2038656163</PI_PHON>
<NSF_ID>000587178</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Haskins Laboratories, Inc.</Name>
<CityName>New Haven</CityName>
<ZipCode>065116610</ZipCode>
<PhoneNumber>2038656163</PhoneNumber>
<StreetAddress>300 George Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>060010147</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>HASKINS LABORATORIES, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Haskins Laboratories]]></Name>
<CityName>New Haven</CityName>
<StateCode>CT</StateCode>
<ZipCode>065116610</ZipCode>
<StreetAddress><![CDATA[300 George Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~105308</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The sounds of speech are a consequence of how the movements of the tongue, jaw and lips shape the buzzing generated at the glottis. &nbsp;Understanding how these movements, or 'gestures', of the speech articulators are coordinated in normal speech is fundamental to prescribing remediation when developmental or degenerative speech issues arise. &nbsp;It is also clear from our previous research that incorporating information of underlying articulation can be used to improve the results of automatic speech recognition methods. &nbsp;But because the speech articulators embedded within the vocal tract are difficult to observe in action, baseline data, particularly of running speech, is scarce. &nbsp;</p> <p>Accordingly there are two main goals of this project. &nbsp;The first is to collect such data: &nbsp;using a method called electromagnetic articulography we track the movement of sensors placed on the tongue, jaw and lips, with additional sensors used to correct for any head movement, while simultaneously recording the corresponding audio during active speech tasks. &nbsp;Eight native American English speakers have been recorded producing 720 phonetically balanced sentences, at normal and fast production rates. &nbsp;The range of utterances, and the rate contrast, are unprecedented in their scope for data of this type, and the database itself is freely available for interested researchers at http://bit.ly/2s4mtOq. &nbsp;Preliminary exploration of this data shows systematic differences in elicited rate (median fast / normal sentence duration ratio = 0.66), that rate effects are not uniform (vowels compress more than continuants, and stops compress least under fast rate), and that gestural overlap is highest across word boundaries, while relative phasing in consonant clusters is unaffected. &nbsp;These results have implications for how reduction effects are accounted for in theories of phonology.</p> <p>The second goal of the project is to use this data to train and test a speech inversion system which estimates articulator movements from speech acoustics. &nbsp;Our previous work has shown that incorporating such estimates into automatic recognition systems improves their results, and enhancements to our modeling based on training with this data are better still, as we now incorporate estimates of the acoustic consequences of underlying gestures that are partially or completely elided by gestural overlap. &nbsp;In future work we envision that the speech inversion system will be implemented within feedback tools that clinicians can use to remediate speech errors, and that second language learners can use to improve pronunciation.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/23/2018<br>      Modified by: Mark&nbsp;Tiede</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The sounds of speech are a consequence of how the movements of the tongue, jaw and lips shape the buzzing generated at the glottis.  Understanding how these movements, or 'gestures', of the speech articulators are coordinated in normal speech is fundamental to prescribing remediation when developmental or degenerative speech issues arise.  It is also clear from our previous research that incorporating information of underlying articulation can be used to improve the results of automatic speech recognition methods.  But because the speech articulators embedded within the vocal tract are difficult to observe in action, baseline data, particularly of running speech, is scarce.    Accordingly there are two main goals of this project.  The first is to collect such data:  using a method called electromagnetic articulography we track the movement of sensors placed on the tongue, jaw and lips, with additional sensors used to correct for any head movement, while simultaneously recording the corresponding audio during active speech tasks.  Eight native American English speakers have been recorded producing 720 phonetically balanced sentences, at normal and fast production rates.  The range of utterances, and the rate contrast, are unprecedented in their scope for data of this type, and the database itself is freely available for interested researchers at http://bit.ly/2s4mtOq.  Preliminary exploration of this data shows systematic differences in elicited rate (median fast / normal sentence duration ratio = 0.66), that rate effects are not uniform (vowels compress more than continuants, and stops compress least under fast rate), and that gestural overlap is highest across word boundaries, while relative phasing in consonant clusters is unaffected.  These results have implications for how reduction effects are accounted for in theories of phonology.  The second goal of the project is to use this data to train and test a speech inversion system which estimates articulator movements from speech acoustics.  Our previous work has shown that incorporating such estimates into automatic recognition systems improves their results, and enhancements to our modeling based on training with this data are better still, as we now incorporate estimates of the acoustic consequences of underlying gestures that are partially or completely elided by gestural overlap.  In future work we envision that the speech inversion system will be implemented within feedback tools that clinicians can use to remediate speech errors, and that second language learners can use to improve pronunciation.                Last Modified: 01/23/2018       Submitted by: Mark Tiede]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
