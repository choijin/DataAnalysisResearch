<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Measuring Classroom Coverage of Content and Practices in the New Generation of Mathematics and Science Standards</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>03/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Karen King</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The National Research Council in 2013 released the report Monitoring Progress toward Successful K-12 STEM Education: A Nation Advancing?  which outlined 14 Indicators as well as the needed research and development to create a system to monitor the quality of STEM education in the nation.  This project is funded in response to a request for research in the Promoting Innovation in Measurement and Evaluation program focused on developing research and tools to advance the nation's capability to measure these indicators. The project will explore existing measures from the social, behavioral, and educational sciences as well as opportunities for developing new measures of Indicator 5, one of the highest priority indicators listed in the report: student experience in science, technology, engineering, and mathematics (STEM) classes that can be adopted on a large scale to collect evidence about classroom coverage of mathematics and science content and practices in standards for college and career readiness. This project will contribute to advancing knowledge about STEM research and education through building consensus among researchers and practitioners about how to improve the measurement of students' exposure to mathematics and science content and practices and providing concrete action steps to inform the development of high-quality, usable indicators.&lt;br/&gt;Research activities will be carried out in two stages. During Stage I, the project team will conduct a review of existing measures of student experience in classrooms and innovative data collection methods. Research methods used include literature review and interviews with developers and subject-matter experts. Key deliverables from Stage I are a draft research report and associated briefings about findings from the reviews of literatures and proposed future plan and action steps to measure instruction aligned with the college and career ready standards in mathematics and science on a large scale. During Stage II, the research team will hold a one and one-half day conference with experts and policymakers to collect feedback on deliverables from Stage I activities and to address ways to adapt existing measures or develop new measures to assess students' exposure to STEM content and practices for use in a large-scale indicator system.  A project website will summarize project activities and serve as a repository for all project documents, which will be made available for downloading free of charge.</AbstractNarration>
<MinAmdLetterDate>07/12/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/12/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1445670</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Stecher</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian M Stecher</PI_FULL_NAME>
<EmailAddress>Brian_Stecher@rand.org</EmailAddress>
<PI_PHON>3103930411</PI_PHON>
<NSF_ID>000438265</NSF_ID>
<StartDate>07/12/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Laura</FirstName>
<LastName>Hamilton</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laura S Hamilton</PI_FULL_NAME>
<EmailAddress>Laura_Hamilton@rand.org</EmailAddress>
<PI_PHON>4126832300</PI_PHON>
<NSF_ID>000215823</NSF_ID>
<StartDate>07/12/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kun</FirstName>
<LastName>Yuan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kun Yuan</PI_FULL_NAME>
<EmailAddress>kyuan@rand.org</EmailAddress>
<PI_PHON>4126832300</PI_PHON>
<NSF_ID>000615457</NSF_ID>
<StartDate>07/12/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rand Corporation</Name>
<CityName>Santa Monica</CityName>
<ZipCode>904013297</ZipCode>
<PhoneNumber>3103930411</PhoneNumber>
<StreetAddress>1776 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006914071</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RAND CORPORATION, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006914071</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rand Corporation]]></Name>
<CityName>SANTA MONICA</CityName>
<StateCode>CA</StateCode>
<ZipCode>904013297</ZipCode>
<StreetAddress><![CDATA[1776 MAIN ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In 2013, the National Research Council identified 14&nbsp;indicators that could be used to monitor the nation&rsquo;s progress toward improving education in science, technology, engineering, and mathematics (STEM). The fifth indicator addresses classroom coverage of the STEM content and practices in new, college- and career-ready standards. Although existing measures such as surveys provide some evidence of students&rsquo; exposure to STEM practices, these methods fail to provide a detailed record of students&rsquo; experiences. One limitation of most existing methods is that they tend to focus on what the teacher is doing rather than what students are doing. This focus on the teacher can be particularly limiting in classrooms in which different students are engaged in different learning activities simultaneously. These within-classroom differences can result from efforts to differentiate instruction to meet the needs of individual students and are likely to be especially prevalent in classrooms that rely on technology-based, personalized-learning approaches. This diversity of experience within a single classroom complicates the task of documenting the content and practices to which students are exposed.</p> <p>This project explored methods that might be used to collect information on students&rsquo; classroom experiences in STEM, particularly in light of the need to address this within-classroom diversity. We reviewed literature, conducted interviews with experts in curriculum, measurement, and technology, and convened a meeting with experts to share information and ideas about new developments in technology that could support innovative, high-quality measurement. Our review examined evidence on currently available measures of instruction that could be used in STEM classrooms, including surveys, logs, artifacts, and observation rubrics. We also reviewed literature on data-collection methods that have not been widely used as measures in classrooms but that might be able to be adapted for the purposes of measuring students&rsquo; STEM experiences.&nbsp;</p> <p>Key findings from these activities include:</p> <ul> <li>Because surveys are relatively inexpensive and easy to administer on a large scale, it is likely that an indicator of STEM content and practices will rely heavily on surveys at least in the near future.</li> <li>At the same time, technology-based learning systems have the potential to support future measurement efforts; they provide opportunities for novel data-collection methods that can be used to collect information on students&rsquo; activities, including what they are doing and how long they spend doing it.</li> <li>The use of technology-based learning systems as the basis for indicators may be limited due to variability across schools in the instructional technology resources they have available.</li> <li>Practices in STEM tend to be enacted differently across different content areas, so any large-scale indicator system is likely to require sampling across schools or classrooms to ensure representation across content areas while keeping respondents&rsquo; burden at a minimum.</li> <li>Some student STEM experiences occur outside of traditional courses, especially in areas such as robotics which is frequently taught via extracurricular opportunities; an indicator would need to be designed to address this. </li> </ul> <p>The report offers several recommendations for future indicator development, including:</p> <ul> <li>Developers of indicators should consider creating a working group to provide input from various stakeholders on the design of the indicator system.</li> <li>A comprehensive picture of students&rsquo; exposure to standards-aligned content and practices is likely to require multiple measures that provide complementary perspectives.</li> <li>Developers should being by building on existing data-collection efforts such as large-scale surveys.</li> <li>The measures should be designed to support longitudinal comparisons, allowing users to track changes in students&rsquo; experiences over time.</li> <li>Although the NRC indicators did not directly address measures of student achievement, developers might consider incorporating such measures into the broader indicator system because they could provide evidence that would be helpful in understanding whether students have access to high-quality instruction.</li> <li>The indicator of students&rsquo; exposure to STEM content and practices should not have high stakes attached to it.</li> <li>Additional research on STEM instruction is needed to understand the specific practices that promote student learning, which could then inform the design of the indicator system. </li> </ul> <p>The project methods and findings were presented in a RAND report:</p> <p><a href="http://www.rand.org/pubs/research_reports/RR1913.html">http://www.rand.org/pubs/research_reports/RR1913.html</a></p><br> <p>            Last Modified: 04/11/2017<br>      Modified by: Laura&nbsp;S&nbsp;Hamilton</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In 2013, the National Research Council identified 14 indicators that could be used to monitor the nation?s progress toward improving education in science, technology, engineering, and mathematics (STEM). The fifth indicator addresses classroom coverage of the STEM content and practices in new, college- and career-ready standards. Although existing measures such as surveys provide some evidence of students? exposure to STEM practices, these methods fail to provide a detailed record of students? experiences. One limitation of most existing methods is that they tend to focus on what the teacher is doing rather than what students are doing. This focus on the teacher can be particularly limiting in classrooms in which different students are engaged in different learning activities simultaneously. These within-classroom differences can result from efforts to differentiate instruction to meet the needs of individual students and are likely to be especially prevalent in classrooms that rely on technology-based, personalized-learning approaches. This diversity of experience within a single classroom complicates the task of documenting the content and practices to which students are exposed.  This project explored methods that might be used to collect information on students? classroom experiences in STEM, particularly in light of the need to address this within-classroom diversity. We reviewed literature, conducted interviews with experts in curriculum, measurement, and technology, and convened a meeting with experts to share information and ideas about new developments in technology that could support innovative, high-quality measurement. Our review examined evidence on currently available measures of instruction that could be used in STEM classrooms, including surveys, logs, artifacts, and observation rubrics. We also reviewed literature on data-collection methods that have not been widely used as measures in classrooms but that might be able to be adapted for the purposes of measuring students? STEM experiences.   Key findings from these activities include:  Because surveys are relatively inexpensive and easy to administer on a large scale, it is likely that an indicator of STEM content and practices will rely heavily on surveys at least in the near future. At the same time, technology-based learning systems have the potential to support future measurement efforts; they provide opportunities for novel data-collection methods that can be used to collect information on students? activities, including what they are doing and how long they spend doing it. The use of technology-based learning systems as the basis for indicators may be limited due to variability across schools in the instructional technology resources they have available. Practices in STEM tend to be enacted differently across different content areas, so any large-scale indicator system is likely to require sampling across schools or classrooms to ensure representation across content areas while keeping respondents? burden at a minimum. Some student STEM experiences occur outside of traditional courses, especially in areas such as robotics which is frequently taught via extracurricular opportunities; an indicator would need to be designed to address this.    The report offers several recommendations for future indicator development, including:  Developers of indicators should consider creating a working group to provide input from various stakeholders on the design of the indicator system. A comprehensive picture of students? exposure to standards-aligned content and practices is likely to require multiple measures that provide complementary perspectives. Developers should being by building on existing data-collection efforts such as large-scale surveys. The measures should be designed to support longitudinal comparisons, allowing users to track changes in students? experiences over time. Although the NRC indicators did not directly address measures of student achievement, developers might consider incorporating such measures into the broader indicator system because they could provide evidence that would be helpful in understanding whether students have access to high-quality instruction. The indicator of students? exposure to STEM content and practices should not have high stakes attached to it. Additional research on STEM instruction is needed to understand the specific practices that promote student learning, which could then inform the design of the indicator system.    The project methods and findings were presented in a RAND report:  http://www.rand.org/pubs/research_reports/RR1913.html       Last Modified: 04/11/2017       Submitted by: Laura S Hamilton]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
