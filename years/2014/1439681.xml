<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I/UCRC Phase I: Robots and Sensors for the Human Well-being</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>325000.00</AwardTotalIntnAmount>
<AwardAmount>260000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ann Von Lehmen</SignBlockName>
<PO_EMAI>avonlehm@nsf.gov</PO_EMAI>
<PO_PHON>7032924756</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The proposed I/UCRC for Robots and Sensors for the Human Well-being (RoSeHuB) will focus on complementing a broad variety of off-the-self sensors with intelligent processing software that enables them to extract useful information about the operating environments in medicine and agriculture. RoSeHuB research will make heavy use of commercial cameras that can work in different parts of the electro-magnetic spectrum (i.e., visible, IR, Thermal, etc.), laser or radar sensors, etc. Sensors or sensor systems may exhibit different degrees of mobility. They may be embedded in robots or flying drones or they may be fixed with limited degrees of motion (PTZ cameras). In the areas of algorithms and learning methods the focus and the challenge is on creating methodologies that can balance real-time operation and computational power while providing high level semantic information either for planning, interaction or situational awareness for human operators. With respect to robots, efforts will focus on building systems with advanced mobility, manipulation, human-machine interaction, and coordination skills.&lt;br/&gt;&lt;br/&gt;Robots and sensors can lead to more effective precision agriculture techniques that provide more food than current levels while they save water and prevent soil erosion. Similarly, robots play a critical and growing role in modern medicine, from training the next generation of doctors, dentists, and nurses, to comforting and protecting elderly patients in the early stages of dementia. The proposed Center will attract large companies to the pertinent domains and energize innovative startup companies, both through research and through the production of highly trained graduate students with advanced coursework in sensory-based robotic systems and hands-on exposure to multi-disciplinary, integrative systems. The Center will also fund a projects to pursue new pertinent high-risk initiatives, ensuring that technology continues to meet emerging societal needs. RoSeHuB faculty will aggressively recruit women and minority graduate and undergraduate students and host an annual summer camp for middle-schoolers from underrepresented groups.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/09/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439681</AwardID>
<Investigator>
<FirstName>R. Vijay</FirstName>
<LastName>Kumar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>R. Vijay Kumar</PI_FULL_NAME>
<EmailAddress>Kumar@seas.upenn.edu</EmailAddress>
<PI_PHON>2158983630</PI_PHON>
<NSF_ID>000280506</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Camillo</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Camillo J Taylor</PI_FULL_NAME>
<EmailAddress>cjtaylor@central.cis.upenn.edu</EmailAddress>
<PI_PHON>2158980376</PI_PHON>
<NSF_ID>000097685</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of the University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress><![CDATA[3451 Walnut Street P-221 FB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramReference>
<Code>5761</Code>
<Text>INDUSTRY/UNIV COOP RES CENTERS</Text>
</ProgramReference>
<ProgramReference>
<Code>8039</Code>
<Text>Information, Communication &amp; Computing</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~65000</FUND_OBLG>
<FUND_OBLG>2015~65000</FUND_OBLG>
<FUND_OBLG>2016~65000</FUND_OBLG>
<FUND_OBLG>2017~65000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-a089db9f-7fff-aefb-5213-5cd5a7821237"> <p dir="ltr"><span>This program included a variety of projects that tried to bridge the gap between basic research and technology innovations and products.&nbsp;</span></p> <br /> <p dir="ltr"><span>The analysis of 3D postures and movement of humans is essential for many applications like augmented reality, sports analytics, ergonomics, rehabilitation, as well as for basic sciences like kinesiology, behavior analysis, and neuroscience. In a series of results of this project, it was shown that it is possible to capture both the posture and the body shape from monocular images in the wild and without any marker.</span></p> <br /> <p dir="ltr"><span>For the purpose of robot-assisted, rapid experiment preparation system in a pharmaceutical lab a robot was trained to&nbsp; autonomously pour from unknown symmetric containers found in a typical wet laboratory. The robot estimates the pouring container geometry, then leverages simulated pours as priors for a given fluid to pour precisely and quickly in a single attempt. The change of volume in the receiver is a function of the geometry of the pouring container, the pouring angle, and rate. To determine the volumetric flow rate, the profile for maximum containable volume for a given angle is estimated along with the time delay of the fluid exiting the container. The system is capable of pouring quickly and precisely from varying symmetric containers in a single attempt with limited priors, and a novel fluid detection method.&nbsp;</span></p> <br /> <p dir="ltr"><span>Periodical inspection and maintenance of critical infrastructure such as dams, penstocks and locks are of significant importance to prevent catastrophic failures. Conventional manual inspection methods require inspectors to climb along a penstock to spot corrosion, rust and crack formation which is unsafe, labor-intensive, and requires intensive training. This work presents an alternative approach using a Micro Aerial Vehicle that autonomously flies to collect imagery which is then fed into a pre-trained deep-learning model to identify corrosion. The method can be used to offer a complete, safe and cost-efficient solution to autonomous infrastructure inspection.</span></p> <p dir="ltr"><span>Four doctoral&nbsp; students among them one African-American and one female graduated by conducting research on this project.</span></p> <div><span><br /></span></div> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 02/14/2020<br>      Modified by: Kostas&nbsp;Daniilidis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1439681/1439681_10341408_1580104345172_forNSFhumans--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1439681/1439681_10341408_1580104345172_forNSFhumans--rgov-800width.jpg" title="Reconstruction of multiple 3D humans from monocular imagery"><img src="/por/images/Reports/POR/2020/1439681/1439681_10341408_1580104345172_forNSFhumans--rgov-66x44.jpg" alt="Reconstruction of multiple 3D humans from monocular imagery"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Humans can be reconstructed in terms of their posture and body-shape from single images. When multiple humans are in the picture occlusions must be handled and the relative global pose between humans must be estimated.</div> <div class="imageCredit">Nikos Kolotouros</div> <div class="imageSubmitted">Kostas&nbsp;Daniilidis</div> <div class="imageTitle">Reconstruction of multiple 3D humans from monocular imagery</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  This program included a variety of projects that tried to bridge the gap between basic research and technology innovations and products.    The analysis of 3D postures and movement of humans is essential for many applications like augmented reality, sports analytics, ergonomics, rehabilitation, as well as for basic sciences like kinesiology, behavior analysis, and neuroscience. In a series of results of this project, it was shown that it is possible to capture both the posture and the body shape from monocular images in the wild and without any marker.   For the purpose of robot-assisted, rapid experiment preparation system in a pharmaceutical lab a robot was trained to  autonomously pour from unknown symmetric containers found in a typical wet laboratory. The robot estimates the pouring container geometry, then leverages simulated pours as priors for a given fluid to pour precisely and quickly in a single attempt. The change of volume in the receiver is a function of the geometry of the pouring container, the pouring angle, and rate. To determine the volumetric flow rate, the profile for maximum containable volume for a given angle is estimated along with the time delay of the fluid exiting the container. The system is capable of pouring quickly and precisely from varying symmetric containers in a single attempt with limited priors, and a novel fluid detection method.    Periodical inspection and maintenance of critical infrastructure such as dams, penstocks and locks are of significant importance to prevent catastrophic failures. Conventional manual inspection methods require inspectors to climb along a penstock to spot corrosion, rust and crack formation which is unsafe, labor-intensive, and requires intensive training. This work presents an alternative approach using a Micro Aerial Vehicle that autonomously flies to collect imagery which is then fed into a pre-trained deep-learning model to identify corrosion. The method can be used to offer a complete, safe and cost-efficient solution to autonomous infrastructure inspection. Four doctoral  students among them one African-American and one female graduated by conducting research on this project.             Last Modified: 02/14/2020       Submitted by: Kostas Daniilidis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
