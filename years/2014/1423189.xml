<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Looking Across the Uncanny Valley: Procedural and Data-Driven Methods for Gaze Modeling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>494347.00</AwardTotalIntnAmount>
<AwardAmount>520513</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Eye movements play a key role in human communication, yet they remain a significant stumbling block for humanoid animation.  Computer generated avatars currently lack realistic gaze, which subconsciously distracts viewers and thereby detracts from the usefulness of the many applications   that employ these graphical actors (e.g., educational / tutoring / training systems that incorporate animated agents), a perceptual issue that has been dubbed the "uncanny valley."  This project represents a collaboration between two investigators with complementary skills who will tackle the problem by developing a holistic model of gaze dynamics for avatars, which combines detailed real-world measurements (with the aid of binocular eye-tracking) and signal analysis of both eye motions and the periocular skin region around the eyes, to generate improved synthetic eye movement animations.  Project activities will include creation of a database of gaze motions,  perceptual experiments to improve our understanding of the saliency of different components of human gaze (eyeball rotation, vergence, jitter, periocular skin motion) and their signal properties in physical space, and the implementation of exemplary tasks (such as reading and conversations) along with a software tool which will enable animators to more easily design convincing gaze in frequently encountered situations.  The PI intends to open-source the software to be developed in this research.  &lt;br/&gt;&lt;br/&gt;The gaze model under development by the PI differs from previous approaches in the following ways.   First, while others have modeled cyclopean avatar gaze few have accessed the steadily growing eye tracking literature for inclusion of binocular eye movement.  The PI argues that binocular eye tracking in three-dimensional physical space allows estimation of where the subject is fixating in depth and, consequently, recording, analysis, and modeling of gaze vergence, which will yield more believable characters.  Second, the proposed model provides a component of subtle gaze jitter, which is critical for dynamic realism as the eyes are never perfectly still.  Third, the description of the rotations of the eyes is mathematically concise, follows physiological laws, and is easy to implement.  Finally, the proposed modeling effort includes perceptual studies designed to investigate the influence of each model component and to optimize the parameters of the resulting complete model.  The procedural model is two-staged and reminiscent of the functionality of human vision: a bottom-up stage of eye rotation, which will be used to represent gaze when selecting a series of look points, followed by a top-down stage of gaze orientation dependent on a given task.  Building a perceptual science underlying gaze modeling will foster the believability of synthetic actors, and will more broadly impact diverse areas such as social robotics where realistic gaze simulation is crucial for creating likable robots.</AbstractNarration>
<MinAmdLetterDate>07/31/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/15/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423189</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Duchowski</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew T Duchowski</PI_FULL_NAME>
<EmailAddress>duchowski@clemson.edu</EmailAddress>
<PI_PHON>8646567677</PI_PHON>
<NSF_ID>000492350</NSF_ID>
<StartDate>07/31/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sophie</FirstName>
<LastName>Joerg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sophie Joerg</PI_FULL_NAME>
<EmailAddress>sjoerg@clemson.edu</EmailAddress>
<PI_PHON>8646560538</PI_PHON>
<NSF_ID>000626746</NSF_ID>
<StartDate>07/31/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Clemson University</Name>
<CityName>CLEMSON</CityName>
<CountyName/>
<ZipCode>296345701</ZipCode>
<PhoneNumber>8646562424</PhoneNumber>
<StreetAddress>230 Kappa Street</StreetAddress>
<StreetAddress2>Suite 200</StreetAddress2>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042629816</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CLEMSON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042629816</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Clemson University]]></Name>
<CityName>Clemson</CityName>
<CountyName>PICKENS</CountyName>
<StateCode>SC</StateCode>
<ZipCode>296340001</ZipCode>
<StreetAddress><![CDATA[300 Brackett Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~177609</FUND_OBLG>
<FUND_OBLG>2015~321738</FUND_OBLG>
<FUND_OBLG>2017~21166</FUND_OBLG>
</Award>
</rootTag>
