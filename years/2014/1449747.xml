<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps:  Commercialization feasibility research of animation-projection ExpressionBots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rathindra DasGupta</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Existing robotic platforms in the market have serious limitations for natural face-to-face communication. An alternative approach that could overcome many of these problems is to use state-of-the-art character animation technologies to project lifelike 3-D models onto a robotic mask that can display the 3-D models? natural speech and facial expressions. Currently, there are no such animation-based human robotic agents available in the market that can be acquired, nor can such capabilities be assembled from plug-and-play mechatronic components without significant design work and customization.  In this proposal the team has developed a robotic head that projects an animation on a translucent custom designed facial mask to mimic emotive facial gestures and visual speech.&lt;br/&gt;&lt;br/&gt;The proposed product (ExpressionBot) is an extremely low-cost and portable robotic head using animation-projection and a custom designed facial mask that can display rich emotive facial gestures and visual speech generated by an avatar agent. The agent can speak, produce and mirror facial expressions synchronized with agent?s visual and prosodic speech. The ExpressionBot can be used as science tutor in classrooms, companionbots in nursing homes, museums tour guide, hotels receptionists, and tourist information kiosks. The ExpressionBot1 comprises a rich Physical Display Portal complete with software libraries for robotic character instantiation including facial expression generation and visual speech generation, human facial recognition, human speech recognition and natural language processing, and world environment recognition.</AbstractNarration>
<MinAmdLetterDate>07/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1449747</AwardID>
<Investigator>
<FirstName>Mohammad</FirstName>
<LastName>Mahoor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mohammad Mahoor</PI_FULL_NAME>
<EmailAddress>mmahoor@du.edu</EmailAddress>
<PI_PHON>3038713745</PI_PHON>
<NSF_ID>000511471</NSF_ID>
<StartDate>07/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Denver</Name>
<CityName>Denver</CityName>
<ZipCode>802104711</ZipCode>
<PhoneNumber>3038712000</PhoneNumber>
<StreetAddress>2199 S. University Blvd.</StreetAddress>
<StreetAddress2><![CDATA[Ofc of Research & Sponsored Prog]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431760</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLORADO SEMINARY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431760</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Denver]]></Name>
<CityName>Denver</CityName>
<StateCode>CO</StateCode>
<ZipCode>802105345</ZipCode>
<StreetAddress><![CDATA[2390 S York Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The major goal of this project was to attend the I-Corps training workshops and discover potential markets and customers for a light-projected social robot being developed at the University of Denver. The customers of this innovative technology are those individuals who have deficits in their socio-emotional and cognitive behaviors, and are at risk while living dependently due to issues caused by Alzheimer&rsquo;s disease and dementia. The two key Value Propositions (VPs) of the proposed innovation are: 1) to increase the longevity, happiness, and independence of elderly individuals with dementia through engaging, effective and always available access to our companionbot, and 2) reduce the workload and improve the productivity and effectiveness of caregivers in nursing homes and Assisted Living Facilities (ALFs). These VPs were evaluated and assessed through the course of this I-Corps grant where 102 potential customers were interviewed within 8 weeks. Our value propositions were refined and our customer segment was identified during interviews with 37 people who were introduced to and interacted with the companionbot, including elderly individuals with and without dementia in homes and residences, decision makers, directors/managers of ALFs, and influencers and recommenders within this customer segment. From these interviews we learned that there a high potential market for the companionbot by people who lives in ALFs. In better words, the served available/addressable market for our innovation is the elderly individuals with dementia in these assisted living facilities and nursing homes. The secondary market is the special education program of school districts.</p><br> <p>            Last Modified: 04/26/2016<br>      Modified by: Mohammad&nbsp;Mahoor</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The major goal of this project was to attend the I-Corps training workshops and discover potential markets and customers for a light-projected social robot being developed at the University of Denver. The customers of this innovative technology are those individuals who have deficits in their socio-emotional and cognitive behaviors, and are at risk while living dependently due to issues caused by AlzheimerÃ†s disease and dementia. The two key Value Propositions (VPs) of the proposed innovation are: 1) to increase the longevity, happiness, and independence of elderly individuals with dementia through engaging, effective and always available access to our companionbot, and 2) reduce the workload and improve the productivity and effectiveness of caregivers in nursing homes and Assisted Living Facilities (ALFs). These VPs were evaluated and assessed through the course of this I-Corps grant where 102 potential customers were interviewed within 8 weeks. Our value propositions were refined and our customer segment was identified during interviews with 37 people who were introduced to and interacted with the companionbot, including elderly individuals with and without dementia in homes and residences, decision makers, directors/managers of ALFs, and influencers and recommenders within this customer segment. From these interviews we learned that there a high potential market for the companionbot by people who lives in ALFs. In better words, the served available/addressable market for our innovation is the elderly individuals with dementia in these assisted living facilities and nursing homes. The secondary market is the special education program of school districts.       Last Modified: 04/26/2016       Submitted by: Mohammad Mahoor]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
