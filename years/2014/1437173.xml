<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: PCK*Lex: Applying Computerized Lexical Analysis to Develop a  Cost-Effective Measure of Science Teacher Pedagogical Content Knowledge</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>532624.00</AwardTotalIntnAmount>
<AwardAmount>532624</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sarah-Kay McDonald</SignBlockName>
<PO_EMAI>sarmcdon@nsf.gov</PO_EMAI>
<PO_PHON>7032924648</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Pedagogical content knowledge (PCK) is a critical conceptual tool for distinguishing effective from ineffective teaching, but difficulty measuring it has resulted in minimal application of the concept to enhance actual teaching practice. This project will apply computerized lexical analysis techniques to develop an efficient, valid and reliable measure of science teacher PCK - the type of teacher knowledge that bridges content knowledge and how to effectively teach the content in classrooms. This project will transform an instrument used in multiple NSF STEM projects, but which is limited by being time and resource intensive to score. Application of recent developments in lexical analysis will allow measurement of PCK to be taken to scale with rapid feedback, and will be a vital resource for STEM researchers, evaluators, PD providers, teacher educators, and teachers themselves. This collaborative research project brings together two teams with unique yet complementary expertise. BSCS researchers have experience measuring PCK in multiple federal projects, and recently led an international summit on STEM PCK. The Michigan State University Automated Analysis of Constructed Response research group is working across STEM disciplines to refine approaches to using lexical analysis for formative educational purposes. Broader impacts of this project include developing a measure that will be invaluable to multiple stakeholders in evaluating and enhancing the understandings and abilities of K-12 science teachers. Enhancing the capacity of K-12 teachers will help increase the numbers of underrepresented students who pursue STEM careers, as well as expand the STEM workforce and increase general scientific literacy.</AbstractNarration>
<MinAmdLetterDate>07/30/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1437173</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Wilson</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher D Wilson</PI_FULL_NAME>
<EmailAddress>cwilson@bscs.org</EmailAddress>
<PI_PHON>7195315550</PI_PHON>
<NSF_ID>000508017</NSF_ID>
<StartDate>07/30/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Molly</FirstName>
<LastName>Stuhlsatz</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Molly A Stuhlsatz</PI_FULL_NAME>
<EmailAddress>mstuhlsatz@bscs.org</EmailAddress>
<PI_PHON>7195315550</PI_PHON>
<NSF_ID>000623913</NSF_ID>
<StartDate>07/30/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>BSCS Science Learning</Name>
<CityName>Colorado Springs</CityName>
<ZipCode>809183842</ZipCode>
<PhoneNumber>7195315550</PhoneNumber>
<StreetAddress>5415 Mark Dabling Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>173848607</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BSCS SCIENCE LEARNING</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Biological Sciences Curriculum Study]]></Name>
<CityName>Colorado Springs</CityName>
<StateCode>CO</StateCode>
<ZipCode>809183842</ZipCode>
<StreetAddress><![CDATA[5415 Mark Dabling Blvd.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~532624</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to apply computerized lexical analysis and machine learning techniques to develop an efficient, valid and reliable measure of science teacher pedagogical content knowledge (PCK) - the type of teacher knowledge that bridges science content knowledge and how to effectively teach that content in classrooms. PCK is a critical conceptual tool for distinguishing effective from ineffective teaching, but difficulty measuring it has resulted in minimal application of the concept to enhance teaching practice.</p> <p>&nbsp;</p> <p>After initial work to explore and define the PCK construct, we focused our work on two key categories of PCK &ndash; the teacher&rsquo;s ability to identify and analyze student thinking around science ideas (Student Thinking), and their ability to reveal and respond to student&rsquo;s ideas (Responsive Teaching). Data collection for this study included a video analysis task in which teachers watched short videos of authentic science classrooms, and then responded to prompts asking them to describe what they observed in the video. In brief, our research process included collecting a large sample of data from teachers, scoring that data, and then using computer-based automated analysis techniques to that could score the teacher writing the same way as human. We then iterated on this process to improve the accuracy of the computer models.</p> <p>&nbsp;</p> <p>We were able to successfully develop a computer to accurately predict scores for the Student Thinking component.&nbsp;When comparing the computer predicted scores to expert assigned scores on the teacher responses, the model displayed an accuracy &gt; 90% and a Cohen&rsquo;s kappa &gt;0.80. Developing computer models for the Responsive Teaching component has been more challenging. After initial low performance, we attempted to improve the accuracy of the models by breaking the Responsive Teaching category into two components: the teacher&rsquo;s observation and then their reasoning behind those observations. We achieved moderate success for one model and low success for the other.&nbsp;For the Observation sub-score we were able to develop a model with Cohen&rsquo;s kappa = 0.71; for the Reason sub-score we were able to develop a model with a low Cohen&rsquo;s kappa = 0.49. This scoring model consistently underpredicted Reasoning in teacher&rsquo;s responses.</p> <p>&nbsp;</p> <p>This difference between model performance in Student Thinking and Responsive Teaching is likely due to differences in teacher writing about these ideas.&nbsp;Since we were able to build a single scoring model to detect occurrences when teachers identified Student Thinking in their responses, this may suggest that there is more common text used in these responses about student thinking, which is largely independent of the video scenario the teacher watches to develop a response.&nbsp; Conversely, the difficulty for the machine learning models to &ldquo;learn&rdquo; how to identify ideas about Responsive Teaching and more specifically, the Reasoning sub-score, may mean these ideas are more context dependent based on the video scenario, or that ideas categorized as Reasoning are written by teachers in a variety of ways in their responses.</p><br> <p>            Last Modified: 12/31/2019<br>      Modified by: Christopher&nbsp;D&nbsp;Wilson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to apply computerized lexical analysis and machine learning techniques to develop an efficient, valid and reliable measure of science teacher pedagogical content knowledge (PCK) - the type of teacher knowledge that bridges science content knowledge and how to effectively teach that content in classrooms. PCK is a critical conceptual tool for distinguishing effective from ineffective teaching, but difficulty measuring it has resulted in minimal application of the concept to enhance teaching practice.     After initial work to explore and define the PCK construct, we focused our work on two key categories of PCK &ndash; the teacher’s ability to identify and analyze student thinking around science ideas (Student Thinking), and their ability to reveal and respond to student’s ideas (Responsive Teaching). Data collection for this study included a video analysis task in which teachers watched short videos of authentic science classrooms, and then responded to prompts asking them to describe what they observed in the video. In brief, our research process included collecting a large sample of data from teachers, scoring that data, and then using computer-based automated analysis techniques to that could score the teacher writing the same way as human. We then iterated on this process to improve the accuracy of the computer models.     We were able to successfully develop a computer to accurately predict scores for the Student Thinking component. When comparing the computer predicted scores to expert assigned scores on the teacher responses, the model displayed an accuracy &gt; 90% and a Cohen’s kappa &gt;0.80. Developing computer models for the Responsive Teaching component has been more challenging. After initial low performance, we attempted to improve the accuracy of the models by breaking the Responsive Teaching category into two components: the teacher’s observation and then their reasoning behind those observations. We achieved moderate success for one model and low success for the other. For the Observation sub-score we were able to develop a model with Cohen’s kappa = 0.71; for the Reason sub-score we were able to develop a model with a low Cohen’s kappa = 0.49. This scoring model consistently underpredicted Reasoning in teacher’s responses.     This difference between model performance in Student Thinking and Responsive Teaching is likely due to differences in teacher writing about these ideas. Since we were able to build a single scoring model to detect occurrences when teachers identified Student Thinking in their responses, this may suggest that there is more common text used in these responses about student thinking, which is largely independent of the video scenario the teacher watches to develop a response.  Conversely, the difficulty for the machine learning models to "learn" how to identify ideas about Responsive Teaching and more specifically, the Reasoning sub-score, may mean these ideas are more context dependent based on the video scenario, or that ideas categorized as Reasoning are written by teachers in a variety of ways in their responses.       Last Modified: 12/31/2019       Submitted by: Christopher D Wilson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
