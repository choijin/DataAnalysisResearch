<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Robotics 2.0 for Disaster Response and Relief Operations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>700000.00</AwardTotalIntnAmount>
<AwardAmount>700000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project develops and tests novel compressive sensing and sensor locating techniques that are adaptable to a myriad of different mobile robot designs while operable on today's wireless communication infrastructures. Unique in-situ laboratory and field experiments provide tangible results to scientists and other stakeholders that can be leveraged to advance these systems into future real-world hazard management scenarios. The research team develops new technological approaches that results in mobilizing more intelligent, automated "eyes and ears on the ground." Outreach efforts include: (i) integration of the activities with practitioners; (ii) Seminars/webcasts to audiences like environmental engineers and first responders; (iii) Annual technology day camps to attract middle-schoolers from under-represented groups to engineering; (iv) Demonstrations to local K-12 institutions; (v) Inclusion of the project themes to the regular curricula; and (vi) International collaborations.&lt;br/&gt;&lt;br/&gt;This project introduces Robotics 2.0; a framework that targets autonomous robots that are co-workers and co-protectors, adapting to and working with humans. The research team develops a Cyber-Control Network (CCN) to allow multiple fixed and mobile robotic environmental sensing and measurements to adapt quickly to the changing environment by dynamically linking sub-networks of actuation, sensing, and control together. The design of such CCN ControlWare, and compressive sensing architectures, could be adapted to other large-scale problems beyond disaster response, mitigation, and management, such as power grid monitoring and reconfiguration, or regional urban traffic operations to respond to traffic congestion and incidents. The robotic sensing platforms do not require a-priori knowledge of the hazardous and dynamically changing environments they are monitoring. The Robotics 2.0 framework allows to swiftly respond, to prepare, and to manage various types of disasters.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1426840</AwardID>
<Investigator>
<FirstName>R. Vijay</FirstName>
<LastName>Kumar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>R. Vijay Kumar</PI_FULL_NAME>
<EmailAddress>Kumar@seas.upenn.edu</EmailAddress>
<PI_PHON>2158983630</PI_PHON>
<NSF_ID>000280506</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046389</ZipCode>
<StreetAddress><![CDATA[3330 Walnut Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~700000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Innovative technological advances are crucial for successful deployment of robots in disaster scenarios. Varying topics including, but not limited to, localization, mapping, planning, and trajectory generation require significant advances to guarantee robustness of such systems. Our work has advanced the state of the art in robots in several of these key areas.</p> <p dir="ltr"><span>In terms of localization, the work on visual odometry by fusing information from 3-D lidar and cameras has enabled robots for full autonomous inspection of completely dark, featureless challenging environments, which were never explored before. The work on robust state estimation and control using only a monocular camera and an IMU has enabled robots to navigate at speeds of 4.5 m/s, accelerations of over 1.5 g, roll and pitch angles of up to 90 degrees, and angular rate of up to 800 degrees/s without requiring any structure in the environment. Such aggressive maneuvers have shown the reliability of our algorithms when stress tested.&nbsp;&nbsp;</span></p> <p dir="ltr"><span>Mapping: Using our autonomous 3D mapping system, a single ground robot is able to achieve near-human level performance when mapping a large hallway environment at the University of Pennsylvania. &nbsp;This is even more impressive given that the human has prior knowledge of the floor plan of the environment while the robot does not. The work on dense 3D mapping has resulted in more accurate representation of the uncertainty of the map, and is able to fill the gaps and produce a map that is reasonably denser. The work on active mapping with noisy sensors has resulted in accurate representation of the uncertainty of the map, and is able to fill the gaps and produce a map that is reasonably denser and more informative. Not limiting to single robot cases, we have developed the first multi-robot, active information gathering algorithm based on random finite sets, a mathematical tool that allows us to simultaneously estimate the number of objects and to track the states of the objects of interest. The work on active information gathering has provided novel approximations that enable real-time exploration with a team of robots in two key scenarios: 1) active detection, localization, and tracking of an unknown number of objects and 2) active mapping on an unknown environment.</span></p> <p dir="ltr"><span>Planning and trajectory generation: The work on trajectory generation has enabled robots with complex dynamics to generate trajectories in real time to fly through cluttered environments. The work on planning of dynamically feasible three-dimensional trajectories in real time enable robots to fly through cluttered environments and flights through narrow windows. The work on motion planning with uncertainty has resulted in solving a practical size real world problem in realtime. The work on deterministic motion planning has enabled planning for robots with limited FOV, planning in SE(3) and in dynamic environments.</span></p> <p dir="ltr"><span>In addition to contributions to the individual key areas we have successfully developed complete robotics systems. All of these systems have been extensively tested in software simulations, in laboratory environments as well as &nbsp;validated in real world scenarios.  We have conducted fully autonomous inspections of Pennstocks (Glen Canyon Dam, AZ,  Center Hill Dam, TN)  in completely dark, featureless challenging environments. Also, inspections of mockup Nuclear reactors have been conducted using micro aerial vehicles. Further, in future we will be deploying robots in collaboration with Tokyo Electric and Power Company (TEPCO) in the damaged Fukushima Nuclear reactors to locate radioactive debris. Experiments using these techniques had previously been lacking in the robotics community.</span></p> <p dir="ltr"><span>Not only have our algorithms been deployed on bulky ground robots and large UAVs, we showed the possibility to concurrently run, onboard micro aerial platform with limited computational capability (comparable to smartphones), the state estimation, control, mapping and planning algorithms with obstacle avoidance to solve a complex inspection tasks.</span></p> <p dir="ltr"><span>Additionally, this grant has supported and trained 13 doctoral students and 2 postdoctoral scholars over the last three years.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/24/2018<br>      Modified by: R. Vijay&nbsp;Kumar</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1426840/1426840_10334466_1540314350795_Image1_2018NRIOutcomes--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1426840/1426840_10334466_1540314350795_Image1_2018NRIOutcomes--rgov-800width.jpg" title="Obstacle avoidance using a stereo camera even with a relatively small stereo baseline"><img src="/por/images/Reports/POR/2018/1426840/1426840_10334466_1540314350795_Image1_2018NRIOutcomes--rgov-66x44.jpg" alt="Obstacle avoidance using a stereo camera even with a relatively small stereo baseline"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The UAS must navigate an obstacle course, starting from a predefined location on the floor and given a single waypoint located at the far end of the obstacle field. Figure shows the obstacle course and the generated map of one of the multiple obstacle avoidance test trials.</div> <div class="imageCredit">Dinesh Thakur</div> <div class="imageSubmitted">R. Vijay&nbsp;Kumar</div> <div class="imageTitle">Obstacle avoidance using a stereo camera even with a relatively small stereo baseline</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1426840/1426840_10334466_1540316100653_NRI2018_Image2--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1426840/1426840_10334466_1540316100653_NRI2018_Image2--rgov-800width.jpg" title="Autonomous navigation in obstacle filled mockup PCV pedestal"><img src="/por/images/Reports/POR/2018/1426840/1426840_10334466_1540316100653_NRI2018_Image2--rgov-66x44.jpg" alt="Autonomous navigation in obstacle filled mockup PCV pedestal"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Top L: PCV pedestal & control rod drive ramp mock-up. Top R: Setup w/unknown obstacles. Bott L: Test scenario of obstacles at unknown locations in pedestal: red is original trajectory as if no obstacles in path; green is state estimate including replanning phases. Bott R: The generated obstacle map.</div> <div class="imageCredit">Dinesh Thakur</div> <div class="imageSubmitted">R. Vijay&nbsp;Kumar</div> <div class="imageTitle">Autonomous navigation in obstacle filled mockup PCV pedestal</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Innovative technological advances are crucial for successful deployment of robots in disaster scenarios. Varying topics including, but not limited to, localization, mapping, planning, and trajectory generation require significant advances to guarantee robustness of such systems. Our work has advanced the state of the art in robots in several of these key areas. In terms of localization, the work on visual odometry by fusing information from 3-D lidar and cameras has enabled robots for full autonomous inspection of completely dark, featureless challenging environments, which were never explored before. The work on robust state estimation and control using only a monocular camera and an IMU has enabled robots to navigate at speeds of 4.5 m/s, accelerations of over 1.5 g, roll and pitch angles of up to 90 degrees, and angular rate of up to 800 degrees/s without requiring any structure in the environment. Such aggressive maneuvers have shown the reliability of our algorithms when stress tested.   Mapping: Using our autonomous 3D mapping system, a single ground robot is able to achieve near-human level performance when mapping a large hallway environment at the University of Pennsylvania.  This is even more impressive given that the human has prior knowledge of the floor plan of the environment while the robot does not. The work on dense 3D mapping has resulted in more accurate representation of the uncertainty of the map, and is able to fill the gaps and produce a map that is reasonably denser. The work on active mapping with noisy sensors has resulted in accurate representation of the uncertainty of the map, and is able to fill the gaps and produce a map that is reasonably denser and more informative. Not limiting to single robot cases, we have developed the first multi-robot, active information gathering algorithm based on random finite sets, a mathematical tool that allows us to simultaneously estimate the number of objects and to track the states of the objects of interest. The work on active information gathering has provided novel approximations that enable real-time exploration with a team of robots in two key scenarios: 1) active detection, localization, and tracking of an unknown number of objects and 2) active mapping on an unknown environment. Planning and trajectory generation: The work on trajectory generation has enabled robots with complex dynamics to generate trajectories in real time to fly through cluttered environments. The work on planning of dynamically feasible three-dimensional trajectories in real time enable robots to fly through cluttered environments and flights through narrow windows. The work on motion planning with uncertainty has resulted in solving a practical size real world problem in realtime. The work on deterministic motion planning has enabled planning for robots with limited FOV, planning in SE(3) and in dynamic environments. In addition to contributions to the individual key areas we have successfully developed complete robotics systems. All of these systems have been extensively tested in software simulations, in laboratory environments as well as  validated in real world scenarios.  We have conducted fully autonomous inspections of Pennstocks (Glen Canyon Dam, AZ,  Center Hill Dam, TN)  in completely dark, featureless challenging environments. Also, inspections of mockup Nuclear reactors have been conducted using micro aerial vehicles. Further, in future we will be deploying robots in collaboration with Tokyo Electric and Power Company (TEPCO) in the damaged Fukushima Nuclear reactors to locate radioactive debris. Experiments using these techniques had previously been lacking in the robotics community. Not only have our algorithms been deployed on bulky ground robots and large UAVs, we showed the possibility to concurrently run, onboard micro aerial platform with limited computational capability (comparable to smartphones), the state estimation, control, mapping and planning algorithms with obstacle avoidance to solve a complex inspection tasks. Additionally, this grant has supported and trained 13 doctoral students and 2 postdoctoral scholars over the last three years.          Last Modified: 10/24/2018       Submitted by: R. Vijay Kumar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
