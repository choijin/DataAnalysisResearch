<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>115000.00</AwardTotalIntnAmount>
<AwardAmount>115000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The era of performance scaling by increasing the performance of&lt;br/&gt;individual processors is over, having been replaced by the era of&lt;br/&gt;massive parallelism via multiple cores.  Amdahl's law tells us that&lt;br/&gt;our ability to parallelize computation is limited by the inherently&lt;br/&gt;sequential portion of a computation.  This unfortunate combination&lt;br/&gt;of facts paints a bleak picture for the future of scalable software.&lt;br/&gt;This work explores a radical new approach to parallelism with the&lt;br/&gt;potential to bypass Amdahl's Law. The approach used involves making&lt;br/&gt;informed predictions about computation likely to happen in the&lt;br/&gt;future, proactively executing likely computations in parallel with&lt;br/&gt;the actual computation, and then "jumping forward in time" if the&lt;br/&gt;actual execution stumbles upon any of the predicted computations&lt;br/&gt;that have already been completed.  This research touches many areas&lt;br/&gt;within Computer Science, i.e., architecture, compilers, machine learning,&lt;br/&gt;systems, and theory.  Additionally, exploiting massively parallel&lt;br/&gt;computation will produce immediate returns in multiple scientific&lt;br/&gt;fields that rely on computation.  The research here provides an&lt;br/&gt;approach to speedup on such real-world problems.&lt;br/&gt;&lt;br/&gt;The approach used in this research views computational execution&lt;br/&gt;as moving a system through the enormously high dimensional space&lt;br/&gt;represented by its registers and memory of a conventional single-threaded&lt;br/&gt;processor.  It uses machine learning algorithms to observe execution&lt;br/&gt;patterns to make predictions about likely future states of the&lt;br/&gt;computation.  Based on these predictions, the system launches&lt;br/&gt;potentially large numbers of speculative threads to execute from&lt;br/&gt;these likely computations, while the actual computation proceeds&lt;br/&gt;serially.  At strategically chosen points, the main computation&lt;br/&gt;queries the speculative executions to determine if any of the&lt;br/&gt;completed computation is useful; if it is, the main thread uses the&lt;br/&gt;speculative computation to immediately begin execution where the&lt;br/&gt;speculative computation left off, achieving a speed-up over the&lt;br/&gt;serial execution.  This approach has the potential to be infinitely&lt;br/&gt;scalable: the more cores, memory, and communication bandwidth&lt;br/&gt;available, the greater the potential for performance improvement.&lt;br/&gt;The approach also scales across programs -- if the program running&lt;br/&gt;today happens upon a state encountered by a program running yesterday,&lt;br/&gt;the program can reuse yesterday's computation.</AbstractNarration>
<MinAmdLetterDate>08/26/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1438983</AwardID>
<Investigator>
<FirstName>Margo</FirstName>
<LastName>Seltzer</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Margo I Seltzer</PI_FULL_NAME>
<EmailAddress>margo@eecs.harvard.edu</EmailAddress>
<PI_PHON>6174965663</PI_PHON>
<NSF_ID>000146422</NSF_ID>
<StartDate>08/26/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Brooks</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David M Brooks</PI_FULL_NAME>
<EmailAddress>dbrooks@eecs.harvard.edu</EmailAddress>
<PI_PHON>6174953989</PI_PHON>
<NSF_ID>000091383</NSF_ID>
<StartDate>08/26/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ryan</FirstName>
<LastName>Adams</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ryan P Adams</PI_FULL_NAME>
<EmailAddress>rpa@princeton.edu</EmailAddress>
<PI_PHON>6092584651</PI_PHON>
<NSF_ID>000623159</NSF_ID>
<StartDate>08/26/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021382933</ZipCode>
<StreetAddress><![CDATA[33 Oxford Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~115000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>For three decades, computer users have enjoyed rapid and steady increase in computer performance due to the development of faster processors, via clock scaling.&nbsp; Sadly, this scaling ground to a halt in 2003.&nbsp; Since then, newer processors have increased compute power by increasing the number of parallel compute engines.&nbsp; The implication of this transition is clear: if software is to run faster, we <em>must</em> leverage parallel computing hardware with current software.&nbsp; Simply waiting for new, parallel versions of programs is impractical: it takes too long, and in many cases, it is not known how to parallelize today's software.</p> <p>This work presents a radically new approach to the problem of parallelizing software: an architecture, the Automatically Scalable Computational Architecture (ASC), that exploits the structure inherent in computation to predict pieces of computation that might be useful in the future, proactively execute those computations (using parallel hardware resources), and then use those computations later, if the predictions turned out to be correct. Using the system's probability model, it is possible to balance expected prediction accuracy against against various costs, such as the price of electricity or the utility of other work that can be offloaded to other cores. Thus, this approach has the attractive property that it can make use of available resources in a principled way -- if there is only a single execution engine available, programs run exactly as they normally do; if there are a few parallel engines, programs may run a few times faster or if prediction accuracy is not sufficiently high, ASC can choose not to use them; if there are hundreds or thousands of parallel engines available and ASC has a model with high accuracy, programs may run hundreds of times faster.</p> <p>ASC is predicated on a model of computation that views program execution as a walk through the enormous state space composed of the memory and registers of a single-threaded processor.&nbsp; Each instruction execution in this model moves the system from its current point in the state space to a specific subsequent point. Proactively executing likely computation equates to predicting some state in this space that the computation is likely to encounter, executing from that state, and then storing the result of that computation in such a way that it can be efficiently found when it is later needed.</p> <p>Under this one-year grant, researchers successfully demonstrated that this approach can produce real speedup on unmodified programs. That is, for some programs, a user runs the program under the guidance of ASC, and if there are parallel resources available, the program completes more quickly than it otherwise would. They accomplished this objective by developing an optimization framework that analyzes program execution to identify places in a program that are relatively easier to predict. The prototype system then gathers samples of the program state each time it encounters one of these attractive locations. These samples serve as training data for a deep neural network, which can be trained on a parallel engine that would otherwise go idle or offloaded to a custom deep neural network accelerator. Once the network has been trained, the prototype uses the resulting model to predict likely future states and then begins executing the program from these states on other available resources. If the program ever encounters the predicted states, the program leaps forward in time to the point the parallel computation reached.&nbsp; It is this exploitation of the predicted executions leading to the "fast-forwarding" that produces speedup.</p> <p>The prototype developed is the result of research breakthroughs in the application of optimization approaches to execution analysis, efficient representation of very large vectors of binary values, rapid program che...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ For three decades, computer users have enjoyed rapid and steady increase in computer performance due to the development of faster processors, via clock scaling.  Sadly, this scaling ground to a halt in 2003.  Since then, newer processors have increased compute power by increasing the number of parallel compute engines.  The implication of this transition is clear: if software is to run faster, we must leverage parallel computing hardware with current software.  Simply waiting for new, parallel versions of programs is impractical: it takes too long, and in many cases, it is not known how to parallelize today's software.  This work presents a radically new approach to the problem of parallelizing software: an architecture, the Automatically Scalable Computational Architecture (ASC), that exploits the structure inherent in computation to predict pieces of computation that might be useful in the future, proactively execute those computations (using parallel hardware resources), and then use those computations later, if the predictions turned out to be correct. Using the system's probability model, it is possible to balance expected prediction accuracy against against various costs, such as the price of electricity or the utility of other work that can be offloaded to other cores. Thus, this approach has the attractive property that it can make use of available resources in a principled way -- if there is only a single execution engine available, programs run exactly as they normally do; if there are a few parallel engines, programs may run a few times faster or if prediction accuracy is not sufficiently high, ASC can choose not to use them; if there are hundreds or thousands of parallel engines available and ASC has a model with high accuracy, programs may run hundreds of times faster.  ASC is predicated on a model of computation that views program execution as a walk through the enormous state space composed of the memory and registers of a single-threaded processor.  Each instruction execution in this model moves the system from its current point in the state space to a specific subsequent point. Proactively executing likely computation equates to predicting some state in this space that the computation is likely to encounter, executing from that state, and then storing the result of that computation in such a way that it can be efficiently found when it is later needed.  Under this one-year grant, researchers successfully demonstrated that this approach can produce real speedup on unmodified programs. That is, for some programs, a user runs the program under the guidance of ASC, and if there are parallel resources available, the program completes more quickly than it otherwise would. They accomplished this objective by developing an optimization framework that analyzes program execution to identify places in a program that are relatively easier to predict. The prototype system then gathers samples of the program state each time it encounters one of these attractive locations. These samples serve as training data for a deep neural network, which can be trained on a parallel engine that would otherwise go idle or offloaded to a custom deep neural network accelerator. Once the network has been trained, the prototype uses the resulting model to predict likely future states and then begins executing the program from these states on other available resources. If the program ever encounters the predicted states, the program leaps forward in time to the point the parallel computation reached.  It is this exploitation of the predicted executions leading to the "fast-forwarding" that produces speedup.  The prototype developed is the result of research breakthroughs in the application of optimization approaches to execution analysis, efficient representation of very large vectors of binary values, rapid program checkpoint and restart, architectural integration of neural networks, and high performance neural network training.   One partic...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
