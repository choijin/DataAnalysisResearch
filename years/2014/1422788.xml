<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR :Small: Exploiting Slowdowns for Speedup in Power-Scalable HPC Systems.</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Advanced computing systems --- that support a wide variety of applications in fields such as economics, sciences, and medicine --- are increasingly being designed with energy efficiency considerations. An extant approach to energy management is to run the underlying processors and devices at varying voltage and frequency. Typically, the approaches push the devices to run as fast as possible within thermal limits using the premise that "faster is better or at least does no harm." There is growing evidence in the prevailing literature that "slower is sometimes better." For example, for benchmark applications such as IOZone, it has been observed that running the processors at a faster speed can lead to significant slowdowns in the overall execution time. At large scale, e.g., in the Amazon Web Services cloud, such performance loss can cost hundreds of thousands of dollars in CPU hours and waste precious energy often begotten from polluting fossil fuels. However, isolating the root cause of such slowdowns in today's complex systems at the scale of data centers is akin to finding a needle in a haystack. Performance is now a function of the complex interaction between application design, system resources, and the underlying hardware. Furthermore, power scaling makes the raw performance of the hardware a variable; thus, further confounding attempts to isolate slowdowns.&lt;br/&gt;&lt;br/&gt;This project builds novel technologies that identify, model and automate the minimization or elimination of slowdowns in parallel and distributed applications when power scaling is enabled. The key approach is fine-grain application and kernel instrumentation to develop in-depth analysis of the interaction between parallel and distributed applications and the software and hardware stack. The intellectual merit of this research involves three intermediate research goals: 1) Exhaustive testing and deep system and code analysis on a large class of applications and a diverse set of systems to classify and isolate the slowdown phenomenon due to power scaling; 2) Design, implementation, and validation of models of the critical paths of applications exhibiting sensitivity to slowdowns; and 3) Analysis of the resulting models and design, implementation, and validation of the automated, open-source, runtime optimization techniques to steer power scaling to minimize or eliminate slowdowns.&lt;br/&gt;&lt;br/&gt;Completion of the project will improve the performance and energy efficiency of advanced systems. Adoption of the resulting runtime tools will enable use of power scaling to save energy while simultaneously reducing time-to-solution for modern applications and systems. The resulting artifacts and technologies will contribute to U.S. competitiveness by addressing the challenge of building large-scale systems within power constraints. The educational activities will help produce diverse graduates with highly marketable skill sets. The integration of the research discoveries and software tools, which will be open source and made public, into the educational curriculum will help capture the interest of the next generation of computer scientists.</AbstractNarration>
<MinAmdLetterDate>08/06/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/06/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422788</AwardID>
<Investigator>
<FirstName>Kirk</FirstName>
<LastName>Cameron</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kirk W Cameron</PI_FULL_NAME>
<EmailAddress>cameron@cs.vt.edu</EmailAddress>
<PI_PHON>5402314238</PI_PHON>
<NSF_ID>000194453</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ali</FirstName>
<LastName>Butt</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ali Butt</PI_FULL_NAME>
<EmailAddress>butta@cs.vt.edu</EmailAddress>
<PI_PHON>5402310489</PI_PHON>
<NSF_ID>000288467</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Virginia Polytechnic Institute and State University</Name>
<CityName>BLACKSBURG</CityName>
<ZipCode>240610001</ZipCode>
<PhoneNumber>5402315281</PhoneNumber>
<StreetAddress>Sponsored Programs 0170</StreetAddress>
<StreetAddress2><![CDATA[300 Turner Street NW, Suite 4200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003137015</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VIRGINIA POLYTECHNIC INSTITUTE AND STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003137015</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Virginia Polytechnic Institute and State University]]></Name>
<CityName/>
<StateCode>VA</StateCode>
<ZipCode>240610001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-879d1f4f-fb8e-f980-932a-276c34c614d7"> </span></p> <p dir="ltr"><span>This project has identified the causal relationships of I/O slowdowns due to changes in processor speeds. Over the course of the project, we have identified and altered the performance of I/O and storage systems using the increased understanding garnered in our research. Our techniques will be adopted by the Linux community to improve I/O performance impacting hundreds of thousands of users and the data centers that rely on Linux.</span></p> <p dir="ltr"><span>Using advanced computer system techniques, we have identified ways to cause slowdown with&nbsp;mem+cpu+thread throttling that has led to fundamental insights. Fundamental principles for consideration in future work transcend the computer science discipline. 1) The performance overlap in parallel systems and applications has grown in impact and importance and must be modelled in isolation in contrast to traditional approaches. &nbsp;2) Variability threatens experimentally-driven application and system design practices and must be addressed.&nbsp;</span></p> <p dir="ltr"><span>In this project, we have created technologies and more than a dozen artifacts released to the public in the form of peer reviewed publications and open source software. Our techniques will lead to more efficient deployments of exascale and cloud infrastructures worldwide and more efficient systems software designs. Our techniques become increasingly important in the context of data center efficiency and achieving exascale capacity since they reduce waste and enable use of key energy saving technologies like mem+cpu+thread throttling. This impacts any scientists or commercial endeavors that rely on data centers. This includes nearly everyone on the planet ultimately.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 11/26/2017<br>      Modified by: Kirk&nbsp;W&nbsp;Cameron</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   This project has identified the causal relationships of I/O slowdowns due to changes in processor speeds. Over the course of the project, we have identified and altered the performance of I/O and storage systems using the increased understanding garnered in our research. Our techniques will be adopted by the Linux community to improve I/O performance impacting hundreds of thousands of users and the data centers that rely on Linux. Using advanced computer system techniques, we have identified ways to cause slowdown with mem+cpu+thread throttling that has led to fundamental insights. Fundamental principles for consideration in future work transcend the computer science discipline. 1) The performance overlap in parallel systems and applications has grown in impact and importance and must be modelled in isolation in contrast to traditional approaches.  2) Variability threatens experimentally-driven application and system design practices and must be addressed.  In this project, we have created technologies and more than a dozen artifacts released to the public in the form of peer reviewed publications and open source software. Our techniques will lead to more efficient deployments of exascale and cloud infrastructures worldwide and more efficient systems software designs. Our techniques become increasingly important in the context of data center efficiency and achieving exascale capacity since they reduce waste and enable use of key energy saving technologies like mem+cpu+thread throttling. This impacts any scientists or commercial endeavors that rely on data centers. This includes nearly everyone on the planet ultimately.          Last Modified: 11/26/2017       Submitted by: Kirk W Cameron]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
