<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Medium: Energy Efficient Memory Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>800000.00</AwardTotalIntnAmount>
<AwardAmount>800000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to develop energy efficient systems. Improving energy efficiency is a defining challenge for information technology and the prerequisite to increasing the capabilities of all computing systems, from smartphones to warehouse-scale data-centers. Most research in this area has focused on energy efficient computing, using specialized cores or near-threshold voltage circuits. To achieve end-to-end energy efficiency, the on-chip and off-chip memory system that feeds cores with data and instructions must also be optimized. The memory system includes large, leaking structures and communication operations that introduce energy overheads orders of magnitude higher than the overheads of compute operations.&lt;br/&gt;&lt;br/&gt;This project proposes a holistic approach towards energy efficient memory systems that rethinks memory system architecture, dynamic runtime management, and circuit design. At the architecture level, it will optimize for emerging, data-centric applications with limited temporal locality by placing computing close to the memory structures so that energy intensive communication is minimized.  It will also explore architectural support for specialization in the memory system, such as engines for specialized prefetch, data transformations, and data distribution. At the runtime management level, it will investigate scalable scheduling algorithms that minimize energy usage in the memory system by maximizing temporal and spatial locality and the use of on-chip and memory-side accelerators. At the circuit design level, it will aggressively optimize the energy consumption of the internal structures of memory chips and memory stacks for the dominant access patterns after efficient runtime management. Finally, this project will create tools for joint exploration of the architecture-management-scheduling space in order to identify Pareto optimal memory systems for different levels of performance.</AbstractNarration>
<MinAmdLetterDate>06/03/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1408911</AwardID>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Horowitz</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark A Horowitz</PI_FULL_NAME>
<EmailAddress>horowitz@stanford.edu</EmailAddress>
<PI_PHON>6507253707</PI_PHON>
<NSF_ID>000235625</NSF_ID>
<StartDate>06/03/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christoforos</FirstName>
<LastName>Kozyrakis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christoforos Kozyrakis</PI_FULL_NAME>
<EmailAddress>kozyraki@stanford.edu</EmailAddress>
<PI_PHON>6507253716</PI_PHON>
<NSF_ID>000486618</NSF_ID>
<StartDate>06/03/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>943054100</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~800000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 5">Information technology (IT) is now a major catalyst for innovation across all aspects of human endeavor. An increasing amount of IT is hosted in the cloud, primarily due to the cost and scalability benefits for both the end-users and the operators of the warehouse-scale datacenters (DC) that host cloud services. Hence, it is vital to continuously improve the capabilities and efficiency of datacenters. In the past ten years, capability has improved by increasing the number servers in a DC and the bandwidth of the network that connects them. Cost and energy efficiency improved by eliminating the high overheads of the power delivery and cooling infrastructure. To achieve further improvements, we must now examine how well we are utilizing the servers themselves, which are the primary determinant for DC performance, cost, and energy efficiency. This is particularly important since the semiconductor chips used in servers are now energy limited and their efficiency does not scale as fast as in the past.</div> <div class="page" title="Page 5"> <div class="page" title="Page 5"> <div class="layoutArea"> <div class="column"> <p>&nbsp;</p> <p><span>The goal of this proposal has been to increase DC resource efficiency, hence improving both capability and cost effectiveness. Sharing compute servers, memory, and storage devices across mul- tiple services is the prevailing approach for resource efficient DCs. However, sharing resources introduces interference, which can be detrimental to performance especially for latency-critical services with strict quality-of-service guarantees (QoS). The difficulty of identifying and reducing interference keeps some DC operators from sharing resources and discourages others from aggressive sharing in the presence of latency- critical services. Consequently, DC server utilization is typically at 10%-20% or even lower. </span></p> </div> </div> </div> </div> <div class="page" title="Page 5">The project used&nbsp;a cross-layer approach towards improving resource efficiency in datacenters that host latency-critical services. The specific outcomes of this project were the following:</div> <div class="page" title="Page 5">- A feedback-based controller, Heracles, that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks.</div> <div class="page" title="Page 5">- A&nbsp;distributed scheduler, Tarcil, that targets both scheduling speed and quality. Tarcil uses an analytically derived sampling framework that adjusts the sample size based on load, and provides statistical guarantees on the quality of allocated resources. It also implements admission control when sampling is unlikely to find suitable resources. This makes it appropriate for large, shared clusters hosting short- and long-running jobs.&nbsp;</div> <div class="page" title="Page 5">- The set of OS mechanisms and dynamic control needed to adjust core allocation and voltage/frequency settings based on the measured delays for latency-critical workloads. This allows for energy proportionality and frees the maximum amount of resources per server for other background applications, while respecting service-level objectives.&nbsp;</div> <div class="page" title="Page 5">- An in depth examination of Flash disaggregation as a way to deal with Flash overprovisioning.&nbsp; We showed that, while remote Flash access introduces a 20% throughput drop at the application level, disaggregation allows us to make up for these overheads through resource-efficient scale-out. Hence, Flash disaggregation allows scaling CPU and Flash resources independently in a cost effective manner.&nbsp;</div> <div class="page" title="Page 5">- A&nbsp;software-based system for remote Flash access, ReFlex, that provides nearly identical performance to accessing local Flash. ReFlex uses a dataplane kernel to closely integrate networking and storage processing to achieve low latency and high throughput at low resource requirements. Specifically, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, while adding 21&micro;s over direct access to local Flash. ReFlex uses a QoS scheduler that can enforce tail latency and throughput service-level objectives (SLOs) for thousands of remote clients.&nbsp;</div> <div class="page" title="Page 5">- A tool (Selecta) that recommends nearoptimal configurations of cloud compute and storage resources for data analytics workloads. Selecta uses latent factor collaborative filtering to predict how an application will perform across different configurations, based on sparse data collected by profiling training workloads. We evaluate Selecta with over one hundred Spark SQL and ML applications, showing that Selecta chooses a near-optimal performance configuration (within 10% of optimal) with 94% probability and a near-optimal cost configuration with 80% probability.&nbsp;</div> <div class="page" title="Page 5">- An in-depth exploration of the suitability of different cloud storage services as remote storage for serverless analytics. Our analysis leads to key insights to guide the design of an ephemeral cloud storage system, including the performance and cost efficiency of Flash storage for serverless application requirements and the need for a paywhat-you-use storage service that can support the high throughput demands of highly parallel applications.</div> <div class="page" title="Page 5"> <p>We open-sourced key systems we developed, including the ReFlex system for remote Flash access.&nbsp; These systems can help the broader community understand the tradeoffs between latency and efficiency and develop further techniques that optimize both.&nbsp;</p> <p>&nbsp;</p> </div><br> <p>            Last Modified: 08/14/2018<br>      Modified by: Christoforos&nbsp;Kozyrakis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Information technology (IT) is now a major catalyst for innovation across all aspects of human endeavor. An increasing amount of IT is hosted in the cloud, primarily due to the cost and scalability benefits for both the end-users and the operators of the warehouse-scale datacenters (DC) that host cloud services. Hence, it is vital to continuously improve the capabilities and efficiency of datacenters. In the past ten years, capability has improved by increasing the number servers in a DC and the bandwidth of the network that connects them. Cost and energy efficiency improved by eliminating the high overheads of the power delivery and cooling infrastructure. To achieve further improvements, we must now examine how well we are utilizing the servers themselves, which are the primary determinant for DC performance, cost, and energy efficiency. This is particularly important since the semiconductor chips used in servers are now energy limited and their efficiency does not scale as fast as in the past.         The goal of this proposal has been to increase DC resource efficiency, hence improving both capability and cost effectiveness. Sharing compute servers, memory, and storage devices across mul- tiple services is the prevailing approach for resource efficient DCs. However, sharing resources introduces interference, which can be detrimental to performance especially for latency-critical services with strict quality-of-service guarantees (QoS). The difficulty of identifying and reducing interference keeps some DC operators from sharing resources and discourages others from aggressive sharing in the presence of latency- critical services. Consequently, DC server utilization is typically at 10%-20% or even lower.      The project used a cross-layer approach towards improving resource efficiency in datacenters that host latency-critical services. The specific outcomes of this project were the following: - A feedback-based controller, Heracles, that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. - A distributed scheduler, Tarcil, that targets both scheduling speed and quality. Tarcil uses an analytically derived sampling framework that adjusts the sample size based on load, and provides statistical guarantees on the quality of allocated resources. It also implements admission control when sampling is unlikely to find suitable resources. This makes it appropriate for large, shared clusters hosting short- and long-running jobs.  - The set of OS mechanisms and dynamic control needed to adjust core allocation and voltage/frequency settings based on the measured delays for latency-critical workloads. This allows for energy proportionality and frees the maximum amount of resources per server for other background applications, while respecting service-level objectives.  - An in depth examination of Flash disaggregation as a way to deal with Flash overprovisioning.  We showed that, while remote Flash access introduces a 20% throughput drop at the application level, disaggregation allows us to make up for these overheads through resource-efficient scale-out. Hence, Flash disaggregation allows scaling CPU and Flash resources independently in a cost effective manner.  - A software-based system for remote Flash access, ReFlex, that provides nearly identical performance to accessing local Flash. ReFlex uses a dataplane kernel to closely integrate networking and storage processing to achieve low latency and high throughput at low resource requirements. Specifically, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, while adding 21&micro;s over direct access to local Flash. ReFlex uses a QoS scheduler that can enforce tail latency and throughput service-level objectives (SLOs) for thousands of remote clients.  - A tool (Selecta) that recommends nearoptimal configurations of cloud compute and storage resources for data analytics workloads. Selecta uses latent factor collaborative filtering to predict how an application will perform across different configurations, based on sparse data collected by profiling training workloads. We evaluate Selecta with over one hundred Spark SQL and ML applications, showing that Selecta chooses a near-optimal performance configuration (within 10% of optimal) with 94% probability and a near-optimal cost configuration with 80% probability.  - An in-depth exploration of the suitability of different cloud storage services as remote storage for serverless analytics. Our analysis leads to key insights to guide the design of an ephemeral cloud storage system, including the performance and cost efficiency of Flash storage for serverless application requirements and the need for a paywhat-you-use storage service that can support the high throughput demands of highly parallel applications.   We open-sourced key systems we developed, including the ReFlex system for remote Flash access.  These systems can help the broader community understand the tradeoffs between latency and efficiency and develop further techniques that optimize both.            Last Modified: 08/14/2018       Submitted by: Christoforos Kozyrakis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
