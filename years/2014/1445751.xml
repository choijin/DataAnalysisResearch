<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative Research: Articulate: Augmenting Data Visualization With Natural Language Interaction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>241521.00</AwardTotalIntnAmount>
<AwardAmount>253521</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Nearly one third of the human brain is devoted to processing visual information. Vision is the dominant sense for the acquisition of information from our everyday world.  It is therefore no surprise that visualization, even in its simplest forms, remains the most effective means for converting large volumes of raw data into insight, a process that can support scientific discovery. However a key challenge hindering scientific users from adopting the latest visualization tools and techniques is the steep learning curve that has to be overcome in order to make use of them. The tendency then is to resort to the simplest tools, such as bar charts and line graphs, even though they may lack the expressive power necessary to bring scientific data into focus.&lt;br/&gt;&lt;br/&gt;The notion that scientists would ideally like to simply speak with a computer to ask questions about their data, and have the computer automatically generate visualizations that answer their queries, has been well known since at least the NSF 2007 report "Enabling Science Discoveries through Visual Exploration."  This is the motivation for the current project, which involves a collaboration among researchers  at two institutions, given that scientists still are unable to do so.  The PIs' ultimate goal is to implement a Virtual Visualization Expert to translate the language of science into the language of visualization.  To demonstrate the concept is indeed viable, the PIs previously developed and evaluated a small prototype, which supported their argument that by relieving the user of the burden of having to learn how to use a complex interface one could enable them to focus on articulating better scientific questions.&lt;br/&gt;&lt;br/&gt;Given this initial success, the focus of this exploratory research is to establish the foundations of a more generalizable approach that can encompass techniques used in scientific visualization.  To this end, the PIs will research the steps needed for mapping natural language requests, which may be accompanied by gestures, into meaningful visualizations and for enabling incremental creation and modifications of visualizations.  They will develop innovative models to understand the intent of the user and the objects s/he is referring to, and they will explore how best to design user interfaces for creating and modifying visualizations using language and direct manipulation.  The PIs' initial study showed that all these capabilities are crucial to enabling users to make the best use of a dialogic interface for data visualization.  Although project outcomes will be geared in the short term to serving the scientific community, the techniques should be applicable more broadly to consumers of information, such as citizen scientists, public policy decision makers, and students.</AbstractNarration>
<MinAmdLetterDate>06/30/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/05/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1445751</AwardID>
<Investigator>
<FirstName>Leland</FirstName>
<LastName>Wilkinson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leland Wilkinson</PI_FULL_NAME>
<EmailAddress>leland.wilkinson@gmail.com</EmailAddress>
<PI_PHON>3122200880</PI_PHON>
<NSF_ID>000154192</NSF_ID>
<StartDate>06/30/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Johnson</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew E Johnson</PI_FULL_NAME>
<EmailAddress>ajohnson@uic.edu</EmailAddress>
<PI_PHON>3129963002</PI_PHON>
<NSF_ID>000332025</NSF_ID>
<StartDate>06/30/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Barbara</FirstName>
<LastName>DiEugenio</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Barbara DiEugenio</PI_FULL_NAME>
<EmailAddress>bdieugen@uic.edu</EmailAddress>
<PI_PHON>3129967566</PI_PHON>
<NSF_ID>000192544</NSF_ID>
<StartDate>06/30/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606124305</ZipCode>
<PhoneNumber>3129962862</PhoneNumber>
<StreetAddress>809 S. Marshfield Avenue</StreetAddress>
<StreetAddress2><![CDATA[MB 502, M/C 551]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>098987217</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Chicago]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>606077128</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~241521</FUND_OBLG>
<FUND_OBLG>2015~12000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Humans communicate with each other naturally through speech and gesture. The goal of this project is to provide&nbsp; insights into how&nbsp; humans can create data visualizations purely through speech and gesture; and to develop&nbsp; computational models that embody those insights in order to develop&nbsp; a conversational interface to create visualizations. <br /><br />The outcomes of this project include experimental data in the form of videos, audio and transcripts of users interacting with a human visualization expert that creates visualization for them. The transcripts were manually annotated with features that allowed the researchers to derive patterns of user behaviors; some patterns were observational, and others, mined through machine learning algorithms. In turn, these patterns were embodied in&nbsp; the software we developed: this software enables the computer to understand spoken requests and pointing gestures to produce and manipulate visualizations of data.<br /><br />Outcomes also include research presentations and papers delivered at top internationally recognized conferences (SIGDIAL,&nbsp; SEMDIAL and&nbsp; IEEE Visualization). This project provided valuable research experience for three PhD students, among them two females; and four undergraduates.<br /><br />Going forward, the outcomes of this project will enable future intelligent computer software to be developed that will allow non-computer experts to make insightful discoveries with data through the use of natural speech commands and gestures, as if conversing with a human data analysis expert.</p><br> <p>            Last Modified: 11/09/2017<br>      Modified by: Barbara&nbsp;Dieugenio</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1445751/1445751_10314446_1510205162077_ScreenShot2015-09-04at2.12.16PM--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1445751/1445751_10314446_1510205162077_ScreenShot2015-09-04at2.12.16PM--rgov-800width.jpg" title="User study"><img src="/por/images/Reports/POR/2017/1445751/1445751_10314446_1510205162077_ScreenShot2015-09-04at2.12.16PM--rgov-66x44.jpg" alt="User study"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A user exploring data with visualizations on a large screen</div> <div class="imageCredit">Electronic Visualization Laboratory, University of Illinois at Chicago</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Barbara&nbsp;Dieugenio</div> <div class="imageTitle">User study</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1445751/1445751_10314446_1510205354672_example_sequence--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1445751/1445751_10314446_1510205354672_example_sequence--rgov-800width.jpg" title="Automatic creation of visualization"><img src="/por/images/Reports/POR/2017/1445751/1445751_10314446_1510205354672_example_sequence--rgov-66x44.jpg" alt="Automatic creation of visualization"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Articulate system interprets a sequence of three spoken requests</div> <div class="imageCredit">Electronic Visualization Laboratory, University of Illinois at Chicago</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Barbara&nbsp;Dieugenio</div> <div class="imageTitle">Automatic creation of visualization</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Humans communicate with each other naturally through speech and gesture. The goal of this project is to provide  insights into how  humans can create data visualizations purely through speech and gesture; and to develop  computational models that embody those insights in order to develop  a conversational interface to create visualizations.   The outcomes of this project include experimental data in the form of videos, audio and transcripts of users interacting with a human visualization expert that creates visualization for them. The transcripts were manually annotated with features that allowed the researchers to derive patterns of user behaviors; some patterns were observational, and others, mined through machine learning algorithms. In turn, these patterns were embodied in  the software we developed: this software enables the computer to understand spoken requests and pointing gestures to produce and manipulate visualizations of data.  Outcomes also include research presentations and papers delivered at top internationally recognized conferences (SIGDIAL,  SEMDIAL and  IEEE Visualization). This project provided valuable research experience for three PhD students, among them two females; and four undergraduates.  Going forward, the outcomes of this project will enable future intelligent computer software to be developed that will allow non-computer experts to make insightful discoveries with data through the use of natural speech commands and gestures, as if conversing with a human data analysis expert.       Last Modified: 11/09/2017       Submitted by: Barbara Dieugenio]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
