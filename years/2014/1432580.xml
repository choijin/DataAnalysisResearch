<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Research in Student Peer Review: A Cooperative Web-Services Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>214479.00</AwardTotalIntnAmount>
<AwardAmount>214479</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Talitha Washington</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Hundreds of thousands of students have used online peer review applications to review their classmates' work. While learning gains from peer review have been documented repeatedly, current systems do not always produce accurate scores and often give inadequate guidance to students about what constitutes a good review, resulting in haphazard feedback. This research addresses these issues with a common set of web services that can be used by any peer review system, as well as new visualizations that identify students' strengths and weaknesses, and gauge improvement over time.&lt;br/&gt;&lt;br/&gt;This project differs from previous research that typically involves a single peer review system. It will develop a set of web services that will be usable by any peer review system in the same way that Google Maps is available to any website that wants to display location data. This common implementation will allow the project team to gather data from large numbers of students in a wide variety of contexts, thereby giving us the statistical power to produce more convincing, highly generalizable results&lt;br/&gt;&lt;br/&gt;Peer review during the writing process is an example of formative assessment, feedback that is received while the recipient still has a chance to improve his/her work. Several studies have found that the students who benefit most from formative assessment are those who typically underperform as measured by exams and standardized tests. Formative assessment tends to level the playing field for underrepresented minorities by allowing these students to receive input from their peers when they are not stressed about how their grade is being affected.</AbstractNarration>
<MinAmdLetterDate>08/05/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1432580</AwardID>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Kidd</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer Kidd</PI_FULL_NAME>
<EmailAddress>jkidd@odu.edu</EmailAddress>
<PI_PHON>7576834293</PI_PHON>
<NSF_ID>000531475</NSF_ID>
<StartDate>08/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Old Dominion University Research Foundation</Name>
<CityName>Norfolk</CityName>
<ZipCode>235082561</ZipCode>
<PhoneNumber>7576834293</PhoneNumber>
<StreetAddress>4111 Monarch Way</StreetAddress>
<StreetAddress2><![CDATA[Suite 204]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>077945947</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OLD DOMINION UNIVERSITY RESEARCH FOUNDATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Old Dominion University]]></Name>
<CityName>Norfolk</CityName>
<StateCode>VA</StateCode>
<ZipCode>235090001</ZipCode>
<StreetAddress><![CDATA[5115 Hampton Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>1998</Code>
<Text>IUSE</Text>
</ProgramElement>
<ProgramReference>
<Code>8209</Code>
<Text>Improv Undergrad STEM Ed(IUSE)</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~214479</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The PeerLogic Project was a five-campus research and development effort focused on the improvement of undergraduate students? online peer review and assessment (OPRA) experiences. The investigators, who brought technical and pedagogical expertise from diverse fields including computer science, business, education, and art, worked collaboratively to achieve three major outcomes: a suite of web services that enhance the functionality of peer-assessment systems; an open repository of anonymized data from millions of reviews; and a body of research exploring online peer review tools, practices, and learning outcomes. The findings from well over a dozen publications have increased understanding of the factors impacting peer review and how OPRA systems can address user needs.</p> <p>The PeerLogic open source web services (<a href="https://www.peerlogic.org/">https://www.peerlogic.org/</a>) can be utilized by any online peer assessment system. This set of tools which help improve review quality, accuracy, and peer review experiences include:</p> <ul> <li>Sentiment Analysis, which analyzes the linguistic features of feedback comments;</li> <li>Reputation Services, which uses algorithms to determine the trustworthiness of reviewers; </li> <li>Team &amp; Topic Assignment, which creates teams based on individual users? ranking of topic preferences;</li> <li>Rubric Checker, which helps instructors improve the reliability of rubric criteria;</li> <li>Automated meta-reviewing, which gives feedback to the reviewer on how to improve their reviews;</li> <li>Summarization Services, which provide instructors and students summaries of all the reviews received for a particular artifact; and</li> <li>Data Visualizations: including Rainbow Graph, Tile Chart, and Bubble Chart, which represent the performance of students on assignments and reviews.</li> </ul> <p>The data warehouse is a free resource for researchers in peer assessment interested in examining OPRA practices across systems and institutions.</p> <p>At Old Dominion University (ODU), approximately 800 future PreK-12 teachers, 200 engineering students, and 175 biology students received training in how to provide effective feedback and practiced these skills&nbsp;by engaging in online peer review. Experience giving and receiving feedback is a critical skill for almost all professions, and accordingly these students are better prepared for workplace demands. The consistent use of online peer assessment systems provided an important feedback mechanism for the iterative development of the web services. Instructors at ODU provided extensive feedback on multiple features including: calibration, visualizations, sentiment analysis, summarization services, and general software revisions and updates.</p> <p>The investigators involved in the PeerLogic project collaboratively developed a systematic research framework and comprehensive survey describing the affordances and constraints of OPRA systems, informing educators and system developers about peer assessment use cases and design options. At ODU, research on online peer review and supporting tools has helped to clarify how students benefit from online peer review and how technology can best support the process. One study explored which type of peer review feedback (positive, negative or mixed) had the greatest effect on student learning. Some evidence indicated students who received negative feedback were slightly more likely to revise, but these findings were not conclusive and suggest other factors, perhaps internal to the student, are more predictive of a student?s decision to revise than the tone or content of the feedback they receive. Another study described two strategies to calibrate student ratings: a preliminary assignment designed to train students and calibration calculations used to identify competent reviewers. Calibration training exercises improved the quality of student comments. Following calibration, students tend to give more extensive and more constructive feedback. It was suggested that instructors use sample artifacts of low and high quality and to provide descriptions for each level of each criterion. A study of OPRA across engineering, biology, English, and education found students have positive perceptions of OPRA and report benefiting from participating in both anonymous and identifiable contexts. Results suggested students preferred anonymity, but valued aspects of the process that can more easily be facilitated through identifiable peer review such as high levels of student engagement, constructive rather than critical feedback, and accountability for reviewers. This study illuminated a need for technology-supported peer review features and pedagogy that enhance student motivation and peer review quality as well as instructional accountability for the significant role that student emotions play in student perceptions of OPRA. A related study investigated OPRA as a means to enhance student writing in engineering courses. Results suggest that peer review can increase student performance, as long as reflections are used to prompt student revision, regardless of the class delivery method or assignment type. Finally, undergraduate students were given the choice to participate in online peer review anonymously or in an identifiable group. Student choice of identity condition was found to be either intellectually motivated (e.g. wanting multiple perspectives, avoiding bias; ability to follow-up), emotionally motivated (e.g. seeking comfort, avoiding discomfort, seeking interaction), or superficially motivated (e.g. convenience). Significant differences were found in student perceptions of review quality. Student reviews in the anonymous condition were rated 10% lower than those in the identified condition.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/04/2018<br>      Modified by: Jennifer&nbsp;Kidd</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The PeerLogic Project was a five-campus research and development effort focused on the improvement of undergraduate students? online peer review and assessment (OPRA) experiences. The investigators, who brought technical and pedagogical expertise from diverse fields including computer science, business, education, and art, worked collaboratively to achieve three major outcomes: a suite of web services that enhance the functionality of peer-assessment systems; an open repository of anonymized data from millions of reviews; and a body of research exploring online peer review tools, practices, and learning outcomes. The findings from well over a dozen publications have increased understanding of the factors impacting peer review and how OPRA systems can address user needs.  The PeerLogic open source web services (https://www.peerlogic.org/) can be utilized by any online peer assessment system. This set of tools which help improve review quality, accuracy, and peer review experiences include:  Sentiment Analysis, which analyzes the linguistic features of feedback comments; Reputation Services, which uses algorithms to determine the trustworthiness of reviewers;  Team &amp; Topic Assignment, which creates teams based on individual users? ranking of topic preferences; Rubric Checker, which helps instructors improve the reliability of rubric criteria; Automated meta-reviewing, which gives feedback to the reviewer on how to improve their reviews; Summarization Services, which provide instructors and students summaries of all the reviews received for a particular artifact; and Data Visualizations: including Rainbow Graph, Tile Chart, and Bubble Chart, which represent the performance of students on assignments and reviews.   The data warehouse is a free resource for researchers in peer assessment interested in examining OPRA practices across systems and institutions.  At Old Dominion University (ODU), approximately 800 future PreK-12 teachers, 200 engineering students, and 175 biology students received training in how to provide effective feedback and practiced these skills by engaging in online peer review. Experience giving and receiving feedback is a critical skill for almost all professions, and accordingly these students are better prepared for workplace demands. The consistent use of online peer assessment systems provided an important feedback mechanism for the iterative development of the web services. Instructors at ODU provided extensive feedback on multiple features including: calibration, visualizations, sentiment analysis, summarization services, and general software revisions and updates.  The investigators involved in the PeerLogic project collaboratively developed a systematic research framework and comprehensive survey describing the affordances and constraints of OPRA systems, informing educators and system developers about peer assessment use cases and design options. At ODU, research on online peer review and supporting tools has helped to clarify how students benefit from online peer review and how technology can best support the process. One study explored which type of peer review feedback (positive, negative or mixed) had the greatest effect on student learning. Some evidence indicated students who received negative feedback were slightly more likely to revise, but these findings were not conclusive and suggest other factors, perhaps internal to the student, are more predictive of a student?s decision to revise than the tone or content of the feedback they receive. Another study described two strategies to calibrate student ratings: a preliminary assignment designed to train students and calibration calculations used to identify competent reviewers. Calibration training exercises improved the quality of student comments. Following calibration, students tend to give more extensive and more constructive feedback. It was suggested that instructors use sample artifacts of low and high quality and to provide descriptions for each level of each criterion. A study of OPRA across engineering, biology, English, and education found students have positive perceptions of OPRA and report benefiting from participating in both anonymous and identifiable contexts. Results suggested students preferred anonymity, but valued aspects of the process that can more easily be facilitated through identifiable peer review such as high levels of student engagement, constructive rather than critical feedback, and accountability for reviewers. This study illuminated a need for technology-supported peer review features and pedagogy that enhance student motivation and peer review quality as well as instructional accountability for the significant role that student emotions play in student perceptions of OPRA. A related study investigated OPRA as a means to enhance student writing in engineering courses. Results suggest that peer review can increase student performance, as long as reflections are used to prompt student revision, regardless of the class delivery method or assignment type. Finally, undergraduate students were given the choice to participate in online peer review anonymously or in an identifiable group. Student choice of identity condition was found to be either intellectually motivated (e.g. wanting multiple perspectives, avoiding bias; ability to follow-up), emotionally motivated (e.g. seeking comfort, avoiding discomfort, seeking interaction), or superficially motivated (e.g. convenience). Significant differences were found in student perceptions of review quality. Student reviews in the anonymous condition were rated 10% lower than those in the identified condition.          Last Modified: 12/04/2018       Submitted by: Jennifer Kidd]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
