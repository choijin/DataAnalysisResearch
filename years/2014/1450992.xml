<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Variance and Invariance in Voice Quality</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>239000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This EArly Grant for Exploratory Research aims at developing a database and model of voice source variability. A model of voice variations across people and speech tasks could improve the naturalness of speech synthesis systems. In addition, understanding what aspects of the voice, if any, are speaker-specific, should aid in developing better speaker identification and verification algorithms.  Knowing how much a person could change his or her voice quality without compromising their vocal identity, could also inform medical rehab applications. A better understanding of the human voice will, thus, be of significant impact scientifically, and for engineering and medical applications. The project has strong outreach and dissemination programs and fosters interdisciplinary activities in Electrical Engineering, Linguistics, and Speech and Hearing Science at UCLA. It will train undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance. &lt;br/&gt;&lt;br/&gt;This exploratory project will analyze and discover how the voice source varies within and across talkers under circumstances that introduce variability in everyday life situations. The project aims to address three questions: 1) Does an individual talker's voice source vary significantly across recording sessions and speech tasks?, 2)  Do bilingual talkers show more or less intra-talker variation when speaking in English?, and 3) Most importantly, how does intra-talker variability from all these sources compare with inter-talker variability?  Understanding these issues will require a high-quality speech database with multiple voice samples from many talkers (in this case 200) which will be collected and distributed to other researchers. Acoustic analyses will reveal inter- and intra-talker variability in the voice source across different situations by generating a multi-dimensional acoustic profile of each talker that specifies the range of parameter values that are typical in the corpus for that talker, and the likelihood of deviations from that usual profile.</AbstractNarration>
<MinAmdLetterDate>07/24/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/29/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1450992</AwardID>
<Investigator>
<FirstName>Patricia</FirstName>
<LastName>Keating</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patricia A Keating</PI_FULL_NAME>
<EmailAddress>keating@humnet.ucla.edu</EmailAddress>
<PI_PHON>3107946316</PI_PHON>
<NSF_ID>000092027</NSF_ID>
<StartDate>07/24/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Abeer</FirstName>
<LastName>Alwan</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abeer A Alwan</PI_FULL_NAME>
<EmailAddress>alwan@ee.ucla.edu</EmailAddress>
<PI_PHON>3102062231</PI_PHON>
<NSF_ID>000090848</NSF_ID>
<StartDate>07/24/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jody</FirstName>
<LastName>Kreiman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jody Kreiman</PI_FULL_NAME>
<EmailAddress>jkreiman@ucla.edu</EmailAddress>
<PI_PHON>3107940102</PI_PHON>
<NSF_ID>000441064</NSF_ID>
<StartDate>07/24/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>900952000</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~200000</FUND_OBLG>
<FUND_OBLG>2016~39000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A talker&rsquo;s voice quality conveys many kinds of information, including emotional state (happy, angry, etc.) and personal identity. As an example, please refer to Figure 1 which shows a spectrogram (which is a time-frequency display) of a person saying &lsquo;Go Bruins&rsquo; in a neutral and another spectrogram of the same person saying &lsquo;Go Bruins&rsquo; in an excited manner. Hence, significant within speaker (intra-speaker) and across speakers (inter-speaker) variability in voice occurs routinely during normal everyday speech. Understanding what aspects of a voice are talker-specific should aid in understanding the human limits in perceiving speaker differences and in developing better automatic speaker identification (SID) algorithms. Despite technological advances, the performance of current SID systems remains far from perfect and is worse than humans&rsquo; ability to identify speakers. The key questions that we aim to answer are: under normal daily life variability, how often does a talker sound less like him- or herself and more like someone else? Which properties in the signal account for speaker similarity? Can automatic speaker identification (SID) and verification algorithms be improved by knowledge of which properties are important for human perception of speaker similarity?</p> <p>&nbsp;</p> <p>Understanding these issues requires a high-quality speech database with multiple voice samples from many talkers (in our case 200) which we collected and analyzed with EAGER funding.</p> <p><strong>Outcomes:</strong></p> <ol> <li><strong>Produced a database of high-quality recordings. </strong>200 talkers provided a total of about 5-7 minutes of speech from multiple recording sessions and speech tasks. The database will be freely available to other researchers soon.</li> <li><strong>Analyzed speech signals </strong>and characterized inter- and intra-talker variability across different situations.</li> <li><strong>Conducted perceptual studies </strong>to determine how much variability can be tolerated before talkers cease to sound like themselves.</li> <li><strong>Automatic speaker verification experiments </strong>determined differences, in performance and strategies adopted, between humans and machines. The experiments showed that using features inspired by human perception can improve machine performance.</li> </ol> <p><strong>Broader Impacts:</strong></p> <p>Understanding what aspects of the source signal are talker-specific should aid in developing better speaker identification and verification algorithms that are robust to varying affect and styles of speaking. A model of voice variations could also improve the naturalness of text-to-speech (TTS) systems. If we knew how much a person could change his or her voice quality, and in what ways, without compromising their vocal identity, this knowledge could also inform medical rehab applications and forensics.</p> <p>&nbsp;</p> <p>The project fostered interdisciplinary activities in Electrical Engineering, Linguistics, and Speech and Hearing Science at UCLA. It trained several undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance.</p> <p>&nbsp;</p> <p>All publications have been posted on our webpages and presented at leading conferences and journals in Engineering, Computer Science, Speech Science and Linguistics.</p> <p>&nbsp;</p> <p>This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/05/2017<br>      Modified by: Abeer&nbsp;A&nbsp;Alwan</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1450992/1450992_10322723_1504658330774_spectrograms--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1450992/1450992_10322723_1504658330774_spectrograms--rgov-800width.jpg" title="Spectrograms (time-frequncy plots)"><img src="/por/images/Reports/POR/2017/1450992/1450992_10322723_1504658330774_spectrograms--rgov-66x44.jpg" alt="Spectrograms (time-frequncy plots)"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Spectrograms of a person saying 'Go Bruins' in a neutral way (left panel) and saying 'Go Bruins' in an excited manner (right panel). Notice the large difference between the two spctrograms.</div> <div class="imageCredit">UCLA</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Abeer&nbsp;A&nbsp;Alwan</div> <div class="imageTitle">Spectrograms (time-frequncy plots)</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A talker?s voice quality conveys many kinds of information, including emotional state (happy, angry, etc.) and personal identity. As an example, please refer to Figure 1 which shows a spectrogram (which is a time-frequency display) of a person saying ?Go Bruins? in a neutral and another spectrogram of the same person saying ?Go Bruins? in an excited manner. Hence, significant within speaker (intra-speaker) and across speakers (inter-speaker) variability in voice occurs routinely during normal everyday speech. Understanding what aspects of a voice are talker-specific should aid in understanding the human limits in perceiving speaker differences and in developing better automatic speaker identification (SID) algorithms. Despite technological advances, the performance of current SID systems remains far from perfect and is worse than humans? ability to identify speakers. The key questions that we aim to answer are: under normal daily life variability, how often does a talker sound less like him- or herself and more like someone else? Which properties in the signal account for speaker similarity? Can automatic speaker identification (SID) and verification algorithms be improved by knowledge of which properties are important for human perception of speaker similarity?     Understanding these issues requires a high-quality speech database with multiple voice samples from many talkers (in our case 200) which we collected and analyzed with EAGER funding.  Outcomes:  Produced a database of high-quality recordings. 200 talkers provided a total of about 5-7 minutes of speech from multiple recording sessions and speech tasks. The database will be freely available to other researchers soon. Analyzed speech signals and characterized inter- and intra-talker variability across different situations. Conducted perceptual studies to determine how much variability can be tolerated before talkers cease to sound like themselves. Automatic speaker verification experiments determined differences, in performance and strategies adopted, between humans and machines. The experiments showed that using features inspired by human perception can improve machine performance.   Broader Impacts:  Understanding what aspects of the source signal are talker-specific should aid in developing better speaker identification and verification algorithms that are robust to varying affect and styles of speaking. A model of voice variations could also improve the naturalness of text-to-speech (TTS) systems. If we knew how much a person could change his or her voice quality, and in what ways, without compromising their vocal identity, this knowledge could also inform medical rehab applications and forensics.     The project fostered interdisciplinary activities in Electrical Engineering, Linguistics, and Speech and Hearing Science at UCLA. It trained several undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance.     All publications have been posted on our webpages and presented at leading conferences and journals in Engineering, Computer Science, Speech Science and Linguistics.     This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.          Last Modified: 09/05/2017       Submitted by: Abeer A Alwan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
