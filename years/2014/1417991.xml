<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CI-P:Collaborative Research: Visual entailment data set and challenge for the language and vision communities</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>50815.00</AwardTotalIntnAmount>
<AwardAmount>50815</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Vision and language provide fundamental means to interpret, learn, and communicate about the world around us. A primary goal of computer vision and natural language processing research is therefore to automatically uncover and analyze the information that images and video, or text and speech, convey about the world.  Both communities are concerned with tasks that require increasingly deeper understanding, including the ability to reason with and draw inferences from this information.  Since vision and language are complementary modalities, there is now also an increasing amount of work at the interface of both fields. However, progress in multimodal analysis requires a tighter collaboration between the two communities, since each currently relies on its own set of techniques, datasets and evaluation criteria. &lt;br/&gt;&lt;br/&gt;This community planning grant explores the need for, feasibility, and usefulness of a "visual entailment" corpus and associated visual entailment recognition task. In natural language, entailment recognition is the problem of determining whether a particular statement can be inferred from a text document. This project explores a novel related problem - visual entailment - where the goal is to determine whether a statement in natural language can be inferred from an image or video.  The outcomes of the project include a novel dataset and prototype research challenge, as well as increased collaboration between the vision and language communities.</AbstractNarration>
<MinAmdLetterDate>05/12/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/12/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1417991</AwardID>
<Investigator>
<FirstName>Tamara</FirstName>
<LastName>Berg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tamara Berg</PI_FULL_NAME>
<EmailAddress>tlberg@cs.unc.edu</EmailAddress>
<PI_PHON>6465093361</PI_PHON>
<NSF_ID>000519059</NSF_ID>
<StartDate>05/12/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S Columbia St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~50815</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>The purpose of this grant was to prototype new datasets and research challenges for the vision and language communities with an ultimate goal of enhancing collaboration efforts between members of these communities. As part of this grant feedback was&nbsp;</span>solicited from the vision and language research communities at a variety of workshops held at vision, language, and machine learning conferences. Based on the above feedback two new datasets were prototyped, the ReferItGame dataset and the Visual Madlibs datasets &nbsp;</p> </div> </div> </div> </div> <div class="page" title="Page 3"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>The first dataset, the <strong>ReferItGame Dataset</strong>, is a collection of natural language descriptions referring to specific &nbsp;objects in complex natural scenes. This task is related to the problem of </span><span>referring expression generation </span><span>(REG) introduced by linguistics in the 1970s and relates to how people will construct language to refer to a particular object, e.g. "the large red ball", based on other objects in the scene. These expressions are highly dependent upon both the object itself, and the other objects in the scene. For example, if there is only one ball in the scene, people may construct an expression like "the ball", while if there are 3 balls in the scene and only one is red then people may say "the red ball".&nbsp;</span></p> <div class="page" title="Page 3"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>To study referring expressions at scale in complex, real-world scenes, a two player referring expression game was developed (</span><span>ReferItGame</span><span>) to crowd-source the data collection. In this game, Player 1 is shown an image with an object outlined in red and provided with a text box in which to write&nbsp;</span>a referring expression. Player 2: is shown the same image and the referring expression written by Player 1 and must click on the location of the described object. This allows us to both collect and verify expressions within the game. Using the ReferItGame, an initial corpus containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of real world scenes was collected.</p> <p>The second dataset, the <strong>Visual Madlibs Dataset</strong>, contains focused, targeted, natural language descriptions. To collect these descriptions, automatically produced fill- in-the-blank templates were designed to collect a range of different contextual inferences for images. For example, a user might be presented with an image and a fill-in-the-blank template such as ``The person is [blank] the dog'' and asked to fill in the [blank] with a description of the relationship between the person and the dog. While the general form of the questions for the Visual Madlibs are chosen by hand, most of the questions are instantiated depending on a subset of the objects present in an image. Fill-in-the-blank questions can be designed to collect descriptions about people and objects, their appearances, activities, and interactions, as well as descriptions of the general scene or the broader emotional, spatial, or temporal context of an image.&nbsp;</p> <div class="page" title="Page 4"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>With these new types of annotations, algorithms to generate targeted natural language interpretations for images can be developed. Additionally, m</span>ultiple-choice questions for images can be produced. In these questions, the computer is provided with an image and a partial description such as ``The person is [blank]''. A set of possible answers is also provided, one answer that was written about the image in question, plus several ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     The purpose of this grant was to prototype new datasets and research challenges for the vision and language communities with an ultimate goal of enhancing collaboration efforts between members of these communities. As part of this grant feedback was solicited from the vision and language research communities at a variety of workshops held at vision, language, and machine learning conferences. Based on the above feedback two new datasets were prototyped, the ReferItGame dataset and the Visual Madlibs datasets            The first dataset, the ReferItGame Dataset, is a collection of natural language descriptions referring to specific  objects in complex natural scenes. This task is related to the problem of referring expression generation (REG) introduced by linguistics in the 1970s and relates to how people will construct language to refer to a particular object, e.g. "the large red ball", based on other objects in the scene. These expressions are highly dependent upon both the object itself, and the other objects in the scene. For example, if there is only one ball in the scene, people may construct an expression like "the ball", while if there are 3 balls in the scene and only one is red then people may say "the red ball".       To study referring expressions at scale in complex, real-world scenes, a two player referring expression game was developed (ReferItGame) to crowd-source the data collection. In this game, Player 1 is shown an image with an object outlined in red and provided with a text box in which to write a referring expression. Player 2: is shown the same image and the referring expression written by Player 1 and must click on the location of the described object. This allows us to both collect and verify expressions within the game. Using the ReferItGame, an initial corpus containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of real world scenes was collected.  The second dataset, the Visual Madlibs Dataset, contains focused, targeted, natural language descriptions. To collect these descriptions, automatically produced fill- in-the-blank templates were designed to collect a range of different contextual inferences for images. For example, a user might be presented with an image and a fill-in-the-blank template such as ``The person is [blank] the dog'' and asked to fill in the [blank] with a description of the relationship between the person and the dog. While the general form of the questions for the Visual Madlibs are chosen by hand, most of the questions are instantiated depending on a subset of the objects present in an image. Fill-in-the-blank questions can be designed to collect descriptions about people and objects, their appearances, activities, and interactions, as well as descriptions of the general scene or the broader emotional, spatial, or temporal context of an image.       With these new types of annotations, algorithms to generate targeted natural language interpretations for images can be developed. Additionally, multiple-choice questions for images can be produced. In these questions, the computer is provided with an image and a partial description such as ``The person is [blank]''. A set of possible answers is also provided, one answer that was written about the image in question, plus several answers written about other images. The computer is then evaluated on how well it can select the correct choice. Currently, the collection contains 359,001 targeted descriptions for 10,738 images on the Microsoft COCO dataset        Construction of these datasets was implemented by Mark Matten, a high school student, Sahar Kazemzadeh, an Undergraduate student, and two graduate students, Vicente Ordonez and Licheng Yu. Datasets and related concepts were also introduced in the PI's "Language and Vision" course.                  Last Modified: 09/07/2015       Submitted by: Tamara Berg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
