<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER-DynamicData: A new paradigm for data analytics: L1-norm based Learning and Processing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>180000.00</AwardTotalIntnAmount>
<AwardAmount>180000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Lawrence Goldberg</SignBlockName>
<PO_EMAI>lgoldber@nsf.gov</PO_EMAI>
<PO_PHON>7032928339</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project aims to carry out fundamental work on the transformative/disruptive idea of data analytics and data-feature extraction by newly defined and calculated principal-component vectors that best represent the main features of a given data set, even in the presence of faulty/missing/outlier data. Based on this idea, an algorithmic framework will be developed to support several targeted applications, such as data processing in social networks, image processing and video libraries, wireless-sensor-network data fusion, economics, genomics and proteomics, and bioinformatics. The potential impact of the project work is immense and may extend well beyond these applications to cover any field of science and engineering where conventional data feature extraction has been used in the past.&lt;br/&gt;&lt;br/&gt;Technically speaking, the investigation aims at rewriting the enormously rewarding over the past century chapter on L2-norm (eigen-vector and singular-vector decomposition) data analysis. Optimal L1-norm data analytics are being developed that are inherently resistant to data contamination and as good as L2-norm analytics on "clean" data. L1-norm data principal component analysis has seen a limited amount of previous research and is non-existent so far in education/textbooks. Several profound differences between L1-norm based principle component analysis (PCA) and standard L2-norm PCA have, to date, blocked progress in the theoretical understanding (and thus in the design of efficient algorithmic solutions) of L1-based PCA.  The project seeks the development of a novel approach toward dimensionality reduction under the L1 norm to deal with processing of outlier-prone/contaminated ?big data? (large amount of high-dimensional data) by interpreting the fundamental L1-norm principal-components optimization problem as an equivalent binary-field maximization problem, and in such opening a spectrum of potentially new analytical and algorithmic techniques. The project goals include: (i) Fundamental algorithmic research on the exact calculation of maximum-L1-norm-projection data features; (ii) fundamental understanding and execution of L1-norm-measured data dimensionality reduction; and (iii) sample-space reduction.</AbstractNarration>
<MinAmdLetterDate>09/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>01/12/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1462341</AwardID>
<Investigator>
<FirstName>Stella</FirstName>
<LastName>Batalama</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stella Batalama</PI_FULL_NAME>
<EmailAddress>sbatalama@fau.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000242903</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dimitris</FirstName>
<LastName>Pados</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dimitris A Pados</PI_FULL_NAME>
<EmailAddress>dpados@fau.edu</EmailAddress>
<PI_PHON>5612972988</PI_PHON>
<NSF_ID>000242904</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate>09/22/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Langberg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Langberg</PI_FULL_NAME>
<EmailAddress>mikel@buffalo.edu</EmailAddress>
<PI_PHON>7166452634</PI_PHON>
<NSF_ID>000657891</NSF_ID>
<StartDate>01/12/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Langberg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Langberg</PI_FULL_NAME>
<EmailAddress>mikel@buffalo.edu</EmailAddress>
<PI_PHON>7166452634</PI_PHON>
<NSF_ID>000657891</NSF_ID>
<StartDate>09/04/2015</StartDate>
<EndDate>09/22/2017</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Buffalo</Name>
<CityName>Buffalo</CityName>
<ZipCode>142282567</ZipCode>
<PhoneNumber>7166452634</PhoneNumber>
<StreetAddress>520 Lee Entrance</StreetAddress>
<StreetAddress2><![CDATA[Suite 211]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY26</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>038633251</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Buffalo]]></Name>
<CityName>Buffalo</CityName>
<StateCode>NY</StateCode>
<ZipCode>142601660</ZipCode>
<StreetAddress><![CDATA[Dept. of Electrical Engineering]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY26</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramElement>
<Code>O395</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>153E</Code>
<Text>Wireless comm &amp; sig processing</Text>
</ProgramReference>
<ProgramReference>
<Code>5384</Code>
<Text>DATA AND DATA SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~180000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">A general intention of subspace data/signal processing is to partition the vector space of the observed data and isolate the subspace of the signal component(s) of interest from the disturbance (noise) subspace. Subspace signal processing theory and practice rely, conventionally, on the familiar L2-norm based singular-value decomposition (SVD) of the data matrix. The SVD solution traces its origin to the fundamental problem of L2-norm low-rank matrix approximation, which is equivalent to the problem of maximum L2-norm data projection with as many projection ('principal') components as the desired low-rank value.<span>&nbsp;</span></p> <p class="p2">&nbsp;Practitioners have long observed, however, that L2-norm principal component analysis&nbsp;(PCA) is sensitive to the presence of outlier values in the data matrix, that is, erroneous values that are away from the nominal data, appear only few times in the data matrix, and are not to appear again under normal system operation upon design. As the presence of outliers is unavoidable in practical systems, the sensitivity of L2-based PCA in this context yields significant disadvantages (that are, at times, devastating) in PCA based data processing. The project at hand studied the design, analysis, and implementation of a novel framework for L1-norm-based PCA data processing which successfully copes with the presence of outliers in learning.</p> <p class="p2">The project put forward and carried out exploratory research on the transformative/disruptive idea of L1-norm principal component analysis for robust data analytics in the presence of potentially missing or faulty or corrupted data measurements. The effort intends to rewrite the enormously rewarding over the past century scientific chapter on eigenvector and singular vector decomposition (EVD and SVD) data analysis that is built on L2-norm error/energy representation. Our findings consistently show that L1-PCA is as good as L2-PCA on clean data, but significantly superior on corrupted/faulty data; supporting the potential in replacing L2-norm PCA by L1-PCA across the board. Specific project outcomes include applying L1-norm PCA in the context of face recognition, image fusion, image and video processing, low-rank matrix approximation, Direction of Arrival estimation, dimension reduction, outlier identification, and sparse reconstruction.</p><br> <p>            Last Modified: 11/29/2018<br>      Modified by: Michael&nbsp;Langberg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[A general intention of subspace data/signal processing is to partition the vector space of the observed data and isolate the subspace of the signal component(s) of interest from the disturbance (noise) subspace. Subspace signal processing theory and practice rely, conventionally, on the familiar L2-norm based singular-value decomposition (SVD) of the data matrix. The SVD solution traces its origin to the fundamental problem of L2-norm low-rank matrix approximation, which is equivalent to the problem of maximum L2-norm data projection with as many projection ('principal') components as the desired low-rank value.   Practitioners have long observed, however, that L2-norm principal component analysis (PCA) is sensitive to the presence of outlier values in the data matrix, that is, erroneous values that are away from the nominal data, appear only few times in the data matrix, and are not to appear again under normal system operation upon design. As the presence of outliers is unavoidable in practical systems, the sensitivity of L2-based PCA in this context yields significant disadvantages (that are, at times, devastating) in PCA based data processing. The project at hand studied the design, analysis, and implementation of a novel framework for L1-norm-based PCA data processing which successfully copes with the presence of outliers in learning. The project put forward and carried out exploratory research on the transformative/disruptive idea of L1-norm principal component analysis for robust data analytics in the presence of potentially missing or faulty or corrupted data measurements. The effort intends to rewrite the enormously rewarding over the past century scientific chapter on eigenvector and singular vector decomposition (EVD and SVD) data analysis that is built on L2-norm error/energy representation. Our findings consistently show that L1-PCA is as good as L2-PCA on clean data, but significantly superior on corrupted/faulty data; supporting the potential in replacing L2-norm PCA by L1-PCA across the board. Specific project outcomes include applying L1-norm PCA in the context of face recognition, image fusion, image and video processing, low-rank matrix approximation, Direction of Arrival estimation, dimension reduction, outlier identification, and sparse reconstruction.       Last Modified: 11/29/2018       Submitted by: Michael Langberg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
