<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: SaTC: Empirical and Analytical Models for the Deployment of Software Updates in Large Vulnerable Populations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2015</AwardEffectiveDate>
<AwardExpirationDate>04/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>170340.00</AwardTotalIntnAmount>
<AwardAmount>170340</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nina Amla</SignBlockName>
<PO_EMAI>namla@nsf.gov</PO_EMAI>
<PO_PHON>7032927991</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Software vulnerabilities are an important vector for malware delivery. The software updating mechanisms, responsible for deploying the vulnerability patches, are in a race with the cyber attackers seeking to exploit the vulnerabilities. Moreover, these updating mechanisms have multiple, potentially conflicting, design goals, as they must quickly deploy patches on millions of hosts worldwide, must not overburden the users, and must avoid breaking dependencies in the deployment environment. &lt;br/&gt;&lt;br/&gt;This project aims to model the dynamics of vulnerable host populations, in order to assess the practical barriers for current software updating mechanisms and the conflicts among their security and reliability goals. Using real-world data sets of update deployment events, the research studies the decay of vulnerable host populations empirically to identify deployment-specific factors that delay updates. Building on these insights, the project develops parameterized analytical models for update deployment, and uses these models to quantify the trade-offs between reliability and security when updating software. The models provide principled methods for reasoning about the properties of software updates in the presence of multiple design goals and enable improvements in software updating mechanisms by exploring a large design space. The researchers are disseminating the results from this project by organizing workshops on data-driven security, by releasing data sets with augmented information about software vulnerabilities, and by collaborating with industry partners to evaluate the proposed techniques in real-world settings.</AbstractNarration>
<MinAmdLetterDate>05/11/2015</MinAmdLetterDate>
<MaxAmdLetterDate>05/11/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1464163</AwardID>
<Investigator>
<FirstName>Tudor</FirstName>
<LastName>Dumitras</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tudor Dumitras</PI_FULL_NAME>
<EmailAddress>tdumitra@umiacs.umd.edu</EmailAddress>
<PI_PHON>3014056269</PI_PHON>
<NSF_ID>000654912</NSF_ID>
<StartDate>05/11/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~170340</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><!--?xml version="1.0" encoding="UTF-8"?--></p> <div>Security vulnerabilities left unpatched in the wild, despite the availability of software updates that can fix these bugs, allow hackers to compromise hosts on a global scale, as demonstrated in 2017 by the WannaCry worm. The funded research resulted in several advances for understanding the practical barriers against effective patching and the role that patching delays play in increasing the risk of cyber attacks.</div> <div></div> <div>First, we measured the daily patching rates for 1,593 vulnerabilities, over a five-year period. This represents the largest corpus of patch deployment measurements, which led to several new insights. For example, our measurements showed that, while patches are usually released on the dates when these vulnerabilities are disclosed, or soon after, the patch deployment rates vary widely across hosts and across vulnerabilities. We also identified several factors that contribute to the patching delays; for example, we found that user decisions play an important role, even for applications that receive software updates automatically. Our large scale measurement provided the basis for two additional tasks investigating the properties of software updates in a broader security context.</div> <div></div> <div>Second, we developed two analytical models describing the software updating process. The first model quantifies the longevity of software vulnerabilities in the wild, by capturing the end&shy; user behavior in delaying software updates and by separating the impact of user and vendor behavior on the vulnerability states of hosts. The second model allows us to infer which vulnerabilities are exploited in the wild, starting from the intuition that variations in the patching rates for different vulnerabilities indicate which vulnerabilities present a higher risk of exploitation.</div> <div></div> <div>Third, we investigated what stops users from installing the patches as soon as they are released. In doing so, we identified subtle biases in how users make patching decisions, by systematically comparing self-reported behaviors, collected through online surveys, with the behaviors recorded in our measurement corpus. For example, users systematically underestimate how quickly they would apply a patch. However, this systematic relationship breaks down when survey respondents are required to notice and act on minor details of experimental manipulations, such as the content of update messages.</div> <div></div> <div>As a broader impact, we are collaborating with industry partners who are interested in adopting our analytical models to assess cyber risk objectively, by relying on empirical data rather than on expert opinions. Our models can also be applied for developing cyber policies in an evidence-based fashion and for risk modeling in cyber insurance. Our results on the users&rsquo; patching behavior have broader implications on understanding security-related behaviors, and they suggest that certain insights from self-report security data can translate to real-world environments, after applying a systematic correction.</div> <div></div> <div>On the educational impact side, this project generated data used in several semester-long projects from a graduate security class taught at UMD. These projects exposed students to the impact of patching on security and provided an opportunity&nbsp;to incorporate experimentation and empirical methods in our security curriculum.&nbsp;</div> <p>&nbsp;</p><br> <p>            Last Modified: 08/29/2018<br>      Modified by: Tudor&nbsp;Dumitras</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Security vulnerabilities left unpatched in the wild, despite the availability of software updates that can fix these bugs, allow hackers to compromise hosts on a global scale, as demonstrated in 2017 by the WannaCry worm. The funded research resulted in several advances for understanding the practical barriers against effective patching and the role that patching delays play in increasing the risk of cyber attacks.  First, we measured the daily patching rates for 1,593 vulnerabilities, over a five-year period. This represents the largest corpus of patch deployment measurements, which led to several new insights. For example, our measurements showed that, while patches are usually released on the dates when these vulnerabilities are disclosed, or soon after, the patch deployment rates vary widely across hosts and across vulnerabilities. We also identified several factors that contribute to the patching delays; for example, we found that user decisions play an important role, even for applications that receive software updates automatically. Our large scale measurement provided the basis for two additional tasks investigating the properties of software updates in a broader security context.  Second, we developed two analytical models describing the software updating process. The first model quantifies the longevity of software vulnerabilities in the wild, by capturing the end&shy; user behavior in delaying software updates and by separating the impact of user and vendor behavior on the vulnerability states of hosts. The second model allows us to infer which vulnerabilities are exploited in the wild, starting from the intuition that variations in the patching rates for different vulnerabilities indicate which vulnerabilities present a higher risk of exploitation.  Third, we investigated what stops users from installing the patches as soon as they are released. In doing so, we identified subtle biases in how users make patching decisions, by systematically comparing self-reported behaviors, collected through online surveys, with the behaviors recorded in our measurement corpus. For example, users systematically underestimate how quickly they would apply a patch. However, this systematic relationship breaks down when survey respondents are required to notice and act on minor details of experimental manipulations, such as the content of update messages.  As a broader impact, we are collaborating with industry partners who are interested in adopting our analytical models to assess cyber risk objectively, by relying on empirical data rather than on expert opinions. Our models can also be applied for developing cyber policies in an evidence-based fashion and for risk modeling in cyber insurance. Our results on the users? patching behavior have broader implications on understanding security-related behaviors, and they suggest that certain insights from self-report security data can translate to real-world environments, after applying a systematic correction.  On the educational impact side, this project generated data used in several semester-long projects from a graduate security class taught at UMD. These projects exposed students to the impact of patching on security and provided an opportunity to incorporate experimentation and empirical methods in our security curriculum.           Last Modified: 08/29/2018       Submitted by: Tudor Dumitras]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
