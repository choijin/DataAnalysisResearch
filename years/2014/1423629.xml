<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TWC SBE: TTP Option: Small: A User-Tailored Approach to Privacy Decision Support</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>666343.00</AwardTotalIntnAmount>
<AwardAmount>666343</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sara Kiesler</SignBlockName>
<PO_EMAI>skiesler@nsf.gov</PO_EMAI>
<PO_PHON>7032928643</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Numerous surveys find that Internet users want to limit the personal data that is being collected about them, as well as control the usage of their data. Existing and proposed regulation in the U.S. accords users such rights, in the form of a "transparency and control" obligation on personal data collectors: users should be informed about the rationale of requests for personal data so that they can make an informed decision on whether or not to disclose their data.&lt;br/&gt; &lt;br/&gt;In the past few years, it became apparent that the transparency and control paradigm "does not work." It overburdens users by the sheer amount of decisions that must be made in a very short period of time. Recent research moreover shows that users are not cognitively able to make rational decisions when they have to trade off immediate benefits of data disclosure with uncertain and unspecific privacy threats sometimes in the future.&lt;br/&gt; &lt;br/&gt;Based on empirical studies, this research helps predict what privacy decisions would be consistent with users' preferences, and generate personalized default settings for privacy choices as well as rationales for disclosure that best suits users' predicted decision-making. Users can override any of the predictions, and such corrections will modify the prediction algorithm over time. The proposed approach thus affords "realistic empowerment": it allows Internet users to make their own privacy decisions if they wish, and also helps them overcome the limits of their bounded rationality by first making personalized default decisions on behalf of them. The multiple methods used in the research design build upon one another, representing a rigorous and systematic approach to develop an in-depth understanding of designing and developing smart default privacy settings. This project can have a transformative impact on the privacy literature by providing a systematic understanding on predicting individuals' contextual privacy preferences.</AbstractNarration>
<MinAmdLetterDate>08/29/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423629</AwardID>
<Investigator>
<FirstName>Alfred</FirstName>
<LastName>Kobsa</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alfred Kobsa</PI_FULL_NAME>
<EmailAddress>kobsa@uci.edu</EmailAddress>
<PI_PHON>9494855020</PI_PHON>
<NSF_ID>000489846</NSF_ID>
<StartDate>08/29/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>926173067</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~666343</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="caret-color: #000000; color: #000000; font-family: Arial; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;">Numerous surveys find that Internet users want to limit the personal data that is being collected about them, as well as control the usage of their data. Existing and proposed regulation in the U.S. often accords users such rights,&nbsp;in the form of &ldquo;transparency and control&rdquo; obligations on personal data collectors: users should be informed about the personal data that is being collected about them as well as the rationale for the collection, so that they can make informed decisions whether or not to disclose their data.</p> <p style="caret-color: #000000; color: #000000; font-family: Arial; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;">In the past few years it became apparent though that the transparency and control paradigm &ldquo;does not work&rdquo;. It overburdens users by the sheer amount of decisions that must be made in a very short period of time. Recent research moreover demonstrated that users are not cognitively able to make rational decisions when they have to trade off immediate benefits of data disclosure with uncertain and unspecific privacy threats sometimes in the future.</p> <p style="caret-color: #000000; color: #000000; font-family: Arial; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;">This research project aimed at predicting what privacy decisions would be consistent with users' preferences, and at generating personalized default settings for privacy choices as well as rationales for disclosure that best suits users&rsquo; predicted decision-making. Users can override any of the predictions, and such corrections will modify the prediction algorithm over time. In this vein, we conducted empirical studies in several countries to analyze the factors that would influence people&rsquo;s privacy decisions.</p> <div style="caret-color: #000000; color: #000000; font-family: Arial; font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;"> <p>In collaboration with Microsoft Technology Policy Group, we studied the influence of cultural background on people&rsquo;s privacy decisions and on the ability of machine-learning algorithms to<span class="Apple-converted-space"> </span>predict these decisions. We explored the idea of an integrated cross-cultural privacy prediction model, using data from a large-scale quantitative study that was conducted in eight countries. Based on cultural characteristics of those countries (e.g., Hofstede's cultural dimensions) as well as contextual parameters, our model uses supervised machine learning to predict users&rsquo; privacy decisions regarding the collection of their personal data. We found that incorporating culture-related predictors into the model can improve the prediction performance in addition to demographic, attitudinal, and contextual predictors. Hofstede&rsquo;s values&mdash;particularly the dimensions of individualism and indulgence&mdash;outperform country and language. We also used linear regression to analyze interaction effects between country characteristics and contextual parameters. In a second study, we analyzed a much larger dataset containing context-based privacy decisions from about 28,000 respondents in 28 countries. Using cluster analysis we could show that gross domestic product per capita is a decisive determinant for privacy decisions.</p> <p>2. In collaboration with a second industry partner (Intel), we aimed to find determinants for people's privacy decisions in Iot environments. We conducted a major experience-sampling study across large parts of our university campus with about 150 participants, to determine the extent to which the determinants identified in our previous studies can be used by machine learning algorithms to predict people's privacy decisions with regard to surrounding IoT technology. A graduate student interned at Intel and developed a privacy-aware architecture for an "IoT service store&rdquo; (ISS) that allows users to easily browse nearby IoT services, understand the privacy implications of these IoT services, and control the collection and usage of sensor data. To better inform users about the potential privacy risks in using IoT services, ISS displays detailed information on what personal information might be inferred from the sensor data being collected. ISS also allows each user to give a rating or to view other users&rsquo; ratings regarding the perceived utility-privacy tradeoff for each IoT service..</p> <p>3. In collaboration with the National University of Singapore, we investigated how characteristics of a personalization provider influence users&rsquo; attitudes towards personalization and their resulting disclosure behavior. We proposed an integrative model that links these characteristics via privacy attitudes to actual disclosure behavior. In another study, we analyzed the effect of variables such as country of origin, self-efficacy, collective self-efficacy, subjective norm, descriptive norm, reciprocity and collective effort on participants' usage of collaborative privacy management techniques in a social networking context. We tested the validity and cultural invariance of a measurement model and predictive model of collective privacy management on survey data from Facebook users in the US, Singapore and South Korea. The results show that the measurement model is only partially culturally invariant, indicating that social media users in different countries interpret the same instruments in different ways. Also, cross-national comparisons of the structural model show that causal pathways from collective privacy management strategies to privacy-related outcomes vary significantly across countries. The practical implications are that researchers should conduct privacy studies in a single country only (and preferably only with long-time residents), or should be methodologically very cautious when doing multi-country studies. Similarly, insights about social media users found in one country should not be automatically generalized to other countries unless proper testing of assumptions and modification of measurement and research models are performed.</p> </div><br> <p>            Last Modified: 09/16/2018<br>      Modified by: Alfred&nbsp;Kobsa</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Numerous surveys find that Internet users want to limit the personal data that is being collected about them, as well as control the usage of their data. Existing and proposed regulation in the U.S. often accords users such rights, in the form of "transparency and control" obligations on personal data collectors: users should be informed about the personal data that is being collected about them as well as the rationale for the collection, so that they can make informed decisions whether or not to disclose their data. In the past few years it became apparent though that the transparency and control paradigm "does not work". It overburdens users by the sheer amount of decisions that must be made in a very short period of time. Recent research moreover demonstrated that users are not cognitively able to make rational decisions when they have to trade off immediate benefits of data disclosure with uncertain and unspecific privacy threats sometimes in the future. This research project aimed at predicting what privacy decisions would be consistent with users' preferences, and at generating personalized default settings for privacy choices as well as rationales for disclosure that best suits users? predicted decision-making. Users can override any of the predictions, and such corrections will modify the prediction algorithm over time. In this vein, we conducted empirical studies in several countries to analyze the factors that would influence people?s privacy decisions.   In collaboration with Microsoft Technology Policy Group, we studied the influence of cultural background on people?s privacy decisions and on the ability of machine-learning algorithms to predict these decisions. We explored the idea of an integrated cross-cultural privacy prediction model, using data from a large-scale quantitative study that was conducted in eight countries. Based on cultural characteristics of those countries (e.g., Hofstede's cultural dimensions) as well as contextual parameters, our model uses supervised machine learning to predict users? privacy decisions regarding the collection of their personal data. We found that incorporating culture-related predictors into the model can improve the prediction performance in addition to demographic, attitudinal, and contextual predictors. Hofstede?s values&mdash;particularly the dimensions of individualism and indulgence&mdash;outperform country and language. We also used linear regression to analyze interaction effects between country characteristics and contextual parameters. In a second study, we analyzed a much larger dataset containing context-based privacy decisions from about 28,000 respondents in 28 countries. Using cluster analysis we could show that gross domestic product per capita is a decisive determinant for privacy decisions.  2. In collaboration with a second industry partner (Intel), we aimed to find determinants for people's privacy decisions in Iot environments. We conducted a major experience-sampling study across large parts of our university campus with about 150 participants, to determine the extent to which the determinants identified in our previous studies can be used by machine learning algorithms to predict people's privacy decisions with regard to surrounding IoT technology. A graduate student interned at Intel and developed a privacy-aware architecture for an "IoT service store" (ISS) that allows users to easily browse nearby IoT services, understand the privacy implications of these IoT services, and control the collection and usage of sensor data. To better inform users about the potential privacy risks in using IoT services, ISS displays detailed information on what personal information might be inferred from the sensor data being collected. ISS also allows each user to give a rating or to view other users? ratings regarding the perceived utility-privacy tradeoff for each IoT service..  3. In collaboration with the National University of Singapore, we investigated how characteristics of a personalization provider influence users? attitudes towards personalization and their resulting disclosure behavior. We proposed an integrative model that links these characteristics via privacy attitudes to actual disclosure behavior. In another study, we analyzed the effect of variables such as country of origin, self-efficacy, collective self-efficacy, subjective norm, descriptive norm, reciprocity and collective effort on participants' usage of collaborative privacy management techniques in a social networking context. We tested the validity and cultural invariance of a measurement model and predictive model of collective privacy management on survey data from Facebook users in the US, Singapore and South Korea. The results show that the measurement model is only partially culturally invariant, indicating that social media users in different countries interpret the same instruments in different ways. Also, cross-national comparisons of the structural model show that causal pathways from collective privacy management strategies to privacy-related outcomes vary significantly across countries. The practical implications are that researchers should conduct privacy studies in a single country only (and preferably only with long-time residents), or should be methodologically very cautious when doing multi-country studies. Similarly, insights about social media users found in one country should not be automatically generalized to other countries unless proper testing of assumptions and modification of measurement and research models are performed.        Last Modified: 09/16/2018       Submitted by: Alfred Kobsa]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
