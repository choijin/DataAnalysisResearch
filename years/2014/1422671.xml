<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Gestural Image Annotation Systems in Coordinated Surgical Practice</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>296648.00</AwardTotalIntnAmount>
<AwardAmount>310648</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Gestures, deictic referencing (pointing at objects and regions of interest), and manipulation of digital images are an integral aspect of decision making in collaborative scientific and medical work.  In modern minimally invasive surgical interventions, medical imaging has come to play an increasingly important role, but due to concerns of asepsis image manipulation during surgical decision making are typically constrained.  In this project, the PI will explore the use of new technology such as the Kinect to address this issue, by developing techniques for "touchless" interaction to coordinate and enhance communication among team members in the operating room.  The results of this research will provide a deeper understanding of collaborative practices around image use and the benefit of technological tools for annotating and referencing those images, which will significantly benefit patient outcomes.  The findings will have broad impact, in that they will be translatable to other expert collaborative environments that utilize imagery in order to have a positive effect on team work practices.  The PI will disseminate project outcomes broadly to both the medical and the human-computer interaction (HCI) communities.&lt;br/&gt;&lt;br/&gt;An understanding of how new imaging interaction techniques can be integrated into and, in turn, shape coordinated practice will constitute an important contribution to the state of the art.  To achieve these goals, the PI will identify coordinated practices and their relationship to imaging use through a detailed field study of laparoscopic surgery.  She will then iteratively design and implement a gestural image annotation prototype for laparoscopic surgeons to reference and annotate endoscopic video.  And she will determine the effects of the imaging manipulation on coordinated surgical practice through an experimental study.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/10/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422671</AwardID>
<Investigator>
<FirstName>Helena</FirstName>
<LastName>Mentis</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Helena M Mentis</PI_FULL_NAME>
<EmailAddress>mentis@umbc.edu</EmailAddress>
<PI_PHON>4104553687</PI_PHON>
<NSF_ID>000637791</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland Baltimore County</Name>
<CityName>Baltimore</CityName>
<ZipCode>212500002</ZipCode>
<PhoneNumber>4104553140</PhoneNumber>
<StreetAddress>1000 Hilltop Circle</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>061364808</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND BALTIMORE COUNTY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland Baltimore County]]></Name>
<CityName/>
<StateCode>MD</StateCode>
<ZipCode>212500002</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~296648</FUND_OBLG>
<FUND_OBLG>2017~14000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Gestures, pointing, and movement of digital images are an integral aspect of decision making in collaborative work.&nbsp;</span>What was needed was an understanding of these collaborative practices around image use and the benefit of technological tools for annotating and referencing images during collaboration.&nbsp;</p> <p>Over the five years of this project, we made numerous discoveries about the problem of communication and coordination around images (specifically during endoscopic surgery). We learned how knowledge for the task content was either directly added or elicited by queries. To coordinate the procedure, a lead surgeons often provided repeated explicit instructions over the course of a surgery. We showed that the surgeons provided detailed instructions to guide their supporting surgeon step-by-step in the intricacies of a particular surgical procedure. We also learned that <span>there is a tight connection between adopting expert gaze and performing surgical tasks, and this suggested that gaze training should be integrated into our system design.</span></p> <p>We invented an approach to address this problem that began with a patent for using gestures and voice commands to point or draw on live endoscopic images. From this starting point we developed a system for use with any endoscopic video feed so collaborating surgeons could both refer to the medical images during surgery. In turn, we evaluated the impact of this method of interaction on communication efficiency, perceived usefulness in surgical training, educational gains, and surgical performance outcomes.&nbsp;We learned that with this virtual pointer, a trainees' eye fixations were more concentrated&nbsp;compared to a standard condition. However, when we investigated the communication structure and content, we show that the communication becomes more imbalanced and the trainees become less active when using the gesturing tool. And trainees had a tendency to attend to the telepointer instruction as the primary source of information when in a distributed environment.&nbsp;</p> <p>We shared the findings from this research broadly to both medical and computer science researchers and practitioners in the form of presentations, publications, and demonstrations. The PI integrated the findings into her graduate level class on computer supported cooperative work. The grant also supported the professional development of the PI, directly supported the research of two doctoral students (one of whom has now defended her dissertation on the topic), and trained a number of undergraduates about research, several of whom have now pursued graduate degrees.&nbsp;&nbsp;</p><br> <p>            Last Modified: 12/27/2019<br>      Modified by: Helena&nbsp;M&nbsp;Mentis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475013474_Figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475013474_Figure1--rgov-800width.jpg" title="OR Room Setup for Medical Image Telestration"><img src="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475013474_Figure1--rgov-66x44.jpg" alt="OR Room Setup for Medical Image Telestration"></a> <div class="imageCaptionContainer"> <div class="imageCaption">OR Room Setup for Medical Image Telestration shows the patient table and surgeons standing before a display which is connected to a computer that has a Kinect sensor above it.</div> <div class="imageCredit">Helena Mentis</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Helena&nbsp;M&nbsp;Mentis</div> <div class="imageTitle">OR Room Setup for Medical Image Telestration</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475505975_Figure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475505975_Figure2--rgov-800width.jpg" title="The Virtual Pointer"><img src="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475505975_Figure2--rgov-66x44.jpg" alt="The Virtual Pointer"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Left side: virtual pointer user interface with list of verbal commands (top left), current mode (top center), gesture recognition feedback window (bottom left), telestration (green lines on anatomy), and pointer (a green dot; not shown); Right Side: trainer drawing with the virtual pointer.</div> <div class="imageCredit">Yuanyuan Feng</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Helena&nbsp;M&nbsp;Mentis</div> <div class="imageTitle">The Virtual Pointer</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475665145_Figure3--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475665145_Figure3--rgov-800width.jpg" title="System diagram for virtual pointer setup"><img src="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475665145_Figure3--rgov-66x44.jpg" alt="System diagram for virtual pointer setup"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Laparoscopic video connects via DVI to Epiphan DVI converter to USB 3.0 into computer. Kinect connects to computer via USB 3.0. Output from computer via HDMI to DVI input on laparoscopic display.</div> <div class="imageCredit">Yuanyuan Feng</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Helena&nbsp;M&nbsp;Mentis</div> <div class="imageTitle">System diagram for virtual pointer setup</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475842002_Figure4--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475842002_Figure4--rgov-800width.jpg" title="Flowchart of Virtual Pointer user interaction"><img src="/por/images/Reports/POR/2019/1422671/1422671_10333998_1577475842002_Figure4--rgov-66x44.jpg" alt="Flowchart of Virtual Pointer user interaction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Diagram of the four commands (ready, point, draw, clear, end) and decision points for interaction.</div> <div class="imageCredit">Yuanyuan Feng</div> <div class="imageSubmitted">Helena&nbsp;M&nbsp;Mentis</div> <div class="imageTitle">Flowchart of Virtual Pointer user interaction</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Gestures, pointing, and movement of digital images are an integral aspect of decision making in collaborative work. What was needed was an understanding of these collaborative practices around image use and the benefit of technological tools for annotating and referencing images during collaboration.   Over the five years of this project, we made numerous discoveries about the problem of communication and coordination around images (specifically during endoscopic surgery). We learned how knowledge for the task content was either directly added or elicited by queries. To coordinate the procedure, a lead surgeons often provided repeated explicit instructions over the course of a surgery. We showed that the surgeons provided detailed instructions to guide their supporting surgeon step-by-step in the intricacies of a particular surgical procedure. We also learned that there is a tight connection between adopting expert gaze and performing surgical tasks, and this suggested that gaze training should be integrated into our system design.  We invented an approach to address this problem that began with a patent for using gestures and voice commands to point or draw on live endoscopic images. From this starting point we developed a system for use with any endoscopic video feed so collaborating surgeons could both refer to the medical images during surgery. In turn, we evaluated the impact of this method of interaction on communication efficiency, perceived usefulness in surgical training, educational gains, and surgical performance outcomes. We learned that with this virtual pointer, a trainees' eye fixations were more concentrated compared to a standard condition. However, when we investigated the communication structure and content, we show that the communication becomes more imbalanced and the trainees become less active when using the gesturing tool. And trainees had a tendency to attend to the telepointer instruction as the primary source of information when in a distributed environment.   We shared the findings from this research broadly to both medical and computer science researchers and practitioners in the form of presentations, publications, and demonstrations. The PI integrated the findings into her graduate level class on computer supported cooperative work. The grant also supported the professional development of the PI, directly supported the research of two doctoral students (one of whom has now defended her dissertation on the topic), and trained a number of undergraduates about research, several of whom have now pursued graduate degrees.         Last Modified: 12/27/2019       Submitted by: Helena M Mentis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
