<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: TTP Option: Anytime Visual Scene Understanding for Heterogeneous and Distributed Cyber-Physical Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2015</AwardEffectiveDate>
<AwardExpirationDate>12/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1397793.00</AwardTotalIntnAmount>
<AwardAmount>1413793</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Corman</SignBlockName>
<PO_EMAI>dcorman@nsf.gov</PO_EMAI>
<PO_PHON>7032928754</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Despite many advances in vehicle automation, much remains to be done: the best autonomous vehicle today still lags behind human drivers, and connected vehicle (V2V) and infrastructure (V2I) standards are only just emerging. In order for such cyber-physical systems to fully realize their potential, they must be capable of exploiting one of the richest and most complex abilities of humans, which we take for granted: seeing and understanding the visual world.  If automated vehicles had this ability, they could drive more intelligently, and share information about road and environment conditions, events, and anomalies to improve situational awareness and safety for other automated vehicles as well as human drivers.  That is the goal of this project, to achieve a synergy between computer vision, machine learning and cyber-physical systems that leads to a safer, cheaper and smarter transportation sector, and which has potential applications to other sectors including agriculture, food quality control and environment monitoring.&lt;br/&gt;&lt;br/&gt;To achieve this goal, this project brings together expertise in  computer vision, sensing, embedded computing, machine learning, big data analytics and sensor networks to develop an  integrated edge-cloud architecture for (1) "anytime scene understanding"  to unify diverse scene understanding methods in computer vision, and (2)  "cooperative scene understanding" that leverages vehicle-to-vehicle and vehicle-to-infrastructure protocols to  coordinate with multiple systems, while (3) emphasizing how security and  privacy should be managed at scale without impacting overall  quality-of-service. This architecture can be used for autonomous driving and driver-assist systems, and can be embedded within infrastructure (digital signs, traffic lights) to avoid traffic congestion, reduce risk of pile-ups and improve situational awareness. Validation and transition of the research to practice are through integration within City of Pittsburgh public works department vehicles, Carnegie Mellon University NAVLAB autonomous vehicles, and across the smart road infrastructure corridor under development in Pittsburgh.  The project also includes activities to foster development of a new cyber-physical systems workforce, though involvement of students in the research, co-taught multi-disciplinary courses, and co-organized workshops.</AbstractNarration>
<MinAmdLetterDate>09/09/2014</MinAmdLetterDate>
<MaxAmdLetterDate>10/23/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1446601</AwardID>
<Investigator>
<FirstName>Martial</FirstName>
<LastName>Hebert</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martial Hebert</PI_FULL_NAME>
<EmailAddress>martial.Hebert@cs.cmu.edu</EmailAddress>
<PI_PHON>4122682585</PI_PHON>
<NSF_ID>000225106</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Hoe</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James Hoe</PI_FULL_NAME>
<EmailAddress>jhoe@cmu.edu</EmailAddress>
<PI_PHON>4122684259</PI_PHON>
<NSF_ID>000464902</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christoph</FirstName>
<LastName>Mertz</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christoph J Mertz</PI_FULL_NAME>
<EmailAddress>cmertz@andrew.cmu.edu</EmailAddress>
<PI_PHON>4126069846</PI_PHON>
<NSF_ID>000099457</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Srinivasa</FirstName>
<LastName>Narasimhan</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Srinivasa G Narasimhan</PI_FULL_NAME>
<EmailAddress>srinivas@cs.cmu.edu</EmailAddress>
<PI_PHON>4122688746</PI_PHON>
<NSF_ID>000149438</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Bagnell</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James A Bagnell</PI_FULL_NAME>
<EmailAddress>dbagnell@ri.cmu.edu</EmailAddress>
<PI_PHON>4126818669</PI_PHON>
<NSF_ID>000483613</NSF_ID>
<StartDate>09/09/2014</StartDate>
<EndDate>10/23/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramReference>
<Code>8235</Code>
<Text>CPS-Synergy</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~1397793</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Autonomous cyber-physical systems such as self-driving vehicles are intended to improve road safety by reducing human error. However, even the most advanced systems do not compare to a human driver because a person has a far richer visual understanding of the the road environment and can always successfully detect and predict events and anomalies under any type of road or weather condition. Inadequate visual understanding is the main reason for a wide technology gap between autonomous and human driving. Visual understanding can be performed within a single vehicle or in the infrastructure and with wireless communication between them valuable information can be shared. Advanced computer vision algorithms in the areas of object detection, recognition and tracking, and 3D reconstruction have been applied to the problem of road scene understanding for lane, pedestrian, and vehicle detection and avoidance. However, these algorithms need to share results at different times depending on the time budget needed for effective, continuous (anytime) actuation, which is a fundamental departure from current autonomous CPS designs. For this project, we have developed computer vision algorithms and systems for multiple Infrastructure-to-Vehicle (I2V) and In-Vehicle CPS applications to demonstrate anytime visual understanding of the driving environment.</p> <p>Our I2V CPS research focused on developing computer vision and machine learning methods for a high-level understanding of vehicular activity at intersections. Methods developed include detecting, tracking, and performing 3D reconstruction of moving vehicles under challenging conditions. Our methods can accurately detect and track vehicles when they are not completely visible, which is a failure case for most vision methods. Dynamic 3D reconstruction of vehicles generally requires capturing images from multiple nearby cameras that are synchronized to simultaneously capture images and carefully calibrated. Our method for 3D reconstruction first self-calibrates and auto-synchronizes the cameras then accurately reconstructs the trajectories of vehicles that pass through an intersection. Finally, we developed a method to observe traffic activity over a long period of time to create a general model for normal driving patterns and speeds. Put together, these methods are the foundation of a robust, real-time framework for detecting anomalous and rare events such as accidents, near-misses, and erratic driving behavior.&nbsp;</p> <p>Our In-Vehicle CPS research focused on systems for monitoring roadway infrastructure, improving night time driver safety, and improving the vehicle platoon efficiency. The infrastructure assessment system uses a smartphone's camera and visual understanding algorithms to provide an affordable way to automatically inspect roads. Algorithms were developed to detect and classify cracks in the road, identify damaged stop signs, and measure snow accumulation and sign retro-reflectivity. We developed an adaptive headlight that can be programmed with algorithms to improve and enhance road visibility at night. We built a prototype and demonstrated that glaring oncoming drivers can be avoided, visibility can be improved in rain and snow storms, lane visibility can be enhanced, and obstacles can be spotlighted. We also built a sensor that can accurately estimate the distance to vehicles that are 0.5 to 3 meters away. Integrating this sensor with a vehicle's accelerator and brakes would allow rapid acceleration and deceleration to maintain a close following distance in a vehicle platoon. Traffic simulations showed that, in an urban environment, a 1 meter following distance would decrease commute times by 32% and vehicle emissions by 22%.</p> <p>To date, this project has produced eight publications at top tier conferences and at least forty demonstrations were given to government officials, students (high school, college, and special education), media, companies, conference attendees, and the general pubic. Over thirty presentations were given to seminars, conferences, meetings, State Representatives, industry visitors, and car companies. Cameras and computers were deployed in four locations in the City of Pittsburgh. Data was also collected at ten additional Pittsburgh intersections and the dataset has been made publicly available to researchers. To further disseminate our work, we are working on a website that will provide real-time analytics for our Pittsburgh deployment sites. The company Roadbotics was created to bring the road infrastructure inventory and assessment system to municipal governments, city planners, planning organizations, and engineering and construction firms. Roadbotics currently serves over two hundred customers (townships, cities, etc) in 34 U.S. states and 14 countries. Their customers have recognized enormous cost savings while also seeing the benefits of robust information that can be regularly updated. The project also acknowledges additional funding from U. S. Department of Transportation and The Heinz Foundation to deploy fundamental research done in this NSF project to the community.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/08/2020<br>      Modified by: Srinivasa&nbsp;G&nbsp;Narasimhan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Autonomous cyber-physical systems such as self-driving vehicles are intended to improve road safety by reducing human error. However, even the most advanced systems do not compare to a human driver because a person has a far richer visual understanding of the the road environment and can always successfully detect and predict events and anomalies under any type of road or weather condition. Inadequate visual understanding is the main reason for a wide technology gap between autonomous and human driving. Visual understanding can be performed within a single vehicle or in the infrastructure and with wireless communication between them valuable information can be shared. Advanced computer vision algorithms in the areas of object detection, recognition and tracking, and 3D reconstruction have been applied to the problem of road scene understanding for lane, pedestrian, and vehicle detection and avoidance. However, these algorithms need to share results at different times depending on the time budget needed for effective, continuous (anytime) actuation, which is a fundamental departure from current autonomous CPS designs. For this project, we have developed computer vision algorithms and systems for multiple Infrastructure-to-Vehicle (I2V) and In-Vehicle CPS applications to demonstrate anytime visual understanding of the driving environment.  Our I2V CPS research focused on developing computer vision and machine learning methods for a high-level understanding of vehicular activity at intersections. Methods developed include detecting, tracking, and performing 3D reconstruction of moving vehicles under challenging conditions. Our methods can accurately detect and track vehicles when they are not completely visible, which is a failure case for most vision methods. Dynamic 3D reconstruction of vehicles generally requires capturing images from multiple nearby cameras that are synchronized to simultaneously capture images and carefully calibrated. Our method for 3D reconstruction first self-calibrates and auto-synchronizes the cameras then accurately reconstructs the trajectories of vehicles that pass through an intersection. Finally, we developed a method to observe traffic activity over a long period of time to create a general model for normal driving patterns and speeds. Put together, these methods are the foundation of a robust, real-time framework for detecting anomalous and rare events such as accidents, near-misses, and erratic driving behavior.   Our In-Vehicle CPS research focused on systems for monitoring roadway infrastructure, improving night time driver safety, and improving the vehicle platoon efficiency. The infrastructure assessment system uses a smartphone's camera and visual understanding algorithms to provide an affordable way to automatically inspect roads. Algorithms were developed to detect and classify cracks in the road, identify damaged stop signs, and measure snow accumulation and sign retro-reflectivity. We developed an adaptive headlight that can be programmed with algorithms to improve and enhance road visibility at night. We built a prototype and demonstrated that glaring oncoming drivers can be avoided, visibility can be improved in rain and snow storms, lane visibility can be enhanced, and obstacles can be spotlighted. We also built a sensor that can accurately estimate the distance to vehicles that are 0.5 to 3 meters away. Integrating this sensor with a vehicle's accelerator and brakes would allow rapid acceleration and deceleration to maintain a close following distance in a vehicle platoon. Traffic simulations showed that, in an urban environment, a 1 meter following distance would decrease commute times by 32% and vehicle emissions by 22%.  To date, this project has produced eight publications at top tier conferences and at least forty demonstrations were given to government officials, students (high school, college, and special education), media, companies, conference attendees, and the general pubic. Over thirty presentations were given to seminars, conferences, meetings, State Representatives, industry visitors, and car companies. Cameras and computers were deployed in four locations in the City of Pittsburgh. Data was also collected at ten additional Pittsburgh intersections and the dataset has been made publicly available to researchers. To further disseminate our work, we are working on a website that will provide real-time analytics for our Pittsburgh deployment sites. The company Roadbotics was created to bring the road infrastructure inventory and assessment system to municipal governments, city planners, planning organizations, and engineering and construction firms. Roadbotics currently serves over two hundred customers (townships, cities, etc) in 34 U.S. states and 14 countries. Their customers have recognized enormous cost savings while also seeing the benefits of robust information that can be regularly updated. The project also acknowledges additional funding from U. S. Department of Transportation and The Heinz Foundation to deploy fundamental research done in this NSF project to the community.           Last Modified: 05/08/2020       Submitted by: Srinivasa G Narasimhan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
