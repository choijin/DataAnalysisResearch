<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Collaborative Research: Enhancing Cloud Performance with On-Demand Isolation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The modern trend in computing systems is towards system architectures containing a large numbers of heterogeneous computational and I/O resources. Unfortunately, while the increase in scale allows increased workload consolidation, wherein a single system runs multiple independent applications in parallel, it does come at the cost of introducing increased interference across the different application workloads. Workload interference is the result of the behavior of one application impacting the performance of another, even if both applications are running on different hardware resources. This can be due to contention on shared hardware resources (such as last level caches, memory controllers, or I/O devices) or even software resources managed by the operating system. Cross workload interference is especially problematic for large scale shared infrastructures such as cloud hosting services, which rely on co-hosting large numbers of widely disparate workloads inside a single datacenter environment. Preventing interference effects is critical for cloud computing to fully deliver on its promise as a universal computing substrate. &lt;br/&gt;&lt;br/&gt;This project addresses the problem of cross workload interference, by providing a holistic system that both detects the impact of interference on applications and mitigates its effects by providing dynamic isolation capabilities in the underlying system software. This approach relies on the ability to dynamically partition the underlying hardware resources such that isolation is achieved at the hardware layer, while also allowing the partitioning of system software to avoid contention on more abstract resources present in the system software itself. To achieve these goals this work implements a "Virtual Platform" abstraction representing an individual and isolatable system domain assigned to a particular task or workload and consisting of one or more virtual machine instances. The virtual platform itself is assigned an allocation of hardware resources consisting of independent "isolatable units." These units are created through the decomposition of local hardware resources into the finest grained subdivision of resources that can be both individually allocated and effectively isolated from the rest of the system. While providing partitioned hardware resources to a virtual platform provides hardware level isolation, it does not address interference generated by the system software. Avoiding software level interference is achieved by partitioning the system software itself through Multi-Stack Virtualization. Multi-stack virtualization allows multiple independent system software layers to co-exist on the same local system by restricting their managed resources to the set allocated to a virtual platform. Taken together this system provides full isolation capabilities at both the hardware and software layers.</AbstractNarration>
<MinAmdLetterDate>08/26/2014</MinAmdLetterDate>
<MaxAmdLetterDate>11/30/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422927</AwardID>
<Investigator>
<FirstName>Karsten</FirstName>
<LastName>Schwan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karsten Schwan</PI_FULL_NAME>
<EmailAddress>schwan@cc.gatech.edu</EmailAddress>
<PI_PHON>4048942589</PI_PHON>
<NSF_ID>000216799</NSF_ID>
<StartDate>08/26/2014</StartDate>
<EndDate>11/30/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ada</FirstName>
<LastName>Gavrilovska</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ada Gavrilovska</PI_FULL_NAME>
<EmailAddress>ada@cc.gatech.edu</EmailAddress>
<PI_PHON>4048940387</PI_PHON>
<NSF_ID>000494213</NSF_ID>
<StartDate>11/30/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As server platforms increase in the complexity and degree of integration of various components, it is more common that individual nodes in datacenters and server systems are shared by multiple consolidated workloads. While this improves the overall utilization of the hardware investments, it creates challenges concerning the ability to provide predictable performance to the co-running applications. The overall goal of this project is to develop better capabilities to provide performance guarantees and to manage isolation in shared infrastructures running collocated HPC and cloud workloads. This is collaborative work between Georgia Tech and University of Pittsburgh.&nbsp;</p> <p>&nbsp;</p> <p>This project developed new cross-stack, predictive and dynamic methods to achieve performance isolation for cloud systems, considering both compute and data intensive workloads. Concerning the first type of workloads, the project&rsquo;s efforts were focused on dynamic monitoring, characterization and management of CPUs and accelerators such as GPUs, whereas the second type of workloads demanded new methods for monitoring and managing the application&rsquo;s use of the memory resources across the entire memory hierarchy, including novel types of heterogeneous memory resources. Specific contributions made though this work include (i) new methods for collaborative resource management across collocated workloads, for large-scale parallel applications, or for workloads sharing different types of CPUs and accelerators on server platforms, (ii) new compiler-assisted method for automatically extracting information concerning workload&rsquo;s changes in execution phases and subsequent resource demands, which can further influence proactive resource management actions, and (iii) new techniques for dynamically characterizing applications&rsquo; demand for different types of memory resources, so as to enable more efficient allocation of different memories in hybrid memory systems.&nbsp;</p> <p>&nbsp;</p> <p>The project involved work with a broad range of applications, software technologies and hardware platforms. Concerning applications, the project targeted resource-intensive applications from the HPC community (including benchmarks from the Department of Energy CORAL suite, as well as real application codes such as the GTC fusion modeling code), standard benchmark suites representative of datacenter workloads (such as Parsec, CloudSuite and others), and new machine learning and graph processing application kernels representative of modern data analytics applications. We used typical Linux-based software stacks, with common libraries such as CUDA and OpenMP, but also considered novel task-based runtimes such as the Open Compute Runtime (OCR) and Charm++. Finally, we used a range of server platforms representative of high-end server configurations, based on both x86 (Xeon and Xeon Phi) and ARM systems, with accelerators such as NVIDIA GPUs, employed emulation-based techniques to model future systems which integrate heterogeneous memory techniques, and used several simulation frameworks to represent future server designs considered by the datacenter and the HPC exascale community.&nbsp;</p> <p>&nbsp;</p> <p>Intellectual merit: The approach developed in this project promotes use of cross-stack technique which combine lightweight monitoring, compiler-assisted methods for understanding application&rsquo;s upcoming resource demands, and fine-grained dynamic management of platform resources. The intellectual merit of the research conducted in this project includes new methods for resource management in consolidated platforms which attain both improved ability to maintain application&rsquo;s performance isolation and high resource efficiency. This is achieved by leveraging application-level information dynamically generated via new cross-stack APIs, inserted via instrumented libraries or statically, during compilation. This information is combined with new, sophisticated resource-specific performance models, to enlighten low-level resource management algorithms, concerning CPU, GPU, or memory allocation, and to execute more timely and more adequate scheduling decisions. One important aspect of this approach is that resource management triggers from the application/upper levels can be triggered &ldquo;ahead-of-time&rdquo;, probabilistically, in response to anticipated shifts in the workload&rsquo;s resource demand, thus opening up a greater scheduling window for the lower-level scheduling operations to be evaluated, executed and to take effect. This leads to new opportunities for future probabilistic scheduling methods.&nbsp;</p> <p>&nbsp;</p> <p>Broader impact: The results of the project were presented at several international conferences spanning several areas of computing: systems, programming languages and compilers, and high-performance computing. Additional technical outreach was achieved through presentations at national labs and at technology companies. Broader impacts were achieved through training and education. A number of graduate students and researchers were exposed to the platforms, software technologies, and methods used or developed in this project. This helped graduate students obtain competitive internships at industry (such as Intel or AMD) or full time positions (such as at Qualcomm). Two female graduate students were involved in the project.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/30/2018<br>      Modified by: Ada&nbsp;Gavrilovska</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As server platforms increase in the complexity and degree of integration of various components, it is more common that individual nodes in datacenters and server systems are shared by multiple consolidated workloads. While this improves the overall utilization of the hardware investments, it creates challenges concerning the ability to provide predictable performance to the co-running applications. The overall goal of this project is to develop better capabilities to provide performance guarantees and to manage isolation in shared infrastructures running collocated HPC and cloud workloads. This is collaborative work between Georgia Tech and University of Pittsburgh.      This project developed new cross-stack, predictive and dynamic methods to achieve performance isolation for cloud systems, considering both compute and data intensive workloads. Concerning the first type of workloads, the project?s efforts were focused on dynamic monitoring, characterization and management of CPUs and accelerators such as GPUs, whereas the second type of workloads demanded new methods for monitoring and managing the application?s use of the memory resources across the entire memory hierarchy, including novel types of heterogeneous memory resources. Specific contributions made though this work include (i) new methods for collaborative resource management across collocated workloads, for large-scale parallel applications, or for workloads sharing different types of CPUs and accelerators on server platforms, (ii) new compiler-assisted method for automatically extracting information concerning workload?s changes in execution phases and subsequent resource demands, which can further influence proactive resource management actions, and (iii) new techniques for dynamically characterizing applications? demand for different types of memory resources, so as to enable more efficient allocation of different memories in hybrid memory systems.      The project involved work with a broad range of applications, software technologies and hardware platforms. Concerning applications, the project targeted resource-intensive applications from the HPC community (including benchmarks from the Department of Energy CORAL suite, as well as real application codes such as the GTC fusion modeling code), standard benchmark suites representative of datacenter workloads (such as Parsec, CloudSuite and others), and new machine learning and graph processing application kernels representative of modern data analytics applications. We used typical Linux-based software stacks, with common libraries such as CUDA and OpenMP, but also considered novel task-based runtimes such as the Open Compute Runtime (OCR) and Charm++. Finally, we used a range of server platforms representative of high-end server configurations, based on both x86 (Xeon and Xeon Phi) and ARM systems, with accelerators such as NVIDIA GPUs, employed emulation-based techniques to model future systems which integrate heterogeneous memory techniques, and used several simulation frameworks to represent future server designs considered by the datacenter and the HPC exascale community.      Intellectual merit: The approach developed in this project promotes use of cross-stack technique which combine lightweight monitoring, compiler-assisted methods for understanding application?s upcoming resource demands, and fine-grained dynamic management of platform resources. The intellectual merit of the research conducted in this project includes new methods for resource management in consolidated platforms which attain both improved ability to maintain application?s performance isolation and high resource efficiency. This is achieved by leveraging application-level information dynamically generated via new cross-stack APIs, inserted via instrumented libraries or statically, during compilation. This information is combined with new, sophisticated resource-specific performance models, to enlighten low-level resource management algorithms, concerning CPU, GPU, or memory allocation, and to execute more timely and more adequate scheduling decisions. One important aspect of this approach is that resource management triggers from the application/upper levels can be triggered "ahead-of-time", probabilistically, in response to anticipated shifts in the workload?s resource demand, thus opening up a greater scheduling window for the lower-level scheduling operations to be evaluated, executed and to take effect. This leads to new opportunities for future probabilistic scheduling methods.      Broader impact: The results of the project were presented at several international conferences spanning several areas of computing: systems, programming languages and compilers, and high-performance computing. Additional technical outreach was achieved through presentations at national labs and at technology companies. Broader impacts were achieved through training and education. A number of graduate students and researchers were exposed to the platforms, software technologies, and methods used or developed in this project. This helped graduate students obtain competitive internships at industry (such as Intel or AMD) or full time positions (such as at Qualcomm). Two female graduate students were involved in the project.           Last Modified: 12/30/2018       Submitted by: Ada Gavrilovska]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
