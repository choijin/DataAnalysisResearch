<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Minimal-Latency Tracking and Display for Head-Worn Augmented Reality Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>499992.00</AwardTotalIntnAmount>
<AwardAmount>499992</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Augmented Reality (AR) enables computer images to be superimposed onto a user's view of his or her surroundings, most naturally via a head-worn display. For decades, many AR applications have been held back by bulky head-gear and inadequate displays. This situation is expected to change soon with the arrival of new commercially-available compact display designs that approach the form factor of eyeglasses. We expect these and other devices to spark renewed scientific and commercial interest in AR and its applications. AR systems, however, still suffer from a fatal flaw in that they are too slow for human vision. This slowness, or latency, hides in every subsystem, from a tracking-camera's capture, to deep rendering pipelines, to rendering frame buffers, to reformatting in the display controller. This latency causes causes misregistration between the synthetic imagery and its real-world counterparts. In other words, as you turn your head, the image that is supposed to remain superimposed on the real-world will start to move when it should not, and only later go back to where it should have stayed. This compromises the utility of the augmentation for many applications, in particular for high-precision uses such as aircraft maintenance or surgery. The proposed AR solution, combined with emerging comfortable, eyeglass-style head-worn displays should enable a wide range of applications to benefit from computer-generated visual augmentation: telepresence, medical examinations &amp; procedures, maintenance, and navigation. Many applications that today use conventional displays for visualization will be able to use head-worn displays and reap the benefits of natural hand-eye coordination with augmented imagery anywhere the user looks. The project will be integrated into multiple courses at the University of North Carolina at Chapel Hill, which will stimulate student exploration of new directions in rendering, tracking, image acquisition and reconstruction, augmented reality and telepresence. Research products will expose to the broader research and development community a new approach to real-time 3D vision and 3D graphics - scanline stream processing, from camera capture to display update - which yields dramatically lower latencies and thus higher-fidelity alignment between real and augmented imagery.&lt;br/&gt;&lt;br/&gt;The project will exploit a characteristic of inexpensive cameras (continuous scanout, or "rolling shutter") that until now has been seen as a significant weakness of these devices, and will demonstrate how this is in fact a significant asset for providing more frequent updates of scene information. The project will replace the classic frame-by-frame processing of each subsystem with a unified scanline-based approach that significantly reduces latency. The user's tracked head pose will be updated at every scanline, just after each scanline is streamed in from a cluster of cameras affixed to the user's head-worn display. To match this tracking performance, the project will render the augmented imagery in scan line fragments by directly controlling the pixels in the fastest available display technology, that of Digital Micro-mirror Displays (DMD). All of these operations will be aimed to be eventually performed within a mobile device, communicating wirelessly with the user's eyeglass-stye display. Such mobile operation should empower precise augmentation over the user's visual field for a wide range of useful AR applications.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423059</AwardID>
<Investigator>
<FirstName>Henry</FirstName>
<LastName>Fuchs</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Henry Fuchs</PI_FULL_NAME>
<EmailAddress>fuchs@cs.unc.edu</EmailAddress>
<PI_PHON>9199714951</PI_PHON>
<NSF_ID>000451367</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Turner</FirstName>
<LastName>Whitted</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Turner Whitted</PI_FULL_NAME>
<EmailAddress>jtw@cs.unc.edu</EmailAddress>
<PI_PHON>9195420261</PI_PHON>
<NSF_ID>000254527</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anselmo</FirstName>
<LastName>Lastra</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anselmo Lastra</PI_FULL_NAME>
<EmailAddress>lastra@cs.unc.edu</EmailAddress>
<PI_PHON>9195906058</PI_PHON>
<NSF_ID>000239343</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jan-Michael</FirstName>
<LastName>Frahm</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jan-Michael Frahm</PI_FULL_NAME>
<EmailAddress>jmf@cs.unc.edu</EmailAddress>
<PI_PHON>9195906003</PI_PHON>
<NSF_ID>000427356</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S. Columbia St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~165149</FUND_OBLG>
<FUND_OBLG>2015~334843</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project sought to reduce a major problem with head worn augmented reality (AR) display systems --the problem of latency. &nbsp;Latency in these systems is the time between when the pose of the headset is measured, and the time when the newly updated image is shown. That new image of the virtual content is then combined with the user's view of the real world. For example in a surgical application, this might be an ultrasound image seen inside the patient's body. A major difficulty is that the users head has moved by the time the image is shown and hence the synthetic objects appear in a slightly wrong place. &nbsp;In virtual reality (VR) systems the problem is not as visually obvious but as severe. It causes problems not only in task performance and is the source of much user discomfort and nausea ("cybersickness").&nbsp;<br />The research focussed eliminating latencies along the entire chain of sensing-processing-display, from the sensing of the motion, to the image generation pipeline, with display controller, and display device. Existing research indicated that overall latency of about 10 msec (milliseconds) may be desirable, and that under 1 ms may be even more effective. However, current system at best achieved 32 msec when using standard HDMI displays and cameras. This project focused on reducing the latencies in both of these crucial parts of an AR system: camera-based tracking and video-based display.<br />The research dramatically reduced the latency in a camera-based headset tracking by processing every scan line of an image rather than every complete image from a camera. &nbsp;We were able to do this by using inexpensive "rolling shutter" cameras, as used in cell phones. &nbsp;They continuously scan out the image sensor, one scan line at a time, keeping the image sensor exposed constantly. The value scanned out from any pixel is proportional to the amount of light energy that has accumulated there since that pixel was last scanned out. Hence, they can be seen as high speed moving line cameras. For purposes of this project on lowering latency, this staggered image capture is a benefit, and we took advantage of it by performing tracking computations on every single line, and updating the estimate of the headset's position and orientation when receiving a scanline from each of a cluster of cameras mounted on the headset. &nbsp;The resulting paper &nbsp;"Towards Kilo-Hertz 6-DoF Visual Tracking Using an Egocentric Cluster of Rolling Shutter Cameras," by Bapat et al. received the Best Paper Award at the 2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), one of the world's leading AR/VR conferences. After those results the project has yielded improved results by taking advantage of radial distortion in the lenses of the cameras. The results reduce the number of cameras needed for tracking. Future work is needed to move these results into real time; all these experiments had to be performed offline, mostly due to the lack of real-time, low-level access to rolling-shutter cameras.<br />The project also dramatically reduced the latency on the other end of the system pipeline: the display. Here the major addressed problem is that the display controllers often spend extra time to reformat the image. &nbsp;The project strategy here was to choose a display that allowed direct access to the pixels and then to eliminate the standard video interface. We had to solve the problem of generating image pixel data at such extremely high rates &mdash; we achieved, in real-time, over 16,000 binary frames per second. &nbsp;This was done by taking the last rendered image output by the rendering hardware (the GPU) and modifying it according to the latest tracking result. Our rendering process generates an image much wider and taller than the display's field of view, then this much-larger image was taken by the post-rendering processor to generate images for the DMD pixels by taking a proper piece out of this larger image, using the latest headset-tracking result, as published in &ldquo;From Motion to Photons in 80 Microseconds: Towards Minimal Latency for Virtual and Augmented Reality" by Lincoln et al. , which won the Best Paper Award at the 2016 IEEE VR Conference. An extension to high dynamic range illuminations, for instance when these AR systems are used in rooms with windows, having a surprisingly large range of illuminations, from near-sunlight to deep shadows is published in, "Scene-Adaptive High Dynamic Range Display for Low Latency Display," by Lincoln et al. in 2017 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, garnering Award for 2nd Best Paper.<br />Our research on low latency tracking and display led to the first AR/VR system pipeline for head worn systems that meets the low-latency demands of a variety of applications, reducing discomfort and sufficiently low latencies to enable the full productivity of the user.</p><br> <p>            Last Modified: 12/29/2017<br>      Modified by: Henry&nbsp;Fuchs</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513808852366_RS_TrackerISMAR2016--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513808852366_RS_TrackerISMAR2016--rgov-800width.jpg" title="Head-tracking with head-mounted RS cameras"><img src="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513808852366_RS_TrackerISMAR2016--rgov-66x44.jpg" alt="Head-tracking with head-mounted RS cameras"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Prototype cluster of rolling-shutter cameras and their sample results.</div> <div class="imageCredit">Akash Bapat, Enrique Dunn, Jan-Michael Frahm (UNC)</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Henry&nbsp;Fuchs</div> <div class="imageTitle">Head-tracking with head-mounted RS cameras</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513810313153_lowlatencydisplaysetupieeeVR2016--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513810313153_lowlatencydisplaysetupieeeVR2016--rgov-800width.jpg" title="Low latency display setup"><img src="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513810313153_lowlatencydisplaysetupieeeVR2016--rgov-66x44.jpg" alt="Low latency display setup"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Physical setup for the low latency head-mounted display, from Lincoln et al, "From motion to photons in 80 microseconds: Towards minimal latency for virtual and augmented reality. Proc. 2016 IEEE VR Conference and IEEE Transactions on Visualization and Computer Graphics, 22(4):1367?1376.</div> <div class="imageCredit">Peter Lincoln, Alex Blate, Montek Singh, Turner Whitted, Andrei State, AnselmoLastra, and Henry Fuchs (UNC)</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Henry&nbsp;Fuchs</div> <div class="imageTitle">Low latency display setup</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513810813187_teapotsandlatencyi3D2017--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513810813187_teapotsandlatencyi3D2017--rgov-800width.jpg" title="Scenes through low-latency head-mounted display"><img src="/por/images/Reports/POR/2017/1423059/1423059_10334002_1513810813187_teapotsandlatencyi3D2017--rgov-66x44.jpg" alt="Scenes through low-latency head-mounted display"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Images from a camera at user's eye position in the low latency AR HMD, showing both synthetic objects like teapots and real objects like the desk. From Lincoln et al "Scene-adaptive high dynamic range display for low latency augmented reality," in 2017 ACM SIGGRAPH Symp. on Interactive 3D & Games</div> <div class="imageCredit">Peter Lincoln, Alex Blate, Montek Singh, Andrei State, Mary Whitton, Turner Whitted, and Henry Fuchs (UNC)</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Henry&nbsp;Fuchs</div> <div class="imageTitle">Scenes through low-latency head-mounted display</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project sought to reduce a major problem with head worn augmented reality (AR) display systems --the problem of latency.  Latency in these systems is the time between when the pose of the headset is measured, and the time when the newly updated image is shown. That new image of the virtual content is then combined with the user's view of the real world. For example in a surgical application, this might be an ultrasound image seen inside the patient's body. A major difficulty is that the users head has moved by the time the image is shown and hence the synthetic objects appear in a slightly wrong place.  In virtual reality (VR) systems the problem is not as visually obvious but as severe. It causes problems not only in task performance and is the source of much user discomfort and nausea ("cybersickness").  The research focussed eliminating latencies along the entire chain of sensing-processing-display, from the sensing of the motion, to the image generation pipeline, with display controller, and display device. Existing research indicated that overall latency of about 10 msec (milliseconds) may be desirable, and that under 1 ms may be even more effective. However, current system at best achieved 32 msec when using standard HDMI displays and cameras. This project focused on reducing the latencies in both of these crucial parts of an AR system: camera-based tracking and video-based display. The research dramatically reduced the latency in a camera-based headset tracking by processing every scan line of an image rather than every complete image from a camera.  We were able to do this by using inexpensive "rolling shutter" cameras, as used in cell phones.  They continuously scan out the image sensor, one scan line at a time, keeping the image sensor exposed constantly. The value scanned out from any pixel is proportional to the amount of light energy that has accumulated there since that pixel was last scanned out. Hence, they can be seen as high speed moving line cameras. For purposes of this project on lowering latency, this staggered image capture is a benefit, and we took advantage of it by performing tracking computations on every single line, and updating the estimate of the headset's position and orientation when receiving a scanline from each of a cluster of cameras mounted on the headset.  The resulting paper  "Towards Kilo-Hertz 6-DoF Visual Tracking Using an Egocentric Cluster of Rolling Shutter Cameras," by Bapat et al. received the Best Paper Award at the 2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), one of the world's leading AR/VR conferences. After those results the project has yielded improved results by taking advantage of radial distortion in the lenses of the cameras. The results reduce the number of cameras needed for tracking. Future work is needed to move these results into real time; all these experiments had to be performed offline, mostly due to the lack of real-time, low-level access to rolling-shutter cameras. The project also dramatically reduced the latency on the other end of the system pipeline: the display. Here the major addressed problem is that the display controllers often spend extra time to reformat the image.  The project strategy here was to choose a display that allowed direct access to the pixels and then to eliminate the standard video interface. We had to solve the problem of generating image pixel data at such extremely high rates &mdash; we achieved, in real-time, over 16,000 binary frames per second.  This was done by taking the last rendered image output by the rendering hardware (the GPU) and modifying it according to the latest tracking result. Our rendering process generates an image much wider and taller than the display's field of view, then this much-larger image was taken by the post-rendering processor to generate images for the DMD pixels by taking a proper piece out of this larger image, using the latest headset-tracking result, as published in "From Motion to Photons in 80 Microseconds: Towards Minimal Latency for Virtual and Augmented Reality" by Lincoln et al. , which won the Best Paper Award at the 2016 IEEE VR Conference. An extension to high dynamic range illuminations, for instance when these AR systems are used in rooms with windows, having a surprisingly large range of illuminations, from near-sunlight to deep shadows is published in, "Scene-Adaptive High Dynamic Range Display for Low Latency Display," by Lincoln et al. in 2017 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, garnering Award for 2nd Best Paper. Our research on low latency tracking and display led to the first AR/VR system pipeline for head worn systems that meets the low-latency demands of a variety of applications, reducing discomfort and sufficiently low latencies to enable the full productivity of the user.       Last Modified: 12/29/2017       Submitted by: Henry Fuchs]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
