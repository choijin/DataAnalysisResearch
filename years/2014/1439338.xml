<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Using Somatosensory Speech And Non-Speech Categories To Test The Brain's General Principles Of Perceptual Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>616351.00</AwardTotalIntnAmount>
<AwardAmount>616351</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kurt Thoroughman</SignBlockName>
<PO_EMAI>kthoroug@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The human brain displays astonishing adaptation to novel types of sensory information. An example of such adaptation is deaf-blind individuals who learned to perceive spoken language through their sense of touch, by placing a hand on the face and throat of someone producing speech. This example tells us that the somatosensory system can carry out speech perception, which is normally thought to be in the domain of hearing. Drs. Maximilian Riesenhuber of Georgetown University and Lynne E. Bernstein of George Washington University along with their multidisciplinary team will use advanced functional magnetic resonance  brain imaging (fMRI) and electroencephalography (EEG) to investigate the neural mechanisms underlying the learning of artificial categories and speech categories by the somatosensory system. In their research they are using a novel transducer to present high-dimensional  stimuli to the forearm of participants who are trained on artificial or speech categories. The team is addressing whether perceptual learning of artificial categories of somatosensory patterns follows principles known to govern auditory and visual category learning. For their second aim, the researchers are training participants to recognize spoken words that are transformed into patterns of vibration. The speech stimuli are designed to address questions about cross-sensory learning and the linking of speech categories across hearing and vision. Before and following training, fMRI and EEG measures are being applied to determine where and when in the brain newly learned categories are represented. This project is pushing the frontiers of knowledge about the brain's plasticity for learning novel somatosensory categories, including showing for the first time the neural bases for speech learning through the sense of touch.&lt;br/&gt;&lt;br/&gt;Understanding the general principles of sensory processing in the brain, and in particular the commonalities and differences in the underlying neural mechanisms across sensory modalities, is of great interest for practical applications such as the design of neuroprostheses for hearing and/or vision disorders. For example, patients who have auditory or visual sensory system damage may benefit from devices that substitute vibrotactile stimuli for information no longer available through their damaged sensory systems. Vibrotactile stimuli can be combined with visual or auditory stimuli to improve speech perception in noisy situations such as the cockpit of a plane. The fMRI and EEG data from this project along with detailed records kept during training of participants will be made available to the research community. The brain measures obtained before and after training will be valuable for cost-effective testing of new hypotheses about brain plasticity and learning. Research results will be broadly disseminated through publications and conference presentations. The research project will also be leveraged extensively to train the next generation of scientists, at the graduate and undergraduate level, with a particular focus on underrepresented minorities.</AbstractNarration>
<MinAmdLetterDate>08/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1439338</AwardID>
<Investigator>
<FirstName>Maximilian</FirstName>
<LastName>Riesenhuber</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maximilian Riesenhuber</PI_FULL_NAME>
<EmailAddress>mr287@georgetown.edu</EmailAddress>
<PI_PHON>2026879198</PI_PHON>
<NSF_ID>000106418</NSF_ID>
<StartDate>08/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgetown University</Name>
<CityName>Washington</CityName>
<ZipCode>200571789</ZipCode>
<PhoneNumber>2026250100</PhoneNumber>
<StreetAddress>37th &amp; O St N W</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049515844</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGETOWN UNIVERSITY (THE)</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049515844</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgetown University]]></Name>
<CityName>WASHINGTON</CityName>
<StateCode>DC</StateCode>
<ZipCode>200072126</ZipCode>
<StreetAddress><![CDATA[3970 RESERVOIR RD NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1699</Code>
<Text>Cognitive Neuroscience</Text>
</ProgramElement>
<ProgramReference>
<Code>1699</Code>
<Text>COGNEURO</Text>
</ProgramReference>
<ProgramReference>
<Code>7298</Code>
<Text>COLLABORATIVE RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~616351</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The human brain's plasticity and learning capacity have produced astonishing examples of adaptation to novel types of sensory information. An example is deaf-blind individuals who learned to perceive spoken language through their sense of touch, through placing a hand on the face and throat of someone producing speech. This example tells us that the somatosensory sensory system can be adapted to a new function, even speech perception, which is often thought to be the domain of hearing.&nbsp; In our project, we used advanced functional magnetic resonance brain imaging (augmented with electroencephalography through our collaborators at George Washington University, led by Dr. Lynne Bernstein, under collaborative award BCS-1439339) together with a novel high-dimensional vibrotactile transducer to investigate the underlying basis for sensory learning in the brain by carrying out research on learning through the sense of touch.</p> <p>The first part of our project focused on category learning, which is needed for understanding language and for recognizing objects and their qualities. Regardless of whether stimuli arrive through touch, hearing, or vision, category learning requires the brain to recognize that some very similar stimuli nevertheless belong to different categories (e.g., an apple and a tennis ball belong to different categories even though they are both round and can be of similar color) while other stimuli with larger differences among them can nevertheless belong to the same category (e.g., an apple and a banana are both fruit). While much progress has been made in understanding learning in the visual and in the auditory sensory systems (including by our team under previous NSF awards), the neural mechanisms underlying learning in the somatosensory system, the third major sensory system in the human brain, are still poorly understood. A fundamental question is whether each of the senses learns and carries out its categorization functions using similar neural mechanisms, as predicted on computational grounds. To investigate this question, we trained human participants on a categorization task the required them to group morphed vibrotactile stimuli into novel categories. The stimuli were presented on the forearm of the trainees. Brain imaging (functional magnetic resonance imaging, fMRI) revealed that category learning in the vibrotactile modality follows a two-stage model that is analogous to the visual and auditory domains, providing evidence for common computational principles and a unified theory of perceptual categorization across the visual, auditory, and somatosensory systems.</p> <p>The finding from the first part of our project, of the same neurocomputational learning principles at work in the brain across the different senses, raised the question whether we could link different sensory processing pathways through learning. In the second part of our study, we trained hearing adult participants to recognize natural speech stimuli that had been transformed from auditory into complex vibrotactile stimuli. Language is a powerful domain to test hypotheses about sensory learning given its complexity, such as consonants and vowels making up syllables and whole words. Language is a powerful domain also because it can be used to test the extent to which the auditory system is unique in its ability to support speech perception. Using fMRI, we showed that both somatosensory and, interestingly, auditory brain regions acquired selectivity for vibrotactile speech stimuli following training. In particular, the right planum temporale, a key auditory processing area, was selective for both vibrotactile and auditory speech following training, and there was evidence that somatosensory brain areas that processed the vibrotactile stimuli came to communicate directly with auditory speech areas. Our results suggest that vibrotactile speech learning achieves integration with the auditory speech system by "piggybacking" onto corresponding auditory speech representations.&nbsp;</p> <p>The project has broad impact on a number of areas.&nbsp;Understanding the general principles of sensory processing in the brain, and in particular the commonalities and differences in the underlying neural mechanisms across sensory modalities, is of great interest for practical applications such as the design of neuroprostheses for hearing and/or vision disorders. For example, patients who have auditory or visual sensory system damage may benefit from devices that substitute vibrotactile stimuli for information no longer available through their damaged sensory systems. Evidence that the somatosensory cortex can be drafted for speech perception will have important implications for rehabilitation following stroke. Vibrotactile stimuli can be combined with visual or auditory stimuli to improve speech perception in noisy situations such as the cockpit of a plane.&nbsp;We are further currently in the process of making the fMRI data from this project available to the research community to allow for cost-effective testing of new hypotheses about brain plasticity and learning.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/29/2019<br>      Modified by: Maximilian&nbsp;Riesenhuber</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The human brain's plasticity and learning capacity have produced astonishing examples of adaptation to novel types of sensory information. An example is deaf-blind individuals who learned to perceive spoken language through their sense of touch, through placing a hand on the face and throat of someone producing speech. This example tells us that the somatosensory sensory system can be adapted to a new function, even speech perception, which is often thought to be the domain of hearing.  In our project, we used advanced functional magnetic resonance brain imaging (augmented with electroencephalography through our collaborators at George Washington University, led by Dr. Lynne Bernstein, under collaborative award BCS-1439339) together with a novel high-dimensional vibrotactile transducer to investigate the underlying basis for sensory learning in the brain by carrying out research on learning through the sense of touch.  The first part of our project focused on category learning, which is needed for understanding language and for recognizing objects and their qualities. Regardless of whether stimuli arrive through touch, hearing, or vision, category learning requires the brain to recognize that some very similar stimuli nevertheless belong to different categories (e.g., an apple and a tennis ball belong to different categories even though they are both round and can be of similar color) while other stimuli with larger differences among them can nevertheless belong to the same category (e.g., an apple and a banana are both fruit). While much progress has been made in understanding learning in the visual and in the auditory sensory systems (including by our team under previous NSF awards), the neural mechanisms underlying learning in the somatosensory system, the third major sensory system in the human brain, are still poorly understood. A fundamental question is whether each of the senses learns and carries out its categorization functions using similar neural mechanisms, as predicted on computational grounds. To investigate this question, we trained human participants on a categorization task the required them to group morphed vibrotactile stimuli into novel categories. The stimuli were presented on the forearm of the trainees. Brain imaging (functional magnetic resonance imaging, fMRI) revealed that category learning in the vibrotactile modality follows a two-stage model that is analogous to the visual and auditory domains, providing evidence for common computational principles and a unified theory of perceptual categorization across the visual, auditory, and somatosensory systems.  The finding from the first part of our project, of the same neurocomputational learning principles at work in the brain across the different senses, raised the question whether we could link different sensory processing pathways through learning. In the second part of our study, we trained hearing adult participants to recognize natural speech stimuli that had been transformed from auditory into complex vibrotactile stimuli. Language is a powerful domain to test hypotheses about sensory learning given its complexity, such as consonants and vowels making up syllables and whole words. Language is a powerful domain also because it can be used to test the extent to which the auditory system is unique in its ability to support speech perception. Using fMRI, we showed that both somatosensory and, interestingly, auditory brain regions acquired selectivity for vibrotactile speech stimuli following training. In particular, the right planum temporale, a key auditory processing area, was selective for both vibrotactile and auditory speech following training, and there was evidence that somatosensory brain areas that processed the vibrotactile stimuli came to communicate directly with auditory speech areas. Our results suggest that vibrotactile speech learning achieves integration with the auditory speech system by "piggybacking" onto corresponding auditory speech representations.   The project has broad impact on a number of areas. Understanding the general principles of sensory processing in the brain, and in particular the commonalities and differences in the underlying neural mechanisms across sensory modalities, is of great interest for practical applications such as the design of neuroprostheses for hearing and/or vision disorders. For example, patients who have auditory or visual sensory system damage may benefit from devices that substitute vibrotactile stimuli for information no longer available through their damaged sensory systems. Evidence that the somatosensory cortex can be drafted for speech perception will have important implications for rehabilitation following stroke. Vibrotactile stimuli can be combined with visual or auditory stimuli to improve speech perception in noisy situations such as the cockpit of a plane. We are further currently in the process of making the fMRI data from this project available to the research community to allow for cost-effective testing of new hypotheses about brain plasticity and learning.          Last Modified: 11/29/2019       Submitted by: Maximilian Riesenhuber]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
