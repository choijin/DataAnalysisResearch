<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Low-Light 3D Imaging: From Fundamental Limits to Practical Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>465189.00</AwardTotalIntnAmount>
<AwardAmount>465189</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Forming images with focused, spatially resolved light is an area of rapid innovation due to the growing use of computational methods and the variety of meanings of ?image.?  Aside from being photograph-like, an image acquired through optical means could show distances, thicknesses, chemical concentrations, molecular energy transfers and many other properties, depending on the methods of collection and analysis.  In many settings, it is desirable to form images from very little detected light; collecting more light could take more energy, time, or space, or it could damage a biological or molecular sample under investigation.  This project will develop new methods for computational image formation from very little detected light.  The goals are both theoretical and practical: to understand the fundamental trade-offs between accuracy and amount of detected light, and to approach the best possible performance.  The project has the potential to improve long-range remote sensing as well as various forms of microscopy.&lt;br/&gt;&lt;br/&gt;The semiclassical model of light detection dictates that light field intensity translates to the rate of a Poisson process observed at a single-photon detector output.  There are significant intellectual challenges in fully exploiting this prevailing model.  While progress has been made in recent years to incorporate piecewise smoothness (e.g., wavelet-domain sparsity) and other structure that is commonly found in natural signals, this progress has largely relied on measurement likelihood functions being Gaussian.  Signal processing methods designed for Poisson processes are uncommon, and most of them apply pointwise rather than to allow the inclusion of useful signal priors.  Naive combinations of convex signal priors with Poisson likelihoods can lead to intractable nonconvex optimizations. This project will develop alternative approaches, especially toward real-time 3D acquisition.</AbstractNarration>
<MinAmdLetterDate>06/13/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/13/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422034</AwardID>
<Investigator>
<FirstName>Vivek</FirstName>
<LastName>Goyal</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vivek K Goyal</PI_FULL_NAME>
<EmailAddress>goyal@bu.edu</EmailAddress>
<PI_PHON>6173534365</PI_PHON>
<NSF_ID>000661823</NSF_ID>
<StartDate>06/13/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~465189</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Lidar is a fundamental technology for measuring distances.&nbsp; It uses directed, monochromatic laser pulses, highly efficient light detectors, and precise timing electronics to record travel times for light that has traveled from the laser source to a scene patch of interest and then to the detector.&nbsp; Since the speed of light is known, the elapsed time reveals a round-trip distance.&nbsp; At the same time, the intensity of light at the detector reveals the reflectivity of the scene patch.&nbsp; By scanning the laser source over a region of interest, a full three-dimensional (3D) map of a scene can be created alongside a grayscale image.&nbsp; Lidar is used to produce very accurate maps in fields such as agriculture, archeology, geology, and surveying.&nbsp; Furthermore, for applications such as robotics and autonomous vehicle navigation, it is important for lidar systems to be able to form 3D maps quickly and in uncontrolled and adverse environments.</p> <p>The main focus of this project was to develop signal processing methods to make lidar systems work well even with very little ?signal? light (light emitted by the laser) reaching the detector and with large amounts of uninformative background light detected.&nbsp; This can enable increased range, increased transverse resolution, decreased acquisition time, decreased illumination power, or some combination of these, chosen based on the application.&nbsp; To make efficient use of light, a lidar system should use a single-photon detector (SPD) or an array of such detectors.&nbsp; These detectors have limitations such as coarse time resolution, dead time, and ?hot pixels,? so the project also developed signal processing methods to mitigate these limitations.</p> <p>The project introduced a method for forming depth and reflectivity images from only about 1 detected photon per pixel (on average over the scene), when about half the detected photons are from background light.&nbsp; This increases photon efficiency 100-fold over traditional image formation.&nbsp; The method combines probabilistic modeling of the photon detection events with exploitation of the spatial correlations present in real-world scenes. &nbsp;For depth, the accuracy performance compares favorably, by about a factor of 20, to state-of-the-art noise removal algorithms applied after conventional image formation.&nbsp; The project subsequently introduced improved methods that allow similar accuracy despite the presence of 25 times as much background light.&nbsp; The improvement is due to concentrating explicitly on how to separate, or ?unmix,? the detections due to laser light reflected back from the scene and the detections due to ambient light.</p> <p>In a conventional lidar system with a time-correlated single-photon counting (TCSPC) module, the time bin size directly determines the distance resolution. &nbsp;The project introduced the concept of shifting the time bin boundaries for different illumination pulses to emulate having finer time bins.&nbsp; In an experimental proof of concept, the root mean-squared depth error was reduced by a factor of 4.2 to 9.0.</p> <p>Dead times in lidar systems with TCSPC generally create biases in histogram counts, and these biases conventionally lead to inaccurate distance estimates.&nbsp; To avoid this, TCSPC hardware manufacturers suggest to keep the incident flux low.&nbsp; For asynchronous systems, which have greater photon efficiency than synchronous systems, the project introduced a Markov chain detection time model that rigorously characterizes the effect of dead times.&nbsp; Using these Markov Chain-based methods, distance estimation from high-flux detection data can achieve lower error than using low-flux data for the same acquisition time or can alternatively achieve the same error from much faster acquisitions.</p> <p>The project also introduced other computational methods for imaging with few photons.&nbsp; These include methods specialized for detector arrays, for handling an amount of background light that varies spatially, for estimating multiple distances at individual transverse locations, and for measurement architectures that produce measurements that are spatially mixed (i.e., combined across transverse locations).</p><br> <p>            Last Modified: 12/30/2018<br>      Modified by: Vivek&nbsp;K&nbsp;Goyal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Lidar is a fundamental technology for measuring distances.  It uses directed, monochromatic laser pulses, highly efficient light detectors, and precise timing electronics to record travel times for light that has traveled from the laser source to a scene patch of interest and then to the detector.  Since the speed of light is known, the elapsed time reveals a round-trip distance.  At the same time, the intensity of light at the detector reveals the reflectivity of the scene patch.  By scanning the laser source over a region of interest, a full three-dimensional (3D) map of a scene can be created alongside a grayscale image.  Lidar is used to produce very accurate maps in fields such as agriculture, archeology, geology, and surveying.  Furthermore, for applications such as robotics and autonomous vehicle navigation, it is important for lidar systems to be able to form 3D maps quickly and in uncontrolled and adverse environments.  The main focus of this project was to develop signal processing methods to make lidar systems work well even with very little ?signal? light (light emitted by the laser) reaching the detector and with large amounts of uninformative background light detected.  This can enable increased range, increased transverse resolution, decreased acquisition time, decreased illumination power, or some combination of these, chosen based on the application.  To make efficient use of light, a lidar system should use a single-photon detector (SPD) or an array of such detectors.  These detectors have limitations such as coarse time resolution, dead time, and ?hot pixels,? so the project also developed signal processing methods to mitigate these limitations.  The project introduced a method for forming depth and reflectivity images from only about 1 detected photon per pixel (on average over the scene), when about half the detected photons are from background light.  This increases photon efficiency 100-fold over traditional image formation.  The method combines probabilistic modeling of the photon detection events with exploitation of the spatial correlations present in real-world scenes.  For depth, the accuracy performance compares favorably, by about a factor of 20, to state-of-the-art noise removal algorithms applied after conventional image formation.  The project subsequently introduced improved methods that allow similar accuracy despite the presence of 25 times as much background light.  The improvement is due to concentrating explicitly on how to separate, or ?unmix,? the detections due to laser light reflected back from the scene and the detections due to ambient light.  In a conventional lidar system with a time-correlated single-photon counting (TCSPC) module, the time bin size directly determines the distance resolution.  The project introduced the concept of shifting the time bin boundaries for different illumination pulses to emulate having finer time bins.  In an experimental proof of concept, the root mean-squared depth error was reduced by a factor of 4.2 to 9.0.  Dead times in lidar systems with TCSPC generally create biases in histogram counts, and these biases conventionally lead to inaccurate distance estimates.  To avoid this, TCSPC hardware manufacturers suggest to keep the incident flux low.  For asynchronous systems, which have greater photon efficiency than synchronous systems, the project introduced a Markov chain detection time model that rigorously characterizes the effect of dead times.  Using these Markov Chain-based methods, distance estimation from high-flux detection data can achieve lower error than using low-flux data for the same acquisition time or can alternatively achieve the same error from much faster acquisitions.  The project also introduced other computational methods for imaging with few photons.  These include methods specialized for detector arrays, for handling an amount of background light that varies spatially, for estimating multiple distances at individual transverse locations, and for measurement architectures that produce measurements that are spatially mixed (i.e., combined across transverse locations).       Last Modified: 12/30/2018       Submitted by: Vivek K Goyal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
