<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Big Science Survey</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>177714.00</AwardTotalIntnAmount>
<AwardAmount>177714</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04010000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SMA</Abbreviation>
<LongName>SBE Off Of Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maryann Feldman</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Many decision processes in science, including hiring, tenure, promotion, and funding give considerable weight to citation-based metrics. It has in fact become commonplace to equate the number of citations received by an article with that article's "impact" or importance. Despite the plausibility of this equation, there are many reasons that scholars cite past work and there is no independent  empirical evidence to support this fundamental postulate of bibliometrics. This lack of independent verification calls into question the meaning and purpose of the entire field of research, and is often used as an argument against the validity of reliance on citation numbers in assessment exercises. This project will test the feasibility and develop the methodology necessary to generate the first large-scale set of data on "true" scientific impact as actually perceived by scientists.&lt;br/&gt;&lt;br/&gt;Measuring the true scientific impact of all articles will require surveying millions of corresponding authors of scientific publications as part of the biggest survey ever conducted in science. The main purpose of the present project is to establish the feasibility of the requisite survey methods. The research will survey small samples of authors to determine the optimal survey format necessary to achieve the appropriate tradeoff between response rate, information, and reliability. In particular, the research will explore variations in two basic ingredients of the survey format: content and design. We will select papers from different sample pools to quantify biases in participant preferences depending on external factors such as authorship, journal of publication, and number of citations accumulated. We will also consider different survey designs, and determine the most effective one able to increase the response of the potential participants and the amount of time that they will dedicate to the survey, without degrading the quality of the data obtained from their answers.</AbstractNarration>
<MinAmdLetterDate>08/01/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1446078</AwardID>
<Investigator>
<FirstName>Filippo</FirstName>
<LastName>Radicchi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Filippo Radicchi</PI_FULL_NAME>
<EmailAddress>filiradi@indiana.edu</EmailAddress>
<PI_PHON>8128559322</PI_PHON>
<NSF_ID>000671177</NSF_ID>
<StartDate>08/01/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474083912</ZipCode>
<StreetAddress><![CDATA[919 East 10th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7626</Code>
<Text>SciSIP-Sci of Sci Innov Policy</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~177714</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The current trend in the evaluation of scientific activities is to rely more and even more on bibliographic data to measure scientific impact. Metrics of scientific impact are based, in most of the cases, on the total number of citations accumulated by publications, i.e., total number of times that a paper appears in the reference list of other papers. As many decision processes in science, including hiring, tenure, and promotion, give considerable weight to citation-based metrics, testing whether the number of citations received by an article is representative for the article's "impact" or importance is a fundamental question that requires empirical validation. The goal of this project was to test the feasibility and develop the methodology necessary to generate the first large-scale set of data on "true" scientific impact as actually perceived by scientists.</p> <p><br />To this end, we designed a large-scale survey at <a title="bigscience.soic.indiana.edu" href="bigscience.soic.indiana.edu" target="_blank">bigscience.soic.indiana.edu</a> to collect responses from experienced scholars in a multitude of disciplines. The electronic survey has a simple design where researchers are asked to make a pairwise decision with regards to their preference of one paper over the other. Data collected from the electronic survey can be then aggregated to quantify the degree of correlation between the pairwise preferences of respondents (i.e., perceived impact) and the actual difference in the number of citations accumulated (i.e., citation impact) for the pair of papers. To build the infrastructure needed for the survey, we collected and cleaned a big dataset of scientific publications. For each publication, we extracted several information, such as title and journal of publication, email address(es) of the corresponding author(s), and total number of citations. We then generated personalized surveys for potential participants. To test the feasibility of the approach, we performed surveys in three major public universities in the US: Indiana University, University of Michigan, and University of Minnesota. We obtained a good response rate (10%) for a total of about 2,000 participants. Overall, respondents expressed positive feedback about the design of the survey, and great interest for the scientific question that we are trying to answer. These facts are encouraging for future experiments. Preliminary results from pilot experiments provided empirical evidence of a positive correlation between "citation impact" and "perceived impact."&nbsp;</p><br> <p>            Last Modified: 11/16/2016<br>      Modified by: Filippo&nbsp;Radicchi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The current trend in the evaluation of scientific activities is to rely more and even more on bibliographic data to measure scientific impact. Metrics of scientific impact are based, in most of the cases, on the total number of citations accumulated by publications, i.e., total number of times that a paper appears in the reference list of other papers. As many decision processes in science, including hiring, tenure, and promotion, give considerable weight to citation-based metrics, testing whether the number of citations received by an article is representative for the article's "impact" or importance is a fundamental question that requires empirical validation. The goal of this project was to test the feasibility and develop the methodology necessary to generate the first large-scale set of data on "true" scientific impact as actually perceived by scientists.   To this end, we designed a large-scale survey at bigscience.soic.indiana.edu to collect responses from experienced scholars in a multitude of disciplines. The electronic survey has a simple design where researchers are asked to make a pairwise decision with regards to their preference of one paper over the other. Data collected from the electronic survey can be then aggregated to quantify the degree of correlation between the pairwise preferences of respondents (i.e., perceived impact) and the actual difference in the number of citations accumulated (i.e., citation impact) for the pair of papers. To build the infrastructure needed for the survey, we collected and cleaned a big dataset of scientific publications. For each publication, we extracted several information, such as title and journal of publication, email address(es) of the corresponding author(s), and total number of citations. We then generated personalized surveys for potential participants. To test the feasibility of the approach, we performed surveys in three major public universities in the US: Indiana University, University of Michigan, and University of Minnesota. We obtained a good response rate (10%) for a total of about 2,000 participants. Overall, respondents expressed positive feedback about the design of the survey, and great interest for the scientific question that we are trying to answer. These facts are encouraging for future experiments. Preliminary results from pilot experiments provided empirical evidence of a positive correlation between "citation impact" and "perceived impact."        Last Modified: 11/16/2016       Submitted by: Filippo Radicchi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
