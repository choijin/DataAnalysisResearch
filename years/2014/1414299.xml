<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: 3D Gaze Control for Assistive Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>136353.00</AwardTotalIntnAmount>
<AwardAmount>136353</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07020000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CBET</Abbreviation>
<LongName>Div Of Chem, Bioeng, Env, &amp; Transp Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michele Grimm</SignBlockName>
<PO_EMAI>mgrimm@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>PI: Zhang, Xiaoli and Nelson, Carl A.&lt;br/&gt;Proposal Number: 1264496 &amp; 1264504&lt;br/&gt;&lt;br/&gt;The objective of this research is to develop a novel 3D gaze control system to intuitively connect elderly or disabled people with robotic assistants. The communication will take place through a 3D gaze as the robot control signal. We will focus on basic Activities of Daily Living, including retrieving an object to the user, and moving an object from one place to another, which enable the individual to live independently. Tasks include: (1) a study of 3D gaze estimation based on eye modeling and simulation with experimental data; (2) investigation and development of a 3D gaze control framework to enable the target selection relative to real-world, everyday objects on which assistive tasks are performed; (3) establishment of a gaze control platform for testing and evaluating the proposed 3D gaze control model with a wide range of assistive robot applications.&lt;br/&gt;&lt;br/&gt;Intellectual Merit: The project investigates a novel 3D gaze control concept for robotic assistants with the goal of increasing the level of independence for motor impaired people due to diseases or senescence. The research also proposes a novel gaze control model which uses gaze-extracted features to guide a robot for object identification and operation, achieving simple and natural human-robot interaction. Finally, the project seeks to develop a novel 3D gaze estimation system to ensure accuracy and reliability, which is currently not well explored. This project seeks to provide solutions for a wide spectrum of robotic assistive applications.&lt;br/&gt;&lt;br/&gt;Broader Impacts: By introducing 3D gaze as the control signal into communication between elderly or disabled people and robotic assistants, the broader impacts of this project are to build a simple and natural control interface to motor impaired people and increase the level of living independence. Persons with disabilities, especially students with disabilities at Wilkes University, will be actively involved in the development of the proposed work including feedback in the form of interviews, surveys, and participation in testing and evaluating the working system. Results, outcomes, software tools, benchmarks, and educational materials will be disseminated through a project web site, as well as through journal and conference publications. A new course on assistive robotic technology is being developed and taught at Wilkes University.</AbstractNarration>
<MinAmdLetterDate>12/10/2013</MinAmdLetterDate>
<MaxAmdLetterDate>12/10/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1414299</AwardID>
<Investigator>
<FirstName>Xiaoli</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaoli Zhang</PI_FULL_NAME>
<EmailAddress>xlzhang@mines.edu</EmailAddress>
<PI_PHON>3033842343</PI_PHON>
<NSF_ID>000654357</NSF_ID>
<StartDate>12/10/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Colorado School of Mines</Name>
<CityName>Golden</CityName>
<ZipCode>804011887</ZipCode>
<PhoneNumber>3032733000</PhoneNumber>
<StreetAddress>1500 Illinois</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>010628170</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE COLORADO SCHOOL OF MINES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>010628170</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[colorado school of mines]]></Name>
<CityName>golden</CityName>
<StateCode>CO</StateCode>
<ZipCode>804011887</ZipCode>
<StreetAddress><![CDATA[1500 illinois st]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~136353</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was focused on the development of a novel 3D gaze control system which enables individuals with severe disabilities to control assistive robots using their eyes. In particular, the emphasis was placed on the development of a novel wearable 3D gaze tracking system and the development of a 3D-gaze-based intelligent control platform for object retrieval.</p> <p>In a first phase of the research, we developed a novel 3D gaze estimation algorithm based on an eye-model-based method. A wearable gaze tracking system was built for testing and evaluating the estimation method. The experimental results demonstrate the capability of accurately estimating 3D gaze using the proposed 3D gaze estimation method. A machine learning method was developed to classify intentional manipulatory gaze from non-intentional gaze (observational gaze).</p> <p>In a second phase of the research, we sought to develop a 3D-gaze-based control platform for reaching and fetching objects. The gaze tracking system and robot systems (a NAO humanoid robot and a MICO robot arm, separately) were integrated into the platform for testing and evaluation. The 3D position and pose of the target object were derived by interpreting the tracked 3D gaze data and used as the control signal for object reaching and fetching. The usefulness and intuitiveness of the overall platform were tested and demonstrated by experiments and questionnaires in user studies.</p> <p>In addition to the scientific contributions of this project described above (algorithm, software, and hardware development), it is envisioned that this work can lead to improved human-robot interfaces for the severely disabled, enabling these individuals to more actively engage with their environment and perform healthy activities of daily living. This can provide a key component of quality of life.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/05/2016<br>      Modified by: Xiaoli&nbsp;Zhang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was focused on the development of a novel 3D gaze control system which enables individuals with severe disabilities to control assistive robots using their eyes. In particular, the emphasis was placed on the development of a novel wearable 3D gaze tracking system and the development of a 3D-gaze-based intelligent control platform for object retrieval.  In a first phase of the research, we developed a novel 3D gaze estimation algorithm based on an eye-model-based method. A wearable gaze tracking system was built for testing and evaluating the estimation method. The experimental results demonstrate the capability of accurately estimating 3D gaze using the proposed 3D gaze estimation method. A machine learning method was developed to classify intentional manipulatory gaze from non-intentional gaze (observational gaze).  In a second phase of the research, we sought to develop a 3D-gaze-based control platform for reaching and fetching objects. The gaze tracking system and robot systems (a NAO humanoid robot and a MICO robot arm, separately) were integrated into the platform for testing and evaluation. The 3D position and pose of the target object were derived by interpreting the tracked 3D gaze data and used as the control signal for object reaching and fetching. The usefulness and intuitiveness of the overall platform were tested and demonstrated by experiments and questionnaires in user studies.  In addition to the scientific contributions of this project described above (algorithm, software, and hardware development), it is envisioned that this work can lead to improved human-robot interfaces for the severely disabled, enabling these individuals to more actively engage with their environment and perform healthy activities of daily living. This can provide a key component of quality of life.          Last Modified: 12/05/2016       Submitted by: Xiaoli Zhang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
