<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Bayesian Optimization for Exploratory Experimentation in the Behavioral Sciences</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2015</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>402275</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project will develop an exploratory experimentation methodology for human behavioral research that will allow cognitive scientists to efficiently identify optimal conditions -- those leading to the most robust learning, the fastest performance, the fewest errors, the best decisions and choices.  The tools to be developed will allow scientists to answer questions they cannot currently address due to the massive data collection effort required.  To understand and predict human behavior, scientists typically perform controlled experiments that compare a small, carefully chosen set of experimental conditions.  For example, in designing instructional software, a comparison might be made between two techniques for teaching students.  The finding that one technique obtains reliably better outcomes has both practical and theoretical implications.  However, this result does not answer the question one often wishes to ask: what is the very best possible technique?  The methodology to be developed will allow scientists to evaluate many experimental conditions with only a few participants, in contrast to the traditional controlled experiment which evaluates only a few conditions each with many participants.  A key product of the project will be black-box software that researchers in various disciplines of the cognitive sciences can use to apply exploratory experimentation to problems in their own field.  Experimental studies also will be conducted to demonstrate the breadth of the approach in domains including: concept acquisition, color aesthetics, formal instruction, and the design of usable and engaging software.&lt;br/&gt;&lt;br/&gt;The project will extend Bayesian optimization methods to human experimental research.  Bayesian optimization has long been used in the geostatistics community for inferring unobserved properties (e.g., oil reserves below the earth's surface) from costly measurements (e.g., drilling tests).  In the current project, the "landscapes" being explored are defined over possible conditions (e.g., training strategies), the unobserved properties are internal cognitive states of the human observer, and the measurements are obtained via behavioral evaluations (e.g., assessments of learning).  To apply Bayesian optimization methods to a range of human experimental research, mathematical models will be developed for multiple behavioral response measures, including choice, ranking, rating, latency, and free recall.  The exploratory nature of the approach requires heuristics for sequentially selecting experimental conditions to obtain maximally informative data given prior observations.  Various heuristics will be evaluated in the context of behavioral research.</AbstractNarration>
<MinAmdLetterDate>04/02/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1461535</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Mozer</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael C Mozer</PI_FULL_NAME>
<EmailAddress>mozer@cs.colorado.edu</EmailAddress>
<PI_PHON>3034924103</PI_PHON>
<NSF_ID>000467581</NSF_ID>
<StartDate>04/02/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090430</ZipCode>
<StreetAddress><![CDATA[UCB 0001]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~178789</FUND_OBLG>
<FUND_OBLG>2016~108040</FUND_OBLG>
<FUND_OBLG>2017~115446</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>The primary goal of this project was to develop an&nbsp;</span><em>exploratory experimentation</em><span>&nbsp;methodology for human behavioral research that allows researchers in the behavioral sciences to efficiently identify optimal conditions&mdash;those leading to the most robust learning, the fastest performance, the fewest errors, the best decisions and choices. The project focused primarily on educational interventions. One line of research investigated how to best sequence examples presented to a learner. The project explored passive versus active examples (i.e., viewing worked through examples versus solving a problem), blocking versus interleaving (i.e., viewing multiple examples of the same concept in a row versus interspersing examples of different concepts), and difficulty fading (i.e., variation in example difficulty with practice). Another line of research examined engagement in educational video games via manipulations such as the covert versus overt manipulation of difficulty (i.e., to what degree should the learner be given hints without their awareness) and leveraging near-win and tension-release effects (manipulations used to make gambling and music/film more engaging). Traditionally, in each of these domains, behavioral scientists have done a form of A-B testing to compare pairs of conditions to determine which one achieves better outcomes. Instead, the focus of this project was to define a parameterized space of conditions and to explore the entire space. For example, our methodology does not simply choose between blocking and interspersing examples; it parameterizes the sequencing in terms of the probability that a task or category is repeated, where blocking and interleaving are two points on this continuum (very high and very low probabilities, respectively). The methodology applies to a range of problems in the behavioral sciences. For instance, we worked with a researcher on human color perception to characterize human preferences over a space of individual colors and color combinations; and our software tool was tested by a researcher in integrative physiology to&nbsp;</span>characterize perceived effort of arm reaches as a function of distance and mass being moved.</p> <p>Two methodologies were developed to perform exploratory experimentation: model free and model based.&nbsp; The model-free methodology leveraged Bayesian optimization techniques in conjunction with psychologically informed observation models. This methodology treats the human as a black box who provides measurements (e.g., accuracy on a test) given conditions (e.g., the manner of training). Gaussian processes are then used&nbsp;to approximate the smooth function that relates conditions to measurement, and the Gaussian process posterior is used as a surrogate model to determine the next conditions to examine in order to achieve efficient estimation with minimal experimentation. The model-based methodology involves collecting human data to construct and constrain models of human perception and cognition, and then performs optimization with these models as human surrogates.</p> <p>In addition to the experimental studies and novel methodologies developed under this project, the project produced several software packages useful for future research. The package <em>BOHE</em> incorporates Bayesian optimization into a toolkit that can be used for human behavioral experimentation. This package has specialized observation models for human choice, latency, accuracy, and preferences; and it can be integrated into the flow of experiment software to select conditions for the next participant conditioned on data already collected. The package <em>psiz </em>infers psychological embeddings&mdash;representations of visual stimuli consistent with cognitive processes and decision making. These embeddings are critical for predicting human learning and performance. The package performs active selection to pick instances for human judgment that will be most effective in constraining the embedding.</p><br> <p>            Last Modified: 01/08/2020<br>      Modified by: Michael&nbsp;C&nbsp;Mozer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary goal of this project was to develop an exploratory experimentation methodology for human behavioral research that allows researchers in the behavioral sciences to efficiently identify optimal conditions&mdash;those leading to the most robust learning, the fastest performance, the fewest errors, the best decisions and choices. The project focused primarily on educational interventions. One line of research investigated how to best sequence examples presented to a learner. The project explored passive versus active examples (i.e., viewing worked through examples versus solving a problem), blocking versus interleaving (i.e., viewing multiple examples of the same concept in a row versus interspersing examples of different concepts), and difficulty fading (i.e., variation in example difficulty with practice). Another line of research examined engagement in educational video games via manipulations such as the covert versus overt manipulation of difficulty (i.e., to what degree should the learner be given hints without their awareness) and leveraging near-win and tension-release effects (manipulations used to make gambling and music/film more engaging). Traditionally, in each of these domains, behavioral scientists have done a form of A-B testing to compare pairs of conditions to determine which one achieves better outcomes. Instead, the focus of this project was to define a parameterized space of conditions and to explore the entire space. For example, our methodology does not simply choose between blocking and interspersing examples; it parameterizes the sequencing in terms of the probability that a task or category is repeated, where blocking and interleaving are two points on this continuum (very high and very low probabilities, respectively). The methodology applies to a range of problems in the behavioral sciences. For instance, we worked with a researcher on human color perception to characterize human preferences over a space of individual colors and color combinations; and our software tool was tested by a researcher in integrative physiology to characterize perceived effort of arm reaches as a function of distance and mass being moved.  Two methodologies were developed to perform exploratory experimentation: model free and model based.  The model-free methodology leveraged Bayesian optimization techniques in conjunction with psychologically informed observation models. This methodology treats the human as a black box who provides measurements (e.g., accuracy on a test) given conditions (e.g., the manner of training). Gaussian processes are then used to approximate the smooth function that relates conditions to measurement, and the Gaussian process posterior is used as a surrogate model to determine the next conditions to examine in order to achieve efficient estimation with minimal experimentation. The model-based methodology involves collecting human data to construct and constrain models of human perception and cognition, and then performs optimization with these models as human surrogates.  In addition to the experimental studies and novel methodologies developed under this project, the project produced several software packages useful for future research. The package BOHE incorporates Bayesian optimization into a toolkit that can be used for human behavioral experimentation. This package has specialized observation models for human choice, latency, accuracy, and preferences; and it can be integrated into the flow of experiment software to select conditions for the next participant conditioned on data already collected. The package psiz infers psychological embeddings&mdash;representations of visual stimuli consistent with cognitive processes and decision making. These embeddings are critical for predicting human learning and performance. The package performs active selection to pick instances for human judgment that will be most effective in constraining the embedding.       Last Modified: 01/08/2020       Submitted by: Michael C Mozer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
