<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation: Investigating the role of grammatical representation in language learnability</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>11710.00</AwardTotalIntnAmount>
<AwardAmount>11710</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Technologies which process natural language have become ubiquitous in the last decade. Web search engines, for example, process billions of pages of text, in order to determine which of those pages best match a user's search query. Many interfaces for interacting with computers -- for example, Apple's Siri personal assistant -- take voice-issued commands from their users, and must process these commands in order to follow the users' instructions. Finally, machine translation technologies have become available for many of the world's most common languages, allowing users to automatically translate text that they find in foreign books or websites. These technologies mostly rely on simple models of language, known as n-gram models or context-free grammars, which were developed in the 1950's and 1960's, and refined in later decades. These simple models of language have many advantages, most notably that they can be used to process large amounts of data very quickly. Because of their simplicity, however, these models are not able to capture many aspects of meaning in natural language. This has resulted in limitations for the technologies discussed above; virtual personal assistants are only able to process very simple types of instructions, and machine translations is still far from being as accurate as human translation. In the current project, Leon Bergen and Dr. Edward Gibson will be investigating more sophisticated kinds of language models, with the goal of increasing the ability of computers to understand language.&lt;br/&gt;&lt;br/&gt;Under the direction of Dr. Gibson, Mr. Berger will be studying language models known as mildly context-sensitive grammars. These grammars are able to express certain types of linguistic knowledge that humans have, but which cannot be expressed using simpler types of grammatical formalisms. For example, native speakers of English know that a declarative sentence like "Mary kicked the ball" is closely related in meaning to the question "What did Mary kick?" Although this fact seems obvious, it is difficult (or impossible) to express using simple types of grammars. However, mildly context-sensitive grammars can be used to express this knowledge in a very natural way. Mr. Bergen and Dr. Gibson will be studying whether mildly context-sensitive grammars can be automatically learned from examples of grammatical sentences. To do this, they will be using techniques from machine learning, a branch of computer science and statistics that develops algorithms that can automatically learn from data. The researchers will integrate these learning algorithms with their grammatical formalism, and will test whether their method learns an accurate grammar. The accuracy of the grammar will be evaluated using a corpus -- a collection of sentences -- in which every sentence has been manually annotated with its correct grammatical structure. If accurate mildly context-sensitive grammars can be learned in this manner, then this provides a potential method for improving the natural language processing technologies which were discussed above. In particular, because this method does not require an expert to write down the complete grammar for a language, it has the potential to be deployed without tremendous engineering effort, and may be deployed easily in foreign languages.</AbstractNarration>
<MinAmdLetterDate>07/08/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/08/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1420785</AwardID>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Gibson</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward A Gibson</PI_FULL_NAME>
<EmailAddress>egibson@mit.edu</EmailAddress>
<PI_PHON>6172538609</PI_PHON>
<NSF_ID>000215436</NSF_ID>
<StartDate>07/08/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Leon</FirstName>
<LastName>Bergen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leon Bergen</PI_FULL_NAME>
<EmailAddress>LBergen@ucsd.edu</EmailAddress>
<PI_PHON>8582462757</PI_PHON>
<NSF_ID>000663312</NSF_ID>
<StartDate>07/08/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~11710</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project aimed to investigate the relationship between grammatical representations and language learning. As an example of this relationship, consider the following sentence:</p> <p>1) John put the socks in the drawer.</p> <p>In example (1), there are three phrases associated with the verb "put": "John" (who is doing the putting), "the socks" (which are being put), and "in the drawer" (which is the destination for the socks). If any of these phrases is removed, the sentence becomes ungrammatical. For example, if "in the drawer" is removed, the sentence becomes:</p> <p>1a) #John put the socks.</p> <p>This indicates that "put" requires three phrases for it to be grammatical. In linguistic terminology, this means that there are three parts to its argument structure: something doing the putting, something being put, and a destination for the putting. More generally, the argument structure for a word is a specification of what types of phrases are required for it to be used grammatically.&nbsp;</p> <p><br />Not all phrases are part of the argument structure of a word. Consider the following example:</p> <p>2) John put the socks in the drawer before dinner.</p> <p>In this sentence, the phrase "before dinner" is grammatically optional; the sentence is still grammatical if it is removed. This phrase is therefore not part of the argument structure of the sentence, but rather is called a modifier of the sentence.</p> <p>&nbsp;</p> <p>Fluent speakers of English know the argument structure for "put", and for the other words in the language. For the different phrases in a sentence, they can determine whether that phrase is a mandatory argument, or whether it is an optional modifier. In our research, we wanted to address two questions. First, how might people learn the argument structures in their language? Second, if we wanted to design a computer system which could speak English or another language, how could that system learn the language's argument structures?&nbsp;<br />In order to address these questions, we developed a computational model of this argument-structure learning task. This system combines grammatical representations developed in linguistics with Bayesian machine learning techniques. It classifies a phrase as an argument structure when this leads to greater compression of the sentences that have been observed. We found that this computational model learns to substantially improve its classification of argument structure.</p> <p><br />In a second part of the project, we investigated how people and computers can learn to understand questions. Consider the following sequence of questions:</p> <p>3a) Who did the man see?</p> <p>3b) Who did the man who plays the trumpet see?</p> <p>3c) Who did the man who plays the trumpet which was stolen see?</p> <p>In each of these questions, the speaker is asking about who was seen. Intuitively, the wh-term "Who" is linked to the verb "see" at the end of the sentence, and serves as the argument of this verb. As the sequence (3a-c) illustrates, the wh-term in a question and the verb it is linked to can be separated by linguistic material of arbitrarily large complexity. For this reason, the dependency between the wh-term and the verb is known as a long-distance dependency.</p> <p><br />There is a challenge in understanding these sentences: how does one determine that the wh-term "who" is associated with the verb "see," rather than the other verbs in the sentence? More generally, how do people learn to resolve these long-distance dependencies, and how can we build computer systems which learn to resolve them?&nbsp;</p> <p><br />In order to try to address these questions, we developed a computational model for learning the structure of questions, and long-distance dependencies more generally. This model uses a grammatical formalism for representing long-distance dependencies known as minimalist grammars, and combines this with ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project aimed to investigate the relationship between grammatical representations and language learning. As an example of this relationship, consider the following sentence:  1) John put the socks in the drawer.  In example (1), there are three phrases associated with the verb "put": "John" (who is doing the putting), "the socks" (which are being put), and "in the drawer" (which is the destination for the socks). If any of these phrases is removed, the sentence becomes ungrammatical. For example, if "in the drawer" is removed, the sentence becomes:  1a) #John put the socks.  This indicates that "put" requires three phrases for it to be grammatical. In linguistic terminology, this means that there are three parts to its argument structure: something doing the putting, something being put, and a destination for the putting. More generally, the argument structure for a word is a specification of what types of phrases are required for it to be used grammatically.    Not all phrases are part of the argument structure of a word. Consider the following example:  2) John put the socks in the drawer before dinner.  In this sentence, the phrase "before dinner" is grammatically optional; the sentence is still grammatical if it is removed. This phrase is therefore not part of the argument structure of the sentence, but rather is called a modifier of the sentence.     Fluent speakers of English know the argument structure for "put", and for the other words in the language. For the different phrases in a sentence, they can determine whether that phrase is a mandatory argument, or whether it is an optional modifier. In our research, we wanted to address two questions. First, how might people learn the argument structures in their language? Second, if we wanted to design a computer system which could speak English or another language, how could that system learn the language's argument structures?  In order to address these questions, we developed a computational model of this argument-structure learning task. This system combines grammatical representations developed in linguistics with Bayesian machine learning techniques. It classifies a phrase as an argument structure when this leads to greater compression of the sentences that have been observed. We found that this computational model learns to substantially improve its classification of argument structure.   In a second part of the project, we investigated how people and computers can learn to understand questions. Consider the following sequence of questions:  3a) Who did the man see?  3b) Who did the man who plays the trumpet see?  3c) Who did the man who plays the trumpet which was stolen see?  In each of these questions, the speaker is asking about who was seen. Intuitively, the wh-term "Who" is linked to the verb "see" at the end of the sentence, and serves as the argument of this verb. As the sequence (3a-c) illustrates, the wh-term in a question and the verb it is linked to can be separated by linguistic material of arbitrarily large complexity. For this reason, the dependency between the wh-term and the verb is known as a long-distance dependency.   There is a challenge in understanding these sentences: how does one determine that the wh-term "who" is associated with the verb "see," rather than the other verbs in the sentence? More generally, how do people learn to resolve these long-distance dependencies, and how can we build computer systems which learn to resolve them?    In order to try to address these questions, we developed a computational model for learning the structure of questions, and long-distance dependencies more generally. This model uses a grammatical formalism for representing long-distance dependencies known as minimalist grammars, and combines this with a Bayesian learning framework similar to that used above.    By providing precise computational accounts of how different aspects of grammar may be learned, this work additionally aims to improve the lin...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
