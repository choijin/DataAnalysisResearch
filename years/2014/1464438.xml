<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CSR: Scaling Geo-Replicated Storage In and Across Many Datacenters</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2015</AwardEffectiveDate>
<AwardExpirationDate>02/28/2017</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project focuses on improving the state of the art in storage for large-scale websites like Amazon, Facebook, and Google that are an integral part of our modern lives.  The research is important because it will make the websites more reliable, less expensive to build, and faster for everyone to access.  The efforts of this project will guide and focus both industrial efforts and future researchers.  This project has the following unique features:  optimal speed for reading data from storage and scalability inside and across data centers when compared to the leading research system and the leading industrial system.  A working prototype of new storage designs will be produced and released during the course of the work. &lt;br/&gt; &lt;br/&gt;To accomplish the goals, the project addresses two major deficiencies in the state of the art.  First, current systems are likely non-optimal for reading data from storage as they all use multiple rounds of communication.  This problem will be addressed by designing, implementing, and evaluating algorithms for reading data that use optimally few rounds of communication.  Second, current systems do not scale across data centers, so that building more data centers does not increase the amount of storage or its throughput.  This project will use partial replication to store only a subset of the data in each data center so that adding new data centers will result in proportional increases in storage capacity and throughput.  An important component is the innovation educational experience in the design phase of the project and systems building experience in the implementation phase for graduate students.&lt;br/&gt;&lt;br/&gt;Large-scale websites are an important and enriching part of modern life.  The algorithms and prototypes developed by this research will make the storage that powers them bigger and faster.  These improvements to storage will in turn result in improvements to the websites themselves and perhaps even enable new types of services that were not previously possible.</AbstractNarration>
<MinAmdLetterDate>03/04/2015</MinAmdLetterDate>
<MaxAmdLetterDate>03/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1464438</AwardID>
<Investigator>
<FirstName>Wyatt</FirstName>
<LastName>Lloyd</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wyatt Lloyd</PI_FULL_NAME>
<EmailAddress>wlloyd@princeton.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000679081</NSF_ID>
<StartDate>03/04/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[3720 S. Flower St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The outcomes of this project include advancements to the state of the art in the design of large-scale distributed storage systems that are the foundational layer on which large web services like Amazon, Facebook, and Google are built.&nbsp; One major advancement is described in a published research paper on the SNOW Theorem.&nbsp; Another major advancement is described in a research paper that is currently under submission on the K2 distributed storage system. Each of these is described in more detail below as are the broader impacts of this project.</p> <p>&nbsp;</p> <p>Large web services interact with massive amounts of data that is far too large to fit on a single machine and so is instead distributed across many machines in a distributed storage system.&nbsp; The previous state of the art for distributed storage systems included some systems that were very fast but hard to program correctly as well as other systems that were slower but easier to program correctly because they provided a mechanism called read-only transactions.&nbsp; The SNOW Theorem is an impossibility result that proves this tradeoff is fundamental: a distributed storage system can either have the best form of read-only transactions or it can be as fast as possible, but it cannot be both.&nbsp; In addition to figuring out what systems can never be built, we also built new systems with read-only transactions that are faster than the previous state of the art.&nbsp; The new systems touch the edge of our impossibility result, meaning we know it is impossible to build systems that are faster than them.</p> <p>&nbsp;</p> <p>Large web services are built on a moderate number of geographically far apart datacenters that each hold many machines.&nbsp; Causal consistency is an attractive contract for such geo-distributed systems because it is relatively easy to program and allows the underlying storage system to be as fast as a storage system that was not geo-distributed.&nbsp; The previous state of the art in causally-consistent storage could either grow the number of datacenters or it could grow the number of machines being used in each datacenter.&nbsp; The K2 distributed storage system shows it is possible to build causally-consistent storage that scales in both dimensions.</p> <p>&nbsp;</p> <p>Large web services are an important and enriching part of modern life. The algorithms and prototypes developed by this research will lead the way in making the storage that powers them bigger and faster. These improvements to storage will in turn result in improvements to the websites themselves and perhaps even enable new types of services that were not previously possible.&nbsp; In addition, the research in this project has trained many students in how to reason about, design, build, and evaluate large-scale distributed systems, which is a skill that is increasingly in demand as more and more web services are created and need to scale.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/30/2017<br>      Modified by: Wyatt&nbsp;Lloyd</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The outcomes of this project include advancements to the state of the art in the design of large-scale distributed storage systems that are the foundational layer on which large web services like Amazon, Facebook, and Google are built.  One major advancement is described in a published research paper on the SNOW Theorem.  Another major advancement is described in a research paper that is currently under submission on the K2 distributed storage system. Each of these is described in more detail below as are the broader impacts of this project.     Large web services interact with massive amounts of data that is far too large to fit on a single machine and so is instead distributed across many machines in a distributed storage system.  The previous state of the art for distributed storage systems included some systems that were very fast but hard to program correctly as well as other systems that were slower but easier to program correctly because they provided a mechanism called read-only transactions.  The SNOW Theorem is an impossibility result that proves this tradeoff is fundamental: a distributed storage system can either have the best form of read-only transactions or it can be as fast as possible, but it cannot be both.  In addition to figuring out what systems can never be built, we also built new systems with read-only transactions that are faster than the previous state of the art.  The new systems touch the edge of our impossibility result, meaning we know it is impossible to build systems that are faster than them.     Large web services are built on a moderate number of geographically far apart datacenters that each hold many machines.  Causal consistency is an attractive contract for such geo-distributed systems because it is relatively easy to program and allows the underlying storage system to be as fast as a storage system that was not geo-distributed.  The previous state of the art in causally-consistent storage could either grow the number of datacenters or it could grow the number of machines being used in each datacenter.  The K2 distributed storage system shows it is possible to build causally-consistent storage that scales in both dimensions.     Large web services are an important and enriching part of modern life. The algorithms and prototypes developed by this research will lead the way in making the storage that powers them bigger and faster. These improvements to storage will in turn result in improvements to the websites themselves and perhaps even enable new types of services that were not previously possible.  In addition, the research in this project has trained many students in how to reason about, design, build, and evaluate large-scale distributed systems, which is a skill that is increasingly in demand as more and more web services are created and need to scale.             Last Modified: 05/30/2017       Submitted by: Wyatt Lloyd]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
