<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Medium: Collaborative Research: Improved Performance Testing and Debugging</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>316000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Software performance is critical for how end users perceive the quality of the deployed software.  Performance problems, also called "performance bugs", typically correspond to software faults that create significant performance degradation.  Much evidence shows that seemingly harmless performance problems can lead to severe scalability reductions and financial losses.  This project develops a set of new techniques and tools that can significantly improve performance testing and debugging.&lt;br/&gt;&lt;br/&gt;Specifically, the project focuses on three key challenges.  First, what are common patterns of performance bugs and how can these patterns be detected during testing, before they manifest in production runs?  Second, how to ensure that code changes for adding new features, fixing bugs, or even improving performance do not have unintended consequence of decreasing performance?  Third, how to find causes of performance bugs from testing runs, regression checks, and in-field execution traces?  The broader impacts of the project are that improved performance testing and debugging can substantially increase the quality of the deployed software and thus the quality of life in a modern society that heavily depends on software.</AbstractNarration>
<MinAmdLetterDate>06/06/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/18/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1409829</AwardID>
<Investigator>
<FirstName>Harry</FirstName>
<LastName>Xu</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Harry G Xu</PI_FULL_NAME>
<EmailAddress>harryxu@cs.ucla.edu</EmailAddress>
<PI_PHON>3107947145</PI_PHON>
<NSF_ID>000599637</NSF_ID>
<StartDate>06/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926973425</ZipCode>
<StreetAddress><![CDATA[3212 Bren Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~300000</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Large-scale software commonly suffers from systemic performance problems, due to inefficiencies inherent in an object-oriented language as well as commonly-adopted design and implementation principles. These problems are becoming increasingly critical as object-oriented languages are used in systems that typically have small memory space and computation power, such as mobile devices. In such systems, memory inefficiencies inherent in an object-oriented language can lead to severe performance degradation and reduced scalability.&nbsp; The major goals of the project are to develop performance testing and debugging techniques that can be used to find inefficiencies and performance bugs in a variety of object-oriented software applications.</p> <p>&nbsp;</p> <p>Under the support of this grant, we have developed a series of performance improvement techniques for various kinds of software systems such as regular Java applications running single machine as well as large-scale distributed data-intensive systems. These techniques have led to 14 publications in top programming languages and systems conferences as well as the release of 6 open-source systems. Here we briefly summarize our research outcome in these two application domains.&nbsp;</p> <p>1. Performance debugging and testing for regular Java applications</p> <p>Despite the employment of faster CPUs and larger memory systems, the levels of inefficiencies in real-world programs grow surprisingly fast and there is an ever-increasing demand for performance optimization in modern software. We have developed several techniques that can help developers find serious performance problems in Java applications.</p> <p>An example piece of work is a technique called PerfBlower [ECOOP'15] that can &ldquo;magnify&rdquo; the effect of small performance problems before so that developers can find and fix them during in-house testing before these problems escape to production runs. &nbsp;PerfBlower was designed based on an important insight that most existing works focuse on how to debug a user-provided test execution in which performance problems already manifest and it remains largely unknown how to effectively find performance bugs before software release. PerfBlower is a general performance testing framework that provides (1) a novel specification language ISL to describe a general class of performance problems that have observable symptoms; (2) an automated test oracle via virtual amplification; and (3) precise reference-path-based diagnostic information via object mirroring. Using this framework, we have amplified three different types of problems. Our experimental results demonstrate that (1) ISL is expressive enough to describe various memory-related performance problems; (2) PerfBlower successfully distinguishes executions with and without problems; 8 unknown problems are quickly discovered under small workloads; and (3) PerfBlower outperforms existing detectors and does not miss any bugs studied before in the literature.</p> <p>&nbsp;</p> <p>2. Tackling inefficiencies in data-intensive systems</p> <p>Modern computing has entered the era of Big Data. Developing systems that can scale to massive amounts of data without significantly increasing resource amounts is a key challenge faced by both researchers and practitioners. Supported by this grant, our work has focused on leveraging compiler and runtime system techniques to scale many different aspects of Big Data applications. Our research efforts have spanned a variety of computation models including dataflow [SOSP&rsquo;15] and graph models [USENIX ATC&rsquo;15, ATC&rsquo;16, ASPLOS&rsquo;17-b], a variety of execution environments including single-machine disk-based [USENIX ATC&rsquo;15, ATC&rsquo;16, TACO&rsquo;16] and distributed cluster-based environments [SOSP&rsquo;15, ASPLOS&rsquo;17-b, OSDI&rsquo;16, as well as applications written in a variety of languages including managed object-oriented languages<strong>&nbsp;</strong>[ISMM&rsquo;13, SOSP&rsquo;15, USENIX ATC&rsquo;15, ASPLOS&rsquo;15, OSDI&rsquo;16] and unmanaged languages [USENIX ATC&rsquo;16, ASPLOS&rsquo;17-a, ASPLOS&rsquo;17-b].</p> <p>Popular data processing frameworks such as Hadoop, Spark, Naiad, or Hyracks are all developed in managed languages, such as Java, C#, or Scala, primarily due to (1) the fast development cycles enabled by these languages, and (2) their abundance of library suites and community support. However, managed languages come at a cost: memory management in Big Data systems is often prohibitively expensive.&nbsp; For example, our study [ISMM&rsquo;13] shows that garbage collection (GC) accounts for close to 50% of the execution time of these systems, severely damaging system performance. The problem becomes increasingly painful in latency-sensitive distributed cloud applications where long GC pause times on one node can make many/all other nodes wait, potentially delaying the processing of user requests for unacceptably long time. &nbsp;Two primary techniques we developed to tackle the problem are Facade [ASPLOS'15] -- a compiler and runtime system that can (almost) statically bound the number of data objects, and Yak [OSDI'16] -- a JVM-based runtime system that splits a Java heap into a control and a data space and use different memory management techniques to manage their memory.&nbsp;</p> <p>To summarize, at the end of the project, we found that (1) performance problems widely exist in various application domains; and (2) through a set of techniques we developed under the support of this grant, we could significantly improve the scalability and performance for a variety of important applications and systems, enabling them to process data more efficiently and scale to larger datasets.</p><br> <p>            Last Modified: 11/02/2017<br>      Modified by: Guoqing&nbsp;Xu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Large-scale software commonly suffers from systemic performance problems, due to inefficiencies inherent in an object-oriented language as well as commonly-adopted design and implementation principles. These problems are becoming increasingly critical as object-oriented languages are used in systems that typically have small memory space and computation power, such as mobile devices. In such systems, memory inefficiencies inherent in an object-oriented language can lead to severe performance degradation and reduced scalability.  The major goals of the project are to develop performance testing and debugging techniques that can be used to find inefficiencies and performance bugs in a variety of object-oriented software applications.     Under the support of this grant, we have developed a series of performance improvement techniques for various kinds of software systems such as regular Java applications running single machine as well as large-scale distributed data-intensive systems. These techniques have led to 14 publications in top programming languages and systems conferences as well as the release of 6 open-source systems. Here we briefly summarize our research outcome in these two application domains.   1. Performance debugging and testing for regular Java applications  Despite the employment of faster CPUs and larger memory systems, the levels of inefficiencies in real-world programs grow surprisingly fast and there is an ever-increasing demand for performance optimization in modern software. We have developed several techniques that can help developers find serious performance problems in Java applications.  An example piece of work is a technique called PerfBlower [ECOOP'15] that can "magnify" the effect of small performance problems before so that developers can find and fix them during in-house testing before these problems escape to production runs.  PerfBlower was designed based on an important insight that most existing works focuse on how to debug a user-provided test execution in which performance problems already manifest and it remains largely unknown how to effectively find performance bugs before software release. PerfBlower is a general performance testing framework that provides (1) a novel specification language ISL to describe a general class of performance problems that have observable symptoms; (2) an automated test oracle via virtual amplification; and (3) precise reference-path-based diagnostic information via object mirroring. Using this framework, we have amplified three different types of problems. Our experimental results demonstrate that (1) ISL is expressive enough to describe various memory-related performance problems; (2) PerfBlower successfully distinguishes executions with and without problems; 8 unknown problems are quickly discovered under small workloads; and (3) PerfBlower outperforms existing detectors and does not miss any bugs studied before in the literature.     2. Tackling inefficiencies in data-intensive systems  Modern computing has entered the era of Big Data. Developing systems that can scale to massive amounts of data without significantly increasing resource amounts is a key challenge faced by both researchers and practitioners. Supported by this grant, our work has focused on leveraging compiler and runtime system techniques to scale many different aspects of Big Data applications. Our research efforts have spanned a variety of computation models including dataflow [SOSP?15] and graph models [USENIX ATC?15, ATC?16, ASPLOS?17-b], a variety of execution environments including single-machine disk-based [USENIX ATC?15, ATC?16, TACO?16] and distributed cluster-based environments [SOSP?15, ASPLOS?17-b, OSDI?16, as well as applications written in a variety of languages including managed object-oriented languages [ISMM?13, SOSP?15, USENIX ATC?15, ASPLOS?15, OSDI?16] and unmanaged languages [USENIX ATC?16, ASPLOS?17-a, ASPLOS?17-b].  Popular data processing frameworks such as Hadoop, Spark, Naiad, or Hyracks are all developed in managed languages, such as Java, C#, or Scala, primarily due to (1) the fast development cycles enabled by these languages, and (2) their abundance of library suites and community support. However, managed languages come at a cost: memory management in Big Data systems is often prohibitively expensive.  For example, our study [ISMM?13] shows that garbage collection (GC) accounts for close to 50% of the execution time of these systems, severely damaging system performance. The problem becomes increasingly painful in latency-sensitive distributed cloud applications where long GC pause times on one node can make many/all other nodes wait, potentially delaying the processing of user requests for unacceptably long time.  Two primary techniques we developed to tackle the problem are Facade [ASPLOS'15] -- a compiler and runtime system that can (almost) statically bound the number of data objects, and Yak [OSDI'16] -- a JVM-based runtime system that splits a Java heap into a control and a data space and use different memory management techniques to manage their memory.   To summarize, at the end of the project, we found that (1) performance problems widely exist in various application domains; and (2) through a set of techniques we developed under the support of this grant, we could significantly improve the scalability and performance for a variety of important applications and systems, enabling them to process data more efficiently and scale to larger datasets.       Last Modified: 11/02/2017       Submitted by: Guoqing Xu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
