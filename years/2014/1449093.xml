<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Preliminary Explorations for the Development of Responsive Prosodic Behaviors for Interactive Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>174000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This EArly Grant for Exploratory Research is motivated by the attractiveness of spoken interaction as a way for people to interact with autonomous intelligent systems.  When systems talk to people, speech can convey not only lexical information but also meta information, such as whether the information requires immediate attention or is just background, how well the system understands the user's goals and situation, and whether the system needs to continue providing information or is done for the moment. In human-human dialog such meta-information is mostly conveyed by prosody (intonation): subtle variations in the pitch, energy, rate and timing within utterances.  Unfortunately this sort of expressiveness for agents is not today available without using pre-recorded prompts or hand-crafted synthesized utterances, neither of which is flexible enough for systems operating in contexts where the possible configurations of communication needs are not fully predictable ahead of time. Thus there is a need to make spoken language technology better support interaction for robots and other systems.  Advances in prosody modeling can also inform language teachers and communication teachers, and health practitioners who deal with communications disorders.&lt;br/&gt;&lt;br/&gt;This project focuses on two preliminary steps to the goal of using prosody in human-computer dialog: first experimentation with different models of speech synthesis to determine which can best support the realization of prosodically well-adapted utterances, and to determine where improvements need to be made, whether to the engine itself or to the training-data collection procedures; and second a pilot data collection and analysis to determine which aspects of context need to be measured and represented to support appropriate prosodic choices.</AbstractNarration>
<MinAmdLetterDate>08/21/2014</MinAmdLetterDate>
<MaxAmdLetterDate>02/22/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1449093</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Novick</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David G Novick</PI_FULL_NAME>
<EmailAddress>novick@utep.edu</EmailAddress>
<PI_PHON>9157476031</PI_PHON>
<NSF_ID>000434916</NSF_ID>
<StartDate>08/21/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nigel</FirstName>
<LastName>Ward</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nigel G Ward</PI_FULL_NAME>
<EmailAddress>nigel@utep.edu</EmailAddress>
<PI_PHON>9157476827</PI_PHON>
<NSF_ID>000327901</NSF_ID>
<StartDate>08/21/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Olac</FirstName>
<LastName>Fuentes</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Olac Fuentes</PI_FULL_NAME>
<EmailAddress>ofuentes@utep.edu</EmailAddress>
<PI_PHON>9157476956</PI_PHON>
<NSF_ID>000492181</NSF_ID>
<StartDate>08/21/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at El Paso</Name>
<CityName>El Paso</CityName>
<ZipCode>799680001</ZipCode>
<PhoneNumber>9157475680</PhoneNumber>
<StreetAddress>ADMIN BLDG RM 209</StreetAddress>
<StreetAddress2><![CDATA[500 West University]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX16</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>132051285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT EL PASO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at El Paso]]></Name>
<CityName>El Paso</CityName>
<StateCode>TX</StateCode>
<ZipCode>799680001</ZipCode>
<StreetAddress><![CDATA[500 W. University Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~150000</FUND_OBLG>
<FUND_OBLG>2015~16000</FUND_OBLG>
<FUND_OBLG>2016~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Spoken interaction is an attractive way for people to interact with autonomous intelligent systems. &nbsp;When systems talk to people, speech can convey not only lexical information but also meta information, such as whether the information requires immediate attention or is just background, how well the system understands the user's goals and situation, and whether the system needs to continue providing information or is done for the moment.<br />In human-human dialog such meta-information is mostly conveyed by prosody: subtle variations in the pitch, energy, rate and timing within utterances. &nbsp;Unfortunately this sort of expressiveness for agents is not today available without using pre-recorded prompts or hand-crafted synthesized utterances, neither of which is flexible enough for systems operating in contexts where the possible configurations of communication needs are not fully predictable ahead of time.<br />In this project our aim was to explore what would be needed to build models to support the synthesis of prosodically appropriate utterances in dynamic domains.<br />To this end we collected a corpus and did preliminary studies of the dimensions of prosodic variation and how these relate to context. &nbsp;In addition, thanks to REU supplements, we developed infrastructure and explored how prosody relates to gaze in dialog, how prosody expresses stance in news broadcasts, and how learners' prosody differs from that of natives.<br />Further, this project enabled us to reimplement and release our mid-level prosodic features toolkit, which has found use not only at UTEP but also at Kyoto University, Columbia, and Paris Tech. This is designed to support research on turn-taking, various dimensions of responsiveness, adaptation, speaker identification, and other problems where a simple-to-use set of prosodic features can have value.</p><br> <p>            Last Modified: 01/11/2017<br>      Modified by: Nigel&nbsp;G&nbsp;Ward</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Spoken interaction is an attractive way for people to interact with autonomous intelligent systems.  When systems talk to people, speech can convey not only lexical information but also meta information, such as whether the information requires immediate attention or is just background, how well the system understands the user's goals and situation, and whether the system needs to continue providing information or is done for the moment. In human-human dialog such meta-information is mostly conveyed by prosody: subtle variations in the pitch, energy, rate and timing within utterances.  Unfortunately this sort of expressiveness for agents is not today available without using pre-recorded prompts or hand-crafted synthesized utterances, neither of which is flexible enough for systems operating in contexts where the possible configurations of communication needs are not fully predictable ahead of time. In this project our aim was to explore what would be needed to build models to support the synthesis of prosodically appropriate utterances in dynamic domains. To this end we collected a corpus and did preliminary studies of the dimensions of prosodic variation and how these relate to context.  In addition, thanks to REU supplements, we developed infrastructure and explored how prosody relates to gaze in dialog, how prosody expresses stance in news broadcasts, and how learners' prosody differs from that of natives. Further, this project enabled us to reimplement and release our mid-level prosodic features toolkit, which has found use not only at UTEP but also at Kyoto University, Columbia, and Paris Tech. This is designed to support research on turn-taking, various dimensions of responsiveness, adaptation, speaker identification, and other problems where a simple-to-use set of prosodic features can have value.       Last Modified: 01/11/2017       Submitted by: Nigel G Ward]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
