<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC*IIE Networking Infrastructure: A Performant and Reliable Science DMZ for National and International Collaboration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The University of Chicago (UChicago) has built a high-speed Science DMZ network that includes a 100Gbps uplink to Research and Education networks. This network provides researchers with the bandwidth required to collaborate both nationally and internationally at extremely high speeds. The initial experimental build was completed in a non-redundant fashion, which means that a single failure or disruption has the potential to impact every project dependent on the Science DMZ. Further, multiple projects have demonstrated high-bandwidth needs that will quickly congest the singular 100Gbps uplink available.&lt;br/&gt;&lt;br/&gt;To address these risks and concerns, UChicago is building a redundant Dense Wave Division Multiplexing (DWDM) System between its main Hyde Park campus and a location in downtown Chicago. This system includes a second 100Gbps capable router that provides UChicago?s Science DMZ network with additional capacity and resiliency. Upon completion, the system will provide researchers with a highly available, extremely high-capacity network foundation that enables them to maximize their scientific productivity. &lt;br/&gt;&lt;br/&gt;Amongst science projects that will benefit are: &lt;br/&gt;- ATLAS collaboration at the CERN Large Hadron Collider (LHC)&lt;br/&gt;- Bionimbus - Open Science Data Cloud Project designed for managing and analyzing genomic data&lt;br/&gt;- The Kavli Institute for Cosmological Physics (KICP) advancing research on the nature of dark matter and dark energy&lt;br/&gt;- The South Pole Telescope (SPT) collaboration ? a survey of cosmic microwave background (CMB) radiation&lt;br/&gt;- The Urban Center for Computation and Data (UrbanCCD) partnering with the City of Chicago to design and implementation its next generation data analytics platform.</AbstractNarration>
<MinAmdLetterDate>09/08/2014</MinAmdLetterDate>
<MaxAmdLetterDate>11/25/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1440785</AwardID>
<Investigator>
<FirstName>Ian</FirstName>
<LastName>Foster</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ian Foster</PI_FULL_NAME>
<EmailAddress>foster@uchicago.edu</EmailAddress>
<PI_PHON>6302524619</PI_PHON>
<NSF_ID>000234022</NSF_ID>
<StartDate>09/08/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Gardner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>Robert Gardner</PI_FULL_NAME>
<EmailAddress>rwg@hep.uchicago.edu</EmailAddress>
<PI_PHON>7738349885</PI_PHON>
<NSF_ID>000458001</NSF_ID>
<StartDate>11/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Gardner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>Robert Gardner</PI_FULL_NAME>
<EmailAddress>rwg@hep.uchicago.edu</EmailAddress>
<PI_PHON>7738349885</PI_PHON>
<NSF_ID>000458001</NSF_ID>
<StartDate>09/08/2014</StartDate>
<EndDate>11/25/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Grossman</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert L Grossman</PI_FULL_NAME>
<EmailAddress>robert.grossman@uchicago.edu</EmailAddress>
<PI_PHON>7738344669</PI_PHON>
<NSF_ID>000278293</NSF_ID>
<StartDate>09/08/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Klara</FirstName>
<LastName>Jelinkova</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Klara Jelinkova</PI_FULL_NAME>
<EmailAddress>klaraj@rice.edu</EmailAddress>
<PI_PHON>7133482211</PI_PHON>
<NSF_ID>000621175</NSF_ID>
<StartDate>09/08/2014</StartDate>
<EndDate>11/25/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606375418</ZipCode>
<StreetAddress><![CDATA[5801 South Ellis Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the project was to strengthen the science network at the University of Chicago, creating a leadership class campus cyberinfrastructure, a so-called "Science DMZ", where high volume scientific data is separated onto a dedicated network to avoid interference with normal internet traffic. The result is a network capable of enabling robust, production quality data intensive science at the highest scales possible. This was accomplished by securing a redundant high bandwidth capacity network connection between the University campus in the Hyde Park neighborhood on the south side of Chicago to a major hub located downtown. &nbsp;With this new link in place we doubled our traffic capacity while providing a backup network which is critical for running continuous operations that many scientific collaborations require. &nbsp;Instrument data from the South Pole Telescope, an underground astrophysics experiment in Italy, and from the Large Hadron Collider project in Geneva, Switzerland, continuously stream into storage servers located at the University of Chicago. These data are processed and analyzed by distributed teams of researchers from hundreds of insitutions worldwide. Having a secure, reliable, and highly performant network is critical to achieving this kind of computational environment. &nbsp;</p> <p>In addition, our Research Computing Center is working with the Department of Organismal Biology and Anatomy to streamline the collection and publication of XROMM (X-ray Reconstruction of Moving Morphology) datasets. &nbsp;This project sends trial data from the XROMM core facility machines on campus to our large computational cluster Midway using the Science DMZ.</p> <p>Another research program that requires the Science DMZ is the National Cancer Institute's Genomic Data Commons (GDC) which provides the cancer research community with a unified data repository that enables data sharing across cancer genomic studies in support of precision medicine. &nbsp;The GDC at the University of Chicago contains over 5 thousand terabytes of data and serves over 1,000 users each day. &nbsp;This is only possible because of the very high capacity network links between the GDC, the campus Science DMZ, and the major research network hubs in Chicago. For 2016 year to date, the GDC, via the Science DMZ, has served over 8000 terabytes of cancer genomic data to the worldwide research community.</p> <p>Having this kind of cyberinfrastructure makes possible research into new forms of computation, in particular evaluating methods for scheduling data-intensive science workflows in distributed environments connected by high-speed networks. One of our computer scientists developed a multi-site workflow scheduling technique that uses performance models to predict the execution time on resources and dynamic probes to identify the achievable network throughput between sites. He evaluated his approach using real world applications and a distributed execution framework. He used two distinct computational environments&ndash;geographically distributed multiple clusters and multiple clouds, and showed that his approach improved the resource utilization and reduced execution time when compared to the default schedule. &nbsp;This could save research dollars while reducing the time for publication of new knowlege and information.</p> <p>The Science DMZ infrastructure and the associated network upgrade helped us improve the performance of large-scale data transfers. The performance of transport protocols largely depend on their control parameter settings, but it is prohibitively time consuming to conduct an exhaustive search in a large parameter space to find the best set of parameter values. We developed a stochastic approximation-based transport profiler, to quickly determine the optimal operational zone of a given data transfer protocol/method over dedicated channels. &nbsp;We focused on selecting between different transport protocols and the appropriate parameters such as buffer size and number of parallel streams for each protocol. &nbsp;We also tested using real-life performance measurements and experiments over physical connections with short (2 millisecond) and long (380 millisecond) delays. &nbsp;We found our tool was capable of reducing the profiling overhead while achieving a comparable level of end-to-end throughput performance with conventional methods. &nbsp;Again, these advances accelerate scientific discoveries.</p> <p>Finally, the Science DMZ enabled our computing faciliies to be efficiently used by hundreds of researchers using the Open Science Grid (OSG). &nbsp;The OSG is a consortium of universities and national labs (of which the University of Chicago is a founding member) that provides a national scale cyberinfrastructure for sharing computational resources, enabling collaboration between researchers at different institutions and accelerating science. One example of this was the processing of data from the NSF-funded Laser Interferometer Gravitational-Wave Observatory (LIGO), which announced discovery of gravitational waves in Feburary 2016, confirming Einstein's theory of general relativity. OSG provided access to numerous high performance computing systems for LIGO, seamlessly integrating supercomputers and university high performance computing centers. Our Science DMZ at the University of Chicago helped in this massive processing effort to confirm an unambiguous detection of gravity waves. The OSG was recipient of two &lsquo;Top Supercomputing Achievement&rsquo; awards for 2016, recognizing the use of computing to verify Einstein&rsquo;s theory of gravitational waves.</p><br> <p>            Last Modified: 12/30/2016<br>      Modified by: Robert&nbsp;W&nbsp;Gardner</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1440785/1440785_10341223_1483085293568_screenshot_2860--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1440785/1440785_10341223_1483085293568_screenshot_2860--rgov-800width.jpg" title="ATLAS experimental data 2016"><img src="/por/images/Reports/POR/2016/1440785/1440785_10341223_1483085293568_screenshot_2860--rgov-66x44.jpg" alt="ATLAS experimental data 2016"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Protons colliding in the ATLAS detector at CERN on June 4, 2016.</div> <div class="imageCredit">CERN</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robert&nbsp;W&nbsp;Gardner</div> <div class="imageTitle">ATLAS experimental data 2016</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1440785/1440785_10341223_1483085812917_screenshot_2801--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1440785/1440785_10341223_1483085812917_screenshot_2801--rgov-800width.jpg" title="Particle physics data flows"><img src="/por/images/Reports/POR/2016/1440785/1440785_10341223_1483085812917_screenshot_2801--rgov-66x44.jpg" alt="Particle physics data flows"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Traffic into (green) and out from (yellow) the ATLAS Tier2 Center and the University of Chicago Science DMZ showing peaks of 70 gigabits per second.  Data at the center was analyzed by more than 1,700 physicists in the worldwide collaboration in 2016.</div> <div class="imageCredit">University of Chicago</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Robert&nbsp;W&nbsp;Gardner</div> <div class="imageTitle">Particle physics data flows</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the project was to strengthen the science network at the University of Chicago, creating a leadership class campus cyberinfrastructure, a so-called "Science DMZ", where high volume scientific data is separated onto a dedicated network to avoid interference with normal internet traffic. The result is a network capable of enabling robust, production quality data intensive science at the highest scales possible. This was accomplished by securing a redundant high bandwidth capacity network connection between the University campus in the Hyde Park neighborhood on the south side of Chicago to a major hub located downtown.  With this new link in place we doubled our traffic capacity while providing a backup network which is critical for running continuous operations that many scientific collaborations require.  Instrument data from the South Pole Telescope, an underground astrophysics experiment in Italy, and from the Large Hadron Collider project in Geneva, Switzerland, continuously stream into storage servers located at the University of Chicago. These data are processed and analyzed by distributed teams of researchers from hundreds of insitutions worldwide. Having a secure, reliable, and highly performant network is critical to achieving this kind of computational environment.    In addition, our Research Computing Center is working with the Department of Organismal Biology and Anatomy to streamline the collection and publication of XROMM (X-ray Reconstruction of Moving Morphology) datasets.  This project sends trial data from the XROMM core facility machines on campus to our large computational cluster Midway using the Science DMZ.  Another research program that requires the Science DMZ is the National Cancer Institute's Genomic Data Commons (GDC) which provides the cancer research community with a unified data repository that enables data sharing across cancer genomic studies in support of precision medicine.  The GDC at the University of Chicago contains over 5 thousand terabytes of data and serves over 1,000 users each day.  This is only possible because of the very high capacity network links between the GDC, the campus Science DMZ, and the major research network hubs in Chicago. For 2016 year to date, the GDC, via the Science DMZ, has served over 8000 terabytes of cancer genomic data to the worldwide research community.  Having this kind of cyberinfrastructure makes possible research into new forms of computation, in particular evaluating methods for scheduling data-intensive science workflows in distributed environments connected by high-speed networks. One of our computer scientists developed a multi-site workflow scheduling technique that uses performance models to predict the execution time on resources and dynamic probes to identify the achievable network throughput between sites. He evaluated his approach using real world applications and a distributed execution framework. He used two distinct computational environments&ndash;geographically distributed multiple clusters and multiple clouds, and showed that his approach improved the resource utilization and reduced execution time when compared to the default schedule.  This could save research dollars while reducing the time for publication of new knowlege and information.  The Science DMZ infrastructure and the associated network upgrade helped us improve the performance of large-scale data transfers. The performance of transport protocols largely depend on their control parameter settings, but it is prohibitively time consuming to conduct an exhaustive search in a large parameter space to find the best set of parameter values. We developed a stochastic approximation-based transport profiler, to quickly determine the optimal operational zone of a given data transfer protocol/method over dedicated channels.  We focused on selecting between different transport protocols and the appropriate parameters such as buffer size and number of parallel streams for each protocol.  We also tested using real-life performance measurements and experiments over physical connections with short (2 millisecond) and long (380 millisecond) delays.  We found our tool was capable of reducing the profiling overhead while achieving a comparable level of end-to-end throughput performance with conventional methods.  Again, these advances accelerate scientific discoveries.  Finally, the Science DMZ enabled our computing faciliies to be efficiently used by hundreds of researchers using the Open Science Grid (OSG).  The OSG is a consortium of universities and national labs (of which the University of Chicago is a founding member) that provides a national scale cyberinfrastructure for sharing computational resources, enabling collaboration between researchers at different institutions and accelerating science. One example of this was the processing of data from the NSF-funded Laser Interferometer Gravitational-Wave Observatory (LIGO), which announced discovery of gravitational waves in Feburary 2016, confirming Einstein's theory of general relativity. OSG provided access to numerous high performance computing systems for LIGO, seamlessly integrating supercomputers and university high performance computing centers. Our Science DMZ at the University of Chicago helped in this massive processing effort to confirm an unambiguous detection of gravity waves. The OSG was recipient of two ?Top Supercomputing Achievement? awards for 2016, recognizing the use of computing to verify Einstein?s theory of gravitational waves.       Last Modified: 12/30/2016       Submitted by: Robert W Gardner]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
