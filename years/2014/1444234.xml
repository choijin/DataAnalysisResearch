<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Toward a General Framework for Words and Pictures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/28/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>269926.00</AwardTotalIntnAmount>
<AwardAmount>269926</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Pictures convey a visual description of the world directly to their viewers. Computer vision strives to design algorithms to extract the underlying world state captured in the camera's eye, with an overarching goal of general computational image understanding. To date much vision research has approached image understanding by focusing on object detection, only one perspective on the image understanding problem.  This project looks at an additional, complimentary way to collect information about the visual world -- by directly analyzing the enormous amount of visually descriptive text on the web to reveal what information is useful to attach to, and extract from pictures. This project presents a comprehensive research program geared toward modeling and exploiting the complimentary nature of words and pictures. One main goal is studying the connection between text and images to learn about depiction -- communication of meaning through pictures. This goal is addressed through 3 broad challenges: 1) Developing a richer vocabulary to describe the information provided by depiction. 2) Developing image representations that can visually capture this more nuanced vocabulary. 3) Constructing a comprehensive joint words and pictures framework. &lt;br/&gt;&lt;br/&gt;This project has direct significance to many concrete tasks that access images on the internet including: image search, browsing, and organization, as well as commercial applications such as product search, and societally important applications such as web assistance for the blind. Additionally, outputs of this project, including progress toward a natural vocabulary and structure for visual description, have great potential for cross-cutting impact in both the computer vision and natural language communities.</AbstractNarration>
<MinAmdLetterDate>05/02/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/29/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1444234</AwardID>
<Investigator>
<FirstName>Tamara</FirstName>
<LastName>Berg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tamara Berg</PI_FULL_NAME>
<EmailAddress>tlberg@cs.unc.edu</EmailAddress>
<PI_PHON>6465093361</PI_PHON>
<NSF_ID>000519059</NSF_ID>
<StartDate>05/02/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S Columbia St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~80564</FUND_OBLG>
<FUND_OBLG>2014~93680</FUND_OBLG>
<FUND_OBLG>2015~95682</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Much of everyday language and discourse concerns the visual world around us, making understanding the relationship between objects in the physical world and language describing those objects an important challenge problem for AI. From robotics, to image search, to situated language learning, and natural language grounding, there are a number of research areas and applications that would benefit from a better understanding of how people refer to physical entities in the world.</span></p> <p><span>This grant initiated a comprehensive research program by the PI to study and model the connection between images and the language people use to describe image content. This goal was addressed through three broad challenges: 1) Developing a richer vocabulary to describe the information provided by depiction, 2) Developing image representations that can visually capture this more nuanced vocabulary, 3) Constructing a comprehensive joint words and pictures framework.</span></p> <p><span>Specific projects include: models for generating natural language descriptions (captions) describing the overall content of images, analyses and methods for naming objects in images in a human-like manner, datasets and algorithms for enabling computers to effectively answer fill-in-the-blank natural language questions about images, methods for generating and comprehending natural language expressions referring to specific content in complex real world scenes, and models for generating natural language stories about image collections.</span></p> <p><span>Concrete products of this grant include collecting new web-scale datasets containing: images with associated natural captions (SBU Captioned Photo Dataset), natural language expressions referring to particular objects in complex scenes (ReferItGame Dataset), natural naming of objects in images (Entry-Level Category Dataset), fill-in-the-blank questions about image content (Visual Madlibs). Additionally, the grant produced publicly released code, and over 30 publications at top computer vision, natural language processing, and machine learning conferences and journals. Multiple graduate, undergraduate, and high school students were involved in research related to the grant, developing knowledge and implementation skills related to large data analysis, modeling, and implementation as well as changes to present their research publicly at conferences and outreach events. Topics were integrated into building new courses at Stony Brook University and the University of North Carolina Chapel Hill, including courses on visual recognition, AI, and language and vision. Demos and talks were given at outreach events for high school and middle school students, industry and academic colloquiums, and events geared toward women in technology.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 08/30/2017<br>      Modified by: Tamara&nbsp;Berg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Much of everyday language and discourse concerns the visual world around us, making understanding the relationship between objects in the physical world and language describing those objects an important challenge problem for AI. From robotics, to image search, to situated language learning, and natural language grounding, there are a number of research areas and applications that would benefit from a better understanding of how people refer to physical entities in the world.  This grant initiated a comprehensive research program by the PI to study and model the connection between images and the language people use to describe image content. This goal was addressed through three broad challenges: 1) Developing a richer vocabulary to describe the information provided by depiction, 2) Developing image representations that can visually capture this more nuanced vocabulary, 3) Constructing a comprehensive joint words and pictures framework.  Specific projects include: models for generating natural language descriptions (captions) describing the overall content of images, analyses and methods for naming objects in images in a human-like manner, datasets and algorithms for enabling computers to effectively answer fill-in-the-blank natural language questions about images, methods for generating and comprehending natural language expressions referring to specific content in complex real world scenes, and models for generating natural language stories about image collections.  Concrete products of this grant include collecting new web-scale datasets containing: images with associated natural captions (SBU Captioned Photo Dataset), natural language expressions referring to particular objects in complex scenes (ReferItGame Dataset), natural naming of objects in images (Entry-Level Category Dataset), fill-in-the-blank questions about image content (Visual Madlibs). Additionally, the grant produced publicly released code, and over 30 publications at top computer vision, natural language processing, and machine learning conferences and journals. Multiple graduate, undergraduate, and high school students were involved in research related to the grant, developing knowledge and implementation skills related to large data analysis, modeling, and implementation as well as changes to present their research publicly at conferences and outreach events. Topics were integrated into building new courses at Stony Brook University and the University of North Carolina Chapel Hill, including courses on visual recognition, AI, and language and vision. Demos and talks were given at outreach events for high school and middle school students, industry and academic colloquiums, and events geared toward women in technology.          Last Modified: 08/30/2017       Submitted by: Tamara Berg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
