<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Investigating How to Enhance Scientific Argumentation through Automated Feedback in the Context of Two High School Earth Science Curriculum Units</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>2495604.00</AwardTotalIntnAmount>
<AwardAmount>2495604</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michael Ford</SignBlockName>
<PO_EMAI>miford@nsf.gov</PO_EMAI>
<PO_PHON>7032925153</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With the current emphasis on learning science by actively engaging in the practices of science, and the call for integration of instruction and assessment; new resources, models, and technologies are being developed to improve K-12 science learning. Student assessment has become a nationwide educational priority due, in part, to the need for relevant and timely data that inform teachers, administrators, researchers, and the public about how all students perform and think while learning science. This project responds to the need for technology-enhanced assessments that promote the critical practice of scientific argumentation--making and explaining a claim from evidence about a scientific question and critically evaluating sources of uncertainty in the claim. It will investigate how to enhance this practice through automated scoring and immediate feedback in the context of two high school curriculum units--climate change and fresh-water availability--in schools with diverse student populations. The project will apply advanced automated scoring tools to students' written scientific arguments, provide individual students with customized feedback, and teachers with class-level information to assist them with improving scientific argumentation. The key outcome of this effort will be a technology-supported assessment model of how to advance the understanding of argumentation, and the use of multi-level feedback as a component of effective teaching and learning. The project will strengthen the program's current set of funded activities on assessment, focusing these efforts on students' argumentation as a complex science practice.&lt;br/&gt;&lt;br/&gt;This design and development research targets high school students (n=1,940) and teachers (n=22) in up to 10 states over four years. The research questions are: (1) To what extent can automated scoring tools, such as c-rater and c-rater-ML, diagnose students' explanations and uncertainty articulations as compared to human diagnosis?; (2) How should feedback be designed and delivered to help students improve scientific argumentation?; (3) How do teachers use and interact with class-level automated scores and feedback to support students' scientific argumentation with real-data and models?; and (4) How do students perceive their overall experience with the automated scores and immediate feedback when learning core ideas in climate change and fresh-water availability topics through scientific argumentation enhanced with modeling? In Years 1 and 2, plans are to conduct feasibility studies to build automated scoring models and design feedback for previously tested assessments for the two curriculum units. In Year 3, the project will implement design studies in order to identify effective feedback through random assignment. In Year 4, a pilot study will investigate if effective feedback should be offered with or without scores. The project will employ a mixed-methods approach. Data-gathering strategies will include classroom observations; screencast and log data of teachers' and students' interaction with automated feedback; teachers' and students' surveys with selected- and open-ended questions; and in-depth interviews with teachers and students. All constructed-response explanations and uncertainty items will be scored using automated scoring engines with fine-grained rubrics. Data analysis strategies will include multiple criteria to evaluate the quality of automated scores; descriptive statistical abalyses; analysis of variance to investigate differences in outcomes from the designed studies' pre/posttests and embedded assessments; analysis of covariance to investigate student learning trajectories; two-level hierarchical linear modeling to study the clustering of students within a class; and analysis of screencasts and log data.</AbstractNarration>
<MinAmdLetterDate>08/04/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1418019</AwardID>
<Investigator>
<FirstName>Amy</FirstName>
<LastName>Pallant</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amy Pallant</PI_FULL_NAME>
<EmailAddress>apallant@concord.org</EmailAddress>
<PI_PHON>9784053205</PI_PHON>
<NSF_ID>000082316</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ou</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ou L Liu</PI_FULL_NAME>
<EmailAddress>lliu@ets.org</EmailAddress>
<PI_PHON>6097341755</PI_PHON>
<NSF_ID>000503826</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hee-Sun</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hee-Sun Lee</PI_FULL_NAME>
<EmailAddress>hlee@concord.org</EmailAddress>
<PI_PHON>5106854014</PI_PHON>
<NSF_ID>000610944</NSF_ID>
<StartDate>08/04/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Educational Testing Service</Name>
<CityName>Princeton</CityName>
<ZipCode>085402218</ZipCode>
<PhoneNumber>6096832734</PhoneNumber>
<StreetAddress>Center for External Research</StreetAddress>
<StreetAddress2><![CDATA[Rosedale Rd., ms 12-T]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002508463</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EDUCATIONAL TESTING SERVICE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002508463</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Educational Testing Service]]></Name>
<CityName/>
<StateCode>NJ</StateCode>
<ZipCode>085402218</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7645</Code>
<Text>Discovery Research K-12</Text>
</ProgramElement>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0416</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0417</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~1309991</FUND_OBLG>
<FUND_OBLG>2016~642846</FUND_OBLG>
<FUND_OBLG>2017~542767</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Teaching students to practice scientific argumentation continues to be a challenge for science educators. In classrooms, students find it difficult to articulate and differentiate among claim, evidence, and reasoning as well as to evaluate the strengths and weaknesses of imperfect data when making interpretations. This happens mainly because students view science as already known facts while having little experience with the evaluative and uncertain nature of science. A pedagogical challenge relates to how to provide feedback in order to engage students in argumentation. Adequate prompting and tailored feedback to students can increase learning. However, teachers have limited time and resources to read students&rsquo; written arguments and provide immediate feedback in real-world classroom settings.</p> <p>Given the importance of complex systems found in Earth science topics such as global climate change and freshwater availability, and the abundance of argumentation opportunities surrounding such systems, helping students understand how to evaluate imperfect data and to examine limitations of climate/freshwater models is crucial. To support secondary students&rsquo; uncertainty-infused scientific argumentation skills, the project <em>Investigating How to Enhance Scientific Argumentation Through Automated Feedback in the Context of Two High School Earth Science Curriculum Units</em> used natural language processing techniques to automatically score students&rsquo; written scientific explanations and uncertainty attributions and provide immediate feedback within two High-Adventure Science (HAS) online curriculum modules developed by the Concord Consortium (CC). The HAS modules contain authentic science activities and real-world data and system models pertaining to climate change and freshwater availability.</p> <p>Sixteen scientific argumentation tasks are featured across the two modules after data-based investigations. Each argumentation task opens with a scientific question that requires students to either generate data by manipulating system models or select from existing real-world data. Students then write a scientific argument following four prompts: (1) selecting a claim, (2) explaining the claim using open-ended response, (3) indicating a level of certainty from not-at-all certain to very certain, and (4) attributing uncertainty sources using open-ended response.</p> <p>This project developed scoring rubrics and automated scoring models for written explanations and uncertainty attributions for eight climate change and eight freshwater availability argumentation tasks. Scoring of written explanations is based on the Claim-Evidence-Reasoning framework and scoring of written uncertainty attributions is based on the taxonomy of uncertainty developed at CC. Automated scoring models were created using c-rater-ML, Educational Testing Service&rsquo;s automated scoring engine. An automated feedback system called HASBot was developed to assess scores on students&rsquo; explanations and uncertainty attributions as part of scientific arguments and deliver instant feedback to students in real-time (less than five seconds). Two types of immediate feedback &mdash; generic and contextualized &mdash; were developed and tested in this project before the contextualized feedback was adopted for the final HASBot design.</p> <p>This project held professional development workshops with 19 teachers to help them understand uncertainty-infused argumentation and automated scoring, support the use of the online curricular modules in their classrooms, and facilitate integration of the material into their lesson plans. Several teacher support materials were developed as an adjunct to these workshops. Over the data collection period from 2014-2018, 18 teachers from 11 states implemented climate and/or freshwater modules containing HASBot feedback with approximately 1400 middle and high school students. The project developed a teacher dashboard that tracks, in real time, individual student and whole-class progress and performance on the scientific argumentation tasks. Teachers used this technology to walk over and help a single struggling student and to know their class&rsquo; strengths and weaknesses.</p> <p>Results indicate that receiving feedback, whether generic or contextualized, led to students making more scientifically-competent arguments during the module. However, the contextualized feedback had an advantage in several of the more challenging argumentation tasks and was preferred by students. After using HASBot feedback to practice revising and improving their scientific arguments, students improved their final argument scores within the modules. Students also made significant pretest-posttest gains in writing uncertainty-infused scientific arguments, Effect Size (Cohen&rsquo;s <em>d</em>) = 1.52 Standard Deviations, <em>p</em> = .001. Screencast video data collected while students interacted with the online modules indicated that students became more deeply engaged with the content and the models after receiving HASBot feedback. Contextualized feedback compared to generic was found to result in better learning outcomes. The data collected were used to develop other automated scoring models related to students&rsquo; modeling interactions with simulations and students&rsquo; images generated as part of evidence when constructing arguments. When comparing students who received model + argumentation feedback with those who received only argumentation feedback, significantly more students reran the simulation after their initial argument submission and improved their modeling interactions, resulting in more data-integrated arguments with stronger reasoning.</p> <p>To date, 11 papers have been published in academic journals and conference proceedings and results have been presented in 25 conferences and guest lectures.</p> <p>The climate change and freshwater availability modules can be found at CC&rsquo;s High-Adventure Science website: <a href="http://has.concord.org/">http://has.concord.org/</a></p> <p>&nbsp;</p><br> <p>            Last Modified: 08/08/2019<br>      Modified by: Ou&nbsp;L&nbsp;Liu</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312013666_Image4--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312013666_Image4--rgov-800width.jpg" title="Image 4"><img src="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312013666_Image4--rgov-66x44.jpg" alt="Image 4"></a> <div class="imageCaptionContainer"> <div class="imageCaption">HASBot Feedback System with contextualized feedback for Trap Task explanation and uncertainty attribution items</div> <div class="imageCredit">The Concord Consortium</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ou&nbsp;L&nbsp;Liu</div> <div class="imageTitle">Image 4</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311494276_Image2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311494276_Image2--rgov-800width.jpg" title="Image 2"><img src="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311494276_Image2--rgov-66x44.jpg" alt="Image 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Explanation scoring rubric and response examples for the Trap Task</div> <div class="imageCredit">The Concord Consortium</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ou&nbsp;L&nbsp;Liu</div> <div class="imageTitle">Image 2</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311704228_Image3--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311704228_Image3--rgov-800width.jpg" title="Image 3"><img src="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311704228_Image3--rgov-66x44.jpg" alt="Image 3"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Uncertainty attribution scoring rubric and response examples for the Trap Task</div> <div class="imageCredit">The Concord Consortium</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ou&nbsp;L&nbsp;Liu</div> <div class="imageTitle">Image 3</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312294475_Image5--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312294475_Image5--rgov-800width.jpg" title="Image 5"><img src="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312294475_Image5--rgov-66x44.jpg" alt="Image 5"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Teacher Dashboard</div> <div class="imageCredit">The Concord Consortium</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ou&nbsp;L&nbsp;Liu</div> <div class="imageTitle">Image 5</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312421632_Image6--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312421632_Image6--rgov-800width.jpg" title="Image 6"><img src="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565312421632_Image6--rgov-66x44.jpg" alt="Image 6"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Students enjoying completing the online modules with HASBot feedback</div> <div class="imageCredit">The Concord Consortium</div> <div class="imageSubmitted">Ou&nbsp;L&nbsp;Liu</div> <div class="imageTitle">Image 6</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311385039_Image1--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311385039_Image1--rgov-800width.jpg" title="Image 1"><img src="/por/images/Reports/POR/2019/1418019/1418019_10327958_1565311385039_Image1--rgov-66x44.jpg" alt="Image 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Example of one of the 16 argumentation tasks, the Trap Task, with system model</div> <div class="imageCredit">The Concord Consortium</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ou&nbsp;L&nbsp;Liu</div> <div class="imageTitle">Image 1</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Teaching students to practice scientific argumentation continues to be a challenge for science educators. In classrooms, students find it difficult to articulate and differentiate among claim, evidence, and reasoning as well as to evaluate the strengths and weaknesses of imperfect data when making interpretations. This happens mainly because students view science as already known facts while having little experience with the evaluative and uncertain nature of science. A pedagogical challenge relates to how to provide feedback in order to engage students in argumentation. Adequate prompting and tailored feedback to students can increase learning. However, teachers have limited time and resources to read students? written arguments and provide immediate feedback in real-world classroom settings.  Given the importance of complex systems found in Earth science topics such as global climate change and freshwater availability, and the abundance of argumentation opportunities surrounding such systems, helping students understand how to evaluate imperfect data and to examine limitations of climate/freshwater models is crucial. To support secondary students? uncertainty-infused scientific argumentation skills, the project Investigating How to Enhance Scientific Argumentation Through Automated Feedback in the Context of Two High School Earth Science Curriculum Units used natural language processing techniques to automatically score students? written scientific explanations and uncertainty attributions and provide immediate feedback within two High-Adventure Science (HAS) online curriculum modules developed by the Concord Consortium (CC). The HAS modules contain authentic science activities and real-world data and system models pertaining to climate change and freshwater availability.  Sixteen scientific argumentation tasks are featured across the two modules after data-based investigations. Each argumentation task opens with a scientific question that requires students to either generate data by manipulating system models or select from existing real-world data. Students then write a scientific argument following four prompts: (1) selecting a claim, (2) explaining the claim using open-ended response, (3) indicating a level of certainty from not-at-all certain to very certain, and (4) attributing uncertainty sources using open-ended response.  This project developed scoring rubrics and automated scoring models for written explanations and uncertainty attributions for eight climate change and eight freshwater availability argumentation tasks. Scoring of written explanations is based on the Claim-Evidence-Reasoning framework and scoring of written uncertainty attributions is based on the taxonomy of uncertainty developed at CC. Automated scoring models were created using c-rater-ML, Educational Testing Service?s automated scoring engine. An automated feedback system called HASBot was developed to assess scores on students? explanations and uncertainty attributions as part of scientific arguments and deliver instant feedback to students in real-time (less than five seconds). Two types of immediate feedback &mdash; generic and contextualized &mdash; were developed and tested in this project before the contextualized feedback was adopted for the final HASBot design.  This project held professional development workshops with 19 teachers to help them understand uncertainty-infused argumentation and automated scoring, support the use of the online curricular modules in their classrooms, and facilitate integration of the material into their lesson plans. Several teacher support materials were developed as an adjunct to these workshops. Over the data collection period from 2014-2018, 18 teachers from 11 states implemented climate and/or freshwater modules containing HASBot feedback with approximately 1400 middle and high school students. The project developed a teacher dashboard that tracks, in real time, individual student and whole-class progress and performance on the scientific argumentation tasks. Teachers used this technology to walk over and help a single struggling student and to know their class? strengths and weaknesses.  Results indicate that receiving feedback, whether generic or contextualized, led to students making more scientifically-competent arguments during the module. However, the contextualized feedback had an advantage in several of the more challenging argumentation tasks and was preferred by students. After using HASBot feedback to practice revising and improving their scientific arguments, students improved their final argument scores within the modules. Students also made significant pretest-posttest gains in writing uncertainty-infused scientific arguments, Effect Size (Cohen?s d) = 1.52 Standard Deviations, p = .001. Screencast video data collected while students interacted with the online modules indicated that students became more deeply engaged with the content and the models after receiving HASBot feedback. Contextualized feedback compared to generic was found to result in better learning outcomes. The data collected were used to develop other automated scoring models related to students? modeling interactions with simulations and students? images generated as part of evidence when constructing arguments. When comparing students who received model + argumentation feedback with those who received only argumentation feedback, significantly more students reran the simulation after their initial argument submission and improved their modeling interactions, resulting in more data-integrated arguments with stronger reasoning.  To date, 11 papers have been published in academic journals and conference proceedings and results have been presented in 25 conferences and guest lectures.  The climate change and freshwater availability modules can be found at CC?s High-Adventure Science website: http://has.concord.org/          Last Modified: 08/08/2019       Submitted by: Ou L Liu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
