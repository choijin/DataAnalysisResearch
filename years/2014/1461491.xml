<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>"Improved Methodologies for Field Experiments:  Maximizing Statistical Power While Promoting Replication."</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>459953.00</AwardTotalIntnAmount>
<AwardAmount>459953</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nancy Lutz</SignBlockName>
<PO_EMAI>nlutz@nsf.gov</PO_EMAI>
<PO_PHON>7032927280</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project proposes new methods for analyzing data from randomized controlled trials (RCTs), which are now increasingly used in some areas of economics and other social sciences.  Researchers often want to use an RCT to test multiple different hypotheses.  However, carrying out this kind of data analysis requires careful consideration of statistical issues; in particular, there is a chance that using the same data to run multiple different statistical analyses will increase the chance of false positives.  As a result, economists conducting RCTs are increasingly specifying pre-analysis plans; that is, they are specifying how they plan to analyze the data before beginning the analysis.  These plans, however, do limit researchers' ability to reuse data and learn from the data before beginning the statistical analysis.  The research team will develop a potential mechanism for researchers to learn from the data without specifying every hypothesis in advance.  They will use concepts from machine-learning as well as probability theory to provide a better framework for how to maximize power within a given RCT experiment.  The results could therefore improve data analysis in important ways.&lt;br/&gt;&lt;br/&gt;The research team has evidence that in practice preanalysis plans include tests for many, even hundreds, of hypotheses.  As a result current preanalysis plan designs are likely to be underpowered for many effects of economic interest.  The team will extend and apply techniques from biostatistics, including gatekeeping, sequential testing, and FDR control to the design of preanalysis plans. They will also develop a split-sample approach in which researchers conduct exploratory analysis on one part of the data set, and, using estimates from that part of the data combined with theory over likely mechanisms, refine their hypotheses before registering a subset of hypotheses to be tested on the remaining part of the data.</AbstractNarration>
<MinAmdLetterDate>09/03/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1461491</AwardID>
<Investigator>
<FirstName>Jeremy</FirstName>
<LastName>Magruder</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeremy Magruder</PI_FULL_NAME>
<EmailAddress>jmagruder@berkeley.edu</EmailAddress>
<PI_PHON>5106438742</PI_PHON>
<NSF_ID>000540394</NSF_ID>
<StartDate>09/03/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Anderson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Anderson</PI_FULL_NAME>
<EmailAddress>mlanderson@berkeley.edu</EmailAddress>
<PI_PHON>5106439676</PI_PHON>
<NSF_ID>000623207</NSF_ID>
<StartDate>09/03/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>947045940</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1320</Code>
<Text>Economics</Text>
</ProgramElement>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~459953</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><dd> <div class="tinyMCEContent"> <p>False discoveries underlie the failure of many experimental results to replicate in subsequent trials. Many field experiments test for effects on dozens or hundreds of outcomes, and the chance of finding one or more spurious results among so many tests is high. This project developed a split-sample framework for controlling false discoveries in field experiments. In our framework researchers conduct exploratory analyses on one part of the data set, and, using estimates from that part of the data combined with theory over likely mechanisms, refine their hypotheses before registering a subset of hypotheses to be tested on the remaining part of the data. The goal of this framework is to control false discoveries while maximizing statistical power (i.e. the probability of rejecting hypotheses, or generating findings) and maintaining researcher flexibility, allowing for novel discoveries. To accomplish this goal we integrated techniques from classical statistics and econometrics, biostatistics, and machine learning.</p> </div> </dd><dd> <div class="tinyMCEContent"> <p>From biostatistics we incorporated the concepts of sequential testing, gatekeeping, and false discovery rate control. Simply put, these techniques allow researchers to focus their studies on testing a small number of important, or primary, hypotheses, while retaining the flexibility to test secondary hypotheses if the primary hypotheses reject. To increase the chance of rejecting the primary hypotheses and exploring the secondary hypotheses, we applied techniques from classical statistics and machine learning to the exploratory sample analysis. These techniques allow researchers to refine their tests in order to maximize the probability that they reject the primary hypotheses and test additional hypotheses.</p> <p>Our framework allows researchers to&nbsp;either specify a single sample split with a blinded confirmation sample (i.e. a third party holds back the confirmation sample from the researcher until the appropriate time), or to specify many splits of the data without blinding. To enable the latter strategy we&nbsp;integrated recent developments in the machine-learning literature.&nbsp;This strategy obviates the need for a third party to hold back the blinded confirmation sample, but it only allows for pre-specified algorithmic analyses in the exploratory sample.</p> <p>We applied these techniques to two influential field experiments. One field experiment tested the effects of a community-driven development intervention in Sierra Leone. This experiment used a preregistered analysis plan to limit false discoveries and found no effects of the intervention on the main outcomes of interest. We show that with our split-sample framework, the researchers likely could have concluded that the intervention had significant, though modest, impacts. The second experiment estimated the effects of Medicaid on healthcare utilization and clinical outcomes. While the original analysis found significant effects on utilization, it did not find effects on any of the preregistered outcomes. We show that with our split-sample framework, the researchers likely could have found effects on clinical outcomes for some subgroups.</p> </div> </dd> <p>&nbsp;</p><br> <p>            Last Modified: 12/30/2019<br>      Modified by: Michael&nbsp;Anderson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   False discoveries underlie the failure of many experimental results to replicate in subsequent trials. Many field experiments test for effects on dozens or hundreds of outcomes, and the chance of finding one or more spurious results among so many tests is high. This project developed a split-sample framework for controlling false discoveries in field experiments. In our framework researchers conduct exploratory analyses on one part of the data set, and, using estimates from that part of the data combined with theory over likely mechanisms, refine their hypotheses before registering a subset of hypotheses to be tested on the remaining part of the data. The goal of this framework is to control false discoveries while maximizing statistical power (i.e. the probability of rejecting hypotheses, or generating findings) and maintaining researcher flexibility, allowing for novel discoveries. To accomplish this goal we integrated techniques from classical statistics and econometrics, biostatistics, and machine learning.     From biostatistics we incorporated the concepts of sequential testing, gatekeeping, and false discovery rate control. Simply put, these techniques allow researchers to focus their studies on testing a small number of important, or primary, hypotheses, while retaining the flexibility to test secondary hypotheses if the primary hypotheses reject. To increase the chance of rejecting the primary hypotheses and exploring the secondary hypotheses, we applied techniques from classical statistics and machine learning to the exploratory sample analysis. These techniques allow researchers to refine their tests in order to maximize the probability that they reject the primary hypotheses and test additional hypotheses.  Our framework allows researchers to either specify a single sample split with a blinded confirmation sample (i.e. a third party holds back the confirmation sample from the researcher until the appropriate time), or to specify many splits of the data without blinding. To enable the latter strategy we integrated recent developments in the machine-learning literature. This strategy obviates the need for a third party to hold back the blinded confirmation sample, but it only allows for pre-specified algorithmic analyses in the exploratory sample.  We applied these techniques to two influential field experiments. One field experiment tested the effects of a community-driven development intervention in Sierra Leone. This experiment used a preregistered analysis plan to limit false discoveries and found no effects of the intervention on the main outcomes of interest. We show that with our split-sample framework, the researchers likely could have concluded that the intervention had significant, though modest, impacts. The second experiment estimated the effects of Medicaid on healthcare utilization and clinical outcomes. While the original analysis found significant effects on utilization, it did not find effects on any of the preregistered outcomes. We show that with our split-sample framework, the researchers likely could have found effects on clinical outcomes for some subgroups.            Last Modified: 12/30/2019       Submitted by: Michael Anderson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
