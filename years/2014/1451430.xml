<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Discrete Algorithms in NLP</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>75000.00</AwardTotalIntnAmount>
<AwardAmount>75000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Algorithms that can understand human language must be able to recognize the underlying structure (e.g., subject-verb-object) of that language. Computational approaches developed in the natural language processing community typically have build ad hoc, one-off algorithms for solving the hard, combinatorial optimization problems that arise in such tasks. Most large-scale systems are built using complex combinations of heuristics applied to try to make approximate search techniques better. Concurrently, the algorithms community has developed scalable exact algorithms and approximation algorithms for solving many of these hard combinatorial optimization problems. This EArly Grant for Exploratory Research investigates the connection between these two extremes: the language processing community with the hard problems they need solved, and the algorithms community with the provably correct algorithms for solving such hard problems. The biggest technical challenge this exploration addresses is how to couple the statistical learning algorithms necessary to build effective language applications with the types of abstractions that make efficient algorithms possible. In particular, this project explores the application of "inverse optimization" to machine learning. For example, if one has access to an efficient algorithm for solving a particular discrete optimization problem, how can one learn parameters that make that particular algorithm as high accuracy as possible? Success in this project will give rise to theoretically principled, efficient algorithms for learning to solve complex linguistic tasks, which can transform to downstream applications like machine translation, automatic question answering and information retrieval.&lt;br/&gt;&lt;br/&gt;This project's main technical innovation is the coupling of "inverse optimization" problems with online learning techniques. For instance, suppose that the end goal is to find some particular structure. The search for this structure can often be cast as a particular form of dynamic programming problem, which in turn often becomes a shortest path problem in a hypergraph. The machine learning challenge then is to learn a model under which the solution to this shortest path search is actually the desired structure. From an algorithmic perspective, this requires finding a set of inputs under which a given structure is optimal: inverse optimization. However, it is not enough for a given structure to be optimal: it must also beat all other (non-optimal) structures by some given margin. This project will develop a combination of online learning algorithms and inverse optimization formulations that enable such advances.</AbstractNarration>
<MinAmdLetterDate>08/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1451430</AwardID>
<Investigator>
<FirstName>Samir</FirstName>
<LastName>Khuller</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Samir Khuller</PI_FULL_NAME>
<EmailAddress>samir.khuller@northwestern.edu</EmailAddress>
<PI_PHON>8474912748</PI_PHON>
<NSF_ID>000177377</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hal</FirstName>
<LastName>Daume</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>III</PI_SUFX_NAME>
<PI_FULL_NAME>Hal Daume</PI_FULL_NAME>
<EmailAddress>hal@umiacs.umd.edu</EmailAddress>
<PI_PHON>8015567863</PI_PHON>
<NSF_ID>000445461</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7796</Code>
<Text>ALGORITHMIC FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~75000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Structured prediction is the task of learning a model that can make joint predictions over a number of variables simultaneously. It forms the heart of problems in natural language processing, computer vision, computational biology, and any applied area that studies objects (e.g., sentences) that have complex internal structure. Producing these structures typically requires expensive discrete optimization (finding the "best" sentence). When learning to predict structured objects, one has access to training data that tells the algorithm what the correct structure is; the inverse optimization problem is addressing the question: how must I change the learned model so that it predicts the correct answer.</p> <p>We developed novel algorithms for inverse optimization, for <em>matroid intersection</em> problems: a class of problems that includes the majority of those found in practice. Using this framework allowed us to solve all of these problems in a unified manner. We then developed highly efficient machine learning algorithms that (a) have low regret (are guaranteed to do as well as the optimal available solution, given enough data), and (b) work well in practice, when applied to two problems from natural language processing: identifying correspondences between words in two languages, and automatically determining the syntactic structure of sentences.</p> <p>We also developed novel algorithms for solving the forward discrete optimization problem for two challenging structured problems. First, we developed novel algorithms to solve a variant of a scheduling problem called <em>digraph ordering, </em>for which we develop an efficient algorithm that produces a solution that's at worse a factor two worse than optimal. Second, we developed novel algorithms for the <em>maximum cut</em> problem on two common types of graphs; our algorithm gives a polynomial-time solution for arbitrarily good approximations to the solution of this problem.</p><br> <p>            Last Modified: 12/13/2015<br>      Modified by: Hal&nbsp;Daume</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Structured prediction is the task of learning a model that can make joint predictions over a number of variables simultaneously. It forms the heart of problems in natural language processing, computer vision, computational biology, and any applied area that studies objects (e.g., sentences) that have complex internal structure. Producing these structures typically requires expensive discrete optimization (finding the "best" sentence). When learning to predict structured objects, one has access to training data that tells the algorithm what the correct structure is; the inverse optimization problem is addressing the question: how must I change the learned model so that it predicts the correct answer.  We developed novel algorithms for inverse optimization, for matroid intersection problems: a class of problems that includes the majority of those found in practice. Using this framework allowed us to solve all of these problems in a unified manner. We then developed highly efficient machine learning algorithms that (a) have low regret (are guaranteed to do as well as the optimal available solution, given enough data), and (b) work well in practice, when applied to two problems from natural language processing: identifying correspondences between words in two languages, and automatically determining the syntactic structure of sentences.  We also developed novel algorithms for solving the forward discrete optimization problem for two challenging structured problems. First, we developed novel algorithms to solve a variant of a scheduling problem called digraph ordering, for which we develop an efficient algorithm that produces a solution that's at worse a factor two worse than optimal. Second, we developed novel algorithms for the maximum cut problem on two common types of graphs; our algorithm gives a polynomial-time solution for arbitrarily good approximations to the solution of this problem.       Last Modified: 12/13/2015       Submitted by: Hal Daume]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
