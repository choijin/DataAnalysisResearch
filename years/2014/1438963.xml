<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: EXPL: CCA: Collaborative Research: Nixing Scale Bugs in HPC Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>166128</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Large-scale simulation is a fundamental component of modern science and engineering. Unfortunately, programs written to perform simulations on large-scale parallel computers frequently suffer from software defects that result from the sheer scale and the variety of parallelization approaches employed. Especially egregious are software bugs that occur when large resource allocations (e.g., memory requests) are made. Formally based active-testing techniques are essential to locate such defects. However, these testing tools are themselves seldom run on parallel machines, let alone at large scale, making it difficult and very time consuming to find scale bugs with high assurance. &lt;br/&gt;&lt;br/&gt;Efforts to parallelize verification tools should reuse existing technology for easy parallelization, result collection, and fault handling. Key innovations of this project include the insight that large-scale verification runs can be described through work-flows, which makes it possible to take advantage of already available distributed computing platforms, in particular Swift/T from Argonne. The complementary backgrounds of the PIs are well matched with the need to push both formal aspects and distributed verification in the context of three widely-used concurrency models, namely MPI, OpenMP, and CUDA. &lt;br/&gt;&lt;br/&gt;This work will help create a public distributed formal active testing framework. The tools and case-study software driving this research will be maintained by the PIs and released freely under open-source licenses through websites and repositories. They will facilitate large-scale debugging of scientific simulation codes by researchers and software developers in academia, government labs, and industry. &lt;br/&gt;&lt;br/&gt;The project will also generate pedagogical material and best practices, helping educate students in the use of existing work-flow based problem solving approaches. It will help train present and future scientists, engineers, and programmers, thus assisting in maintaining our nation's leadership in computing, homeland and energy security, and STEM education.</AbstractNarration>
<MinAmdLetterDate>08/06/2014</MinAmdLetterDate>
<MaxAmdLetterDate>05/12/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1438963</AwardID>
<Investigator>
<FirstName>Martin</FirstName>
<LastName>Burtscher</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martin Burtscher</PI_FULL_NAME>
<EmailAddress>burtscher@txstate.edu</EmailAddress>
<PI_PHON>5122452314</PI_PHON>
<NSF_ID>000572212</NSF_ID>
<StartDate>08/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas State University - San Marcos</Name>
<CityName>San Marcos</CityName>
<ZipCode>786664616</ZipCode>
<PhoneNumber>5122452314</PhoneNumber>
<StreetAddress>601 University Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>28</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX28</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>074602368</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>101405814</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas State University - San Marcos]]></Name>
<CityName>San Marcos</CityName>
<StateCode>TX</StateCode>
<ZipCode>786664684</ZipCode>
<StreetAddress><![CDATA[601 University Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>35</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX35</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~150000</FUND_OBLG>
<FUND_OBLG>2015~16128</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>High-performance computing (HPC) is of key importance to our nation. For example, it is used for weather forecasting, medicine development, and renewable energy design. One of the central prerequisites for achieving meaningful scientific computing is the ability to track down and eliminate programming errors. Doing so on highly parallel systems that may consist of different types of processors is challenging. After all, literally trillions of operations can execute on modern HPC systems between the time an incorrect action takes place (e.g., a wrong value is computed) and the time when this action triggers an observable error (e.g., a crash or an erroneous program output). Since it is generally unknown when an error occurs and when it will manifest itself, software developers would like to be able to capture the entire program&rsquo;s behavior for later analysis. However, doing so in computers with millions of processing cores is a daunting task because recording just a little information per second on each core might already generate more data than the file system can handle in terms of throughput and possibly also in terms of available storage capacity.</p> <p>To make &ldquo;always-on&rdquo; program tracing and debugging possible even for large-scale HPC codes, we developed a first-of-its-kind low-overhead tracing technique. It is based on a custom data compression approach that we designed specifically for this purpose. It compresses the trace not only well and quickly but also in an incremental manner, meaning that it compresses the data right when they are produced and before they are stored. As a consequence, the resulting program traces are small, thus greatly reducing the required storage capacity, and the rate at which trace data need to be recorded is low, thus greatly reducing the required bandwidth. We developed such a compression approach both for the main processors (CPUs) as well as for compute accelerators (GPUs). We also wrote corresponding decompression algorithms and a trace viewer.</p> <p>Our measurements show that just recording a unique 16-bit identifier for every function call and return when running a set of scientific applications on the Stampede supercomputer results in about 2 MB/s of trace data per core on average. Extrapolating this value to all 102,400 cores (not counting the accelerators) yields 205 GB/s of data, which exceeds the filesystem&rsquo;s write performance of 150 GB/s. In contrast, our compression-based tracing method reduces the emitted data by a factor of 100 on average, a ratio that is quite stable with respect to scaling, making it possible to trace full-scale programs while leaving over 98% of the I/O bandwidth to the application. Importantly, using compression typically lowers the tracing overhead in spite of the burden of having to run the extra compression code because so much less data have to be moved through the system.</p> <p>The nation&rsquo;s ability to conduct advanced scientific research depends on HPC. The results of this project directly contribute to speeding up the creation, improving the quality, and lowering the cost of the corresponding software. Moreover, it helps make it possible to successfully develop more advanced software. Thus, our work indirectly enables other transformative research through an increase in exploitable computing performance. Clearly, making it possible to solve important problems faster can help all of humanity, perhaps by discovering cures for cancer or other devastating health conditions, running more accurate hurricane and tsunami predictions to provide earlier and more precise warnings, building more efficient and resilient power grids, etc.</p><br> <p>            Last Modified: 11/18/2017<br>      Modified by: Martin&nbsp;Burtscher</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ High-performance computing (HPC) is of key importance to our nation. For example, it is used for weather forecasting, medicine development, and renewable energy design. One of the central prerequisites for achieving meaningful scientific computing is the ability to track down and eliminate programming errors. Doing so on highly parallel systems that may consist of different types of processors is challenging. After all, literally trillions of operations can execute on modern HPC systems between the time an incorrect action takes place (e.g., a wrong value is computed) and the time when this action triggers an observable error (e.g., a crash or an erroneous program output). Since it is generally unknown when an error occurs and when it will manifest itself, software developers would like to be able to capture the entire program?s behavior for later analysis. However, doing so in computers with millions of processing cores is a daunting task because recording just a little information per second on each core might already generate more data than the file system can handle in terms of throughput and possibly also in terms of available storage capacity.  To make "always-on" program tracing and debugging possible even for large-scale HPC codes, we developed a first-of-its-kind low-overhead tracing technique. It is based on a custom data compression approach that we designed specifically for this purpose. It compresses the trace not only well and quickly but also in an incremental manner, meaning that it compresses the data right when they are produced and before they are stored. As a consequence, the resulting program traces are small, thus greatly reducing the required storage capacity, and the rate at which trace data need to be recorded is low, thus greatly reducing the required bandwidth. We developed such a compression approach both for the main processors (CPUs) as well as for compute accelerators (GPUs). We also wrote corresponding decompression algorithms and a trace viewer.  Our measurements show that just recording a unique 16-bit identifier for every function call and return when running a set of scientific applications on the Stampede supercomputer results in about 2 MB/s of trace data per core on average. Extrapolating this value to all 102,400 cores (not counting the accelerators) yields 205 GB/s of data, which exceeds the filesystem?s write performance of 150 GB/s. In contrast, our compression-based tracing method reduces the emitted data by a factor of 100 on average, a ratio that is quite stable with respect to scaling, making it possible to trace full-scale programs while leaving over 98% of the I/O bandwidth to the application. Importantly, using compression typically lowers the tracing overhead in spite of the burden of having to run the extra compression code because so much less data have to be moved through the system.  The nation?s ability to conduct advanced scientific research depends on HPC. The results of this project directly contribute to speeding up the creation, improving the quality, and lowering the cost of the corresponding software. Moreover, it helps make it possible to successfully develop more advanced software. Thus, our work indirectly enables other transformative research through an increase in exploitable computing performance. Clearly, making it possible to solve important problems faster can help all of humanity, perhaps by discovering cures for cancer or other devastating health conditions, running more accurate hurricane and tsunami predictions to provide earlier and more precise warnings, building more efficient and resilient power grids, etc.       Last Modified: 11/18/2017       Submitted by: Martin Burtscher]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
