<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Language Induction meets Language Documentation: Leveraging bilingual aligned audio for learning and preserving languages</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>470000.00</AwardTotalIntnAmount>
<AwardAmount>470000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>D.  Langendoen</SignBlockName>
<PO_EMAI>dlangend@nsf.gov</PO_EMAI>
<PO_PHON>7032925088</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Thousands of the world's languages are in danger of dying out before they have been systematically documented. Many other languages have millions of speakers, yet they exist only in spoken form, and minimal documentary records are available. As a consequence, important sources of knowledge about human language and culture are inaccessible, and at risk of being lost forever. Moreover, it is difficult to develop technologies for processing these languages, leaving their speech communities on the far side of a widening digital divide. The first step to solving these problems is language documentation, and so the goal of this project is to develop computational methods based on automatic speech recognition and machine translation for documenting endangered and unwritten languages on an unprecedented scale.&lt;br/&gt;&lt;br/&gt;To be successful, any approach must guarantee both the sufficiency and interpretability of the documentation it produces. This project ensures sufficiency by using a combination of community outreach, crowdsourcing techniques, and mobile/web technologies to collect hundreds of hours (millions of words) of speech. The interpretability is enabled by augmenting original speech recordings with careful verbatim repetitions along with translations into a well-resourced language. Finally, computational models are developed to automate transcription of recordings and alignment with translations, resulting in bilingual aligned text. The result is a kind of digital Rosetta Stone: a large-scale key for interpreting the world's languages even if they are not written, or no longer even spoken.</AbstractNarration>
<MinAmdLetterDate>10/29/2014</MinAmdLetterDate>
<MaxAmdLetterDate>04/30/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1464553</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Chiang</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David W Chiang</PI_FULL_NAME>
<EmailAddress>dchiang@nd.edu</EmailAddress>
<PI_PHON>5746319441</PI_PHON>
<NSF_ID>000071687</NSF_ID>
<StartDate>10/29/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Notre Dame</Name>
<CityName>NOTRE DAME</CityName>
<ZipCode>465565708</ZipCode>
<PhoneNumber>5746317432</PhoneNumber>
<StreetAddress>940 Grace Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>824910376</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NOTRE DAME DU LAC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>048994727</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Notre Dame]]></Name>
<CityName>Notre Dame</CityName>
<StateCode>IN</StateCode>
<ZipCode>465565708</ZipCode>
<StreetAddress><![CDATA[940 Grace Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7719</Code>
<Text>DEL</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~470000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the Lido project was to develop technologies for creating interpretable archives of endangered-language data at scale. More specifically, the goal was to transform a collection of parallel speech (speech in an endangered language and its speech translation into English or another contact language) into something resembling interlinear glossed text, i.e., with a text transcription of both languages and with a word-by-word gloss or word alignment between the two transcriptions.</p> <p><strong>Intellectual Merit</strong>: We demonstrated that in the absence of transcribed speech data, we can directly align speech to its translation. A possible application would be to produce word glosses for speech data, helping linguists to interpret the data. We also showed that with a relatively small amount of transcribed speech data and a small amount of parallel text, we can use a speech transcription model to improve a translation model and vice versa.</p> <p>We also developed a model of transcription that replaces onerous phonemic transcription, which can only be performed by trained linguists, with a new method called sparse lexical transcription, which is more scalable and can be performed by minimally-trained speakers.</p> <p>Additionally, we developed new state-of-the-art models for a wide range of other tasks related to low-resource and endangered languages: unsupervised word-spotting, speech transcription of tonal languages, induction of bilingual lexicons and word embeddings, part-of-speech tagging, and dependency parsing.</p> <p><strong>Broader Impacts</strong>: The project has addressed the problem of scaling up the urgent work of documenting the world's languages by: (a) lowering the amount of training data required for computational methods; (b) increasing the productivity of linguists on existing tasks; and (c) developing new documentary methods that can be performed by speakers who have no specialised training.</p> <p>We have released many of the systems developed in this project as open-source software, including: Persephone, an open-source automatic transcription tool intended for use by linguists working in endangered language situations; Zahwa, a mobile web app designed for use by speakers of endangered languages to documenting procedural knowledge; and Penne, an open-source deep learning toolkit designed for ease of use and clarity of implementation.</p> <p>Finally, we created and released datasets of audio with translations in several languages, and non-native English text with Spanish translations.</p><br> <p>            Last Modified: 01/14/2019<br>      Modified by: David&nbsp;W&nbsp;Chiang</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1464553/1464553_10333994_1547498964917_steven-dean--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1464553/1464553_10333994_1547498964917_steven-dean--rgov-800width.jpg" title="Dean Yibarbuk interpreting Bininj culture to PI Steven Bird (West Arnhem, Australia)"><img src="/por/images/Reports/POR/2019/1464553/1464553_10333994_1547498964917_steven-dean--rgov-66x44.jpg" alt="Dean Yibarbuk interpreting Bininj culture to PI Steven Bird (West Arnhem, Australia)"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Dean Yibarbuk interpreting Bininj culture to PI Steven Bird (West Arnhem, Australia)</div> <div class="imageCredit">Kirsten Bird</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">David&nbsp;W&nbsp;Chiang</div> <div class="imageTitle">Dean Yibarbuk interpreting Bininj culture to PI Steven Bird (West Arnhem, Australia)</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1464553/1464553_10333994_1547498889436_zahwa-scrub-wallaby--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1464553/1464553_10333994_1547498889436_zahwa-scrub-wallaby--rgov-800width.jpg" title="Conrad Maralngurra documenting the preparation of scrub wallaby (West Arnhem, Australia)"><img src="/por/images/Reports/POR/2019/1464553/1464553_10333994_1547498889436_zahwa-scrub-wallaby--rgov-66x44.jpg" alt="Conrad Maralngurra documenting the preparation of scrub wallaby (West Arnhem, Australia)"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Conrad Maralngurra documenting the preparation of scrub wallaby (West Arnhem, Australia)</div> <div class="imageCredit">Alexandra Marley and Mat Bettinson</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">David&nbsp;W&nbsp;Chiang</div> <div class="imageTitle">Conrad Maralngurra documenting the preparation of scrub wallaby (West Arnhem, Australia)</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the Lido project was to develop technologies for creating interpretable archives of endangered-language data at scale. More specifically, the goal was to transform a collection of parallel speech (speech in an endangered language and its speech translation into English or another contact language) into something resembling interlinear glossed text, i.e., with a text transcription of both languages and with a word-by-word gloss or word alignment between the two transcriptions.  Intellectual Merit: We demonstrated that in the absence of transcribed speech data, we can directly align speech to its translation. A possible application would be to produce word glosses for speech data, helping linguists to interpret the data. We also showed that with a relatively small amount of transcribed speech data and a small amount of parallel text, we can use a speech transcription model to improve a translation model and vice versa.  We also developed a model of transcription that replaces onerous phonemic transcription, which can only be performed by trained linguists, with a new method called sparse lexical transcription, which is more scalable and can be performed by minimally-trained speakers.  Additionally, we developed new state-of-the-art models for a wide range of other tasks related to low-resource and endangered languages: unsupervised word-spotting, speech transcription of tonal languages, induction of bilingual lexicons and word embeddings, part-of-speech tagging, and dependency parsing.  Broader Impacts: The project has addressed the problem of scaling up the urgent work of documenting the world's languages by: (a) lowering the amount of training data required for computational methods; (b) increasing the productivity of linguists on existing tasks; and (c) developing new documentary methods that can be performed by speakers who have no specialised training.  We have released many of the systems developed in this project as open-source software, including: Persephone, an open-source automatic transcription tool intended for use by linguists working in endangered language situations; Zahwa, a mobile web app designed for use by speakers of endangered languages to documenting procedural knowledge; and Penne, an open-source deep learning toolkit designed for ease of use and clarity of implementation.  Finally, we created and released datasets of audio with translations in several languages, and non-native English text with Spanish translations.       Last Modified: 01/14/2019       Submitted by: David W Chiang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
