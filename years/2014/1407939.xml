<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Next-Generation Statistical Optimization Methods for Big Data Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>499955.00</AwardTotalIntnAmount>
<AwardAmount>499955</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops a new generation of optimization methods to address data mining and knowledge discovery challenges in large-scale scientific data analysis. The project is constructed in the context that modern computing architectures are enabling us to fit complex statistical models (Big Models) on large and complex datasets (Big Data).  However, despite significant progress in each subfield of Big Data, Big Model, and modern computing architecture, we are still lacking  powerful optimization  techniques to effectively integrate these key components.&lt;br/&gt;&lt;br/&gt;One important bottleneck is that many general-purpose optimization  methods are not specifically designed for statistical learning problems. Even some of them are tailored to utilize specific problem structures, they have not actually incorporated sophisticated statistical thinking into algorithm design and analysis. To tackle this bottleneck, the project extends traditional theory to open new possibilities for nontraditional optimization  problems, such as nonconvex and infinite-dimensional examples.  The project develops deeper theoretical understanding of several challenging  issues in optimization (such as nonconvexity), develops new algorithms that will lead to better practical methods in the big data era, and demonstrates the new methods on challenging bio-informatics problems.&lt;br/&gt;&lt;br/&gt;The project is closely related to NSF's mission to promote Big Data research, and will have broad impacts. In the Big Data era, we see an urgent need for powerful optimization methods to handle the increasing complexity of modern datasets.  However, we still lack adequate methods, theory, and computational techniques.  By simultaneously addressing these aspects, this project will deliver novel and useful statistical optimization methods that benefit all relevant scientific areas. The project will deliver easy-to-use software packages which directly help scientists to explore and analyze complex datasets.  Both PIs will also design and develop new classes to teach modern techniques in handling big data optimization problems. All the course materials - including lecture notes, problem sets, source code, solutions and working  examples - will be freely  accessed online.  Moreover, both PIs will write tutorial  papers and disseminate the results of this research through the internet, academic conferences, workshops,  and journals.  Through senior theses and potentially the REU (Research Experiences for Undergraduates) program, the proposed project will also actively include undergraduates and engage under-represented minority groups.&lt;br/&gt;&lt;br/&gt;To achieve these goals, this project develops (i) a new research area named statistical optimization, which incorporates sophisticated statistical thinking into modern optimization, and will effectively bridge machine learning, statistics, optimization,  and stochastic analysis; (ii) new theoretical frameworks and computational methods for nonconvex and infinite-dimensional optimization, which will motivate effective optimization methods with theoretical  guarantees that are applicable to a wide variety of prominent statistical models; (iii) new scalable optimization methods, which aim at fully harnessing the horsepower of modern large-scale distributed computing infrastructure.  The project will shed new theoretical light on large-scale optimization, advance practice through novel algorithms and software, and demonstrate the methods on challenging bio-informatics problems.</AbstractNarration>
<MinAmdLetterDate>08/13/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1407939</AwardID>
<Investigator>
<FirstName>Cun-Hui</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cun-Hui Zhang</PI_FULL_NAME>
<EmailAddress>czhang@stat.rutgers.edu</EmailAddress>
<PI_PHON>8484457685</PI_PHON>
<NSF_ID>000185504</NSF_ID>
<StartDate>06/30/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tong</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tong Zhang</PI_FULL_NAME>
<EmailAddress>tzhang@stat.rutgers.edu</EmailAddress>
<PI_PHON>8484457643</PI_PHON>
<NSF_ID>000102616</NSF_ID>
<StartDate>08/13/2014</StartDate>
<EndDate>06/30/2016</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548019</ZipCode>
<StreetAddress><![CDATA[110 Frelinghuysen Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~143757</FUND_OBLG>
<FUND_OBLG>2015~137360</FUND_OBLG>
<FUND_OBLG>2016~107920</FUND_OBLG>
<FUND_OBLG>2017~110918</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We have been developing new nonconvex statistical optimization methods which provide theoretical guarantees for a large family of penalized M-estimators. In particular, we developed a new model based framework to study large-scale optimization, which forms a new area that lies at the intersection of statistical learning, robust optimization, and stochastic optimization. We apply ideas from modern probability and statistics to solve large and complex optimization problems. Under the model based optimization framework, we have developed rigorous theoretical framework to characterize the interaction between informational and computational complexity. In addition, we have investigated various methods for stochastic optimization that are suitable for large scale datasets, and distributed optimization methods for big data problems. We have also studied deep neural networks and complex models for application problems.&nbsp;</p> <p>We have investigated theoretical and algorithmic aspects of statistical optimization. We aim to provide theoretical analysis of two important algorithmic strategies for solving nonconvex penalized M-estimators. We studied some fundamental algorithms in stochastic optimization, including a new strategy for exploiting local smoothness when performing stochastic optimization. We have also studied stochastic algorithms for solving eigenvalue problems and generalized eigenvalue problems. In addition, we investigated methods for distributed optimization, as well as applications of nonlinear methods such as neural networks, and hierarchical models for various applications such as text categorization and influenza vaccine prediction problems. We also aim to characterize the fundamental statistical limits of certain optimization methods.</p> <p>We have proposed a new computational framework using path-wise coordinate optimization to solve nonconvex regularization problems in high dimension with fast convergence and optimal statistical performance. We published novel algorithms for stochastic optimization, and for distributed optimization, which can speed up machine learning computation with big datasets considerably. We studied eigenvalue problems and generalized eigenvalue problems, and derived new theoretical results and new numerical algorithms that can be applied to a variety of data. We also investigated new region-based embedding methods for neural networks and applied such techniques to text categorization. We also develop new theoretical results to characterize the computational lower bounds of convex relaxation methods (e.g., the sum of squares formulation of semidefinite programming). We also developed a new oracle computational model to explore the fundamental limits and phase transition phenomena of different optimization based learning problems. &nbsp;</p> <p>We wrote several journal articles and several conference articles. Some of them have been published on the leading optimization journals (e.g., Mathematical Programming A and Mathematical Programming Computation) and leading statistical journals (e.g., The Annals of Statistics). Our conference articles have been published on the leading conferences including the NIPS, ICML, and AISTATS.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/11/2019<br>      Modified by: Cun-Hui&nbsp;Zhang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We have been developing new nonconvex statistical optimization methods which provide theoretical guarantees for a large family of penalized M-estimators. In particular, we developed a new model based framework to study large-scale optimization, which forms a new area that lies at the intersection of statistical learning, robust optimization, and stochastic optimization. We apply ideas from modern probability and statistics to solve large and complex optimization problems. Under the model based optimization framework, we have developed rigorous theoretical framework to characterize the interaction between informational and computational complexity. In addition, we have investigated various methods for stochastic optimization that are suitable for large scale datasets, and distributed optimization methods for big data problems. We have also studied deep neural networks and complex models for application problems.   We have investigated theoretical and algorithmic aspects of statistical optimization. We aim to provide theoretical analysis of two important algorithmic strategies for solving nonconvex penalized M-estimators. We studied some fundamental algorithms in stochastic optimization, including a new strategy for exploiting local smoothness when performing stochastic optimization. We have also studied stochastic algorithms for solving eigenvalue problems and generalized eigenvalue problems. In addition, we investigated methods for distributed optimization, as well as applications of nonlinear methods such as neural networks, and hierarchical models for various applications such as text categorization and influenza vaccine prediction problems. We also aim to characterize the fundamental statistical limits of certain optimization methods.  We have proposed a new computational framework using path-wise coordinate optimization to solve nonconvex regularization problems in high dimension with fast convergence and optimal statistical performance. We published novel algorithms for stochastic optimization, and for distributed optimization, which can speed up machine learning computation with big datasets considerably. We studied eigenvalue problems and generalized eigenvalue problems, and derived new theoretical results and new numerical algorithms that can be applied to a variety of data. We also investigated new region-based embedding methods for neural networks and applied such techniques to text categorization. We also develop new theoretical results to characterize the computational lower bounds of convex relaxation methods (e.g., the sum of squares formulation of semidefinite programming). We also developed a new oracle computational model to explore the fundamental limits and phase transition phenomena of different optimization based learning problems.    We wrote several journal articles and several conference articles. Some of them have been published on the leading optimization journals (e.g., Mathematical Programming A and Mathematical Programming Computation) and leading statistical journals (e.g., The Annals of Statistics). Our conference articles have been published on the leading conferences including the NIPS, ICML, and AISTATS.           Last Modified: 07/11/2019       Submitted by: Cun-Hui Zhang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
