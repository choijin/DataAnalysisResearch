<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Telescopic Analysis for Black-Box Troubleshooting of Distributed Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>498241.00</AwardTotalIntnAmount>
<AwardAmount>498241</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is developing scalable mechanisms to debug, monitor, and&lt;br/&gt;assess the quality of the complex distributed systems that represent&lt;br/&gt;the backbone of modern software infrastructure.  These methods are&lt;br/&gt;necessarily highly-automated; they reason about the operation of&lt;br/&gt;distributed systems while treating the components of such systems as&lt;br/&gt;black boxes.  This means that the methods do not require source code,&lt;br/&gt;programmer annotation, or developer input to troubleshoot a&lt;br/&gt;distributed system.  Instead, they rely on detailed information&lt;br/&gt;gleaned from pre-existing log messages that are nearly ubiquitous in&lt;br/&gt;every large-scale distributed system and data extracted via binary&lt;br/&gt;analysis of components as they run.&lt;br/&gt;&lt;br/&gt;These new methods, termed telescopic analysis, combine the ability to&lt;br/&gt;collect extremely detailed, low-level information about systems&lt;br/&gt;executing large numbers of requests with "big data" analysis that&lt;br/&gt;mines insights and create models of system operation from the corpus&lt;br/&gt;of detailed observations.  Telescopic analysis uses targeted,&lt;br/&gt;sample-based logging and/or binary analysis to generate substantial&lt;br/&gt;quantities of high-precision data about specific runs of the system&lt;br/&gt;under observation.  It then combines these observations into models&lt;br/&gt;that capture the aggregate behavior of the system. Comparing the&lt;br/&gt;general model with the detailed observations of each run allows&lt;br/&gt;understanding of how that run conforms to or deviates from the common&lt;br/&gt;operation of the system.  The project is also developing tools and&lt;br/&gt;query languages that allow understanding of the results of such&lt;br/&gt;comparisons, both in aggregate and as pertains to specific runs, for&lt;br/&gt;performance analysis, debugging data quality failures, understanding&lt;br/&gt;outlier behavior, and performing "what-if" analysis.</AbstractNarration>
<MinAmdLetterDate>08/13/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1421441</AwardID>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Flinn</LastName>
<PI_MID_INIT>N</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jason N Flinn</PI_FULL_NAME>
<EmailAddress>jflinn@umich.edu</EmailAddress>
<PI_PHON>7349365983</PI_PHON>
<NSF_ID>000096770</NSF_ID>
<StartDate>08/13/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Cafarella</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Cafarella</PI_FULL_NAME>
<EmailAddress>michjc@umich.edu</EmailAddress>
<PI_PHON>7347641817</PI_PHON>
<NSF_ID>000544946</NSF_ID>
<StartDate>08/13/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName>Ann Arbor</CityName>
<StateCode>MI</StateCode>
<ZipCode>481092121</ZipCode>
<StreetAddress><![CDATA[2260 Hayward]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~498241</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed new scalable mechanisms, termed telescopic analysis, to debug, monitor, and assess the quality of complex distributed systems.&nbsp; Telescopic analysis is designed to be highly-automated; it reasons about the operation of distributed systems while treating the individual components of the systems as black boxes.&nbsp; The techniques that we created combine the ability to collect extremely detailed, low-level information about systems executing large numbers of request with "big data" analysis that mines for insights and creates models of system operation from the corpus of detailed observations.</p> <p>The outcomes of the project have substantially improved the state-of-the-art for monitoring and debugging Internet-scale distributed systems that form the backbone of our nation's computing infrastrucutre.&nbsp; Our results have been disseminated through publication at top peer-reviewed forums, through open-source release of source code, and through technology transfer to industry.</p> <p><span>Our first major outcome derived from applying telescopic analysis to performance debugging of large-scale Internet pipelines. &nbsp;We observed that the scale of such systems means that even at low sampling rates, we can gather a tremendous amount of empirical performance observations through low-level request logging. &nbsp;We can then apply &ldquo;big data&rdquo; techniques to analyze those observations. &nbsp; We developed a system to automatically construct a model of request execution from pre-existing component logs by generating a large number of potential hypotheses about program behavior and rejecting hypotheses contradicted by the empirical observations. &nbsp;Our system is also able to validate potential performance improvements without costly implementation effort by leveraging the variation in component behavior that arises naturally over large numbers of requests to measure the impact of optimizing individual components or changing scheduling behavior.&nbsp; </span></p> <p><span>I<span>n collaboration with Facebook engineers, we integrated telescopic analysis into the company's existing logging infrastructure. &nbsp;As a result of this work, we were able to analyze performance traces of over 1.3 million requests to Facebook servers. &nbsp;We generated a detailed study of the factors that affect the end-to-end latency of such requests and used telescopic analysis to suggest and validate a scheduling optimization for improving Facebook request latency. &nbsp;We published the results of our study so that the broader computer science community could benefit from our insights.&nbsp; Through this technology transfer, telescopic analysis is now continuously used by thousands of Facebook software engineers as part of their Canopy and Kraken systems.</span></span></p> <p>Another major outcome of the project was the invesitgation of data-quality tradeoffs, which we define to be explicit decisions to return lower-fidelity data in order to improve response time or minimize resource usage. &nbsp;These tradeoffs arise because modern Internet services often involve hundreds of distinct software components cooperating to handle a single user request. &nbsp;Each component must balance the competing goals of minimizing service response time and maximizing the quality of the service provided.</p> <p>We built a system, called DQBarge, that enables better data-quality tradeoffs by propagating critical information along the causal path of request processing. &nbsp;This information includes data provenance, load metrics, and request critical path predictions. &nbsp;DBarge generates performance and quality models that help low-level components make better, more proactive, tradeoffs.&nbsp;&nbsp;</p> <p>Our next major outcome was showing how telescopic analysis can enable better understanding of the data and control flow of software components in distributed systems. &nbsp;Our JetStream system parallelizes information flow queries across a compute cluster. &nbsp;Parallelization of information flow tracking was a previously unsolved problem due to the difficulty of handling billions of fine-grained dependencies in program execution. &nbsp;JetStream uses two techniques to enable efficient parallelization. &nbsp;First, it uses time-slicing of recorded program executions to parallelize program instrumentation and data gathering. &nbsp;Second, it uses stream-style processing of the distributed graph of causal dependencies to parallelize the work of aggregation and output generation.</p> <p>JetStream is a fundamental building block for making telescopic analysis interactive. &nbsp;We have used JetStream to build debugging and troubleshooting tools that allow complex analyses of distributed systems to complete in a few seconds rather than minutes or hours.&nbsp; The insight in this work is that the execution of a program can be regarded as a queryable object.&nbsp; &nbsp;We can then define relations over the execution that can be evaluated at granularities as small as individual instructions.&nbsp; These relations produce a tremendous amount of data during program execution, so evaluating them requires "big-data" style scalable analysis techniques and a significant amount of optimization.&nbsp; One relation we developed for debugging is termed a&nbsp;<em>continuous function invariant</em>.&nbsp; The abstraction lets developers define a function over the state of their execution that is logically evaluated after every instruction.&nbsp;&nbsp;</p> <p>The scientific results of the project have also been incorporated into course modules used in the undergraduate operating systems and web systems courses.&nbsp; We have used the results of telescopic analysis to give students insight into the performance of Internet-scale systems, using actual results from measuring millions of requests at Facebook as an example of such systems.</p><br> <p>            Last Modified: 09/16/2018<br>      Modified by: Jason&nbsp;N&nbsp;Flinn</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed new scalable mechanisms, termed telescopic analysis, to debug, monitor, and assess the quality of complex distributed systems.  Telescopic analysis is designed to be highly-automated; it reasons about the operation of distributed systems while treating the individual components of the systems as black boxes.  The techniques that we created combine the ability to collect extremely detailed, low-level information about systems executing large numbers of request with "big data" analysis that mines for insights and creates models of system operation from the corpus of detailed observations.  The outcomes of the project have substantially improved the state-of-the-art for monitoring and debugging Internet-scale distributed systems that form the backbone of our nation's computing infrastrucutre.  Our results have been disseminated through publication at top peer-reviewed forums, through open-source release of source code, and through technology transfer to industry.  Our first major outcome derived from applying telescopic analysis to performance debugging of large-scale Internet pipelines.  We observed that the scale of such systems means that even at low sampling rates, we can gather a tremendous amount of empirical performance observations through low-level request logging.  We can then apply "big data" techniques to analyze those observations.   We developed a system to automatically construct a model of request execution from pre-existing component logs by generating a large number of potential hypotheses about program behavior and rejecting hypotheses contradicted by the empirical observations.  Our system is also able to validate potential performance improvements without costly implementation effort by leveraging the variation in component behavior that arises naturally over large numbers of requests to measure the impact of optimizing individual components or changing scheduling behavior.    In collaboration with Facebook engineers, we integrated telescopic analysis into the company's existing logging infrastructure.  As a result of this work, we were able to analyze performance traces of over 1.3 million requests to Facebook servers.  We generated a detailed study of the factors that affect the end-to-end latency of such requests and used telescopic analysis to suggest and validate a scheduling optimization for improving Facebook request latency.  We published the results of our study so that the broader computer science community could benefit from our insights.  Through this technology transfer, telescopic analysis is now continuously used by thousands of Facebook software engineers as part of their Canopy and Kraken systems.  Another major outcome of the project was the invesitgation of data-quality tradeoffs, which we define to be explicit decisions to return lower-fidelity data in order to improve response time or minimize resource usage.  These tradeoffs arise because modern Internet services often involve hundreds of distinct software components cooperating to handle a single user request.  Each component must balance the competing goals of minimizing service response time and maximizing the quality of the service provided.  We built a system, called DQBarge, that enables better data-quality tradeoffs by propagating critical information along the causal path of request processing.  This information includes data provenance, load metrics, and request critical path predictions.  DBarge generates performance and quality models that help low-level components make better, more proactive, tradeoffs.    Our next major outcome was showing how telescopic analysis can enable better understanding of the data and control flow of software components in distributed systems.  Our JetStream system parallelizes information flow queries across a compute cluster.  Parallelization of information flow tracking was a previously unsolved problem due to the difficulty of handling billions of fine-grained dependencies in program execution.  JetStream uses two techniques to enable efficient parallelization.  First, it uses time-slicing of recorded program executions to parallelize program instrumentation and data gathering.  Second, it uses stream-style processing of the distributed graph of causal dependencies to parallelize the work of aggregation and output generation.  JetStream is a fundamental building block for making telescopic analysis interactive.  We have used JetStream to build debugging and troubleshooting tools that allow complex analyses of distributed systems to complete in a few seconds rather than minutes or hours.  The insight in this work is that the execution of a program can be regarded as a queryable object.   We can then define relations over the execution that can be evaluated at granularities as small as individual instructions.  These relations produce a tremendous amount of data during program execution, so evaluating them requires "big-data" style scalable analysis techniques and a significant amount of optimization.  One relation we developed for debugging is termed a continuous function invariant.  The abstraction lets developers define a function over the state of their execution that is logically evaluated after every instruction.    The scientific results of the project have also been incorporated into course modules used in the undergraduate operating systems and web systems courses.  We have used the results of telescopic analysis to give students insight into the performance of Internet-scale systems, using actual results from measuring millions of requests at Facebook as an example of such systems.       Last Modified: 09/16/2018       Submitted by: Jason N Flinn]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
