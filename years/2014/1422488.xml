<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Database Algorithms for Modern CPU Memory Hierarchies</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>499998.00</AwardTotalIntnAmount>
<AwardAmount>499998</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A modern database server typically runs on a cluster of machines with a large amount of RAM, to ensure fast query responses when the commonly accessed data (or even the whole database) can fit in main memory. Data may reside in remote or local memory, or in one of several levels of cache; each type of memory has its own characteristic size and performance properties.  This project will develop query processing algorithms and a query processing system tailored to such memory hierarchies. Analytic database systems have emerged as a key technology for extracting actionable information from "big data" collections ranging from medicine to science to business. Improvements in the performance of core database operations achieved by the project will have impact on many application domains. The project will also contribute to education by contributing technology for use in teaching database system implementation techniques.&lt;br/&gt;&lt;br/&gt;The project will develop a database architecture with efficient partitioning as the core operation, using different partitioning techniques at each level of the memory hierarchy. In-place partitioning will be employed where appropriate to avoid allocating extra memory, with only a small decrease in partitioning speed. Partitioning will be used as a building block for other operators such as sorting, joins and aggregation. A variety of database algorithms will be implemented, taking advantage of the efficiency of partitioning to perform well at scale.  Network transfer volumes will be reduced using specialized join algorithms, since such transfers are likely to be the bottleneck for queries involving large distributed joins.  These algorithms will form the basis of a database system prototype to be developed during the course of the project. The project will provide new techniques to exploit modern machines for efficient analytic query processing. The proposed system will significantly improve the throughput of data-intensive queries.&lt;br/&gt;&lt;br/&gt;For further information see the project web site at:  http://www.cs.columbia.edu/~kar/cpumem.html</AbstractNarration>
<MinAmdLetterDate>08/05/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1422488</AwardID>
<Investigator>
<FirstName>Kenneth</FirstName>
<LastName>Ross</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kenneth A Ross</PI_FULL_NAME>
<EmailAddress>kar@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397058</PI_PHON>
<NSF_ID>000446976</NSF_ID>
<StartDate>08/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~499998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern computer processors (CPUs) include instructions to enhance performance by processing multiple data items at once using a single instruction.&nbsp; Current single-instruction mulitple data (SIMD) instructions can process up to 512 bits of data at once.&nbsp; For some computations used in high performance database management systems, efficient SIMD algorithms were not known, particularly for instruction sets that include advanced scatter and gather operations.</p> <p>Our research has examined fundamental operators such as selections, hash tables, and partitioning, and combined them to implement more complex operators such as sorting and joins. Our evaluation shows that our SIMD designs improved the state-of-the-art scalar and SIMD approaches, in some cases by an order of magnitude. Efficient use of SIMD instructions also impacts the algorithmic design of in-memory database operators, as well as the architectural design and power efficiency of hardware, by making simple cores with SIMD instructions comparably fast to complex CPU cores.&nbsp;</p> <p>Recent industry trends have led to an increasing main-memory capacity, allowing query execution to occur primarily in main memory. Database systems employ compression, not only to fit the data in main memory, but also to address the memory bandwidth bottleneck. Lightweight compression schemes focus on efficiency over compression rate and allow query operators to process the data in compressed form. For instance, dictionary compression keeps the distinct column values in a sorted dictionary and stores the values as index codes with the minimum number of bits. Packing the bits of each code contiguously, namely horizontal bit packing, has been optimized by using SIMD instructions for unpacking and by evaluating predicates in parallel per processor word for selection scans. Interleaving the bits of codes, namely vertical bit packing, provides faster scans, but incurs prohibitive costs for packing and unpacking. Our work has improved packing and unpacking for vertical bit packing using SIMD instructions, achieving more than an order of magnitude speedup relative to prior work. While no single variant is better in all cases, vertical bit packing offers a good trade-off by combining the fastest scans with comparably fast packing and unpacking.</p> <p><span>Searching for patterns in string databases often involves specifying patterns using regular expressions. We derived new techniques for processing regular expression queries on SIMD processors. Using scatter/gather operations, we&nbsp; use one SIMD lane per string, and terminate a particular lane early if the string is known to match (or to not match) without further processing. This is a significant improvement on past SIMD algorithms for regular expression matching that operated in lockstep and always processed full strings in each lane.</span></p> <p><span>When querying a distributed database, data transmission between nodes can be the performance bottleneck. Prior algorithms such as track join exploit existing locality of the data to minimize the volume of data shipped between sites. We devised methods to distribute the data among many sites in such a way that track join finds the right kind of locality that it can exploit. When tables are involved in multiple joins, there may be many (potentially conflicting) ways to partition or otherwise co-locate data. Because track join computes an optimal schedule for each join key, we have more flexibility in placement than just simple partitioning strategies. Motivated by this observation, we developed algorihms that perform a key-by-key analysis of the data&nbsp;(for multiple joins in a workload) to derive good placements. We performed extensive experiments that showed significant improvements could be achieved by placing data well.</span></p> <p><span>One approach to obtaining improved time and energy performance for database workloads is to design specialized accelerators tuned to the specific needs of database management.&nbsp;<span>Previous database accelerator proposals provide a fixed set of database operators, chosen to support a target query workload. Some queries may not be well-supported by a fixed accelerator, typically because they need more resources/operators of a particular kind than the accelerator provides. By Amdahl's law, these queries become relatively more expensive as they are not fully accelerated. We propose a second-level accelerator, DB-Mesh, to take up some of this workload. DB-Mesh is an asynchronous systolic array that can be configured to run a variety of operators with configurable parameters such as record widths. While a naive implementation has the potential for deadlock, our work shows how to avoid deadlock with a careful design.</span></span></p> <p><span><span><br /></span></span></p><br> <p>            Last Modified: 12/10/2018<br>      Modified by: Kenneth&nbsp;A&nbsp;Ross</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern computer processors (CPUs) include instructions to enhance performance by processing multiple data items at once using a single instruction.  Current single-instruction mulitple data (SIMD) instructions can process up to 512 bits of data at once.  For some computations used in high performance database management systems, efficient SIMD algorithms were not known, particularly for instruction sets that include advanced scatter and gather operations.  Our research has examined fundamental operators such as selections, hash tables, and partitioning, and combined them to implement more complex operators such as sorting and joins. Our evaluation shows that our SIMD designs improved the state-of-the-art scalar and SIMD approaches, in some cases by an order of magnitude. Efficient use of SIMD instructions also impacts the algorithmic design of in-memory database operators, as well as the architectural design and power efficiency of hardware, by making simple cores with SIMD instructions comparably fast to complex CPU cores.   Recent industry trends have led to an increasing main-memory capacity, allowing query execution to occur primarily in main memory. Database systems employ compression, not only to fit the data in main memory, but also to address the memory bandwidth bottleneck. Lightweight compression schemes focus on efficiency over compression rate and allow query operators to process the data in compressed form. For instance, dictionary compression keeps the distinct column values in a sorted dictionary and stores the values as index codes with the minimum number of bits. Packing the bits of each code contiguously, namely horizontal bit packing, has been optimized by using SIMD instructions for unpacking and by evaluating predicates in parallel per processor word for selection scans. Interleaving the bits of codes, namely vertical bit packing, provides faster scans, but incurs prohibitive costs for packing and unpacking. Our work has improved packing and unpacking for vertical bit packing using SIMD instructions, achieving more than an order of magnitude speedup relative to prior work. While no single variant is better in all cases, vertical bit packing offers a good trade-off by combining the fastest scans with comparably fast packing and unpacking.  Searching for patterns in string databases often involves specifying patterns using regular expressions. We derived new techniques for processing regular expression queries on SIMD processors. Using scatter/gather operations, we  use one SIMD lane per string, and terminate a particular lane early if the string is known to match (or to not match) without further processing. This is a significant improvement on past SIMD algorithms for regular expression matching that operated in lockstep and always processed full strings in each lane.  When querying a distributed database, data transmission between nodes can be the performance bottleneck. Prior algorithms such as track join exploit existing locality of the data to minimize the volume of data shipped between sites. We devised methods to distribute the data among many sites in such a way that track join finds the right kind of locality that it can exploit. When tables are involved in multiple joins, there may be many (potentially conflicting) ways to partition or otherwise co-locate data. Because track join computes an optimal schedule for each join key, we have more flexibility in placement than just simple partitioning strategies. Motivated by this observation, we developed algorihms that perform a key-by-key analysis of the data (for multiple joins in a workload) to derive good placements. We performed extensive experiments that showed significant improvements could be achieved by placing data well.  One approach to obtaining improved time and energy performance for database workloads is to design specialized accelerators tuned to the specific needs of database management. Previous database accelerator proposals provide a fixed set of database operators, chosen to support a target query workload. Some queries may not be well-supported by a fixed accelerator, typically because they need more resources/operators of a particular kind than the accelerator provides. By Amdahl's law, these queries become relatively more expensive as they are not fully accelerated. We propose a second-level accelerator, DB-Mesh, to take up some of this workload. DB-Mesh is an asynchronous systolic array that can be configured to run a variety of operators with configurable parameters such as record widths. While a naive implementation has the potential for deadlock, our work shows how to avoid deadlock with a careful design.          Last Modified: 12/10/2018       Submitted by: Kenneth A Ross]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
