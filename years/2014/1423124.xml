<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: DBMS+: Management System for the Next-Generation Database</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Aidong Zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A large number of data processing systems---including NoSQL systems, column-stores, MapReduce, data stream managers, complex event processors, in-memory databases, and others---have  been developed in recent years. It can be hard for an application developer to pick one system when the needs of her application match the features of multiple of these systems. The choice becomes considerably harder when different components of an application fit the features of different systems. Considerable manual effort goes into creating and tuning such multi-system applications. Current applications are coupled tightly with individual systems, which makes dealing with changes to application execution requirements or workload characteristics nontrivial. Moreover, an application's data and workload properties may change over time, often in unpredictable and bursty ways. Consequently, the system that is best for an application can change over time. Adapting to change can be hard when application development is coupled tightly with any individual system.&lt;br/&gt;&lt;br/&gt;This project will design, implement, and evaluate a new breed of Database Management Systems that is termed DBMS+. While  conventional Database Management Systems (DBMS) support applications with a single execution engine and a storage engine, a DBMS+ integrates and manages multiple compute and storage systems. An application specifies its execution requirements on aspects like performance, availability, consistency, change, and cost to the DBMS+ declaratively. For all requests (e.g., queries) made by the application, the DBMS+ will select the execution plan that meets the application's requirements best. A novel aspect of the execution plan in a DBMS+ is that the plan includes the selection of one or more specialized systems. The optimizer in the DBMS+ is responsible for determining the most suitable execution plan, which includes choosing the most suitable compute and storage systems for a given application and its requirements. The DBMS+ approach has the potential to address the hurdles that application developers and system administrators face from the vast and growing number of specialized systems.&lt;br/&gt;&lt;br/&gt;For further information, see the project web site at: http://www.cs.duke.edu/starfish/cyclops/</AbstractNarration>
<MinAmdLetterDate>08/13/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1423124</AwardID>
<Investigator>
<FirstName>Jun</FirstName>
<LastName>Yang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jun Yang</PI_FULL_NAME>
<EmailAddress>junyang@cs.duke.edu</EmailAddress>
<PI_PHON>9196606587</PI_PHON>
<NSF_ID>000486379</NSF_ID>
<StartDate>01/13/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shivnath</FirstName>
<LastName>Babu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shivnath Babu</PI_FULL_NAME>
<EmailAddress>shivnath@cs.duke.edu</EmailAddress>
<PI_PHON>9196606579</PI_PHON>
<NSF_ID>000488390</NSF_ID>
<StartDate>08/13/2014</StartDate>
<EndDate>01/12/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName/>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-659914f2-7fff-d29e-ca8e-c760a1b0230a"> </span></p> <p dir="ltr"><span>Recent years have witnessed an explosion in the number of data-processing platforms:&nbsp;</span>Hadoop, Spark, HBase, Cassandra, Flink, Presto, and others. Following the &ldquo;no one size fits all&rdquo; philosophy, active research in these platforms has focused on creating an environment for multiple &ldquo;one-size&rdquo; systems to co-exist and cooperate in the same composite platform called a DBMS+. DBMS+ platforms are built to handle huge datasets by distributing data processing across hundreds of machines in a fault-tolerant manner. These platforms are run in multi-tenant environments where resources are allocated and isolated using containers, e.g., using resource managers like Yarn, Mesos, and Kubernetes. Most DBMS+ platforms run on the Java Virtual Machine (JVM) and are written in JVM-based languages like Java, Scala, and Clojure. In-memory data storage is increasingly the focus in these platforms.</p> <p dir="ltr"><span>Many businesses, governments, as well as scientific researchers are using DBMS+ platforms to process the large datasets that they generate. However, there is limited understanding of how these platforms behave and how to troubleshoot performance and reliability issues for these multi-tenant and composite platforms. The focus of this project was to provide an integrated management solution that provides a database-centric view of the underlying multi-system DBMS+ platform.</span></p> <p dir="ltr"><span>The project has made significant contributions foundationally and empirically along multiple subprojects. The ROBUS subproject developed a memory allocation technique that accounted for the multi-tenant nature of a DBMS+. ROBUS developed a cache allocation strategy that, while speeding up the overall data analytics workload in a DBMS+, also guarantees fairness in the speedups experienced by individual tenants of the DBMS+. The Memory Tuner and RelM subprojects studied the impact of memory allocation and usage across the various levels of the software stack in a DBMS+, namely, across the application, the resource manager like Yarn, the platform like Spark, and the JVM. Optimally sizing memory pools at these levels is important to meet desired performance requirements. The Thoth subproject developed a smart DBMS+ platform that applies all the memory-based algorithms to the same workload. Thoth was released as open source software. The MIFO subproject developed scheduling policies to provide a balanced optimization of FAIR and FIFO goals based on the multi-tenancy requirements. The Sniffer subproject helped database administrators narrow down the many possibilities of query performance degradation in a DBMS+ using a modeling of resource conflicts among concurrent queries. &nbsp;&nbsp;</span></p> <p dir="ltr"><span> </span></p> <p dir="ltr"><span>The strength of these contributions comes from the wide spectrum along which big data platform workloads were explored: (a) Scale of processing: where clusters of many different sizes were studied to understand how these workloads work at different types of scale; (b) Diversity of processing: where workloads of different types ranging from SQL to matrix processing were studied; (c) Performance requirements: where workloads being used for batch processing, real-time stream processing, graph processing, etc., were studied; and (d) Infrastructure used: where workloads on the elastic cloud as well as multi-tenant on-premises clusters were studied.</span></p> <p dir="ltr"><span>This project has generated operational insights from real-life clusters used by analysts, data scientists, and statisticians working on different mission-critical activities at a large enterprise. Operational experiences from such large multi-tenant platforms were used to showcase challenges that database administrators of DBMS+ platforms face, and how the work from this project benefits them. Multiple demonstrations of the contributions of this project have been given in venues like VLDB. Talks at research conferences like SIGMOD as well as at popular venues like the Hadoop and Spark Summit have been given. The project has also contributed to two PhD dissertations.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 12/08/2018<br>      Modified by: Jun&nbsp;Yang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Recent years have witnessed an explosion in the number of data-processing platforms: Hadoop, Spark, HBase, Cassandra, Flink, Presto, and others. Following the "no one size fits all" philosophy, active research in these platforms has focused on creating an environment for multiple "one-size" systems to co-exist and cooperate in the same composite platform called a DBMS+. DBMS+ platforms are built to handle huge datasets by distributing data processing across hundreds of machines in a fault-tolerant manner. These platforms are run in multi-tenant environments where resources are allocated and isolated using containers, e.g., using resource managers like Yarn, Mesos, and Kubernetes. Most DBMS+ platforms run on the Java Virtual Machine (JVM) and are written in JVM-based languages like Java, Scala, and Clojure. In-memory data storage is increasingly the focus in these platforms. Many businesses, governments, as well as scientific researchers are using DBMS+ platforms to process the large datasets that they generate. However, there is limited understanding of how these platforms behave and how to troubleshoot performance and reliability issues for these multi-tenant and composite platforms. The focus of this project was to provide an integrated management solution that provides a database-centric view of the underlying multi-system DBMS+ platform. The project has made significant contributions foundationally and empirically along multiple subprojects. The ROBUS subproject developed a memory allocation technique that accounted for the multi-tenant nature of a DBMS+. ROBUS developed a cache allocation strategy that, while speeding up the overall data analytics workload in a DBMS+, also guarantees fairness in the speedups experienced by individual tenants of the DBMS+. The Memory Tuner and RelM subprojects studied the impact of memory allocation and usage across the various levels of the software stack in a DBMS+, namely, across the application, the resource manager like Yarn, the platform like Spark, and the JVM. Optimally sizing memory pools at these levels is important to meet desired performance requirements. The Thoth subproject developed a smart DBMS+ platform that applies all the memory-based algorithms to the same workload. Thoth was released as open source software. The MIFO subproject developed scheduling policies to provide a balanced optimization of FAIR and FIFO goals based on the multi-tenancy requirements. The Sniffer subproject helped database administrators narrow down the many possibilities of query performance degradation in a DBMS+ using a modeling of resource conflicts among concurrent queries.      The strength of these contributions comes from the wide spectrum along which big data platform workloads were explored: (a) Scale of processing: where clusters of many different sizes were studied to understand how these workloads work at different types of scale; (b) Diversity of processing: where workloads of different types ranging from SQL to matrix processing were studied; (c) Performance requirements: where workloads being used for batch processing, real-time stream processing, graph processing, etc., were studied; and (d) Infrastructure used: where workloads on the elastic cloud as well as multi-tenant on-premises clusters were studied. This project has generated operational insights from real-life clusters used by analysts, data scientists, and statisticians working on different mission-critical activities at a large enterprise. Operational experiences from such large multi-tenant platforms were used to showcase challenges that database administrators of DBMS+ platforms face, and how the work from this project benefits them. Multiple demonstrations of the contributions of this project have been given in venues like VLDB. Talks at research conferences like SIGMOD as well as at popular venues like the Hadoop and Spark Summit have been given. The project has also contributed to two PhD dissertations.          Last Modified: 12/08/2018       Submitted by: Jun Yang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
