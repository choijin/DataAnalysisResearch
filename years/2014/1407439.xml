<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative research: Statistical and computational efficiency for massive datasets via approximation-regularization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>89911.00</AwardTotalIntnAmount>
<AwardAmount>89911</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project integrates approximation methodology from computer science with modern statistical theory to improve analysis of large data sets. Modern statistical analysis requires methods that are computationally feasible on large datasets while at the same time preserving statistical efficiency. Frequently, these two concerns are seen as contradictory: approximation methods that enable computation are assumed to degrade statistical performance relative to exact methods. The statistical perspective is that the exact solution is undesirable, and a regularized solution is preferred. Regularization can be thought of as formalizing a trade-off between fidelity to the data and adherence to prior knowledge about the data-generating process such as smoothness or sparsity. The resulting estimator tends to be more useful, interpretable, and suitable as an input to other methods. Conversely, in computer science applications, where much of the current work on approximation methods resides, the inputs are generally considered to be observed exactly. The prevailing philosophy is that while the exact problem is, regrettably, unsolvable, any approximate solution should be as close as possible to the exact one. We make a crucial realization: that the approximation methods themselves naturally lead to regularization, suggesting the intriguing possibility that some computational approximations can simultaneously enable the analysis of massive data while enhancing statistical performance. &lt;br/&gt;&lt;br/&gt;Our research develops new methods that leverage this phenomenon, which we have dubbed 'approximation-regularization.' The first method uses a matrix pre-conditioner to stabilize the least-squares criterion. If properly calibrated, this approach provides computational and storage advantages over regularized least squares while providing a statistically superior solution. A second innovation addresses principal components analysis (PCA) for regression on large data sets where PCA is both computationally infeasible and known to be statistically inconsistent. By employing randomized approximations, we can address both of these issues, while improving predictions at the same time. Lastly, we introduce new methods for unsupervised dimension reduction, whereby approximation algorithms that leverage sparsity, and statistical methods that induce it, enable the use of spectral techniques on very large matrices. In each of these cases, approximation-regularization yields both computational and statistical gains relative to existing methodologies. This research recognizes that approximation is regularization and can thereby increase statistical accuracy while enabling computation. It will result in new statistical methods for large datasets, which are computationally and statistically preferable to existing approaches, while also bringing attention to this important area in statistics. Additionally, these methods will permit scientists in other fields, such as astronomy, genetics, text and image processing, climate science, and forecasting, to make ready use of available data.</AbstractNarration>
<MinAmdLetterDate>08/20/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1407439</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>McDonald</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel J McDonald</PI_FULL_NAME>
<EmailAddress>dajmcdon@gmail.com</EmailAddress>
<PI_PHON>8128557828</PI_PHON>
<NSF_ID>000656759</NSF_ID>
<StartDate>08/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474083825</ZipCode>
<StreetAddress><![CDATA[309 N. Park Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~89911</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project introduces and publicizes the idea that computational approximations need not necessarily degrade statistical performance. This concept, which we dub "approximation-&#8203;regularization", contradicts the prevailing wisdom which holds that approximations require trading decreased statistical performance for improved computational/storage complexity. We have demonstrated, in a number of related large-data and high-dimensional settings, that computational approximations can occasionally lead to improved performance. This improvement occurs because most real datasets contain outliers and other departures from standard assumptions. Computational approximations applied in these scenarios will tend to compensate for such departures, and, hence, statistical efficiency can actually be improved, leading to more accurate estimators of the true data-generating process. In more standard scenarios, the loss in statistical efficiency can be minimized (though not eliminated) by using specially tailored procedures we developed.&nbsp;</p> <p><br />Our methodological contributions have enhanced scientific understanding in a number of fields. Our work develops and justifies statistical/computational methods and applies them to problems in genetics, atmospheric science, astronomy, chemistry, and music. For example, an application in genetics examined a typical study of 240 patients with diffuse large B-cell lymphoma. Our methods identified twenty-six genes as being predictive of the disease. Of these twenty-six, twenty-two had been previously identified as important for predicting lymphoma (sixteen in the biology literature and nineteen using statistical methods). The other four are novel, and warrant further investigation. In our atmospheric sciences work, we find changes over time in the volatility of temperature fluctuations in global-scale climate data in contrast to existing work in the field.</p> <p><br />This project provided support for undergraduate and graduate student participation in reading groups and thesis projects. It also created publicly available software and course materials which implement and discuss our methods. These are accessible from the PI's website.</p><br> <p>            Last Modified: 10/24/2018<br>      Modified by: Daniel&nbsp;J&nbsp;Mcdonald</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project introduces and publicizes the idea that computational approximations need not necessarily degrade statistical performance. This concept, which we dub "approximation-&#8203;regularization", contradicts the prevailing wisdom which holds that approximations require trading decreased statistical performance for improved computational/storage complexity. We have demonstrated, in a number of related large-data and high-dimensional settings, that computational approximations can occasionally lead to improved performance. This improvement occurs because most real datasets contain outliers and other departures from standard assumptions. Computational approximations applied in these scenarios will tend to compensate for such departures, and, hence, statistical efficiency can actually be improved, leading to more accurate estimators of the true data-generating process. In more standard scenarios, the loss in statistical efficiency can be minimized (though not eliminated) by using specially tailored procedures we developed.    Our methodological contributions have enhanced scientific understanding in a number of fields. Our work develops and justifies statistical/computational methods and applies them to problems in genetics, atmospheric science, astronomy, chemistry, and music. For example, an application in genetics examined a typical study of 240 patients with diffuse large B-cell lymphoma. Our methods identified twenty-six genes as being predictive of the disease. Of these twenty-six, twenty-two had been previously identified as important for predicting lymphoma (sixteen in the biology literature and nineteen using statistical methods). The other four are novel, and warrant further investigation. In our atmospheric sciences work, we find changes over time in the volatility of temperature fluctuations in global-scale climate data in contrast to existing work in the field.   This project provided support for undergraduate and graduate student participation in reading groups and thesis projects. It also created publicly available software and course materials which implement and discuss our methods. These are accessible from the PI's website.       Last Modified: 10/24/2018       Submitted by: Daniel J Mcdonald]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
