<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Example-based Audio Editing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Contemporary users of technology interact with photos and video by editing them, but still use audio only passively, by capturing, storing, transmitting, and playing it back. These two different ways of interacting with contemporary media persist because current software tools make it very difficult for general users to manipulate audio. This project will develop novel technologies that will make audio editing and manipulation accessible to non-experts. These tools will allow a user to guide the software with audio editing requests by vocalizing the desired edits, providing before/after examples of the desired effects, or by presenting other recordings that exhibit the desired audio manipulations. For example, a user might issue a command to the software to equalize sounds by using a booming voice for more bass, or a nasal tone for middle frequencies; to add echoes by mimicking the desired effect by uttering "hello, hello, hello ..." with each successive "hello" in a lower volume; or to add reverb by providing examples of recordings with the desired reverb. Making it easier for general computer users to manipulate and edit audio recordings can impact many fields, such as medical bioacoustics, seismic signal analysis, underwater monitoring, audio forensics, surveillance applications, oil exploration probing, conversational data gathering, and mechanical vibration measuring. The goals of this project are to provide novel and practical audio tools that will allow non-expert practitioners from these fields to easily achieve required audio manipulations.&lt;br/&gt;&lt;br/&gt;The project will exploit modern signal processing and machine learning techniques to produce more intuitive interfaces that help people accomplish what are currently difficult audio editing tasks. This will include developing novel estimators to extract editing-intent parameters directly from audio recordings. The project will focus on three different editing operations: equalization, noise control, and echo/reverberation. A number of different approaches will be explored for each operation. For example, for equalization, one approach will have users select before and after sounds to identify their desired modification, and the system will then use spectral deconvolution estimations to directly compute the transfer function that maps the spectrum of the before sound to that of the after sound, and apply that function to the audio recording that the user is editing. For noise control, one approach will have users vocalize what types of noise to remove, and then match the user's input with the corresponding component in the recording that is being edited by using low-rank spectral decomposition. For reverb and echo, one approach will have users utter "one, two, three, ..." to illustrate the desired number of repetitions, temporal spacing, and attenuation between echoes, and then use voice detection measurements to extract the echo parameters, while correcting for vocalization errors such as random inconsistency in the echo spacing. The project will create new theories of how human guidance and automated audio-intelligent processing can work in tandem to solve fundamental and practical problems.</AbstractNarration>
<MinAmdLetterDate>08/21/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/21/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1451380</AwardID>
<Investigator>
<FirstName>Paris</FirstName>
<LastName>Smaragdis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paris Smaragdis</PI_FULL_NAME>
<EmailAddress>paris@illinois.edu</EmailAddress>
<PI_PHON>2172656893</PI_PHON>
<NSF_ID>000573387</NSF_ID>
<StartDate>08/21/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Making an audio or speech recording sound good, or even ok, is not easy. From YouTube bloggers, to politicians speaking on stage, to local TV stations, we often hear poor quality audio that is either hard to parse, or just bad enough to tip us to change the channel.</span></p> <p><span>&nbsp;</span></p> <p><span>Being able to make clean and appealing audio recordings is key to good communication, especially today where anyone can produce content and instantly share it globally. Being able to produce appealing recordings can allow scientists to better disseminate their results, journalists to maintain an interested audience, budding artists and media companies to get started, and of course to serve the massive amount of people who share their thoughts online, whether that involves teaching courses, talking to family, or producing content for massive consumption.</span></p> <p><span>&nbsp;</span></p> <p><span>Making a recording of palatable quality requires a strong background in recording technology, an experienced ear, and plenty of time. Unfortunately, these are luxuries that are available only to high-end productions, leaving the rest of us to struggle trying to obtain the sound we want.</span></p> <p><span>&nbsp;</span></p> <p><span>In this project we examined alternatives to existing audio recording technology that can result in increased audio recording quality without necessitating that a user is a recording expert. Our approach&nbsp; uses recording examples that the user can use to point an intelligent system towards how they want their recording to sound.</span></p> <p><span>&nbsp;</span></p> <p><span>For example, if a voice recording sounds too nasal, the user can point our system to a recording of James Earl Jones, and have it automatically equalized to match his boomy voice. Providing an example of a richly reverberant recording from a cathedral, would make the recording have the same lush quality. So instead of requiring that a user has the expertise to properly position a microphone, equalize a recording, and properly adjust to account for noise and echoes, we allow the user to provide examples of the desired sound quality and automatically match them.</span></p> <p><span>&nbsp;</span></p> <p><span>We show that this idea carries out to various types of operations. We can use examples to apply global effects, e.g. remove or match noise and reverberation and match the equalization of a recording; or we can use this approach for local editing. We have successfully shown how a user can easily replace a small mistake in a long recording by simply speaking the sentence they want fixed, and our system automatically matches to the right part and replaces it.</span></p> <p><span>&nbsp;</span></p> <p><span>In the process of designing these features we also needed to characterize multiple recording quality attributes. For the final year of this project we developed a deep learning algorithm that can characterize various attributes of a recording and provide real-time feedback to the user. This allows a user to quickly act to address problems in a recording. For example, the system can detect if the user is too far from a microphone and prompt him/her to move closer.</span></p> <p><span>&nbsp;</span></p> <p><span>What we have shown in this project is that there is a lot of room to rethink how users can make recordings, and bypass the need for technical expertise. Just as cameras are becoming smart enough to take good pictures even when misused by novices, we have shown that we can do the same thing with audio recordings and provided a glimpse of new ways to design audio recording systems. Thanks for a fruitful collaboration with industry partners we are expecting to see some of this technology become broadly available within the next couple of years.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 09/15/2018<br>      Modified by: Paris&nbsp;Smaragdis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Making an audio or speech recording sound good, or even ok, is not easy. From YouTube bloggers, to politicians speaking on stage, to local TV stations, we often hear poor quality audio that is either hard to parse, or just bad enough to tip us to change the channel.     Being able to make clean and appealing audio recordings is key to good communication, especially today where anyone can produce content and instantly share it globally. Being able to produce appealing recordings can allow scientists to better disseminate their results, journalists to maintain an interested audience, budding artists and media companies to get started, and of course to serve the massive amount of people who share their thoughts online, whether that involves teaching courses, talking to family, or producing content for massive consumption.     Making a recording of palatable quality requires a strong background in recording technology, an experienced ear, and plenty of time. Unfortunately, these are luxuries that are available only to high-end productions, leaving the rest of us to struggle trying to obtain the sound we want.     In this project we examined alternatives to existing audio recording technology that can result in increased audio recording quality without necessitating that a user is a recording expert. Our approach  uses recording examples that the user can use to point an intelligent system towards how they want their recording to sound.     For example, if a voice recording sounds too nasal, the user can point our system to a recording of James Earl Jones, and have it automatically equalized to match his boomy voice. Providing an example of a richly reverberant recording from a cathedral, would make the recording have the same lush quality. So instead of requiring that a user has the expertise to properly position a microphone, equalize a recording, and properly adjust to account for noise and echoes, we allow the user to provide examples of the desired sound quality and automatically match them.     We show that this idea carries out to various types of operations. We can use examples to apply global effects, e.g. remove or match noise and reverberation and match the equalization of a recording; or we can use this approach for local editing. We have successfully shown how a user can easily replace a small mistake in a long recording by simply speaking the sentence they want fixed, and our system automatically matches to the right part and replaces it.     In the process of designing these features we also needed to characterize multiple recording quality attributes. For the final year of this project we developed a deep learning algorithm that can characterize various attributes of a recording and provide real-time feedback to the user. This allows a user to quickly act to address problems in a recording. For example, the system can detect if the user is too far from a microphone and prompt him/her to move closer.     What we have shown in this project is that there is a lot of room to rethink how users can make recordings, and bypass the need for technical expertise. Just as cameras are becoming smart enough to take good pictures even when misused by novices, we have shown that we can do the same thing with audio recordings and provided a glimpse of new ways to design audio recording systems. Thanks for a fruitful collaboration with industry partners we are expecting to see some of this technology become broadly available within the next couple of years.          Last Modified: 09/15/2018       Submitted by: Paris Smaragdis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
