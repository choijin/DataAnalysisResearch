<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Chameleon: A Large-Scale, Reconfigurable Experimental Environment for Cloud Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>600000.00</AwardTotalIntnAmount>
<AwardAmount>600000</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Brassil</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A persistent problem facing academic cloud research is the lack of infrastructure and data to perform experimental research: large-scale hardware is needed to investigate the scalability of cloud infrastructure and applications, heterogeneous hardware is needed to investigate algorithmic and implementation tradeoffs, fully-configurable software environments are needed to investigate the performance of virtualization techniques and the differences between cloud software stacks, and data about how clouds are used is needed to evaluate virtual machine scheduling and data placement algorithms. &lt;br/&gt;&lt;br/&gt;The Chameleon project will addresses these needs by providing a large-scale, fully configurable experimental testbed driven by the needs of the cloud research and education communities. The testbed, and the ecosystem associated with it, will enable researchers to explore a range of cloud research challenges, from large scale to small scale, including exploring low-level problems in hardware architecture, systems research, network configuration, and software design, or at higher levels of abstraction looking at cloud scheduling, cloud platforms, and cloud applications. &lt;br/&gt;&lt;br/&gt;Chameleon will significantly enhance the ability of the computing research community to understand the behavior of Internet scale cloud systems, and to develop new software, ideas and algorithms for the cloud environment. As the tremendous shift to cloud as the primary means of providing computing infrastructure continues, a large-scale testbed tailored to researchers' needs is essential to the continued relevance of a large fraction of computing research. &lt;br/&gt;&lt;br/&gt;The project is led by the University of Chicago and includes partners from the Texas Advanced Computing Center (TACC), Northwestern University, the Ohio State University, and the University of Texas at San Antonio, comprising a highly qualified and experienced team, with research leaders from the cloud and networking world blended with providers of production quality cyberinfrastructure.  The team includes members from the NSF-supported FutureGrid project and from the GENI community, both forerunners of the NSFCloud solicitation under which this project is funded. &lt;br/&gt;&lt;br/&gt;The Chameleon testbed, will be deployed at the University of Chicago (UC) and the Texas Advanced Computing Center (TACC) and will consist of 650 multi-core cloud nodes, 5PB of total disk space, and leverage 100 Gbps connection between the sites. While a large part of the testbed will consist of homogenous hardware to support large-scale experiments, a portion of it will support heterogeneous units allowing experimentation with high-memory, large-disk, low-power, GPU, and co-processor units. The project will also leverage existing FutureGrid hardware at UC and TACC in its first year to provide a transition period for the existing FutureGrid community of experimental users. &lt;br/&gt;&lt;br/&gt;To support a broad range of experiments emphasizing a range of requirements ranging from a high degree of control to ease of use the project will support a graduated configuration system allowing full user configurability of the stack, from provisioning of bare metal and network interconnects to delivery of fully functioning cloud environments. In addition, to facilitate experiments, Chameleon will support a set of services designed to meet researchers needs, including support for experimental management, reproducibility, and repositories of trace and workload data of production cloud workloads. &lt;br/&gt;&lt;br/&gt;To facilitate the latter, the project will form a set of partnerships with commercial as well as academic clouds, such as Rackspace and Open Science Data Cloud (OSDC). It will also partner with other testbeds, notably GENI and INRIA's Grid5000 testbed, and reach out to the user community to shape the policy an direction of the testbed. &lt;br/&gt;&lt;br/&gt;The Chameleon project will bring a new dimension and scale of resources to the CS community who wish to educate their students about design, implementation, operation and applications of cloud computing, a critical skillset for future computing professionals. It will enhance the understanding and application of experimental methodology in computer science and generate new educational materials and resources, with the participation of, and for, Minority Serving Institution (MSI) students.</AbstractNarration>
<MinAmdLetterDate>08/19/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/21/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1419123</AwardID>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dhabaleswar K Panda</PI_FULL_NAME>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142925199</PI_PHON>
<NSF_ID>000487085</NSF_ID>
<StartDate>08/19/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName/>
<StateCode>OH</StateCode>
<ZipCode>432101063</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>2890</Code>
<Text>CISE Research Resources</Text>
</ProgramElement>
<ProgramReference>
<Code>7363</Code>
<Text>RES IN NETWORKING TECH &amp; SYS</Text>
</ProgramReference>
<ProgramReference>
<Code>8002</Code>
<Text>CISE Research Resources</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~200000</FUND_OBLG>
<FUND_OBLG>2015~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern cloud computing infrastructures with multi-core processors,<br />high-performance interconnects/protocols (such as InfiniBand, RDMA<br />over Converged Enhanced Ethernet (RoCE), and 10-Gigabit Ethernet with<br />iWARP) have been widely used in the enterprise computing domain for<br />easy system management and efficient resource utilization.&nbsp; In this<br />project, we have proposed Chameleon, a large-scale, fully<br />configurable, community-driven cloud testbed for the computing<br />research community.&nbsp; Chameleon has covered the full range of cloud<br />research cases, from large scale to small scale, including exploring<br />low-level issues in hardware architecture, network configuration, and<br />system software design, or at higher levels of abstraction looking at<br />cloud scheduling, cloud platforms, and cloud applications. In addition<br />to deploying a large physical testbed, this project surrounds<br />Chameleon with a set of software and services to meet researchers<br />needs, including support for experimental management, reproducibility,<br />and repositories of trace and workload data.<br /><br />Contributions were made in all these areas. Some highlights of these<br />results are:<br /><br />1. The team developed the testbed and it has been operational with<br />very little downtime and continued to provide an experimental platform<br />to our users. The user community doubled in the third year and the<br />number of research projects grew substantially.<br /><br />2. The team has significantly broadened experimental capabilities of<br />the testbed through the addition of new hardware and testbed<br />capabilities and improving the ease of use of the testbed via such<br />capabilities as support for complex appliances.&nbsp; The testbed has 26<br />appliances in the appliance catalog, 13 of which are developed and<br />supported by the Chameleon team. Based on those Chameleon-supported<br />appliances, our users have configured 550 appliances, 96 of which have<br />been made public (i.e., shareable with the user community), and 13 of<br />those have been accepted into the Chameleon catalog (appliances<br />including additional documentation and meta-data such as<br />support information).</p> <p>3. The OSU team members have purposed new designs to solve many<br />critical challenges to support building efficient HPC clouds. They<br />have proposed designs of high-performance MPI library (MVAPICH2) over<br />SR-IOV enabled InfiniBand clusters and developed the associated<br />appliance on Chameleon Cloud.&nbsp; This design can achieve near-native<br />performance for benchmarks and end HPC applications. This design can<br />also support virtual machine live migration.&nbsp; For NAS and P3DFFT, the<br />benefits of the proposed design can be up to 43% and 33%,<br />respectively.&nbsp; Compared with Amazon EC2, the proposed experimental HPC<br />cloud built by MVAPICH2-Virt over OpenStack can deliver up to 12X<br />reduction in application execution time.<br /><br />4. On the Big Data processing direction, the OSU team members have<br />designed high-performance Hadoop library over SR-IOV enabled<br />InfiniBand clusters and developed the associated appliance on<br />Chameleon Cloud.&nbsp; For RDMA-Hadoop designs on clouds, the proposed<br />designs can achieve up to 30.5% and 55.7% improvement for CloudBurst<br />and Self-join, respectively.<br /><br />The results of this research (new designs, performance results,<br />benchmarks, etc.) have been made available to the community through<br />Chameleon, MVAPICH2-Virt, and RDMA-Hadoop project websites.<br /><br />We have seen strong growth in the number of projects and researchers<br />served on Chameleon. To-date, Chameleon has served nearly 2,000 users<br />across 265 research and education projects from 38 NSF Fields of<br />Science with 210 distinct PIs, 116 institutions spanning 31 states,<br />the District of Columbia and Puerto Rico of which 11 are Minority<br />Serving and includes schools from 9 EPSCoR regions.<br /><br />In addition to the software distribution, the results have been<br />presented at various conferences and events through Keynote talks,<br />invited talks, tutorials, and hands-on sessions.&nbsp; Multiple Ph.D. and<br />Masters students have performed research work and received their<br />degrees as a part of this project.</p><br> <p>            Last Modified: 02/26/2018<br>      Modified by: Dhabaleswar&nbsp;K&nbsp;Panda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern cloud computing infrastructures with multi-core processors, high-performance interconnects/protocols (such as InfiniBand, RDMA over Converged Enhanced Ethernet (RoCE), and 10-Gigabit Ethernet with iWARP) have been widely used in the enterprise computing domain for easy system management and efficient resource utilization.  In this project, we have proposed Chameleon, a large-scale, fully configurable, community-driven cloud testbed for the computing research community.  Chameleon has covered the full range of cloud research cases, from large scale to small scale, including exploring low-level issues in hardware architecture, network configuration, and system software design, or at higher levels of abstraction looking at cloud scheduling, cloud platforms, and cloud applications. In addition to deploying a large physical testbed, this project surrounds Chameleon with a set of software and services to meet researchers needs, including support for experimental management, reproducibility, and repositories of trace and workload data.  Contributions were made in all these areas. Some highlights of these results are:  1. The team developed the testbed and it has been operational with very little downtime and continued to provide an experimental platform to our users. The user community doubled in the third year and the number of research projects grew substantially.  2. The team has significantly broadened experimental capabilities of the testbed through the addition of new hardware and testbed capabilities and improving the ease of use of the testbed via such capabilities as support for complex appliances.  The testbed has 26 appliances in the appliance catalog, 13 of which are developed and supported by the Chameleon team. Based on those Chameleon-supported appliances, our users have configured 550 appliances, 96 of which have been made public (i.e., shareable with the user community), and 13 of those have been accepted into the Chameleon catalog (appliances including additional documentation and meta-data such as support information).  3. The OSU team members have purposed new designs to solve many critical challenges to support building efficient HPC clouds. They have proposed designs of high-performance MPI library (MVAPICH2) over SR-IOV enabled InfiniBand clusters and developed the associated appliance on Chameleon Cloud.  This design can achieve near-native performance for benchmarks and end HPC applications. This design can also support virtual machine live migration.  For NAS and P3DFFT, the benefits of the proposed design can be up to 43% and 33%, respectively.  Compared with Amazon EC2, the proposed experimental HPC cloud built by MVAPICH2-Virt over OpenStack can deliver up to 12X reduction in application execution time.  4. On the Big Data processing direction, the OSU team members have designed high-performance Hadoop library over SR-IOV enabled InfiniBand clusters and developed the associated appliance on Chameleon Cloud.  For RDMA-Hadoop designs on clouds, the proposed designs can achieve up to 30.5% and 55.7% improvement for CloudBurst and Self-join, respectively.  The results of this research (new designs, performance results, benchmarks, etc.) have been made available to the community through Chameleon, MVAPICH2-Virt, and RDMA-Hadoop project websites.  We have seen strong growth in the number of projects and researchers served on Chameleon. To-date, Chameleon has served nearly 2,000 users across 265 research and education projects from 38 NSF Fields of Science with 210 distinct PIs, 116 institutions spanning 31 states, the District of Columbia and Puerto Rico of which 11 are Minority Serving and includes schools from 9 EPSCoR regions.  In addition to the software distribution, the results have been presented at various conferences and events through Keynote talks, invited talks, tutorials, and hands-on sessions.  Multiple Ph.D. and Masters students have performed research work and received their degrees as a part of this project.       Last Modified: 02/26/2018       Submitted by: Dhabaleswar K Panda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
