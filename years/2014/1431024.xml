<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase II: Emotionally Immersive Tele-Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>1048057</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Benaiah Schrag</SignBlockName>
<PO_EMAI>bschrag@nsf.gov</PO_EMAI>
<PO_PHON>7032928323</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase II project aims to incorporate novel machine vision and social networking functionality into technologies used in online education and webinars. Researchers have long identified engagement as the key ingredient for success in any learning environment and particularly the online environment, but current online teaching systems lack the means by which instructors can gauge their students' level of engagement because these students are not visible. Therefore in order to improve current online teaching modalities, it is necessary to find ways to communicate to the instructor the level of engagement of their unseen online students. The successful outcome of the project will allow the lecturer to receive real-time feedback from facial expressions, gaze and other body kinesics, which when averaged across the virtual classroom, provides feedback related to the reception of information delivery. The project supports NSF's mission in education, which seeks to answer questions about how teachers can provide effective cognitive and motivational support for students. This project aims to promote richer interactions between tutor and online students through integrated cognitive and motivational scaffolding, leading to higher levels of student success enabling them to compete more effectively as skilled artisans in the 21st century workforce. &lt;br/&gt; &lt;br/&gt; &lt;br/&gt;This project contributes four significant innovations: a machine-vision recognition system for gaze direction and facial expressions of engagement, which aggregates data across participants to improve signal to noise; a machine-vision recognition system for detecting hand gestures and postural kinesics; enabling third party pedagogical applications within a framework using ancillary hardware, and which can be sold through an educational application store; and to integrate social network functionality that replicates pre- and post-lecture socialization including pair sharing, breakout groups, team teaching, and support for online teaching assistance. The goals and scope of research required to support the above innovations include: improving machine-vision classifier data and functionality; optimization of user interface and integrated user calibration process; extending the system's functionality to both synchronous and asynchronous modalities; developing neuro-psychological and cognitive models of the online pedagogical process; design and integration of content/media interoperability and social networking capabilities; creation of open application programming interfaces for third-party developers and application store functionality with digital whiteboards and tablets; development of ancillary software modules to help students manage data related to their educational efforts, diagnose study and achievement patterns, and provide expert advice based on that data.</AbstractNarration>
<MinAmdLetterDate>08/03/2014</MinAmdLetterDate>
<MaxAmdLetterDate>01/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1431024</AwardID>
<Investigator>
<FirstName>Ian</FirstName>
<LastName>Bennett</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ian Bennett</PI_FULL_NAME>
<EmailAddress>ianmbennettc2y@gmail.com</EmailAddress>
<PI_PHON>6507969517</PI_PHON>
<NSF_ID>000463799</NSF_ID>
<StartDate>08/03/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>The Spirituality Network, Inc.</Name>
<CityName>Palo Alto</CityName>
<ZipCode>943033222</ZipCode>
<PhoneNumber>6507969517</PhoneNumber>
<StreetAddress>2275 East Bayshore Road</StreetAddress>
<StreetAddress2><![CDATA[Suite 130]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>968386065</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SPIRITUALITY NETWORK, INC., THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Spirituality Network, Inc.]]></Name>
<CityName>Mill Valley</CityName>
<StateCode>CA</StateCode>
<ZipCode>949413515</ZipCode>
<StreetAddress><![CDATA[211 Cleveland Ct.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5373</Code>
<Text>SBIR Phase II</Text>
</ProgramElement>
<ProgramReference>
<Code>168E</Code>
<Text>SBIR/STTR/ERC Collab (SECO)</Text>
</ProgramReference>
<ProgramReference>
<Code>169E</Code>
<Text>SBIR Tech Enhan Partner (TECP)</Text>
</ProgramReference>
<ProgramReference>
<Code>5373</Code>
<Text>SMALL BUSINESS PHASE II</Text>
</ProgramReference>
<ProgramReference>
<Code>8031</Code>
<Text>Education Products</Text>
</ProgramReference>
<ProgramReference>
<Code>8033</Code>
<Text>Hardware Software Integration</Text>
</ProgramReference>
<ProgramReference>
<Code>8039</Code>
<Text>Information, Communication &amp; Computing</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~748800</FUND_OBLG>
<FUND_OBLG>2016~299257</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goals of this SBIR Phase II project to incorporate novel machine vision and social networking functionality into technologies used in online education and webinars were met successfully. Researchers have long identified engagement as the key ingredient for success in any learning environment and particularly the online environment. Current online teaching systems lack the means by which instructors can gauge their students' level of engagement because these students are not visible. Student behavior and lecturer oversight in the traditional classroom are known to modulate study behaviors and impact performance and learning outcomes, but cannot be managed for students in a virtual audience. Quantifying and automatically measuring student engagement during lectures in a scalable and accessible manner for these students is essential for improving academic success, but has not been studied widely in natural distance learning environments. This project support NSF's mission in education, and especially goals for STEM-based education; it resulted in technology that helps teachers provide effective cognitive and motivational support for students. Most importantly, this project aimed to develop a system for promoting richer interactions between tutor and online students through integrated cognitive and motivational scaffolding, leading to higher levels of student success thus enabling them to compete more effectively as skilled artisans in the 21st century workforce.</p> <p>The project culminated in a system framework - <em>Emotionally Immersive Tele-Learning (EITL<sup>TM</sup></em>), contributing&nbsp; four significant state-of-the art innovations to educational technology: 1) a machine vision recognition system integrating eye gaze direction and facial expressions from which engagement metrics aggregates data across participants in a virtual classroom; 2) a machine vision recognition system for detecting hand gestures and postural kinesics; 3) integration of third-party pedagogical applications within the EITL system framework; 4) integration of social network functionality that replicates pre&shy; and post&shy;lecture socialization including pair-sharing, breakout groups, team teaching, and administrative support for online teaching assistance.</p> <p>The EITL&nbsp;framework incorporates novel computer vision and machine learning approaches to technologies used in online education and webinars. EITL measures the student&rsquo;s willingness to participate in the learning process (i.e., behavioral engagement) and his/her emotional attitude towards learning (i.e., emotional engagement). The technology as developed and shown in Fig. 1 captures the student via video and tracks the student&rsquo;s face through the video&rsquo;s frames. Different features are extracted from the student&rsquo;s face such as facial fiducial points, head pose, eye gaze, and Gabor-based features. These features, which are used to detect action units (AU&rsquo;s) as defined by the Facial Action Coding System (FACS), decompose facial expressions in terms of the fundamental actions of individual muscles or groups of muscles. Then these action units are used to identify facial expressions as well as the behavioral and the emotional engagements of the student. This technology as shown in Fig.2 allows the lecturer to receive real-time feedback from facial features, gaze and other body kinesics, which when averaged across the virtual classroom, provide feedback related to the reception of information delivery.</p> <p>The technology development for the EITL system prototype was developed and perfected over the 2-year span of the SBIR Phase II project with all the technical objectives being met. However, because of the focus on the prototype development during this period, it was not possible to seek follow-on funding commitments from possible technical partners or from the private investment market. The company is now actively seeking technical partners and/or a possible acquisition for the technology by companies in the facial recognition/emotion detection market sector.</p> <p><strong><span style="text-decoration: underline;">Publications</span></strong>:</p> <ul> <li><em>Toward Active and Unobtrusive Engagement Assessment of Distance&nbsp;</em><em>Learners, Brandon Booth et al., </em>Seventh International Conference on Affective Computing and Intelligent Interaction (ACII 2017), October 23-26, 2017, San Antonio, Texas.</li> <li><em>Facial Action Units Detection Under Pose Variations Using Deep Regions&nbsp;</em><em>Learning, Asem Ali et al., </em>Seventh International Conference on Affective Computing and Intelligent Interaction (ACII 2017), October 23-26, 2017, San Antonio, Texas.</li> </ul> <ul> <li><em>Emotionally Immersive Tele-Learning</em> (EITL) - by Asem Ali, Ph.D. et al - paper acceptance and demonstration slated for International Society for Technology in Education (ISTE 2018), June 24-27, 2018, Chicago, IL.</li> <li><em>Deep Regions Learning For Facial Action Units Detection Using Both Learned and Geometric Features</em> by Asem Ali, Ph.D. et al, CVPR 2018 - IEEE Conference on Computer Vision and Pattern Recognition, June 18-24, Salt Lake City, UT. </li> <li><em>&lsquo;Facial Expressions Recognition Based on Muscle Movement Estimation&rsquo;,&nbsp;</em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2018 Conference, April 15-20, 2018, Calgary Alberta, Canada, (in submission).</li> </ul> <ul> <li>&lsquo;<em>Measuring Student Engagement in Online Education using Facial Information</em>&rsquo;, IEEE International Conference on Image<em> </em>Processing (ICIP) 2018, October 7-10, Athens, Greece, (in submission).</li> </ul> <p><span style="text-decoration: underline;"><strong>Patents</strong></span>:</p> <ul> <li>Two (2) patent applications (In preparation) to the USPTO.</li> </ul> <p><span style="text-decoration: underline;"><strong>Other Outcomes</strong></span>:</p> <ul> <li>Linkages to third-party solutions and participation in leading industry ecosystems, such as the Nvidia Inception Program, Google Tensor Flow Initiative (TensorFlow.org), Embedded Vision Alliance and others in areas of computer vision, pattern recognition, deep learning and artificial intelligence (AI).</li> </ul> <p>&nbsp;</p><br> <p>            Last Modified: 03/02/2018<br>      Modified by: Ian&nbsp;Bennett</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520042647604_AverageEngagementStatisticsOveraMonthforAdministrativeUsers--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520042647604_AverageEngagementStatisticsOveraMonthforAdministrativeUsers--rgov-800width.jpg" title="Average Engagement Statistics over a Month for Educational Class Administrators"><img src="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520042647604_AverageEngagementStatisticsOveraMonthforAdministrativeUsers--rgov-66x44.jpg" alt="Average Engagement Statistics over a Month for Educational Class Administrators"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig. 5: Average Engagement Statistics over a Month for Educational Class Administrators</div> <div class="imageCredit">TSN</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ian&nbsp;Bennett</div> <div class="imageTitle">Average Engagement Statistics over a Month for Educational Class Administrators</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520038507607_EITLDeepRegionArchitecture--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520038507607_EITLDeepRegionArchitecture--rgov-800width.jpg" title="EITL&rsquo;s Deep Region-based Learning Computational Neural Network (CNN) Architecture"><img src="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520038507607_EITLDeepRegionArchitecture--rgov-66x44.jpg" alt="EITL&rsquo;s Deep Region-based Learning Computational Neural Network (CNN) Architecture"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig. 1: EITL?s Deep Region-based Learning Computational Neural Network (CNN) Architecture</div> <div class="imageCredit">TSN</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ian&nbsp;Bennett</div> <div class="imageTitle">EITL?s Deep Region-based Learning Computational Neural Network (CNN) Architecture</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520038576219_RepresentativeEngagementCurves--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520038576219_RepresentativeEngagementCurves--rgov-800width.jpg" title="Representative EITL Engagement Curves for  Seven Viewers In a Virtual Classroom"><img src="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520038576219_RepresentativeEngagementCurves--rgov-66x44.jpg" alt="Representative EITL Engagement Curves for  Seven Viewers In a Virtual Classroom"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig. 2: Representative EITL Engagement Curves for Seven Viewers In a Virtual Classroom</div> <div class="imageCredit">TSN</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ian&nbsp;Bennett</div> <div class="imageTitle">Representative EITL Engagement Curves for  Seven Viewers In a Virtual Classroom</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520041881245_EITLLiveViewinAnInstructor'sDashboard--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520041881245_EITLLiveViewinAnInstructor'sDashboard--rgov-800width.jpg" title="EITL Live View In An Instructor's Dashboard"><img src="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520041881245_EITLLiveViewinAnInstructor'sDashboard--rgov-66x44.jpg" alt="EITL Live View In An Instructor's Dashboard"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig. 3: EITL Live View In An Instructor's Dashboard</div> <div class="imageCredit">TSN</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ian&nbsp;Bennett</div> <div class="imageTitle">EITL Live View In An Instructor's Dashboard</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520042086471_ExampleofAverageEngagementStatistics--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520042086471_ExampleofAverageEngagementStatistics--rgov-800width.jpg" title="Average Engagement Statistics for Educational Class Administrators"><img src="/por/images/Reports/POR/2018/1431024/1431024_10327671_1520042086471_ExampleofAverageEngagementStatistics--rgov-66x44.jpg" alt="Average Engagement Statistics for Educational Class Administrators"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig 4: Average Engagement Statistics Over a Semester -- For Educational Class Administrators</div> <div class="imageCredit">TSN</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ian&nbsp;Bennett</div> <div class="imageTitle">Average Engagement Statistics for Educational Class Administrators</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goals of this SBIR Phase II project to incorporate novel machine vision and social networking functionality into technologies used in online education and webinars were met successfully. Researchers have long identified engagement as the key ingredient for success in any learning environment and particularly the online environment. Current online teaching systems lack the means by which instructors can gauge their students' level of engagement because these students are not visible. Student behavior and lecturer oversight in the traditional classroom are known to modulate study behaviors and impact performance and learning outcomes, but cannot be managed for students in a virtual audience. Quantifying and automatically measuring student engagement during lectures in a scalable and accessible manner for these students is essential for improving academic success, but has not been studied widely in natural distance learning environments. This project support NSF's mission in education, and especially goals for STEM-based education; it resulted in technology that helps teachers provide effective cognitive and motivational support for students. Most importantly, this project aimed to develop a system for promoting richer interactions between tutor and online students through integrated cognitive and motivational scaffolding, leading to higher levels of student success thus enabling them to compete more effectively as skilled artisans in the 21st century workforce.  The project culminated in a system framework - Emotionally Immersive Tele-Learning (EITLTM), contributing  four significant state-of-the art innovations to educational technology: 1) a machine vision recognition system integrating eye gaze direction and facial expressions from which engagement metrics aggregates data across participants in a virtual classroom; 2) a machine vision recognition system for detecting hand gestures and postural kinesics; 3) integration of third-party pedagogical applications within the EITL system framework; 4) integration of social network functionality that replicates pre&shy; and post&shy;lecture socialization including pair-sharing, breakout groups, team teaching, and administrative support for online teaching assistance.  The EITL framework incorporates novel computer vision and machine learning approaches to technologies used in online education and webinars. EITL measures the student?s willingness to participate in the learning process (i.e., behavioral engagement) and his/her emotional attitude towards learning (i.e., emotional engagement). The technology as developed and shown in Fig. 1 captures the student via video and tracks the student?s face through the video?s frames. Different features are extracted from the student?s face such as facial fiducial points, head pose, eye gaze, and Gabor-based features. These features, which are used to detect action units (AU?s) as defined by the Facial Action Coding System (FACS), decompose facial expressions in terms of the fundamental actions of individual muscles or groups of muscles. Then these action units are used to identify facial expressions as well as the behavioral and the emotional engagements of the student. This technology as shown in Fig.2 allows the lecturer to receive real-time feedback from facial features, gaze and other body kinesics, which when averaged across the virtual classroom, provide feedback related to the reception of information delivery.  The technology development for the EITL system prototype was developed and perfected over the 2-year span of the SBIR Phase II project with all the technical objectives being met. However, because of the focus on the prototype development during this period, it was not possible to seek follow-on funding commitments from possible technical partners or from the private investment market. The company is now actively seeking technical partners and/or a possible acquisition for the technology by companies in the facial recognition/emotion detection market sector.  Publications:  Toward Active and Unobtrusive Engagement Assessment of Distance Learners, Brandon Booth et al., Seventh International Conference on Affective Computing and Intelligent Interaction (ACII 2017), October 23-26, 2017, San Antonio, Texas. Facial Action Units Detection Under Pose Variations Using Deep Regions Learning, Asem Ali et al., Seventh International Conference on Affective Computing and Intelligent Interaction (ACII 2017), October 23-26, 2017, San Antonio, Texas.   Emotionally Immersive Tele-Learning (EITL) - by Asem Ali, Ph.D. et al - paper acceptance and demonstration slated for International Society for Technology in Education (ISTE 2018), June 24-27, 2018, Chicago, IL. Deep Regions Learning For Facial Action Units Detection Using Both Learned and Geometric Features by Asem Ali, Ph.D. et al, CVPR 2018 - IEEE Conference on Computer Vision and Pattern Recognition, June 18-24, Salt Lake City, UT.  ?Facial Expressions Recognition Based on Muscle Movement Estimation?, International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2018 Conference, April 15-20, 2018, Calgary Alberta, Canada, (in submission).   ?Measuring Student Engagement in Online Education using Facial Information?, IEEE International Conference on Image Processing (ICIP) 2018, October 7-10, Athens, Greece, (in submission).   Patents:  Two (2) patent applications (In preparation) to the USPTO.   Other Outcomes:  Linkages to third-party solutions and participation in leading industry ecosystems, such as the Nvidia Inception Program, Google Tensor Flow Initiative (TensorFlow.org), Embedded Vision Alliance and others in areas of computer vision, pattern recognition, deep learning and artificial intelligence (AI).           Last Modified: 03/02/2018       Submitted by: Ian Bennett]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
