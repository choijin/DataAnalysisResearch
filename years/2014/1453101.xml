<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Simulating Nonlinear Audiovisual Dynamics for Simulated Environments and Interactive Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2015</AwardEffectiveDate>
<AwardExpirationDate>01/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>501790.00</AwardTotalIntnAmount>
<AwardAmount>517790</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computer simulated virtual dynamics are increasingly recognized as important tools for creating immersive simulated environments for understanding physical systems that are too costly or perilous to investigate in real experiments.  Most simulation techniques, however, are still inherently silent; they encompass only the visual modality, resulting in a limited sensory depiction of phenomena which is in sharp contrast to the real world where we are constantly immersed in rich audiovisual experiences involving both scene illumination and auditory cues.  Consequently, most computer animations, regardless of how efficiently they are simulated, currently rely on manual editing to add sounds as afterthoughts, and many computational design tools are still appearance-oriented, modeling only geometry or visible motion.  In this project the PI's central objective is to build efficient models for simulating nonlinear audiovisual dynamics in a realistic and synchronized way.  This is a highly challenging task, as the simulation models need to faithfully resolve visible and audible details across a wide range of spatial and temporal scales because multiple scales of motion are often coupled together to produce perceivable nonlinear sound effects such as pitch shift and mode locking.  The PI will address these challenges in three stages.  First, he will create efficient multi-scale models which adaptively exploit underlying dynamical structures.  He will then systematically evaluate these models using ground-truth data, experimental statistics and user studies.  Finally, building upon his new models, he will develop tools to enable interactive parameter exploration of desired audiovisual effects.&lt;br/&gt;&lt;br/&gt;This work will seek to answer two fundamental questions: what needs to be faithfully simulated in order to produce audiovisual virtual dynamics, and how to perform the simulation at minimal computational cost?  The PI seeks to develop principled new techniques for complex nonlinear audiovisual simulations, which will lay the groundwork for building immersive simulated environments and interactive applications, and also for connecting to other areas such as computational design.  At a technical level, the PI's approach will bridge the longstanding gap between theoretical tools (including multi-scale analysis and dynamical system analysis) and practical computational schemes.  The PI argues that by decomposing the simulated dynamics into visible and audible scales, efficient simulation models adapted to individual scales can be built.  This scheme can yield methods with high efficiency while retaining simulation fidelity.  It also poses a new challenge: to develop adaptive simulation models across a wide range of audiovisual scales.  The PI's approach will address this challenge by exploiting dynamical system analysis at individual scales and linking the resulting models together.  If successful, this research will enable new simulated environment applications with fully synchronized audiovisual effects, and lead to new ways of creating and editing multimedia content.</AbstractNarration>
<MinAmdLetterDate>02/02/2015</MinAmdLetterDate>
<MaxAmdLetterDate>02/19/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1453101</AwardID>
<Investigator>
<FirstName>Changxi</FirstName>
<LastName>Zheng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Changxi Zheng</PI_FULL_NAME>
<EmailAddress>cxz@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397036</PI_PHON>
<NSF_ID>000648688</NSF_ID>
<StartDate>02/02/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100277003</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~120407</FUND_OBLG>
<FUND_OBLG>2016~95586</FUND_OBLG>
<FUND_OBLG>2017~98042</FUND_OBLG>
<FUND_OBLG>2018~100574</FUND_OBLG>
<FUND_OBLG>2019~103181</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-121b0a35-7fff-69c6-e451-fb67305588db"> </span></p> <p dir="ltr"><span>Over the course of this program, we have developed a series of numerical models for simulating nonlinear audiovisual phenomena---ones that produce both visible motions and audible sounds. Our studies cover a wide range of audiovisual phenomena, from water flows with air bubbles to crumpling soda cans. For each audiovisual phenomenon, we model the dynamics that contribute to the visible motion as well as the dynamics crucial to the sound production. Oftentimes, the mathematical models behind these dynamics are complex and highly nonlinear, and they must be described at multiple spatial and temporal scales. To reduce the computational cost, we devise numerical methods by leveraging the multi-scale nature of these phenomena. At different spatiotemporal scales, the audiovisual phenomenon is modeled by different numerical methods which are tailored for computational efficiency at individual scales. We then integrate different numerical algorithms together to capture the nonlinear essence of an audiovisual phenomenon and produce realistic animations and sound effects. In the end, we have contributed a rich set of numerical tools for reproducing audiovisual phenomena in the computer.</span></p> <p>We also explore the use of our numerical models beyond producing audiovisual effects. For example, by integrating a fast sound simulation model in the video processing pipeline, we have proposed a method to easily enhance and edit the spatial audio signal in 360-degree videos. The edited audio overlay respects the reverberation and other room acoustics in the video?s capturing environment, and playing back the 360-degree video produces realistic spatial audio effects. In another work, the simulation model has been used in a numerical optimization framework to automatically design the shape of acoustic filters (such as mufflers). It has also been used in tandem with machine learning models to improve the state-of-the-art of audio enhancement algorithms. A notable outcome of this program---in addition to the fundamental simulation models---is that we have broadened the use of numerical simulation to such areas as signal processing, user interaction, and engineering design.</p> <p>This program has supported two REUs, and both of them, after conducting the REU projects, have moved on to pursue their PhD degrees. Some of the audiovisual models and results have also been integrated into our classroom teaching materials. Results from this program have also attracted great interests from industry and other research disciplines. For example, we have transferred some key ideas originated from our acoustic wave simulation models to simulate optical waves. This opens interdisciplinary opportunities for us to work with engineers and physists in nanophotonics and other areas. A few preliminary works have been published to demonstrate the promise of this future research direction.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/31/2021<br>      Modified by: Changxi&nbsp;Zheng</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1453101/1453101_10349197_1622431398180_final--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1453101/1453101_10349197_1622431398180_final--rgov-800width.jpg" title="Gallery of our acoustic simulation models."><img src="/por/images/Reports/POR/2021/1453101/1453101_10349197_1622431398180_final--rgov-66x44.jpg" alt="Gallery of our acoustic simulation models."></a> <div class="imageCaptionContainer"> <div class="imageCaption">We have developed various simulation models for predicting sound from different physical phenomena, including (a) complex water flows, (b) nonlinear, turbulent thin-shells, (c) linear modal vibration, and (d) buckling shells.</div> <div class="imageCredit">Changxi Zheng</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Changxi&nbsp;Zheng</div> <div class="imageTitle">Gallery of our acoustic simulation models.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Over the course of this program, we have developed a series of numerical models for simulating nonlinear audiovisual phenomena---ones that produce both visible motions and audible sounds. Our studies cover a wide range of audiovisual phenomena, from water flows with air bubbles to crumpling soda cans. For each audiovisual phenomenon, we model the dynamics that contribute to the visible motion as well as the dynamics crucial to the sound production. Oftentimes, the mathematical models behind these dynamics are complex and highly nonlinear, and they must be described at multiple spatial and temporal scales. To reduce the computational cost, we devise numerical methods by leveraging the multi-scale nature of these phenomena. At different spatiotemporal scales, the audiovisual phenomenon is modeled by different numerical methods which are tailored for computational efficiency at individual scales. We then integrate different numerical algorithms together to capture the nonlinear essence of an audiovisual phenomenon and produce realistic animations and sound effects. In the end, we have contributed a rich set of numerical tools for reproducing audiovisual phenomena in the computer.  We also explore the use of our numerical models beyond producing audiovisual effects. For example, by integrating a fast sound simulation model in the video processing pipeline, we have proposed a method to easily enhance and edit the spatial audio signal in 360-degree videos. The edited audio overlay respects the reverberation and other room acoustics in the video?s capturing environment, and playing back the 360-degree video produces realistic spatial audio effects. In another work, the simulation model has been used in a numerical optimization framework to automatically design the shape of acoustic filters (such as mufflers). It has also been used in tandem with machine learning models to improve the state-of-the-art of audio enhancement algorithms. A notable outcome of this program---in addition to the fundamental simulation models---is that we have broadened the use of numerical simulation to such areas as signal processing, user interaction, and engineering design.  This program has supported two REUs, and both of them, after conducting the REU projects, have moved on to pursue their PhD degrees. Some of the audiovisual models and results have also been integrated into our classroom teaching materials. Results from this program have also attracted great interests from industry and other research disciplines. For example, we have transferred some key ideas originated from our acoustic wave simulation models to simulate optical waves. This opens interdisciplinary opportunities for us to work with engineers and physists in nanophotonics and other areas. A few preliminary works have been published to demonstrate the promise of this future research direction.          Last Modified: 05/31/2021       Submitted by: Changxi Zheng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
