<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ITR:     Recognizing and Understanding Emotion in Speech</AwardTitle>
<AwardEffectiveDate>09/15/2003</AwardEffectiveDate>
<AwardExpirationDate>08/31/2010</AwardExpirationDate>
<AwardAmount>2172551</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim P. Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Spoken language is much more than simply audible text.  The way we produce words and phrases tells our hearers much about our mental state and the intentions that underlie our words and actions.  While machines have become increasingly proficient at recognizing sentences in current spoken dialog systems, they are still very poor at detecting, inter alia, whether speakers are frustrated or confident, or whether they are trying to deceive or to convey helpful information to their hearers.  Despite promising work in identifying verbal and non-verbal cues to emotion and intention in acted, laboratory speech, and early results identifying limited types of emotion in more natural settings, we have only a limited understanding of reliable verbal cues (acoustic, prosodic, lexical and syntactic) to these phenomena and consequently do not know how to automatically recognize the phenomena automatically.&lt;br/&gt;&lt;br/&gt;The PIs will first conduct a series of laboratory experiments to identify acoustic, prosodic, lexical and syntactic cues to emotion and intention in elicited (non-acted) speech.  They will, in parallel, discover new features that may be useful in automatically identifying emotions and intentions such as deceptiveness, confidence, and frustration/anger using available corpora with augmented labeling and labeling new corpora for these speaker states/intentions.  They will subsequently test these features on the laboratory recordings and identify new features that may be suggested by analysis of these recordings.  The result should provide a better understanding of what auditory cues characterize certain speaker states/intentions and which of these provide reliable features for their automatic identification.&lt;br/&gt;&lt;br/&gt;From a practical point of view, identifying speaker state/intentions automatically should be of considerable benefit for interactive voice response systems such as call center response systems, over the phone banking or travel reservation systems, or tutorial systems.  Automatic identification of speaker state/intentions should also provide useful information for speaker screening in a variety of applications that currently depend upon human assessment of speaker state/intention.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/15/2003</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2010</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0325399</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Jurafsky</LastName>
<EmailAddress>jurafsky@stanford.edu</EmailAddress>
<StartDate>09/15/2003</StartDate>
<EndDate>12/18/2003</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Elizabeth</FirstName>
<LastName>Shriberg</LastName>
<EmailAddress>elizabeth.shriberg@sri.com</EmailAddress>
<StartDate>09/15/2003</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Laura</FirstName>
<LastName>Michaelis</LastName>
<EmailAddress>michaeli@spot.colorado.edu</EmailAddress>
<StartDate>12/18/2003</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bryan</FirstName>
<LastName>Pellom</LastName>
<EmailAddress>pellom@cslr.colorado.edu</EmailAddress>
<StartDate>09/15/2003</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Julia</FirstName>
<LastName>Hirschberg</LastName>
<EmailAddress>julia@cs.columbia.edu</EmailAddress>
<StartDate>09/15/2003</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Venditti-Ramprashad</LastName>
<EmailAddress>jjv@cs.columbia.edu</EmailAddress>
<StartDate>09/15/2003</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1687</Code>
<Text>ITR MEDIUM (GROUP) GRANTS</Text>
</ProgramElement>
<ProgramElement>
<Code>T044</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>V380</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>V568</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>V792</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1687</Code>
<Text>ITR MEDIUM (GROUP) GRANTS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
