<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ITR:     Risk, Reward, and Reinforcement</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2003</AwardEffectiveDate>
<AwardExpirationDate>07/31/2008</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>339998</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Douglas H. Fisher</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The objectives of this project are to develop efficient and reliable algorithms for direct reinforcement, to learn risk-averse behaviors for problems with high degrees of uncertainty, and to apply the methods developed to an economically important problem: global asset allocation.  Reinforcement learning (RL) enables a goal-directed agent to discover strategies through trial and error exploration with only limited feedback.  Direct Reinforcement (DR, or "policy gradient") methods enable an agent to discover a strategy without the need to learn a value function.&lt;br/&gt;&lt;br/&gt;Dynamic programming and related value function RL methods are often found to be inefficient, to produce unstable solutions, and to have difficulty scaling up to large problems.  Hence, there have been relatively few real-world applications of the value function type RL.  This project seeks to make several advancements in Direct Reinforcement that will enable the development of efficient and effective practical applications.&lt;br/&gt;&lt;br/&gt;By controlling the "exploration vs. exploitation" trade-off during on-line learning, DR agents will be able to discover better policies and do so more efficiently.  Stochastic optimization methods, such as stochastic "search then converge" or annealing of a Boltzmann temperature are candidate approaches.  By developing risk-averse reinforcement methods, DR agents will be able to learn robust policies for uncertain or risky environments.  Using risk-sensitive intertemporal utilities, DR agents will learn to avoid risky states or actions while they pursue long-term reward.  Dynamic programming is widely used in economics and finance, but few attempts have been made to solve important financial problems with reinforcement learning.  As a demonstration of risk-averse DR, this project will build a prototype global asset allocation system.&lt;br/&gt;&lt;br/&gt;Risk-averse direct reinforcement may find application in a variety of engineering domains, from robotics to industrial control to autonomous agents.  Many industries, such as energy and the airlines, need to manage operational and financial risks together, in order to avoid supply shortfalls or bankruptcy.  Individual investors must manage risk while building their investment portfolios to meet future needs, such as children's college expenses or retirement.  Risk-averse Direct Reinforcement may find application in many such contexts.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/01/2003</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2007</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0342634</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Moody</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John E Moody</PI_FULL_NAME>
<EmailAddress>moody@icsi.berkeley.edu</EmailAddress>
<PI_PHON>5037505942</PI_PHON>
<NSF_ID>000436642</NSF_ID>
<StartDate>08/01/2003</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>International Computer Science Institute</Name>
<CityName>Berkeley</CityName>
<CountyName>ALAMEDA</CountyName>
<ZipCode>947041345</ZipCode>
<PhoneNumber>5106662900</PhoneNumber>
<StreetAddress>2150 Shattuck Ave, Suite 1100</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>187909478</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>INTERNATIONAL COMPUTER SCIENCE INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name>International Computer Science Institute</Name>
<CityName>Berkeley</CityName>
<CountyName>ALAMEDA</CountyName>
<StateCode>CA</StateCode>
<ZipCode>947041345</ZipCode>
<StreetAddress>2150 Shattuck Ave, Suite 1100</StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>1686</Code>
<Text>ITR SMALL GRANTS</Text>
</ProgramElement>
<ProgramReference>
<Code>1654</Code>
<Text>HUMAN COMPUTER INTERFACE</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0103</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2003~339998</FUND_OBLG>
</Award>
</rootTag>
