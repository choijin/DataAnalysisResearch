<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Using Machine Translation to Bootstrap Speech Recognition Systems</AwardTitle>
<AwardEffectiveDate>09/01/2003</AwardEffectiveDate>
<AwardExpirationDate>09/30/2008</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>412000</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The development of automatic speech recognition (ASR) systems is severely constrained by the need for large amounts of training data.  Furthermore, available training data often does not match the recognition task in style and domain, which particularly affects the language modeling component in an ASR system.&lt;br/&gt;&lt;br/&gt;This project is aimed at developing ways of artificially generating language model training data for ASR.  Specifically, statistical machine translation (SMT) models are used to produce task-specific data from different but related data representing, for example, a different speech style, dialect, or domain.  First, SMT models are trained on a small amount of parallel in-domain and out-of-domain data.  The trained model is then applied to a larger set of out-of-domain data.  Finally, the 'translated' output is filtered with respect to its relevance to the target task.  In addition to using existing SMT models for data generation, a new type of SMT model is introduced in which words are represented as collections of features.  This results in a factorized probability model that can be estimated more robustly than a standard model.  In this project the above strategy is used to create training data for conversational speech from written text.  It is evaluated by comparison with standard language model adaptation and training methods.&lt;br/&gt; &lt;br/&gt;This technique is expected to significantly reduce the requirements for task-specific data when developing or porting an ASR system to a new recognition task.  Moreover, this work will contribute to increased cross-fertilization between machine translation and ASR research.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/02/2003</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2007</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0308297</AwardID>
<Investigator>
<FirstName>Katrin</FirstName>
<LastName>Kirchhoff</LastName>
<EmailAddress>katrin@ee.washington.edu</EmailAddress>
<StartDate>09/02/2003</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>1686</Code>
<Text>ITR SMALL GRANTS</Text>
</ProgramElement>
<ProgramElement>
<Code>6845</Code>
<Text>HUMAN COMPUTER INTER PROGRAM</Text>
</ProgramElement>
<ProgramElement>
<Code>6846</Code>
<Text>UNIVERSAL ACCESS</Text>
</ProgramElement>
<ProgramElement>
<Code>7274</Code>
<Text>HUMAN LANGUAGE &amp; COMMUNICATION</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code/>
<Text/>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
