<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ITR:  Knowledge-Enhanced Discovery System (KEDS):  Incorporating Background Knowledge for Scientific Discovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2003</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>770000.00</AwardTotalIntnAmount>
<AwardAmount>795640</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The KEDS project is developing a toolkit for scientific discovery, which refers to the identification of new predictive models for a set of phenomena of interest from observational data about those phenomena.  KEDS goes beyond existing methods by incorporating background knowledge (such as dependencies between variables, partial models, and information about the task for which the learned models will be used) into the learning process, and by providing analysis tools to aid the user in understanding, evaluating, and comparing the learned models.&lt;br/&gt;&lt;br/&gt;KEDS provides a more flexible, interactive approach to scientific discovery than current tools.  The interactive techniques in KEDS are also being applied to science education, supporting an iterative process in which students refine their mental models, based on personalized, targeted feedback from the system.&lt;br/&gt;&lt;br/&gt;The impact of the project will be an advance in how information technology is used in scientific investigations and science education.  In particular, KEDS will support a more interactive style of scientific discovery, which will allow human domain experts to integrate their previous knowledge into the discovery process more effectively. Expected results include improved interactive methods for scientific discovery and science education, data sets and benchmarks for interactive scientific discovery, and a documented software package.  The KEDS system is currently being applied to astronomy science domains, but the underlying techniques have broad applicability to many other science domains, including earth sciences, biology and medicine, chemistry, and materials science.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/24/2003</MinAmdLetterDate>
<MaxAmdLetterDate>05/18/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0325329</AwardID>
<Investigator>
<FirstName>Marie</FirstName>
<LastName>desJardins</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marie E desJardins</PI_FULL_NAME>
<EmailAddress>mariedj@cs.umbc.edu</EmailAddress>
<PI_PHON>4104553967</PI_PHON>
<NSF_ID>000231667</NSF_ID>
<StartDate>09/24/2003</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kiri</FirstName>
<LastName>Wagstaff</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kiri L Wagstaff</PI_FULL_NAME>
<EmailAddress>Kiri.Wagstaff@jpl.nasa.gov</EmailAddress>
<PI_PHON>8183936393</PI_PHON>
<NSF_ID>000423921</NSF_ID>
<StartDate>09/24/2003</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland Baltimore County</Name>
<CityName>Baltimore</CityName>
<ZipCode>212500002</ZipCode>
<PhoneNumber>4104553140</PhoneNumber>
<StreetAddress>1000 Hilltop Circle</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>061364808</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND BALTIMORE COUNTY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland Baltimore County]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212500002</ZipCode>
<StreetAddress><![CDATA[1000 Hilltop Circle]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1687</Code>
<Text>ITR MEDIUM (GROUP) GRANTS</Text>
</ProgramElement>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1687</Code>
<Text>ITR MEDIUM (GROUP) GRANTS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0103</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0104</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0105</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2003~120000</FUND_OBLG>
<FUND_OBLG>2004~132000</FUND_OBLG>
<FUND_OBLG>2005~120000</FUND_OBLG>
<FUND_OBLG>2006~120000</FUND_OBLG>
<FUND_OBLG>2007~290000</FUND_OBLG>
<FUND_OBLG>2010~13640</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The focus of our research has been the incorporation of background knowledge into various types of learning/discovery techniques, including preference learning, clustering, and classification. The types of background knowledge we have explored include information about feature relevance and value similarity; costs associated with feature acquisition, labeling (classification testing), and prediction errors; knowledge about feature relevance; previously learned models for similar tasks; and direct user feedback about cluster quality. The ability to incorporate different kinds of background knowledge into scientific discovery techniques enables domain experts to interact more effectively with discovery systems. We have made contributions to the discipline in seven main areas:</p> <p>(1) Novel methods for incorporating abstraction hierarchies into Bayesian network learning. We developed techniques for incorporating value abstraction hierarchies into Bayesian network learning, extending standard heuristic search and scoring techniques to identify an appropriate level of abstraction for representing the Bayesian network.</p> <p>(2) A novel framework for reasoning about preferences over sets, innovative methods for incorporating typicality and instance annotations into preference learning, and efficient methods for preference learning using SVMs. &nbsp;The primary contribution in this area was the DD-Pref language for expressing preferences over collections of items, expressed in terms of diversity and depth, and introduced automated learning methods to learn these preferences from observations (user-selected sets). These methods were applied to numeric, symbolic, and text data, and were validated in several domains.&nbsp;</p> <p>(3) A variety of techniques for using constrained clustering to solve different learning problems. &nbsp;Key contributions include framework for interactive visual clustering framework that enables users to interactively discover meaningful data clusters, a method for using background knowledge as soft constraints to handle data with missing values, and an active constrained spectral clustering method.</p> <p>(4) A new framework for constructing test-cost sensitive regression models. &nbsp;We studied the problem of regression (specifically, predicting runtimes for different AI planners on a given problem) in the presence of features with varying costs. In this case, the 'cost' is the computational time to measure various properties of the planning problem. The technique is capable of constructing a regression model from training data that can be used to maximize prediction quality, while minimizing feature acquisition costs, at test time.</p> <p>(5) Confidence-Based Feature Acquisition (CFA), the first technique for cost-sensitive feature acquisition when feature values are missing at both training and test time. Taking into account a user's desired confidence level for prediction accuracy, the CFA method builds a model that minimizes the cost of training and using the classifier. We extended the basic CFA technique to incorporate background or learned knowledge about feature relevance and to include more sophisticated cost-accuracy tradeoffs.&nbsp;</p> <p>(6) Methods for selecting previous learning tasks to transfer to new learning problems in both classification learning and reinforcement learning, and several inductive transfer learning algorithms. Key contributions include spectral graph methods for determining which tasks are most relevant for transfer and transfer learning methods for reinforcement learning that can permit an agent to scale its learning to more complex problem domains.</p> <p>(7) Techniques for improved estimation of model confidence. It is well known that existing methods for estimating model confidence can lead to under- or over-estimates, and there has been previous work on rescaling these confidence estimates in the co...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The focus of our research has been the incorporation of background knowledge into various types of learning/discovery techniques, including preference learning, clustering, and classification. The types of background knowledge we have explored include information about feature relevance and value similarity; costs associated with feature acquisition, labeling (classification testing), and prediction errors; knowledge about feature relevance; previously learned models for similar tasks; and direct user feedback about cluster quality. The ability to incorporate different kinds of background knowledge into scientific discovery techniques enables domain experts to interact more effectively with discovery systems. We have made contributions to the discipline in seven main areas:  (1) Novel methods for incorporating abstraction hierarchies into Bayesian network learning. We developed techniques for incorporating value abstraction hierarchies into Bayesian network learning, extending standard heuristic search and scoring techniques to identify an appropriate level of abstraction for representing the Bayesian network.  (2) A novel framework for reasoning about preferences over sets, innovative methods for incorporating typicality and instance annotations into preference learning, and efficient methods for preference learning using SVMs.  The primary contribution in this area was the DD-Pref language for expressing preferences over collections of items, expressed in terms of diversity and depth, and introduced automated learning methods to learn these preferences from observations (user-selected sets). These methods were applied to numeric, symbolic, and text data, and were validated in several domains.   (3) A variety of techniques for using constrained clustering to solve different learning problems.  Key contributions include framework for interactive visual clustering framework that enables users to interactively discover meaningful data clusters, a method for using background knowledge as soft constraints to handle data with missing values, and an active constrained spectral clustering method.  (4) A new framework for constructing test-cost sensitive regression models.  We studied the problem of regression (specifically, predicting runtimes for different AI planners on a given problem) in the presence of features with varying costs. In this case, the 'cost' is the computational time to measure various properties of the planning problem. The technique is capable of constructing a regression model from training data that can be used to maximize prediction quality, while minimizing feature acquisition costs, at test time.  (5) Confidence-Based Feature Acquisition (CFA), the first technique for cost-sensitive feature acquisition when feature values are missing at both training and test time. Taking into account a user's desired confidence level for prediction accuracy, the CFA method builds a model that minimizes the cost of training and using the classifier. We extended the basic CFA technique to incorporate background or learned knowledge about feature relevance and to include more sophisticated cost-accuracy tradeoffs.   (6) Methods for selecting previous learning tasks to transfer to new learning problems in both classification learning and reinforcement learning, and several inductive transfer learning algorithms. Key contributions include spectral graph methods for determining which tasks are most relevant for transfer and transfer learning methods for reinforcement learning that can permit an agent to scale its learning to more complex problem domains.  (7) Techniques for improved estimation of model confidence. It is well known that existing methods for estimating model confidence can lead to under- or over-estimates, and there has been previous work on rescaling these confidence estimates in the context of a specific data set. We have further improved these confidence estimates by taking into account properties of the source...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
