<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ITR: Scalable Non-Stop Blade-Based Servers</AwardTitle>
<AwardEffectiveDate>10/01/2003</AwardEffectiveDate>
<AwardExpirationDate>09/30/2007</AwardExpirationDate>
<AwardTotalIntnAmount>1200000.00</AwardTotalIntnAmount>
<AwardAmount>1200000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
</ProgramOfficer>
<AbstractNarration>&lt;br/&gt;This project proposes the Scalable Non-stOP Server (SNOPS) architecture: a reliable, available, and serviceable (RAS) hardware platform. SNOPS offers both cost and performance scalability unparalleled by conventional RAS-oriented servers by using commodity blade components interconnected through a scalable network and hardware distributed shared memory (DSM). SNOPS offers non-stop service through fast application-transparent detection and recovery of soft/transient errors and/or single processor/memory component failure, and hot-swapping of a module upon hardware component failure without state loss.&lt;br/&gt;&lt;br/&gt;The project proposes the MEMory BaRrier Across the NEtwork (MEMBRANE), an abstraction layer to regulate memory data transfer between processor and memory modules. MEMBRANE serves as an impenetrable barrier for faulty data both from the processor and the memory sides, allowing only error-free data to transfer across. On the processor side, the MEMBRANE coherence protocols detect and trigger recovery for errors originating from redundant processors of a group by comparing their requests. On the memory side, the MEMBRANE memory redundancy protocols detect and recover from errors originating from the memory components via a RAID-like distributed parity scheme.&lt;br/&gt;&lt;br/&gt;This prototype will support a commodity OS (such as Linux) and deliver the necessary performance to permit full-scale evaluation of our ideas against commercial-grade server applications. Scalable server simulation infrastructure will be also be available for distribution, allowing for fast, accurate, and full-system simulation of servers. Several proof-of-concept prototypes have been built in industrial and academic settings and especially for students are an invaluable experience, but with a high potential to have impact on industrial research and development as well.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/17/2003</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2005</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0325802</AwardID>
<Investigator>
<FirstName>Babak</FirstName>
<LastName>Falsafi</LastName>
<EmailAddress>babak@ece.cmu.edu</EmailAddress>
<StartDate>09/17/2003</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Hoe</LastName>
<EmailAddress>jhoe@cmu.edu</EmailAddress>
<StartDate>06/08/2005</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Hoe</LastName>
<EmailAddress>jhoe@cmu.edu</EmailAddress>
<StartDate>09/17/2003</StartDate>
<EndDate>06/08/2005</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andreas</FirstName>
<LastName>Nowatzyk</LastName>
<EmailAddress>agn@acm.org</EmailAddress>
<StartDate>09/17/2003</StartDate>
<EndDate>06/08/2005</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
</Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>1687</Code>
<Text>ITR MEDIUM (GROUP) GRANTS</Text>
</ProgramElement>
<ProgramReference>
<Code>1652</Code>
<Text>HIGH END COMPUTATION AND INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
