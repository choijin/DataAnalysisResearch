<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Coding Techniques for Distributed Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/30/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2022</AwardExpirationDate>
<AwardTotalIntnAmount>372267.00</AwardTotalIntnAmount>
<AwardAmount>388267</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern machine learning models have achieved great success and have been widely deployed across many sectors. As the size of data used to train machine learning models keeps growing, it is now routine to use distributed computing infrastructures such as the cloud. This strategy allows the computation of training to be distributed among a large number of nodes hosted in the cloud, where each node processes a partition of the whole data set. However, the performance of nodes in the cloud is often unreliable, due to system failures, resource contention, load imbalance, etc., and that unreliability can significantly delay the training process. This project pursues a coding-based framework that not only tolerates the effects of faulty nodes, but also further enhances the performance of machine learning training by dynamically taking advantage of the resources available on all nodes, whether they are faulty or not. The outcomes of this project should lead to a significant performance boost for distributed training of machine learning models.&lt;br/&gt;&lt;br/&gt;To enable the efficient use of distributed computing across unreliable infrastructure for training machine learning models from big data sets, the technical objectives of this project are divided into three levels. This project will first study coding theory for distributed matrix multiplication, a universal operation in various machine learning algorithms, and propose a coding framework with both fault tolerance and a significant performance boost. This framework will then be applied into parameter servers at the architecture level and deep neural networks at the model level, respectively. Combining these three parts, this work will lead to a practical coding framework that can efficiently scale out computation on heterogeneous unreliable nodes, where the coding schemes will be applied to distributed machine learning at different levels including fundamental arithmetic, architectures, and models.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>11/18/2020</MinAmdLetterDate>
<MaxAmdLetterDate>11/18/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2101388</AwardID>
<Investigator>
<FirstName>Jun</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jun Li</PI_FULL_NAME>
<EmailAddress>jun.li@qc.cuny.edu</EmailAddress>
<PI_PHON>7189973500</PI_PHON>
<NSF_ID>000760317</NSF_ID>
<StartDate>11/18/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>CUNY Queens College</Name>
<CityName>Flushing</CityName>
<CountyName>QUEENS</CountyName>
<ZipCode>113671575</ZipCode>
<PhoneNumber>7189975400</PhoneNumber>
<StreetAddress>65 30 Kissena Blvd</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>619346146</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073268849</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[CUNY Queens College]]></Name>
<CityName>Flushing</CityName>
<CountyName>QUEENS</CountyName>
<StateCode>NY</StateCode>
<ZipCode>113671575</ZipCode>
<StreetAddress><![CDATA[65 30 Kissena Blvd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~372267</FUND_OBLG>
<FUND_OBLG>2020~16000</FUND_OBLG>
</Award>
</rootTag>
