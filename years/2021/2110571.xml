<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Adaptive Neural Networks for Partial Differential Equations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2021</AwardEffectiveDate>
<AwardExpirationDate>05/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>330000.00</AwardTotalIntnAmount>
<AwardAmount>330000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuliya Gorb</SignBlockName>
<PO_EMAI>ygorb@nsf.gov</PO_EMAI>
<PO_PHON>7032922113</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Neural networks have achieved astonishing performance in computer vision, natural language processing, and many other artificial intelligence tasks. Despite their great successes in many practical applications, it is widely accepted that approximation properties of neural networks are not yet well-understood. This project would have the potential to understand why and how neural networks work, particularly, why a neural network is much better than other functional classes for computer simulations of problems with singularities and discontinuities. The self-adaptive neural network method developed in this project would have the potential to dramatically improve the computational capabilities for computer simulations of complex physical, biological, and human-engineered systems exhibiting computational difficulties. Understanding of the role of neural network width and depth in approximation and the new ideas gained in this project would have significant impacts on many other artificial intelligence tasks such as transfer learning, online control, and pattern recognition. Training of at least one graduate on the topics of the proposed research is expected. &lt;br/&gt;&lt;br/&gt;A fundamental, open question in machine learning is on how to design the architecture of neural networks, in terms of their width and depth, in order to approximate functions or numerically solve partial differential equations accurately and efficiently. Addressing this issue for applications to computationally challenging problems is the focus of this project. More precisely, for any given partial differential equation, this project is going to develop an adaptive neuron enhancement (ANE) method that adaptively constructs a neural network with a nearly minimum number of neurons and parameters such that its approximation accuracy is within the prescribed tolerance. A key component of developing ANE methods is an error indicator for determining the number of new neurons to be added at either the current or next layer.  This project plans to develop efficient indicators by studying the role of additional neurons in the current and next layers. This knowledge is crucial for designing efficient and accurate ANE methods, but is generally not provided by the standard a priori error estimates. The exceptional approximation powers of neural networks come with a price: the procedure for determining the values of the parameters is now a problem in nonlinear optimization even if the underlying partial differential equation is linear. Nonlinear optimizations usually have many solutions, and the desired one is obtained only if one starts from a close enough first approximation. The ANE method to be developed in this project is a good continuation process. This project will focus on how to initialize parameters of new neurons at each adaptive stage of the ANE method. This will be done by analyzing the physical meanings of neurons at each layer.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/03/2021</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2110571</AwardID>
<Investigator>
<FirstName>Zhiqiang</FirstName>
<LastName>Cai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhiqiang Cai</PI_FULL_NAME>
<EmailAddress>zcai@math.purdue.edu</EmailAddress>
<PI_PHON>7654941921</PI_PHON>
<NSF_ID>000418047</NSF_ID>
<StartDate>06/03/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Min</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Min Liu</PI_FULL_NAME>
<EmailAddress>liu66@purdue.edu</EmailAddress>
<PI_PHON>7655887866</PI_PHON>
<NSF_ID>000844196</NSF_ID>
<StartDate>06/03/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072067</ZipCode>
<StreetAddress><![CDATA[150N University Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~330000</FUND_OBLG>
</Award>
</rootTag>
