<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Physically-Based Learning for Shape, Lighting and Material in Complex Indoor Scenes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2021</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Indoor scenes involve visible and invisible light sources interacting with spatially-varying materials and shapes, together with interreflections and shadows, to produce images that display complex local variations. Applications such as augmented reality require editing an indoor scene to insert new objects, changing materials in some parts of the scene, or visualizing a room under a different illumination. Estimating those factors of image formation constitutes the problem of inverse rendering, which is especially ill-posed when the input is only a few images of an indoor scene acquired with a commodity camera. While traditional measurement-based methods need expensive devices and do not scale to large-scale scenes, conventional learning methods suffer from a lack of data and expressive power to handle the above complex visual effects. This research addresses the longstanding computer vision and graphics challenge of inverse rendering through a holistic approach of modeling complex materials and illumination, developing novel physically inspired deep networks and generating large-scale training data. The project will have a transformative effect on industry and society through photorealistic augmented reality and image editing applications such as view synthesis, object insertion, material, and light source editing, with accurate shadows and interreflections. The project also incorporates experiential insights from the above technological advances into college and K-12 education through a new interdisciplinary curriculum in computer vision and computer graphics.&lt;br/&gt;&lt;br/&gt;This project develops physically based deep networks that incorporate the inductive bias of image formation to achieve generalizable representations that estimate shape, material and especially lighting with unprecedented detail, by accounting for visibilities in a single image, consistency in multiple images and complex light transport in challenging or dynamic scenes. It designs novel modules such as neural rendering layers and reflectance volumes that incorporate the domain knowledge of image formation. It achieves scalability through parsimonious representations that retain descriptive power while generalizing to complex lighting effects such as refraction, scattering and dynamic light transport. It develops large-scale training datasets with realistic spatially varying lighting and complex high-quality material.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/28/2021</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2110409</AwardID>
<Investigator>
<FirstName>Ravi</FirstName>
<LastName>Ramamoorthi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ravi Ramamoorthi</PI_FULL_NAME>
<EmailAddress>ravir@cs.ucsd.edu</EmailAddress>
<PI_PHON>5102258896</PI_PHON>
<NSF_ID>000486826</NSF_ID>
<StartDate>07/28/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Manmohan</FirstName>
<LastName>Chandraker</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Manmohan K Chandraker</PI_FULL_NAME>
<EmailAddress>mkchandraker@eng.ucsd.edu</EmailAddress>
<PI_PHON>8584010407</PI_PHON>
<NSF_ID>000727279</NSF_ID>
<StartDate>07/28/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930404</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~500000</FUND_OBLG>
</Award>
</rootTag>
