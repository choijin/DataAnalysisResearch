<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Enabling On-Device Bayesian Neural Network Training via An Integrated Architecture-System Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2021</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>499999.00</AwardTotalIntnAmount>
<AwardAmount>499999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Deep-learning-based AI technologies, such as deep convolutional neural networks (DNNs), have recently achieved amazing success in numerous applications. However, DNN models can become unreliable due to the uncertainty in data, hence giving a false judgement and possibly incurring a disaster. To address this issue, Bayesian Neural Networks (BNNs), which possess a property of uncertainty estimation, have been increasingly employed in a wide range of real-world AI applications that demand reliable and robust decisions (e.g., self-driving, rescue robots, medical image diagnosis). Recently, training models in a distributed manner at mobile devices (e.g., federated learning) has become a popular and efficient training paradigm that achieves stronger data privacy, reduced data traffic and less response time compared to the cloud-centric training. Especially, federate learning on BNN models has received extensive attention. Since on-device learning is the essential step towards distributed BNN training, this project aims to enable fast and energy-efficient BNN training locally on resource-limited mobile devices. The project will have transformative impact on adopting AI technologies in many emergent domains that require swift, low-power, and most importantly, robust and reliable model training on edge and mobile devices (such as automation, medical, transportation, oil and gas, business, Internet-of-Things, and so on), helping to better explore the world and making everyday living and working more convenient and efficient. This project will also contribute to society through engaging under-represented groups, outreach to high-school students, curriculum development on machine learning, and disseminating research infrastructure for education and training.&lt;br/&gt;&lt;br/&gt;BNNs can be viewed as a probabilistic model where each model parameter (i.e., weight) is a probability distribution. There are two major BNN training approaches, which train the model via weight sampling (called Gaussian-based BNNs (GBNNs)) on the one hand and output feature map sampling (called Dropout-based Bayesian Convolutional NNs (DBCNNs)) on the other. Existing DNN accelerators are oblivious to the BNN stochastic training processes and achieving low training efficiency, and a uniform BNN accelerator is impractical due to the different sampling methods applied in GBNNs and DBCNNs. The investigators are developing a synergetic architecture-system research program that exploits and leverages the unique features of GBNNs and DBCNNs to achieve an order-of-magnitude reduction on training cost while still maintaining the model robustness, thus enabling on-device BNN training. The program comprises three objectives. (1) To enable on-device GBNN training, the investigators are dynamically eliminating/reducing the Gaussian random variables and intermediate data induced data movements, and furthermore, boosting the GBNN training speed by resorting to multi-chip-module-based DNN accelerators. (2) To enable on-device DBCNN training, the investigators are dynamically exploiting and eliminating the computation and data movement redundancy buried in the stochastic training processes. (3) The investigators are studying the efficient on-device training for mobile AI devices that involved in the distributed training for both global GBNN and DBCNN models and evaluating the overall system.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/27/2021</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2130688</AwardID>
<Investigator>
<FirstName>Xin</FirstName>
<LastName>Fu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xin Fu</PI_FULL_NAME>
<EmailAddress>xfu8@central.uh.edu</EmailAddress>
<PI_PHON>7137436104</PI_PHON>
<NSF_ID>000583715</NSF_ID>
<StartDate>08/27/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036837920</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042916627</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>772042015</ZipCode>
<StreetAddress><![CDATA[4800 Calhoun Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~499999</FUND_OBLG>
</Award>
</rootTag>
