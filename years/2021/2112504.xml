<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: SMALL: kNN methods for functional estimation and machine learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2021</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Scott Acton</SignBlockName>
<PO_EMAI>sacton@nsf.gov</PO_EMAI>
<PO_PHON>7032922124</PO_PHON>
</ProgramOfficer>
<AbstractNarration>K Nearest Neighbor (kNN) methods are a class of nonparametric statistical methods. Compared with other methods, kNN methods have several advantages. In particular, kNN methods can automatically adapt to any continuous underlying functions without relying on any specific models. In addition, kNN methods are simple to use and do not require too much parameter tuning. Furthermore, kNN methods have achieved excellent empirical results. Due to these advantages, kNN methods are widely used in a large variety of statistical problems, including functional-estimation and machine-learning problems. However, the understanding of theoretical properties of kNN methods for these applications is incomplete. As the result, there is a pressing need to investigate the theoretical properties of kNN methods. By addressing the main sources of estimation errors identified by these investigations, one can design improved kNN methods that have better performance.&lt;br/&gt;&lt;br/&gt;In this project, the investigator is investigating: 1) theoretical properties of kNN methods in functional estimation and machine learning problems; and 2) the design of improved kNN algorithms with better performance for these applications. Despite many existing studies, several theoretical problems still need further investigation. In particular: 1) The theoretical convergence rates of kNN methods for functional estimations, classification and regression, etc., are still not fully established; 2) For many applications of practical interests, it is not clear under what conditions this type of methods is optimal; 3) Most of the existing analysis of kNN methods rely on availability of independent and identically distributed training data, while in certain applications (such as those involving Markov chains) the available data are dependent; 4) While there are many applications and analysis of kNN methods for supervised learning, the applications and analysis of kNN methods for reinforcement learning etc. are limited. To address these challenges, this project is focusing on two interconnected thrusts. In the first thrust, the project is investigating the application of kNN methods in the estimation of information-theoretic functionals, including entropy, mutual information, Kullback-Leibler divergence, directed information, etc. In the second thrust, the project is designing and analyzing kNN based algorithms for machine learning problems, including supervised learning, nonconvex optimization and reinforcement learning.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/28/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/28/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2112504</AwardID>
<Investigator>
<FirstName>Lifeng</FirstName>
<LastName>Lai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lifeng Lai</PI_FULL_NAME>
<EmailAddress>lflai@ucdavis.edu</EmailAddress>
<PI_PHON>5307527978</PI_PHON>
<NSF_ID>000541215</NSF_ID>
<StartDate>05/28/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Davis</Name>
<CityName>Davis</CityName>
<ZipCode>956186134</ZipCode>
<PhoneNumber>5307547700</PhoneNumber>
<StreetAddress>OR/Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1850 Research Park Dr., Ste 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047120084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, DAVIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Davis]]></Name>
<CityName>Davis</CityName>
<StateCode>CA</StateCode>
<ZipCode>956186134</ZipCode>
<StreetAddress><![CDATA[1 Shields Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~500000</FUND_OBLG>
</Award>
</rootTag>
