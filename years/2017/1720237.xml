<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Operator Splitting Methods: Certificates and Second-Order Acceleration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>205000.00</AwardTotalIntnAmount>
<AwardAmount>205000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project is centered on development of improved numerical algorithms for application to large-scale systems that include, for example, signal/image/video reconstruction and processing, bioinformatics, and automated learning or mining of information from very large data sets. Operator splitting is a class of methods that decomposes a difficult problem into simple sub-problems.  Within the past decade, operator splitting methods gained popularity due to the growing demand to handle ever-larger models. For example, signal processing and machine learning applications often have multiple parts that are easy to handle separately but are very challenging when combined. Ideas from operator splitting have led to efficient algorithms for broad classes of objective functions that are used to define the underlying systems. There is still, however, much to be done to handle complex situations. Through further development of operator splitting techniques, this research has the potential to provide efficient and stable approaches to solve a yet wider class of challenging problems. The project also includes educational impact through the development of courses, presentation of seminars, and graduate student training opportunities.&lt;br/&gt;&lt;br/&gt;The principal investigator intends to design and implement algorithms that improve the speed and stability of operator splitting methods. This project aims to extend the principle of operator splitting in two ways. First, operator splitting algorithms will be introduced that recognize infeasible and feasible-but-unbounded optimization problems, as well as those that have finite optimal values but unattainable solutions. Such pathological problems are not rare and cripple existing techniques. The new algorithms will address these pathologies and make future solvers more robust. Second, by incorporating second-order information in a novel fashion, the project will address two significant drawbacks of operator splitting algorithms. These are the slow tail convergence, and the sensitivity to severe problem conditions. Techniques to ensure global convergence will be developed. Because operator splitting is a high-level abstraction, the results of the project will apply to a broad range of numerical methods that arise in science and engineering.</AbstractNarration>
<MinAmdLetterDate>06/07/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1720237</AwardID>
<Investigator>
<FirstName>Wotao</FirstName>
<LastName>Yin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wotao Yin</PI_FULL_NAME>
<EmailAddress>wotaoyin@math.ucla.edu</EmailAddress>
<PI_PHON>3108257764</PI_PHON>
<NSF_ID>000180296</NSF_ID>
<StartDate>06/07/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951555</ZipCode>
<StreetAddress><![CDATA[520 Portola Plaza, 6363 MSB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~205000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The focus of this project is splitting methods, which are computational approaches that generate and solve smaller and simpler subproblems so that a larger and complicated problem can be solvered. Within the past decade, the demand to solve ever larger problems grew quickly, operator splitting has been found surprisingly helpful in developing algorithms for problems with nonsmooth objective functions, various constraints, and distributed data. Solving many of these problems is tractable only by splitting.&nbsp;</p> <p>The project proposes&nbsp;to apply operator splitting algorithm to produce certificates for infeasible or unboundedconvex optimization problems, as well as those that have finite but unattainable optimal values. The project also&nbsp;comes up with techniques that address the need for solving large scale optimization problems in decentralized networks (which have low latency and are robust to single-point failures), the finite-sum form (which arise in the training of machine learning models), and the online setting (where the data arrive in a streaming fashion).</p> <p>A selection of the results under this project are as follows.</p> <ul> <li>Liu, Yanli, Ernest K. Ryu, and Wotao Yin. 2019. &ldquo;A New Use of Douglas-Rachford Splitting and ADMM for Identifying Infeasible, Unbounded, and Pathological Conic Programs.&rdquo; Mathematical Programming 177: 225&ndash;53. https://doi.org/10.1007/s10107-018-1265-5.</li> <li> <div class="csl-bib-body"> <div class="csl-entry">Ryu, Ernest K., Yanli Liu, and Wotao Yin. 2019. &ldquo;Douglas-Rachford Splitting for Pathological Convex Optimization.&rdquo; Computational Optimization and Applications 74 (3): 747&ndash;78. https://doi.org/10.1007/s10589-019-00130-9.</div> </div> </li> <li>Hannah, Robert, and Wotao Yin. 2018. &ldquo;On Unbounded Delays in Asynchronous Parallel Fixed-Point Algorithms.&rdquo; Journal of Scientific Computing 76 (1): 299&ndash;326. https://doi.org/10.1007/s10915-017-0628-z.</li> <li>Zeng, Jinshan, and Wotao Yin. 2018. &ldquo;On Nonconvex Decentralized Gradient Descent.&rdquo; IEEE Transactions on Signal Processing 66 (11): 2834&ndash;48. https://doi.org/10.1109/TSP.2018.2818081.</li> <li>Liu, Jialin, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. 2018. &ldquo;First and Second Order Methods for Online Convolutional Dictionary Learning.&rdquo; SIAM Journal on Imaging Sciences 11 (2): 1589&ndash;1628. https://doi.org/10.1137/17M1145689.</li> <li>Wang, Yu, Wotao Yin, and Jinshan Zeng. 2019. &ldquo;Global Convergence of ADMM in Nonconvex Nonsmooth Optimization.&rdquo; Journal of Scientific Computing 78 (1): 29&ndash;63. https://doi.org/10.1007/s10915-018-0757-z.</li> <li>Chow, Yat Tin, J&eacute;r&ocirc;me Darbon, Stanley Osher, and Wotao Yin. 2019. &ldquo;Algorithm for Overcoming the Curse of Dimensionality for State-Dependent Hamilton-Jacobi Equations.&rdquo; Journal of Computational Physics 387: 376&ndash;409. https://doi.org/10.1016/j.jcp.2019.01.051.</li> <li>Chen, Xiaohan, Jialin Liu, Zhangyang Wang, and Wotao Yin. 2018. &ldquo;Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds.&rdquo; In Advances in Neural Information Processing Systems (NIPS), edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 9061&ndash;9071. http://papers.nips.cc/paper/8120-theoretical-linear-convergence-of-unfolded-ista-and-its-practical-weights-and-thresholds.pdf.</li> <li>Sun, Tao, Yuejiao Sun, and Wotao Yin. 2018. &ldquo;On Markov Chain Gradient Descent.&rdquo; In Advances in Neural Information Processing Systems (NIPS), edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 9896&ndash;9905. http://papers.nips.cc/paper/8195-on-markov-chain-gradient-descent.pdf.</li> <li>Hannah, Robert, Yanli Liu, Daniel O&rsquo;Connor, and Wotao Yin. 2018. &ldquo;Breaking the Span Assumption Yields Fast Finite-Sum Minimization.&rdquo; In Advances in Neural Information Processing Systems 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 2312&ndash;2321. Curran Associates, Inc. http://papers.nips.cc/paper/7499-breaking-the-span-assumption-yields-fast-finite-sum-minimization.pdf.</li> <li>Mao, X., Y. Gu, and W. Yin. 2019. &ldquo;Walk Proximal Gradient: An Energy-Efficient Algorithm for Consensus Optimization.&rdquo; IEEE Internet of Things Journal 6 (2): 2048&ndash;60. https://doi.org/10.1109/JIOT.2018.2875057.&nbsp;</li> </ul> <p>Software codes of some of these outcomes are open &shy;source. All papersand software codes are posted on the PI's homepage and arXiv.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/29/2020<br>      Modified by: Wotao&nbsp;Yin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The focus of this project is splitting methods, which are computational approaches that generate and solve smaller and simpler subproblems so that a larger and complicated problem can be solvered. Within the past decade, the demand to solve ever larger problems grew quickly, operator splitting has been found surprisingly helpful in developing algorithms for problems with nonsmooth objective functions, various constraints, and distributed data. Solving many of these problems is tractable only by splitting.   The project proposes to apply operator splitting algorithm to produce certificates for infeasible or unboundedconvex optimization problems, as well as those that have finite but unattainable optimal values. The project also comes up with techniques that address the need for solving large scale optimization problems in decentralized networks (which have low latency and are robust to single-point failures), the finite-sum form (which arise in the training of machine learning models), and the online setting (where the data arrive in a streaming fashion).  A selection of the results under this project are as follows.  Liu, Yanli, Ernest K. Ryu, and Wotao Yin. 2019. "A New Use of Douglas-Rachford Splitting and ADMM for Identifying Infeasible, Unbounded, and Pathological Conic Programs." Mathematical Programming 177: 225&ndash;53. https://doi.org/10.1007/s10107-018-1265-5.   Ryu, Ernest K., Yanli Liu, and Wotao Yin. 2019. "Douglas-Rachford Splitting for Pathological Convex Optimization." Computational Optimization and Applications 74 (3): 747&ndash;78. https://doi.org/10.1007/s10589-019-00130-9.   Hannah, Robert, and Wotao Yin. 2018. "On Unbounded Delays in Asynchronous Parallel Fixed-Point Algorithms." Journal of Scientific Computing 76 (1): 299&ndash;326. https://doi.org/10.1007/s10915-017-0628-z. Zeng, Jinshan, and Wotao Yin. 2018. "On Nonconvex Decentralized Gradient Descent." IEEE Transactions on Signal Processing 66 (11): 2834&ndash;48. https://doi.org/10.1109/TSP.2018.2818081. Liu, Jialin, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. 2018. "First and Second Order Methods for Online Convolutional Dictionary Learning." SIAM Journal on Imaging Sciences 11 (2): 1589&ndash;1628. https://doi.org/10.1137/17M1145689. Wang, Yu, Wotao Yin, and Jinshan Zeng. 2019. "Global Convergence of ADMM in Nonconvex Nonsmooth Optimization." Journal of Scientific Computing 78 (1): 29&ndash;63. https://doi.org/10.1007/s10915-018-0757-z. Chow, Yat Tin, J&eacute;r&ocirc;me Darbon, Stanley Osher, and Wotao Yin. 2019. "Algorithm for Overcoming the Curse of Dimensionality for State-Dependent Hamilton-Jacobi Equations." Journal of Computational Physics 387: 376&ndash;409. https://doi.org/10.1016/j.jcp.2019.01.051. Chen, Xiaohan, Jialin Liu, Zhangyang Wang, and Wotao Yin. 2018. "Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds." In Advances in Neural Information Processing Systems (NIPS), edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 9061&ndash;9071. http://papers.nips.cc/paper/8120-theoretical-linear-convergence-of-unfolded-ista-and-its-practical-weights-and-thresholds.pdf. Sun, Tao, Yuejiao Sun, and Wotao Yin. 2018. "On Markov Chain Gradient Descent." In Advances in Neural Information Processing Systems (NIPS), edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 9896&ndash;9905. http://papers.nips.cc/paper/8195-on-markov-chain-gradient-descent.pdf. Hannah, Robert, Yanli Liu, Daniel O’Connor, and Wotao Yin. 2018. "Breaking the Span Assumption Yields Fast Finite-Sum Minimization." In Advances in Neural Information Processing Systems 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 2312&ndash;2321. Curran Associates, Inc. http://papers.nips.cc/paper/7499-breaking-the-span-assumption-yields-fast-finite-sum-minimization.pdf. Mao, X., Y. Gu, and W. Yin. 2019. "Walk Proximal Gradient: An Energy-Efficient Algorithm for Consensus Optimization." IEEE Internet of Things Journal 6 (2): 2048&ndash;60. https://doi.org/10.1109/JIOT.2018.2875057.    Software codes of some of these outcomes are open &shy;source. All papersand software codes are posted on the PI's homepage and arXiv.          Last Modified: 08/29/2020       Submitted by: Wotao Yin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
