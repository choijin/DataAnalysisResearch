<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Deterministic Sampling through Energy Minimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>199999.00</AwardTotalIntnAmount>
<AwardAmount>199999</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project aims at developing optimal deterministic methods for statistical sampling / statistical observations, in contrast to commonly-used random sampling methods such as Monte Carlo (MC) and Markov Chain Monte Carlo (MCMC). The MC/MCMC methods have revolutionized statistics, allowing statisticians to model and solve complex and high-dimensional problems that would have been intractable using conventional techniques. One drawback of these methods is that very many observations or data samples are needed due to the slow convergence rate inherent in random sampling. This becomes an issue when the sampling is expensive. The deterministic method under study in this project attempts to overcome this problem by sampling points more intelligently, so that the same information provided by a random sample can be obtained with fewer deterministic samples. This can significantly cut down the cost of sampling and subsequent computations. The method under development has applications in many fields, such as uncertainty quantification, computer experiments, and machine learning.&lt;br/&gt;&lt;br/&gt;The project aims to provide deterministic samples obtained through the minimization of certain energies. The goal is to use carefully developed optimization techniques to reduce the number of expensive evaluations of a probability distribution, thereby reducing the overall computational cost. Furthermore, the deterministic sample provides a much better representative set of points for the distribution, which can further reduce the cost of subsequent computations involving integrals. Compared to the existing Quasi-Monte Carlo methods, which are mostly developed for sampling from the uniform hypercube, the methods under study are much more general and can be used to directly sample from any probability distribution. Two methods for deterministic sampling will be investigated. The first method, known as minimum energy designs, is useful when the probability density is expensive to evaluate. The second method, known as support points, is useful when the integrand is expensive but sampling from the probability density is easy. The minimum energy design possesses an important property: its empirical distribution asymptotically converges to the target distribution. This is a property not shared by some of the competing representative point sets in the literature, such as principal points. On the other hand, support points are obtained by minimizing an energy distance which is used for goodness-of-fit testing. In this light, support points can be viewed as point sets that optimally compact a continuous probability distribution. The project focuses on developing efficient optimization methods for these energy functions using as few function evaluations as possible, and improving the distributional properties of the point sets so that they can be used in problems where MC/MCMC methods are computationally impracticable.</AbstractNarration>
<MinAmdLetterDate>04/19/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/14/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1712642</AwardID>
<Investigator>
<FirstName>Roshan</FirstName>
<LastName>Joseph</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Roshan V Joseph</PI_FULL_NAME>
<EmailAddress>roshan@isye.gatech.edu</EmailAddress>
<PI_PHON>4048940056</PI_PHON>
<NSF_ID>000330281</NSF_ID>
<StartDate>04/19/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~64923</FUND_OBLG>
<FUND_OBLG>2018~66750</FUND_OBLG>
<FUND_OBLG>2019~68326</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Monte Carlo methods are ubiquitous in all fields of science and engineering. They have revolutionized the field of statistics, especially Bayesian statistics. Using Monte Carlo (MC) and Markov chain Monte Carlo (MCMC) methods, statisticians are able to model and solve complex and high-dimensional Bayesian problems, which otherwise would not have been possible with conventional methods such as numerical quadrature. However, MC/MCMC methods require large samples from the probability distribution (unnormalized posterior), which are difficult to obtain when the density is expensive to evaluate. Since statistical modeling is iterative in nature, statisticians need to go back and forth between the models and data several times and therefore, the computationally expensive sampling methods can become a hindrance to model development and refinement. Moreover, once the sample is obtained, they are used for computing certain posterior summaries or predictions, which can also be computationally expensive. This project has developed some deterministic sampling methods to reduce the sampling effort.</p> <p>Joseph, Wang, Gu, Lyu, and Tuo (2019) developed an efficient algorithm to generate a deterministic sample using minimum energy design criterion. An example is shown in Figure 1, where 109 minimum energy design points are generated using only 654 evaluations of the unnormalized posterior. In the same problem, MCMC would require more than 3,000 samples to get the same quality of approximation to the posterior, thus can be much more expensive than the minimum energy design. The minimum energy designs can be generated using the R package mined.</p> <p>In another paper, Mak and Joseph (2019), developed projected support points which can be used for reducing big data to a smaller size. This can save storage and future computational costs. For example, thinning is a common method used in MCMC to reduce the number of samples, but it uses a simple strategy such as keeping one sample after generating every 100 MCMC samples. Instead, projected support points can be used to optimally reduce the MCMC samples. Compared to the recently developed support points method (Mak and Joseph 2018, AoS), projected support points maintain the marginal distributions in a much better way. An example is shown in Figure 2. Projected support points have a much broader application than thinning in Bayesian statistics, which can be used to optimally reduce any high-dimensional and big data.</p> <p>The project also developed many other deterministic sampling methods with applications to uncertainty quantification, robust parameter design, and computer experiments.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/26/2020<br>      Modified by: Roshan&nbsp;V&nbsp;Joseph</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1712642/1712642_10482450_1603732288339_banana1-6--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1712642/1712642_10482450_1603732288339_banana1-6--rgov-800width.jpg" title="Figure 1. Minimum Energy Design Algorithm"><img src="/por/images/Reports/POR/2020/1712642/1712642_10482450_1603732288339_banana1-6--rgov-66x44.jpg" alt="Figure 1. Minimum Energy Design Algorithm"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The figure shows the progression of the minimum energy design algorithm for generating 109 samples in six steps from a banana-shaped posterior distribution.</div> <div class="imageCredit">Roshan V Joseph</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Roshan&nbsp;V&nbsp;Joseph</div> <div class="imageTitle">Figure 1. Minimum Energy Design Algorithm</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1712642/1712642_10482450_1603731844359_PSP--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1712642/1712642_10482450_1603731844359_PSP--rgov-800width.jpg" title="Figure 2 Support Points and Projected Support Points"><img src="/por/images/Reports/POR/2020/1712642/1712642_10482450_1603731844359_PSP--rgov-66x44.jpg" alt="Figure 2 Support Points and Projected Support Points"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The plot shows a comparison of support points and projected support points from a two-dimensional Beta (2,4) distribution.</div> <div class="imageCredit">Simon Mak</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Roshan&nbsp;V&nbsp;Joseph</div> <div class="imageTitle">Figure 2 Support Points and Projected Support Points</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Monte Carlo methods are ubiquitous in all fields of science and engineering. They have revolutionized the field of statistics, especially Bayesian statistics. Using Monte Carlo (MC) and Markov chain Monte Carlo (MCMC) methods, statisticians are able to model and solve complex and high-dimensional Bayesian problems, which otherwise would not have been possible with conventional methods such as numerical quadrature. However, MC/MCMC methods require large samples from the probability distribution (unnormalized posterior), which are difficult to obtain when the density is expensive to evaluate. Since statistical modeling is iterative in nature, statisticians need to go back and forth between the models and data several times and therefore, the computationally expensive sampling methods can become a hindrance to model development and refinement. Moreover, once the sample is obtained, they are used for computing certain posterior summaries or predictions, which can also be computationally expensive. This project has developed some deterministic sampling methods to reduce the sampling effort.  Joseph, Wang, Gu, Lyu, and Tuo (2019) developed an efficient algorithm to generate a deterministic sample using minimum energy design criterion. An example is shown in Figure 1, where 109 minimum energy design points are generated using only 654 evaluations of the unnormalized posterior. In the same problem, MCMC would require more than 3,000 samples to get the same quality of approximation to the posterior, thus can be much more expensive than the minimum energy design. The minimum energy designs can be generated using the R package mined.  In another paper, Mak and Joseph (2019), developed projected support points which can be used for reducing big data to a smaller size. This can save storage and future computational costs. For example, thinning is a common method used in MCMC to reduce the number of samples, but it uses a simple strategy such as keeping one sample after generating every 100 MCMC samples. Instead, projected support points can be used to optimally reduce the MCMC samples. Compared to the recently developed support points method (Mak and Joseph 2018, AoS), projected support points maintain the marginal distributions in a much better way. An example is shown in Figure 2. Projected support points have a much broader application than thinning in Bayesian statistics, which can be used to optimally reduce any high-dimensional and big data.  The project also developed many other deterministic sampling methods with applications to uncertainty quantification, robust parameter design, and computer experiments.          Last Modified: 10/26/2020       Submitted by: Roshan V Joseph]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
