<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SPX: Secure, Highly-Parallel Training of Deep Neural Networks in the Cloud Using General-Purpose  Shared-Memory Platforms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Society is beginning to witness an explosion in the use of Deep Neural Networks (DNNs), with major impacts on many facets of human life, including health, finances, family life, and entertainment. To train DNNs, practitioners have preferred to use GPUs and, recently, specialized hardware accelerators.  Despite constituting the bulk of a data center?s compute resources, general-purpose shared-memory multiprocessors have been regarded as unattractive platforms. In this project, the Principal Investigators (PIs) think that these platforms have high potential. Consequently, this project will develop new techniques to dramatically improve shared-memory multiprocessor performance in training DNNs.  Already, shared-memory servers are compelling for several reasons: they can support a high-degree of parallelism, are general-purpose and easy to program, and provide flexible, fine-grain inter-core communication.  However, efficiently using shared-memory servers to train DNNs imposes &lt;br/&gt;significant challenges. First, fine-grain synchronization is still expensive, and latencies are non-trivial. In addition, when DNN training moves to an environment with multiple users sharing the same physical shared-memory platform in the cloud, privacy and integrity become major concerns.&lt;br/&gt;&lt;br/&gt;To overcome these challenges, this project will synergistically address architecture and security issues.  On the architecture side, it will augment a highly-parallel shared-memory server with support for synchronization, data movement, data sharing, and DNN sparsity structuring.  On the security side, it will investigate how shared-memory servers create novel privacy and integrity threats (for example, leaking the DNN?s sparse structure and forcing incorrect model generation), and how to defend against those threats.  The project?s broader impact is to help enable ?neural network training for everyone,? by making a ubiquitous and easy-to-program platform a viable and safe target for running these important, emerging workloads.</AbstractNarration>
<MinAmdLetterDate>08/30/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/30/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1725734</AwardID>
<Investigator>
<FirstName>Josep</FirstName>
<LastName>Torrellas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Josep Torrellas</PI_FULL_NAME>
<EmailAddress>torrellas@cs.uiuc.edu</EmailAddress>
<PI_PHON>2172444148</PI_PHON>
<NSF_ID>000488177</NSF_ID>
<StartDate>08/30/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Fletcher</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher Fletcher</PI_FULL_NAME>
<EmailAddress>cwfletch@illinois.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000743178</NSF_ID>
<StartDate>08/30/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>042Y</Code>
<Text>PPoSS-PP of Scalable Systems</Text>
</ProgramElement>
<ProgramReference>
<Code>026Z</Code>
<Text>NSCI: National Strategic Computing Initi</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this proposal, the Principal Investigators (PIs) have focused on designinghigh-performance and secure general-purpose computers for performing deep&nbsp;neural network (DNN) training. General-purpose computers such as CPUs haveseveral advantages: they are easy to program, exhibit a high degree of&nbsp;parallelism, and support flexible, fine-grain communication between cores.&nbsp;Hence, applications can be written with many irregular tasks synchronizing&nbsp;at fine granularity.</p> <p>The technical outcome of this work has been the design of two mechanisms to&nbsp;speed-up the training of DNNs on processors, and the discovery of three attacks&nbsp;on DNN training (and the outline of possible defenses).&nbsp;</p> <p>The PIs speed-up the execution of DNN training by skipping operations when&nbsp;there are zeros involved in the computations. The presence of zeros, called&nbsp;Sparsity, is very common, and such zeros cause operations to be ineffectual(that is, to have no effect). The PIs have designed changes to the DNN training&nbsp;algorithms that skip such ineffectual operations. They have also designed new&nbsp;processor hardware that, automatically and without requiring any program change,&nbsp;skip ineffectual operations.</p> <p>The attacks discovered by the PIs operate as follows. In one attack, calledCache Telepathy, the attacker observes the cache hierarchy use during the&nbsp;training operation, and is able to obtain the architecture (shape and other&nbsp;parameters) of the DNN. In the second attack, called Game of Threads, theattacker changes the scheduling of the threads that participate in thetraining of a DNN. With this technique, the attacker is able to change the&nbsp;training outcome, resulting in either a degradation of the training accuracy&nbsp;or an incorrect training outcome. Finally, in the third attack, calledMicroScope, the attacker is able to repeatedly re-execute the same instructionsof the victim program without the victim noticing it. As a result, the attackercan fully characterize the instructions executed and, therefore, leak&nbsp;information about the program.</p> <p>This work has been done in close collaboration with researchers at Intel&nbsp;Corporation. It is hoped that the designs and attacks/defenses proposed willhave an impact on Intel products. In addition, the work has funded 8 graduatestudents, of which two already completed their PhD.</p><br> <p>            Last Modified: 01/24/2021<br>      Modified by: Josep&nbsp;Torrellas</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this proposal, the Principal Investigators (PIs) have focused on designinghigh-performance and secure general-purpose computers for performing deep neural network (DNN) training. General-purpose computers such as CPUs haveseveral advantages: they are easy to program, exhibit a high degree of parallelism, and support flexible, fine-grain communication between cores. Hence, applications can be written with many irregular tasks synchronizing at fine granularity.  The technical outcome of this work has been the design of two mechanisms to speed-up the training of DNNs on processors, and the discovery of three attacks on DNN training (and the outline of possible defenses).   The PIs speed-up the execution of DNN training by skipping operations when there are zeros involved in the computations. The presence of zeros, called Sparsity, is very common, and such zeros cause operations to be ineffectual(that is, to have no effect). The PIs have designed changes to the DNN training algorithms that skip such ineffectual operations. They have also designed new processor hardware that, automatically and without requiring any program change, skip ineffectual operations.  The attacks discovered by the PIs operate as follows. In one attack, calledCache Telepathy, the attacker observes the cache hierarchy use during the training operation, and is able to obtain the architecture (shape and other parameters) of the DNN. In the second attack, called Game of Threads, theattacker changes the scheduling of the threads that participate in thetraining of a DNN. With this technique, the attacker is able to change the training outcome, resulting in either a degradation of the training accuracy or an incorrect training outcome. Finally, in the third attack, calledMicroScope, the attacker is able to repeatedly re-execute the same instructionsof the victim program without the victim noticing it. As a result, the attackercan fully characterize the instructions executed and, therefore, leak information about the program.  This work has been done in close collaboration with researchers at Intel Corporation. It is hoped that the designs and attacks/defenses proposed willhave an impact on Intel products. In addition, the work has funded 8 graduatestudents, of which two already completed their PhD.       Last Modified: 01/24/2021       Submitted by: Josep Torrellas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
