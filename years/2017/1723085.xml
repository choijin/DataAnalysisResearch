<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Beyond With-replacement Sampling for Large-Scale Data Analysis and Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2017</AwardEffectiveDate>
<AwardExpirationDate>12/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>125001.00</AwardTotalIntnAmount>
<AwardAmount>125001</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Advances in sensing and processing technologies, communication capabilities and smart devices have enabled deployment of systems where a massive amount of data is collected to make decisions. Many key problems of interest for analyzing and processing big data result in large-scale optimization problems.  For a core, very widely used optimization method, which is efficient for such problems where the data points are sampled and processed in a sequential manner, there is a large gap between the theory and practice of this method. This project is about filling this gap by providing novel performance guarantees relevant to practical problems as well as developing novel and faster variants of the optimization method. The methods and techniques developed under the scope of this project will contribute to the efficiency and mathematical foundations of optimization algorithms targeted for big data challenges, contributing to more efficient decision making for a wide variety of large-scale data analysis problems. &lt;br/&gt;&lt;br/&gt;Incremental gradient (IG) is the core, very widely used optimization method mentioned above and subsumes popular optimization methods in data analysis and machine learning practice such as stochastic gradient descent, randomized coordinate descent and Kaczmarz methods. Various performance guarantees for IG are available if data points are sampled with replacement in an independent identically distributed (i.i.d.) manner. However, these are not helpful in practical scenarios: In practice, data is often sampled in a non-i.i.d fashion without-replacement instead, as the resulting convergence is typically much faster. A first goal in this project is to study and quantify this discrepancy over an interesting class of regression problems, which has been a key open problem. Several techniques and methods are proposed for obtaining asymptotic and non-asymptotic theoretical guarantees for without-replacement sampling schemes. A second goal is to develop fast algorithms with convergence guarantees that go beyond the limitations of the i.i.d. sampling. For this purpose, a new framework for studying several alternative sampling schemes and their performance is developed. Using this framework, novel sampling schemes based on weighted without-replacement sampling and cyclic sampling that can adapt to the dataset and improve upon the performance of the traditional i.i.d. sampling in terms of limiting accuracy are developed.</AbstractNarration>
<MinAmdLetterDate>06/14/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/11/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1723085</AwardID>
<Investigator>
<FirstName>Mert</FirstName>
<LastName>Gurbuzbalaban</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mert Gurbuzbalaban</PI_FULL_NAME>
<EmailAddress>mg1366@rutgers.edu</EmailAddress>
<PI_PHON>3472609624</PI_PHON>
<NSF_ID>000739809</NSF_ID>
<StartDate>06/14/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University Newark</Name>
<CityName>Newark</CityName>
<ZipCode>071021896</ZipCode>
<PhoneNumber>9739720283</PhoneNumber>
<StreetAddress>Blumenthal Hall, Suite 206</StreetAddress>
<StreetAddress2><![CDATA[249 University Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>130029205</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University Newark]]></Name>
<CityName>Newark</CityName>
<StateCode>NJ</StateCode>
<ZipCode>071023122</ZipCode>
<StreetAddress><![CDATA[1 Washington Park #1053A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~41593</FUND_OBLG>
<FUND_OBLG>2018~41672</FUND_OBLG>
<FUND_OBLG>2019~41736</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Advances in sensing and processing technologies, communication capabilities and smart devices have enabled deployment of systems where a massive amount of data is collected to make decisions. Many key problems of interest for analyzing and processing big data result in large-scale optimization problems. Incremental gradient (IG) and coordinate descent (CD) methods are very widely used core optimization methods, which are efficient for such problems where the data points are sampled and processed in a sequential manner. There is still a large gap between the theory and practice of these methods. This project is about filling this gap by providing novel performance guarantees relevant to practical problems as well as developing novel and faster variants of these optimization methods. &nbsp;The methods and techniques developed under the scope of this project contributed to the efficiency and mathematical foundations of optimization algorithms targeted for big data challenges, contributing to more efficient decision making for a wide variety of large-scale data analysis problems.<br /><br /></p> <p>Various performance guarantees for IG and RCD are available if data points are sampled with replacement in an independent identically distributed (i.i.d.) manner. However, these are not helpful in practical scenarios: In practice, data is often sampled in a non-i.i.d fashion without-replacement instead, as the resulting convergence is typically much faster. In this project, we study and quantify this discrepancy for IG and RCD methods over different classes of optimization problems.&nbsp; Several techniques and methods are proposed for obtaining asymptotic and non-asymptotic theoretical guarantees for without-replacement sampling schemes. A second goal was to develop fast algorithms with convergence guarantees that can improve upon the performance of IG methods with randomized sampling. We developed a number of randomized IG methods that are optimal in terms of sample complexity, i.e. number of samples required to achieve a target accuracy. We also developed related optimization methods that are "optimally" robust with respect to the amount of noise in the gradient computations. Furthermore, we studied the behavior of IG methods for a number of applications such as deep learning and in settings where computations are distributed on a number of computing nodes. Graduate students were trained and the results were presented in a number of venues. Some of our methods were implemented as part of a software package and were made available to the public.</p><br> <p>            Last Modified: 03/02/2021<br>      Modified by: Mert&nbsp;Gurbuzbalaban</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Advances in sensing and processing technologies, communication capabilities and smart devices have enabled deployment of systems where a massive amount of data is collected to make decisions. Many key problems of interest for analyzing and processing big data result in large-scale optimization problems. Incremental gradient (IG) and coordinate descent (CD) methods are very widely used core optimization methods, which are efficient for such problems where the data points are sampled and processed in a sequential manner. There is still a large gap between the theory and practice of these methods. This project is about filling this gap by providing novel performance guarantees relevant to practical problems as well as developing novel and faster variants of these optimization methods.  The methods and techniques developed under the scope of this project contributed to the efficiency and mathematical foundations of optimization algorithms targeted for big data challenges, contributing to more efficient decision making for a wide variety of large-scale data analysis problems.    Various performance guarantees for IG and RCD are available if data points are sampled with replacement in an independent identically distributed (i.i.d.) manner. However, these are not helpful in practical scenarios: In practice, data is often sampled in a non-i.i.d fashion without-replacement instead, as the resulting convergence is typically much faster. In this project, we study and quantify this discrepancy for IG and RCD methods over different classes of optimization problems.  Several techniques and methods are proposed for obtaining asymptotic and non-asymptotic theoretical guarantees for without-replacement sampling schemes. A second goal was to develop fast algorithms with convergence guarantees that can improve upon the performance of IG methods with randomized sampling. We developed a number of randomized IG methods that are optimal in terms of sample complexity, i.e. number of samples required to achieve a target accuracy. We also developed related optimization methods that are "optimally" robust with respect to the amount of noise in the gradient computations. Furthermore, we studied the behavior of IG methods for a number of applications such as deep learning and in settings where computations are distributed on a number of computing nodes. Graduate students were trained and the results were presented in a number of venues. Some of our methods were implemented as part of a software package and were made available to the public.       Last Modified: 03/02/2021       Submitted by: Mert Gurbuzbalaban]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
