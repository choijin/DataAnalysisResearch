<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Collaborative Research:  Data Visualizations for Linguistically Annotated, Publicly Shared, Video Corpora for American Sign Language (ASL)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>54999.00</AwardTotalIntnAmount>
<AwardAmount>54999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Linguistic research on ASL has been held back by the lack of precise tools for measurement, over large corpora, of the non-manual articulations (i.e., facial expressions and head gestures) that carry key grammatical information in sign languages.  The same limitations have, until now, also held back computer science research on sign language recognition and generation.  The PIs have created valuable resources, through prior NSF support, to serve the research, education, and sign language communities, including: computational techniques for analysis of American Sign Language (ASL) videos and the SignStream software for linguistic annotation of sign language data; large linguistically annotated and computationally analyzed corpora with videos from native signers; and an online Data Access Interface (DAI) that enables intuitive and flexible searching, browsing, and download, to provide easy access to these publicly shared corpora.  They have also exploited these corpora for research on the linguistic structure of ASL and on computer-based sign language recognition from video.  Recently, they have developed new versions of SignStream and the DAI with many new features that are now ready to be released publicly.  Both represent major improvements over earlier versions of these applications and, combined with the public release of large new richly annotated and readily searchable data sets, constitute resources that will be of great value to researchers, educators, and students in linguistics and computer science, by opening up whole new avenues of research and enabling dramatic improvements in computer-based sign language recognition and generation.  The resulting wide-ranging research advances will also contribute to future computer-based applications that will enhance communication for and with deaf individuals, as well as applications that will have educational benefits and overall improve the lives of those who are deaf and hard-of-hearing.  The part-time effort to be funded for the two key software developers will also enable them to provide the limited technical support that is essential during the first year of the public release of SignStream 3 and DAI 2.&lt;br/&gt;&lt;br/&gt;The goal of this project is to further improve the existing applications by incorporating several powerful enhancements and additional functionalities to enable the shared tools and data to support new kinds of research in both linguistics (for analysis of linguistic properties of ASL and other signed languages) and computer science (for work in sign language recognition and generation).  Specifically, the PIs will incorporate into their displays, within both the annotation software and the Web interface, graphical representations of computer-generated analyses of ASL videos, so that users will be able to visualize the distribution and characteristics of key aspects of facial expressions and head movements that carry critical linguistic information in sign languages (e.g., head nods and shakes, eyebrow height, and eye aperture).  The most challenging aspect of sign language generation has been the production of natural-looking, appropriately timed, facial expressions and head movements.  The sophisticated approach to tracking and 3D modeling of such expressions that has been developed recently by Metaxas et al. makes it possible to derive precise information about these facial expressions and head gestures for large sets of video files.</AbstractNarration>
<MinAmdLetterDate>07/31/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1748022</AwardID>
<Investigator>
<FirstName>Dimitris</FirstName>
<LastName>Metaxas</LastName>
<PI_MID_INIT>N</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dimitris N Metaxas</PI_FULL_NAME>
<EmailAddress>dnm@cs.rutgers.edu</EmailAddress>
<PI_PHON>7324452914</PI_PHON>
<NSF_ID>000236186</NSF_ID>
<StartDate>07/31/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548072</ZipCode>
<StreetAddress><![CDATA[617 Bowser Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~54999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to enhance the applications we have been developing for (1) the linguistic annotation of video data from American Sign Language (SignStream(R)), and (2) sharing of our linguistically annotated corpora and ASL Sign Bank on the Internet (through our Data Access Interface (DAI) 2).&nbsp;&nbsp;In particular, we sought to develop functionalities for incorporation into both SignStream(R) and DAI 2 of display of graphical representations of computer-generated analyses of ASL videos, thereby making it possible for users to visualize the distribution and characteristics of key aspects of facial expressions and head movements that carry critical linguistic information in sign languages (e.g., head nods and shakes, eyebrow height, and eye aperture).&nbsp;&nbsp;</p> <p><strong>Intellectual merits</strong></p> <p>The application we have been developing for linguistic annotation of video data for sign languages, along with the intuitive Web interface through which we share our linguistically annotated corpora and Bank, are aimed to facilitate education and linguistic and computer science research. They facilitate the analysis of video sign language data by making our corpora easily accessible to the broader ASL and research communities; this enables kinds of learning and research that would otherwise be difficult or impossible.&nbsp;</p> <p>The role and precise distribution of linguistically essential non-manual expressions are not yet fully understood; this has been difficult to study in the absence of precise measurements of these components over large data sets. Sharing the data from state-of-the-art computer-based face and head tracking of the videos in our corpora will open up new avenues for research.&nbsp;&nbsp;We are still in the process of further refining the methods for analysis of facial expressions and head movements; but we will, in the near future, be in a position to make the visualizations (and the raw data) corresponding to the analysis of the video data in our corpora available to linguists and computer scientists thanks to the DAI 2 functionalities that have been developed through this project.&nbsp;&nbsp;The newly developed functionality for visualizing changes in facial expressions and head positions within SignStream also offers the potential for greatly improving the accuracy and efficiency of the annotation process and for enabling types of linguistic analysis that have not previously been possible.</p> <p><strong>Specific accomplishments:</strong></p> <p>A new version of SignStream(R)&nbsp;was released in May 2018, and another new release is in the final stages of testing, set to be released by the end of 2018.</p> <p>Improvements to&nbsp;SignStream(R) that were made as part of this&nbsp;project include the following:</p> <ol> <li>Substantial bug fixes</li> <li>Important enhancements to the user interface (especially in regard to the Sign Bank window, which is now integrated with the web-based DAI Sign Bank)&nbsp;and to the mechanism whereby users upgrade to newer versions of the program</li> <li>Implementation of functionality to enable import of graphical representations of computer-generated analyses of facial expressions and head movements</li> </ol> <p>With respect to the DAI, major advances were also made, including:</p> <ol> <li>Substantial bug fixes&nbsp;</li> <li>Important enhancements to the user interface</li> <li>Enabling online connections to the SignBank from within the SignStream? application</li> <li>Improvements to the functionality for semi-automated incorporation of signs from our continuous signing corpora into the Sign Bank (which was based initially on our ASLLVD data set containing isolated, citation-form, signs).</li> <li>Incorporation of download functionalities</li> <li>Incorporation of functionalities for display and sharing of computer-generated graphical representations of non-manual components of the signing (eyebrow height, eye aperture, and head position in 3 dimensions)</li> </ol> <p><strong>Broader impacts</strong></p> <p>We expect that the enhancements made possible by this project will increase the value and utility of the resources we make publicly available to facilitate linguistic and computer research on sign language, as well as increasing the educational value of these materials.</p> <p>Our data sets are being used by researchers, educators, and students around the world for linguistic and computer science research and doctoral training. These materials will also be invaluable for those teaching/studying ASL and Deaf culture, ASL literature, and interpreting. We expect that the community that benefits from these tools will grow as the tools become more powerful.</p> <p>The research made possible by these resources holds great promise for leading to technologies that will benefit the deaf community. These include tools for language learning, mobile sign language dictionaries and retrieval, and tools for searching for signs by example. Ultimately, these resources are likely to contribute to systems for automated machine translation and human-computer interaction.&nbsp;For example, the most challenging aspect of sign language generation has been the production of natural-looking, appropriately timed, facial expressions and head movements.&nbsp;&nbsp;The sophisticated approach to tracking and 3D modeling of such expressions that has been developed recently by Metaxas et al. makes it possible to derive precise information about these facial expressions and head gestures for large sets of video files, which will advance research in sign language generation by computer.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/07/2019<br>      Modified by: Dimitris&nbsp;N&nbsp;Metaxas</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1748022/1748022_10509033_1549555013107_FIGURE-1.-outcomes-report-png--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1748022/1748022_10509033_1549555013107_FIGURE-1.-outcomes-report-png--rgov-800width.jpg" title="Figure 1: Computer Generated Non-manual Gestures into SignStream (Registered Copyright)"><img src="/por/images/Reports/POR/2019/1748022/1748022_10509033_1549555013107_FIGURE-1.-outcomes-report-png--rgov-66x44.jpg" alt="Figure 1: Computer Generated Non-manual Gestures into SignStream (Registered Copyright)"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Illustration of import into SignStream of computer-generated data about non-manual gestures (head position (yaw, pitch tilt) as well aseyebrow height and eye aperture); the latter two graphs are shown here, along with the previously completed human annotations</div> <div class="imageCredit">Dimitris Metaxas and Carol Neidle</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Dimitris&nbsp;N&nbsp;Metaxas</div> <div class="imageTitle">Figure 1: Computer Generated Non-manual Gestures into SignStream (Registered Copyright)</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to enhance the applications we have been developing for (1) the linguistic annotation of video data from American Sign Language (SignStream(R)), and (2) sharing of our linguistically annotated corpora and ASL Sign Bank on the Internet (through our Data Access Interface (DAI) 2).  In particular, we sought to develop functionalities for incorporation into both SignStream(R) and DAI 2 of display of graphical representations of computer-generated analyses of ASL videos, thereby making it possible for users to visualize the distribution and characteristics of key aspects of facial expressions and head movements that carry critical linguistic information in sign languages (e.g., head nods and shakes, eyebrow height, and eye aperture).    Intellectual merits  The application we have been developing for linguistic annotation of video data for sign languages, along with the intuitive Web interface through which we share our linguistically annotated corpora and Bank, are aimed to facilitate education and linguistic and computer science research. They facilitate the analysis of video sign language data by making our corpora easily accessible to the broader ASL and research communities; this enables kinds of learning and research that would otherwise be difficult or impossible.   The role and precise distribution of linguistically essential non-manual expressions are not yet fully understood; this has been difficult to study in the absence of precise measurements of these components over large data sets. Sharing the data from state-of-the-art computer-based face and head tracking of the videos in our corpora will open up new avenues for research.  We are still in the process of further refining the methods for analysis of facial expressions and head movements; but we will, in the near future, be in a position to make the visualizations (and the raw data) corresponding to the analysis of the video data in our corpora available to linguists and computer scientists thanks to the DAI 2 functionalities that have been developed through this project.  The newly developed functionality for visualizing changes in facial expressions and head positions within SignStream also offers the potential for greatly improving the accuracy and efficiency of the annotation process and for enabling types of linguistic analysis that have not previously been possible.  Specific accomplishments:  A new version of SignStream(R) was released in May 2018, and another new release is in the final stages of testing, set to be released by the end of 2018.  Improvements to SignStream(R) that were made as part of this project include the following:  Substantial bug fixes Important enhancements to the user interface (especially in regard to the Sign Bank window, which is now integrated with the web-based DAI Sign Bank) and to the mechanism whereby users upgrade to newer versions of the program Implementation of functionality to enable import of graphical representations of computer-generated analyses of facial expressions and head movements   With respect to the DAI, major advances were also made, including:  Substantial bug fixes  Important enhancements to the user interface Enabling online connections to the SignBank from within the SignStream? application Improvements to the functionality for semi-automated incorporation of signs from our continuous signing corpora into the Sign Bank (which was based initially on our ASLLVD data set containing isolated, citation-form, signs). Incorporation of download functionalities Incorporation of functionalities for display and sharing of computer-generated graphical representations of non-manual components of the signing (eyebrow height, eye aperture, and head position in 3 dimensions)   Broader impacts  We expect that the enhancements made possible by this project will increase the value and utility of the resources we make publicly available to facilitate linguistic and computer research on sign language, as well as increasing the educational value of these materials.  Our data sets are being used by researchers, educators, and students around the world for linguistic and computer science research and doctoral training. These materials will also be invaluable for those teaching/studying ASL and Deaf culture, ASL literature, and interpreting. We expect that the community that benefits from these tools will grow as the tools become more powerful.  The research made possible by these resources holds great promise for leading to technologies that will benefit the deaf community. These include tools for language learning, mobile sign language dictionaries and retrieval, and tools for searching for signs by example. Ultimately, these resources are likely to contribute to systems for automated machine translation and human-computer interaction. For example, the most challenging aspect of sign language generation has been the production of natural-looking, appropriately timed, facial expressions and head movements.  The sophisticated approach to tracking and 3D modeling of such expressions that has been developed recently by Metaxas et al. makes it possible to derive precise information about these facial expressions and head gestures for large sets of video files, which will advance research in sign language generation by computer.                 Last Modified: 02/07/2019       Submitted by: Dimitris N Metaxas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
