<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Collaborative Research: F: From Data Geometries to Information Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>12/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>499937.00</AwardTotalIntnAmount>
<AwardAmount>499937</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Big Data often results from multiple sources, giving collections that contain multiple, often partial, "views" of the same object, space, or phenomenon from various observers.  Extracting information robustly from such data sets calls for a joint analysis of a large collection of data sets.  The project is developing a novel geometric framework for modeling, structure detection, and information extraction from a collection of large related data sets, with an emphasis on the relationships between data.  While this approach clearly applies to data with a clear geometric character (e.g., objects in images), the work is also applied to datasets as diverse as computer networks (identifying common structure in subnets) and Massive Open Online Course homework data (automatically carrying grader annotations to similar problems in other students' homeworks).&lt;br/&gt;&lt;br/&gt;The novel framework is based on the construction of maps between the objects under considerations (point clouds, graphs, images, etc...), and on the analysis of the networks of maps that result as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. These tasks define a new field of map processing between data sets and require tool sets with new ideas from functional analysis, non-convex optimization, and homological algebra in mathematics, and geometric algorithms, machine learning, optimization, and approximation algorithms in computer science.  Sophisticated algorithmic techniques for attacking the large-scale non-linear optimization problems that emerge within the framework will also be investigated.</AbstractNarration>
<MinAmdLetterDate>04/28/2017</MinAmdLetterDate>
<MaxAmdLetterDate>04/28/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1708553</AwardID>
<Investigator>
<FirstName>Mauro</FirstName>
<LastName>Maggioni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mauro Maggioni</PI_FULL_NAME>
<EmailAddress>mauro.maggioni@jhu.edu</EmailAddress>
<PI_PHON>4105166524</PI_PHON>
<NSF_ID>000398937</NSF_ID>
<StartDate>04/28/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University Krieger School of Arts and Sciences]]></Name>
<CityName/>
<StateCode>MD</StateCode>
<ZipCode>212182682</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~499937</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Capitalizing on the exploding availability of large data sets across a wide variety of industries and research fields urgently demands the development of novel mathematical formulations and scalable algorithms for extracting as much information and insight as possible from data. The size of the data is, however, only part of the problem. The modern data sets are often incomplete and approximate; arriving in continuous streams, get updated frequently and are acquired in a piecemeal manner; contain multiple, often partial, ``views'' of the same object, space, or phenomenon from various observers. Extracting information robustly from such data sets calls for a&nbsp;<em>joint analysis&nbsp;</em>of a large collection of data sets. Furthermore, much of the data being gathered today has a geometric character, either directly or indirectly, and appropriate tools need to be developed for data in less regular formats, including point clouds and meshes.&nbsp;</p> <p>In this project we developed novel frameworks, algorithms, and ways of analyzing them, for modeling, structure detection, and information extraction from a collection of large related data sets, with a focus on the relationships between data. We focused on the construction of maps between the objects under considerations (point clouds, graphs, images, physical systems, etc...), as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. These tasks define a new field of&nbsp;<em>map processing&nbsp;</em>between data sets, and this project addressed many key underlying&nbsp;technical challenges, with novel ideas and techniques that we believe will be useful for further advancement in the field.</p> <p>Key outcomes include the development of algorithms, with guaranteed performance, for learning and approximating maps between data sets, both when matching information is given and when it is missing or it is noisy; efficient algorithms for time-varying data sets, to track those changes and to track how data structures needed for efficient computation change; novel algorithms for discovering groupings/clusters in high-dimensional data, in a completely unsupervised way, or with a human-in-the-loop; novel techniques for learning in an automatic and flexible fashion the interaction laws in systems of agents or particles (from fundamental physics particles to planets to birds in a flock to groups of cells), with the ability of transferring them to new systems; the ability to learn hidden structure, linear or nonlinear, for high-dimensional prediction tasks; the ability to study and process data on shapes, including applications to medicine, such as the study of scarred heart tissue in view of predicting health complications.</p> <p>Several of these algorithms have been implemented, with publicly available code, for ease of use from researchers in other disciplines, and for fostering new collaborations, for example with researchers in biomedical engineering and in medical schools, as well as with businesses working on large scale data analytics.</p><br> <p>            Last Modified: 07/20/2020<br>      Modified by: Mauro&nbsp;Maggioni</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Capitalizing on the exploding availability of large data sets across a wide variety of industries and research fields urgently demands the development of novel mathematical formulations and scalable algorithms for extracting as much information and insight as possible from data. The size of the data is, however, only part of the problem. The modern data sets are often incomplete and approximate; arriving in continuous streams, get updated frequently and are acquired in a piecemeal manner; contain multiple, often partial, ``views'' of the same object, space, or phenomenon from various observers. Extracting information robustly from such data sets calls for a joint analysis of a large collection of data sets. Furthermore, much of the data being gathered today has a geometric character, either directly or indirectly, and appropriate tools need to be developed for data in less regular formats, including point clouds and meshes.   In this project we developed novel frameworks, algorithms, and ways of analyzing them, for modeling, structure detection, and information extraction from a collection of large related data sets, with a focus on the relationships between data. We focused on the construction of maps between the objects under considerations (point clouds, graphs, images, physical systems, etc...), as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. These tasks define a new field of map processing between data sets, and this project addressed many key underlying technical challenges, with novel ideas and techniques that we believe will be useful for further advancement in the field.  Key outcomes include the development of algorithms, with guaranteed performance, for learning and approximating maps between data sets, both when matching information is given and when it is missing or it is noisy; efficient algorithms for time-varying data sets, to track those changes and to track how data structures needed for efficient computation change; novel algorithms for discovering groupings/clusters in high-dimensional data, in a completely unsupervised way, or with a human-in-the-loop; novel techniques for learning in an automatic and flexible fashion the interaction laws in systems of agents or particles (from fundamental physics particles to planets to birds in a flock to groups of cells), with the ability of transferring them to new systems; the ability to learn hidden structure, linear or nonlinear, for high-dimensional prediction tasks; the ability to study and process data on shapes, including applications to medicine, such as the study of scarred heart tissue in view of predicting health complications.  Several of these algorithms have been implemented, with publicly available code, for ease of use from researchers in other disciplines, and for fostering new collaborations, for example with researchers in biomedical engineering and in medical schools, as well as with businesses working on large scale data analytics.       Last Modified: 07/20/2020       Submitted by: Mauro Maggioni]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
