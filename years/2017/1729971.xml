<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:   CI-P: ShapeNet: An Information-Rich 3D Model Repository for Graphics, Vision and Robotics Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>33000.00</AwardTotalIntnAmount>
<AwardAmount>33000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to plan the development of a richly annotated repository of 3D models called ShapeNet that currently exists only in a preliminary form. ShapeNet will include 3-4 million 3D models of everyday objects in 4-5 thousand categories, in a variety of representations. Models in the ShapeNet repository will be annotated with multiple annotation types: geometric (parts, symmetries), semantic (keywords for the shape and its parts), physical (weight, size), and functional (affordances, scene context). The availability of ShapeNet data, capturing the 3D geometry of a significant fraction of object categories in the world, together with associated detailed meta-data and semantic information, will catalyze major developments in graphics, vision and robotics by providing adequate data against which new proposed techniques and methodologies for shape or scene analysis and synthesis can be vetted -- and with which machine learning algorithms can be trained. ShapeNet can be considered an encyclopedia that facilitates the creation of intelligent systems and agents capable of operating autonomously in the world --- because they can have deep knowledge of that world.&lt;br/&gt;&lt;br/&gt;While most of the ShapeNet models will be initially found on the Web, the annotations will be obtained through an active learning combination of modest human input (including crowd-sourcing), extensive algorithmic transport, and human verification. During the planning period the effort will focus on mathematical representations of the semantic knowledge associated with 3D models, as well as on a design framework for key algorithms allowing knowledge transport from one model to another. Further challenges to be addressed include the quantification of data quality issues and the specification of all the multimodal (3D, image, language) UIs and APIs needed for users to be able to exploit and search this wealth of data, or to contribute additional models and annotations to it.</AbstractNarration>
<MinAmdLetterDate>06/07/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1729971</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Funkhouser</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas A Funkhouser</PI_FULL_NAME>
<EmailAddress>funk@cs.princeton.edu</EmailAddress>
<PI_PHON>6092581748</PI_PHON>
<NSF_ID>000092134</NSF_ID>
<StartDate>06/07/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[87 Prospect Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~33000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project resulted in a large-scale repository of richly annotated 3D shapes for the communities of graphics, vision, and robotics. The dataset facilitate this development of the next generation of algorithms and techniques for understanding and synthesizing 3D data, and for generating synthetic image data useful for training in computer vision tasks. A key aspect of the project is the rich annotations associated with 3D shapes, including consistent orientations, consistent and hierarchical part annotations, keypoint annotations, physical object sizes, and rendered images.</p> <p>In addition to the release of the dataset, the team built community around 3D data analysis and synthesis by organizing a workshop and competition. In the Large-scale 3D Shape Reconstruction and Segmentation Challenge, ten teams from different parts of the world participated, competing with algorithms that significantly advanced the state-of-the-art on shape analysis and representation. In the ShapeNet Challenge Workshop at ICCV 2017, hundred(s) of researchers gathered to compare ideas for 3D deep learning, presenting results of experiments based on ShapeNet. Overall, ShapeNet has been downloaded by ~3000 people and cited by several hundred research papers, including over 300 citations in the past year according to Google Scholar.</p> <p>Further, the project resulted in several publications by the team on new methods related to 3D deep learning using a large 3D dataset. Papers explored predicting relationships between 3D surfaces and color images, keypoints, structures, part segmentations, text phrases and object functionality. These papers were published at venues in vision (ECCV, 3DV), graphics (Computer Graphics Forum), and NLP (ACCV, Global WordNet Conference).</p> <p>Finally, the project produced educations and mentorship opportunities for undergraduate, graduate students, and postdocs. The team taught courses on 3D deep learning and mentored&nbsp;several Ph.D. and bachelor's theses on topics related to the project. In particular, two postdocs have moved to new faculty positions in the past year (Savva, Chang) and several undergraduates have moved to Ph.D. programs at top schools.</p><br> <p>            Last Modified: 01/30/2019<br>      Modified by: Thomas&nbsp;A&nbsp;Funkhouser</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project resulted in a large-scale repository of richly annotated 3D shapes for the communities of graphics, vision, and robotics. The dataset facilitate this development of the next generation of algorithms and techniques for understanding and synthesizing 3D data, and for generating synthetic image data useful for training in computer vision tasks. A key aspect of the project is the rich annotations associated with 3D shapes, including consistent orientations, consistent and hierarchical part annotations, keypoint annotations, physical object sizes, and rendered images.  In addition to the release of the dataset, the team built community around 3D data analysis and synthesis by organizing a workshop and competition. In the Large-scale 3D Shape Reconstruction and Segmentation Challenge, ten teams from different parts of the world participated, competing with algorithms that significantly advanced the state-of-the-art on shape analysis and representation. In the ShapeNet Challenge Workshop at ICCV 2017, hundred(s) of researchers gathered to compare ideas for 3D deep learning, presenting results of experiments based on ShapeNet. Overall, ShapeNet has been downloaded by ~3000 people and cited by several hundred research papers, including over 300 citations in the past year according to Google Scholar.  Further, the project resulted in several publications by the team on new methods related to 3D deep learning using a large 3D dataset. Papers explored predicting relationships between 3D surfaces and color images, keypoints, structures, part segmentations, text phrases and object functionality. These papers were published at venues in vision (ECCV, 3DV), graphics (Computer Graphics Forum), and NLP (ACCV, Global WordNet Conference).  Finally, the project produced educations and mentorship opportunities for undergraduate, graduate students, and postdocs. The team taught courses on 3D deep learning and mentored several Ph.D. and bachelor's theses on topics related to the project. In particular, two postdocs have moved to new faculty positions in the past year (Savva, Chang) and several undergraduates have moved to Ph.D. programs at top schools.       Last Modified: 01/30/2019       Submitted by: Thomas A Funkhouser]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
