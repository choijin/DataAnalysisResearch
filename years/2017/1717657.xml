<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR:   Small:   Collaborative Research:  EUReCa:   Enabling Untethered VR/AR System via Human-centric Graphic Computing and Distributed Data Processing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Erik Brunvand</SignBlockName>
<PO_EMAI>ebrunvan@nsf.gov</PO_EMAI>
<PO_PHON>7032922767</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Virtual Reality (VR) and Augmented Reality (AR) devices, especially their mobile versions are newly emergent technologies. However, a major challenge that VR/AR technologies faces is the gap between the increasing needs for graphic and data processing and the limited computing capability of the mobile hardware. Two researchers from GMU and Duke form a team to develop an innovative VR/AR system, namely, "EUReCa", which tackles the challenge of human-centric graphic processing and distributed data processing. This research studies VR/AR system design using standard workloads to understand the computation source utilization that leads to development of usage model, and reduces the computation loads via task allocation, thereby enhancing the efficiency and scalability of computation. The research outcomes will benefit both research and industry at large by integrating the innovations of human interaction and advanced data processing technologies. The education plan enhances existing curricula and pedagogy by integrating interdisciplinary modules on computer graphic, embedded systems, and machine learning with newly developed teaching practices, and gives special attention to women and underrepresented minority groups.&lt;br/&gt; &lt;br/&gt;The project performs three tasks. Task 1 models the computation resource utilization of VR/AR systems by considering the system configuration and dynamics of user operations. Task 2 explores efficient human-centric graphic rendering framework for reducing computation loads of VR/AR systems. Task 3 exploits novel schemes to enhance computation efficiency via balancing the computation loads and data allocations in graphic rendering and deep neural network (DNN) applications. The techniques will be evaluated on mobile devices.</AbstractNarration>
<MinAmdLetterDate>08/03/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/03/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1717657</AwardID>
<Investigator>
<FirstName>Yiran</FirstName>
<LastName>Chen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yiran Chen</PI_FULL_NAME>
<EmailAddress>yiran.chen@duke.edu</EmailAddress>
<PI_PHON>6127039849</PI_PHON>
<NSF_ID>000575362</NSF_ID>
<StartDate>08/03/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277080001</ZipCode>
<StreetAddress><![CDATA[Hudson Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this three-year project, we explore several critical techniques that can untether VR/AR systems from the limitations of existing graphic rendering and data processing capabilities through a revolutionary application of human-centric computing and distributed data processing methodologies. The detailed goals of the project include: 1) Demystifying computation resource utilization of VR/AR systems; 2) Exploring an efficient human-centric graphic rendering framework; and 3) Exploiting novel schemes to enhance computation efficiency of mobile VR/AR systems.</p> <p>&nbsp;</p> <p>Deep Neural Networks (DNNs) are pervasively used in many applications and platforms including AR/VR applications and systems. To enhance the execution efficiency of large-scale DNNs, previous attempts focus mainly on client-server paradigms, relying on powerful external infrastructure, or model compression, with complicated pre-processing phases. Though effective, these methods overlook the optimization of DNNs on distributed mobile devices. To efficiently deploy large-scale DNNs, we design and implement MeDNN - a local distributed mobile computing system that can adaptively partition DNN models onto several mobile devices and perform mobile-friendly structured sparsity pruning on the DNN model. We also implement AdaLearner ? an adaptive distributed mobile learning system for neural networks that trains a single network with heterogenous mobile resources under the same local network in parallel.</p> <p>&nbsp;</p> <p>The known vulnerability of neural networks to adversarial attack raises a severe security concern of DNN-based VR/AR systems. In some applications where users have limited access to the systems, defense technologies that require changing the training methods of the systems, e.g., adversarial training become impracticable. To solve this challenge, we propose AdverQuil ? an efficient adversarial detection and alleviation technique for black-box DNN-based computing systems. AdverQuil can identify the adversarial strength of input examples and guide a black-box DNN-based VR/AR system to select the best strategy to respond to the attack. No changes in the parameters, the configuration, and the training method of the protected DNN model are needed.</p> <p>&nbsp;</p> <p>In recent years, machine learning research has largely shifted focus from the cloud to the edge. While the resulting algorithm- and hardware-level optimizations have enabled local execution for the majority of DNNs on edge devices, the sheer magnitude of DNNs associated with real-time video detection workloads has forced them to remain relegated to remote execution in the cloud. This problematic when combined with the strict latency requirements that are coupled with these workloads and imposes a unique set of challenges not directly addressed in prior works. Hence, we design MobiEye, a cloud-based video detection system optimized for deployment in real-time mobile applications. Benefiting from the reduced latency in cloud computation contexts, MobiEye achieves up to a 32% reduction in latency when compared to a conventional implementation of video detection system with only a marginal reduction in accuracy.</p> <p>&nbsp;</p> <p>The invention of the recent Transformer model structure boosts the performance of Neural Machine Translation (NMT) tasks to an unprecedented level. Many previous works have been done to make the Transformer model more execution-friendly on resource-constrained platforms. In particular, model pruning methods are popular for their simplicity in practice and promising compression rate and have achieved great success in the field of convolution neural networks (CNNs) for many vision tasks. Nonetheless, previous Transformer pruning works did not perform a thorough model analysis on each Transformer component and evaluation when deploying these pruned models on off-the-shelf mobile devices. We analyze and prune transformer models at the line-wise granularity and also implement our pruning method on real mobile platforms. We name our whole Transformer analysis and pruning pipeline as TPrune. Experimental results show that our pruned models achieve 1.16X-1.92X speedup on mobile devices with 0%-8% BLEU score degradation compared with the original Transformer model.</p> <p>&nbsp;</p> <p>This 3-year research project supported 4 graduate students and publications of total 7 conference papers, 1 journal papers, and 2 journal submissions under review.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/16/2020<br>      Modified by: Yiran&nbsp;Chen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this three-year project, we explore several critical techniques that can untether VR/AR systems from the limitations of existing graphic rendering and data processing capabilities through a revolutionary application of human-centric computing and distributed data processing methodologies. The detailed goals of the project include: 1) Demystifying computation resource utilization of VR/AR systems; 2) Exploring an efficient human-centric graphic rendering framework; and 3) Exploiting novel schemes to enhance computation efficiency of mobile VR/AR systems.     Deep Neural Networks (DNNs) are pervasively used in many applications and platforms including AR/VR applications and systems. To enhance the execution efficiency of large-scale DNNs, previous attempts focus mainly on client-server paradigms, relying on powerful external infrastructure, or model compression, with complicated pre-processing phases. Though effective, these methods overlook the optimization of DNNs on distributed mobile devices. To efficiently deploy large-scale DNNs, we design and implement MeDNN - a local distributed mobile computing system that can adaptively partition DNN models onto several mobile devices and perform mobile-friendly structured sparsity pruning on the DNN model. We also implement AdaLearner ? an adaptive distributed mobile learning system for neural networks that trains a single network with heterogenous mobile resources under the same local network in parallel.     The known vulnerability of neural networks to adversarial attack raises a severe security concern of DNN-based VR/AR systems. In some applications where users have limited access to the systems, defense technologies that require changing the training methods of the systems, e.g., adversarial training become impracticable. To solve this challenge, we propose AdverQuil ? an efficient adversarial detection and alleviation technique for black-box DNN-based computing systems. AdverQuil can identify the adversarial strength of input examples and guide a black-box DNN-based VR/AR system to select the best strategy to respond to the attack. No changes in the parameters, the configuration, and the training method of the protected DNN model are needed.     In recent years, machine learning research has largely shifted focus from the cloud to the edge. While the resulting algorithm- and hardware-level optimizations have enabled local execution for the majority of DNNs on edge devices, the sheer magnitude of DNNs associated with real-time video detection workloads has forced them to remain relegated to remote execution in the cloud. This problematic when combined with the strict latency requirements that are coupled with these workloads and imposes a unique set of challenges not directly addressed in prior works. Hence, we design MobiEye, a cloud-based video detection system optimized for deployment in real-time mobile applications. Benefiting from the reduced latency in cloud computation contexts, MobiEye achieves up to a 32% reduction in latency when compared to a conventional implementation of video detection system with only a marginal reduction in accuracy.     The invention of the recent Transformer model structure boosts the performance of Neural Machine Translation (NMT) tasks to an unprecedented level. Many previous works have been done to make the Transformer model more execution-friendly on resource-constrained platforms. In particular, model pruning methods are popular for their simplicity in practice and promising compression rate and have achieved great success in the field of convolution neural networks (CNNs) for many vision tasks. Nonetheless, previous Transformer pruning works did not perform a thorough model analysis on each Transformer component and evaluation when deploying these pruned models on off-the-shelf mobile devices. We analyze and prune transformer models at the line-wise granularity and also implement our pruning method on real mobile platforms. We name our whole Transformer analysis and pruning pipeline as TPrune. Experimental results show that our pruned models achieve 1.16X-1.92X speedup on mobile devices with 0%-8% BLEU score degradation compared with the original Transformer model.     This 3-year research project supported 4 graduate students and publications of total 7 conference papers, 1 journal papers, and 2 journal submissions under review.          Last Modified: 09/16/2020       Submitted by: Yiran Chen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
