<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Big Data, Outliers, and Group Marginalization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2018</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>14624.00</AwardTotalIntnAmount>
<AwardAmount>14624</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Frederick Kronz</SignBlockName>
<PO_EMAI>fkronz@nsf.gov</PO_EMAI>
<PO_PHON>7032927283</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This award supports a doctoral dissertation research project. The primary aim of the project is to contribute to understanding how diversity is navigated, compromised, and negotiated within big data practices. It will bring to light values and biases that privilege certain groups while marginalizing others, meaning those values that are unconsciously integrated into the design of technologies, such as data collection and sorting practices. Such results will have substantial social significance. They will serve to provide a better understanding of the various ways in which data can be different for different people, and of the potential biases that can result in inequity and injustice for the most vulnerable. The outcomes of this project also have the potential to address the social aspects of representational compression at the interface of humans and technology; they can be applied to integrating intersectional perspectives in technological design and technologically-driven policy decisions.  &lt;br/&gt;&lt;br/&gt;The researcher will engage in an ethnographic study of how data may enact particular formulations and subjectivities of categorization. She grounds her theoretical approach in Science and Technology Studies to identify instances where dominant discourses of data come into confrontation with alternative narratives. Of particular interest are instances where the data produced is unusual; the data is sometimes referred to as an "outlier" or, in more derogatory terms, "dirty data" that needs "scrubbing." This project aims to analyze such "unusual" data through ethnographic investigation of self-tracking cultures. This approach will contribute to knowledge of how big data practices encourage and impede certain social positions and subjectivities. It will also contribute to eliciting the underlying values of big data practices; such values speak to how data is variously deployed for use in civilian projects, economics, governance, and knowledge production.</AbstractNarration>
<MinAmdLetterDate>01/16/2018</MinAmdLetterDate>
<MaxAmdLetterDate>01/16/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1753635</AwardID>
<Investigator>
<FirstName>Casey</FirstName>
<LastName>O'Donnell</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Casey K O'Donnell</PI_FULL_NAME>
<EmailAddress>caseyod@msu.edu</EmailAddress>
<PI_PHON>5178844538</PI_PHON>
<NSF_ID>000537678</NSF_ID>
<StartDate>01/16/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Janine</FirstName>
<LastName>Slaker</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Janine Slaker</PI_FULL_NAME>
<EmailAddress>slakerja@msu.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000753867</NSF_ID>
<StartDate>01/16/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488242600</ZipCode>
<StreetAddress><![CDATA[404 Wilson Road, Room 409, Commu]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7603</Code>
<Text>STS-Sci, Tech &amp; Society</Text>
</ProgramElement>
<ProgramReference>
<Code>7567</Code>
<Text>SOC STUDIES OF SCI, ENG &amp; TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~14624</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There is a growing concern amongst the public and institutional actors regarding how internet-enabled devices and applications collect, analyze, and store data about human behaviors and attitudes. Much of the conversation has focused on the risks of violation of privacy and civil liberties, and increasing accounts of digital inequality. The presented ethnography contributes to such conversations about quantification, data, and power occurring within science and technology studies, human-computer interaction, and media and information studies. Drawing from participant observation and in-depth interviews with individuals across the United States who design and use self-tracking applications, this research outlines how individuals make sense of and navigate social differences via quantification methods.</p> <p><span style="white-space: pre;"> </span>Critiques of quantifying the body (i.e., biometrics) continue to point to the need to examine how quantification systems enact social difference. Social differences refer to a shared belief in human distinctions such as gender, age, class, race, etc., that undergird our societal structures. Social differences are how society organizes concepts of inequality and offers a mechanism to identify prejudicial beliefs and actions, such as the biased treatment of people because of their gender or age. To this end, the pervasiveness of self-tracking offers a unique entry for examining how social difference is understood and enacted via quantification methods.</p> <p><span style="white-space: pre;"> </span>In practice, self-tracking includes devices, software, institutions, infrastructure, and human capital, which produce quantified representations of human experiences, behaviors, and needs. Previous scholarship has shown that quantification helps to naturalize categories of social difference; it also aids in the development of new modes of production that affirm subordinate social groupings and inequitable dispersion of benefits. When behavior or thoughts are presented as a quantified outcome of social difference, it can naturalize marginalizing structures as it implies an inherent truth to socially constructed categories. The breadth of the sociotechnical system of self-tracking also presents a point of convergence, bringing together various communities and perspectives (e.g., physicians, designers, data scientists, law enforcement, etc.).</p> <p><span style="white-space: pre;"> </span>Rather than interpreting self-tracking through an existing theoretical frame, this research focused on how social difference is experienced by the individuals who participate in self-tracking practices and making. Hence, study participants have various roles and relations with self-tracking, identifying as designers, users, engineers, public health professionals, educators, activists, grandparents, athletes, data scientists, and patients, for instance. However, all are invested in self-tracking by way of design or use, and all investigation was aimed at surfacing sense-making practices of social differences sought after or captured in tracked data.</p> <p><span style="white-space: pre;"> </span>An emergent theme from this research was the absence of an isolated "self" to improve. It was rare for participants to talk about self-tracking as stemming from motivations to increase productivity, get a more restful sleep, or for weight loss. Instead, participants spoke about engaging with self-tracking to defend against social alienation from chronic illness, life-threatening disease, ageism, gendered body standards, racism, Islamophobia, xenophobia, and the commodification of surveillance. At times, participants spoke of engaging with self-tracking as a means for avoiding a state worse than their current position, seeking protection or stabilization rather than progression.</p> <p><span style="white-space: pre;"> </span>A second theme addresses an implicit bias among technologists that pathologize narratives of need. The pathologizing of narratives of need entails the assumption that social marginalization is an inherent characteristic of one's personhood rather than the outcome of structures and systems of oppression. Naturalizing marginalization as intrinsic to one's personhood helps to rationalize the exclusion of minoritized groups and justify their limited involvement in designing technologies that collect, store, and analyze data about them.</p> <p><span style="white-space: pre;"> </span>To summarize, this research contributes to our methods for identifying quantification practices that automate systems of social marginalization and exclusion. This research also emphasizes the proactive ways in which users of self-tracking engage in the renegotiation of social difference via its tools and practices. Broadly, the research findings address a tension between designers and technologists of self-tracking and the people who use them. Participant stories indicate that betterment is a privilege, yet it is a central design commitment of self-tracking devices. When improvement is assumed as a default goal of self-tracking, it helps to affirm existing social inequalities by generalizing the implicit biases of technologists and designers.</p> <p><span style="white-space: pre;"> </span>In turn, this research offers several strategies for identifying areas of misalignment between practitioners and users of self-tracking technologies and techniques to broaden pathways for diversifying the assumptions of technologists and designers. Current solutions to this problem have primarily focused on integrating ethics into computer science and design education and broadening data literacy efforts to the general public. The strategies and resources generated from this project expand such efforts by addressing how data literacy can include the expert position by offering aids to minimize biased design commitments. Lastly, this research provides guidance for enhancing community-based design practices for integration into educational, professional, and community settings.</p><br> <p>            Last Modified: 06/23/2021<br>      Modified by: Janine&nbsp;Slaker</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There is a growing concern amongst the public and institutional actors regarding how internet-enabled devices and applications collect, analyze, and store data about human behaviors and attitudes. Much of the conversation has focused on the risks of violation of privacy and civil liberties, and increasing accounts of digital inequality. The presented ethnography contributes to such conversations about quantification, data, and power occurring within science and technology studies, human-computer interaction, and media and information studies. Drawing from participant observation and in-depth interviews with individuals across the United States who design and use self-tracking applications, this research outlines how individuals make sense of and navigate social differences via quantification methods.   Critiques of quantifying the body (i.e., biometrics) continue to point to the need to examine how quantification systems enact social difference. Social differences refer to a shared belief in human distinctions such as gender, age, class, race, etc., that undergird our societal structures. Social differences are how society organizes concepts of inequality and offers a mechanism to identify prejudicial beliefs and actions, such as the biased treatment of people because of their gender or age. To this end, the pervasiveness of self-tracking offers a unique entry for examining how social difference is understood and enacted via quantification methods.   In practice, self-tracking includes devices, software, institutions, infrastructure, and human capital, which produce quantified representations of human experiences, behaviors, and needs. Previous scholarship has shown that quantification helps to naturalize categories of social difference; it also aids in the development of new modes of production that affirm subordinate social groupings and inequitable dispersion of benefits. When behavior or thoughts are presented as a quantified outcome of social difference, it can naturalize marginalizing structures as it implies an inherent truth to socially constructed categories. The breadth of the sociotechnical system of self-tracking also presents a point of convergence, bringing together various communities and perspectives (e.g., physicians, designers, data scientists, law enforcement, etc.).   Rather than interpreting self-tracking through an existing theoretical frame, this research focused on how social difference is experienced by the individuals who participate in self-tracking practices and making. Hence, study participants have various roles and relations with self-tracking, identifying as designers, users, engineers, public health professionals, educators, activists, grandparents, athletes, data scientists, and patients, for instance. However, all are invested in self-tracking by way of design or use, and all investigation was aimed at surfacing sense-making practices of social differences sought after or captured in tracked data.   An emergent theme from this research was the absence of an isolated "self" to improve. It was rare for participants to talk about self-tracking as stemming from motivations to increase productivity, get a more restful sleep, or for weight loss. Instead, participants spoke about engaging with self-tracking to defend against social alienation from chronic illness, life-threatening disease, ageism, gendered body standards, racism, Islamophobia, xenophobia, and the commodification of surveillance. At times, participants spoke of engaging with self-tracking as a means for avoiding a state worse than their current position, seeking protection or stabilization rather than progression.   A second theme addresses an implicit bias among technologists that pathologize narratives of need. The pathologizing of narratives of need entails the assumption that social marginalization is an inherent characteristic of one's personhood rather than the outcome of structures and systems of oppression. Naturalizing marginalization as intrinsic to one's personhood helps to rationalize the exclusion of minoritized groups and justify their limited involvement in designing technologies that collect, store, and analyze data about them.   To summarize, this research contributes to our methods for identifying quantification practices that automate systems of social marginalization and exclusion. This research also emphasizes the proactive ways in which users of self-tracking engage in the renegotiation of social difference via its tools and practices. Broadly, the research findings address a tension between designers and technologists of self-tracking and the people who use them. Participant stories indicate that betterment is a privilege, yet it is a central design commitment of self-tracking devices. When improvement is assumed as a default goal of self-tracking, it helps to affirm existing social inequalities by generalizing the implicit biases of technologists and designers.   In turn, this research offers several strategies for identifying areas of misalignment between practitioners and users of self-tracking technologies and techniques to broaden pathways for diversifying the assumptions of technologists and designers. Current solutions to this problem have primarily focused on integrating ethics into computer science and design education and broadening data literacy efforts to the general public. The strategies and resources generated from this project expand such efforts by addressing how data literacy can include the expert position by offering aids to minimize biased design commitments. Lastly, this research provides guidance for enhancing community-based design practices for integration into educational, professional, and community settings.       Last Modified: 06/23/2021       Submitted by: Janine Slaker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
