<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STTR Phase II:  Dynamic Robust Hand Model for Gesture Intent Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>1091828</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this project stems from addressing the&lt;br/&gt;important hand gesture based input challenges of VR/AR (expected to grow to $150B by&lt;br/&gt;2020), Robotics and IoT ($135B by 2019 and $1.7T by 2020 respectively). This technology,&lt;br/&gt;if successful in mitigating the high technical risks, represents a huge leap in the state of the&lt;br/&gt;art in 3D hand models for gesture recognition and has the potential to be the industry&lt;br/&gt;standard for AR, VR, Robotics and IoT applications with broad societal impact in education,&lt;br/&gt;medical and healthcare. Its broader impact is further amplified by the potential in serving the&lt;br/&gt;needs of the disabled community in improving their quality of life by being better able to&lt;br/&gt;communicate, learn and adapt to their interaction needs.&lt;br/&gt;&lt;br/&gt;This Small Business Technology Transfer (STTR) Phase 2 project aims to significantly&lt;br/&gt;advance current 3D hand gesture recognition technology by developing a dynamic hand&lt;br/&gt;tracking model for gesture intent recognition. It is robust against occlusion and tolerant to&lt;br/&gt;variations in camera orientation and position. This research will result in a transformative&lt;br/&gt;leap above the current state of academic and commercial hand models and overcome key&lt;br/&gt;technical hurdles that have so far proven difficult to overcome. It solves the following key&lt;br/&gt;challenges and involves very high technical risks: 1) robust hand tracking while holding&lt;br/&gt;objects and 2) robust tangible interactions using objects without using any fiducial markers&lt;br/&gt;2) low profile hand wearable for touch interaction detection. This Phase 2 project will&lt;br/&gt;achieve these objectives by 1) data acquisition and hand-object pose estimation, 2)&lt;br/&gt;understanding user intents with enhanced tangible interactions, and 3) system validation&lt;br/&gt;and user testing.</AbstractNarration>
<MinAmdLetterDate>08/31/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/18/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1738888</AwardID>
<Investigator>
<FirstName>Karthik</FirstName>
<LastName>Ramani</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karthik Ramani</PI_FULL_NAME>
<EmailAddress>ramani@purdue.edu</EmailAddress>
<PI_PHON>7654945725</PI_PHON>
<NSF_ID>000284152</NSF_ID>
<StartDate>08/31/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Raja</FirstName>
<LastName>Jasti</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raja Jasti</PI_FULL_NAME>
<EmailAddress>raja@zeroui.com</EmailAddress>
<PI_PHON>4084898622</PI_PHON>
<NSF_ID>000632415</NSF_ID>
<StartDate>08/31/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>ZeroUI Inc</Name>
<CityName>Cupertino</CityName>
<ZipCode>950144442</ZipCode>
<PhoneNumber>4088630555</PhoneNumber>
<StreetAddress>10570 Whitney Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078658977</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ZEROUI, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[ZeroUI Inc]]></Name>
<CityName>San Jose</CityName>
<StateCode>CA</StateCode>
<ZipCode>951132260</ZipCode>
<StreetAddress><![CDATA[100 W. San Fernando St. Ste 114]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA19</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1591</Code>
<Text>STTR Phase II</Text>
</ProgramElement>
<ProgramReference>
<Code>1591</Code>
<Text>STTR PHASE II</Text>
</ProgramReference>
<ProgramReference>
<Code>165E</Code>
<Text>SBIR Phase IIB</Text>
</ProgramReference>
<ProgramReference>
<Code>169E</Code>
<Text>SBIR Tech Enhan Partner (TECP)</Text>
</ProgramReference>
<ProgramReference>
<Code>4080</Code>
<Text>ADVANCED COMP RESEARCH PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>5373</Code>
<Text>SMALL BUSINESS PHASE II</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8035</Code>
<Text>Hardware Devices</Text>
</ProgramReference>
<ProgramReference>
<Code>8240</Code>
<Text>SBIR/STTR CAP</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~750000</FUND_OBLG>
<FUND_OBLG>2018~149999</FUND_OBLG>
<FUND_OBLG>2019~10000</FUND_OBLG>
<FUND_OBLG>2020~181829</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has developed a breakthrough innovation in 3D hand gesture intent recognition for AR and VR&nbsp;that can 1) robustly work across different 3D cameras,orientations, positions and occlusions 2) allow for using any object as a handheld tool 3) enable touch interactions on hands. This research is a transformative leap above the current state of academic and commercialhand models and overcomes key technical hurdles that have so far proven difficult to overcome. It addresses a key challenge in gesture recognition while enabling natural spatial interactions for Virtual and Augmented Reality (VR/AR) and many other applications enabled by 3D depth cameras. It also resulted in a novel hand wearable for gesure intent recognition for use in robotics, AR and VR. It resolves some very challenging issues that have prevented pure vision based approaches from succeeding in AR/VR such as start-stop of gestures, unintended interactions, modality switching, simple operations such as attending to other devices, and so on. It reduces fatigue and enables never before possible interactions and workflows with hands in roboitics, AR and VR.&nbsp;</p> <p>This project's broad commercial impact stems from addressing the important hand gesture based input challenges of VR/AR (expected to grow to $150B by 2020 - Digi-Capital), Robotics and IoT ($135B by2019 and $1.7T by 2020 respectively - IDC). Piper Jaffray highlights new market opportunities for peripheral devices that bring hands and feet into VR. This technology, being successful in mitigating the high technical risks, represents a huge leap in the state of the art in 3D hand models for gesture recognition and has the potential to be the industry standard for AR, VR, Robotics and IoT applications with broadsocietal impact in education, medical and healthcare.</p><br> <p>            Last Modified: 10/19/2020<br>      Modified by: Raja&nbsp;Jasti</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1738888/1738888_10519881_1603159589283_Photo2copy--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1738888/1738888_10519881_1603159589283_Photo2copy--rgov-800width.jpg" title="Ziro"><img src="/por/images/Reports/POR/2020/1738888/1738888_10519881_1603159589283_Photo2copy--rgov-66x44.jpg" alt="Ziro"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Hand controlled Educational Robotics Kit</div> <div class="imageCredit">ZeroUI</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Raja&nbsp;Jasti</div> <div class="imageTitle">Ziro</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has developed a breakthrough innovation in 3D hand gesture intent recognition for AR and VR that can 1) robustly work across different 3D cameras,orientations, positions and occlusions 2) allow for using any object as a handheld tool 3) enable touch interactions on hands. This research is a transformative leap above the current state of academic and commercialhand models and overcomes key technical hurdles that have so far proven difficult to overcome. It addresses a key challenge in gesture recognition while enabling natural spatial interactions for Virtual and Augmented Reality (VR/AR) and many other applications enabled by 3D depth cameras. It also resulted in a novel hand wearable for gesure intent recognition for use in robotics, AR and VR. It resolves some very challenging issues that have prevented pure vision based approaches from succeeding in AR/VR such as start-stop of gestures, unintended interactions, modality switching, simple operations such as attending to other devices, and so on. It reduces fatigue and enables never before possible interactions and workflows with hands in roboitics, AR and VR.   This project's broad commercial impact stems from addressing the important hand gesture based input challenges of VR/AR (expected to grow to $150B by 2020 - Digi-Capital), Robotics and IoT ($135B by2019 and $1.7T by 2020 respectively - IDC). Piper Jaffray highlights new market opportunities for peripheral devices that bring hands and feet into VR. This technology, being successful in mitigating the high technical risks, represents a huge leap in the state of the art in 3D hand models for gesture recognition and has the potential to be the industry standard for AR, VR, Robotics and IoT applications with broadsocietal impact in education, medical and healthcare.       Last Modified: 10/19/2020       Submitted by: Raja Jasti]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
