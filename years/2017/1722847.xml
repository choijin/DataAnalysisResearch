<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SCH: INT: Multispectral Panoramic 3-D Endoscopic Imaging</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>02/28/2022</AwardExpirationDate>
<AwardTotalIntnAmount>850111.00</AwardTotalIntnAmount>
<AwardAmount>850111</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project seeks to develop a new smart sensor technology for a widely-used clinical procedure called endoscopy, which is the most critical component in minimum- and non-invasive robotic procedures. Robotic medical diagnosis and surgery offers significant benefits both to the patient by minimizing damage and hence reducing recovery time, and to the clinician by enhancing his/her operation accuracy, dexterity, and efficiency. A main challenge in these robotic procedures is the lack of real-time, full-color, true three-dimensional (3-D) endoscopic imaging with good spatial resolution, large field of view and low cost. Mature 3-D imaging technologies, such as Computed Tomography (CT), Magnetic Resonance Imaging (MRI), and ultrasound, take time to complete and process, rendering them impractical in most real-time clinical procedures. Stereo-vision microscopic cameras similar to sensors in Microsoft Kinect can be adopted to generate 3-D endoscope images, but the depth resolution and field of view are severely limited due to the endoscope's small physical dimensions. Further, a stereo-vision camera can only create the perception of 3-D depth in the clinician's brain, but cannot reconstruct a true 3-D image. The latter is needed to match the real-time 3-D image to the ones acquired before the procedure by other methods (CT or MRI) and guide the clinician to identify the anatomy and locate the point of interest. The developed smart sensor technology can greatly improve the efficiency, prevent the errors, and lower the cost in our healthcare systems. This project will advance smart medical technologies beyond the current state-of-the-arts, such as electronic medical documentation and wireless networking of existing medical devices, by incorporating the latest sensor technologies into the medical devices themselves and integrating these devices into a complete smart system. The synergy between devices comes not only through data communications but also by complementing each other's medical functionalities, and thus improving the performance and enabling new clinical capabilities for the whole system. Smart medical systems built on the sensor technology developed in this project have the potential to generate significantly technological, economical, and social impacts.&lt;br/&gt;&lt;br/&gt;This project seeks to create a new microscopic, true 3-D imaging technology based on the latest advances in silicon photonics, 3-D integration, and imaging analysis techniques. It will be applied to endoscopy applications with good spatial resolution (1 degree angular resolution and sub-millimeter depth resolution), panoramic field of view, multi-spectral capability, high reliability, small size, and low cost. It will enable real-time, ``full-color", endoscopic imaging in a complex anatomy such as inside a bladder with cancer tissues, and the generated true 3-D images can be viewed without glasses and matched with the pre-procedure reference. The fully-integrated sensor prototypes will be fabricated and assembled in the PIs' laboratories, and then tested in a setting similar to the clinical application. The algorithms and computation schemes for 3-D image analysis will be developed and demonstrated. The vertically integrated research activities span clinical applications, software, systems, circuits, and devices, and will be carried out by a multidisciplinary research team that involves Electrical and Computer Engineering, Computer Science, and medicine.</AbstractNarration>
<MinAmdLetterDate>07/27/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1722847</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Huang</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael C Huang</PI_FULL_NAME>
<EmailAddress>michael.huang@rochester.edu</EmailAddress>
<PI_PHON>5852752111</PI_PHON>
<NSF_ID>000300067</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hui</FirstName>
<LastName>Wu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hui Wu</PI_FULL_NAME>
<EmailAddress>hui.wu@rochester.edu</EmailAddress>
<PI_PHON>5852752112</PI_PHON>
<NSF_ID>000487102</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jiebo</FirstName>
<LastName>Luo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jiebo Luo</PI_FULL_NAME>
<EmailAddress>jluo@cs.rochester.edu</EmailAddress>
<PI_PHON>5852765784</PI_PHON>
<NSF_ID>000601671</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Guan</FirstName>
<LastName>Wu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guan Wu</PI_FULL_NAME>
<EmailAddress>guan_wu@urmc.rochester.edu</EmailAddress>
<PI_PHON>5852754031</PI_PHON>
<NSF_ID>000680230</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041294109</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ROCHESTER</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041294109</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Rochester]]></Name>
<CityName>Rochester</CityName>
<StateCode>NY</StateCode>
<ZipCode>146270166</ZipCode>
<StreetAddress><![CDATA[160 Trustee Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramElement>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8062</Code>
<Text>SCH Type II: INT</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~850111</FUND_OBLG>
</Award>
</rootTag>
