<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: INT: COLLAB: Robust, Scalable, Distributed Semantic Mapping for Search-and-Rescue and Manufacturing Co-Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>426161.00</AwardTotalIntnAmount>
<AwardAmount>442161</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to enable multiple co-robots to map and understand the environment they are in to efficiently collaborate among themselves and with human operators in education, medical assistance, agriculture, and manufacturing applications. The first distinctive characteristic of this project is that the environment will be modeled semantically, that is, it will contain human-interpretable labels (e.g., object category names) in addition to geometric data. This will be achieved through a novel, robust integration of methods from both computer vision and robotics, allowing easier communications between robots and humans in the field. The second distinctive characteristic of this project is that the increased computation load due to the addition of human-interpretable information will be handled by judiciously approximating and spreading the computations across the entire network. The novel developed methods will be evaluated by emulating real-world scenarios in manufacturing and for search-and-rescue operations, leading to potential benefits for large segments of the society. The project will include opportunities for training students at the high-school, undergraduate, and graduate levels by promoting the development of marketable skills.&lt;br/&gt;&lt;br/&gt;The project will advance the state of the art in robust semantic mapping from multiple robots by 1) developing a new optimization framework that can handle large, dynamic, uncertain environments under significant measurement errors, 2) explicitly allowing and studying interactions and information exchanges with humans with an hybrid discrete-continuous extension of the optimization framework, and 3) allowing an intelligent use and sharing of the limited computational resources possessed by the network of co-robots as a whole by enabling approximations and balancing of the computations. These developments will be driven by two particular case studies: a job-shop (small factory) scenario, where robots and fixed cameras are used to track and assist human workers during production and assembly of parts; and a classic search-and-rescue scenario, where operators use an heterogeneous team of robots to quickly assess damages and to discover survivors. These two applications, when considered together, highlight all the limitations of the currently prevalent geometric mapping solutions, and will be used as benchmarks for the project's results.</AbstractNarration>
<MinAmdLetterDate>08/03/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/07/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1734362</AwardID>
<Investigator>
<FirstName>Dario</FirstName>
<LastName>Pompili</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dario Pompili</PI_FULL_NAME>
<EmailAddress>pompili@rutgers.edu</EmailAddress>
<PI_PHON>8484458533</PI_PHON>
<NSF_ID>000501685</NSF_ID>
<StartDate>08/03/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088543925</ZipCode>
<StreetAddress><![CDATA[94 Brett Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~426161</FUND_OBLG>
<FUND_OBLG>2019~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project enabled networks of co-robots to collaborate among themselves and with humans to understand their surrounding environment at a semantic level. This allowed co-robots to work alongside human operators in a wide range of environments and applications. This project advanced the state of the art in robust semantic mapping from multiple robots as it introduced a new framework that can handle large, dynamic, and uncertain environments that explicitly allowed the study of interactions and information exchanges with humans; also, the framework allows an intelligent use and sharing of the limited computational resources possessed by the network of co-robots as a whole. These developments were driven by two case studies: a job-shop (small factory) scenario, where robots and fixed cameras track and assist human workers during production and assembly of parts; and a classic search-and-rescue scenario, where operators use a heterogeneous team of robots to quickly assess damages and discover survivors. &nbsp;</p> <p>In terms of intellectual merit, this project achieved three goals: 1) robust, scalable, semantic mapping of static environments, 2) semantic mapping of dynamic environments with multiple robots, and 3) computation control. By semantic mapping, we intend solutions that, using computer-vision algorithms, detect and classify meaningful entities, such as objects and geometric primitives (lines and planes) and incorporate them in a geometric map by following semantically meaningful constraints (e.g., objects should be on top of horizontal planes). The complexity of non-linear measurements from multiple robots and the combinatorial nature of the semantic constraints prevented previous solutions from being robust, flexible, and scaling to large problems. Our work started from where previous geometric SLAM algorithms ended, and provided novel, robust and scalable algorithms, while characterizing uncertainties and ambiguities in the produced maps.&nbsp;</p> <p>Besides supporting MS and PhD students, this project exposed undergraduate students to research and allowed them to contribute building, debugging, and testing a testbed that consists of three main parts: 1) CPU-based computational units, 2) GPU-based computational units, and 3) Robotic platforms. The testbed was used to verify the feasibility of our approach on a single CPU-based computational unit such as a Raspberry Pi and then helped us validate our approach to a team of Raspberry Pis. We then extended our solution to GPU-based computational units. We carefully profiled the resource utilization, both in terms of memory and processing power of the different computational units under different locations, input sizes, application deadline, and different conditions such as illumination and background clutter.</p> <p>Specific achievements accomplished during this project are summarized below:</p> <p>-- Implementation of end-to-end object detection algorithm on Raspberry Pi. This involved implementing traditional computer-vision algorithms and convolution neural network-based algorithms. Both exact and approximate versions of these algorithms were implemented. Furthermore, a Markov Decision Process (MDP)-based decision framework for adaptive selection of computer vision algorithms was implemented.</p> <p>-- Establishing connection between Raspberry Pi and the quadcopter using the Robot Operating System (ROS) and running experiments in different locations and conditions to study the performance of the proposed solutions.</p> <p>-- Implementation of a distributed computing framework to establish communication between multiple quadcopters. This activity involved building a resource-task mapper to decide the different tasks of the object-detection algorithm executed by quadcopters in the team based on the resources available.</p> <p>-- Finally, we extended our analysis to GPUs, whose parallel structure makes them more effective than general-purpose CPUs for algorithms like deep learning where the processing of large blocks of visual data is done in parallel. We showed that it is possible to bring a many-fold increase in the overall processing speed by running an application in parallel (e.g., by exploiting the inherent parallel structure of the application or executing computer-vision algorithm on multiple images in parallel) along with approximation.</p><br> <p>            Last Modified: 01/29/2021<br>      Modified by: Dario&nbsp;Pompili</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project enabled networks of co-robots to collaborate among themselves and with humans to understand their surrounding environment at a semantic level. This allowed co-robots to work alongside human operators in a wide range of environments and applications. This project advanced the state of the art in robust semantic mapping from multiple robots as it introduced a new framework that can handle large, dynamic, and uncertain environments that explicitly allowed the study of interactions and information exchanges with humans; also, the framework allows an intelligent use and sharing of the limited computational resources possessed by the network of co-robots as a whole. These developments were driven by two case studies: a job-shop (small factory) scenario, where robots and fixed cameras track and assist human workers during production and assembly of parts; and a classic search-and-rescue scenario, where operators use a heterogeneous team of robots to quickly assess damages and discover survivors.    In terms of intellectual merit, this project achieved three goals: 1) robust, scalable, semantic mapping of static environments, 2) semantic mapping of dynamic environments with multiple robots, and 3) computation control. By semantic mapping, we intend solutions that, using computer-vision algorithms, detect and classify meaningful entities, such as objects and geometric primitives (lines and planes) and incorporate them in a geometric map by following semantically meaningful constraints (e.g., objects should be on top of horizontal planes). The complexity of non-linear measurements from multiple robots and the combinatorial nature of the semantic constraints prevented previous solutions from being robust, flexible, and scaling to large problems. Our work started from where previous geometric SLAM algorithms ended, and provided novel, robust and scalable algorithms, while characterizing uncertainties and ambiguities in the produced maps.   Besides supporting MS and PhD students, this project exposed undergraduate students to research and allowed them to contribute building, debugging, and testing a testbed that consists of three main parts: 1) CPU-based computational units, 2) GPU-based computational units, and 3) Robotic platforms. The testbed was used to verify the feasibility of our approach on a single CPU-based computational unit such as a Raspberry Pi and then helped us validate our approach to a team of Raspberry Pis. We then extended our solution to GPU-based computational units. We carefully profiled the resource utilization, both in terms of memory and processing power of the different computational units under different locations, input sizes, application deadline, and different conditions such as illumination and background clutter.  Specific achievements accomplished during this project are summarized below:  -- Implementation of end-to-end object detection algorithm on Raspberry Pi. This involved implementing traditional computer-vision algorithms and convolution neural network-based algorithms. Both exact and approximate versions of these algorithms were implemented. Furthermore, a Markov Decision Process (MDP)-based decision framework for adaptive selection of computer vision algorithms was implemented.  -- Establishing connection between Raspberry Pi and the quadcopter using the Robot Operating System (ROS) and running experiments in different locations and conditions to study the performance of the proposed solutions.  -- Implementation of a distributed computing framework to establish communication between multiple quadcopters. This activity involved building a resource-task mapper to decide the different tasks of the object-detection algorithm executed by quadcopters in the team based on the resources available.  -- Finally, we extended our analysis to GPUs, whose parallel structure makes them more effective than general-purpose CPUs for algorithms like deep learning where the processing of large blocks of visual data is done in parallel. We showed that it is possible to bring a many-fold increase in the overall processing speed by running an application in parallel (e.g., by exploiting the inherent parallel structure of the application or executing computer-vision algorithm on multiple images in parallel) along with approximation.       Last Modified: 01/29/2021       Submitted by: Dario Pompili]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
