<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Bundle Level Type Gradient Sliding Methods for Large Scale Convex Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>154975.00</AwardTotalIntnAmount>
<AwardAmount>154975</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this research is to develop novel algorithms for tackling the computational challenges involved in analyzing data for applications with huge data sets. These include, for example, image processing, data mining, bioinformatics, and statistical learning. The algorithms to be developed in this research will be able to significantly reduce the number of required expensive computations, so that they can be applied to efficiently extract useful information from massive data sets. The research has the potential to advance the algorithms for large scale problems, and greatly increase the applicability for many emerging technologies. An example is the efficient reconstruction of images acquired using partial parallel magnetic resonance imaging. The development of the new methods will also enable researchers to build multi-level complex networks for better learning and prediction in many applications. This project also supports education through undergraduate and graduate student training, course development, and seminar and conference presentations. &lt;br/&gt;&lt;br/&gt;This research intends to develop a novel class of accelerated bundle level type gradient sliding methods and related theories for solving large scale composite convex optimization problems and functional constrained convex optimization problems. This new class of algorithms is expected to achieve optimal iteration complexity for each component separately, but will be more general and able to handle the composition of functions with various degrees of smoothness. The algorithms offer the advantages of effectively using historical information, having a scalable scheme for solving the involved sub-problem, providing practical termination conditions for the gradient sliding, and do not impose restrictions on step sizes or require the information on the Lipschitz constants in the cost functions. Moreover, the development of these techniques for the functionally constrained problems will significantly reduce the iteration complexities and improve the practical performance of the existing techniques for functions that are smooth or weakly smooth. Further, the composite gradient sliding and accelerated approach reduces the number of gradient evaluations without increasing  the iteration complexity, while maintaining existing good properties of the approaches for the composite convex problems. The iteration complexity of all the new algorithms will be analyzed, and the practical performance will be validated through numerical simulations and for  practical applications  arising from imaging and machine learning.</AbstractNarration>
<MinAmdLetterDate>07/05/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/05/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1719932</AwardID>
<Investigator>
<FirstName>Yunmei</FirstName>
<LastName>Chen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yunmei Chen</PI_FULL_NAME>
<EmailAddress>yun@math.ufl.edu</EmailAddress>
<PI_PHON>3523920281</PI_PHON>
<NSF_ID>000483042</NSF_ID>
<StartDate>07/05/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>Gainesville</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress><![CDATA[University Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~154975</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>The research u<span>nder this support </span>has led to more efficient algorithms , especially <span>bundle level (BL) type of methods for solving large scale convex optimization problems arising from machine learning and data analysis problems in various disciplines. Three types of accelerated bundle level (BL) methods have been developed that can solve large scale smooth, weakly smooth and nonsmooth convex optimization problems in a united way. They are the exact and inexact BL methods for unconstrained and ball constrained problems, and exact BL method for functional constrained problems. Most of those algorithms achieve optimal iteration complexity. We also extend our research to study distributed consensus problem and integration of optimization and deep learning. We developed a randomized incremental primal dual algorithm for distributed consensus problem on networks that reduces much communication cost. Recently, motivated by machine learning, we developed two learnable optimization algorithms for solving nonconvex and nonsmooth inverse problems and provided convergence analysis.&nbsp;The developed algorithms have been applied to image analysis in medicine and material science.</span></span></p> <p><span>The research has a broad impact on education. Five of my students have been involved in every aspect of the research. Three of them received their Ph.D.degree. Students are co-authors of the publications and presenters in conferences including SIAM Conferences, <span>MICCAI Workshop</span> and SPIE Medical Imaging conference. The PI has organized special sessions, presented the findings at several international conferences. The PI also developed graduate topic courses to introduce this research to students. The new results have been disseminated to communities of interest by posting the manuscripts and code on the web to public.</span></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/30/2020<br>      Modified by: Yunmei&nbsp;Chen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The research under this support has led to more efficient algorithms , especially bundle level (BL) type of methods for solving large scale convex optimization problems arising from machine learning and data analysis problems in various disciplines. Three types of accelerated bundle level (BL) methods have been developed that can solve large scale smooth, weakly smooth and nonsmooth convex optimization problems in a united way. They are the exact and inexact BL methods for unconstrained and ball constrained problems, and exact BL method for functional constrained problems. Most of those algorithms achieve optimal iteration complexity. We also extend our research to study distributed consensus problem and integration of optimization and deep learning. We developed a randomized incremental primal dual algorithm for distributed consensus problem on networks that reduces much communication cost. Recently, motivated by machine learning, we developed two learnable optimization algorithms for solving nonconvex and nonsmooth inverse problems and provided convergence analysis. The developed algorithms have been applied to image analysis in medicine and material science.  The research has a broad impact on education. Five of my students have been involved in every aspect of the research. Three of them received their Ph.D.degree. Students are co-authors of the publications and presenters in conferences including SIAM Conferences, MICCAI Workshop and SPIE Medical Imaging conference. The PI has organized special sessions, presented the findings at several international conferences. The PI also developed graduate topic courses to introduce this research to students. The new results have been disseminated to communities of interest by posting the manuscripts and code on the web to public.                            Last Modified: 08/30/2020       Submitted by: Yunmei Chen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
