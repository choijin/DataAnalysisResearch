<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Identifying When People Need a Robot's Assistance</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2018</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Robot collaborators and assistants have the potential to improve lives by helping people perform physical tasks more safely, quickly, and effectively. For example, wheelchair-mounted assistive robot arms can help people with motor impairments perform activities of daily living (like eating) independently, increasing their self-sufficiency and quality of life. However, robot assistance is limited by the fact that robots cannot always recognize when people want or need help. The goal of this research is to develop algorithms that enable robots to recognize when a person is having difficulty with a physical task, based on their behavior before they reach a failure point, and then provide the necessary assistance to complete the task. This work will draw from psychology to explore how nonverbal behaviors like eye gaze, body posture, and facial expression can reveal people's need for assistance. The project will include a data collection study of nonverbal behavior during robot operation. The nonverbal behavior collected during this study will be open sourced to enable other researchers to draw insights about human behavior during human-robot interactions. The work will improve the usefulness of collaborative and assistive robots and lead to better integration of personal robots in workplaces, homes, and assistive care environments.&lt;br/&gt;&lt;br/&gt;The main research question in this work is: can robots recognize that a person needs assistance based on their nonverbal behaviors during a human-robot interaction? To investigate this, the project has four goals. Goal 1: Recognize the need for assistance. The work will begin with a large-scale data collection of people's nonverbal behavior (eye gaze, body posture, and facial expressions) during an assistive human-robot manipulation task. Using machine learning approaches, the project team will train predictors on the data that can use nonverbal behavior patterns to recognize when people need assistance. Goal 2: Provide assistance. By monitoring real-time nonverbal behaviors during a human-robot interaction, this system will use the trained predictors from Goal 1 to identify when a person needs help. Once the system predicts that assistance is required, it should be able to provide that assistance seamlessly and in real time using shared autonomy. Goal 3: Evaluate the system. Individual system components will be validated separately, then a full-scale evaluation will be conducted to measure the utility of the implemented system in a real-world assistive human-robot interaction. Goal 4: Create and disseminate an open source data set. A major goal of this project is to collect and share a data set of nonverbal behavior during assistive human-robot interaction, which represents a novel contribution to the field.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/09/2018</MinAmdLetterDate>
<MaxAmdLetterDate>03/09/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1755823</AwardID>
<Investigator>
<FirstName>Henny</FirstName>
<LastName>Admoni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Henny Admoni</PI_FULL_NAME>
<EmailAddress>hadmoni@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122689527</PI_PHON>
<NSF_ID>000754294</NSF_ID>
<StartDate>03/09/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Overview: Robot assistants have the potential to help people perform physical tasks more safely, quickly, and effectively. For example, wheelchair-mounted assistive robot arms can help people with motor impairments perform activities of daily living (like eating) independently, increasing their self-sufficiency and quality of life. Similarly, remotely teleoperated robots can enter hazardous environments, like a collapsed building, allowing operators to accomplish tasks like clearing rubble or searching for survivors. Using a process called shared control, these robots can assist their operators by taking independent, intelligent actions toward the operator's goal, saving operators the effort of full teleoperation. However, shared control is limited by the fact that robots cannot always recognize when and how people need help. The goal of the proposed research is to develop algorithms that enable robots to recognize when an operator is having difficulty with a physical task, based on their behavior before they reach a failure point, and then provide the necessary assistance to complete the task.&nbsp;</p> <p>Intellectual Merit: The domain for this work is assistive manipulation, in which operators teleoperate a robot manipulator, by using a joystick, to complete physical actions such as picking up objects. At the same time, the robot provides intelligent assistive actions such as independently positioning the robot's hand near the target object.</p> <p>To accomplish our goal of developing intelligent assistive algorithms, we established four sub-goals: (1) Train models to recognize the need for assistance. By monitoring natural behaviors like eye gaze, our system should learn to predict that a person needs help before they fail to complete their task, and without requiring the user to make a specific help request. (2) Develop algorithms that provide assistance. Once the system predicts that assistance is required, it should be able to provide that assistance seamlessly and in real time. (3) Evaluate the system. Individual system components should be validated separately, then a full-scale evaluation should show the utility of the implemented system in a real-world human-robot interaction study. (4) Create and disseminate an open source data set. The data collected during this project can also be useful to other researchers interested in human behavior during robot interactions.</p> <p>As a result of our research, we developed new models and algorithms for robot assistance. Our methods use natural human behaviors---specifically, where people are looking with their eyes---to interpret operators' intended targets during manipulation. The system then assists operators in moving the robot toward that target, reducing the amount of effort they have to exert. We evaluated our system on existing data, showing that our gaze-based prediction works better than the prior method of target prediction that uses only joystick input. We also ran a real-world human subjects study to assess our novel assistance system.</p> <p>Furthermore, this project supported the creation and use of an open-source dataset of human behavior during human-robot assisted manipulation, called HARMONIC. This dataset, which provides about 5 hours of video, gaze tracking, and other data, will be useful to future researchers interested in human-robot interaction, computer vision, and machine learning.</p> <p>Impact: Our findings have impacted the field of human-robot interaction. We established natural eye gaze as a viable source of information about operators' intentions during robot teleoperation. We've introduced models for using gaze to recognize targets, and integrated these models into existing shared control algorithms. We've also identified the (potentially unwanted) impact of anticipatory robot assistance, opening this area up for future study.&nbsp;</p> <p>The research developed under this project also provided new teaching and outreach opportunities. Seven graduate and undergraduate students directly contributed to research on this project, gaining new skills in the process. The dataset and case studies from the research were used to illustrate key concepts in the PI's courses at both graduate and undergraduate levels. We shared knowledge about this research with the public through talks and demonstrations, with audiences ranging from K-12 school children to senior executives at international corporations.</p><br> <p>            Last Modified: 07/07/2021<br>      Modified by: Henny&nbsp;Admoni</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1755823/1755823_10533777_1625683154695_overview_smaller--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1755823/1755823_10533777_1625683154695_overview_smaller--rgov-800width.jpg" title="Illustration of the teleoperation system"><img src="/por/images/Reports/POR/2021/1755823/1755823_10533777_1625683154695_overview_smaller--rgov-66x44.jpg" alt="Illustration of the teleoperation system"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our robot uses natural (unconstrained) eye gaze to predict its operator's intention, and then provides assistance toward achieving their goal.</div> <div class="imageCredit">Nadia Al Mutlak / HARP Lab</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Henny&nbsp;Admoni</div> <div class="imageTitle">Illustration of the teleoperation system</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Overview: Robot assistants have the potential to help people perform physical tasks more safely, quickly, and effectively. For example, wheelchair-mounted assistive robot arms can help people with motor impairments perform activities of daily living (like eating) independently, increasing their self-sufficiency and quality of life. Similarly, remotely teleoperated robots can enter hazardous environments, like a collapsed building, allowing operators to accomplish tasks like clearing rubble or searching for survivors. Using a process called shared control, these robots can assist their operators by taking independent, intelligent actions toward the operator's goal, saving operators the effort of full teleoperation. However, shared control is limited by the fact that robots cannot always recognize when and how people need help. The goal of the proposed research is to develop algorithms that enable robots to recognize when an operator is having difficulty with a physical task, based on their behavior before they reach a failure point, and then provide the necessary assistance to complete the task.   Intellectual Merit: The domain for this work is assistive manipulation, in which operators teleoperate a robot manipulator, by using a joystick, to complete physical actions such as picking up objects. At the same time, the robot provides intelligent assistive actions such as independently positioning the robot's hand near the target object.  To accomplish our goal of developing intelligent assistive algorithms, we established four sub-goals: (1) Train models to recognize the need for assistance. By monitoring natural behaviors like eye gaze, our system should learn to predict that a person needs help before they fail to complete their task, and without requiring the user to make a specific help request. (2) Develop algorithms that provide assistance. Once the system predicts that assistance is required, it should be able to provide that assistance seamlessly and in real time. (3) Evaluate the system. Individual system components should be validated separately, then a full-scale evaluation should show the utility of the implemented system in a real-world human-robot interaction study. (4) Create and disseminate an open source data set. The data collected during this project can also be useful to other researchers interested in human behavior during robot interactions.  As a result of our research, we developed new models and algorithms for robot assistance. Our methods use natural human behaviors---specifically, where people are looking with their eyes---to interpret operators' intended targets during manipulation. The system then assists operators in moving the robot toward that target, reducing the amount of effort they have to exert. We evaluated our system on existing data, showing that our gaze-based prediction works better than the prior method of target prediction that uses only joystick input. We also ran a real-world human subjects study to assess our novel assistance system.  Furthermore, this project supported the creation and use of an open-source dataset of human behavior during human-robot assisted manipulation, called HARMONIC. This dataset, which provides about 5 hours of video, gaze tracking, and other data, will be useful to future researchers interested in human-robot interaction, computer vision, and machine learning.  Impact: Our findings have impacted the field of human-robot interaction. We established natural eye gaze as a viable source of information about operators' intentions during robot teleoperation. We've introduced models for using gaze to recognize targets, and integrated these models into existing shared control algorithms. We've also identified the (potentially unwanted) impact of anticipatory robot assistance, opening this area up for future study.   The research developed under this project also provided new teaching and outreach opportunities. Seven graduate and undergraduate students directly contributed to research on this project, gaining new skills in the process. The dataset and case studies from the research were used to illustrate key concepts in the PI's courses at both graduate and undergraduate levels. We shared knowledge about this research with the public through talks and demonstrations, with audiences ranging from K-12 school children to senior executives at international corporations.       Last Modified: 07/07/2021       Submitted by: Henny Admoni]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
