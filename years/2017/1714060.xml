<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAPSI: Multimodal Interaction Algorithm for Human-Robot Interaction with Biologically-Inspired Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>5400.00</AwardTotalIntnAmount>
<AwardAmount>5400</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In order for robots to engage in physical interactions with humans, their behavior needs to be intrinsically safe. Current state-of-the-art algorithms do so by maintaining an awareness of humans with single-modality data models, for example limb positions derived from a camera. However, approaches using only a single input modality typically neglect other potentially critical sources of information. In addition, they are prone to noise, failures, or line-sight problems, since no redundant data sources are available. This project will address this issue with the development of a machine learning algorithm which performs human intention inference and determines an appropriate robot response for human-robot interaction using multi-modal data sources, such as camera, accelerometer, shoe-based pressure sensor, and electromyography data. This research will be conducted in collaboration with Dr. Shuhei Ikemoto and Dr. Koh Hosoda at Osaka University, who specialize in biologically-inspired robotics and human-robot interaction. This project benefits from Dr. Ikemoto's and Dr. Hosoda's invaluable expertise, as well as access to a human-inspired, pneumatically-actuated robot located at Osaka University which will enable unique human-robot interaction scenarios that are not currently possible at the researcher's institution.&lt;br/&gt;&lt;br/&gt;Interaction Primitives are a state-of-the-art framework for modeling the interaction that takes place between a robot and a human during collaborative, physical activities. However, this existing framework is designed to work in low dimensional space with a single data modality for each agent. When introducing additional modalities in this project, the dimensionality of the input data will increase rapidly. At the same time, the number of training samples will remain constant, since that is physically constrained by the number of demonstrations that can be performed. To resolve this issue, this project will develop an extension of Interaction Primitives capable of performing density estimation with high dimension, low sample size data sets with the goal of producing safer, and more accurate interactions.&lt;br/&gt;&lt;br/&gt;This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science.</AbstractNarration>
<MinAmdLetterDate>05/04/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/04/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1714060</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Campbell</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph Campbell</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>4802969922</PI_PHON>
<NSF_ID>000685877</NSF_ID>
<StartDate>05/04/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Campbell                Joseph</Name>
<CityName>Chandler</CityName>
<ZipCode>852266097</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Campbell                Joseph]]></Name>
<CityName>Chandler</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852266097</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5921</Code>
<Text>JAPAN</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~5400</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As robotic systems are employed in an ever-expanding variety of roles, interactions with humans will inevitably occur. The primary objective in all human-robot interactions should be the safety of the human, and current state-of-the-art algorithms do so by maintaining an awareness of humans with low-modality data models, for example with pose estimates generated from observations of limb positions. These models are subject to noise and do not yield much insight into the dynamics of the human body, which can result in a misunderstanding of the human's true actions and consequently unexpected behavior by the robot, possibly leading to injury. Furthermore, traditional robots which consist of non-compliant, rigid limbs actuated by electric gear motors are not an ideal choice for close-contact human-robot interaction, potentially amplifying the risk posed by unexpected behavior.</p> <p>This project&rsquo;s primary goal was to address these issues with the development of a machine learning-based interaction algorithm which identifies human actions, recognizes the human's intent, and produces an appropriate robotic response based on multimodal sensor observations, such as those from cameras, accelerometers, and electromyographs. This algorithm was to be deployed on a biologically-inspired humanoid robot with a musculoskeletal structure consisting of non-rigid limbs and pneumatic actuators, which is capable of inherently safer interactions due to its compliant nature and ability to generate human-like motions. This research was performed under the guidance of Dr. Shuhei Ikemoto and Dr. Koh Hosoda from Osaka University, who specialize in biologically inspired robots and human-robot interaction.</p> <p>The project utilized an algorithm known as Bayesian Interaction Primitives, which uses sensor observations to generate a robotic response during interaction with a human based on prior demonstrations. This project specifically focused on a hand shake scenario in which the robot shakes the hand of a human partner; while seemingly simple, this task is challenging due to the variability of a handshake in both its speed and its location in three-dimensional space. Intention inference is required on both sides: the robot must anticipate where and when the human partner wishes to shake hands while the human naturally responds to the robot&rsquo;s perceived actions.</p> <p>Experiments based on this physical human-robot interaction yielded promising results. Despite the difficulty of controlling such a dynamic robot, the interaction algorithm produced robot actions resulting in successful interactions. The robot was able to generalize its motion in both time and space, including extreme scenarios such as the human refusing to shake the robot&rsquo;s hand. Furthermore, it was found that due to the biologically-inspired structure of the robot it produced human-like motions with no extra effort. This allowed the human to infer the intention of the robot and resulted in smooth and efficient interactions.</p> <p>This work shows great promise in the area of human-robot interaction. Despite the difficulty in working with a compliant, pneumatically-actuated robot there are many benefits in doing so, chief among them safety. These initial results show that it is indeed possible to produce safe and efficient interactions between a human and such a robot.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/16/2018<br>      Modified by: Joseph&nbsp;Campbell</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As robotic systems are employed in an ever-expanding variety of roles, interactions with humans will inevitably occur. The primary objective in all human-robot interactions should be the safety of the human, and current state-of-the-art algorithms do so by maintaining an awareness of humans with low-modality data models, for example with pose estimates generated from observations of limb positions. These models are subject to noise and do not yield much insight into the dynamics of the human body, which can result in a misunderstanding of the human's true actions and consequently unexpected behavior by the robot, possibly leading to injury. Furthermore, traditional robots which consist of non-compliant, rigid limbs actuated by electric gear motors are not an ideal choice for close-contact human-robot interaction, potentially amplifying the risk posed by unexpected behavior.  This project?s primary goal was to address these issues with the development of a machine learning-based interaction algorithm which identifies human actions, recognizes the human's intent, and produces an appropriate robotic response based on multimodal sensor observations, such as those from cameras, accelerometers, and electromyographs. This algorithm was to be deployed on a biologically-inspired humanoid robot with a musculoskeletal structure consisting of non-rigid limbs and pneumatic actuators, which is capable of inherently safer interactions due to its compliant nature and ability to generate human-like motions. This research was performed under the guidance of Dr. Shuhei Ikemoto and Dr. Koh Hosoda from Osaka University, who specialize in biologically inspired robots and human-robot interaction.  The project utilized an algorithm known as Bayesian Interaction Primitives, which uses sensor observations to generate a robotic response during interaction with a human based on prior demonstrations. This project specifically focused on a hand shake scenario in which the robot shakes the hand of a human partner; while seemingly simple, this task is challenging due to the variability of a handshake in both its speed and its location in three-dimensional space. Intention inference is required on both sides: the robot must anticipate where and when the human partner wishes to shake hands while the human naturally responds to the robot?s perceived actions.  Experiments based on this physical human-robot interaction yielded promising results. Despite the difficulty of controlling such a dynamic robot, the interaction algorithm produced robot actions resulting in successful interactions. The robot was able to generalize its motion in both time and space, including extreme scenarios such as the human refusing to shake the robot?s hand. Furthermore, it was found that due to the biologically-inspired structure of the robot it produced human-like motions with no extra effort. This allowed the human to infer the intention of the robot and resulted in smooth and efficient interactions.  This work shows great promise in the area of human-robot interaction. Despite the difficulty in working with a compliant, pneumatically-actuated robot there are many benefits in doing so, chief among them safety. These initial results show that it is indeed possible to produce safe and efficient interactions between a human and such a robot.          Last Modified: 03/16/2018       Submitted by: Joseph Campbell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
