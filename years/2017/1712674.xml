<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Small: A Virtualized Network Resource Pool for Software-Defined Network Management</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>350702.00</AwardTotalIntnAmount>
<AwardAmount>350702</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Management of today's networks usually requires an army of operators who devote tremendous time and energy. Software-defined networking (SDN) has been shown to be a promising paradigm for simplifying network management. However, management tasks in SDNs require the use of constrained network resources: switch memory and CPU, and the switch-controller network bandwidth. Given these resource constraints, network operators may have to reason about resource usage when initiating network management tasks.&lt;br/&gt;&lt;br/&gt;This project seeks to explore the space of resource management for software-defined network management. Specifically, it will examine the design of virtualized resource pools that provide the abstraction of (nearly) infinite resources (memory, CPU and switch), while dynamically adapting to the resource needs of these network management tasks. This approach has the advantage that network operators do not have to reason about resource usage when instantiating network management tasks. The research will yield resource management algorithms, improved resource usage designs, and systems implementations for virtualized resource pools.&lt;br/&gt;&lt;br/&gt;Network operators make billion-dollar investments in network infrastructures, but put little thought into visibility and control of these infrastructures. This project can lead to better-managed, more reliable, and more resource-efficient network infrastructures in ISPs, enterprises and clouds. These efforts eventually will have broader societal impact by enabling commercial, social, and scientific advances. The proposal will give underrepresented groups and undergraduates opportunities to participate in research and will likely result in technology transfer to major cloud providers and ISPs.</AbstractNarration>
<MinAmdLetterDate>11/21/2016</MinAmdLetterDate>
<MaxAmdLetterDate>10/26/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1712674</AwardID>
<Investigator>
<FirstName>Zhong</FirstName>
<LastName>Shao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhong Shao</PI_FULL_NAME>
<EmailAddress>zhong.shao@yale.edu</EmailAddress>
<PI_PHON>2034326828</PI_PHON>
<NSF_ID>000201856</NSF_ID>
<StartDate>12/12/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Minlan</FirstName>
<LastName>Yu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Minlan Yu</PI_FULL_NAME>
<EmailAddress>minlanyu@seas.harvard.edu</EmailAddress>
<PI_PHON>8572725123</PI_PHON>
<NSF_ID>000607447</NSF_ID>
<StartDate>11/21/2016</StartDate>
<EndDate>12/12/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Yale University</Name>
<CityName>New Haven</CityName>
<ZipCode>065208327</ZipCode>
<PhoneNumber>2037854689</PhoneNumber>
<StreetAddress>Office of Sponsored Projects</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 208327]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043207562</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>YALE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043207562</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Yale University]]></Name>
<CityName>New Haven</CityName>
<StateCode>CT</StateCode>
<ZipCode>065208285</ZipCode>
<StreetAddress><![CDATA[AKWatson Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~350702</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p dir="ltr"><span>Management of today's networks usually requires a huge army of people (i.e., operators), who devote tremendous time and energy. To simplify network management, software-defined networking (SDN) has been shown to be a promising paradigm. However, management tasks in SDNs require the use of constrained network resources: switch memory and CPU, host CPUs, and the network bandwidth to the controller. This project seeks to explore the space of resource management for software-defined network management.&nbsp;</span></p> <p dir="ltr"><span>We first focus on resource management for measurement tasks at both hosts and switches. The first system we built is Trumpet, an event monitoring system that leverages CPU resources and end-host programmability, to monitor every packet and report at millisecond timescales.&nbsp; As data centers grow larger and strive to provide tight performance and availability SLAs, their monitoring infrastructure must move from passive systems that provide aggregated inputs to human operators, to active systems that enable programmed control. Trumpet allows operators to describe new network events such as detecting correlated bursts and loss, identifying the root cause of transient congestion, and detecting short-term anomalies at the scale of a data center tenant. The system efficiently detects these events using triggers at end-hosts, evaluate thousands of such triggers by inspecting every packet at full line rate, and report events to a controller within 10 milliseconds. The second system is FlowRadar, we designed a new way to maintain flows and their counters that scales to a large number of flows with small memory and bandwidth overhead. The key idea of FlowRadar is to encode perflow counters with a small memory and constant insertion time at switches, and then to leverage the computing power at the remote collector to perform network-wide decoding and analysis of the flow counters. With FlowRadar, operators can get better views into their networks as demonstrated by two new monitoring applications we build on top of FlowRadar.</span></p> <p dir="ltr"><span>A complementary aspect of resource management is the design of networks and topologies with adequate resources for the task. The project pioneered work in data-center topology design with the Condor work. The design space for large, multipath datacenter networks is large and complex, and no one design fits all purposes. Network architects must trade off many criteria to design cost-effective, reliable, and maintainable networks, and typically cannot explore much of the design space. Condor allows architects to express their requirements as constraints via a Topology Description Language (TDL), rather than having to directly specify network structures. Condor then uses constraint-based synthesis to rapidly generate candidate topologies, which can be analyzed against multiple criteria. TDL supports concise descriptions of topologies such as fat-trees, BCube, and DCell; Condor can generate known and novel variants of fat-trees with simple changes to a TDL file; and can synthesize large topologies in tens of seconds. A second, more recent, piece of work explores the manageability of data centers, focusing on lifecycle management. That work develops metrics for deployment and expansion of data centers and shows that existing topology designs fall short by some of these metrics. It then goes on to develop a class of topologies called FatClique which is superior to existing topology designs by all manageability metrics.</span></p> <p dir="ltr"><span>These outcomes have resulted in multiple publications, several presentations, and we have transitioned code to industry. In addition, the project has trained 2 PhD students. One undergraduate student and a few master students have also participated in the research.</span></p><br> <p>            Last Modified: 11/19/2019<br>      Modified by: Zhong&nbsp;Shao</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Management of today's networks usually requires a huge army of people (i.e., operators), who devote tremendous time and energy. To simplify network management, software-defined networking (SDN) has been shown to be a promising paradigm. However, management tasks in SDNs require the use of constrained network resources: switch memory and CPU, host CPUs, and the network bandwidth to the controller. This project seeks to explore the space of resource management for software-defined network management.  We first focus on resource management for measurement tasks at both hosts and switches. The first system we built is Trumpet, an event monitoring system that leverages CPU resources and end-host programmability, to monitor every packet and report at millisecond timescales.  As data centers grow larger and strive to provide tight performance and availability SLAs, their monitoring infrastructure must move from passive systems that provide aggregated inputs to human operators, to active systems that enable programmed control. Trumpet allows operators to describe new network events such as detecting correlated bursts and loss, identifying the root cause of transient congestion, and detecting short-term anomalies at the scale of a data center tenant. The system efficiently detects these events using triggers at end-hosts, evaluate thousands of such triggers by inspecting every packet at full line rate, and report events to a controller within 10 milliseconds. The second system is FlowRadar, we designed a new way to maintain flows and their counters that scales to a large number of flows with small memory and bandwidth overhead. The key idea of FlowRadar is to encode perflow counters with a small memory and constant insertion time at switches, and then to leverage the computing power at the remote collector to perform network-wide decoding and analysis of the flow counters. With FlowRadar, operators can get better views into their networks as demonstrated by two new monitoring applications we build on top of FlowRadar. A complementary aspect of resource management is the design of networks and topologies with adequate resources for the task. The project pioneered work in data-center topology design with the Condor work. The design space for large, multipath datacenter networks is large and complex, and no one design fits all purposes. Network architects must trade off many criteria to design cost-effective, reliable, and maintainable networks, and typically cannot explore much of the design space. Condor allows architects to express their requirements as constraints via a Topology Description Language (TDL), rather than having to directly specify network structures. Condor then uses constraint-based synthesis to rapidly generate candidate topologies, which can be analyzed against multiple criteria. TDL supports concise descriptions of topologies such as fat-trees, BCube, and DCell; Condor can generate known and novel variants of fat-trees with simple changes to a TDL file; and can synthesize large topologies in tens of seconds. A second, more recent, piece of work explores the manageability of data centers, focusing on lifecycle management. That work develops metrics for deployment and expansion of data centers and shows that existing topology designs fall short by some of these metrics. It then goes on to develop a class of topologies called FatClique which is superior to existing topology designs by all manageability metrics. These outcomes have resulted in multiple publications, several presentations, and we have transitioned code to industry. In addition, the project has trained 2 PhD students. One undergraduate student and a few master students have also participated in the research.       Last Modified: 11/19/2019       Submitted by: Zhong Shao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
