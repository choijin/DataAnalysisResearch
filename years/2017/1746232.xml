<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  A Hybrid Brain-Computer Interface for Virtual and Augmented Reality</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2018</AwardEffectiveDate>
<AwardExpirationDate>03/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>224915.00</AwardTotalIntnAmount>
<AwardAmount>224915</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Henry Ahn</SignBlockName>
<PO_EMAI>hahn@nsf.gov</PO_EMAI>
<PO_PHON>7032927069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project addresses the need for non-invasive brain-computer interfaces (BCIs) and hands-free control of technologies, including artificial and virtual reality (AR/VR) and smart devices. The proposed multi-purposed BCI is expected to have immediate applications for several industries, including manufacturing and medicine. Currently, existing systems are either too expensive or limited for real-time control. The proposed BCI is specifically designed for 3 dimensional environments, and is intended to leverage multiple ('hybrid') signals from the human body to allow increased performance using affordable hardware. It is also designed for and expected to allow AR/VR control, which can enable productivity applications, as well as model BCI use in real-world scenarios. The long term goal is to enable users to scroll menus, select objects, and even type using their brain activity. The platform uses the existing form-factor of AR/VR headsets to incorporate brain-sensing electrodes, and will be compatible with popular devices, independently or in parallel with their existing controllers. The electrodes are designed to be safe, non-invasive, and dry (requiring conductive gel or saline). The high-risk, high-reward research to be conducted under this project will significantly advance the applications of BCI systems in general, with an emphasis on AR/VR technologies.&lt;br/&gt;&lt;br/&gt;The proposed project concerns a novel hybrid BCI by combining oculomotor and electroencephalography (EEG) signals via a custom machine learning platform. BCIs detect and interpret neural signals enabling control over a variety of technologies. However, current BCIs remain extremely limited in their applicability. They either require expensive equipment, invasive surgery, or have too low performance when using affordable noninvasive hardware. This BCI aims to provide real-time control in 3-dimensional scenarios, (e.g., AR/VR/real-world smart devices), while using affordable hardware. This SBIR Phase I project seeks to combine three distinct innovations: high-performance EEG signal analysis, high-speed eye movement classification, and custom multi-signal ensemble classification techniques. Specifically, the project seeks to use a custom machine learning and artificial intelligence approach informed by physiology to combine oculomotor and EEG signals to specifically enable3D AR/VR control. The ultimate goal is to develop a high performance BCI system that affords flexible user control across hardware, software, and mobile applications.</AbstractNarration>
<MinAmdLetterDate>12/20/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/05/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1746232</AwardID>
<Investigator>
<FirstName>Ramses</FirstName>
<LastName>Alcaide</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ramses Alcaide</PI_FULL_NAME>
<EmailAddress>reaa@neurable.com</EmailAddress>
<PI_PHON>2066964469</PI_PHON>
<NSF_ID>000720735</NSF_ID>
<StartDate>03/05/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jay</FirstName>
<LastName>Jantz</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jay J Jantz</PI_FULL_NAME>
<EmailAddress>jayj@neurable.com</EmailAddress>
<PI_PHON>9788272222</PI_PHON>
<NSF_ID>000749402</NSF_ID>
<StartDate>12/20/2017</StartDate>
<EndDate>03/05/2019</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Neurable Inc.</Name>
<CityName>Cambridge</CityName>
<ZipCode>021411802</ZipCode>
<PhoneNumber>9173124551</PhoneNumber>
<StreetAddress>25 1st Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 303]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080657325</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEURABLE INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Neurable Inc.]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021411802</ZipCode>
<StreetAddress><![CDATA[25 1st Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>108E</Code>
<Text>Bioelectronics</Text>
</ProgramReference>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8042</Code>
<Text>Health and Safety</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~224915</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In summary, the work accomplished in this grant encompasses the following: signal processing and classification pipelines designed for both EEG ERP and oculomotor (eye-tracking) data streams, a method of fusing predictive information from both streams to create a more robust predictor of user intent, as well as a real-time means of recording and predicting based on EEG and ET, and a means of exploiting Saccade prediction from ET data was devised in order to minimize the use of gaze error in prediction. In regards to EEG, a multi-trial/multi-sequence test paradigm was used to evaluate results and train models.&nbsp; Specifically, an experiment was designed wherein a user would be given a choice of targets to &ldquo;activate&rdquo; during multiple trials. Within a trial, multiple attempts would given for activation. Each attempt is denoted as a sequence. This allows for the activation of a target at an earlier sequence count if sufficiently predictive criteria has been met. State-of-the-art mathematical models were combined with novel algorithms, which exploit contextual information (regarding the experiment) to better inform prediction of the user intent. For eye tracking, estimation of gaze position was improved by incorporating stochastic noise models into dynamic estimation of eye motion parameters. Gaze information was used to focus on a given on-screen target, while ERP prediction was used as a means of activation (to prevent false positives from overconfidence based on gaze position). New gaze information, which occurred during a predicted Saccade, was ignored to prevent erroneous localization of onscreen targets (prediction of Saccade occurred prior to actual Saccade event).&nbsp; In order to accomplish an ERP accuracy of &gt;90%, the fusion of information from several sequences was required (requiring at least several seconds), invalidating the hoped for milestone of reliable subsecond prediction.&nbsp; Despite this shortcoming, mean ERP (P300 detection) accuracy at sequence counts of 2, 3, 6 and 10 were improved by ~30% with regards to signal processing/classification pipelines available from current COTS and open source software solutions.&nbsp;</p><br> <p>            Last Modified: 04/05/2019<br>      Modified by: Ramses&nbsp;Alcaide</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In summary, the work accomplished in this grant encompasses the following: signal processing and classification pipelines designed for both EEG ERP and oculomotor (eye-tracking) data streams, a method of fusing predictive information from both streams to create a more robust predictor of user intent, as well as a real-time means of recording and predicting based on EEG and ET, and a means of exploiting Saccade prediction from ET data was devised in order to minimize the use of gaze error in prediction. In regards to EEG, a multi-trial/multi-sequence test paradigm was used to evaluate results and train models.  Specifically, an experiment was designed wherein a user would be given a choice of targets to "activate" during multiple trials. Within a trial, multiple attempts would given for activation. Each attempt is denoted as a sequence. This allows for the activation of a target at an earlier sequence count if sufficiently predictive criteria has been met. State-of-the-art mathematical models were combined with novel algorithms, which exploit contextual information (regarding the experiment) to better inform prediction of the user intent. For eye tracking, estimation of gaze position was improved by incorporating stochastic noise models into dynamic estimation of eye motion parameters. Gaze information was used to focus on a given on-screen target, while ERP prediction was used as a means of activation (to prevent false positives from overconfidence based on gaze position). New gaze information, which occurred during a predicted Saccade, was ignored to prevent erroneous localization of onscreen targets (prediction of Saccade occurred prior to actual Saccade event).  In order to accomplish an ERP accuracy of &gt;90%, the fusion of information from several sequences was required (requiring at least several seconds), invalidating the hoped for milestone of reliable subsecond prediction.  Despite this shortcoming, mean ERP (P300 detection) accuracy at sequence counts of 2, 3, 6 and 10 were improved by ~30% with regards to signal processing/classification pipelines available from current COTS and open source software solutions.        Last Modified: 04/05/2019       Submitted by: Ramses Alcaide]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
