<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: The Loop Chain Abstraction for Balancing Locality and Parallelism</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/16/2016</AwardEffectiveDate>
<AwardExpirationDate>01/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>259994.00</AwardTotalIntnAmount>
<AwardAmount>259994</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Title: SHF: Small: The Loop Chain Abstraction for Balancing Locality and Parallelism&lt;br/&gt;&lt;br/&gt;Computational science, which involves modeling and simulation of phenomena such as combustion in engines, has become the third pillar of science and engineering research. Computer simulations test design parameters much more cheaply than physical experiments.  Also, computer simulations participate in a fortuitous cycle with theory by enabling inexpensive experimentation of theoretical models.  Mapping computer simulations to high performance computer architectures is a challenging computer science problem; constraints include achieving high performance and effective use of computing resources while not overburdening scientific programmers.  This challenge is becoming more severe as architectures continue to evolve in ways that make them ever more difficult to use.  In this project, the PIs will remove programmer burden by developing a programming abstraction called loop chaining, which enables architecture-specific program optimizations by compilers.  This work enables scientists to spend less time dealing with annoying performance programming details and more time evolving their scientific models that help push science and engineering forward.&lt;br/&gt;&lt;br/&gt;Exposing opportunities for parallelization while explicitly managing data locality is the primary challenge to porting and optimizing existing computational science simulation codes. The most popular programming models used in these codes such as MPI (Message Passing Interface) require that programmers explicitly determine the data and computation distribution. This has led to good scaling between compute nodes, but parallelism and locality are needed within a node as well. There are many approaches for implementing shared memory parallelism, but with most of them it is the programmer's responsibility to group computations to improve data locality. This project focuses on the development of the loop chain abstraction to provide compilers with sufficient information to automate the parallelism versus data locality tradeoff.  Preliminary results show that using the loop chain abstraction can significantly improve parallel scalability.  The intellectual merits are that the loop chain abstraction will enable existing codes to maintain their software modularity while exposing information critical to performance optimizations that improve parallel scalability.  Some important contributions of this research are the re-casting of existing program optimizations to use the loop chain abstraction as input and the eventual incorporation of the loop chain abstraction into parallel programming languages.  The broader impacts include reducing the burden on scientists developing computational simulations, sharing the developed compiler prototypes as open-source software, and providing tutorials for doing source-to-source loop chain-based tiling transformations in C++ and Fortran code.  The testbed for loop chaining will include atmospheric science, materials, and combustion codes, therefore tunable versions of these applications will be released.  Additionally, a new course module will be developed, through which students will be trained in computational science and specifically, on how to expose loop chains within simulation software.</AbstractNarration>
<MinAmdLetterDate>10/28/2016</MinAmdLetterDate>
<MaxAmdLetterDate>10/28/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1700723</AwardID>
<Investigator>
<FirstName>Catherine</FirstName>
<LastName>Olschanowsky</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Catherine Olschanowsky</PI_FULL_NAME>
<EmailAddress>cathie@cs.boisestate.edu</EmailAddress>
<PI_PHON>9704606804</PI_PHON>
<NSF_ID>000642763</NSF_ID>
<StartDate>10/28/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Boise State University</Name>
<CityName>Boise</CityName>
<ZipCode>837250001</ZipCode>
<PhoneNumber>2084261574</PhoneNumber>
<StreetAddress>1910 University Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Idaho</StateName>
<StateCode>ID</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>ID02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072995848</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOISE STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072995848</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Boise State University]]></Name>
<CityName>Boise</CityName>
<StateCode>ID</StateCode>
<ZipCode>837251135</ZipCode>
<StreetAddress><![CDATA[1910 University Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Idaho</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>ID02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~259994</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many scientific applications take advantage of leadership class compute resources. These resources vary in their configuration and efficient execution using them depends on configuration-specific optimization. The optimizations cost a great deal of developer time and are typical not portable to other resources causing significant difficulty and cost for computational scientists. This project aimed to develop automated methods to optimize the applications. The methods take advantage of patterns commonly found in scientific applications.&nbsp;</p> <p><br />Exposing opportunities for parallelization while explicitly managing data locality is the primary challenge to porting and optimizing existing computational science simulation codes to improve performance and accuracy. The most popular programming models in these codes such as MPI require that programmers explicitly determine the data and computation distribution. This has led to good scaling between nodes, but parallelism and locality are needed within the node as well. There are many approaches for implementing shared memory parallelism, but with most of them it is the programmer's responsibility to group computations to improve data locality. We propose the loop chain abstraction for providing compilers with sufficient information to automate the parallelism versus data locality tradeoff.</p> <p><br />This project developed a source-to-source translation pipeline and developed different approaches to optimizing the application. The pipeline depends on annotations that are added to the source code (see figure 1). The annotations provide essential information to the translator. The pipeline involves parsing the annotations into a internal data structure, the loop chain IR. The IR is tool agnostic. For this project we integrated it with the Rose compiler framework. The pipeline transforms the input code according to instructions indicated in the annotations. The output code is then passed to a standard compiler framework.</p> <p><br />Enumerating the optimization options and choosing among them are challenging tasks for compiler frameworks. Two forms of dataflow analysis were explored to tackle the enumeration of options. First, a visual macro dataflow technique was explored. This results in static scheduling. Second, a dataflow representation used at runtime was explored. Both techniques resulted in performance improvements and show potential for future work.<br /><br /><strong>Intellectual Merit</strong><br />The loop chain abstraction was inspired by loop sequences that commonly occur in scientific codes from a number of scientific domains. Due to the difficulty and often impossibility of performing precise, inter-procedural pointer and data dependence analysis within these codes, program optimizations that could automatically navigate the parallelism versus data locality tradeoff in these loop sequences have not and probably will not be incorporated into general purpose compilers. The loop chain abstraction enables existing codes to maintain their software modularity while exposing information critical to performance optimizations that improve parallel scalability.&nbsp;<br />The performance experiments performed during this project clearly demonstrate the importance of reducing interactions with all levels of the memory hierarchy. These results will influence ongoing research in optimization as well as how scientific applications are written.</p> <p><br /><strong>Broader Impact</strong><br />Boise State University has recently started a PhD in computing program. One of the emphasis areas for this program is computational science. The PI has developed a parallel computing course that will be taken by computer and computational sciences and will use the application exemplar and dataflow programming tools developed by this project.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/03/2018<br>      Modified by: Catherine&nbsp;Olschanowsky</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1700723/1700723_10315321_1525379206339_overview--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1700723/1700723_10315321_1525379206339_overview--rgov-800width.jpg" title="An example of Loop Chain Pragmas being used to inform an internal representation."><img src="/por/images/Reports/POR/2018/1700723/1700723_10315321_1525379206339_overview--rgov-66x44.jpg" alt="An example of Loop Chain Pragmas being used to inform an internal representation."></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Loop Chaining Abstraction communicates information from the programmer to the compiler. In this example the the information is being used to inform a dataflow graph internal representation.</div> <div class="imageCredit">Catherine Olschanowsky, Boise State University</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Catherine&nbsp;Olschanowsky</div> <div class="imageTitle">An example of Loop Chain Pragmas being used to inform an internal representation.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many scientific applications take advantage of leadership class compute resources. These resources vary in their configuration and efficient execution using them depends on configuration-specific optimization. The optimizations cost a great deal of developer time and are typical not portable to other resources causing significant difficulty and cost for computational scientists. This project aimed to develop automated methods to optimize the applications. The methods take advantage of patterns commonly found in scientific applications.    Exposing opportunities for parallelization while explicitly managing data locality is the primary challenge to porting and optimizing existing computational science simulation codes to improve performance and accuracy. The most popular programming models in these codes such as MPI require that programmers explicitly determine the data and computation distribution. This has led to good scaling between nodes, but parallelism and locality are needed within the node as well. There are many approaches for implementing shared memory parallelism, but with most of them it is the programmer's responsibility to group computations to improve data locality. We propose the loop chain abstraction for providing compilers with sufficient information to automate the parallelism versus data locality tradeoff.   This project developed a source-to-source translation pipeline and developed different approaches to optimizing the application. The pipeline depends on annotations that are added to the source code (see figure 1). The annotations provide essential information to the translator. The pipeline involves parsing the annotations into a internal data structure, the loop chain IR. The IR is tool agnostic. For this project we integrated it with the Rose compiler framework. The pipeline transforms the input code according to instructions indicated in the annotations. The output code is then passed to a standard compiler framework.   Enumerating the optimization options and choosing among them are challenging tasks for compiler frameworks. Two forms of dataflow analysis were explored to tackle the enumeration of options. First, a visual macro dataflow technique was explored. This results in static scheduling. Second, a dataflow representation used at runtime was explored. Both techniques resulted in performance improvements and show potential for future work.  Intellectual Merit The loop chain abstraction was inspired by loop sequences that commonly occur in scientific codes from a number of scientific domains. Due to the difficulty and often impossibility of performing precise, inter-procedural pointer and data dependence analysis within these codes, program optimizations that could automatically navigate the parallelism versus data locality tradeoff in these loop sequences have not and probably will not be incorporated into general purpose compilers. The loop chain abstraction enables existing codes to maintain their software modularity while exposing information critical to performance optimizations that improve parallel scalability.  The performance experiments performed during this project clearly demonstrate the importance of reducing interactions with all levels of the memory hierarchy. These results will influence ongoing research in optimization as well as how scientific applications are written.   Broader Impact Boise State University has recently started a PhD in computing program. One of the emphasis areas for this program is computational science. The PI has developed a parallel computing course that will be taken by computer and computational sciences and will use the application exemplar and dataflow programming tools developed by this project.                Last Modified: 05/03/2018       Submitted by: Catherine Olschanowsky]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
