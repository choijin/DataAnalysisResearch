<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: General Intelligence through Algorithm Invention and Selection</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>427000.00</AwardTotalIntnAmount>
<AwardAmount>427000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Roger Mailler</SignBlockName>
<PO_EMAI>rmailler@nsf.gov</PO_EMAI>
<PO_PHON>7032927982</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Creating better artificial intelligence has plenty of applications in different areas of society, from self-driving cars and aircraft to production planning, control of machines and music composition. Most current artificial intelligence research focuses on creating algorithms that can only do a single thing, or solve a single problem.  To achieve artificial general intelligence we must learn how to create algorithms that can solve many different problems, without a human having to adjust the algorithm for every problem. The research in this project aims to understand how such artificial general intelligence can be created. The basic idea is to build algorithms that can create their own more specific algorithms, and learn to automatically select the right algorithm for the right problem. In order to develop these algorithms, we need a large set of good problems to test them on. Games are widely used to test AI algorithms, because they model real-world problems but are fast and easy to execute. The general algorithms developed in this project will be tested on a set of classic games, and a real-time strategy game.&lt;br/&gt;&lt;br/&gt;This research project aims to investigate how we can create more general artificial intelligence through online stochastic search for algorithms, combined with and informed by online selection among discovered algorithms. In other words, the project will investigate the combination of genetic programming in the space of tree search algorithms with algorithm selection, also called hyper-heuristics, for creating more general problem-solving abilities. These capabilities will be evaluated through a sequence of experiments on two different test beds.  Successful completion of the research will clarify the potential of search in algorithm space as a method for creating more general artificial intelligence, and produce a number of algorithms. This includes both the algorithms that will be designed for searching for algorithms and searching among algorithms, as well as the new algorithms that will be discovered by the search in algorithm space. The methods produced are expected to ultimately be generally applicable to a large number of problems.</AbstractNarration>
<MinAmdLetterDate>07/25/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1717324</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Nealen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew Nealen</PI_FULL_NAME>
<EmailAddress>nealen@nyu.edu</EmailAddress>
<PI_PHON>6464687978</PI_PHON>
<NSF_ID>000513299</NSF_ID>
<StartDate>07/25/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Julian</FirstName>
<LastName>Togelius</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Julian Togelius</PI_FULL_NAME>
<EmailAddress>julian.togelius@nyu.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000686152</NSF_ID>
<StartDate>07/25/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>112013846</ZipCode>
<StreetAddress><![CDATA[2 MetroTech Center]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~427000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project made a number of advancements in AI methods for games, and in video game-based environments for AI developments. Some of these advancements concerned ways of teaching agents to perform tasks in environments, others concerned ways of generating new environments in which agents can learn to perform tasks. While our research has been done in the domain of video games, many of the algorithms developed can easily be applied to other domains, such as robotics and interior design.</p> <p><br />Much of the work involved designing new algorithms for generating environments, or in the context games, maps and levels. We started from existing methods based on artificial evolution (algorithms inspired by Darwinian evolution) and developed new methods for environment generation based on quality-diversity algorithms. These algorithm don't just search for a single optimal solution, but a set of solutions that maximize a criterion while covering a space defined by one or several criteria. In other words, this type of algorithms find many different good solutions instead of just one. We showed that we can use quality-diversity ideas to create algorithms that generate various kinds of puzzles and levels for platformers with good results.</p> <p><br />We also explored the use of reinforcement learning algorithms for generating levels and environments. Reinforcement learning algorithms are typically used for learning to control robots or vehicles, or playing games. Here, we sort of turned the tables on the problem formulation: instead of training agents to navigate/solve environments, we trained agents to generate them. Our work here was some of the first to take this perspective, and we believe that this is a very fertile area for further study.</p> <p><br />Another result of our project is the finding thatreinforcement learning for learning to play games, in particular in two-dimensional scenarios, suffer from serious problems with overfitting. Using the versatile game-based environment that was developed partly in the project, we found that agents trained to solve one particular environment generally performed very badly on slightly different environments, although the rules were the same. While this finding was made in the context of video games, it extends to e.g. robot control problems with the same observation parameters. We introduced two ways of alleviating this overfitting problem. One is to constantly re-generate environments in training, so as to vary the training continually. The other is to modify the observation space so that the agent observes the world from a more ego-centric perspective.</p><br> <p>            Last Modified: 06/04/2021<br>      Modified by: Julian&nbsp;Togelius</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project made a number of advancements in AI methods for games, and in video game-based environments for AI developments. Some of these advancements concerned ways of teaching agents to perform tasks in environments, others concerned ways of generating new environments in which agents can learn to perform tasks. While our research has been done in the domain of video games, many of the algorithms developed can easily be applied to other domains, such as robotics and interior design.   Much of the work involved designing new algorithms for generating environments, or in the context games, maps and levels. We started from existing methods based on artificial evolution (algorithms inspired by Darwinian evolution) and developed new methods for environment generation based on quality-diversity algorithms. These algorithm don't just search for a single optimal solution, but a set of solutions that maximize a criterion while covering a space defined by one or several criteria. In other words, this type of algorithms find many different good solutions instead of just one. We showed that we can use quality-diversity ideas to create algorithms that generate various kinds of puzzles and levels for platformers with good results.   We also explored the use of reinforcement learning algorithms for generating levels and environments. Reinforcement learning algorithms are typically used for learning to control robots or vehicles, or playing games. Here, we sort of turned the tables on the problem formulation: instead of training agents to navigate/solve environments, we trained agents to generate them. Our work here was some of the first to take this perspective, and we believe that this is a very fertile area for further study.   Another result of our project is the finding thatreinforcement learning for learning to play games, in particular in two-dimensional scenarios, suffer from serious problems with overfitting. Using the versatile game-based environment that was developed partly in the project, we found that agents trained to solve one particular environment generally performed very badly on slightly different environments, although the rules were the same. While this finding was made in the context of video games, it extends to e.g. robot control problems with the same observation parameters. We introduced two ways of alleviating this overfitting problem. One is to constantly re-generate environments in training, so as to vary the training continually. The other is to modify the observation space so that the agent observes the world from a more ego-centric perspective.       Last Modified: 06/04/2021       Submitted by: Julian Togelius]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
