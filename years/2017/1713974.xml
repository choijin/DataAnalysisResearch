<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAPSI: Using Prosody to Learn Phonetic Categories</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>5400.00</AwardTotalIntnAmount>
<AwardAmount>5400</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Speech is highly variable. Though we often do not notice, every time we say a word or sound, it comes out differently. Our best language technologies (e.g. Siri) often fail precisely because of this variability, which shows just how difficult it is to learn about the sounds of language from variable speech. Nonetheless, infants learn about the sounds of their language within the first year of their life, and they do this with much less data and fewer resources than language technologies have access to. This project tests hypotheses about how infants solve this learning problem so effectively, by using computational models to simulate the learning process. This work will be done in collaboration with Dr. Reiko Mazuka at RIKEN Brain Science Institute, who has the only existing dataset with the necessary annotations for this project. Understanding how infants, the most successful language learners, learn can help improve language technologies so that they work more successfully across a wider range of languages and are more usable by a larger proportion of society, including people with accented speech and children, whose speech continues to pose problems for such systems. &lt;br/&gt;&lt;br/&gt;Various unsupervised sound category learning models will be tested on their ability to learn the Japanese vowel contrast, which no previous models have successfully learned. One of the factors that contributes to the difficulty of this particular learning problem is prosody - or the rhythm and intonation of speech - which systematically lengthens vowels in particular positions and causes overlap between short and long vowels through a process known as phrase-final lengthening. Knowing how prosody affects the duration of vowels and factoring out its influence could be helpful in learning the Japanese sound categories. This project will incorporate prosody into two existing computational models of sound category learning and see whether this improves learning outcomes. &lt;br/&gt;&lt;br/&gt;This award, under the East Asia and Pacific Summer Institutes program, supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science.</AbstractNarration>
<MinAmdLetterDate>05/08/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/08/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1713974</AwardID>
<Investigator>
<FirstName>Katarzyna</FirstName>
<LastName>Hitczenko</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katarzyna A Hitczenko</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>8564174214</PI_PHON>
<NSF_ID>000733691</NSF_ID>
<StartDate>05/08/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Hitczenko               Katarzyna      A</Name>
<CityName>Riverdale</CityName>
<ZipCode>207371008</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Hitczenko               Katarzyna      A]]></Name>
<CityName>Riverdale</CityName>
<StateCode>MD</StateCode>
<ZipCode>207371008</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5921</Code>
<Text>JAPAN</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~5400</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-14f4f33f-26d5-7612-131e-097cf42ef32a"> </span></p> <p dir="ltr"><span>Speech is highly variable. Though we often do not notice, every time we hear a word or sound, its acoustics differ. For example, adults speak very differently from children, speakers from Minnesota speak differently from speakers from North Carolina, and slow speech is acoustically different than fast speech. Nonetheless, adult listeners effortlessly identify the sounds they hear (e.g. as a /p/, /t/, or /o/) - despite huge variability in how those sounds are actually produced across contexts - and infants learn about the sounds of their language within the first year of life. Although these are effortless tasks for listeners, researchers have been unable to locate </span><span>a consistent one-to-one mapping between the acoustics and what vowel/consonant was produced. That is, researchers have not located any properties in the acoustics that definitively indicate the identity of the sound. This problem becomes particularly apparent in language technologies (e.g. Siri), which often fail because of variable speech, in cases where humans succeed.</span></p> <p dir="ltr"><span>The goal of this project was to test hypotheses about how humans overcome variability in the speech stream so effectively when learning and processing speech. Understanding how humans, the most successful language users, make use of and learn from variable acoustics can help improve language technologies so that they work more robustly across a wider range of languages and are more usable by a larger proportion of society, including people with accented speech, as well as children, whose speech continues to pose problems for such systems.</span></p> <p dir="ltr"><span>In collaboration with my host researcher, Dr. Reiko Mazuka, w</span><span>e built computational models that implemented various </span><span>possible strategies that listeners could be using and tested whether they lead to successful sound category learning and processing (i.e. whether they correctly map from the speech acoustics to the categories that infants acquire). In particular, this work contrasted two ideas for how context may supplement ambiguous acoustic information. We applied these two ideas to speech from Dr. Mazuka&rsquo;s RIKEN Japanese Mother Infant Conversation Corpus (R-JMICC), which is the only existing data set with the necessary annotations for the project. We found interesting differences in their efficacy, which gives us a better idea of how listeners might or might not be overcoming variability, and suggests promising future ideas for research.</span></p> <p dir="ltr"><span>A particularly important contribution of this work is that it tested these hypotheses on spontaneously produced speech. With a few exceptions, they have primarily been </span><span>tested on synthesized, read, or carefully enunciated speech produced in laboratories - all of which differ substantially from the spontaneously produced speech that listeners primarily hear and learn from on a daily basis. Our results highlight the importance of making sure that findings from work on simplified speech generalize to spontaneous speech.</span></p> <p dir="ltr"><span>Thanks to the EAPSI program, I received opportunities that have helped me develop as a researcher. I got to spend a summer working in Japan at the RIKEN Brain Sciences Institute, in collaboration with Dr. Mazuka, a leading expert in the field of language acquisition. Although the members of her lab and I work on similar research questions, we often use different methods in our research. Working in her lab exposed me to experimental and corpus work, while I was able to present to them about computational modeling work. Finally, I gained international work experience, developed a professional network spanning various universities and countries, and started strong international collaborations that are still in progress.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 03/14/2018<br>      Modified by: Katarzyna&nbsp;A&nbsp;Hitczenko</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Speech is highly variable. Though we often do not notice, every time we hear a word or sound, its acoustics differ. For example, adults speak very differently from children, speakers from Minnesota speak differently from speakers from North Carolina, and slow speech is acoustically different than fast speech. Nonetheless, adult listeners effortlessly identify the sounds they hear (e.g. as a /p/, /t/, or /o/) - despite huge variability in how those sounds are actually produced across contexts - and infants learn about the sounds of their language within the first year of life. Although these are effortless tasks for listeners, researchers have been unable to locate a consistent one-to-one mapping between the acoustics and what vowel/consonant was produced. That is, researchers have not located any properties in the acoustics that definitively indicate the identity of the sound. This problem becomes particularly apparent in language technologies (e.g. Siri), which often fail because of variable speech, in cases where humans succeed. The goal of this project was to test hypotheses about how humans overcome variability in the speech stream so effectively when learning and processing speech. Understanding how humans, the most successful language users, make use of and learn from variable acoustics can help improve language technologies so that they work more robustly across a wider range of languages and are more usable by a larger proportion of society, including people with accented speech, as well as children, whose speech continues to pose problems for such systems. In collaboration with my host researcher, Dr. Reiko Mazuka, we built computational models that implemented various possible strategies that listeners could be using and tested whether they lead to successful sound category learning and processing (i.e. whether they correctly map from the speech acoustics to the categories that infants acquire). In particular, this work contrasted two ideas for how context may supplement ambiguous acoustic information. We applied these two ideas to speech from Dr. Mazuka?s RIKEN Japanese Mother Infant Conversation Corpus (R-JMICC), which is the only existing data set with the necessary annotations for the project. We found interesting differences in their efficacy, which gives us a better idea of how listeners might or might not be overcoming variability, and suggests promising future ideas for research. A particularly important contribution of this work is that it tested these hypotheses on spontaneously produced speech. With a few exceptions, they have primarily been tested on synthesized, read, or carefully enunciated speech produced in laboratories - all of which differ substantially from the spontaneously produced speech that listeners primarily hear and learn from on a daily basis. Our results highlight the importance of making sure that findings from work on simplified speech generalize to spontaneous speech. Thanks to the EAPSI program, I received opportunities that have helped me develop as a researcher. I got to spend a summer working in Japan at the RIKEN Brain Sciences Institute, in collaboration with Dr. Mazuka, a leading expert in the field of language acquisition. Although the members of her lab and I work on similar research questions, we often use different methods in our research. Working in her lab exposed me to experimental and corpus work, while I was able to present to them about computational modeling work. Finally, I gained international work experience, developed a professional network spanning various universities and countries, and started strong international collaborations that are still in progress.          Last Modified: 03/14/2018       Submitted by: Katarzyna A Hitczenko]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
