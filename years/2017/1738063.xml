<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Quantifying and Reducing Data Bias in Object Detection Using Physics-based Image Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>55074.00</AwardTotalIntnAmount>
<AwardAmount>55074</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops improved computer vision methods for automatic recognition of arbitrary objects in images from realistic environments. Object recognition is typically performed by fitting a function that maps an image to likely object locations and labels. Such a function is fitted (trained) on a database of example images along with their human-assigned object locations and labels. This research can result in more accurate visual perception for socially relevant applications, such as robots performing household tasks, assisting the elderly, responding to disasters and quickly learning new manufacturing and service skills. It can also provide a common codebase for the wider community, new dataset challenges for domain adaptation problems, the dissemination of scientific and technical results and associated courseware, and specific outreach to ensure broad participation of underrepresented groups.&lt;br/&gt;&lt;br/&gt;The specific research agenda is structured around two aims. The first aim is to establish bounds on the coverage of latent physical factors in datasets needed for human-level performance on arbitrary domains. The study involves both existing datasets and new datasets generated using graphics rendering techniques at various degrees of photorealism. The goal is to develop a theory of the physical complexity of a given dataset and how it affects generalization to real world object recognition tasks, with respect to a given image representation and learning framework. Physical parameters include but are not limited to: 3D shape, surface color, texture, background/scene, camera viewpoint, sensor noise, lighting, specularities and cast shadows. The second research aim is to learn image representations invariant to some of the physical causes of data bias. The goal is to develop model and representation learning methods that are able to learn from a combination of real and non-photorealistic synthetic data, and are resistant to common sources of data bias. The representations include simple edge-based descriptors, and more generally hierarchical representations based on layers of convolution and pooling operations.</AbstractNarration>
<MinAmdLetterDate>03/08/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/08/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1738063</AwardID>
<Investigator>
<FirstName>Kate</FirstName>
<LastName>Saenko</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kate Saenko</PI_FULL_NAME>
<EmailAddress>saenko@bu.edu</EmailAddress>
<PI_PHON>9789343699</PI_PHON>
<NSF_ID>000601038</NSF_ID>
<StartDate>03/08/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName>BOSTON</CityName>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress><![CDATA[881 COMMONWEALTH AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~55074</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-dd924a3d-7fff-c489-6900-d7ecd3148bd3"> </span></p> <p dir="ltr"><span>This project has resulted in the development of new algorithms and datasets for automatic recognition of objects in images. State-of-the-art object recognition is based on supervised learning and achieves accurate results, but requires a lot of annotated data from the target domain. Our project has produced alternative data collection and learning methods that require a lot less supervision.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The first contribution of the project is a method for testing the generalization properties of object detectors transferred to novel visual domains where labelled data is scarce. We asked: how much variation in the training data is enough for good generalization? The method trains detectors on synthetic images, which are simple to generate and label, and then applies the resulting detectors to real images taken &ldquo;in the wild&rdquo;. Several datasets rendered from 3D CAD models of objects collected from open source online resources were released to the public along with standardized evaluation protocols. The project resulted in optimal methods for creating synthetic object methods with sufficient intra-class variations by simulating rendering conditions such as object pose, size and texture. These datasets and techniques have since been used by other researchers to develop better and more generalizable object detectors.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Another important outcome of the project is a set of algorithms and software for learning image representations that are invariant to domain shifts, such as shifts from synthetic to real data, shifts from one geographic location to another, to a different modality, etc. The main idea behind the algorithms is to align the distributions of features such that the distribution in the (labelled) source domain matches the distribution in the (unlabelled) target domain. This work on feature alignment for domain adaptation has led to state-of-the-art performance in domain shift settings, in particular when applied to deep learning models. The work has had a significant impact on the field and has been widely used by other researchers in academia and industry. A significant portion of the current research in domain adaptation for object recognition builds on feature-alignment approaches developed in this project.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The grant provided funding and travel support for two PhD students that worked on several aspects of the research, which represented a significant part of their thesis work. The research has been presented at academic workshops and conferences and resulted in several well-cited publications. The datasets and software packages are freely available from the websites associated with each publication.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/05/2018<br>      Modified by: Kate&nbsp;Saenko</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   This project has resulted in the development of new algorithms and datasets for automatic recognition of objects in images. State-of-the-art object recognition is based on supervised learning and achieves accurate results, but requires a lot of annotated data from the target domain. Our project has produced alternative data collection and learning methods that require a lot less supervision.    The first contribution of the project is a method for testing the generalization properties of object detectors transferred to novel visual domains where labelled data is scarce. We asked: how much variation in the training data is enough for good generalization? The method trains detectors on synthetic images, which are simple to generate and label, and then applies the resulting detectors to real images taken "in the wild". Several datasets rendered from 3D CAD models of objects collected from open source online resources were released to the public along with standardized evaluation protocols. The project resulted in optimal methods for creating synthetic object methods with sufficient intra-class variations by simulating rendering conditions such as object pose, size and texture. These datasets and techniques have since been used by other researchers to develop better and more generalizable object detectors.    Another important outcome of the project is a set of algorithms and software for learning image representations that are invariant to domain shifts, such as shifts from synthetic to real data, shifts from one geographic location to another, to a different modality, etc. The main idea behind the algorithms is to align the distributions of features such that the distribution in the (labelled) source domain matches the distribution in the (unlabelled) target domain. This work on feature alignment for domain adaptation has led to state-of-the-art performance in domain shift settings, in particular when applied to deep learning models. The work has had a significant impact on the field and has been widely used by other researchers in academia and industry. A significant portion of the current research in domain adaptation for object recognition builds on feature-alignment approaches developed in this project.    The grant provided funding and travel support for two PhD students that worked on several aspects of the research, which represented a significant part of their thesis work. The research has been presented at academic workshops and conferences and resulted in several well-cited publications. The datasets and software packages are freely available from the websites associated with each publication.               Last Modified: 11/05/2018       Submitted by: Kate Saenko]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
