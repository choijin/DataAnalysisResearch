<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative Research: Malleable Media to Support Interaction through Bi-Directional Touch Displays</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>226040.00</AwardTotalIntnAmount>
<AwardAmount>226040</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Interaction with touchscreens is compelling because, in part, it replicates interaction with objects in our environment; you can use a touchscreen to visually apprehend, reach toward, touch and manipulate an object.  Of course, this is only partly true because, at the last moment, your hand contacts glass and the rest of the manipulation is simulated.  For many applications, the glass barrier is an acceptable compromise because the simulated response of the on-screen application to your action (delivered visually) is sufficient to close a perceptual gap, to fool you into believing that you are actually touching a button or icon; the visual feedback serves as proxy for the missing tactile interaction with the object.  However, in some situations, and for some users, the presence of the glass barrier causes the interaction to fail completely.  For blind users in particular, for whom closing a perceptual loop through vision is not an option, other means of interaction with computationally mediated environments must be developed.  The primary objective of this exploratory project is to develop a full-page interactive tactile display that can render what the PIs term "malleable media" by combining touch sensing with their existing microfluidic actuators to create a full-page responsive surface.  Such a display could provide an immediate tangible response in reaction to how it is being touched.  By transcending the glass barrier of the screen, such a display would quite literally support a direct manipulation paradigm by making both interface elements, such as buttons and icons, and application content touchable.  Thus, project outcomes are likely to provide greater accessibility to digital media by blind users.  But the sighted community also stands to benefit.  Both students and professionals working in STEM fields require access to increasingly high dimensional data that cannot be easily accessed via speech. An interactive tactile display such as that which is the focus of this work would provide such access, to blind computer users and their sighted counterparts alike.&lt;br/&gt;&lt;br/&gt;The intellectual merit of this exploratory project derives from combining two strands of existing research by the PIs.  The first is their recent work on a microfluidic tactile display, while the second is their established record in applying principles of enactive cognition to the design of human-computer interfaces.  Enactive cognition posits a tight coupling between our actions on the environment and our perception of how the environment is responding.  With the new technology the PIs have the means of creating such an interactive surface, but it is not yet known what such interactions should "feel" like.  What would it be like to sculpt an object by molding the surface of a tablet computer with one's hands?  For blind users, this would provide a means for not just feeling tactile objects, but also for creating those objects.  Such objects might be used as new ways of representing, for example, patterns present in complex data sets or of interacting with mathematical models.</AbstractNarration>
<MinAmdLetterDate>05/31/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/09/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1741435</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Gillespie</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard B Gillespie</PI_FULL_NAME>
<EmailAddress>brentg@engin.umich.edu</EmailAddress>
<PI_PHON>7346476907</PI_PHON>
<NSF_ID>000463310</NSF_ID>
<StartDate>05/31/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sile</FirstName>
<LastName>O'Modhrain</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sile O'Modhrain</PI_FULL_NAME>
<EmailAddress>sileo@umich.edu</EmailAddress>
<PI_PHON>7347641817</PI_PHON>
<NSF_ID>000608825</NSF_ID>
<StartDate>05/31/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName/>
<StateCode>MI</StateCode>
<ZipCode>481091274</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~226040</FUND_OBLG>
</Award>
</rootTag>
