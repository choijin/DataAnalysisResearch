<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Medium: Collaborative Research: Scalable Learning of Nonlinear Models in Large Neural Populations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Fundamental to understanding information processing in the brain are methods that can systematically characterize the structure and dynamics of neural circuits that underlie perception and cognition. Micro- electrocorticography (µECoG) is the practice of using microelectrodes placed directly on the exposed surface of the brain to record electrical activity from the cerebral cortex. Recent advances in µECoG provide unique opportunities to observe large regions of the neural cortex at unprecedented spatial and temporal resolution. However, uncovering the structure of complex neural circuits is challenging. This interdisciplinary project develops methods for learning high-dimensional nonlinear systems with a particular focus on these systems as they arise in cortical networks and validates these techniques on state-of-the-art µECoG systems.&lt;br/&gt;&lt;br/&gt;Three thrusts are considered: The first considers the general problem of state estimation in high-dimensional dynamical systems using decomposition methods including distributed Kalman and particle filtering and graphical models. The main goal is to provide computationally scalable and flexible approaches with provable guarantees. The second combines these state estimation methods with Bayesian parameter estimation and compressed sensing techniques to identify connectivity and nonlinear dynamics in the networks. The third validates these methods on identification of neural models from µECoG arrays. Applications to neural mapping, auditory and visual stimuli decoding are explored. In particular, the project seeks to demonstrate the method on using recordings from rat primary auditory cortex and cat visual cortex using a novel, flexible, high-resolution electrode array.</AbstractNarration>
<MinAmdLetterDate>05/01/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/16/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1738286</AwardID>
<Investigator>
<FirstName>Alyson</FirstName>
<LastName>Fletcher</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alyson K Fletcher</PI_FULL_NAME>
<EmailAddress>akfletcher@ucla.edu</EmailAddress>
<PI_PHON>2019651900</PI_PHON>
<NSF_ID>000624146</NSF_ID>
<StartDate>05/01/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>los angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900952000</ZipCode>
<StreetAddress><![CDATA[11000 Kinross ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~192790</FUND_OBLG>
<FUND_OBLG>2018~207210</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A key challenge in models in modern machine learning problems is their high dimensionality arising from the large as the number of parameters in the model as well the large size of the datasets needed to train the models. This project attempted to develop fundamental theory and algorithms for learning high-dimensional non-linear dynamical systems, meaning systems that have input and outputs that vary over time. &nbsp;High dimensionality arises often in dynamical systems modeling, particularly in cases where there are complex relations between the system inputs and outputs.&nbsp; As one example, the focus application of this project was on modeling sensory responses in neurological systems where the input is some time-varying stimuli (e.g. sound and visual data) and the output are recordings from a population of neurons or brain regions.&nbsp; While recent years have seen tremendous practical successes in modeling complex time-series data and dynamical systems, there is a lack of theory to understand how these methods work and how they can be improved.&nbsp;</p> <p>The project made a number of key breakthroughs in both the algorithms and theory of learning of these high-dimensional systems.&nbsp; One particular focus area was on large, random systems.&nbsp; The project extended a powerful class of algorithms called approximate message passing (AMP).&nbsp; AMP enabled exact predictions of algorithm performance, but were numerically fragile, and apply only to limited simple classes of models.&nbsp; The work extended the models to include larger classes of linear transforms, multi-layer networks, bilinear systems, systems with parametric uncertainty and modular systems built from other estimators.&nbsp; These works enabled initial insights into models where few prior theoretical tools such as neural networks and systems with non-convex losses.&nbsp; The work also provided insight into key problems such as when do models generalize and when can these models be learned.</p> <p>The project also developed new results on recurrent neural networks (RNNs), which are the principal models for dynamical systems.&nbsp; One line of work developed key results relating the expressibility of the network the problem of the exploding / vanishing gradient. &nbsp;A second line of work developed characterizations of when certain nonlinear feedback models used in neural processing could be learned.</p> <p>Some of these techniques were successfully illustrated in neural modeling.&nbsp; Specifically, we develop a succinct model for modeling sensory responses in the rat auditory cortex.&nbsp; This model was trained using new data collected as part of the collaboration in the project with the Viventi lab at Duke</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/28/2020<br>      Modified by: Alyson&nbsp;K&nbsp;Fletcher</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A key challenge in models in modern machine learning problems is their high dimensionality arising from the large as the number of parameters in the model as well the large size of the datasets needed to train the models. This project attempted to develop fundamental theory and algorithms for learning high-dimensional non-linear dynamical systems, meaning systems that have input and outputs that vary over time.  High dimensionality arises often in dynamical systems modeling, particularly in cases where there are complex relations between the system inputs and outputs.  As one example, the focus application of this project was on modeling sensory responses in neurological systems where the input is some time-varying stimuli (e.g. sound and visual data) and the output are recordings from a population of neurons or brain regions.  While recent years have seen tremendous practical successes in modeling complex time-series data and dynamical systems, there is a lack of theory to understand how these methods work and how they can be improved.   The project made a number of key breakthroughs in both the algorithms and theory of learning of these high-dimensional systems.  One particular focus area was on large, random systems.  The project extended a powerful class of algorithms called approximate message passing (AMP).  AMP enabled exact predictions of algorithm performance, but were numerically fragile, and apply only to limited simple classes of models.  The work extended the models to include larger classes of linear transforms, multi-layer networks, bilinear systems, systems with parametric uncertainty and modular systems built from other estimators.  These works enabled initial insights into models where few prior theoretical tools such as neural networks and systems with non-convex losses.  The work also provided insight into key problems such as when do models generalize and when can these models be learned.  The project also developed new results on recurrent neural networks (RNNs), which are the principal models for dynamical systems.  One line of work developed key results relating the expressibility of the network the problem of the exploding / vanishing gradient.  A second line of work developed characterizations of when certain nonlinear feedback models used in neural processing could be learned.  Some of these techniques were successfully illustrated in neural modeling.  Specifically, we develop a succinct model for modeling sensory responses in the rat auditory cortex.  This model was trained using new data collected as part of the collaboration in the project with the Viventi lab at Duke          Last Modified: 12/28/2020       Submitted by: Alyson K Fletcher]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
