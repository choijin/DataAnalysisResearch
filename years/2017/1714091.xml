<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Making Strassen's Algorithm Practical</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>465884.00</AwardTotalIntnAmount>
<AwardAmount>465884</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>High-performance linear algebra software libraries are at the core of scientific computing and machine learning applications.  At the core of many high-performance linear algebra libraries lies the matrix multiplication operation because many other matrix operations can be cast in terms of matrix multiplication and matrix multiplication itself can attain high performance.  Strassen?s algorithm, first proposed in 1969, is a clever scheme for reducing the number of arithmetic calculations that must be performed when computing a matrix multiplication.  It has mostly been a theoretical curiosity that has led to a sequence of improvements over the years. Some practical applications of Strassen?s algorithm for very large problem sizes have been encountered in, for example, the aerospace industry.  Very recently, it was shown that Strassen?s algorithm, and some of its variations, can be made practical for small problem sizes, opening up a range of new academic and practical directions of research.  The project will pursue these directions and will incorporate the advances in high-performance software libraries.  In essence, it will give the user a performance boost of up to around 30%, for free. &lt;br/&gt;&lt;br/&gt;The proposed work will create a practical framework and analysis for the implementation of a broad family of Strassen-like algorithms, building on a model of computation that captures the interaction between software and hardware. This will yield the most thorough understanding to date of the practical implementation of such algorithms.  The proposed project will also deliver a software library for practical use in computational science and machine learning applications that cast computation in terms of matrix-matrix multiplication and/or tensor contractions, with a mechanism for choosing the best algorithm from that family. It builds on recent advances regarding the high-performance implementation of linear algebra software libraries.  What was shown was that such libraries can be composed from small kernels that can be highly optimized for a specific architecture.  These kernels have become the building blocks for traditional algorithms for matrix operations.  In this research, they also become the building blocks for high-performance algorithms that incorporate Strassen?s algorithm and closely related so-called fast matrix multiplication algorithms.  The resulting software will be released under open source license to facilitate its use and study. Pedagogical outreach will include the development of a Massive Open Online Course on "Programming for Performance" in which Strassen-like algorithms and their practical implementation will be a prominent enrichment. The project involves several members from traditionally underrepresented groups and will continue a long tradition of involvement by undergraduates.</AbstractNarration>
<MinAmdLetterDate>07/19/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/19/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1714091</AwardID>
<Investigator>
<FirstName>Field</FirstName>
<LastName>Van Zee</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Field G Van Zee</PI_FULL_NAME>
<EmailAddress>field@cs.utexas.edu</EmailAddress>
<PI_PHON>5124152863</PI_PHON>
<NSF_ID>000637140</NSF_ID>
<StartDate>07/19/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Margaret</FirstName>
<LastName>Myers</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Margaret E Myers</PI_FULL_NAME>
<EmailAddress>myers@cs.utexas.edu</EmailAddress>
<PI_PHON>5124619533</PI_PHON>
<NSF_ID>000532126</NSF_ID>
<StartDate>07/19/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>van de Geijn</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert A van de Geijn</PI_FULL_NAME>
<EmailAddress>rvdg@cs.utexas.edu</EmailAddress>
<PI_PHON>5124719720</PI_PHON>
<NSF_ID>000336892</NSF_ID>
<StartDate>07/19/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName/>
<StateCode>TX</StateCode>
<ZipCode>787121532</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~465884</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Matrix multiplication is a key operation employed in computational science and machine learning. &nbsp;For this reason, one often wants this computation to finish in the least amount of time possible. &nbsp;Significant progress has been made towards understanding how to achieve performance when computing a matrix product on the fastest processor. &nbsp;In addition, since around 1970, it has been understood how to reduce the number of operations that are needed for the computation via techniques that build on Strassen's algorithm, a result that has long fascinated the computer science community. &nbsp;This project has shown how to achieve practical high performance for Strassen's algorithm and its variants,&nbsp;yielding what is now the state of the art on the subject.</p> <div><span><br /></span></div> <div>Over many decades, implementations of Strassen's algorithm reinforced 'street wisdom' that &nbsp;<span>it is only practical for large, relatively square matrices, that it requires considerable workspace, and that it is difficult to achieve thread-level parallelism. The&nbsp;intellectual&nbsp;merit of this project dispels these notions, demonstrating&nbsp;techniques that reduce time to completion for small and non-square matrices, require no workspace beyond what is already incorporated in high-performance implementations of matrix-matrix multiplication, and achieve performance benefits on processors with many cores and even GPUs. The research resulted in a Ph.D. dissertation and four highly-cited publications in top venues (and additional minor papers). &nbsp;The broader impact comes from the implementations that have been released under an open-source license and a Massive Open Online Course (MOOC) titled 'LAFF-On Programming for High Performance,' offered on the edX platform, that shares the underlying techniques with a broad audience. &nbsp;</span></div> <p>&nbsp;</p><br> <p>            Last Modified: 06/29/2021<br>      Modified by: Robert&nbsp;A&nbsp;Van De Geijn</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Matrix multiplication is a key operation employed in computational science and machine learning.  For this reason, one often wants this computation to finish in the least amount of time possible.  Significant progress has been made towards understanding how to achieve performance when computing a matrix product on the fastest processor.  In addition, since around 1970, it has been understood how to reduce the number of operations that are needed for the computation via techniques that build on Strassen's algorithm, a result that has long fascinated the computer science community.  This project has shown how to achieve practical high performance for Strassen's algorithm and its variants, yielding what is now the state of the art on the subject.   Over many decades, implementations of Strassen's algorithm reinforced 'street wisdom' that  it is only practical for large, relatively square matrices, that it requires considerable workspace, and that it is difficult to achieve thread-level parallelism. The intellectual merit of this project dispels these notions, demonstrating techniques that reduce time to completion for small and non-square matrices, require no workspace beyond what is already incorporated in high-performance implementations of matrix-matrix multiplication, and achieve performance benefits on processors with many cores and even GPUs. The research resulted in a Ph.D. dissertation and four highly-cited publications in top venues (and additional minor papers).  The broader impact comes from the implementations that have been released under an open-source license and a Massive Open Online Course (MOOC) titled 'LAFF-On Programming for High Performance,' offered on the edX platform, that shares the underlying techniques with a broad audience.            Last Modified: 06/29/2021       Submitted by: Robert A Van De Geijn]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
