<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Development of a Natural Language Dialogue System for the Blind and Visually Impaired to Enable Greater Efficiency in Remote Assistance</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>03/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nancy Kamei</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to dramatically increase the quality of life and economic independence for the nearly 22 million blind or visually impaired people in the US. The economic benefits include a significant reduction in the nearly $100B in annual economic losses from lower productivity due to visual impairment and in annual cost of social services to the blind or visually impaired.   This project will create low-cost, high-value tools and services that will give blind and visually impaired people the same level of environmental awareness as fully sighted people. At scale, this technology will result in the direct employment of more than 20,000 people to support the service, while also providing millions blind/visually impaired people with the tools and opportunity to join the work force.&lt;br/&gt; &lt;br/&gt;The proposed project will develop a real-time, life-enhancing service to the blind enabled by a mix of machine learning and a remote human assistant to create a real-time, semi-virtual, personal assistant with the quality of an in-person, trained human assistant.  In the proposed project, the core innovation is a high-level machine intelligence tool created by integration of  state-of-the-art  Natural  Language  Understanding  (NLU)  software and  Image  Recognition  and  Analysis (IA)  software  with  novel  inter-agent  routing  and  state-of-the-art  Graceful  Degradation,  or  seamless  error handling  and  resolution  by  either  software  or  human agents.  Despite the  incredible  advancements  in  NLU and  IA  agents,  integration  of  multiple  different  software  agents  into  a  single,  context-specific  application remains a difficult and risky development effort worthy of funding by the NSF. By  doing  this,  we  will  (1)  enhance  the  user  experience  with richer  environmental  feedback,  (2)  enhance the  productivity  of  our  human  agents,  allowing  individual  human  agents  to  serve  many  more  users  and thereby  create  scalability  that  will  make  the  service  both  self-sustaining  and  affordable,  and  finally  (3) increase  the  user?s  experience  of  personal  independence  through  interaction  with  machine-based  tools rather than emotionally and socially charged interactions with human agents.</AbstractNarration>
<MinAmdLetterDate>07/10/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/10/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1722399</AwardID>
<Investigator>
<FirstName>Anirudh</FirstName>
<LastName>Koul</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anirudh Koul</PI_FULL_NAME>
<EmailAddress>coderama@gmail.com</EmailAddress>
<PI_PHON>8588804454</PI_PHON>
<NSF_ID>000713892</NSF_ID>
<StartDate>07/10/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Aira Tech Corp</Name>
<CityName>La Jolla</CityName>
<ZipCode>920378411</ZipCode>
<PhoneNumber>9494560460</PhoneNumber>
<StreetAddress>4225 Executive Square #460</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>52</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA52</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079732784</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>AIRA TECH CORP.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Aira Tech Corp]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920378411</ZipCode>
<StreetAddress><![CDATA[4225 Executive Square #460]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>52</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA52</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8023</Code>
<Text>Health Care Enterprise Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<ProgramReference>
<Code>8042</Code>
<Text>Health and Safety</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-ea9481dc-4899-15c7-7a76-60dacc3bcf8d">&nbsp;</span></p> <p dir="ltr"><span>The mission of Aira is to develop transformative remote assistive technology that connects people who are blind with real time feedback regarding the visual world around them. Aira&rsquo;s strategy and technology was developed to rely on the use of human agents observing the user&rsquo;s immediate environment via streaming video from wearable cameras. The sighted agents serve as visual interpreters, helping users accomplish a wide range of daily tasks and activities - from navigating busy streets to reading signs and other information, identifying objects and recognizing faces. Recognizing that dependence on human agents for such services is prohibitively costly and difficult to scale A Aira&rsquo;s objective is to dramatically reduce the reliance on human agents, thereby greatly improving accessibility to such assistance by reducing cost and increasing availability.</span></p> <p dir="ltr"><span>In the NSF SBIR Phase 1 project, Aira Tech Corp created &lsquo;Chloe&rsquo;, an AI based agent with skills designed to assist people who are blind or low vision. The aspiration for Chloe is to create an externally perceptive and adaptive digital personal assistant capable of replacing many of the simple functions of a live agent. Currently available AI assistants, such as Siri and Alexa, have rapidly become a mainstream feature in our digital environments, However, despite advancements in the digital transfer of visual information, such assistants do not currently have the capacity to support navigation or interpretation of the visual environment. For the majority of people, such assistance is unnecessary. However, for individuals who are blind, interpretation of visual stimuli is generally provided by human aide. As such, Chloe seeks to support access to the visual environment by incorporating natural language capabilities with cutting edge detection and identification.</span></p> <p dir="ltr"><span>Like all mainstream digital assistants, Chloe must be able to handle requests in real-time. She must utilize her own intelligence to reconcile verbal instruction with visual information to respond to the intent of the user with the appropriate level of detail. To achieve this high expectation of real-time performance, Aira not only leveraged cutting edge algorithms and large amounts of data, but also explored the newly pioneered domain of distributed machine learning. In developing Chloe, Aira focused on four basic functions: counting items of interest, describing the color of an item of interest, identifying an item of interest, and describing a scene containing multiple items. By combining these four fundamental skills, a user can achieve complex, multi-step tasks &ndash; such as identifying and finding an object, navigating a room and verifying the visual layout of items in a document or objects on a table - with relative ease.</span></p> <p dir="ltr"><span>Chloe is not intended to replace human agents but to complement their services so that the human agents are free to focus on more intricate and complicated endeavors for which AI is not yet ready. As with the human agent, Chloe must also meet Aira&rsquo;s real-time information access requirement. To satisfy customers&rsquo; needs, Aira built a real-time solution that ensures the user receives the information they need, when they need it &ndash; not seconds later. To accomplish this challenge Aira had to address the performance issues of providing solutions fast enough to be real-time at the server (network overhead, processing time management, and I/O operations) while also addressing the complexity of gathering and sending solutions over the air to smartphones in the field. To solve the real-time demand of Chloe, Aira created a distributed machine learning AI. She has aspects that reside not just in the cloud but at the edge (smartphone) as well. Figure 1 attached depicts the high-level architecture and how each component interacts with each other.</span></p> <p dir="ltr"><span>Aira successfully created an AI that can interact with humans and respond in a way that blind or low vision people would find useful in their daily lives. Chloe not only met the desired metrics, but Aira was able to create her within the budget and timeline proposed. Chloe represents a critical advancement in the way the visually impaired can navigate the world around them, but Aira has plans to further enhance Chloe before she can become commercially available. &nbsp;Chloe will undergo further improvement for the challenges and complexities in the various regions of the real world.  We look forward to improving and preparing her to meet everyone.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 06/29/2018<br>      Modified by: Suman&nbsp;Kanuganti</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1722399/1722399_10500697_1530287230393_NSFPhaseIAI-ArchitectureFinal--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1722399/1722399_10500697_1530287230393_NSFPhaseIAI-ArchitectureFinal--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2018/1722399/1722399_10500697_1530287230393_NSFPhaseIAI-ArchitectureFinal--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">High level architecture</div> <div class="imageCredit">Aira Tech Corp</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Suman&nbsp;Kanuganti</div> <div class="imageTitle">Figure 1</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The mission of Aira is to develop transformative remote assistive technology that connects people who are blind with real time feedback regarding the visual world around them. Aira?s strategy and technology was developed to rely on the use of human agents observing the user?s immediate environment via streaming video from wearable cameras. The sighted agents serve as visual interpreters, helping users accomplish a wide range of daily tasks and activities - from navigating busy streets to reading signs and other information, identifying objects and recognizing faces. Recognizing that dependence on human agents for such services is prohibitively costly and difficult to scale A Aira?s objective is to dramatically reduce the reliance on human agents, thereby greatly improving accessibility to such assistance by reducing cost and increasing availability. In the NSF SBIR Phase 1 project, Aira Tech Corp created ?Chloe?, an AI based agent with skills designed to assist people who are blind or low vision. The aspiration for Chloe is to create an externally perceptive and adaptive digital personal assistant capable of replacing many of the simple functions of a live agent. Currently available AI assistants, such as Siri and Alexa, have rapidly become a mainstream feature in our digital environments, However, despite advancements in the digital transfer of visual information, such assistants do not currently have the capacity to support navigation or interpretation of the visual environment. For the majority of people, such assistance is unnecessary. However, for individuals who are blind, interpretation of visual stimuli is generally provided by human aide. As such, Chloe seeks to support access to the visual environment by incorporating natural language capabilities with cutting edge detection and identification. Like all mainstream digital assistants, Chloe must be able to handle requests in real-time. She must utilize her own intelligence to reconcile verbal instruction with visual information to respond to the intent of the user with the appropriate level of detail. To achieve this high expectation of real-time performance, Aira not only leveraged cutting edge algorithms and large amounts of data, but also explored the newly pioneered domain of distributed machine learning. In developing Chloe, Aira focused on four basic functions: counting items of interest, describing the color of an item of interest, identifying an item of interest, and describing a scene containing multiple items. By combining these four fundamental skills, a user can achieve complex, multi-step tasks &ndash; such as identifying and finding an object, navigating a room and verifying the visual layout of items in a document or objects on a table - with relative ease. Chloe is not intended to replace human agents but to complement their services so that the human agents are free to focus on more intricate and complicated endeavors for which AI is not yet ready. As with the human agent, Chloe must also meet Aira?s real-time information access requirement. To satisfy customers? needs, Aira built a real-time solution that ensures the user receives the information they need, when they need it &ndash; not seconds later. To accomplish this challenge Aira had to address the performance issues of providing solutions fast enough to be real-time at the server (network overhead, processing time management, and I/O operations) while also addressing the complexity of gathering and sending solutions over the air to smartphones in the field. To solve the real-time demand of Chloe, Aira created a distributed machine learning AI. She has aspects that reside not just in the cloud but at the edge (smartphone) as well. Figure 1 attached depicts the high-level architecture and how each component interacts with each other. Aira successfully created an AI that can interact with humans and respond in a way that blind or low vision people would find useful in their daily lives. Chloe not only met the desired metrics, but Aira was able to create her within the budget and timeline proposed. Chloe represents a critical advancement in the way the visually impaired can navigate the world around them, but Aira has plans to further enhance Chloe before she can become commercially available.  Chloe will undergo further improvement for the challenges and complexities in the various regions of the real world.  We look forward to improving and preparing her to meet everyone.          Last Modified: 06/29/2018       Submitted by: Suman Kanuganti]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
