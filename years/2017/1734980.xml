<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NCS-FO: Collaborative Research: A Neurally-Inspired, Event-Based Computer Vision Pipeline</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2017</AwardEffectiveDate>
<AwardExpirationDate>12/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>224431.00</AwardTotalIntnAmount>
<AwardAmount>224431</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this research is to make machines more intelligent by more closely mimicking biology, specifically, harnessing the information present in event-driven input along with top-down and lateral feedback.  Although computers continue to make inroads into everyday life, they still cannot perform many tasks that are readily performed by humans and other animals, or lag far behind their biological counterparts.  For example, on publically available datasets designed to measure performance on object detection tasks, the best computer software running on the fastest available hardware fails to locate between 20-40% of the human-annotated targets.  No animal would long survive, at least not in the wild, if its visual system made so many errors.  This project therefore will investigate how information processing strategies used by biological neural systems can be exploited to design more powerful computer hardware and software.   The project will have impact on training in neurally-inspired approaches to computer design, and technological leadership in neuromorphic computing for both defense and commercial uses.&lt;br/&gt;&lt;br/&gt;The project will investigate processing mechanisms that are ubiquitous in biology but typically are not found in computer software and hardware.  The first of these mechanisms is event-driven input.  The retina sends visual information to the brain in the form of discrete pulses that propagate down the optic nerve.  Likewise, the cochlea encodes sound as action potentials or spikes that propagate along the auditory nerve.  In both cases, the precise time at which a spike occurs in a given nerve fiber, relative to the time at which spikes occur in other nerve fibers, can encode information that is critical to subsequent processing by the brain.  In the case of the retina, relative spike timing may help to separate foreground from background regions or even help us to read more quickly the words on this page.  In the cochlea, relative timing can be used to distinguish one sound source from another, which in turn allows us to hear what our companion is saying even while in a noisy room.  In contrast, the types of artificial neural networks most commonly used today do not employ event-driven input.  This research will test the hypothesis that event-driven input can be used to improve the performance of artificial neural networks on image and audio segmentation tasks.  The second biological processing mechanism to be investigated is top-down feedback.  Whereas most artificial neural networks studied today are strictly feed-forward, the vast preponderance of synapses in the brain arise from top-down and lateral feedback connections.  A mathematically tractable model of top-down and lateral feedback will be used to test the hypothesis that such connections can be used to improve the performance of artificial neural networks on object localization tasks.  Whether such feedback can reduce the susceptibility of networks to adversarial training will also be explored.  Finally, the project will explore whether a new type of neuromorphic chip that self-organizes in response to environmental input can learn more powerful representation from event-driven input, and develop strategies for combining such chips into hierarchical networks with lateral and top-down feedback.</AbstractNarration>
<MinAmdLetterDate>08/07/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1734980</AwardID>
<Investigator>
<FirstName>Garrett</FirstName>
<LastName>Kenyon</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Garrett T Kenyon</PI_FULL_NAME>
<EmailAddress>garkenyon@gmail.com</EmailAddress>
<PI_PHON>5054120416</PI_PHON>
<NSF_ID>000460690</NSF_ID>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Mascarenas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Mascarenas</PI_FULL_NAME>
<EmailAddress>dmascarenas@lanl.gov</EmailAddress>
<PI_PHON>5054124200</PI_PHON>
<NSF_ID>000734656</NSF_ID>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New Mexico Consortium</Name>
<CityName>Los Alamos</CityName>
<ZipCode>875442587</ZipCode>
<PhoneNumber>5054124200</PhoneNumber>
<StreetAddress>4200 West Jemez Road, Suite 301</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Mexico</StateName>
<StateCode>NM</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NM03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>801181467</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NMC, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[New Mexico Consortium]]></Name>
<CityName>Los Alamos</CityName>
<StateCode>NM</StateCode>
<ZipCode>875442587</ZipCode>
<StreetAddress><![CDATA[4200 West Jemez Rd., Suite 200]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Mexico</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NM03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<ProgramReference>
<Code>8551</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~224431</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Project Outcomes</p> <p>Our overall goal was to help make small, mobile systems like drones and robots more intelligent by emulating biological brains.&nbsp; In particular, our goal was to enable drones and robots to respond more intelligently to visual input.&nbsp; The eye does not send camera-like photographs to the brain, however.&nbsp; Instead, the eye transmits visual information to the brain as trains of binary impulses, or spikes.&nbsp; Such a transmission strategy allows the eye to only convey information about parts of the scene that are most interesting, for example, movement or sharp outlines, and to ignore parts of the scene that are uniform or unchanging and therefore less interesting.&nbsp; Such biological transmission schemes are very energy efficient but are also very noisy and result in considerable information loss.&nbsp;&nbsp; Our NSF research project showed how the brain might clean up such noisy transmissions and indeed even restore much of the lost information.&nbsp; Specifically, we showed that a widely used model of cortical development is able to learn to clean up and restore noisy transmissions from the eye.&nbsp; This research was published in the proceedings of the International Conference of Neuromorphic Systems (ICONS) in Oakridge, TN, 2018.&nbsp;&nbsp;</p> <p>In order to equip drones and robots with biological levels of intelligence, however, it will be necessary to equip them with next-generation neuromorphic processors that can efficiently implement these models of cortical development.&nbsp; Neuromorphic processors are fascinating devices but they can also be very difficult to train.&nbsp; When we attempted to simulate the implementation of our model of cortical development on a neuromorphic processor, the resulting circuit became unstable as learning proceeded.&nbsp; Remarkably, we found that these models of cortical development, even if designed to execute on a neuromorphic processor, could be stabilized by incorporating periods analogous to slow-wave sleep in which we drive the system with varying levels of white noise.&nbsp; This finding has recently been published and could be an important step in making drones and robots more intelligent.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/09/2020<br>      Modified by: Garrett&nbsp;T&nbsp;Kenyon</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Project Outcomes  Our overall goal was to help make small, mobile systems like drones and robots more intelligent by emulating biological brains.  In particular, our goal was to enable drones and robots to respond more intelligently to visual input.  The eye does not send camera-like photographs to the brain, however.  Instead, the eye transmits visual information to the brain as trains of binary impulses, or spikes.  Such a transmission strategy allows the eye to only convey information about parts of the scene that are most interesting, for example, movement or sharp outlines, and to ignore parts of the scene that are uniform or unchanging and therefore less interesting.  Such biological transmission schemes are very energy efficient but are also very noisy and result in considerable information loss.   Our NSF research project showed how the brain might clean up such noisy transmissions and indeed even restore much of the lost information.  Specifically, we showed that a widely used model of cortical development is able to learn to clean up and restore noisy transmissions from the eye.  This research was published in the proceedings of the International Conference of Neuromorphic Systems (ICONS) in Oakridge, TN, 2018.    In order to equip drones and robots with biological levels of intelligence, however, it will be necessary to equip them with next-generation neuromorphic processors that can efficiently implement these models of cortical development.  Neuromorphic processors are fascinating devices but they can also be very difficult to train.  When we attempted to simulate the implementation of our model of cortical development on a neuromorphic processor, the resulting circuit became unstable as learning proceeded.  Remarkably, we found that these models of cortical development, even if designed to execute on a neuromorphic processor, could be stabilized by incorporating periods analogous to slow-wave sleep in which we drive the system with varying levels of white noise.  This finding has recently been published and could be an important step in making drones and robots more intelligent.          Last Modified: 07/09/2020       Submitted by: Garrett T Kenyon]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
