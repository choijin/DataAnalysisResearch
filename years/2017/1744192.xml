<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps: Dexterous Robotic Prosthetic Control Using Deep Learning Pattern Prediction from Ultrasound Signal</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anita La Salle</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this I-Corps project lies in the development a novel system that would allow people with transradial and partial hand amputations to gain unparalleled precise individuation of prosthetic digit motion including continuous and simultaneous movement for individual digits without requiring long and complicated training process. To allow for such functionality a novel set of deep learning algorithms are designed to model muscle movement patterns from ultrasound images. The network is pre-trained with a large amount of data in an effort to minimize later individual training. In addition to power prosthetics, the proposed technology can provide broad impacts in other markets where easy-to-use and accurate gestural control of robotics and/or digital environment are required. These include tele-robotics, exoskeleton operation, virtual reality, gaming, glove boxes as well as work related Personal Protective Equipment, and Performance Augmentation and Amplification Devices.&lt;br/&gt;&lt;br/&gt;This I-Corps project will develop and utilize a novel ultrasound sensor and novel deep learning algorithms to recognize continuous muscle activity patterns that can predict accurate and dexterous finger motion. Current myoelectric powered prostheses use discrete classifiers that can only predict a limited number of discrete gestures from noisy electromyography (EMG) signal. Feeding deep learning architectures, such as Convolutional Neural Networks, with rich and detailed ultrasound signal promises to allow for the modeling and prediction of detailed continuous and simultaneous muscle movements patterns, which can be mapped to control continuous and simultaneous movements of individual prosthetic fingers. An additional intellectual of this project merit is the pre-training of these deep neural network with a large amount of data, which would allow for short fine tuning training for individual users, allowing for wide and easy adoption of the technology. The proposed project could therefore allow amputees and people with upper body disabilities to perform finger-by-finger movement activities such as fine object manipulation, typing or playing a musical instrument.</AbstractNarration>
<MinAmdLetterDate>07/12/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1744192</AwardID>
<Investigator>
<FirstName>Gil</FirstName>
<LastName>Weinberg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gil Weinberg</PI_FULL_NAME>
<EmailAddress>gil.weinberg@coa.gatech.edu</EmailAddress>
<PI_PHON>4048948939</PI_PHON>
<NSF_ID>000105700</NSF_ID>
<StartDate>07/12/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>During the life of the award we have conducted close to 150 interviews with potential customers, users, payers and partners for the Skywalker ultrasonic sensor.&nbsp;We have started by exploring the&nbsp;utilization&nbsp;of our sensor for the power prosthetic market.&nbsp;We learned that although we have a great product-market-fit (allowing amputees to gain much more dexterous control of their&nbsp;prosthetic&nbsp;fingers), the market is too small for a meaningful business. We then moved to explore the Augmented Reality / Virtual Reality market. Here, our technology could have allowed gamers to interact with the virtual world through dexterous finger movements. While this market is much larger than the prosthetic market, we discovered that we do not really address a significant "pain point" and that users are happy enough with current hand-held controller technology. Towards the end of the process we explored the rehabilitation market, where the pain point was clear (stroke patients would give a lot to regain finger function) our technology couldn't deliver an effective solution (our ultrasound sensor could not sense muscle movement of stroke patients). &nbsp;These findings led us to a NO-GO decision, although the team continues to develop the technology with the hope of reaching a product-market-fit that would address a significant pain point in other markets.</p><br> <p>            Last Modified: 08/02/2018<br>      Modified by: Gil&nbsp;Weinberg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ During the life of the award we have conducted close to 150 interviews with potential customers, users, payers and partners for the Skywalker ultrasonic sensor. We have started by exploring the utilization of our sensor for the power prosthetic market. We learned that although we have a great product-market-fit (allowing amputees to gain much more dexterous control of their prosthetic fingers), the market is too small for a meaningful business. We then moved to explore the Augmented Reality / Virtual Reality market. Here, our technology could have allowed gamers to interact with the virtual world through dexterous finger movements. While this market is much larger than the prosthetic market, we discovered that we do not really address a significant "pain point" and that users are happy enough with current hand-held controller technology. Towards the end of the process we explored the rehabilitation market, where the pain point was clear (stroke patients would give a lot to regain finger function) our technology couldn't deliver an effective solution (our ultrasound sensor could not sense muscle movement of stroke patients).  These findings led us to a NO-GO decision, although the team continues to develop the technology with the hope of reaching a product-market-fit that would address a significant pain point in other markets.       Last Modified: 08/02/2018       Submitted by: Gil Weinberg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
