<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>S&amp;AS: FND: Reflective Learning of Stochastic Physical Models for Robust Manipulation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>682646.00</AwardTotalIntnAmount>
<AwardAmount>682646</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In order for robots to function in everyday life environments, from flexible manufacturing and warehouse domains to households, they need to autonomously grasp and manipulate a wide variety of potentially unknown objects. Currently, autonomous robots are practically unable to work outside highly-controlled environments wherein an accurate model of every object is provided. This limitation is partially due to the lack of robust algorithms for grasping and manipulating objects with unknown geometric or mechanical properties. The proposed project will perform fundamental research into robust robotic manipulation in a way that will enable autonomous robots to interact efficiently with a large variety of everyday physical objects for extended periods of time. The objective is for autonomous robotic manipulators to effectively learn from experience how objects may physically interact with each other and with the robotic arm. The next step is to utilize this experience so as to perform robust manipulation tasks. There are many exemplary robotic tasks that can be benefited from the proposed improvements and which will form the basis of the project's experimentation process. They include the pushing of objects to desired poses, reconfiguration of objects to simplify their picking and the handling of tools. &lt;br/&gt;&lt;br/&gt;The project develops three key components: (1) An algorithm for learning inertial, elastic, and friction properties of an unknown object by observing how the object moves when manipulated by a robot. The project will research novel Bayesian optimization techniques for black-box system identification in order to learn probabilistic models of objects. (2) A physically realistic simulator that can provide a stochastic model of an object's motion given the physical parameters learned by the first component. This will be achieved by utilizing online non-parametric learning methods for speeding up physically realistic simulations under uncertainty. (3), A robust planning algorithm that utilizes the simulator for finding  optimal actions to apply on the object given the learned stochastic model. The objective is to converge to increasingly robust solutions as computation time increases and the robot acquires increased experience with objects in an environment. To strengthen the project's broader impact, the PIs will provide implementations of their solutions to the research community as open-source software packages. This will be coupled with the generation of educational material, which will aim to attract undergraduate students early in their studies to STEM. The PIs will also aim to organize academic meetings that will bring together researchers from foundational domains, robotics experts and industry representatives.</AbstractNarration>
<MinAmdLetterDate>08/17/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/17/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1723869</AwardID>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Bekris</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Bekris</PI_FULL_NAME>
<EmailAddress>kostas.bekris@cs.rutgers.edu</EmailAddress>
<PI_PHON>7324452001</PI_PHON>
<NSF_ID>000520262</NSF_ID>
<StartDate>08/17/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mubbasir</FirstName>
<LastName>Kapadia</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mubbasir Kapadia</PI_FULL_NAME>
<EmailAddress>mubbasir.kapadia@rutgers.edu</EmailAddress>
<PI_PHON>7323106331</PI_PHON>
<NSF_ID>000692969</NSF_ID>
<StartDate>08/17/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Abdeslam</FirstName>
<LastName>Boularias</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abdeslam Boularias</PI_FULL_NAME>
<EmailAddress>abdeslam.boularias@rutgers.edu</EmailAddress>
<PI_PHON>8489320150</PI_PHON>
<NSF_ID>000717794</NSF_ID>
<StartDate>08/17/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548014</ZipCode>
<StreetAddress><![CDATA[617 Bowser Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>039Y</Code>
<Text>S&amp;AS - Smart &amp; Autonomous Syst</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>046Z</Code>
<Text>S&amp;AS - Smart &amp; Autonomous Systems</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~682646</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Robots typically rely on models that capture physical interactions with objects or humans in order to perform manipulation and navigation tasks reliably. Models can describe shapes and mechanical properties of objects or the pose and motion pattern of people. They can be used to simulate different outcomes given actions the robot can perform. The actions with the best forecasted outcomes are then selected for execution. In practice, the forecasted outcomes are often different from the real outcomes due to model inaccuracies. This difference is often referred to as the&nbsp;<em>reality gap</em>. Manually designed models are inherently inaccurate. While this problem is less pronounced in industrial robots that operate in closed, structured and controlled environments, it severely limits robot deployment in open human environments with novel objects or crowds of moving people. For example, an assistant robot in a repair shop needs to manipulate various tools and operate on new objects every day, while interacting with human partners.&nbsp;</p> <div class="gs"> <div id=":2b4" class="ii gt"> <div id=":2b3" class="a3s aXjCH "> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <div dir="ltr"> <p><strong>Intellectual Merit:&nbsp;</strong>This project developed automated and data-driven object and people modeling methods. This allows robots to build geometric and mechanical models of objects on the fly while manipulating them in a safe manner.&nbsp;It also allows to define predictive models for the future motion of people in a crowded environment. Given access to such stochastic physical models, this work introduced novel planning manipulation actions that are robust in terms of maximizing the probability of task completion. To achieve this, anytime solutions for simulation-guided planning under uncertainty were further developed and evaluated.</p> <p><strong>A. Object Modeling:&nbsp;</strong>The outcomes include new algorithms supported by empirical evaluations and theoretical analyses&nbsp;when possible. Three aspects of object modeling were considered: volumetric shapes, 6D poses (i.e., position and orientation of rigid bodies), and&nbsp;mechanical properties.&nbsp;The proposed algorithms were inspired from cognitive science, where intuitive Newtonian principles and probabilistic representations have been shown important for complex scene understanding. Unlike previous work, complete 3D shapes are learned on the fly using inverse physics reasoning. A&nbsp;robot starts by poking objects randomly and observing how they move and interact. Then, a physics&nbsp;engine and a 3D renderer generate synthetic images of the objects. The robot's poking actions&nbsp;along with gravitational and interaction forces are re-played in simulation using hypothesized models,&nbsp;while comparing against the observed real motions. Finally, the confidence in models with the most realistic simulations is increased. We demonstrated the importance of Newtonian principles in complex scene understanding given work in object detection and&nbsp;6D pose estimation, which is important in less structured environments.</p> <p>&nbsp;We used these probabilistic models in belief-space planning to robustly perform manipulation, such as pre-grasp sliding, open-loop peg-in-hole, in-hand manipulation with under-actuated hands,&nbsp;de-cluttering, bin packing, and constrained pick-and-place. These methods outperformed at the time of publication state-of-the-art alternatives given&nbsp;three criteria: data efficiency, time efficiency, and accuracy. Some of these tasks have not been previously performed by autonomous robots on unknown objects. The complexity and difficulty involved sets the resulting robotic systems at the forefront of robot manipulation research.&nbsp;</p> <p>&nbsp;<strong>B. Human Modeling:&nbsp;</strong>We developed novel methods for simulating human behavior using synthetic data to learn models and policies of human-agent interaction as well as representations for human pose estimation. Key contributions include: (a)&nbsp;<em>Cognitively-guided synthetic models of human movement and behavior&nbsp;</em>to mitigate the bias in synthetic, ad-hoc models. Our approach combines the benefits of cognitively-grounded experimentation and synthetic modeling. We conducted a series of Virtual Reality (VR) experiments to monitor human behavior and calibrate our synthetic models. (b)&nbsp;<em>Utility-Guided Sampling of Synthetic Data for Model Learning&nbsp;</em>to generate low-confidence synthetic observations. This allows to supplement sparse, high-confidence real datasets, improve prediction accuracy and generalization. We demonstrated the approach's efficacy in multiple domains including human crowd behavior forecasting. (c)&nbsp;<em>Graph Representations for 3D Human Pose Estimation&nbsp;</em>to leverage the inherent structure of the human body. We proposed graph convolutional networks (GCNs) for 3D human pose estimation from monocular images. Our approach can incorporate structure and semantics of the human anatomy to achieve state-of-the-art performance.&nbsp;</p> <p><strong>Broader Impact:&nbsp;</strong>Given existing contributions, we provided&nbsp;implementations of our solutions to the community as open-source software packages, which were used&nbsp;by several other researchers. We publicly shared benchmarks and datasets for modeling manipulation challenges&nbsp;where physical and geometric properties play a critical role. This work helped promote&nbsp;the use of robots in multiple domains, such as logistics and manufacturing, as demonstrated by industrial partnerships. Most peer-reviewed papers produced by this project appeared at ICRA and IROS, the two flagship robotics conferences. Most of these articles were first-authored by Rutgers graduate students and often included undergraduate researchers as co-authors.&nbsp;The remaining articles were published in leading AI, machine learning and robotics conferences. One of the articles was finalist for a best paper award at ICRA 2019 and was featured in media articles.</p> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div><br> <p>            Last Modified: 10/19/2020<br>      Modified by: Abdeslam&nbsp;Boularias</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603151410966_12--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603151410966_12--rgov-800width.jpg" title="Model identification with Bayesian Optimization"><img src="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603151410966_12--rgov-66x44.jpg" alt="Model identification with Bayesian Optimization"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Mechanical properties of an unknown object are inferred on the fly by pushing the object, and comparing the observed motion to simulated ones using a physics engine. A belief (probability distribution) on the true unknown parameters is returned at the end.</div> <div class="imageCredit">Abdeslam Boularias (Rutgers)</div> <div class="imageSubmitted">Abdeslam&nbsp;Boularias</div> <div class="imageTitle">Model identification with Bayesian Optimization</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603147054714_dif_engine_tensegrity--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603147054714_dif_engine_tensegrity--rgov-800width.jpg" title="Differentiable Physics Engine"><img src="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603147054714_dif_engine_tensegrity--rgov-66x44.jpg" alt="Differentiable Physics Engine"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An explainable differential physics engine based on first principles for modeling cable-driven robots (bottom). It models the interaction of rigid rod elements connected by two springs (top left). A tensegrity robot in simulation (top middle), and in the real world (top right).</div> <div class="imageCredit">Kun Wang (Rutgers University)</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Abdeslam&nbsp;Boularias</div> <div class="imageTitle">Differentiable Physics Engine</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603149665688_Simulated_Geometry--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603149665688_Simulated_Geometry--rgov-800width.jpg" title="Shape inference from partial views using physics simulations"><img src="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603149665688_Simulated_Geometry--rgov-66x44.jpg" alt="Shape inference from partial views using physics simulations"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Top row: unknown objects seen from a single camera view. Middle and bottom rows: front and back views of the hallucinated models with the highest probabilities.</div> <div class="imageCredit">Changkyu Song (Rutgers University)</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Abdeslam&nbsp;Boularias</div> <div class="imageTitle">Shape inference from partial views using physics simulations</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603060719117_se3tracknet--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603060719117_se3tracknet--rgov-800width.jpg" title="SE(3)-TrackNet"><img src="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603060719117_se3tracknet--rgov-66x44.jpg" alt="SE(3)-TrackNet"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Pose of a rigid body in a cluttered scene as predicted by the se(3)-TrackNet approach developed as part of this project. Notice that the method is able to recover from complete occlusion without any re-initialization.</div> <div class="imageCredit">Bowen Wen (Rutgers University)</div> <div class="imageSubmitted">Abdeslam&nbsp;Boularias</div> <div class="imageTitle">SE(3)-TrackNet</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603152939143_Untitled--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603152939143_Untitled--rgov-800width.jpg" title="Active model identification via differentiable physics simulations"><img src="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603152939143_Untitled--rgov-66x44.jpg" alt="Active model identification via differentiable physics simulations"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Pre-grasp push and examples of identified friction-mass distributions</div> <div class="imageCredit">Changkyu Song (Rutgers University)</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Abdeslam&nbsp;Boularias</div> <div class="imageTitle">Active model identification via differentiable physics simulations</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603153743602_Untitled2--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603153743602_Untitled2--rgov-800width.jpg" title="Examples of object manipulation tasks considered in this project."><img src="/por/images/Reports/POR/2020/1723869/1723869_10515831_1603153743602_Untitled2--rgov-66x44.jpg" alt="Examples of object manipulation tasks considered in this project."></a> <div class="imageCaptionContainer"> <div class="imageCaption">(a) Pre-grasping actions. (b,c,d) Searching for an object through pushing actions and scene de-cluttering. (e) Transferring objects from one bin to another.</div> <div class="imageCredit">Abdeslam Boularias (Rutgers)</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Abdeslam&nbsp;Boularias</div> <div class="imageTitle">Examples of object manipulation tasks considered in this project.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Robots typically rely on models that capture physical interactions with objects or humans in order to perform manipulation and navigation tasks reliably. Models can describe shapes and mechanical properties of objects or the pose and motion pattern of people. They can be used to simulate different outcomes given actions the robot can perform. The actions with the best forecasted outcomes are then selected for execution. In practice, the forecasted outcomes are often different from the real outcomes due to model inaccuracies. This difference is often referred to as the reality gap. Manually designed models are inherently inaccurate. While this problem is less pronounced in industrial robots that operate in closed, structured and controlled environments, it severely limits robot deployment in open human environments with novel objects or crowds of moving people. For example, an assistant robot in a repair shop needs to manipulate various tools and operate on new objects every day, while interacting with human partners.                   Intellectual Merit: This project developed automated and data-driven object and people modeling methods. This allows robots to build geometric and mechanical models of objects on the fly while manipulating them in a safe manner. It also allows to define predictive models for the future motion of people in a crowded environment. Given access to such stochastic physical models, this work introduced novel planning manipulation actions that are robust in terms of maximizing the probability of task completion. To achieve this, anytime solutions for simulation-guided planning under uncertainty were further developed and evaluated.  A. Object Modeling: The outcomes include new algorithms supported by empirical evaluations and theoretical analyses when possible. Three aspects of object modeling were considered: volumetric shapes, 6D poses (i.e., position and orientation of rigid bodies), and mechanical properties. The proposed algorithms were inspired from cognitive science, where intuitive Newtonian principles and probabilistic representations have been shown important for complex scene understanding. Unlike previous work, complete 3D shapes are learned on the fly using inverse physics reasoning. A robot starts by poking objects randomly and observing how they move and interact. Then, a physics engine and a 3D renderer generate synthetic images of the objects. The robot's poking actions along with gravitational and interaction forces are re-played in simulation using hypothesized models, while comparing against the observed real motions. Finally, the confidence in models with the most realistic simulations is increased. We demonstrated the importance of Newtonian principles in complex scene understanding given work in object detection and 6D pose estimation, which is important in less structured environments.   We used these probabilistic models in belief-space planning to robustly perform manipulation, such as pre-grasp sliding, open-loop peg-in-hole, in-hand manipulation with under-actuated hands, de-cluttering, bin packing, and constrained pick-and-place. These methods outperformed at the time of publication state-of-the-art alternatives given three criteria: data efficiency, time efficiency, and accuracy. Some of these tasks have not been previously performed by autonomous robots on unknown objects. The complexity and difficulty involved sets the resulting robotic systems at the forefront of robot manipulation research.    B. Human Modeling: We developed novel methods for simulating human behavior using synthetic data to learn models and policies of human-agent interaction as well as representations for human pose estimation. Key contributions include: (a) Cognitively-guided synthetic models of human movement and behavior to mitigate the bias in synthetic, ad-hoc models. Our approach combines the benefits of cognitively-grounded experimentation and synthetic modeling. We conducted a series of Virtual Reality (VR) experiments to monitor human behavior and calibrate our synthetic models. (b) Utility-Guided Sampling of Synthetic Data for Model Learning to generate low-confidence synthetic observations. This allows to supplement sparse, high-confidence real datasets, improve prediction accuracy and generalization. We demonstrated the approach's efficacy in multiple domains including human crowd behavior forecasting. (c) Graph Representations for 3D Human Pose Estimation to leverage the inherent structure of the human body. We proposed graph convolutional networks (GCNs) for 3D human pose estimation from monocular images. Our approach can incorporate structure and semantics of the human anatomy to achieve state-of-the-art performance.   Broader Impact: Given existing contributions, we provided implementations of our solutions to the community as open-source software packages, which were used by several other researchers. We publicly shared benchmarks and datasets for modeling manipulation challenges where physical and geometric properties play a critical role. This work helped promote the use of robots in multiple domains, such as logistics and manufacturing, as demonstrated by industrial partnerships. Most peer-reviewed papers produced by this project appeared at ICRA and IROS, the two flagship robotics conferences. Most of these articles were first-authored by Rutgers graduate students and often included undergraduate researchers as co-authors. The remaining articles were published in leading AI, machine learning and robotics conferences. One of the articles was finalist for a best paper award at ICRA 2019 and was featured in media articles.                       Last Modified: 10/19/2020       Submitted by: Abdeslam Boularias]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
