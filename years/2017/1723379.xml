<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AitF:  FULL: Collaborative Research:   PEARL: Perceptual Adaptive Representation Learning in the Wild</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>173754.00</AwardTotalIntnAmount>
<AwardAmount>173754</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or "domain." In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a "dog" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms "in the field."&lt;br/&gt;&lt;br/&gt;This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets.</AbstractNarration>
<MinAmdLetterDate>03/14/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/14/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1723379</AwardID>
<Investigator>
<FirstName>Kate</FirstName>
<LastName>Saenko</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kate Saenko</PI_FULL_NAME>
<EmailAddress>saenko@bu.edu</EmailAddress>
<PI_PHON>9789343699</PI_PHON>
<NSF_ID>000601038</NSF_ID>
<StartDate>03/14/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress><![CDATA[881 COMMONWEALTH AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7239</Code>
<Text>Algorithms in the Field</Text>
</ProgramElement>
<ProgramReference>
<Code>012Z</Code>
<Text>AitF FULL Projects</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~173754</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Over the past ten years Artificial Intelligence has been revolutionized by advances in statistical learning algorithms and access to much larger quantities of online data. In particular, applying deep learning techniques based on artificial neural networks to internet-scale datasets have pushed the state of the art forward in automatic image analysis, question answering and other AI tasks. However, artificial neural networks have a major limitation: to work well, they need to be trained on large quantities of annotated examples from the exact type of domain that they will later be tested on. The annotation process is costly and tedious, requiring human labelers to assign tags to images or draw bounding boxes or polygons around each object in a photo. If the annotated training dataset does not exactly reflect the new data that the neural network is tested on later, it will make more mistakes and achieve lower accuracy than on training-domain data.</p> <p>For example, a neural network trained on a fair-weather dataset annotated with car objects might achieve high accuracy on held-out examples from that dataset, but will do much worse on new images of foggy conditions, or rainy weather, etc. This problem is referred to as "domain shift" or "dataset bias." It applies to other types of visual shifts, such as from real photos to clipart or drawings. Dataset shift also happens in non-visual modalities, such as from Twitter-style text data to New York Times news articles.</p> <p>This project investigated the design of novel neural network architectures and training regimes that improve the network's robustness to such domain shifts. We focused on "domain adaptation" algorithms, which do not require explicit knowledge of what changed in the domain, but learn this information automatically from unlabeled data from the target domain, assumed to be given. This is much more cost effective than collecting new annotations on every target domain, but requires us to have a way to update the original model trained on the auxiliary "source" domain to the new target domain. It is also a general approach that is not tailored to any specific type of shift (such as fog) but applies to any type of shift, be it from weather, viewpoint, style, etc.</p> <p>We have developed several approaches to this problem throughout the project. One approach is based on distributional alignment of the source and target domains. The idea behind alignment is to train the network to discover how samples in the source domain can best be aligned (matched) to samples in the target domain. Such alignment can be done on the activations of the neural network's neurons (feature space) or on the images themselves (pixel space). This approach is motivated by theoretical results that bound the generalization error partially by the size of the discrepancy between domains. Theoretically, the smaller the discrepancy between source and target domains, the better the network will transfer.</p> <p>However, alignment methods assume that the source and target domains do in fact contain corresponding pairs of samples. In practice, this may not always be a valid assumption. For example, the source may have street scenes with lots of vehicles but the target may be outside the city and have few vehicles. Forcing full alignment will then remove useful information about individual vehicles in the source. To handle such cases, we came up with alternative methods based on "strong-weak" alignment, and other methods based on feature disentanglement.&nbsp;</p> <p>We evaluated our proposed domain adaptation methods on many benchmark datasets, some of which were collected for this project and have been made available to the broader research community (<a href="http://ai.bu.edu/visda-2020/">VisDA</a>, <a href="http://ai.bu.edu/M3SDA/">DomainNet</a>.) Our methods improved object recognition accuracy relative to using the unadapted source model, and also compared to existing domain adaptation techniques. One example was an object detector trained on a real image dataset that achieved 44 mean average precision (MAP) on a watercolor painting domain without any adaptation, but improved to 53 MAP with our adaptation technique. Importantly, the adaptation was done without any additional labeled data from the target domain. Thus the adaptation can be thought of as unsupervised fine-tuning of the network on target data.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/04/2021<br>      Modified by: Kate&nbsp;Saenko</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1723379/1723379_10477172_1612484035351_swda--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1723379/1723379_10477172_1612484035351_swda--rgov-800width.jpg" title="Object recognition in different visual domains"><img src="/por/images/Reports/POR/2021/1723379/1723379_10477172_1612484035351_swda--rgov-66x44.jpg" alt="Object recognition in different visual domains"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Object recognition in different visual domains: clipart, watercolor, foggy weather.</div> <div class="imageCredit">credit: Kuniaki Saito</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Kate&nbsp;Saenko</div> <div class="imageTitle">Object recognition in different visual domains</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Over the past ten years Artificial Intelligence has been revolutionized by advances in statistical learning algorithms and access to much larger quantities of online data. In particular, applying deep learning techniques based on artificial neural networks to internet-scale datasets have pushed the state of the art forward in automatic image analysis, question answering and other AI tasks. However, artificial neural networks have a major limitation: to work well, they need to be trained on large quantities of annotated examples from the exact type of domain that they will later be tested on. The annotation process is costly and tedious, requiring human labelers to assign tags to images or draw bounding boxes or polygons around each object in a photo. If the annotated training dataset does not exactly reflect the new data that the neural network is tested on later, it will make more mistakes and achieve lower accuracy than on training-domain data.  For example, a neural network trained on a fair-weather dataset annotated with car objects might achieve high accuracy on held-out examples from that dataset, but will do much worse on new images of foggy conditions, or rainy weather, etc. This problem is referred to as "domain shift" or "dataset bias." It applies to other types of visual shifts, such as from real photos to clipart or drawings. Dataset shift also happens in non-visual modalities, such as from Twitter-style text data to New York Times news articles.  This project investigated the design of novel neural network architectures and training regimes that improve the network's robustness to such domain shifts. We focused on "domain adaptation" algorithms, which do not require explicit knowledge of what changed in the domain, but learn this information automatically from unlabeled data from the target domain, assumed to be given. This is much more cost effective than collecting new annotations on every target domain, but requires us to have a way to update the original model trained on the auxiliary "source" domain to the new target domain. It is also a general approach that is not tailored to any specific type of shift (such as fog) but applies to any type of shift, be it from weather, viewpoint, style, etc.  We have developed several approaches to this problem throughout the project. One approach is based on distributional alignment of the source and target domains. The idea behind alignment is to train the network to discover how samples in the source domain can best be aligned (matched) to samples in the target domain. Such alignment can be done on the activations of the neural network's neurons (feature space) or on the images themselves (pixel space). This approach is motivated by theoretical results that bound the generalization error partially by the size of the discrepancy between domains. Theoretically, the smaller the discrepancy between source and target domains, the better the network will transfer.  However, alignment methods assume that the source and target domains do in fact contain corresponding pairs of samples. In practice, this may not always be a valid assumption. For example, the source may have street scenes with lots of vehicles but the target may be outside the city and have few vehicles. Forcing full alignment will then remove useful information about individual vehicles in the source. To handle such cases, we came up with alternative methods based on "strong-weak" alignment, and other methods based on feature disentanglement.   We evaluated our proposed domain adaptation methods on many benchmark datasets, some of which were collected for this project and have been made available to the broader research community (VisDA, DomainNet.) Our methods improved object recognition accuracy relative to using the unadapted source model, and also compared to existing domain adaptation techniques. One example was an object detector trained on a real image dataset that achieved 44 mean average precision (MAP) on a watercolor painting domain without any adaptation, but improved to 53 MAP with our adaptation technique. Importantly, the adaptation was done without any additional labeled data from the target domain. Thus the adaptation can be thought of as unsupervised fine-tuning of the network on target data.                            Last Modified: 02/04/2021       Submitted by: Kate Saenko]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
