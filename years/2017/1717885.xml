<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR:  Small:  Collaborative Research:  GAMBIT:  Efficient Graph Processing on a Memristor-based Embedded Computing Platform</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2017</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Matt Mutka</SignBlockName>
<PO_EMAI>mmutka@nsf.gov</PO_EMAI>
<PO_PHON>7032927344</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recently, graph processing received intensive interests in light of a wide range of needs to understand relationships. Graph analytics are widely used in key domains in our society, such as cyber security, social media, infrastructure monitoring (e.g., smart building), natural language processing, system biology, recommendation systems. These important applications all fall into fast-growing sectors in computer science and engineering research. On the other hand, in many emerging applications, the graph analytics are ideally performed in the edge (e.g., a mobile or embedded system) in order to allow the relationships between events to be discovered in the field where they are unfold. Unfortunately, the existing embedded systems equipped with conventional computing units like CPU/GPU cannot efficiently process large graphs in real time. Instead, large data centers are required to perform the graph processing, either incurring extra latency and energy due to data communication or only providing forensic (offline) graph analysis. This research aims to effectively enable graph analytics in embedded system with disruptive emerging technology. &lt;br/&gt;&lt;br/&gt;To support graph analytic applications with the limited hardware resources in embedded systems, this project seeks to develop GAMBIT -- a memristor-based embedded computing framework for efficient graph processing. Our research program aims to develop multi-layer techniques to enable highly efficient (e.g., 1000X) and scalable real-time graph analytics in embedded systems (i.e., network edge). It contains research efforts across circuit, architecture, system and vertical integration. (1) At the circuit level, the project proposes a memristor-based graph computing core to enable efficient computations for graph processing. (2) At the architecture level, the project proposes the complete memristor-based graph processing architecture for partitioned graph and various algorithms. (3) At the system level, the project develops a graph analytics framework for embedded systems and integrates it with a popular embedded OS. (4) For integration, the project proposes to develop an emulator of the proposed architecture and cross-layer HW/SW co-design techniques. This project contributes to society through engaging high-school and undergraduate students from minority-serving institutions into research, attracting women and under-represented groups into graduate education, expanding the computer engineering curriculum with graph processing and other emerging applications in embedded systems, disseminating research infrastructure for education and training, and collaborating with the industry.</AbstractNarration>
<MinAmdLetterDate>08/02/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/02/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1717885</AwardID>
<Investigator>
<FirstName>Hai</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hai Li</PI_FULL_NAME>
<EmailAddress>hai.li@duke.edu</EmailAddress>
<PI_PHON>6127039853</PI_PHON>
<NSF_ID>000538107</NSF_ID>
<StartDate>08/02/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress><![CDATA[2200 W. Main St, Suite 710]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The objective of this project is to improve graph processing efficiency on embedded computing platforms through integrated innovations at circuit, architecture, and system/application levels. Design the memristor (aka ReRAM) technology based graph computing unit in graphic analytics has been investigated. Furthermore, the in-memory graph processing accelerator architecture for low energy consumption of both computation and data movements has been explored.</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; During the first year, we mainly focused on the work on ?GraphR: Accelerating Graph Processing Using ReRAM.? Graph processing is well-known for the poor locality and high memory bandwidth requirement. GraphR is the first ReRAM-based graph processing accelerator. GraphR follows the principle of near-data processing and explores the opportunity of performing massive parallel analog operations with low hardware and energy cost. The GraphR architecture consists of two components: memory ReRAM and graph engine (GE). The core graph computations are performed in sparse matrix format in GEs (ReRAM crossbars). With small subgraphs processed by GEs, the gain of performing parallel operations overshadows the wastes due to sparsity. A conference paper has been published and presented in the Proceeding of the 24th IEEE International Symposium on High Performance Computer Architecture (HPCA?18) in February 2018.</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We then move our study to ?HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array.? To achieve high performance and energy efficiency, hardware acceleration (especially inference) of Deep Neural Networks (DNNs) is intensively studied both in academia and industry. However, we still face two challenges: large DNN models and datasets, which incur frequent off-chip memory accesses; and the training of DNNs, which is not well-explored in recent accelerator designs. To truly provide high throughput and energy efficient acceleration for the training of deep and large models, we inevitably need to use multiple accelerators to explore the coarse-grain parallelism, compared to the fine-grain parallelism inside a layer considered in most of the existing architectures. In this work, we propose a solution HYPAR to determine layer-wise parallelism for deep neural network training with an array of DNN accelerators. A conference paper has been published and presented in HPCA 2019.</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Toward to the end of the project, we systematically and constructively analyzed the latest developments in the field of machine learning accelerator research. Based on the parallelism being explored, we classify the design space of current deep learning accelerators into three levels: (1) Processing engine level parallelism comes naturally from the fact that the massive multiplication and accumulation operations can be processed independently. At this level, operation primitives are organized in a high-parallel fashion to exploit the temporal and/or spatial parallelism. (2) Memory level parallelism is achieved through parallel and efficient memory accessing to alleviate the memory wall effect. At this level, memory is optimized for parallel accessing to disjoint memory space and high data utilization. (3) Accelerator level parallelism emerges as a recent research direction to address the coordination of multiple accelerators in a heterogeneous system. At this level, parallelism for multiple accelerators is proposed. Our study help researchers identify and evaluate the effectiveness of different DNN hardware implementations and apply these concepts to future DNN accelerator designs and accelerator designs in other areas such as graph processing and genome sequencing. Our research leads to a conference publication in the Proceeding of the Asia and South Pacific Design Automation Conference (ASPDAC) in January 2020.</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The proposed project makes great efforts to make the students aware of the challenges in future intelligent computing systems. Moreover, the proposed project inspires students' interests in bio-inspired computing and prepares them for future systems that require the combined enhancements at both software and hardware implementation levels. The developed research outcomes, including the hardware implementation, architectural level optimization methodologies, and software/hardware co-design methodologies, have been seamlessly integrated into classes of ECE590-04 ? Emerging Memory and Computer Architecture, ECE590-12 ? Neuromorphic Computing, and ECE590-10 Computer Engineering ML &amp; Deep Neural Nets. Tutorial talks have been given in other classes for senior undergraduate students and graduate students, such as ECE 2193: Advanced VLSI Design, and ECE/CoE 1885: Departmental Seminar. The corresponding course materials and project information become available to the students to conduct the analysis and evaluation of the heterogeneous system integrating both conventional CPUs and GPUs and novel neuromorphic computing accelerators.</p> <p>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;The research has been conducted in close collaboration with our industrial and military partners, such as HPE Labs, and Air Force Research Lab. We also work on the novel circuit development with AFRL. We envision the direct transfer of our proposed techniques to the industry during our whole research process. This research's outcomes have direct impacts on future graph computing systems by making the research outcomes publicly accessible to those parameters. The internship or other opportunities provided by our collaborators also offer versatile training for undergraduate and graduate students as well as the ideal initial step of technology transferring.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/30/2020<br>      Modified by: Hai&nbsp;Li</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[       The objective of this project is to improve graph processing efficiency on embedded computing platforms through integrated innovations at circuit, architecture, and system/application levels. Design the memristor (aka ReRAM) technology based graph computing unit in graphic analytics has been investigated. Furthermore, the in-memory graph processing accelerator architecture for low energy consumption of both computation and data movements has been explored.        During the first year, we mainly focused on the work on ?GraphR: Accelerating Graph Processing Using ReRAM.? Graph processing is well-known for the poor locality and high memory bandwidth requirement. GraphR is the first ReRAM-based graph processing accelerator. GraphR follows the principle of near-data processing and explores the opportunity of performing massive parallel analog operations with low hardware and energy cost. The GraphR architecture consists of two components: memory ReRAM and graph engine (GE). The core graph computations are performed in sparse matrix format in GEs (ReRAM crossbars). With small subgraphs processed by GEs, the gain of performing parallel operations overshadows the wastes due to sparsity. A conference paper has been published and presented in the Proceeding of the 24th IEEE International Symposium on High Performance Computer Architecture (HPCA?18) in February 2018.        We then move our study to ?HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array.? To achieve high performance and energy efficiency, hardware acceleration (especially inference) of Deep Neural Networks (DNNs) is intensively studied both in academia and industry. However, we still face two challenges: large DNN models and datasets, which incur frequent off-chip memory accesses; and the training of DNNs, which is not well-explored in recent accelerator designs. To truly provide high throughput and energy efficient acceleration for the training of deep and large models, we inevitably need to use multiple accelerators to explore the coarse-grain parallelism, compared to the fine-grain parallelism inside a layer considered in most of the existing architectures. In this work, we propose a solution HYPAR to determine layer-wise parallelism for deep neural network training with an array of DNN accelerators. A conference paper has been published and presented in HPCA 2019.        Toward to the end of the project, we systematically and constructively analyzed the latest developments in the field of machine learning accelerator research. Based on the parallelism being explored, we classify the design space of current deep learning accelerators into three levels: (1) Processing engine level parallelism comes naturally from the fact that the massive multiplication and accumulation operations can be processed independently. At this level, operation primitives are organized in a high-parallel fashion to exploit the temporal and/or spatial parallelism. (2) Memory level parallelism is achieved through parallel and efficient memory accessing to alleviate the memory wall effect. At this level, memory is optimized for parallel accessing to disjoint memory space and high data utilization. (3) Accelerator level parallelism emerges as a recent research direction to address the coordination of multiple accelerators in a heterogeneous system. At this level, parallelism for multiple accelerators is proposed. Our study help researchers identify and evaluate the effectiveness of different DNN hardware implementations and apply these concepts to future DNN accelerator designs and accelerator designs in other areas such as graph processing and genome sequencing. Our research leads to a conference publication in the Proceeding of the Asia and South Pacific Design Automation Conference (ASPDAC) in January 2020.        The proposed project makes great efforts to make the students aware of the challenges in future intelligent computing systems. Moreover, the proposed project inspires students' interests in bio-inspired computing and prepares them for future systems that require the combined enhancements at both software and hardware implementation levels. The developed research outcomes, including the hardware implementation, architectural level optimization methodologies, and software/hardware co-design methodologies, have been seamlessly integrated into classes of ECE590-04 ? Emerging Memory and Computer Architecture, ECE590-12 ? Neuromorphic Computing, and ECE590-10 Computer Engineering ML &amp; Deep Neural Nets. Tutorial talks have been given in other classes for senior undergraduate students and graduate students, such as ECE 2193: Advanced VLSI Design, and ECE/CoE 1885: Departmental Seminar. The corresponding course materials and project information become available to the students to conduct the analysis and evaluation of the heterogeneous system integrating both conventional CPUs and GPUs and novel neuromorphic computing accelerators.        The research has been conducted in close collaboration with our industrial and military partners, such as HPE Labs, and Air Force Research Lab. We also work on the novel circuit development with AFRL. We envision the direct transfer of our proposed techniques to the industry during our whole research process. This research's outcomes have direct impacts on future graph computing systems by making the research outcomes publicly accessible to those parameters. The internship or other opportunities provided by our collaborators also offer versatile training for undergraduate and graduate students as well as the ideal initial step of technology transferring.          Last Modified: 12/30/2020       Submitted by: Hai Li]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
