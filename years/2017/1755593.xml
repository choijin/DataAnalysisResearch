<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Predicting When, Why, and How Multiple People Will Disagree when Answering a Visual Question</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2018</AwardEffectiveDate>
<AwardExpirationDate>04/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>174947.00</AwardTotalIntnAmount>
<AwardAmount>174947</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of a visual question answering (VQA) system is to empower people to find the answer to any question about any image.  For example, a VQA system could enable blind people to address daily visual challenges such as learning whether a pair of socks match or learning what type of food is in a can.  VQA services could also facilitate the creation of smarter environments, say to monitor how many defective products are on a factory assembly line at any given time.  A limitation of existing VQA systems is that they do not account for the fact that a visual question may elicit different answers from different people.  VQA systems could save time and reduce user frustration if they empowered users to anticipate and resolve any answer disagreements that may arise.  Blind and sighted people could more rapidly and accurately learn about the diversity of human perspectives on the visual world.  VQA services also could teach people how to ask visual questions that elicit the desired answer diversity.&lt;br/&gt;&lt;br/&gt;This project will create artificial intelligence (AI) models that can account for the possible diversity of answers inherent in crowd intelligence.  Specifically, AI models will be designed to predict when, why, and how human answer disagreement occurs, which in turn will enable new designs for human-computer partnerships.  This is challenging because it necessitates designing frameworks that simultaneously model and synthesize different and potentially conflicting perceptions of images and language for the many possible causes of disagreement.  To ensure that the AI models generalize across a broad range of applications, an existing corpus of over one million visual questions asked by blind and sighted people will be used to create annotated datasets that indicate when, why, and how much answer disagreement arises.  Methods will then be developed for automatically predicting directly from a visual question how much answer diversity will arise from a crowd, and why disagreement arises when it does.  Finally, a system will be designed for guiding visually-impaired users to more quickly formulate visual questions so they can receive a single, unambiguous crowd response (e.g., guide the person to better frame the visual content of interest with a mobile phone camera).  User studies with blind users will be conducted to empirically test the efficacy of the new system, with a focus on uncovering human-based issues in real-world, real-time situations.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/16/2018</MinAmdLetterDate>
<MaxAmdLetterDate>03/16/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1755593</AwardID>
<Investigator>
<FirstName>Danna</FirstName>
<LastName>Gurari</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Danna Gurari</PI_FULL_NAME>
<EmailAddress>danna.gurari@ischool.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000751475</NSF_ID>
<StartDate>03/16/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787019997</ZipCode>
<StreetAddress><![CDATA[1616 Guadalupe St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~174947</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Intellectual Merit.</p> <p>Contributions include human-annotated datasets and trained machine learning models that predict for a given visual question, an answer, the expected answer distribution from a crowd, and why answers will differ.&nbsp; Such machine learning models that understand the collective perspectives among a crowd represent a new breed of algorithms that can transform how people interact with computers and what can be achieved. Looking ahead, future VQA systems can offer a richer opportunity for collaborative conversations between users and computers. For instance, individuals could gather valuable feedback about the clarity (and so efficiency) of their visual question asking skills; e.g., are the visual questions too closed and so shutting down conversation or too open-ended and so leading to frustratingly long conversations to clarify the visual question. Transforming society&rsquo;s VQA systems is critical to more rapidly and accurately answer the vast range of questions asked by blind people seeking assistance in their daily lives as well as sighted individuals seeking to enhance their understanding of the visual world around them.</p> <p>&nbsp;</p> <p>Broader Impact.</p> <p>The PI&rsquo;s team proposed five AI challenges around a new dataset and set-up public evaluation servers with leaderboards in order to encourage a larger community to focus on developing algorithms that can assist people who are blind to overcome their real daily visual challenges. This has spurred international competition from over 10 teams on important accessibility problems.&nbsp; To encourage engagement by the community, the PI&rsquo;s team released code for all algorithms and pre-trained models.</p> <p>&nbsp;</p> <p>The PI also co-founded and organized three workshops that foster a community around the new dataset challenges as part of top computer vision conferences.&nbsp; These workshops center on the topic of visual question answering for blind people to inspire a larger community to focus on tackling assistive technology challenges faced by visually-impaired people.</p> <p>&nbsp;</p> <p>The PI also led activities that impact the education of students, both in the classroom and research lab. The PI involved her students in her &ldquo;Introduction to Machine Learning&rdquo; courses by introducing an assignment that required students to design algorithms to tackle dataset challenges established for this grant. In addition, the PI mentored 14 students to conduct research and co-publish about the development, analysis, and dissemination of these inclusive labeled datasets.</p> <p>&nbsp;</p> <p>Finally, the findings have been disseminated for broader audiences.&nbsp; The PI gave talks about this work to many academic and industry players (e.g., Microsoft, IBM, NLP Highlights Podcast, and conferences for the computer vision research and natural language processing communities).&nbsp; Her work has received media coverage including by MIT Technology Review, TechCrunch, and Korea IT Times.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/04/2021<br>      Modified by: Danna&nbsp;Gurari</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit.  Contributions include human-annotated datasets and trained machine learning models that predict for a given visual question, an answer, the expected answer distribution from a crowd, and why answers will differ.  Such machine learning models that understand the collective perspectives among a crowd represent a new breed of algorithms that can transform how people interact with computers and what can be achieved. Looking ahead, future VQA systems can offer a richer opportunity for collaborative conversations between users and computers. For instance, individuals could gather valuable feedback about the clarity (and so efficiency) of their visual question asking skills; e.g., are the visual questions too closed and so shutting down conversation or too open-ended and so leading to frustratingly long conversations to clarify the visual question. Transforming society’s VQA systems is critical to more rapidly and accurately answer the vast range of questions asked by blind people seeking assistance in their daily lives as well as sighted individuals seeking to enhance their understanding of the visual world around them.     Broader Impact.  The PI’s team proposed five AI challenges around a new dataset and set-up public evaluation servers with leaderboards in order to encourage a larger community to focus on developing algorithms that can assist people who are blind to overcome their real daily visual challenges. This has spurred international competition from over 10 teams on important accessibility problems.  To encourage engagement by the community, the PI’s team released code for all algorithms and pre-trained models.     The PI also co-founded and organized three workshops that foster a community around the new dataset challenges as part of top computer vision conferences.  These workshops center on the topic of visual question answering for blind people to inspire a larger community to focus on tackling assistive technology challenges faced by visually-impaired people.     The PI also led activities that impact the education of students, both in the classroom and research lab. The PI involved her students in her "Introduction to Machine Learning" courses by introducing an assignment that required students to design algorithms to tackle dataset challenges established for this grant. In addition, the PI mentored 14 students to conduct research and co-publish about the development, analysis, and dissemination of these inclusive labeled datasets.     Finally, the findings have been disseminated for broader audiences.  The PI gave talks about this work to many academic and industry players (e.g., Microsoft, IBM, NLP Highlights Podcast, and conferences for the computer vision research and natural language processing communities).  Her work has received media coverage including by MIT Technology Review, TechCrunch, and Korea IT Times.          Last Modified: 05/04/2021       Submitted by: Danna Gurari]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
