<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Nonlinear Partial Differential Equations, Monotone Numerical Schemes, and Scaling Limits for Semi-Supervised Learning on Graphs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>163136.00</AwardTotalIntnAmount>
<AwardAmount>163136</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Victor Roytburd</SignBlockName>
<PO_EMAI>vroytbur@nsf.gov</PO_EMAI>
<PO_PHON>7032928584</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Machine learning aims to harness the power of data to train algorithms to make predictions and perform complex tasks in science, engineering, medicine, and everyday life.  Despite the wide success of machine learning, the growing size of typical data sets is creating serious computational challenges, partially due to our incomplete understanding of how algorithms work in the big data regime.  The first goal of this project is to address these issues within a branch of machine learning called graph-based semi-supervised learning, which has found applications in problems such as protein sequencing, website classification, and speech recognition.  The investigator analyzes some recently-proposed algorithms for semi-supervised learning.  A deep mathematical understanding of these algorithms explains why they work and, more importantly, when they will fail.  The investigator uses these insights to develop new, fast, and more efficient algorithms for semi-supervised learning.  The second goal of this project is to develop and study new algorithms for curvature motions of curves and surfaces that are stable, efficient, and convergent.  Curvature motion has found many applications in science and engineering, including materials science, computer vision, image processing, and recently in data science and machine learning.  Curvature motion describes, for example, the motion of soap bubbles as well as the evolution of polycrystalline materials (such as metals and ceramic) during the manufacturing process.  Because both machine learning and curvature motions have very broad applications across science and engineering, any improvement in algorithms and in our understanding of them could have broad societal impacts.&lt;br/&gt;&lt;br/&gt;The first objective of this project is to study rigorously a recently-proposed algorithm for graph-based semi-supervised learning based on Lp-Laplacian regularization.  In particular, the investigator proves that the learning algorithms have continuum limits that correspond to solving a weighted p-Laplace equation in the viscosity sense.  In the limit of small labeled data and infinite unlabeled data, it has recently been conjectured that Lp-Laplacian regularization is ill-posed when p is smaller than the intrinsic dimension of the data.  The project aims to settle this conjecture rigorously.  The investigator uses these continuum limits to study the relationship between the fraction of labeled data and the performance of the algorithms, and to develop new and more efficient computational algorithms based on these insights.  The second objective of the project is to develop and analyze a new class of monotone finite difference schemes for curvature-driven motions of curves and surfaces, for which rigorous convergence proofs are available.  The investigator has recently discovered a general technique for constructing monotone finite difference schemes for a wide class of curvature motions of curves and surfaces.  He implements the new schemes for a variety of motions, experimentally tests convergence rates, and rigorously proves convergence to the viscosity solution using the Barles-Souganidis framework.  Monotonicity of the new schemes allows for the development of fully implicit and unconditionally monotone time-stepping schemes, and guarantees the approximations are capturing the correct continuum dynamics.</AbstractNarration>
<MinAmdLetterDate>08/08/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1713691</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Calder</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey Calder</PI_FULL_NAME>
<EmailAddress>jwcalder@umn.edu</EmailAddress>
<PI_PHON>6126261324</PI_PHON>
<NSF_ID>000677566</NSF_ID>
<StartDate>08/08/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Universiy of MInnesota, School of Mathematics- Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554550488</ZipCode>
<StreetAddress><![CDATA[206 Church St SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1266</Code>
<Text>APPLIED MATHEMATICS</Text>
</ProgramElement>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~163136</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was focused on theoretical and algorithmic developments in graph-based learning, and in particular, semi-supervised learning. Semi-supervised learning aims to learn new classes with as few training examples as possible. For example, even a child can look at a single image of, say, a train, and can easily learn the class and recognize new images of trains, even if they are different colors, sizes, shapes, etc. In contrast, current machine learning algorithms need hundreds, if not thousands, of examples of a new class to learn it well. Semi-supervised learning aims to bridge this gap.</p> <p><br />One main technical aspect of the project was to rigorously study some common algorithms for semi-supervised learning in the setting of very few labeled examples per class. The commonly used algorithm Laplace learning does not perform well in this regime. The theoretical work developed in this project explains why this is the case, and led directly to a new class of algorithms that do work well for semi-supervised learning with very few labels, including p-Laplace learning, the Properly Weighted graph Laplacian, and Poisson learning. The PI studied these algorithms rigorously, proving they work well with very few labels, and developed algorithms to efficiently find solutions to these semi-supervised learning problems. This work has the potential to have a broad impact in fields that use semi-supervised learning, such as DNA sequence classification, speech recognition, and image classification, among many other problems.</p> <p><br />Another contribution of the project is the analysis of several commonly used algorithms in the field of unsupervised learning. Unsupervised learning aims to uncover structure in data, such as the existence of distinct clusters. The PI studied unsupervised algorithms for dimension reduction, which aim to give simple representations of complicated high dimensional data. In particular, the PI studied spectral embedding algorithms, which use the spectrum of the graph Laplacian to embed data into low dimensional Euclidean spaces. The PI proved that these embeddings have a smoother structure than was previously understood, by establishing Lipschitz regularity for Laplacian eigenfunctions. These results were then used to strongly connect the graph Laplacian eigenfunctions to their continuum counterparts (a weighted Laplace-Beltrami operator on a data manifold), which provides improved tools for studying this important class of algorithms. The PI also established a continuum limit for Google's PageRank algorithm, which opens the door to a better understanding of ranking algorithms on directed graphs, which have applications in biology, chemistry, ecology, neuroscience, physics, and computer systems.</p> <p><br />A third, and final, contribution is the development of a new algorithm, called PDE acceleration, for solving convex calculus of variations problems. The algorithm draws inspiration from momentum, also called acceleration, methods for optimization that are popular in machine learning, and in particular, in deep learning. The PI generalized the ideas to the setting of solving partial differential equations that arise from calculus of variations problems, and proved convergence with an exponential rate of the resulting PDE acceleration algorithm. This work has the potential to have an impact on many areas of science and engineering that rely on variational formulations, such as geometric optics, thin film design, optimal control theory, and many other areas.</p><br> <p>            Last Modified: 11/25/2020<br>      Modified by: Jeffrey&nbsp;Calder</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was focused on theoretical and algorithmic developments in graph-based learning, and in particular, semi-supervised learning. Semi-supervised learning aims to learn new classes with as few training examples as possible. For example, even a child can look at a single image of, say, a train, and can easily learn the class and recognize new images of trains, even if they are different colors, sizes, shapes, etc. In contrast, current machine learning algorithms need hundreds, if not thousands, of examples of a new class to learn it well. Semi-supervised learning aims to bridge this gap.   One main technical aspect of the project was to rigorously study some common algorithms for semi-supervised learning in the setting of very few labeled examples per class. The commonly used algorithm Laplace learning does not perform well in this regime. The theoretical work developed in this project explains why this is the case, and led directly to a new class of algorithms that do work well for semi-supervised learning with very few labels, including p-Laplace learning, the Properly Weighted graph Laplacian, and Poisson learning. The PI studied these algorithms rigorously, proving they work well with very few labels, and developed algorithms to efficiently find solutions to these semi-supervised learning problems. This work has the potential to have a broad impact in fields that use semi-supervised learning, such as DNA sequence classification, speech recognition, and image classification, among many other problems.   Another contribution of the project is the analysis of several commonly used algorithms in the field of unsupervised learning. Unsupervised learning aims to uncover structure in data, such as the existence of distinct clusters. The PI studied unsupervised algorithms for dimension reduction, which aim to give simple representations of complicated high dimensional data. In particular, the PI studied spectral embedding algorithms, which use the spectrum of the graph Laplacian to embed data into low dimensional Euclidean spaces. The PI proved that these embeddings have a smoother structure than was previously understood, by establishing Lipschitz regularity for Laplacian eigenfunctions. These results were then used to strongly connect the graph Laplacian eigenfunctions to their continuum counterparts (a weighted Laplace-Beltrami operator on a data manifold), which provides improved tools for studying this important class of algorithms. The PI also established a continuum limit for Google's PageRank algorithm, which opens the door to a better understanding of ranking algorithms on directed graphs, which have applications in biology, chemistry, ecology, neuroscience, physics, and computer systems.   A third, and final, contribution is the development of a new algorithm, called PDE acceleration, for solving convex calculus of variations problems. The algorithm draws inspiration from momentum, also called acceleration, methods for optimization that are popular in machine learning, and in particular, in deep learning. The PI generalized the ideas to the setting of solving partial differential equations that arise from calculus of variations problems, and proved convergence with an exponential rate of the resulting PDE acceleration algorithm. This work has the potential to have an impact on many areas of science and engineering that rely on variational formulations, such as geometric optics, thin film design, optimal control theory, and many other areas.       Last Modified: 11/25/2020       Submitted by: Jeffrey Calder]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
