<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Exploring the Feasibility of Software Testing Techniques to Evaluate Fairness Algorithms in Software Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>131230.00</AwardTotalIntnAmount>
<AwardAmount>131230</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Today, software is making more automated decisions with societal impact. For example, software determines who gets a loan or gets hired, computes risk-assessment scores that help decide who goes to jail and who is set free, and aids in diagnosing and treating medical patients. The increased role of software in such decisions makes software fairness a critical property. As more societal functions operate in cyberspace, the importance of software fairness increases. This project evaluates the feasibility of using software testing technology to identify behaviors whose outputs are more favorable for certain inputs. Using testing in such a manner is aimed at capturing causal relationships between characteristics of the software inputs and the software behavior.  The approach is novel compared to typical machine learning classification techniques that analyze data but do not test the behavior of application software. The central idea is to identify causal relationships between software inputs and the way the software behaves, e.g., its outputs.&lt;br/&gt; &lt;br/&gt;Software testing enables conducting causal experiments consisting of running the software with nearly identical inputs that vary only in a key characteristic under test. Variations in that characteristic that affect behavior provide evidence of a causal relationship. Measuring such causal relationships requires test suites that focus on small variability in a key set of characteristics under test, while existing testing techniques focus on large variability that leads to greater coverage. As a result, existing techniques are ill-suited for measuring causal relationships and new technology is necessary. The result is the ability to test software for new properties for which no testing procedures existed.</AbstractNarration>
<MinAmdLetterDate>06/30/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1744471</AwardID>
<Investigator>
<FirstName>Yuriy</FirstName>
<LastName>Brun</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Yuriy Brun</PI_FULL_NAME>
<EmailAddress>brun@cs.umass.edu</EmailAddress>
<PI_PHON>4135770233</PI_PHON>
<NSF_ID>000559414</NSF_ID>
<StartDate>06/30/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexandra</FirstName>
<LastName>Meliou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexandra Meliou</PI_FULL_NAME>
<EmailAddress>ameli@cs.umass.edu</EmailAddress>
<PI_PHON>4135450698</PI_PHON>
<NSF_ID>000637003</NSF_ID>
<StartDate>06/30/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>010039242</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>025Z</Code>
<Text>SaTC: Secure and Trustworthy Cyberspace</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~131230</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern software makes automated decisions with significant societal impact in domains such as finance, employment, healthcare, and the criminal justice system. Such software may exhibit bias, e.g., systematically exhibiting preferential treatment for certain groups. The software industry struggles to remedy this situation because the inherent complexity of the software, coupled with modern software's use of data and machine learning, make understanding software behavior difficult. This project made progress toward developing a theoretical foundation and techniques to engineering software systems that satisfy fairness requirements imposed on them.<br /><br />Intellectual Merit: The project defined a new measure of software discrimination, causal fairness, aimed at capturing causal relationships between characteristics of the software inputs and the software behavior. The project also produced evidence that this measure captures software discrimination missed by prior definitions. The project then demonstrated the feasibility of using automated test generation techniques to measure causal fairness on real-world systems using real-world data. Specifically, the project found that test suites generated by existing automated test-input generation techniques are not effective at measuring fairness properties, and that generating test suites specifically aimed at measuring such properties is computationally feasible. Further, the project developed techniques for automatically diversifying database query results, which can aid the process of engineering fair data-driven software systems. These results suggest that fairness testing is an important research challenge and that automated test generation is a feasible approach to solving this challenge. The project communicated its results via five academic papers published at premier Software Engineering and Data Management conferences, with one winning an ACM SIGSOFT Distinguished Paper Award.<br /><br />Broader Impacts: The project's broader impact includes advancing the state-of-the-art of knowledge about fairness properties of software systems, and how to detect and mitigate fairness defects and bias. These advances can both improve software quality, in terms of fairness, and increase the public's trust in software. The project fostered a new collaboration between two PIs and helped four PhD students, one postdoctoral researcher, and two undergraduate students make progress in developing their academic careers. Of the nine personnel involved in the project, five are female and two are members of other underrepresented groups in Computer Science. Further, this project helped organize the IEEE/ACM International Workshop on Software Fairness (FairWare 2018) colocated with the IEEE/ACM 40th International Conference on Software Engineering (ICSE 2018) in Gothenburg, Sweden on May 29, 2018. This workshop brought together researchers from the Machine Learning and Software Engineering communities to increase the awareness of the software bias problem, share recent advances in the field, and create collaborations aimed at solving research challenges in software fairness.<br /><br /></p><br> <p>            Last Modified: 11/21/2018<br>      Modified by: Yuriy&nbsp;Brun</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern software makes automated decisions with significant societal impact in domains such as finance, employment, healthcare, and the criminal justice system. Such software may exhibit bias, e.g., systematically exhibiting preferential treatment for certain groups. The software industry struggles to remedy this situation because the inherent complexity of the software, coupled with modern software's use of data and machine learning, make understanding software behavior difficult. This project made progress toward developing a theoretical foundation and techniques to engineering software systems that satisfy fairness requirements imposed on them.  Intellectual Merit: The project defined a new measure of software discrimination, causal fairness, aimed at capturing causal relationships between characteristics of the software inputs and the software behavior. The project also produced evidence that this measure captures software discrimination missed by prior definitions. The project then demonstrated the feasibility of using automated test generation techniques to measure causal fairness on real-world systems using real-world data. Specifically, the project found that test suites generated by existing automated test-input generation techniques are not effective at measuring fairness properties, and that generating test suites specifically aimed at measuring such properties is computationally feasible. Further, the project developed techniques for automatically diversifying database query results, which can aid the process of engineering fair data-driven software systems. These results suggest that fairness testing is an important research challenge and that automated test generation is a feasible approach to solving this challenge. The project communicated its results via five academic papers published at premier Software Engineering and Data Management conferences, with one winning an ACM SIGSOFT Distinguished Paper Award.  Broader Impacts: The project's broader impact includes advancing the state-of-the-art of knowledge about fairness properties of software systems, and how to detect and mitigate fairness defects and bias. These advances can both improve software quality, in terms of fairness, and increase the public's trust in software. The project fostered a new collaboration between two PIs and helped four PhD students, one postdoctoral researcher, and two undergraduate students make progress in developing their academic careers. Of the nine personnel involved in the project, five are female and two are members of other underrepresented groups in Computer Science. Further, this project helped organize the IEEE/ACM International Workshop on Software Fairness (FairWare 2018) colocated with the IEEE/ACM 40th International Conference on Software Engineering (ICSE 2018) in Gothenburg, Sweden on May 29, 2018. This workshop brought together researchers from the Machine Learning and Software Engineering communities to increase the awareness of the software bias problem, share recent advances in the field, and create collaborations aimed at solving research challenges in software fairness.         Last Modified: 11/21/2018       Submitted by: Yuriy Brun]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
