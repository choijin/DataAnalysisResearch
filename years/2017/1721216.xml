<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Automatic Video Interpretation and Description</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>160000.00</AwardTotalIntnAmount>
<AwardAmount>160000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Digital information processing has become an essential part of modern life. It is nowadays often expressed in a form of multimedia, involving videos accompanied with images, captions, and audio. Given the explosive growth of such multimedia data, it is extremely critical that it is accurately summarized and organized for automatic processing in artificial intelligence. One important yet challenging problem is automatic interpretation and summarization of video content, having enormous applications in video advertisements, online video searching and browsing, movie recommendation based on personal preference, and essentially any electronic commerce platform. In this project, the research team plans to develop statistical tools to raise our capacity of processing digital information to respond to a rapid growth of video content in real-world applications. The primary objective is to create a learning system to decipher the meaning of visual expressions as perceived by the audience, with a focus on understanding semantic meaning conveyed by a video.&lt;br/&gt;&lt;br/&gt;This project aims to develop methods of automatic video interpretation and description, which understands visual thoughts expressed by a video and generates semantic expressions of the content of a video. Particularly, it will utilize conditional dependence structures of entities as well as between entities and their pertinent actions, in a framework of multi-label and hierarchical classification. It will focus on three areas: 1) entity and action learning, 2) semantic learning for long videos and content-based segmentation, and 3) automatic video description generation, each of which develops techniques in novel ways. In each area, classification will be performed collaboratively based on pairwise conditional label dependencies and temporal dependencies of video frames, characterized by graphical and hidden Markov models. Special effort will be devoted to learning from multiple sources and extracting latent structures corresponding to scenes of a video. The PIs also plan to release the software developed as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the codebase of the software developed. This will allow a wider growth of the  project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award.</AbstractNarration>
<MinAmdLetterDate>08/20/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/20/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1721216</AwardID>
<Investigator>
<FirstName>Xiaotong</FirstName>
<LastName>Shen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaotong Shen</PI_FULL_NAME>
<EmailAddress>xshen@umn.edu</EmailAddress>
<PI_PHON>6126247098</PI_PHON>
<NSF_ID>000280976</NSF_ID>
<StartDate>08/20/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName/>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramElement>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~160000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1"><span class="s1"><span>&nbsp;</span>Digital media has become an essential part of modern life, which&nbsp;</span><span class="s1">is often expressed in, for example, images, captions, and&nbsp;</span><span class="s1">audios. The project supported by this award studies method and algorithms&nbsp;</span><span class="s1">for automatic image interpretation and description, which understands visual expressions&nbsp;</span>of an image or a video and generates semantic interpretations.&nbsp;<span class="s1">The results of the project shed some new insight into how the content of an image can be&nbsp;</span>extracted and accurately described through examples. On this ground, we develop methods to train image and language models jointly for interpreting the content of an image. As a result of our effort, human intelligence is integrated with the processing capacity of the machine for understanding and describing visual expressions. Moreover, we also learned about the importance of the dependence structures of different image frames for action recognition.</p> <p class="p1"><span class="s1">The collaborative project has incorporated research results in teaching to create an exciting opportunity&nbsp;</span>for students in learning state-of-art technology. Moreover, during the project period, the PIs have mentored several undergraduates, four M.S. students, and eight Ph.D. students. Concerning the dissemination of the research results, the project has produced several image processing platforms for end-users. The PI has also delivered more than 20 invited presentations at national and international conferences, workshops, and institutional colloquia.</p><br> <p>            Last Modified: 09/02/2020<br>      Modified by: Xiaotong&nbsp;Shen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Digital media has become an essential part of modern life, which is often expressed in, for example, images, captions, and audios. The project supported by this award studies method and algorithms for automatic image interpretation and description, which understands visual expressions of an image or a video and generates semantic interpretations. The results of the project shed some new insight into how the content of an image can be extracted and accurately described through examples. On this ground, we develop methods to train image and language models jointly for interpreting the content of an image. As a result of our effort, human intelligence is integrated with the processing capacity of the machine for understanding and describing visual expressions. Moreover, we also learned about the importance of the dependence structures of different image frames for action recognition. The collaborative project has incorporated research results in teaching to create an exciting opportunity for students in learning state-of-art technology. Moreover, during the project period, the PIs have mentored several undergraduates, four M.S. students, and eight Ph.D. students. Concerning the dissemination of the research results, the project has produced several image processing platforms for end-users. The PI has also delivered more than 20 invited presentations at national and international conferences, workshops, and institutional colloquia.       Last Modified: 09/02/2020       Submitted by: Xiaotong Shen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
