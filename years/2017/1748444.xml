<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Constrained Statistical Estimation and Inference: Theory, Algorithms and Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>144973.00</AwardTotalIntnAmount>
<AwardAmount>144973</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project lies at the boundary of statistics and machine learning. The underlying theme is to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The project will explore theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications. The overall goal of the research is to develop theory and tools that can help scientists to conduct more effective data analysis.&lt;br/&gt;&lt;br/&gt;Many statistical methods are purely "data driven" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central. The project will develop minimax theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. Other constraints to be studied include shape restrictions such as convexity and monotonicity for high dimensional data. The project will also investigate the incorporation of physical constraints through the use of PDEs and models of physical dynamics and mechanics, focusing on both algorithms and theoretical bounds.</AbstractNarration>
<MinAmdLetterDate>07/26/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/26/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1748444</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Lafferty</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John D Lafferty</PI_FULL_NAME>
<EmailAddress>john.lafferty@yale.edu</EmailAddress>
<PI_PHON>2034368105</PI_PHON>
<NSF_ID>000092106</NSF_ID>
<StartDate>07/26/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Yale University</Name>
<CityName>New Haven</CityName>
<ZipCode>065208327</ZipCode>
<PhoneNumber>2037854689</PhoneNumber>
<StreetAddress>Office of Sponsored Projects</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 208327]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043207562</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>YALE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043207562</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Yale University]]></Name>
<CityName>New Haven</CityName>
<StateCode>CT</StateCode>
<ZipCode>065116814</ZipCode>
<StreetAddress><![CDATA[24 Hillhouse Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~144973</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The underlying theme of this project was to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The research was at the interface of statistics and machine learning. The project explored theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications.&nbsp; Many statistical methods are purely "data driven" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central.</p> <p>The project developed new theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. In particular, new algorithms were developed in the setting where data are obtained from from multiple distributed sensors, and a signal is to be estimated by aggregating the information. Sharp analysis was carried out for this setting under smoothness assumptions commonly made in nonparametric regression.&nbsp; Placing limits on the number of bits that each machine can use to transmit information to the central machine, the results obtained included both asymptotic lower bounds and matching upper bounds on the statistical risk under various settings. Three regimes were identified, depending on the relationship among the number of machines, the size of data available at each machine, and the communication budget.&nbsp;</p> <p>Other constraints studied included shape restrictions such as convexity and monotonicity for high dimensional data. In particular, new algorithms were developed for imposing monotonicity constraints in machine learning procedures for handling high dimensional data. These methods reshape pre-trained prediction rules to satisfy shape constraints. One such method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests. In both cases, efficient algorithms were developed for computing the estimators, and experiments were performed to demonstrate their performance.</p> <p>In another line of research in this project, procedures for numerical optimization were studied, and a new mathematical framework was developed for characterizing the complexity of optimizing individual convex functions. In particular, function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize the function were obtained, in a form that relates to the curvature of the function at the optimum. A new computational invariant was studied that can be seen as a computational analogue of the Fisher information in classical statistical estimation.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/10/2018<br>      Modified by: John&nbsp;D&nbsp;Lafferty</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The underlying theme of this project was to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The research was at the interface of statistics and machine learning. The project explored theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications.  Many statistical methods are purely "data driven" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central.  The project developed new theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. In particular, new algorithms were developed in the setting where data are obtained from from multiple distributed sensors, and a signal is to be estimated by aggregating the information. Sharp analysis was carried out for this setting under smoothness assumptions commonly made in nonparametric regression.  Placing limits on the number of bits that each machine can use to transmit information to the central machine, the results obtained included both asymptotic lower bounds and matching upper bounds on the statistical risk under various settings. Three regimes were identified, depending on the relationship among the number of machines, the size of data available at each machine, and the communication budget.   Other constraints studied included shape restrictions such as convexity and monotonicity for high dimensional data. In particular, new algorithms were developed for imposing monotonicity constraints in machine learning procedures for handling high dimensional data. These methods reshape pre-trained prediction rules to satisfy shape constraints. One such method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests. In both cases, efficient algorithms were developed for computing the estimators, and experiments were performed to demonstrate their performance.  In another line of research in this project, procedures for numerical optimization were studied, and a new mathematical framework was developed for characterizing the complexity of optimizing individual convex functions. In particular, function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize the function were obtained, in a form that relates to the curvature of the function at the optimum. A new computational invariant was studied that can be seen as a computational analogue of the Fisher information in classical statistical estimation.           Last Modified: 12/10/2018       Submitted by: John D Lafferty]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
