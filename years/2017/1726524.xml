<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Acquisition of a Mobile Manipulation Robot Platform for Research and Education</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2017</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project, acquiring a mobile manipulation robot platform, aims to address several major challenges of robotics technologies needed to boost robot functionality and applications by integrating manipulation and locomotion capabilities, such that a robot can operate and make changes in a broader environment without territorial restrictions. Specifically, the project specifies enabling:&lt;br/&gt;- Vision-based mobile robot navigation;&lt;br/&gt;- Vision-guided autonomous object manipulation and application to assembly;&lt;br/&gt;- Unified motion generation and control for locomotion and manipulation; and&lt;br/&gt;- Robot-human interaction and collaborations.&lt;br/&gt;This work addresses applications in many areas, such as manufacturing, agriculture health care, homeland security, public service, and entertainment.&lt;br/&gt;&lt;br/&gt;Current robots, particularly in manufacturing, have mainly been designed to be bolted on a production line and only execute a pre-defined action with high accuracy and repeatability with limited sensing and no autonomy or intelligence, which significantly restricts their further applications. To enhance the functionality of robots and extend their application scopes, an essential element is not often present: integration of manipulation and locomotion capabilities so that the robot can operate and make changes in a broader environment without territorial restrictions. Moreover, robots need to be equipped with a certain level of autonomy and intelligence so they can decide on their own what to do and how to do it.   The proponents aim to maximize the usage of the robot platform by providing this integration.&lt;br/&gt;&lt;br/&gt;Broader Impacts:&lt;br/&gt;The outcomes of the proposed research should significantly enhance the university's research and education infrastructure and facilitate close interaction and collaboration between local industries, nearby institutions, and the university. The enthusiasm and expectations for robots to further promote human science and technological level open up a large demand for new developments on technologies and education, as well as in training qualified researchers and engineers in robotics and related fields. The outcomes of this work offer not only needed results but also educational experience. Research problems will be adapted as senior design projects and thesis topics for undergraduates and graduate programs. Some of this work will be integrated into core courses as special lectures and lab projects. Furthermore, industrial involvement from Omron and General Motors will strengthen existing university-industry relations and create collaborative opportunities.</AbstractNarration>
<MinAmdLetterDate>09/20/2017</MinAmdLetterDate>
<MaxAmdLetterDate>10/16/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1726524</AwardID>
<Investigator>
<FirstName>Yubao</FirstName>
<LastName>Chen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yubao Chen</PI_FULL_NAME>
<EmailAddress>yubao@umich.edu</EmailAddress>
<PI_PHON>3135935579</PI_PHON>
<NSF_ID>000336245</NSF_ID>
<StartDate>09/20/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alex Y.</FirstName>
<LastName>Yi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alex Y. Yi</PI_FULL_NAME>
<EmailAddress>yashayi@umich.edu</EmailAddress>
<PI_PHON>3135836318</PI_PHON>
<NSF_ID>000544501</NSF_ID>
<StartDate>09/20/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yu</FirstName>
<LastName>Zheng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yu Zheng</PI_FULL_NAME>
<EmailAddress>yuuzheng@umich.edu</EmailAddress>
<PI_PHON>3135935072</PI_PHON>
<NSF_ID>000673965</NSF_ID>
<StartDate>09/20/2017</StartDate>
<EndDate>10/16/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Samir</FirstName>
<LastName>Rawashdeh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Samir Rawashdeh</PI_FULL_NAME>
<EmailAddress>srawa@umich.edu</EmailAddress>
<PI_PHON>3135935466</PI_PHON>
<NSF_ID>000677427</NSF_ID>
<StartDate>10/16/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Samir</FirstName>
<LastName>Rawashdeh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Samir Rawashdeh</PI_FULL_NAME>
<EmailAddress>srawa@umich.edu</EmailAddress>
<PI_PHON>3135935466</PI_PHON>
<NSF_ID>000677427</NSF_ID>
<StartDate>09/20/2017</StartDate>
<EndDate>10/16/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stanley</FirstName>
<LastName>Baek</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stanley Baek</PI_FULL_NAME>
<EmailAddress>stanbaek@umich.edu</EmailAddress>
<PI_PHON>7347636438</PI_PHON>
<NSF_ID>000688184</NSF_ID>
<StartDate>09/20/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan - Dearborn]]></Name>
<CityName>Dearborn</CityName>
<StateCode>MI</StateCode>
<ZipCode>481282406</ZipCode>
<StreetAddress><![CDATA[4901 Evergreen Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project supported the acquisition of a mobile manipulator platform, consisting of a mobile base, two robot arms with multi-fingered hands as end-effectors, and a vision system, to support various educational and research activities at the University of Michigan ? Dearborn (UM-D). Research topics included autonomous mobile manipulation, vision-guided robot control, robot-human interaction and collaboration, and mobile robot localization, mapping, and navigation. In parallel with the research activities, the platform provides students at UM-D with a state-of-the-art robot system to work on and gain hands-on research experience in robotics. The robot supports several undergraduate and graduate courses in robotics, which support a range of degree programs as core courses and electives. These include a new master?s program in robotics engineering which was created during the project period. The project has substantially enhanced UM-D?s capability to conduct robotics research and provide high-quality education and research training.</p> <p>We named the robot R2ED (Robot for Research and Education at Dearborn). It was designed to be modular and modifiable. For example, the face is 3D printed and it is a relatively simple task to redesign the face for new target applications or sensor form factors. Figure 1 shows a photo of the robot as well as a 3D model.&nbsp;&nbsp;</p> <p>The robot served as a basis for several research and educational projects. Example projects include (i) developing an object perception approach using a depth camera, (ii) robot grasp planning and control, (iii) face detection and social gestures recognition, and (iv) robot arm and hand teleoperation using wearable motion capture sensors.</p> <p>A simulated model of the robot has been developed and made publicly available for the broader robotics community. The COVID-19 pandemic caused lab closure and access restrictions. During this time, the team shifted efforts to developing a simulation model of the robot, which is essential and necessary in any case in order to test out algorithms and mitigate risk before trying them on the real robot. Figure 2 shows a screenshot of the simulated robot in the ROS/Gazebo environment.</p> <div> <div></div> </div> <p>&nbsp;</p><br> <p>            Last Modified: 03/02/2021<br>      Modified by: Samir&nbsp;Rawashdeh</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1726524/1726524_10522709_1614716287996_image_2021-03-02_151751--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1726524/1726524_10522709_1614716287996_image_2021-03-02_151751--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2021/1726524/1726524_10522709_1614716287996_image_2021-03-02_151751--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Photo and 3D rendering of R2ED, Robot for Research and Education at Dearborn.</div> <div class="imageCredit">University of Michigan - Dearborn</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Samir&nbsp;Rawashdeh</div> <div class="imageTitle">Figure 1</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1726524/1726524_10522709_1614716218315_image_2021-03-02_151635--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1726524/1726524_10522709_1614716218315_image_2021-03-02_151635--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2021/1726524/1726524_10522709_1614716218315_image_2021-03-02_151635--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">R2ED model in Gazebo simulation environment under the Robot Operating System</div> <div class="imageCredit">University of Michigan - Dearborn</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Samir&nbsp;Rawashdeh</div> <div class="imageTitle">Figure 2</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project supported the acquisition of a mobile manipulator platform, consisting of a mobile base, two robot arms with multi-fingered hands as end-effectors, and a vision system, to support various educational and research activities at the University of Michigan ? Dearborn (UM-D). Research topics included autonomous mobile manipulation, vision-guided robot control, robot-human interaction and collaboration, and mobile robot localization, mapping, and navigation. In parallel with the research activities, the platform provides students at UM-D with a state-of-the-art robot system to work on and gain hands-on research experience in robotics. The robot supports several undergraduate and graduate courses in robotics, which support a range of degree programs as core courses and electives. These include a new master?s program in robotics engineering which was created during the project period. The project has substantially enhanced UM-D?s capability to conduct robotics research and provide high-quality education and research training.  We named the robot R2ED (Robot for Research and Education at Dearborn). It was designed to be modular and modifiable. For example, the face is 3D printed and it is a relatively simple task to redesign the face for new target applications or sensor form factors. Figure 1 shows a photo of the robot as well as a 3D model.    The robot served as a basis for several research and educational projects. Example projects include (i) developing an object perception approach using a depth camera, (ii) robot grasp planning and control, (iii) face detection and social gestures recognition, and (iv) robot arm and hand teleoperation using wearable motion capture sensors.  A simulated model of the robot has been developed and made publicly available for the broader robotics community. The COVID-19 pandemic caused lab closure and access restrictions. During this time, the team shifted efforts to developing a simulation model of the robot, which is essential and necessary in any case in order to test out algorithms and mitigate risk before trying them on the real robot. Figure 2 shows a screenshot of the simulated robot in the ROS/Gazebo environment.             Last Modified: 03/02/2021       Submitted by: Samir Rawashdeh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
