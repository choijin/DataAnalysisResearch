<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:   CI-P: ShapeNet: An Information-Rich 3D Model Repository for Graphics, Vision and Robotics Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>33333.00</AwardTotalIntnAmount>
<AwardAmount>33333</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Roger Mailler</SignBlockName>
<PO_EMAI>rmailler@nsf.gov</PO_EMAI>
<PO_PHON>7032927982</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to plan the development of a richly annotated repository of 3D models called ShapeNet that currently exists only in a preliminary form. ShapeNet will include 3-4 million 3D models of everyday objects in 4-5 thousand categories, in a variety of representations. Models in the ShapeNet repository will be annotated with multiple annotation types: geometric (parts, symmetries), semantic (keywords for the shape and its parts), physical (weight, size), and functional (affordances, scene context). The availability of ShapeNet data, capturing the 3D geometry of a significant fraction of object categories in the world, together with associated detailed meta-data and semantic information, will catalyze major developments in graphics, vision and robotics by providing adequate data against which new proposed techniques and methodologies for shape or scene analysis and synthesis can be vetted -- and with which machine learning algorithms can be trained. ShapeNet can be considered an encyclopedia that facilitates the creation of intelligent systems and agents capable of operating autonomously in the world --- because they can have deep knowledge of that world.&lt;br/&gt;&lt;br/&gt;While most of the ShapeNet models will be initially found on the Web, the annotations will be obtained through an active learning combination of modest human input (including crowd-sourcing), extensive algorithmic transport, and human verification. During the planning period the effort will focus on mathematical representations of the semantic knowledge associated with 3D models, as well as on a design framework for key algorithms allowing knowledge transport from one model to another. Further challenges to be addressed include the quantification of data quality issues and the specification of all the multimodal (3D, image, language) UIs and APIs needed for users to be able to exploit and search this wealth of data, or to contribute additional models and annotations to it.</AbstractNarration>
<MinAmdLetterDate>06/07/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1729205</AwardID>
<Investigator>
<FirstName>Leonidas</FirstName>
<LastName>Guibas</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leonidas J Guibas</PI_FULL_NAME>
<EmailAddress>guibas@cs.stanford.edu</EmailAddress>
<PI_PHON>4157230350</PI_PHON>
<NSF_ID>000467730</NSF_ID>
<StartDate>06/07/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[450 Serra Mall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~33333</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has studied how to leverage relationships between multiple correlated data sets so as to improve our understanding of each data set separately. Our primary focus has been information propagation among visual data sets, such as 2D images and videos, or 3D models and scans. We have explored how to start with noisy maps and correspondences connecting multiple such data sets and clean them by exploiting consistency conditions among the maps, even in the presence of ambiguities caused by symmetry. In supervised learning tasks, we have shown how human part annotations on a sparse set of 3D models can be propagated to many more related models, easing the annotation burden. We have also demonstrated texture transfer from object images to 3D models, enriching the visual fidelity of the latter. In general, our tools facilitate the reliable transport of information between data sets, thus enabling novel collaboration methodologies between communities of people (e.g., scientists, doctors, etc.) with shared data of interest (e.g., images, 3D scans, graphs, etc.). The project has produced multiple publications in top venues in computer vision, computer graphics, and machine learning. It also contributed to data sets useful to the research community -- one example is the largest extant repository of 3D models with detailed semantic part annotations (ShapeNetCore).</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/15/2018<br>      Modified by: Leonidas&nbsp;J&nbsp;Guibas</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has studied how to leverage relationships between multiple correlated data sets so as to improve our understanding of each data set separately. Our primary focus has been information propagation among visual data sets, such as 2D images and videos, or 3D models and scans. We have explored how to start with noisy maps and correspondences connecting multiple such data sets and clean them by exploiting consistency conditions among the maps, even in the presence of ambiguities caused by symmetry. In supervised learning tasks, we have shown how human part annotations on a sparse set of 3D models can be propagated to many more related models, easing the annotation burden. We have also demonstrated texture transfer from object images to 3D models, enriching the visual fidelity of the latter. In general, our tools facilitate the reliable transport of information between data sets, thus enabling novel collaboration methodologies between communities of people (e.g., scientists, doctors, etc.) with shared data of interest (e.g., images, 3D scans, graphs, etc.). The project has produced multiple publications in top venues in computer vision, computer graphics, and machine learning. It also contributed to data sets useful to the research community -- one example is the largest extant repository of 3D models with detailed semantic part annotations (ShapeNetCore).          Last Modified: 12/15/2018       Submitted by: Leonidas J Guibas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
