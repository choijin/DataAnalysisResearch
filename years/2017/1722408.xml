<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Container Grids for Software Defined Security</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>224999.00</AwardTotalIntnAmount>
<AwardAmount>224999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project appears in three areas.  First, there is an $8B dollar market for legacy hardware network security products today that is in transition to Software Defined Security. Adoption of this general SDSec platform has the potential to accelerate that market transition and improve enterprise security.  Second, this project's success as an open platform could dramatically shorten the time between research insight and at-scale production deployments; security software built to a lower bar of scale/resiliency is faster and easier to build. The end result will be an increased rate of innovation, and new markets for niche, fit-for-purpose security functions that today are not profitable for vendors to build.  Last, this platform could harmonize enterprise security investments and practices for on premise workloads with security investments for public cloud workloads, solving a key dilemma faced by leading IT organizations. This will make those organizations more competitive on the world stage.&lt;br/&gt; &lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project addresses the two hardest technical challenges in building an enterprise-grade network security function, scale and resiliency, in a general way that can be applied to both new products and legacy codebases with no code change. Addressing scale/resiliency problems in a general way across many network security functions is novel, and this particular approach of massively parallel, lightweight packet processing functions to achieve this is new to this domain.  Why now? The massively parallel design proposed here is far too expensive and impractical with traditional hardware-based network functions. Even traditional hypervisor-based virtual machines carry too much hardware overhead to make this design cost competitive. By using vSwitches and network security functions packaged as Linux containers, the hardware overhead cost per instance drops dramatically, and this class of approach for scale/resiliency may be proven practical.</AbstractNarration>
<MinAmdLetterDate>06/28/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/01/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1722408</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Forster</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard K Forster</PI_FULL_NAME>
<EmailAddress>kyle@forster.com</EmailAddress>
<PI_PHON>4159902670</PI_PHON>
<NSF_ID>000724154</NSF_ID>
<StartDate>06/28/2017</StartDate>
<EndDate>08/22/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Lenrow</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Lenrow</PI_FULL_NAME>
<EmailAddress>cn3drl@gmail.com</EmailAddress>
<PI_PHON>4159902670</PI_PHON>
<NSF_ID>000755011</NSF_ID>
<StartDate>08/22/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>CN3 Systems, Inc</Name>
<CityName>San Francisco</CityName>
<ZipCode>941151022</ZipCode>
<PhoneNumber>4159902670</PhoneNumber>
<StreetAddress>3007 Jackson St</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065672091</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CN3, INC. WHICH WILL DO BUSINESS IN CALIFORNIA AS CN3 SYSTEMS, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[CN3 Systems, Inc]]></Name>
<CityName>San Francisco</CityName>
<StateCode>CA</StateCode>
<ZipCode>941151022</ZipCode>
<StreetAddress><![CDATA[3007 Jackson Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~224999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our Phase I research involved building a framework to push&nbsp; traffic through multiple containerized virtual network functions, building orchestration tools to automate the operation of large numbers of such containers, and building an automated test framework to allow us to experiment with different combinations of network functions from small configurations to very large configurations, initially on a simulated network running under a single physical server host, and then finally on a clous platform using high speed compute and networking devices. Our innovation consists of taking functions that were designed to run on a monolithic scale-up system and distributing identical, immutable copies of such functions across a large number of physical servers in a scale-out system configuration. Our phase I research plan was to allow us to quantify the behavior of our system design. In order to achieve commercial and economic success we needed be sure that the additional inter-system communications, software forwarding logic, and container packaging/execution overheads did not diminish the benefits of adding system capacity by deploying additional processing nodes. We had to insure that the various overheads inherent in our design only amounted to a small percentage of total resources, and grew linearly from small to large scale. :</p> <ol> <li>We plan to deploy multiple containers per node and need to observe behavior for a range of containers per node.&nbsp;</li> <li>We plan to deploy multiple instances of every virtual function, one per node and need to observe behavior across variable number of nodes</li> <li>We plan to use a variety of virtual functions and need to observe behaviors across different combinations and permutations of composed service chains.&nbsp;</li> </ol> <p>Overall, we successfully and emperically proved that our system design scales linearly across a wide range of node counts, containers per node, and varying service chain composition. The total overhead, as compared to a single system running a virtual function on bare metal, was observed to be less than 5% for all test scenarios&nbsp; and does not suffer non-linear growth for our target system size range. Our design leads to dramatic improvements in system elasticity, capacity, efficiency and resiliencey compared to monolithic implementations and we feel strongly that these benefits more than offset a 5% reduction in per-node capacity.&nbsp;</p> <p>Selected charts from our experimental results are included to summarize major themes.</p> <p>These experimental results suggest that it is possible to use our design to achieve the system benefits we predicted. During the course of our phase I commercial assessment, we determined that deploying containerized VNFs in a different, more distributed topology, would ease insertion into existing networks and as a result are pursuing this approach, which is closely related, and reuses much of the technology developed during our NSF SBIR Phase I project.&nbsp;</p><br> <p>            Last Modified: 09/15/2018<br>      Modified by: David&nbsp;Lenrow</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1722408/1722408_10496861_1537044802500_100x25flows--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1722408/1722408_10496861_1537044802500_100x25flows--rgov-800width.jpg" title="100 x .25Gbs streams"><img src="/por/images/Reports/POR/2018/1722408/1722408_10496861_1537044802500_100x25flows--rgov-66x44.jpg" alt="100 x .25Gbs streams"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This graph shows the per-stream throughput for a large number of flows each configured at .25Gbs. Note that as the total bandwidth approaches the maximum physical link bandwidth there is negligible degrade in the per-stream throughput for all streams</div> <div class="imageCredit">David Lenrow</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">David&nbsp;Lenrow</div> <div class="imageTitle">100 x .25Gbs streams</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1722408/1722408_10496861_1537044548919_1to3stackedx100--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1722408/1722408_10496861_1537044548919_1to3stackedx100--rgov-800width.jpg" title="Varying chain depth"><img src="/por/images/Reports/POR/2018/1722408/1722408_10496861_1537044548919_1to3stackedx100--rgov-66x44.jpg" alt="Varying chain depth"></a> <div class="imageCaptionContainer"> <div class="imageCaption">100 Instances of 1, 2, and 3 VNF chains</div> <div class="imageCredit">David Lenrow</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">David&nbsp;Lenrow</div> <div class="imageTitle">Varying chain depth</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our Phase I research involved building a framework to push  traffic through multiple containerized virtual network functions, building orchestration tools to automate the operation of large numbers of such containers, and building an automated test framework to allow us to experiment with different combinations of network functions from small configurations to very large configurations, initially on a simulated network running under a single physical server host, and then finally on a clous platform using high speed compute and networking devices. Our innovation consists of taking functions that were designed to run on a monolithic scale-up system and distributing identical, immutable copies of such functions across a large number of physical servers in a scale-out system configuration. Our phase I research plan was to allow us to quantify the behavior of our system design. In order to achieve commercial and economic success we needed be sure that the additional inter-system communications, software forwarding logic, and container packaging/execution overheads did not diminish the benefits of adding system capacity by deploying additional processing nodes. We had to insure that the various overheads inherent in our design only amounted to a small percentage of total resources, and grew linearly from small to large scale. :  We plan to deploy multiple containers per node and need to observe behavior for a range of containers per node.  We plan to deploy multiple instances of every virtual function, one per node and need to observe behavior across variable number of nodes We plan to use a variety of virtual functions and need to observe behaviors across different combinations and permutations of composed service chains.    Overall, we successfully and emperically proved that our system design scales linearly across a wide range of node counts, containers per node, and varying service chain composition. The total overhead, as compared to a single system running a virtual function on bare metal, was observed to be less than 5% for all test scenarios  and does not suffer non-linear growth for our target system size range. Our design leads to dramatic improvements in system elasticity, capacity, efficiency and resiliencey compared to monolithic implementations and we feel strongly that these benefits more than offset a 5% reduction in per-node capacity.   Selected charts from our experimental results are included to summarize major themes.  These experimental results suggest that it is possible to use our design to achieve the system benefits we predicted. During the course of our phase I commercial assessment, we determined that deploying containerized VNFs in a different, more distributed topology, would ease insertion into existing networks and as a result are pursuing this approach, which is closely related, and reuses much of the technology developed during our NSF SBIR Phase I project.        Last Modified: 09/15/2018       Submitted by: David Lenrow]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
