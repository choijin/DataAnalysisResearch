<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Automatic Video Interpretation and Description</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>160000.00</AwardTotalIntnAmount>
<AwardAmount>160000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Digital information processing has become an essential part of modern life. It is nowadays often expressed in a form of multimedia, involving videos accompanied with images, captions, and audio. Given the explosive growth of such multimedia data, it is extremely critical that it is accurately summarized and organized for automatic processing in artificial intelligence. One important yet challenging problem is automatic interpretation and summarization of video content, having enormous applications in video advertisements, online video searching and browsing, movie recommendation based on personal preference, and essentially any electronic commerce platform. In this project, the research team plans to develop statistical tools to raise our capacity of processing digital information to respond to a rapid growth of video content in real-world applications. The primary objective is to create a learning system to decipher the meaning of visual expressions as perceived by the audience, with a focus on understanding semantic meaning conveyed by a video.&lt;br/&gt;&lt;br/&gt;This project aims to develop methods of automatic video interpretation and description, which understands visual thoughts expressed by a video and generates semantic expressions of the content of a video. Particularly, it will utilize conditional dependence structures of entities as well as between entities and their pertinent actions, in a framework of multi-label and hierarchical classification. It will focus on three areas: 1) entity and action learning, 2) semantic learning for long videos and content-based segmentation, and 3) automatic video description generation, each of which develops techniques in novel ways. In each area, classification will be performed collaboratively based on pairwise conditional label dependencies and temporal dependencies of video frames, characterized by graphical and hidden Markov models. Special effort will be devoted to learning from multiple sources and extracting latent structures corresponding to scenes of a video. The PIs also plan to release the software developed as open source and build a user community around the language by ensuring that interested researchers are able to contribute to the codebase of the software developed. This will allow a wider growth of the  project. This aspect is of special interest to the software cluster in the Office of Advanced Cyberinfrastructure, which has provided co-funding for this award.</AbstractNarration>
<MinAmdLetterDate>08/20/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/20/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1721550</AwardID>
<Investigator>
<FirstName>Wing Hung</FirstName>
<LastName>Wong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wing Hung Wong</PI_FULL_NAME>
<EmailAddress>whwong@stanford.edu</EmailAddress>
<PI_PHON>6507252915</PI_PHON>
<NSF_ID>000441379</NSF_ID>
<StartDate>08/20/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943054000</ZipCode>
<StreetAddress><![CDATA[390 Serra Mall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramElement>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~160000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Digital media has become an essential part of modern life, which is most often expressed in, for example, images, captions, and audios. The project supported by this award studies method and algorithms for automatic image interpretation and description, which aim to understand visual expressions of an image or a video and generates semantic interpretations.</p> <p>The results of the project shed some new insight into how the content of an image can be extracted and accurately described through examples. On this ground, we develop methods to train image and language models jointly for interpreting the content of an image. As a result of our effort, human intelligence is integrated with the processing capacity of the machine for understanding and describing visual expressions. Moreover, we also learned about the importance of the dependence structures of different image frames for action recognition.</p> <p>The research in this project also produced new statistical modeling and machine learning methods that, although motivated by the automatic interpretation of digital media data, are applicable more generally to the analysis other types of high dimensional data.</p> <p>The collaborative project has incorporated research results in teaching to create an exciting opportunity for students in learning state-of-art technology. Moreover, during the project period, the PIs have mentored several students in statistics and data science, and have disseminated of the research results via publications as well as presentations at conferences, workshops, and institutional colloquia.</p><br> <p>            Last Modified: 09/04/2020<br>      Modified by: Wing Hung&nbsp;Wong</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Digital media has become an essential part of modern life, which is most often expressed in, for example, images, captions, and audios. The project supported by this award studies method and algorithms for automatic image interpretation and description, which aim to understand visual expressions of an image or a video and generates semantic interpretations.  The results of the project shed some new insight into how the content of an image can be extracted and accurately described through examples. On this ground, we develop methods to train image and language models jointly for interpreting the content of an image. As a result of our effort, human intelligence is integrated with the processing capacity of the machine for understanding and describing visual expressions. Moreover, we also learned about the importance of the dependence structures of different image frames for action recognition.  The research in this project also produced new statistical modeling and machine learning methods that, although motivated by the automatic interpretation of digital media data, are applicable more generally to the analysis other types of high dimensional data.  The collaborative project has incorporated research results in teaching to create an exciting opportunity for students in learning state-of-art technology. Moreover, during the project period, the PIs have mentored several students in statistics and data science, and have disseminated of the research results via publications as well as presentations at conferences, workshops, and institutional colloquia.       Last Modified: 09/04/2020       Submitted by: Wing Hung Wong]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
