<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Grading 10x Faster with AI Assistance</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2018</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>224700.00</AwardTotalIntnAmount>
<AwardAmount>224700</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajesh Mehta</SignBlockName>
<PO_EMAI>rmehta@nsf.gov</PO_EMAI>
<PO_PHON>7032922174</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project will show feasibility of a method for instant and accurate grading of student answers to a previously unseen question, after observing an instructor grade no more than 10% of the answers to that question. This project will use instructor-defined grading rubrics, which ensure consistency and provide helpful feedback to the student. Initial work will evaluate the method on multiple choice and fill-in-the-blank questions from a variety of Science, Technology, Engineering, and Math (STEM) subjects. The same method will be general enough to allow extension to short-answer and diagram questions. This work is important because although constructed-response questions are far more effective at assessing student knowledge and guiding student learning, they are significantly more time-consuming and onerous to grade than multiple-choice questions, and are therefore underused in many courses. Giving instructors the ability to grade constructed-response questions 10 times faster will increase the prevalence of this type of assessment and thereby improve STEM education outcomes. It will also increase educator effectiveness, because time that instructors currently spend grading can be better spent in personalized interaction with students.&lt;br/&gt;&lt;br/&gt;This project will apply and extend recently developed deep neural network methods for few-shot learning to the task of using a small sample of graded student answers to automatically grade the rest. Deep neural networks have recently emerged as the best approach to most machine learning problems. However, unlike humans, who can often learn a new concept after just a few examples, deep neural networks typically need thousands of examples to do the same. Recent developments have enabled deep neural networks to learn new concepts from just a few examples, after seeing millions of examples of other concepts. The first objective of this project is to evaluate these methods using millions of answers to a range of multiple-choice and fill-in-the-blank questions from a diverse set of STEM subjects graded on an existing service. This few-shot learning problem is significantly more complex than problems studied in the research literature. The second objective is to extend these methods, and to develop new methods, in order to reach human-level accuracy after observing the grading of no more than 10% of student answers. The last objective is to ensure that the proposed system is useful to real instructors by implementing a prototype user interface.</AbstractNarration>
<MinAmdLetterDate>12/20/2017</MinAmdLetterDate>
<MaxAmdLetterDate>12/20/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1747365</AwardID>
<Investigator>
<FirstName>Sergey</FirstName>
<LastName>Karayev</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sergey Karayev</PI_FULL_NAME>
<EmailAddress>sergeyk@gradescope.com</EmailAddress>
<PI_PHON>4252189416</PI_PHON>
<NSF_ID>000750833</NSF_ID>
<StartDate>12/20/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Gradescope Inc</Name>
<CityName>Berkeley</CityName>
<ZipCode>947042658</ZipCode>
<PhoneNumber>7029857442</PhoneNumber>
<StreetAddress>2030 Addison St Suite 500</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079771662</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GRADESCOPE, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>081112535</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Gradescope Inc]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>947042658</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8031</Code>
<Text>Education Products</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~224700</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Gradescope web application allows instructors to grade paper-based exams and homework online. In a typical scenario, an instructor creates an assignment template, prints out copies for students to fill out with their answers, and then scans and uploads the student work to Gradescope. After specifying the regions of each page that contain student responses, the instructor is able to grade the answers to each question in sequence, with a scoring rubric that is is always visible, modifiable, and shared with other graders of the assignment.</p> <p><br />As of September 2018, over 50 million student answers have been graded on Gradescope. Instructors report that grading with Gradescope is about 2x faster than grading on paper. While this is already highly impactful, the proposed innovation is to enable instructors to grade 10x faster using "AI-assisted grading." The idea is that the instructor should be able to grade a fraction of all answers to a given question, and have our system grade the rest of the answers automatically, with high accuracy. Specifically, we aimed for 95% accuracy after seeing the instructor grade 10% of the answers.<br />The proposed innovation of AI-assisted grading would cut grading time 10x (because only 10% of the answers would need to be graded by the instructor), such that the average K-12 teacher or higher-ed instructor will not need to spend more than a few minutes a day on grading complex assessments of their choosing. This would free up educators' time for more frequent and more attentive interaction with students, planning better lessons, and developing innovative assessments. AI-assisted grading would also make it possible for students to get instant feedback on their homework, and to take practice exams that are immediately graded exactly as the instructor would grade them: all that is needed is for a few sample answers to the practice questions to have been previously graded by the instructor.</p> <p><br />The innovation applies and extends recently developed deep neural network methods (a type of machine learning particularly well suited to complex pattern recognition problems) to the problem of few-shot learning -- learning quickly from just a few examples. In our Phase I work, we successfully implemented and extended several state-of-the-art methods for few-shot learning, applied them to the problem of grading, and built a user interface to interact with the system.</p> <p><br />First, we built a special annotation interface and went through over 25,000 real questions in the Gradescope database, labeling their type and other characteristics. This effort gave us insight into what kind of questions instructors are actually grading, and enabled us to compose a reference dataset of Simple Multiple-Choice, Complex Multiple-Choice, Fill-in-the-blank, and Drawing type questions, for testing different few-shot learning methods.<br />We implemented three main deep learning methods for few-shot learning, compared our implementations to results reported in literature on a benchmark dataset, and applied them to our own reference dataset. We extended the methods to make them work on our data, which has larger images and presents a harder learning problem. We also implemented an active-learning approach to few-shot learning (where the system can choose which answers the instructor has to grade), developing a novel Set-Based Active Policy approach. The system we developed met our initial goal of 95% accuracy after 10% answers graded for Simple and Complex Multiple-Choice questions, and reached 80% accuracy after 10% answers graded on the overall dataset, which also includes Fill-in-the-blank and Drawing questions. While we did not reach our goal of 95% grading accuracy after seeing the instructor grade 10% of the answers, we are confident in the methods and believe that more training data will allow the desired accuracy.</p> <p><br />Lastly, we developed a prototype user interface for interacting with the trained system. The interface gives the user the ability to select one example of each unique answer, click "AI Assistance" to form groups of answers based on the selected examples, and then to go through the groups to remove mistakes and to group answers that were left ungrouped. In testing, we discovered that this interface is insufficient for truly benefitting from the system we developed, and that we need to develop a novel interface in the active-learning paradigm that led to the highest accuracy on our dataset. Iterating development of this interface, in which the system is able to query users for labels, is the next step in our technical development.</p><br> <p>            Last Modified: 10/01/2018<br>      Modified by: Sergey&nbsp;Karayev</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533934874311_figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533934874311_figure1--rgov-800width.jpg" title="Explanation of proposed innovation"><img src="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533934874311_figure1--rgov-66x44.jpg" alt="Explanation of proposed innovation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Grading on paper is time-consuming and painful. Grading the same assignments on Gradescope is 2x as fast. AI-assisted grading (proposed innovation) aims to make it 10x faster.</div> <div class="imageCredit">Gradescope</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sergey&nbsp;Karayev</div> <div class="imageTitle">Explanation of proposed innovation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533935007713_question_types--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533935007713_question_types--rgov-800width.jpg" title="Question Types"><img src="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533935007713_question_types--rgov-66x44.jpg" alt="Question Types"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Examples of different question types gathered in our data annotation effort.</div> <div class="imageCredit">Gradescope</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Sergey&nbsp;Karayev</div> <div class="imageTitle">Question Types</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533935943988_ux2.001--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533935943988_ux2.001--rgov-800width.jpg" title="Prototype User Interface"><img src="/por/images/Reports/POR/2018/1747365/1747365_10526581_1533935943988_ux2.001--rgov-66x44.jpg" alt="Prototype User Interface"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our prototype user interface allows the grader to select all unique answers, then review groups that our system forms based on that information.</div> <div class="imageCredit">Gradescope</div> <div class="imageSubmitted">Sergey&nbsp;Karayev</div> <div class="imageTitle">Prototype User Interface</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Gradescope web application allows instructors to grade paper-based exams and homework online. In a typical scenario, an instructor creates an assignment template, prints out copies for students to fill out with their answers, and then scans and uploads the student work to Gradescope. After specifying the regions of each page that contain student responses, the instructor is able to grade the answers to each question in sequence, with a scoring rubric that is is always visible, modifiable, and shared with other graders of the assignment.   As of September 2018, over 50 million student answers have been graded on Gradescope. Instructors report that grading with Gradescope is about 2x faster than grading on paper. While this is already highly impactful, the proposed innovation is to enable instructors to grade 10x faster using "AI-assisted grading." The idea is that the instructor should be able to grade a fraction of all answers to a given question, and have our system grade the rest of the answers automatically, with high accuracy. Specifically, we aimed for 95% accuracy after seeing the instructor grade 10% of the answers. The proposed innovation of AI-assisted grading would cut grading time 10x (because only 10% of the answers would need to be graded by the instructor), such that the average K-12 teacher or higher-ed instructor will not need to spend more than a few minutes a day on grading complex assessments of their choosing. This would free up educators' time for more frequent and more attentive interaction with students, planning better lessons, and developing innovative assessments. AI-assisted grading would also make it possible for students to get instant feedback on their homework, and to take practice exams that are immediately graded exactly as the instructor would grade them: all that is needed is for a few sample answers to the practice questions to have been previously graded by the instructor.   The innovation applies and extends recently developed deep neural network methods (a type of machine learning particularly well suited to complex pattern recognition problems) to the problem of few-shot learning -- learning quickly from just a few examples. In our Phase I work, we successfully implemented and extended several state-of-the-art methods for few-shot learning, applied them to the problem of grading, and built a user interface to interact with the system.   First, we built a special annotation interface and went through over 25,000 real questions in the Gradescope database, labeling their type and other characteristics. This effort gave us insight into what kind of questions instructors are actually grading, and enabled us to compose a reference dataset of Simple Multiple-Choice, Complex Multiple-Choice, Fill-in-the-blank, and Drawing type questions, for testing different few-shot learning methods. We implemented three main deep learning methods for few-shot learning, compared our implementations to results reported in literature on a benchmark dataset, and applied them to our own reference dataset. We extended the methods to make them work on our data, which has larger images and presents a harder learning problem. We also implemented an active-learning approach to few-shot learning (where the system can choose which answers the instructor has to grade), developing a novel Set-Based Active Policy approach. The system we developed met our initial goal of 95% accuracy after 10% answers graded for Simple and Complex Multiple-Choice questions, and reached 80% accuracy after 10% answers graded on the overall dataset, which also includes Fill-in-the-blank and Drawing questions. While we did not reach our goal of 95% grading accuracy after seeing the instructor grade 10% of the answers, we are confident in the methods and believe that more training data will allow the desired accuracy.   Lastly, we developed a prototype user interface for interacting with the trained system. The interface gives the user the ability to select one example of each unique answer, click "AI Assistance" to form groups of answers based on the selected examples, and then to go through the groups to remove mistakes and to group answers that were left ungrouped. In testing, we discovered that this interface is insufficient for truly benefitting from the system we developed, and that we need to develop a novel interface in the active-learning paradigm that led to the highest accuracy on our dataset. Iterating development of this interface, in which the system is able to query users for labels, is the next step in our technical development.       Last Modified: 10/01/2018       Submitted by: Sergey Karayev]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
