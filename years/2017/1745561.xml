<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>366453.00</AwardTotalIntnAmount>
<AwardAmount>366453</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration>CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems&lt;br/&gt;&lt;br/&gt;Assistive machines - like powered wheelchairs, myoelectric prostheses and robotic arms - promote independence and ability in those with severe motor impairments. As the state- of-the-art in these assistive Cyber-Physical Systems (CPSs) advances, more dexterous and capable machines hold the promise to revolutionize ways in which those with motor impairments can interact within society and with their loved ones, and to care for themselves with independence. However, as these machines become more capable, they often also become more complex. Which raises the question: how to control this added complexity? A new paradigm is proposed for controlling complex assistive Cyber-Physical Systems (CPSs), like robotic arms mounted on wheelchairs, via simple low-dimensional control interfaces that are accessible to persons with severe motor impairments, like 2-D joysticks or 1-D Sip-N-Puff interfaces. Traditional interfaces cover only a portion of the control space, and during teleoperation it is necessary to switch between different control modes to access the full control space. Robotics automation may be leveraged to anticipate when to switch between different control modes. This approach is a departure from the majority of control sharing approaches within assistive domains, which either partition the control space and allocate different portions to the robot and human, or augment the human's control signals to bridge the dimensionality gap. How to best share control within assistive domains remains an open question, and an appealing characteristic of this approach is that the user is kept maximally in control since their signals are not altered or augmented. The public health impact is significant, by increasing the independence of those with severe motor impairments and/or paralysis. Multiple efforts will facilitate large-scale deployment of our results, including a collaboration with Kinova, a manufacturer of assistive robotic arms, and a partnership with Rehabilitation Institute of Chicago. &lt;br/&gt;&lt;br/&gt;The proposal introduces a formalism for assistive mode-switching that is grounded in hybrid dynamical systems theory, and aims to ease the burden of teleoperating high-dimensional assistive robots. By modeling this CPS as a hybrid dynamical system, assistance can be modeled as optimization over a desired cost function. The system's uncertainty over the user's goals can be modeled via a Partially Observable Markov Decision Processes. This model provides the natural scaffolding for learning user preferences. Through user studies, this project aims to address the following research questions: (Q1)  Expense: How expensive is mode-switching? (Q2)  Customization Need: Do we need to learn mode-switching from specific users? (Q3)  Learning Assistance: How can we learn mode-switching paradigms from a user? (Q4)  Goal Uncertainty: How should the assistance act under goal uncertainty? How will users respond? The proposal leverages the teams shared expertise in manipulation, algorithm development, and deploying real-world robotic systems. The proposal also leverages the teams complementary strengths on deploying advanced manipulation platforms, robotic motion planning and manipulation, and human-robot comanipulation, and on robot learning from human demonstration, control policy adaptation, and human rehabilitation. The proposed work targets the easier operation of robotic arms by severely paralyzed users. The need to control many degrees of freedom (DoF) gives rise to mode-switching during teleoperation. The switching itself can be cumbersome even with 2- and 3-axis joysticks, and becomes prohibitively so with more limited (1-D) interfaces. Easing the operation of switching not only lowers this burden on those already able to operate robotic arms, but may open use to populations to whom assistive robotic arms are currently inaccessible. This work is clearly synergistic: at the intersection of robotic manipulation, human rehabilitation, control theory, machine learning, human-robot interaction and clinical studies. The project addresses the science of CPS by developing new models of the interaction dynamics between the system and the user, the technology of CPS by developing new interfaces and interaction modalities with strong theoretical foundations, and the engineering of CPS by deploying our algorithms on real robot hardware and extensive studies with able-bodied and users with sprinal cord injuries.</AbstractNarration>
<MinAmdLetterDate>09/15/2017</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1745561</AwardID>
<Investigator>
<FirstName>Siddhartha</FirstName>
<LastName>Srinivasa</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Siddhartha Srinivasa</PI_FULL_NAME>
<EmailAddress>siddhartha.srinivasa@gmail.com</EmailAddress>
<PI_PHON>4129739615</PI_PHON>
<NSF_ID>000557185</NSF_ID>
<StartDate>09/15/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7918</Code>
<Text>CPS-Cyber-Physical Systems</Text>
</ProgramElement>
<ProgramReference>
<Code>7918</Code>
<Text>CYBER-PHYSICAL SYSTEMS (CPS)</Text>
</ProgramReference>
<ProgramReference>
<Code>8235</Code>
<Text>CPS-Synergy</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~366453</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-aa0933ba-7fff-4421-cc83-915e7263db7c">&nbsp;</span></p> <p dir="ltr"><span>Assistive robotic arms are increasingly enabling users with upper extremity disabilities to perform activities of daily living on their own. However, the increased capability and dexterity of the arms also make them harder to control with simple, low-dimensional interfaces like joysticks and sip-and-puff interfaces. A common technique to control a high-dimensional system like an arm with a low-dimensional input like a joystick is through switching between multiple control modes. However, our interviews with daily users of the Kinova JACO arm identified mode switching as a key problem, both in terms of time and cognitive load. We further confirmed objectively that mode switching consumes about 17.4% of execution time even for able-bodied users controlling the JACO.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Our key insight is that using even a simple model of mode switching, like time optimality, and a simple intervention, like automatically switching modes, significantly improves user satisfaction. A study using a 2D simulated mobile robot allowed us to determine that we can model the mode changing behavior of the users as time-optimal. Using time as the cost to optimize, we can use Dijkstra?s algorithm to predict when the robot should automatically change modes for the user. Having developed a policy for when to change modes, we tested how able-bodied users performed with the robot changed modes automatically. Users reported feeling comfortable with the assistance, and that the modes were changed at the same times and places that they would have wanted to change modes themselves.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Providing effective assistance would require knowing the user?s goal, but most shared autonomy methods do not assist when the goal is unknown or when the confidence for a particular goal is low for tasks with multiple goals. We present a method for shared autonomy that does not rely on predicting a single user goal, but assists for a distribution over goals. To assist for any distribution over goals, we formulate shared autonomy as a partially observable Markov decision process with uncertainty over user goals. To provide assistance in real-time over continuous state and action spaces, we used hindsight optimization to approximate solutions. We tested our method on two shared-control teleoperation scenarios, and one human-robot teaming scenario. Compared to predict-then-act methods, our method achieves goals faster, requires less user input, decreases user idling time, and results in fewer user-robot collisions. In our shared control teleoperation experiments, we found user preference differed for each task, even though our method outperformed a predict-then-act method across all objective measures for both tasks. In our studies, user?s tended to prefer a predict-then-act approach for the simpler grasping scenario, though not significantly so. For the more complex feeding task, users significantly preferred our shared autonomy method to a predict-then-act method. In fact, our method and blending were the only pair of algorithms that had a significant difference across all objective measures and the subjective measuring of like and rank.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Finally, we explored the role of complete autonomy for assistance with a core activity of daily living, feeding. Successful robotic assistive feeding depends on reliable bite acquisition and easy bite transfer. We believe that the ease of transfer not only depends on the transfer action but also is tightly coupled with the way a food item was acquired in the first place. Our studies with able-bodied human participants show that an intelligent food item dependent skewering strategy improves the bite acquisition success rate and that the choice of skewering location and the fork orientation affects the ease of bite transfer significantly. By leveraging contexts from previous bite acquisition attempts, our robot was also able to learn online how to acquire previously-unseen food items. When evaluated with people with mobility limitations,&nbsp; we found that more autonomy is not always better, as participants did not have a preference to use a robot with partial autonomy with perceived risk of errors over a robot with low autonomy. We also analyzed user preferences based on their range of mobility, and found that people with limited mobility had higher expectations of robot performance when compared to people with no mobility.</span></p> <p><br /><br /></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/15/2020<br>      Modified by: Siddhartha&nbsp;Srinivasa</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1745561/1745561_10398897_1581777493474_ADA_WEB003--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1745561/1745561_10398897_1581777493474_ADA_WEB003--rgov-800width.jpg" title="Assisted Feeding with the ADA robot"><img src="/por/images/Reports/POR/2020/1745561/1745561_10398897_1581777493474_ADA_WEB003--rgov-66x44.jpg" alt="Assisted Feeding with the ADA robot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Once a bite is secured, ADA transfers it safely to the user by tracking and reacting to their facial gestures and movement, and via safe compliant control.</div> <div class="imageCredit">University of Washington</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Siddhartha&nbsp;Srinivasa</div> <div class="imageTitle">Assisted Feeding with the ADA robot</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1745561/1745561_10398897_1581777361540_ADA_WEB002--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1745561/1745561_10398897_1581777361540_ADA_WEB002--rgov-800width.jpg" title="Assisted Feeding with the ADA robot"><img src="/por/images/Reports/POR/2020/1745561/1745561_10398897_1581777361540_ADA_WEB002--rgov-66x44.jpg" alt="Assisted Feeding with the ADA robot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">ADA selects a safe and secure bite of a strawberry by perceiving the plate via its eye-in-hand and implementing a learned compliant controller for grasping.</div> <div class="imageCredit">University of Washington</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Siddhartha&nbsp;Srinivasa</div> <div class="imageTitle">Assisted Feeding with the ADA robot</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Assistive robotic arms are increasingly enabling users with upper extremity disabilities to perform activities of daily living on their own. However, the increased capability and dexterity of the arms also make them harder to control with simple, low-dimensional interfaces like joysticks and sip-and-puff interfaces. A common technique to control a high-dimensional system like an arm with a low-dimensional input like a joystick is through switching between multiple control modes. However, our interviews with daily users of the Kinova JACO arm identified mode switching as a key problem, both in terms of time and cognitive load. We further confirmed objectively that mode switching consumes about 17.4% of execution time even for able-bodied users controlling the JACO.     Our key insight is that using even a simple model of mode switching, like time optimality, and a simple intervention, like automatically switching modes, significantly improves user satisfaction. A study using a 2D simulated mobile robot allowed us to determine that we can model the mode changing behavior of the users as time-optimal. Using time as the cost to optimize, we can use Dijkstra?s algorithm to predict when the robot should automatically change modes for the user. Having developed a policy for when to change modes, we tested how able-bodied users performed with the robot changed modes automatically. Users reported feeling comfortable with the assistance, and that the modes were changed at the same times and places that they would have wanted to change modes themselves.    Providing effective assistance would require knowing the user?s goal, but most shared autonomy methods do not assist when the goal is unknown or when the confidence for a particular goal is low for tasks with multiple goals. We present a method for shared autonomy that does not rely on predicting a single user goal, but assists for a distribution over goals. To assist for any distribution over goals, we formulate shared autonomy as a partially observable Markov decision process with uncertainty over user goals. To provide assistance in real-time over continuous state and action spaces, we used hindsight optimization to approximate solutions. We tested our method on two shared-control teleoperation scenarios, and one human-robot teaming scenario. Compared to predict-then-act methods, our method achieves goals faster, requires less user input, decreases user idling time, and results in fewer user-robot collisions. In our shared control teleoperation experiments, we found user preference differed for each task, even though our method outperformed a predict-then-act method across all objective measures for both tasks. In our studies, user?s tended to prefer a predict-then-act approach for the simpler grasping scenario, though not significantly so. For the more complex feeding task, users significantly preferred our shared autonomy method to a predict-then-act method. In fact, our method and blending were the only pair of algorithms that had a significant difference across all objective measures and the subjective measuring of like and rank.    Finally, we explored the role of complete autonomy for assistance with a core activity of daily living, feeding. Successful robotic assistive feeding depends on reliable bite acquisition and easy bite transfer. We believe that the ease of transfer not only depends on the transfer action but also is tightly coupled with the way a food item was acquired in the first place. Our studies with able-bodied human participants show that an intelligent food item dependent skewering strategy improves the bite acquisition success rate and that the choice of skewering location and the fork orientation affects the ease of bite transfer significantly. By leveraging contexts from previous bite acquisition attempts, our robot was also able to learn online how to acquire previously-unseen food items. When evaluated with people with mobility limitations,  we found that more autonomy is not always better, as participants did not have a preference to use a robot with partial autonomy with perceived risk of errors over a robot with low autonomy. We also analyzed user preferences based on their range of mobility, and found that people with limited mobility had higher expectations of robot performance when compared to people with no mobility.                 Last Modified: 02/15/2020       Submitted by: Siddhartha Srinivasa]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
