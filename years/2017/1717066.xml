<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Optical Skin For Robots: Tactile Sensing and Whole Body Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>440000.00</AwardTotalIntnAmount>
<AwardAmount>440000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Roger Mailler</SignBlockName>
<PO_EMAI>rmailler@nsf.gov</PO_EMAI>
<PO_PHON>7032927982</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will enable robots to feel what they touch. The key idea&lt;br/&gt;is to put cameras inside the body of the robot, looking outward at the&lt;br/&gt;robot skin as it deforms, and also through the robot skin to see nearby objects as they are grasped or avoided.  This approach addresses several challenges: 1) achieving close to human resolution (a million biological sensors) using millions of pixels, 2) reducing occlusion during grasping and manipulation, and detecting obstacles before impact, and 3) protecting expensive electronics and wiring while allowing replacement of worn out or damaged inexpensive skin. Humans replace the outer layer of our skin every month.  One theory as to why current robots are so clumsy is that they have little or no feeling in their skin.  Robots that can feel when they touch will make better servants and be more useful, especially when taking care of older adults and others needing help walking, dressing, cleaning, or feeding. A soft touch is needed in many tasks in the home and workplace. Teaching robots to do new tasks, and robot learning,&lt;br/&gt;will be much easier if robots can feel what they are doing. Robots will also be safer, and safer to work with, if they can feel accidental contact or the size of forces they are applying. Possible applications of this approach include aware furniture and car interiors that feel what a human is doing or wants to do, aware&lt;br/&gt;clothes, aware tools, and aware floors, walls, and ceilings. Optical skin-based sensing is a practical, affordable, manufacturable, and maintainable approach to sensing for touching and helping people.&lt;br/&gt;&lt;br/&gt;Technical goals for the project include first building and then installing on a robot a network of about 100 off-the-shelf small cameras (less than 1 cubic centimeter) that is capable of collecting information, deciding what video streams to pay attention to, and processing the video streams to estimate forces, slip, and object shape. The wiring, solder joints, and other connections in the sensing system don't have to repeatedly deform or face cyclic or variable stresses--major sources of failure. The materials that do deform or are stressed (the outer layer of the skin) can be optimized for sensing, grasping and manipulation, and mechanical robustness, and can be easily and cheaply replaced when worn or damaged.  The researchers will evaluate their work by co-developing an "optical skin" for hands and a synergistic soft hand. A transformative idea is to aggressively distribute high resolution imaging over the entire robot body. This reduces occlusion, a major issue in perception for manipulation. Given the low cost of imaging sensors, there is no longer a need to restrict optical sensing to infrared range finders (single pixel depth&lt;br/&gt;cameras), line cameras, or low resolution area cameras.</AbstractNarration>
<MinAmdLetterDate>07/25/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1717066</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Atkeson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher Atkeson</PI_FULL_NAME>
<EmailAddress>cga@cs.cmu.edu</EmailAddress>
<PI_PHON>4122685544</PI_PHON>
<NSF_ID>000477583</NSF_ID>
<StartDate>07/25/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Akihiko</FirstName>
<LastName>Yamaguchi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Akihiko Yamaguchi</PI_FULL_NAME>
<EmailAddress>akihikoy@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122689527</PI_PHON>
<NSF_ID>000712058</NSF_ID>
<StartDate>07/25/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[CMU Robotics Institute]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~440000</FUND_OBLG>
</Award>
</rootTag>
