<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Smooth National Measurement of Public Opinion Across Boundaries and Levels: A View From the Bayesian Spatial Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>122802.00</AwardTotalIntnAmount>
<AwardAmount>122802</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project will measure public opinion in voting constituencies around the United States. The project will provide estimates of opinion in districts with little or no survey data, such as state legislative districts. The project's intellectual merit comes from establishing a new means for measuring public opinion that not only uses survey respondents' answers to polling questions but also incorporates important information about where respondents are located and what that implies about geographic patterns in public opinion. Coupled with population information from the U.S. Census, the project will produce stronger estimates of public sentiment when survey data are sparsely distributed.  The investigators will release free software that includes user-friendly functions allowing any citizen to determine public opinion in his or her own district or in districts that have not yet cast any votes, such as proposed congressional districts in a redistricting cycle.  The software will allow more sophisticated users to obtain measures for any variable (even if unrelated to public opinion) in relationship to geographic boundaries, which will have extensions to research in public health, epidemiology, economics, sociology, business, and law. The broader impact to society will be that the data and software from this project will provide more information for the news media, the public, and elected officials regarding the outlook of the nation by constituency and locale, thereby providing a better understanding of the American representation process. The project will recruit a diverse group of research assistants that will be trained in this kind of statistical analysis.&lt;br/&gt;&lt;br/&gt;Studies relating to public opinion often settle for less-than-ideal data. Frequently, researchers will measure public opinion in the 50 states or the 435 congressional districts by pooling together several surveys taken over time (losing a sense of change over time), using old measures of public opinion (which may not be consistent with current public views), or using presidential vote share to approximate public sentiment (which is prone to error because factors besides ideology affect vote choices). With smaller districts than these, such as state legislative districts, the problem is magnified considerably, because it is rare to have many survey respondents in such a small area. In this project, the investigators ask: How can survey responses and the geographic location of the respondents be used to reliably forecast constituency public opinion? To answer this question, the investigators will use the method of Bayesian universal kriging. This technique fits a training model over survey data to determine how demographic factors shape public opinion and how the portion of survey responses that cannot be explained by demographics can be explained by a geographically smoothed process. With a model like this, public opinion in constituencies can be predicted with known population demographics and the values of the geographically smoothed error process over that district.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/28/2018</MinAmdLetterDate>
<MaxAmdLetterDate>03/28/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1761582</AwardID>
<Investigator>
<FirstName>Jeff</FirstName>
<LastName>Gill</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeff Gill</PI_FULL_NAME>
<EmailAddress>jgill@american.edu</EmailAddress>
<PI_PHON>3148253469</PI_PHON>
<NSF_ID>000441493</NSF_ID>
<StartDate>03/28/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>American University</Name>
<CityName>Washington</CityName>
<ZipCode>200168001</ZipCode>
<PhoneNumber>2028853440</PhoneNumber>
<StreetAddress>4400 Massachusetts Avenue, NW</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>077795060</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>AMERICAN UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>077795060</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[American University]]></Name>
<CityName/>
<StateCode>DC</StateCode>
<ZipCode>200168003</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<ProgramElement>
<Code>1371</Code>
<Text>Political Science</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~122802</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>So how would one estimate an unobserved outcome that is geographically<br />between two observed outcomes. Obviously averaging or interpolating<br />would provide a solution, but these simple approaches do not account<br />for variation in this measurement that are a function of other<br />explanatory variables. A goal of spatial statistics is to relate a set<br />of these explanatory variables to the outcome in a way that relates<br />case-specific information as well as geolocation. So for instance, a<br />particular fracking site is likely to be as productive as sites<br />nearby but particular geological characteristics of that location<br />may matter as much or more.<br /><br />In this grant work, we first extended the geospatial method of kriging<br />(spatial smoothing) and focus on how to estimate a Bayesian multilevel<br />kriging model. This allows us to treat the data in more flexible ways,<br />to include prior information before collecting the data, to produce a<br />smooth, continuous ``density blanket'' of the outcomes over a<br />geographical, reason, and how to deal with very large datasets. This<br />approach led to some serious computational challenges, which we<br />overcame by working on developing new mathematical and computational<br />algorithms to make analysis possible and in realistic times. These are<br />new treatments of large data matrices that contribute a related and<br />growing area in spatial ``big data.'' Geospatial statistical models<br />are clearly important in the big data world.&nbsp; There is little doubt<br />that marketeers, political campaigns, government agencies,<br />epidemiologists, and a wide variety of social science researchers<br />will continue to gather and use finely-grained geographical data. But<br />such work is uniquely challenging since they provide not only<br />conventional covariate analysis but also relational specifications<br />between cases, thus exploding the impact of data size. Here we<br />address a key tradeoff that is imposed by a combination of big data<br />and spatial model specification: the choice between realistic<br />computational time and the ability to use the full dataset.<br /><br />Our specific intellectual merit contributions are centered on<br />improvements in Bayesian spatial modeling, numerical handling of large<br />datasets, algorithmic developments implemented in publicly distributed<br />software, graphical summarization tools, and a new computational<br />estimation and prediction process: Bootstrap Random Spatial Sampling<br />(BRSS). Using the BRSS method, researchers will be able to examine<br />research questions with spatial correlation with less computation time<br />and power. Utilizing the BRSS method, researchers can take a random<br />sample of the data as well as a random sample of the cases to predict.<br />With each round of the bootstrap, we therefore create an unbiased<br />approximation of both parameter estimates from the model and forecasts<br />of the selected predictive observations.&nbsp; At the end, we pool the<br />sample values of parameter estimates in order to summarize the results.<br /><br />Measurable results from this work include two referee journal articles<br />at high impact outlets, two packages of freely distributed software,<br />and the training of five graduate students plus two undergraduates in<br />a technical area. Furthermore, the broader impact of these activities<br />means that the tools are disseminated to a wide audience of empirical<br />researchers, and these students are trained in advanced statistical<br />estimation and computational algorithmic advancing their academic and<br />professional careers.</p><br> <p>            Last Modified: 11/04/2020<br>      Modified by: Jeff&nbsp;Gill</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ So how would one estimate an unobserved outcome that is geographically between two observed outcomes. Obviously averaging or interpolating would provide a solution, but these simple approaches do not account for variation in this measurement that are a function of other explanatory variables. A goal of spatial statistics is to relate a set of these explanatory variables to the outcome in a way that relates case-specific information as well as geolocation. So for instance, a particular fracking site is likely to be as productive as sites nearby but particular geological characteristics of that location may matter as much or more.  In this grant work, we first extended the geospatial method of kriging (spatial smoothing) and focus on how to estimate a Bayesian multilevel kriging model. This allows us to treat the data in more flexible ways, to include prior information before collecting the data, to produce a smooth, continuous ``density blanket'' of the outcomes over a geographical, reason, and how to deal with very large datasets. This approach led to some serious computational challenges, which we overcame by working on developing new mathematical and computational algorithms to make analysis possible and in realistic times. These are new treatments of large data matrices that contribute a related and growing area in spatial ``big data.'' Geospatial statistical models are clearly important in the big data world.  There is little doubt that marketeers, political campaigns, government agencies, epidemiologists, and a wide variety of social science researchers will continue to gather and use finely-grained geographical data. But such work is uniquely challenging since they provide not only conventional covariate analysis but also relational specifications between cases, thus exploding the impact of data size. Here we address a key tradeoff that is imposed by a combination of big data and spatial model specification: the choice between realistic computational time and the ability to use the full dataset.  Our specific intellectual merit contributions are centered on improvements in Bayesian spatial modeling, numerical handling of large datasets, algorithmic developments implemented in publicly distributed software, graphical summarization tools, and a new computational estimation and prediction process: Bootstrap Random Spatial Sampling (BRSS). Using the BRSS method, researchers will be able to examine research questions with spatial correlation with less computation time and power. Utilizing the BRSS method, researchers can take a random sample of the data as well as a random sample of the cases to predict. With each round of the bootstrap, we therefore create an unbiased approximation of both parameter estimates from the model and forecasts of the selected predictive observations.  At the end, we pool the sample values of parameter estimates in order to summarize the results.  Measurable results from this work include two referee journal articles at high impact outlets, two packages of freely distributed software, and the training of five graduate students plus two undergraduates in a technical area. Furthermore, the broader impact of these activities means that the tools are disseminated to a wide audience of empirical researchers, and these students are trained in advanced statistical estimation and computational algorithmic advancing their academic and professional careers.       Last Modified: 11/04/2020       Submitted by: Jeff Gill]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
