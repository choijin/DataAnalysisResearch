<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Improving the Validity and Reliability of  Creativity Ratings in Engineering Design</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>185852.00</AwardTotalIntnAmount>
<AwardAmount>185852</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Georgia-Ann Klutke</SignBlockName>
<PO_EMAI>gaklutke@nsf.gov</PO_EMAI>
<PO_PHON>7032922443</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Without creativity, there is no potential for innovation. Consequently, creativity is seen as an essential component of engineering design. Numerous creativity metrics have been developed to assess the creativity of designs people produce. Such metrics are important for assessing business and engineering practices, a key part of improving US innovation and economic competitiveness. However, existing metrics are often scattered across different, specialized domains and it is difficult to validate their ability to accurately measure the creativity of the wide variety of engineered solutions that are produced. In addition, past research provides limited guidance on how to use and validate creativity metrics for a given design problem. This has led to universally applying metrics without systematic assessment of where and when a given metric is appropriate for a given task. This award supports fundamental research into how to evaluate the effectiveness of different creativity metrics for different types of products or services. This project does so by unifying statistical models that can experimentally validate how well different creativity metrics perform across design domains. Thus, the work will impact society by providing a verifiable method for identifying what does and does not improve creativity. Because creativity and innovation are the drivers of economic success, the work has the opportunity to drive design innovation and, as a bi-product, help stimulate the economy. In addition, the unified statistical framework developed as part of the research will advance the field of engineering design and applied mathematics by providing evidence on the utility of this approach for measuring accuracy and precision. The research involves several disciplines including engineering, psychology and applied mathematics. The multidisciplinary approach will help broaden participation of underrepresented groups in research.&lt;br/&gt;&lt;br/&gt;The technical objectives of this project are to: (1) evaluate the effectiveness of creativity metrics through the development of a unified statistical framework that combines two techniques--the minimax conditional entropy principle and the Lovasz-Bregman Divergence--to compare the accuracy and precision of different metrics for a given problem; and (2) experimentally validate a methodology for validating the transfer of creativity metrics across different design domains to identify metrics that are robust across different applications within design and systems engineering via variance and confidence measures of the Lovasz-Bregman Divergence. The results from this project will advance the field of engineering by developing a unified method of measuring and comparing mathematical, computational, and human-judgment models of creativity. This new knowledge will provide a rigorous foundation upon which to build and verify methods of design creativity across a wide variety of design disciplines (e.g., arts and architecture, psychology, engineering, business).</AbstractNarration>
<MinAmdLetterDate>07/17/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/17/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1728086</AwardID>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Fuge</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Fuge</PI_FULL_NAME>
<EmailAddress>fuge@umd.edu</EmailAddress>
<PI_PHON>3014056269</PI_PHON>
<NSF_ID>000703948</NSF_ID>
<StartDate>07/17/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1464</Code>
<Text>ESD-Eng &amp; Systems Design</Text>
</ProgramElement>
<ProgramReference>
<Code>067E</Code>
<Text>DESIGN TOOLS</Text>
</ProgramReference>
<ProgramReference>
<Code>068E</Code>
<Text>DESIGN THEORY</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~185852</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project studied measurement instruments for evaluating the creativity of ideas produced by engineers or designers. Such measurement instruments inform both research and practice in how to make organizations more innovative and increase economic competitiveness. However, correctly calibrating such instruments and trusting that they work as intended for a given application is difficult and error-prone. In response, this project studied several tools from statistics to help quantify and improve common (as well as new) measurement instruments for engineering creativity.&nbsp;</p> <p><br />Specific outcomes of this project include the following. First, the project created new methods for quantifying the accuracy and reliability of engineering creativity metrics and comparing them across different problem domains to determine when a metric is appropriate to a given problem. This provided new mathematical tools to those seeking to improve how we measure engineering creativity. Second, the project conducted statistical analyses that compared commonly used metrics to evaluations provided by domain experts. These analyses provided insight into where (or under what type of application) common metrics do well versus poorly. Third, the project provided new visualization techniques for understanding differences in ideas created by designers and what makes them similar, different, or creative compared to others. These techniques provided new mechanisms to understand, both visually and quantitatively, which ideas produced by a person or team were considered creative and help disentangle the factors that experts use to judge whether ideas are creative or not. Forth, the project produced several datasets and code repositories that help other researchers or practitioners benchmark their work against the new metrics produced by this project. These open data and code resources advance the US scientific research infrastructure and improve the efficiency and replicability of future research in this area. Lastly, this project produced several archival publications documenting the above outcomes such that future researchers or practioners can build upon or leverage this project's outcomes.</p><br> <p>            Last Modified: 02/03/2021<br>      Modified by: Mark&nbsp;Fuge</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project studied measurement instruments for evaluating the creativity of ideas produced by engineers or designers. Such measurement instruments inform both research and practice in how to make organizations more innovative and increase economic competitiveness. However, correctly calibrating such instruments and trusting that they work as intended for a given application is difficult and error-prone. In response, this project studied several tools from statistics to help quantify and improve common (as well as new) measurement instruments for engineering creativity.    Specific outcomes of this project include the following. First, the project created new methods for quantifying the accuracy and reliability of engineering creativity metrics and comparing them across different problem domains to determine when a metric is appropriate to a given problem. This provided new mathematical tools to those seeking to improve how we measure engineering creativity. Second, the project conducted statistical analyses that compared commonly used metrics to evaluations provided by domain experts. These analyses provided insight into where (or under what type of application) common metrics do well versus poorly. Third, the project provided new visualization techniques for understanding differences in ideas created by designers and what makes them similar, different, or creative compared to others. These techniques provided new mechanisms to understand, both visually and quantitatively, which ideas produced by a person or team were considered creative and help disentangle the factors that experts use to judge whether ideas are creative or not. Forth, the project produced several datasets and code repositories that help other researchers or practitioners benchmark their work against the new metrics produced by this project. These open data and code resources advance the US scientific research infrastructure and improve the efficiency and replicability of future research in this area. Lastly, this project produced several archival publications documenting the above outcomes such that future researchers or practioners can build upon or leverage this project's outcomes.       Last Modified: 02/03/2021       Submitted by: Mark Fuge]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
