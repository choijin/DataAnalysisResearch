<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF:Small: Continuous Perspectives on Accelerated Methods for Combinatorial Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>497504.00</AwardTotalIntnAmount>
<AwardAmount>497504</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration>&lt;br/&gt;Efficient algorithms are at the heart of any modern computing systems. In the classical study of algorithms, the notion of efficiency has long been taken to equal polynomial running time in the size of the input. However, the recent rise of massive datasets, for which even quadratic running times may be practically infeasible, has led to a rethinking of this assumption. This effort has led to a number of breakthroughs on fundamental problems, such as computing the maximum flow through a network, which can now be solved in essentially linear time in many cases. These advances are largely based on novel algorithmic tools stemming from convex and numerical optimization, which heavily rely on continuous mathematics. Based on this paradigm, the PIs aim to provide improved algorithms for two classes of problems that are important both in theory and practice: maximum flow problems and submodular optimization problems. These problems are central to computer science and have wide applicability to many real-life problems. The project will support and train two Ph.D. students and a postdoc in algorithm design and optimization at Boston University. The proposed research requires a useful exchange of ideas between theoretical computer science and continuous optimization, strengthening existing ties as well as forging new connections between the two areas. The continuous viewpoint provides a new perspective on algorithm design that the PIs plan to disseminate broadly and to incorporate into their optimization courses at Boston University.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;In this project, the PIs will delve deeply into the connection between efficient discrete optimization and continuous mathematics by considering the following continuous approach to algorithm design: algorithms are initially conceived as continuous trajectories through the space of solutions to the problems, e.g., as described by a set of differential equations; these trajectories are discretized to yield true discrete-time algorithms that are provably fast. We plan to exploit the new understanding of algorithms obtained through this framework to provide improved algorithms for two classes of problems that are important both in theory and practice: maximum flow problems and submodular optimization problems. The main technical focus of the project will be understanding and leveraging the idea of "acceleration", which plays a central role in convex optimization, as it yields optimal gradient-descent algorithms for a large class of functions. A very recent line of work initiated the study of accelerated methods from a continuous-time perspective using ordinary differential equations (ODEs) and classical discretization tools. More precisely, these works aim to represent a class of accelerated methods via ODEs whose dynamics describe the continuous-time limits of the discrete algorithms, while the discrete algorithms can be interpreted as appropriate discretizations of the continuous-time curves described by the ODEs.&lt;br/&gt;&lt;br/&gt;The project will build on this recently established viewpoint to make progress on central combinatorial optimization problems involving graphs and submodular functions. In particular, the project aims to exploit the rich combinatorial structure present in these problems to construct improved discretization procedures for known dynamical systems corresponding to accelerated algorithms. Specific problems of interest include: (a) Maximum s-t flows and connectivity problems in graphs and networks - The emphasis will be on leveraging convex optimization techniques such as acceleration in order to obtain faster approximate solutions for these problems in undirected and directed graphs. (b) Constrained submodular maximization problems - A particular area of focus will be on designing new continuous algorithms and discretization for solving known continuous relaxations of submodular objectives under structured constraints, such as packing and covering constraints.&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>06/22/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/22/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1718342</AwardID>
<Investigator>
<FirstName>Lorenzo</FirstName>
<LastName>Orecchia</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lorenzo Orecchia</PI_FULL_NAME>
<EmailAddress>orecchia@uchicago.edu</EmailAddress>
<PI_PHON>5103882071</PI_PHON>
<NSF_ID>000572818</NSF_ID>
<StartDate>06/22/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alina</FirstName>
<LastName>Ene</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alina Ene</PI_FULL_NAME>
<EmailAddress>aene@bu.edu</EmailAddress>
<PI_PHON>6173538919</PI_PHON>
<NSF_ID>000727264</NSF_ID>
<StartDate>06/22/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~497504</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project led to the development of novel algorithmic frameworks for discrete and continuous optimization. On the continuous optimization front, the novel techniques and associated algorithms developed as part of this award have significantly advanced the understanding of accelerated methods, an important class of first-order methods that provide optimal convergence guarantees for smooth convex optimization problems, i.e., problems involving the mininimization of smooth (continuously differentiable) convex functions. Many practically important problems, such as regression problems, belong to this class. Moreover, techniques inspired by accelerated methods are of great importance beyond the convex case, as they are often applied to speed up the solution of non-convex problems in machine learning applications, such as deep learning. Specifically, the novel approximate-duality-gap framework gives a simple, unified explanation of accelerated methods and standard first-order methods. It shows that all such methods are based on iteratively constructing a primal-dual pair of solutions, i.e., a primal solution to the original problem and a dual solution certifying a lower bound on the value of optimum. All methods can be methodically derived by specifying the form of such primal-dual pair, which is an immediate consequence of the problem assumptions, and by driving the gap between values of primal and dual solutions to 0. The benefits of this new insights are already evident from related results funded by this award, including novel algorithms for acceleration in the presence of noise and improved methods for resource allocation problems. Beyond the time frame of this award, the approximate-duality-gap framework provides a systematic foundation to connect algorithm design within theoretical computer science to the study of natural continuous-time dynamical systems and to the calculus of variations.</p> <p>On the discrete optimization front, the project led to the development of very efficient sequential and parallel algorithms for maximizing submodular functions, a broad class of discrete functions that capture the key diminishing returns property of utility functions in Economics, joint entropy functions in information theory, clustering and other objective functions in information retrieval and machine learning. The parallel algorithms developed achieve exponential speedups in parallel running time over existing methods. The algorithms smoothly interpolate between the linear and more general submodular objectives. In practice, where the objective does not exhibit worst-case submodular behavior, this interpolation leads to further improved performance. The sequential algorithms make progress toward achieving very fast and practical running times that scale nearly-linearly with the size of the input data. The project also tackled the problem of finding a maximum flow in a graph, a fundamental graph optimization problem that is a building block in many applications. The project led to the development of a faster algorithm for finding a minimum cost flow in a graph. The aforementioned results combine the combinatorial structure with techniques from the field of continuous optimization, such as continuous relaxations, gradient descent primitives, and differential equations. The connections between discrete and continuous optimization established have the potential to lead to further progress in theoretical computer science, machine learning, and beyond.</p><br> <p>            Last Modified: 12/29/2020<br>      Modified by: Alina&nbsp;Ene</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project led to the development of novel algorithmic frameworks for discrete and continuous optimization. On the continuous optimization front, the novel techniques and associated algorithms developed as part of this award have significantly advanced the understanding of accelerated methods, an important class of first-order methods that provide optimal convergence guarantees for smooth convex optimization problems, i.e., problems involving the mininimization of smooth (continuously differentiable) convex functions. Many practically important problems, such as regression problems, belong to this class. Moreover, techniques inspired by accelerated methods are of great importance beyond the convex case, as they are often applied to speed up the solution of non-convex problems in machine learning applications, such as deep learning. Specifically, the novel approximate-duality-gap framework gives a simple, unified explanation of accelerated methods and standard first-order methods. It shows that all such methods are based on iteratively constructing a primal-dual pair of solutions, i.e., a primal solution to the original problem and a dual solution certifying a lower bound on the value of optimum. All methods can be methodically derived by specifying the form of such primal-dual pair, which is an immediate consequence of the problem assumptions, and by driving the gap between values of primal and dual solutions to 0. The benefits of this new insights are already evident from related results funded by this award, including novel algorithms for acceleration in the presence of noise and improved methods for resource allocation problems. Beyond the time frame of this award, the approximate-duality-gap framework provides a systematic foundation to connect algorithm design within theoretical computer science to the study of natural continuous-time dynamical systems and to the calculus of variations.  On the discrete optimization front, the project led to the development of very efficient sequential and parallel algorithms for maximizing submodular functions, a broad class of discrete functions that capture the key diminishing returns property of utility functions in Economics, joint entropy functions in information theory, clustering and other objective functions in information retrieval and machine learning. The parallel algorithms developed achieve exponential speedups in parallel running time over existing methods. The algorithms smoothly interpolate between the linear and more general submodular objectives. In practice, where the objective does not exhibit worst-case submodular behavior, this interpolation leads to further improved performance. The sequential algorithms make progress toward achieving very fast and practical running times that scale nearly-linearly with the size of the input data. The project also tackled the problem of finding a maximum flow in a graph, a fundamental graph optimization problem that is a building block in many applications. The project led to the development of a faster algorithm for finding a minimum cost flow in a graph. The aforementioned results combine the combinatorial structure with techniques from the field of continuous optimization, such as continuous relaxations, gradient descent primitives, and differential equations. The connections between discrete and continuous optimization established have the potential to lead to further progress in theoretical computer science, machine learning, and beyond.       Last Modified: 12/29/2020       Submitted by: Alina Ene]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
