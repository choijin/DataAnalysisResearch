<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>PFI: AIR-TT: Commercializing a new genre of Intelligent Science Stations for informal and formal learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2017</AwardEffectiveDate>
<AwardExpirationDate>01/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>216000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jesus Soriano Molla</SignBlockName>
<PO_EMAI>jsoriano@nsf.gov</PO_EMAI>
<PO_PHON>7032927795</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This PFI: AIR Technology Translation project aims to build and commercialize a reusable mixed-reality platform that engages children in scientific experimentation and uses computer vision and automated feedback to help them effectively learn scientific principles. The project focuses on translating prior science and technology research that produced the first Intelligent Science Station, called EarthShake.  EarthShake is a mixed-reality educational game in which children discover physical properties of balance by running experiments with block towers on a simulated earthquake table. This project has the potential to have substantial societal, commercial and educational impacts. It aims to impact millions of children and families that spend time in Children's and Science Museums, libraries, schools, and indoor play spaces. It will transform traditional hands-on exhibits and play spaces into intelligent learning environments that foster children's curiosity and improve their learning in an engaging and collaborative way. It will also help to transform the traditional educational system in schools, introducing an interactive, engaging, mixed-reality system that can help students understand the underlying evidence for scientific principles, not just memorize them.  With over 270 Children's and Science Museums in the US alone, over 800 malls with play spaces for children, 100s of indoor play/learning spaces, over 4,000 clubs and after-school programs, 119,487 libraries, 36,767 private and charter elementary schools and 103,460 elementary schools, this project can have a commercial impact in addition to societal and educational benefit.&lt;br/&gt;&lt;br/&gt;The system uses computer vision to follow children's progress and provide intelligent guidance in a predict-observe-explain scientific inquiry process. Experiments demonstrate that grades K to 3 children learn much more from EarthShake than from an otherwise identical flatscreen tablet or laptop implementation showing as much as a 5 times greater pre to post increase on tests of prediction, explanation, and stable tower building.  The initial prototype will be translated into a reusable platform for Intelligent Science Stations and demonstrated in scalable development of multiple instances of three games, EarthShake, RaceCars, and BalloonScale. The technical challenges are to generalize the vision algorithm to work across the variety of objects and configurations in these games and create modular hardware components that provide for scale in development, distribution, and new products. This project draws on technical expertise in computer vision/AI, mechatronics, tangible interfaces, physical computing, human/child-computer interaction, and educational technology.&lt;br/&gt;&lt;br/&gt;Intelligent Science Stations bridge the advantages of physical and virtual worlds to improve children's science and inquiry learning in an enjoyable and collaborative way and can be commercialized to do so in various settings, indoor play spaces, museums, libraries, and schools. The project contributes to the general robotics challenge by creating an automated intelligent tutoring system operating in the physical 3D world, incorporating sensors and perceptual algorithms to track what learners are doing, actuators to manipulate the scientific apparatus, and an intelligent dialogue system to orchestrate interaction.  It addresses technical challenges of creating a generalizable vision algorithm and a modular physical hardware platform that allows customers to easily switch between different applications. It will create new knowledge about how to develop a general and reusable mixed-reality platform that integrates scientific apparatus including mechatronic components (e.g,. sensors, actuators, servo motors, physical experimental set-up) with an intelligent tracking system (including hardware and software components with an AI vision algorithm).</AbstractNarration>
<MinAmdLetterDate>07/17/2017</MinAmdLetterDate>
<MaxAmdLetterDate>02/25/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1701107</AwardID>
<Investigator>
<FirstName>Ken</FirstName>
<LastName>Koedinger</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ken Koedinger</PI_FULL_NAME>
<EmailAddress>Koedinger@cmu.edu</EmailAddress>
<PI_PHON>4122687667</PI_PHON>
<NSF_ID>000333626</NSF_ID>
<StartDate>07/17/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nesra</FirstName>
<LastName>Yannier</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nesra Yannier</PI_FULL_NAME>
<EmailAddress>nyannier@andrew.cmu.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000734303</NSF_ID>
<StartDate>07/17/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1662</Code>
<Text>PFI-Partnrships for Innovation</Text>
</ProgramElement>
<ProgramElement>
<Code>8019</Code>
<Text>Accelerating Innovation Rsrch</Text>
</ProgramElement>
<ProgramReference>
<Code>116E</Code>
<Text>RESEARCH EXP FOR UNDERGRADS</Text>
</ProgramReference>
<ProgramReference>
<Code>1662</Code>
<Text>PARTNRSHIPS FOR INNOVATION-PFI</Text>
</ProgramReference>
<ProgramReference>
<Code>8019</Code>
<Text>Accelerating Innovation Rsrch</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~200000</FUND_OBLG>
<FUND_OBLG>2019~16000</FUND_OBLG>
</Award>
</rootTag>
