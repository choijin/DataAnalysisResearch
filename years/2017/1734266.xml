<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: FND: Scene Understanding and Predictive Monitoring for Safe Human-Robot Collaboration in Unstructured and Dynamic Construction Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>750000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The construction industry has the highest number of fatalities and injuries due to hazardous working conditions. The introduction of robots on construction sites has the potential to relieve human workers from dangerous and repetitive tasks by making machines intelligent and autonomous. However, robotic solutions for construction face significant challenges. This project will develop technologies of automated monitoring and intervention through computer vision to provide a means to dramatically improve the perception of construction safety in the presence of co-robots. The new methods developed in this project will impact computer vision, machine learning, and effective human-robot collaboration in unstructured environments, while significantly contributing to safety. Further, the developed methodologies can be broadly applicable in situations where robots are deployed in human-centered environments (hospitals, airports, shipyards, etc.) and have other priorities such as productivity and efficiency as their objective. This project will engage a diverse group of individuals by training graduate and undergraduate students (including women and underrepresented minorities), reaching out to K-12 students, and interacting with industry professionals for broad dissemination of the research results.&lt;br/&gt;&lt;br/&gt;This research will investigate new computer vision based methods that can be coupled with other sensing modalities for holistic understanding and predictive analysis of jobsite safety on co-robotic construction sites. The project will consist of two main research thrusts. First, holistic scene understanding will be pursued on construction sites using graphical models to enable joint reasoning of various scene components. This holistic understanding in turn will help evaluate compliance with established safety rules expressed as formal statements. Second, predictive analysis will be investigated by exploiting the fact that, for safety intervention, the complex dynamics of a construction scene make it necessary to simulate what will happen next. In particular, Recurrent Neural Networks will be leveraged to predict future events and prevent impending accidents. Finally, an integrated demonstration system will be built and tested on real construction sites.</AbstractNarration>
<MinAmdLetterDate>07/27/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/15/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1734266</AwardID>
<Investigator>
<FirstName>Vineet</FirstName>
<LastName>Kamat</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vineet R Kamat</PI_FULL_NAME>
<EmailAddress>vkamat@umich.edu</EmailAddress>
<PI_PHON>7347644325</PI_PHON>
<NSF_ID>000244987</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>SangHyun</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>SangHyun Lee</PI_FULL_NAME>
<EmailAddress>shdpm@umich.edu</EmailAddress>
<PI_PHON>7347641817</PI_PHON>
<NSF_ID>000573145</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jia</FirstName>
<LastName>Deng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jia Deng</PI_FULL_NAME>
<EmailAddress>jiadeng@princeton.edu</EmailAddress>
<PI_PHON>6092581203</PI_PHON>
<NSF_ID>000662553</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName>Ann Arbor</CityName>
<StateCode>MI</StateCode>
<ZipCode>481091274</ZipCode>
<StreetAddress><![CDATA[2350 Hayward St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~750000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Body">The construction industry delivers or rehabilitates infrastructure, industrial plants, commercial buildings, and housing, which are vital for our society. However, construction has not only remained among the least productive industrial sectors but also suffered a significant number of fatalities. Labor-productivity growth in the construction industry has averaged only 1% a year since 1995, which is less than a third of the growth seen by an industry like manufacturing. Conversely, the construction industry has reported more than 60,000 fatal occupational injuries each year worldwide, accounting for approximately 17% of the total global occupational fatalities. Meanwhile, the construction workforce is aging, and the industry suffers from a lack of new skilled young laborers. To counteract these negative trends, the construction industry is gradually gearing up to employ co-robotic construction, an important aspect of which is human-robot collaboration. Robots can carry out repetitive and physically demanding tasks quickly while workers can instead focus on supervising the robots dealing with or accounting for uncertainties. The synergy enabled by cohesive human-robot collaboration is expected to be a key driver to improve productivity and worker safety. Promising as co-robotic construction may be, we must first focus on preventing potential safety problems caused by robots to build a productive climate for human-robot collaboration. Unfortunately, existing safety practices are not adequate for extensive use of robots on construction sites. Manual observation is so time-consuming and labor-intensive. Several sensing technologies have been used in the past, but their applications are limited due to their heavy reliance on infrastructure. These shortcomings highlight the need for a new means of effective safety monitoring and intervention to move forward with co-robotic construction.</p> <p class="Body">To address it, we devised vision-based safety monitoring and intervention method which leverages computer vision, deep learning, and multimodal data fusion. Our method can monitor a jobsite continuously using multisource data streamed from various sources&mdash;surveillance cameras, ordinary camera-mounted unmanned aerial vehicles (UAVs), and robots&rsquo; kinematic sensors&mdash;and then detect robotic hazards using holistic and predictive scene understanding models. The holistic scene understanding model can detect robotic hazards with high fidelity by analyzing not only an individual entity&rsquo;s spatial attributes (e.g., location and pose) but also the situational context shared by multiple entities (e.g., proximity and relation). Additionally, the predictive safety analysis feature further enhances the model&rsquo;s capabilities by enabling it to not only detect current hazards but also predict ones that will happen within the next five seconds. The predictive hazard detection feature allows for proactive intervention for both robots and workers, and thus has far-reaching potential for accident prevention. The information provided by this method enables robots to take evasive actions (e.g. path re-routing and movement adjustments) beforehand, thereby reducing the chance of potential near misses. Furthermore, this system can also alert workers via a simple wearable device (e.g., smartwatch) to help them avoid potential hazards with more time and ease.&nbsp;&nbsp;&nbsp;</p> <p class="Body">We tested our method at real construction sites, where the holistic and predictive scene understanding models demonstrated its validity with promising accuracy. Our method showed high fidelity for tracking individual entities, recording 97% mean Average Precision for object detection and around 4 cm of Euclidean distance error for 3D machine pose estimation. It also displayed high performance in holistic scene understanding with an average proximity error of 0.3 m and relation classification recall of 93% with a classification score post-processing. Lastly, our method was able to attain predictive functionality for hazard detection at high accuracy, achieving a proximity prediction error of 0.9 m for prediction over the next five seconds. Combining all these features in one integrated framework, our method proved its capability to detect robotic hazards (e.g., contact-driven hazards) at high fidelity, with more than 90% accuracy.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p> <p class="Body">Our holistic and predictive scene understanding methods will enable proactive identification of potential robotic hazards, thereby making timely intervention possible. This will contribute to ensuring worker safety during collaborative tasks with robots as well as promoting human-robot team cohesion, which lays the technical foundation for safe and productive human-robot collaboration. This will in turn eventually help the construction industry successfully deliver needed infrastructures and buildings in a timely manner and with improved productivity and safety. The findings are also transferable to robotics in other fields. Nowadays, industries seek to embrace robots for more challenging tasks in a variety of difficult environments, such as customized assembly and military missions in adversarial conditions. A robot&rsquo;s mission may vary according to its field and objective, but it is now a common requirement that robots understand a scene holistically and predictively so that they can collaborate cohesively and safely with humans. Our method can remotely help a robot understand their evolving surroundings and take better actions in the midst of ever-changing conditions. Therefore, our method has the potential to impact the development of adaptive and collaborative robots across diverse workspaces.</p> <p class="Body">&nbsp;</p><br> <p>            Last Modified: 11/11/2020<br>      Modified by: Sanghyun&nbsp;Lee</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The construction industry delivers or rehabilitates infrastructure, industrial plants, commercial buildings, and housing, which are vital for our society. However, construction has not only remained among the least productive industrial sectors but also suffered a significant number of fatalities. Labor-productivity growth in the construction industry has averaged only 1% a year since 1995, which is less than a third of the growth seen by an industry like manufacturing. Conversely, the construction industry has reported more than 60,000 fatal occupational injuries each year worldwide, accounting for approximately 17% of the total global occupational fatalities. Meanwhile, the construction workforce is aging, and the industry suffers from a lack of new skilled young laborers. To counteract these negative trends, the construction industry is gradually gearing up to employ co-robotic construction, an important aspect of which is human-robot collaboration. Robots can carry out repetitive and physically demanding tasks quickly while workers can instead focus on supervising the robots dealing with or accounting for uncertainties. The synergy enabled by cohesive human-robot collaboration is expected to be a key driver to improve productivity and worker safety. Promising as co-robotic construction may be, we must first focus on preventing potential safety problems caused by robots to build a productive climate for human-robot collaboration. Unfortunately, existing safety practices are not adequate for extensive use of robots on construction sites. Manual observation is so time-consuming and labor-intensive. Several sensing technologies have been used in the past, but their applications are limited due to their heavy reliance on infrastructure. These shortcomings highlight the need for a new means of effective safety monitoring and intervention to move forward with co-robotic construction. To address it, we devised vision-based safety monitoring and intervention method which leverages computer vision, deep learning, and multimodal data fusion. Our method can monitor a jobsite continuously using multisource data streamed from various sources&mdash;surveillance cameras, ordinary camera-mounted unmanned aerial vehicles (UAVs), and robots’ kinematic sensors&mdash;and then detect robotic hazards using holistic and predictive scene understanding models. The holistic scene understanding model can detect robotic hazards with high fidelity by analyzing not only an individual entity’s spatial attributes (e.g., location and pose) but also the situational context shared by multiple entities (e.g., proximity and relation). Additionally, the predictive safety analysis feature further enhances the model’s capabilities by enabling it to not only detect current hazards but also predict ones that will happen within the next five seconds. The predictive hazard detection feature allows for proactive intervention for both robots and workers, and thus has far-reaching potential for accident prevention. The information provided by this method enables robots to take evasive actions (e.g. path re-routing and movement adjustments) beforehand, thereby reducing the chance of potential near misses. Furthermore, this system can also alert workers via a simple wearable device (e.g., smartwatch) to help them avoid potential hazards with more time and ease.    We tested our method at real construction sites, where the holistic and predictive scene understanding models demonstrated its validity with promising accuracy. Our method showed high fidelity for tracking individual entities, recording 97% mean Average Precision for object detection and around 4 cm of Euclidean distance error for 3D machine pose estimation. It also displayed high performance in holistic scene understanding with an average proximity error of 0.3 m and relation classification recall of 93% with a classification score post-processing. Lastly, our method was able to attain predictive functionality for hazard detection at high accuracy, achieving a proximity prediction error of 0.9 m for prediction over the next five seconds. Combining all these features in one integrated framework, our method proved its capability to detect robotic hazards (e.g., contact-driven hazards) at high fidelity, with more than 90% accuracy.      Our holistic and predictive scene understanding methods will enable proactive identification of potential robotic hazards, thereby making timely intervention possible. This will contribute to ensuring worker safety during collaborative tasks with robots as well as promoting human-robot team cohesion, which lays the technical foundation for safe and productive human-robot collaboration. This will in turn eventually help the construction industry successfully deliver needed infrastructures and buildings in a timely manner and with improved productivity and safety. The findings are also transferable to robotics in other fields. Nowadays, industries seek to embrace robots for more challenging tasks in a variety of difficult environments, such as customized assembly and military missions in adversarial conditions. A robot’s mission may vary according to its field and objective, but it is now a common requirement that robots understand a scene holistically and predictively so that they can collaborate cohesively and safely with humans. Our method can remotely help a robot understand their evolving surroundings and take better actions in the midst of ever-changing conditions. Therefore, our method has the potential to impact the development of adaptive and collaborative robots across diverse workspaces.         Last Modified: 11/11/2020       Submitted by: Sanghyun Lee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
