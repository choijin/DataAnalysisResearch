<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Evaluating the Feasibility of Estimating Cognitive Load Using Microsaccadic Eye Motion and Oscillation in Pupil Diameter</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>124977.00</AwardTotalIntnAmount>
<AwardAmount>124977</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balakrishnan Prabhakaran</SignBlockName>
<PO_EMAI>bprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032924847</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Systems that can detect and respond to their users' workload have the potential to improve both users' experiences and outcomes in many domains: students and teachers, drivers and pilots, rescue workers and soldiers might all benefit from systems that can detect when their jobs are too hard or easy and dynamically adapt the difficulty.  Key to this promise is the ability to accurately estimate a person's workload without distracting them from their tasks.  A promising, non-invasive approach is to estimate workload from eye movements, which are both known to correlate with cognitive activity and likely to become more widely available to computer systems as gaze tracking tools are developed for both commercial-grade web cameras and for devices that support augmented and virtual reality interfaces.  However, commonly-proposed metrics for estimating workload, notably pupil diameter, have severe practical limitations around head motion, ambient light, and camera angle.  Using a high-speed eye tracker and commonly used cognitive load and visual search tasks, this project will develop mathematical processing techniques to translate tiny eye movements ("micorsaccades") and pupil oscillation into workload estimates, then study the feasibility and accuracy of using those measures relative to pupil diameter and other ways of estimating workload.  The work will take place in conjunction with several international partners, provide a student with international research experience, and help inform a future workshop on ubiquitous gaze sensing.&lt;br/&gt;&lt;br/&gt;The team will first develop and refine multiple candidate measures of cognitive load that focus on pupil motion and change.  The first, Change in Pupil Diameter (relative to a baseline), represents commonly-proposed approaches.  The second, the Index of Pupillary Activity, is based on a prior measure called the Index of Cognitive Activity derived from the oscillation frequency of pupil diameter ("pupil unrest").  It is likely to be better than raw pupil diameter, but still suffers from drawbacks around ambient light and off-axis distortion.  The other is a technique based on microsaccadic activity that can be extracted from eye gaze positions rather than pupil diameter, ideally eliminating the drawbacks to diameter-based metrics.  The team has already shown that their current versions of these metrics show promise in fixed-gaze tasks in which participants are asked to focus on a single, centralized fixation point while performing more and less difficult metal calculations.  The next step is to generalize this work to fixed gaze at arbitrary angles, first by adapting the metrics to include techniques for off-axis pupil diameter compensation, then conducting similar workload tasks but where participants focus on one of a number of gaze targets located around the screen.  This experiment to show that gaze can be shifted and off-center then sets up more realistic, unrestricted visual search tasks in which the team will ask participants to locate elevation-based terrain features in simulated maps that allow the team to control the location and workload involved in the search task.</AbstractNarration>
<MinAmdLetterDate>07/31/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/10/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1748380</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Duchowski</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew T Duchowski</PI_FULL_NAME>
<EmailAddress>duchowski@clemson.edu</EmailAddress>
<PI_PHON>8646567677</PI_PHON>
<NSF_ID>000492350</NSF_ID>
<StartDate>07/31/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Clemson University</Name>
<CityName>CLEMSON</CityName>
<ZipCode>296345701</ZipCode>
<PhoneNumber>8646562424</PhoneNumber>
<StreetAddress>230 Kappa Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042629816</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CLEMSON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042629816</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Clemson University]]></Name>
<CityName>Clemson</CityName>
<StateCode>SC</StateCode>
<ZipCode>296340001</ZipCode>
<StreetAddress><![CDATA[100 McAdams Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~124977</FUND_OBLG>
</Award>
</rootTag>
