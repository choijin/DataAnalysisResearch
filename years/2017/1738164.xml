<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps: Smart Speech Perception Feedback for Training and Diagnostics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2017</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anita La Salle</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this I-Corps project is to provide technologies and services based on models of speech signals and human speech perception. These models incorporate a deep understanding of the physical speech stimulus and the perceptual representations that drive speech perception.  Technology applications to be explored can break new ground in areas such as: (1) Improving the efficacy of audiological testing; (2) Improving the realism of computer-synthesized voices (3) Providing speech perception training for better understanding of non-native speakers or speakers with neurological disorders; (4) Enhancing multisensory speech perception through training of individuals with audiovisual speech processing deficits such as individuals on the autism spectrum; (5) Developing sensory substitution or augmentation through vibrotactile stimuli; and (6) Improving the quality of older adults? lives through visual speech perception training to ameliorate their declines in perceiving auditory speech in noisy backgrounds.&lt;br/&gt;&lt;br/&gt;This I-Corps project will investigate commercial applications of technologies and services based on models of multisensory speech signals and human speech perception.  Human speech perception can be highly errorful in the presence of noise and/or distortions that originate in the talker, the physical communication channel, and/or the perceiver (e.g., hearing loss). The technology to be explored models how speech perception degrades, and how multisensory stimuli compensate for perceptual errors. For example, this technology supports the characterization of speech perception difficulties using a simple talk-back task in which the perceiver repeats what was just said. In formal laboratory experiments, this technology has been used to improve speech perception training outcomes through improvements in the contingencies between perceptual errors and training feedback. This technology can be used in developing media that are designed to reduce the potential for perceptual errors.</AbstractNarration>
<MinAmdLetterDate>03/28/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/28/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1738164</AwardID>
<Investigator>
<FirstName>Silvio</FirstName>
<LastName>Eberhardt</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Silvio P Eberhardt</PI_FULL_NAME>
<EmailAddress>silvio@gwu.edu</EmailAddress>
<PI_PHON>4845576593</PI_PHON>
<NSF_ID>000575821</NSF_ID>
<StartDate>03/28/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lynne</FirstName>
<LastName>Bernstein</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lynne E Bernstein</PI_FULL_NAME>
<EmailAddress>lbernste@gwu.edu</EmailAddress>
<PI_PHON>2029947403</PI_PHON>
<NSF_ID>000575826</NSF_ID>
<StartDate>03/28/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043990498</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520086</ZipCode>
<StreetAddress><![CDATA[810 22nd Street, NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We participated in the Boston cohort of the NSF Innovation Corps (I-Corps) in the spring of 2017. Our team members were: Lynne E. Bernstein and Edward T. Auer, co-Principal Investigators (PIs); Silvio P. Eberhardt, Entrepreneurial Lead (EL); and Gregory Miller, Industry Mentor. Our goal was to explore product-market fit for applications of tools we developed for multimodal speech perception characterization, training, and testing. These tools can be used to evaluate speech perception errors during connected speech at the level of phonemes (i.e., consonants and vowels) rather than the conventional word level. Such fine-grained characterization of perception can be used to improve diagnosis of problems with perception, particularly in the context of hearing loss, and can drive substantial improvements to the effectiveness of speech perception training systems. Among promising commercial applications for our technology that we explored were: 1) Lipreading training for hearing-impaired individuals to improve their audiovisual speech understanding in noisy social situations; 2) Audiological tests that provide higher information yield to improve client outcomes; 3) Animation of more realistic &ldquo;talking faces;&rdquo; and 4) Training to improve speech production in a foreign language.</p> <p>Initial interviews, including with animators and researchers in affective computing, led us to focus on Applications 1 and 2 above. Over 110 subsequent interviews that we conducted were mostly with audiologists and other professionals in the hearing-health ecosystems, as well as persons with hearing loss (and their spouses). We learned that while we do have a good product-market fit for lipreading training, many people with hearing loss are in denial about their hearing for many years. While denying that they have a problem they are unlikely to engage with training systems. When they &ldquo;take charge&rdquo; of their hearing loss, usually by visiting an audiologist, they will be more likely to use our system if the audiologist recommends or offers it. Therefore, we concluded that at least two market segments will need to be developed, adults with hearing loss and audiologists. But audiologists need scientific proof in order to recommend interventions, so our commercialization plan includes continuing to carry out rigorous scientific research. A further advantage to developing the audiologist market segment is that it is also the market segment for the second application of our technologies, that is, improved audiological tests.</p> <p>Since participating in I-Corps training, our team has received two Phase I SBIR awards from NIH/NIDCD. The first SBIR funded the development of a web-based lipreading training system that is currently being evaluated by older individuals with hearing loss. The second SBIR was used to modify standard audiological tests in order to collect data from speech audiometry tests that use words and sentences that are mixed with noise. We are applying our technology to the data from a hearing clinic in order to determine whether measures that we extract with our technology can be used to improve hearing-health outcomes for clients with hearing loss.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/30/2018<br>      Modified by: Lynne&nbsp;E&nbsp;Bernstein</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We participated in the Boston cohort of the NSF Innovation Corps (I-Corps) in the spring of 2017. Our team members were: Lynne E. Bernstein and Edward T. Auer, co-Principal Investigators (PIs); Silvio P. Eberhardt, Entrepreneurial Lead (EL); and Gregory Miller, Industry Mentor. Our goal was to explore product-market fit for applications of tools we developed for multimodal speech perception characterization, training, and testing. These tools can be used to evaluate speech perception errors during connected speech at the level of phonemes (i.e., consonants and vowels) rather than the conventional word level. Such fine-grained characterization of perception can be used to improve diagnosis of problems with perception, particularly in the context of hearing loss, and can drive substantial improvements to the effectiveness of speech perception training systems. Among promising commercial applications for our technology that we explored were: 1) Lipreading training for hearing-impaired individuals to improve their audiovisual speech understanding in noisy social situations; 2) Audiological tests that provide higher information yield to improve client outcomes; 3) Animation of more realistic "talking faces;" and 4) Training to improve speech production in a foreign language.  Initial interviews, including with animators and researchers in affective computing, led us to focus on Applications 1 and 2 above. Over 110 subsequent interviews that we conducted were mostly with audiologists and other professionals in the hearing-health ecosystems, as well as persons with hearing loss (and their spouses). We learned that while we do have a good product-market fit for lipreading training, many people with hearing loss are in denial about their hearing for many years. While denying that they have a problem they are unlikely to engage with training systems. When they "take charge" of their hearing loss, usually by visiting an audiologist, they will be more likely to use our system if the audiologist recommends or offers it. Therefore, we concluded that at least two market segments will need to be developed, adults with hearing loss and audiologists. But audiologists need scientific proof in order to recommend interventions, so our commercialization plan includes continuing to carry out rigorous scientific research. A further advantage to developing the audiologist market segment is that it is also the market segment for the second application of our technologies, that is, improved audiological tests.  Since participating in I-Corps training, our team has received two Phase I SBIR awards from NIH/NIDCD. The first SBIR funded the development of a web-based lipreading training system that is currently being evaluated by older individuals with hearing loss. The second SBIR was used to modify standard audiological tests in order to collect data from speech audiometry tests that use words and sentences that are mixed with noise. We are applying our technology to the data from a hearing clinic in order to determine whether measures that we extract with our technology can be used to improve hearing-health outcomes for clients with hearing loss.             Last Modified: 12/30/2018       Submitted by: Lynne E Bernstein]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
