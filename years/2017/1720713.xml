<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Collaborative Research: Modeling Social Context to Improve Human-Robot Interaction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/16/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>161250.00</AwardTotalIntnAmount>
<AwardAmount>161250</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>For robots to be truly useful to people, it is critical that they be able to understand and operate independently in human social environments (HSEs).  Decades of research into this problem by many investigators have to date failed to yield a solution.  The PIs on this collaborative project that spans two partner institutions argue that a fundamental paradigm shift is necessary to enable progress, and that this shift can be ignited through a contextually driven approach to mobile robotics.  Thus, the goal of this proposal is to create and evaluate new models of social context in order to enable mobile robots to interact appropriately with people in HSEs.  Project outcomes will help give all "things that think", from social robots to smart homes, a better understanding of human social context and a greater capability to operate effectively in real-world human spaces.  By contributing new theoretical models, techniques, and open source implementations that will accelerate the development and adoption of machines that operate in HSEs, the PIs anticipate this work will have a transformative impact on many fields within computer and information science, including robotics, human-machine interaction, and ubiquitous computing.  The PIs also will make their context models available to other researchers.  &lt;br/&gt;&lt;br/&gt;The PIs' approach to context is unique in that, rather than being techno-centric and entirely representational, it fully adopts Dourish's approach to an interactional model of context inspired by the social sciences - relational, dynamic, occasioned, and arising from activity - i.e., fluid.  The key contribution of the approach is that it ties real-time sensor data to models of situational context, which then inform a robot of the social affordances of the environment.  The PIs will engage in two primary research activities.  First, they will create a situational context processing capability that can accept a multimodal, social scene snapshot from a robot and return back a situational context frame (SCF) that contains an estimate of the scene's salient objects, social activities, and situational context.  Then, they will contribute algorithmic techniques that enable a robot to leverage the SCF to perform guided exploration to refine its model, and ultimately, goal-driven task execution adapted to conform to social norms.  The PIs' research emphasis on using solely naturalistic, real-world, multimodal data, will enable them to make unique contributions for increasing robustness in multimodal processing.</AbstractNarration>
<MinAmdLetterDate>07/13/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1720713</AwardID>
<Investigator>
<FirstName>Laurel</FirstName>
<LastName>Riek</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laurel D Riek</PI_FULL_NAME>
<EmailAddress>lriek@eng.ucsd.edu</EmailAddress>
<PI_PHON>8588222550</PI_PHON>
<NSF_ID>000602866</NSF_ID>
<StartDate>07/13/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~161250</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Human environments are always changing - spaces are repurposed, objects are rearranged, and behaviors are altered. Humans are very skilled at perceiving and adapting to these changes in a dynamic and robust way; however, machines are very poor at this. In order to create&nbsp; future systems which can help people, such as assistive robots, it is important they can sense, adapt to, learn from, and team with humans. This project focused on developing new, biologically-inspired methods for robots to enable them to perceive and understand human environments. We focused on two key research themes: 1) learning to recognize previously unseen objects (e.g., something a robot has never seen before), and 2) sensing and understanding human groups in the real world.&nbsp;</span></p> <p><span>&nbsp;</span></p> <p><span>1) We developed an unsupervised, unseen object detection method that enabled robots to learn about unknown or unseen objects. In contrast to the dominant paradigm in the literature, our method does not rely on prior knowledge of known objects. Instead, our method is able to detect unseen objects using bottom-up perceptual cues similar to how the human perceptual system works. Compared to other unsupervised object detection algorithms, we found that our method has comparable detection accuracy, while being up to 20 times faster than the state-of-the-art. &nbsp;</span></p> <p><span>&nbsp;</span></p> <p><span>Our results showed that our method is the first (among those evaluated against) to be able to detect multiple objects simultaneously at near real-time speed (0.5s), while other methods could only detect a single object at speeds unsuitable for robotics applications (&gt;15s). To our knowledge, our method is the first unsupervised object detection algorithm to be designed specifically for robot perception, and will serve as a milestone to enable robots to be more perceptive of their surroundings, learn about objects in a scalable manner, and overall increase their autonomy in real-world environments.&nbsp;</span></p> <p><span>&nbsp;</span></p> <p><span>2) The majority of human spaces are populated by groups, where people walk, work, and interact with one another. To enable robots to fluently assist and interact with groups, robots need a high-level understanding of groups.&nbsp;</span></p> <p><span>Robots have the potential to solve many exciting technical and socio-technical challenges such as how robots can sense collocated groups, help with decision-making, and support human workflow.&nbsp;</span></p> <p><span>&nbsp;</span></p> <p><span>We have focused on developing new perception methods for human-robot teaming. We developed a method that detects groups of people in real-world environments, called the Robot-Centric Group Estimation Model (RoboGEM). RoboGEM can predict and cluster pedestrians into groups. Unlike previously proposed methods, RoboGEM can detect groups from a first-person or ego-centric perspective, while pedestrians and the robot are in motion, using an unsupervised approach. To evaluate our method, we compared it to a state-of-the-art group detection method. RoboGEM outperformed this top-performing method by 10% in terms of accuracy, and 50% in terms of recall, suggesting it can be used in real-world environments to enable robots to work in teams. Our recent work, RoboGEM 2.0, improves upon this work to support real time human group detection and tracking in crowded environments in real time.</span></p> <p><span>&nbsp;</span></p> <p><span>Finally, we contextualized this research within the context of a real-world application domain - human-robot teaming in healthcare. This is a dynamic setting in which teams experience coordination, communication, and decision making challenges, rendering it a well-suited application domain for our work. We explored how robots could be used to reduce preventable medical errors, which kill over 400,000 patients annually in hospitals alone. We found that nurses are primary patient safety advocates, and are uniquely positioned to identify and prevent such errors as they are the primary advocates for patients. However, asymmetrical power dynamics between physicians and nurses often result in penalties for nurses who speak up to "stop the line" of behavior that causes errors. We conducted a study with nurses to envision how robots could empower them in teams, with a specific analysis of the RoboGEM system. Our study revealed that nurses want robots to assist with team decision making, supply delivery, and team "choreography" during surgery and resuscitation procedures. This work was awarded a Best Paper Honorable Mention at the 2019 ACM Conference on Computer Supported Collaborative Work (CSCW).&nbsp;</span></p> <p><span>&nbsp;</span></p> <p><span>Ultimately, the results of this project will help give all "things that think", from robots to smart homes, a better understanding of human social context. We anticipate this work will have a transformative impact on many fields, such as robotics, social computing, ubiquitous computing, augmented reality, human-machine interaction, and affective computing. To make significant progress, many of these fields require models of human social contexts to enable a transition from content-focused to context-driven approaches. This research will provide those models and serve as a catalyst for this transition.&nbsp;</span></p> <div><span><br /></span></div> <p>&nbsp;</p><br> <p>            Last Modified: 04/16/2020<br>      Modified by: Laurel&nbsp;D&nbsp;Riek</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Human environments are always changing - spaces are repurposed, objects are rearranged, and behaviors are altered. Humans are very skilled at perceiving and adapting to these changes in a dynamic and robust way; however, machines are very poor at this. In order to create  future systems which can help people, such as assistive robots, it is important they can sense, adapt to, learn from, and team with humans. This project focused on developing new, biologically-inspired methods for robots to enable them to perceive and understand human environments. We focused on two key research themes: 1) learning to recognize previously unseen objects (e.g., something a robot has never seen before), and 2) sensing and understanding human groups in the real world.      1) We developed an unsupervised, unseen object detection method that enabled robots to learn about unknown or unseen objects. In contrast to the dominant paradigm in the literature, our method does not rely on prior knowledge of known objects. Instead, our method is able to detect unseen objects using bottom-up perceptual cues similar to how the human perceptual system works. Compared to other unsupervised object detection algorithms, we found that our method has comparable detection accuracy, while being up to 20 times faster than the state-of-the-art.       Our results showed that our method is the first (among those evaluated against) to be able to detect multiple objects simultaneously at near real-time speed (0.5s), while other methods could only detect a single object at speeds unsuitable for robotics applications (&gt;15s). To our knowledge, our method is the first unsupervised object detection algorithm to be designed specifically for robot perception, and will serve as a milestone to enable robots to be more perceptive of their surroundings, learn about objects in a scalable manner, and overall increase their autonomy in real-world environments.      2) The majority of human spaces are populated by groups, where people walk, work, and interact with one another. To enable robots to fluently assist and interact with groups, robots need a high-level understanding of groups.   Robots have the potential to solve many exciting technical and socio-technical challenges such as how robots can sense collocated groups, help with decision-making, and support human workflow.      We have focused on developing new perception methods for human-robot teaming. We developed a method that detects groups of people in real-world environments, called the Robot-Centric Group Estimation Model (RoboGEM). RoboGEM can predict and cluster pedestrians into groups. Unlike previously proposed methods, RoboGEM can detect groups from a first-person or ego-centric perspective, while pedestrians and the robot are in motion, using an unsupervised approach. To evaluate our method, we compared it to a state-of-the-art group detection method. RoboGEM outperformed this top-performing method by 10% in terms of accuracy, and 50% in terms of recall, suggesting it can be used in real-world environments to enable robots to work in teams. Our recent work, RoboGEM 2.0, improves upon this work to support real time human group detection and tracking in crowded environments in real time.     Finally, we contextualized this research within the context of a real-world application domain - human-robot teaming in healthcare. This is a dynamic setting in which teams experience coordination, communication, and decision making challenges, rendering it a well-suited application domain for our work. We explored how robots could be used to reduce preventable medical errors, which kill over 400,000 patients annually in hospitals alone. We found that nurses are primary patient safety advocates, and are uniquely positioned to identify and prevent such errors as they are the primary advocates for patients. However, asymmetrical power dynamics between physicians and nurses often result in penalties for nurses who speak up to "stop the line" of behavior that causes errors. We conducted a study with nurses to envision how robots could empower them in teams, with a specific analysis of the RoboGEM system. Our study revealed that nurses want robots to assist with team decision making, supply delivery, and team "choreography" during surgery and resuscitation procedures. This work was awarded a Best Paper Honorable Mention at the 2019 ACM Conference on Computer Supported Collaborative Work (CSCW).      Ultimately, the results of this project will help give all "things that think", from robots to smart homes, a better understanding of human social context. We anticipate this work will have a transformative impact on many fields, such as robotics, social computing, ubiquitous computing, augmented reality, human-machine interaction, and affective computing. To make significant progress, many of these fields require models of human social contexts to enable a transition from content-focused to context-driven approaches. This research will provide those models and serve as a catalyst for this transition.             Last Modified: 04/16/2020       Submitted by: Laurel D Riek]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
