<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Towards Learning Skills from First Person Demonstrations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2018</AwardEffectiveDate>
<AwardExpirationDate>01/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Humans learn a skill from an expert's demonstrations such as playing tennis, which requires understanding subtle details of sequential actions, e.g., eye-hand coordination across swing motion. This project develops technologies to learn such skills by observing demonstrations from first-person videos. The first-person videos are highly dynamic, local, and person-biased due to severe head movements, which generates a larger variation of visual data. Analyzing the videos produced by the head-mounted camera system is challenging because state-of-the-art computer vision systems built upon third-person videos cannot be directly applied. The research team addresses these challenges by developing both hardware and computational models. The principal investigator of the project will integrate the research results into a sequence of newly designed computer vision courses in the University of Minnesota. The research team will publicly share the dataset, representation, and trained models, and organize workshops and tutorials to broader audiences in computer vision and robotics.&lt;br/&gt;&lt;br/&gt;This research investigates problems in learning skills from first person demonstrations. This project designs a head-mounted camera system composed of a first-person camera and multiple proxemic cameras that can fully cover the space of interactions. The project also develops a new representation specific to the head-mounted camera system, called proxemic affordance map, to efficiently represent visual scene and action in 3D. The proxemic affordance map encodes 3D visual semantics in a form of 3D depth map, visual attention, and body pose, which enables measuring the correlation between action and its surroundings. This allows learning the dynamics of proxemic affordance map to model diverse physical activities, e.g., how an action will change the state of its surrounding contexts.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/30/2018</MinAmdLetterDate>
<MaxAmdLetterDate>03/30/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1755895</AwardID>
<Investigator>
<FirstName>Hyun Soo</FirstName>
<LastName>Park</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hyun Soo Park</PI_FULL_NAME>
<EmailAddress>hspark@umn.edu</EmailAddress>
<PI_PHON>6123011745</PI_PHON>
<NSF_ID>000741915</NSF_ID>
<StartDate>03/30/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 Union Street SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This research program aims to develop a visual computational model that encodes the space around a person, e.g., surrounding objects, people, and scenes, using an egocentric camera. As a proof-of-concept, my team focuses on a problem of visual navigation in a novel environment by (1) designing a representation of 3D world with semantic meaning, e.g., items in a shelf, road, and (2) developing an algorithm to leverage the representation to navigate into the scene.</span></p> <p><span>Intellectual Merit: We develop a new computer vision theories and algorithms to learn semantics and geometry of surroundings. The key innovation is to leverage multiview images that can provide visually redundant yet geometrically distinctive signals that can supervise each other's views. We have developed various algorithms to understand egocentric scenes in the form of depth, surface normal, and semantics. We used multiple geometric insight to supervise learning the visual representation: epipolar constraint, temporal consistency, and keypoint consistency. With this high-risk-high-payoff project, we have published in top conferences in computer vision including CVPR, ECCV, and ICCV and we have been conducting the follow-up research. For instance, based on the previous work, now we are building a large multiview dataset called HUMBI for human body expressions that can be used to learn view dependent appearance from assorted people. Notable papers include:</span></p> <p><span>+ T. Do, K. Vuong, S. Roumeliotis, H.S. Park "Surface Normal Estimation of Tilted Images via Spatial Rectifier", ECCV 2020, Spotlight presentation---</span>We present a spatial rectifier to estimate surface normals of tilted (egocentric) images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion.</p> <p>+ J. S. Yoon, T. Shiratori, S.-I. Yu, H. S. Park, "Self-supervised Adaptation of High-fidelity Face Models for Monocular Performance Tracking", CVPR 2019 Oral presentation---W<span>e propose a self-supervised domain adaptation approach to enable the animation of high-fidelity face models from a commodity (egocentric) camera. Our approach first circumvents the requirement for special input data by training a new network that can directly drive a face model just from a single 2D image. Then, we overcome the domain mismatch between lab and uncontrolled environments by performing self-supervised domain adaptation based on "consecutive frame texture consistency" based on the assumption that the appearance of the face is consistent over consecutive frames, avoiding the necessity of modeling the new environment such as lighting or background. Experiments show that we are able to drive a high-fidelity face model to perform complex facial motion from a cellphone camera without requiring any labeled data from the new domain.</span></p> <p>Broader Impact: The outcomes of the project are integrated in computer vision courses in the University of Minnesota: CSCI 5561 Introduction to Computer Vision and CSCI 5563 Multiview 3D Computer Vision. We used the datasets and codes generated from this project as course materials. PI has given talks in various settings including conferences, universities and industrial partners for dissemination. PI also disseminated the results in public settings in local communities such as Minnesota State Fair.&nbsp;</p> <p><span><br /></span></p><br> <p>            Last Modified: 04/07/2021<br>      Modified by: Hyun Soo&nbsp;Park</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1755895/1755895_10536977_1617807711492_Clipboard01--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1755895/1755895_10536977_1617807711492_Clipboard01--rgov-800width.jpg" title="Egocentric Scene Understanding"><img src="/por/images/Reports/POR/2021/1755895/1755895_10536977_1617807711492_Clipboard01--rgov-66x44.jpg" alt="Egocentric Scene Understanding"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We collect a new dataset of egocentric RGBD images to study egocentric scene understanding: predicting depths, surface normals, semantics from an egocentri image.</div> <div class="imageCredit">Tien Do</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Hyun Soo&nbsp;Park</div> <div class="imageTitle">Egocentric Scene Understanding</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This research program aims to develop a visual computational model that encodes the space around a person, e.g., surrounding objects, people, and scenes, using an egocentric camera. As a proof-of-concept, my team focuses on a problem of visual navigation in a novel environment by (1) designing a representation of 3D world with semantic meaning, e.g., items in a shelf, road, and (2) developing an algorithm to leverage the representation to navigate into the scene.  Intellectual Merit: We develop a new computer vision theories and algorithms to learn semantics and geometry of surroundings. The key innovation is to leverage multiview images that can provide visually redundant yet geometrically distinctive signals that can supervise each other's views. We have developed various algorithms to understand egocentric scenes in the form of depth, surface normal, and semantics. We used multiple geometric insight to supervise learning the visual representation: epipolar constraint, temporal consistency, and keypoint consistency. With this high-risk-high-payoff project, we have published in top conferences in computer vision including CVPR, ECCV, and ICCV and we have been conducting the follow-up research. For instance, based on the previous work, now we are building a large multiview dataset called HUMBI for human body expressions that can be used to learn view dependent appearance from assorted people. Notable papers include:  + T. Do, K. Vuong, S. Roumeliotis, H.S. Park "Surface Normal Estimation of Tilted Images via Spatial Rectifier", ECCV 2020, Spotlight presentation---We present a spatial rectifier to estimate surface normals of tilted (egocentric) images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion.  + J. S. Yoon, T. Shiratori, S.-I. Yu, H. S. Park, "Self-supervised Adaptation of High-fidelity Face Models for Monocular Performance Tracking", CVPR 2019 Oral presentation---We propose a self-supervised domain adaptation approach to enable the animation of high-fidelity face models from a commodity (egocentric) camera. Our approach first circumvents the requirement for special input data by training a new network that can directly drive a face model just from a single 2D image. Then, we overcome the domain mismatch between lab and uncontrolled environments by performing self-supervised domain adaptation based on "consecutive frame texture consistency" based on the assumption that the appearance of the face is consistent over consecutive frames, avoiding the necessity of modeling the new environment such as lighting or background. Experiments show that we are able to drive a high-fidelity face model to perform complex facial motion from a cellphone camera without requiring any labeled data from the new domain.  Broader Impact: The outcomes of the project are integrated in computer vision courses in the University of Minnesota: CSCI 5561 Introduction to Computer Vision and CSCI 5563 Multiview 3D Computer Vision. We used the datasets and codes generated from this project as course materials. PI has given talks in various settings including conferences, universities and industrial partners for dissemination. PI also disseminated the results in public settings in local communities such as Minnesota State Fair.           Last Modified: 04/07/2021       Submitted by: Hyun Soo Park]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
