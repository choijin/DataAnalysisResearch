<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CompCog:  Collaborative Research:  Learning Visuospatial Reasoning Skills from Experience</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Soo-Siang Lim</SignBlockName>
<PO_EMAI>slim@nsf.gov</PO_EMAI>
<PO_PHON>7032927878</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project uses methods from artificial intelligence (AI) to better understand how people learn visuospatial reasoning skills like mental rotation, which are a critical ingredient in the development of strong math and science abilities.  In particular, this project proposes a new approach to quantify the learning value contained in different visual experiences, using wearable cameras combined with a new AI system that learns visuospatial reasoning skills from video examples.  Results from this project will not only advance the state of the art in AI but also will enable researchers to measure how valuable different real-world visual experiences are in helping people to learn visuospatial reasoning skills.  For example, certain types of object play activities might be particularly valuable for helping a child to learn certain visuospatial reasoning skills.  Ultimately, this new measurement approach could be used to identify early signs of visuospatial reasoning difficulties in children and could also help in the design of new visuospatial training interventions to boost children's early math and science development.&lt;br/&gt;&lt;br/&gt;The core scientific question that this project aims to answer is: How are visuospatial reasoning skills learned from first-person visual experiences?  This question will be answered through computational experiments with a new AI system---the Mental Imagery Engine (MIME)---that learns visuospatial reasoning skills, like mental rotation, from video examples.  Training data will include first-person, wearable-camera videos from two different settings that are both important for human learning:  unstructured object manipulation by infants and visuospatial training interventions designed for children.  Results from experiments with the MIME AI system will advance the state of the art in both AI and the science of human learning by helping to explain how visuospatial reasoning skills can be learned from visual experiences, and, in particular, how having different kinds of visual experiences can affect the quality of a person's learning outcomes in different ways.</AbstractNarration>
<MinAmdLetterDate>08/16/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/16/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1730044</AwardID>
<Investigator>
<FirstName>Bethany</FirstName>
<LastName>Rittle-Johnson</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bethany M Rittle-Johnson</PI_FULL_NAME>
<EmailAddress>bethany.rittle-johnson@vanderbilt.edu</EmailAddress>
<PI_PHON>6153222631</PI_PHON>
<NSF_ID>000214507</NSF_ID>
<StartDate>08/16/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Maithilee</FirstName>
<LastName>Kunda</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maithilee Kunda</PI_FULL_NAME>
<EmailAddress>mkunda@vanderbilt.edu</EmailAddress>
<PI_PHON>6158758469</PI_PHON>
<NSF_ID>000649232</NSF_ID>
<StartDate>08/16/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Vanderbilt University</Name>
<CityName>Nashville</CityName>
<ZipCode>372350002</ZipCode>
<PhoneNumber>6153222631</PhoneNumber>
<StreetAddress>Sponsored Programs Administratio</StreetAddress>
<StreetAddress2><![CDATA[PMB 407749 2301 Vanderbilt Place]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>965717143</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VANDERBILT UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004413456</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Vanderbilt University]]></Name>
<CityName>Nashville</CityName>
<StateCode>TN</StateCode>
<ZipCode>372350002</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>004Y</Code>
<Text>Science of Learning</Text>
</ProgramElement>
<ProgramReference>
<Code>059Z</Code>
<Text>Science of Learning</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Visuospatial reasoning--how we think about objects, shapes, physical and spatial relationships, etc.--is a critical part of human intelligence.&nbsp; It affects how we think about many everyday activities like looking at maps, loading the dishwasher, fixing things, etc., as well as more abstract activities like reading books, reasoning about numbers and other mathematical concepts, and so on.&nbsp; Visuospatial reasoning is increasingly recognized as being extremely important in STEM education and careers, and we are learning more and more about its role in other educational and career settings every day.</p> <p>This project used computational modeling methods from artificial intelligence (AI) to study how people might learn visuospatial reasoning skills.&nbsp; Just like we learn language, numbers, etc., starting as infants and continuing throughout our lifetimes, we also learn how to do various kinds of visuospatial reasoning over time.</p> <p>Because visuospatial reasoning is such a broad topic, in this project we narrowed down the scope of our research to focus on visuospatial intelligence tests.&nbsp; These are tests that many people take (for example, in the context of a clinical cognitive evaluation, in schools, or as part of an IQ assessment), and these tests often ask people to reason about visual shapes and complex relationships among them.&nbsp; Our goal in this project was to find out more about the kinds of visuospatial reasoning needed to solve such tests, and how these various kinds of reasoning can be learned through experience.</p> <p>We developed a conceptual framework that highlights four different components of visuospatial reasoning: basic reasoning operations like mental rotation; the procedure or algorithm that uses these operations to solve problems; the synthesis process that creates the problem-solving algorithm; and the interpretation process that parses the incoming problem and establishes what kind of problem it is in the first place.&nbsp; We then investigated computational models for these components on various visuospatial intelligence tests, looking not only at how each component works, but how each one might be learned by an artificial agent through various types of input experiences, instead of being programmed by hand.&nbsp;</p> <p>We also looked at how these learning agents perform on the kinds of visual inputs that human infants and children receive, for example when playing with toys.&nbsp; We developed a new dataset for studying these questions, called the Toybox dataset (available online here: https://aivaslab.github.io/toybox/), and we also worked in collaboration with developmental psychologists to study examples of real-world visual inputs from infant-worn head-cameras.</p> <p>Ultimately, we expect that the broader impacts of this research include using our findings to feed into new ways to think about human visuospatial learning, especially to understand what kinds of real-life experiences are valuable for supporting these important kinds of learning, and how we can design new educational and training experiences for people to boost their visuospatial reasoning skills.</p> <p>This project also provided valuable research and education experiences for many students, including undergraduate students in computer science, psychology, and cognitive science, as well as graduate students and postdoctoral fellows.</p><br> <p>            Last Modified: 01/20/2021<br>      Modified by: Maithilee&nbsp;Kunda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Visuospatial reasoning--how we think about objects, shapes, physical and spatial relationships, etc.--is a critical part of human intelligence.  It affects how we think about many everyday activities like looking at maps, loading the dishwasher, fixing things, etc., as well as more abstract activities like reading books, reasoning about numbers and other mathematical concepts, and so on.  Visuospatial reasoning is increasingly recognized as being extremely important in STEM education and careers, and we are learning more and more about its role in other educational and career settings every day.  This project used computational modeling methods from artificial intelligence (AI) to study how people might learn visuospatial reasoning skills.  Just like we learn language, numbers, etc., starting as infants and continuing throughout our lifetimes, we also learn how to do various kinds of visuospatial reasoning over time.  Because visuospatial reasoning is such a broad topic, in this project we narrowed down the scope of our research to focus on visuospatial intelligence tests.  These are tests that many people take (for example, in the context of a clinical cognitive evaluation, in schools, or as part of an IQ assessment), and these tests often ask people to reason about visual shapes and complex relationships among them.  Our goal in this project was to find out more about the kinds of visuospatial reasoning needed to solve such tests, and how these various kinds of reasoning can be learned through experience.  We developed a conceptual framework that highlights four different components of visuospatial reasoning: basic reasoning operations like mental rotation; the procedure or algorithm that uses these operations to solve problems; the synthesis process that creates the problem-solving algorithm; and the interpretation process that parses the incoming problem and establishes what kind of problem it is in the first place.  We then investigated computational models for these components on various visuospatial intelligence tests, looking not only at how each component works, but how each one might be learned by an artificial agent through various types of input experiences, instead of being programmed by hand.   We also looked at how these learning agents perform on the kinds of visual inputs that human infants and children receive, for example when playing with toys.  We developed a new dataset for studying these questions, called the Toybox dataset (available online here: https://aivaslab.github.io/toybox/), and we also worked in collaboration with developmental psychologists to study examples of real-world visual inputs from infant-worn head-cameras.  Ultimately, we expect that the broader impacts of this research include using our findings to feed into new ways to think about human visuospatial learning, especially to understand what kinds of real-life experiences are valuable for supporting these important kinds of learning, and how we can design new educational and training experiences for people to boost their visuospatial reasoning skills.  This project also provided valuable research and education experiences for many students, including undergraduate students in computer science, psychology, and cognitive science, as well as graduate students and postdoctoral fellows.       Last Modified: 01/20/2021       Submitted by: Maithilee Kunda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
