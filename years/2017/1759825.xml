<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Developing Methodology for Commensuration Bias Detection in Grant Application Peer Review</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2018</AwardEffectiveDate>
<AwardExpirationDate>03/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>259982.00</AwardTotalIntnAmount>
<AwardAmount>259982</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04010000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SMA</Abbreviation>
<LongName>SBE Off Of Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Brian Humes</SignBlockName>
<PO_EMAI>bhumes@nsf.gov</PO_EMAI>
<PO_PHON>7032927284</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Through its six federal grant agencies, the United States invests billions of dollars annually to promote science, technology, and engineering research at colleges and universities. The long-term goal of this public investment of money and trust is to improve our nation's health, economy, and social policies. In order to determine which researchers and projects will receive funding, grant agencies employ a process of peer review in which expert researchers evaluate the merits of submitted proposals. This longstanding social technology -- of relying on expert evaluation to inform determinations of merit -- empowers grant agencies to make funding decisions based on a fuller understanding of the social and scientific excellence of each project. As such, it is critical for grant agencies to employ rigorous and fair peer review processes in order to recruit, retain, and fund the best minds. This project builds on a growing scientific literature that studies how grant peer review works with an eye towards identifying ways of improving its effectiveness. More specifically, grant proposal review procedures commonly require reviewers to score applications along multiple dimensions -- for example, a proposal's approach, innovation, versus significance -- as an intermediate step in determining the proposal's overall score. When procedures are left unspecified for how reviewers should combine individual scores (along multiple dimensions) into overall scores, evaluators might arrive at overall scores in ways that subtly advantage and disadvantage grant proposals submitted by applicants from different social groups. Any such difference is what we call commensuration bias. This research identifies and evaluates approaches for measuring commensuration bias by analyzing peer review data from applications submitted to an ongoing intramural collaborative biomedical research program that utilized the independent peer review services of the American Institute of Biological Sciences. This project aims to offer concrete, efficient policies that ensure fair review for any grant agency that requires scoring of applications along multiple criteria, including the National Institutes of Health, which is the world's largest public funder of biomedical research in the world.&lt;br/&gt;&lt;br/&gt;There is currently no established methodology for detecting commensuration bias. The availability of criteria and overall scores from individual reviewers makes it possible to examine how individual reviewers evaluate multiple criteria simultaneously and how they unconsciously combine criteria scores to arrive at overall scores. In addition to hierarchical linear models that assume overall application score is an additive function of criteria scores, this study uses Bayesian regression trees to model non-linear decision processes and multivariate analyses to model the distribution of criteria scores. This range of statistical approaches allows this study to avoid making strong assumptions about the nature of commensuration and provides tools needed to inform two main types of potential policy recommendations: (a) those that focus on the restructuring or modification of programmatic review procedures as a whole and (b) those that focus on the monitoring of unusual peer review scores.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/13/2018</MinAmdLetterDate>
<MaxAmdLetterDate>04/26/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1759825</AwardID>
<Investigator>
<FirstName>Carole</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carole J Lee</PI_FULL_NAME>
<EmailAddress>c3@uw.edu</EmailAddress>
<PI_PHON>2065439888</PI_PHON>
<NSF_ID>000729219</NSF_ID>
<StartDate>03/13/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Elena</FirstName>
<LastName>Erosheva</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elena Erosheva</PI_FULL_NAME>
<EmailAddress>erosheva@uw.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000674500</NSF_ID>
<StartDate>03/13/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName/>
<StateCode>WA</StateCode>
<ZipCode>981951111</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7626</Code>
<Text>SciSIP-Sci of Sci Innov Policy</Text>
</ProgramElement>
<ProgramReference>
<Code>7626</Code>
<Text>SCIENCE OF SCIENCE POLICY</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~259982</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Unbiased peer review is critical for ensuring that grant agencies can fulfill their mandate to recruit, retain, and fund the best minds. &nbsp;The introduction of scoring hierarchies ? where reviewers provide an overall score that is informed by scores for more specific peer review criteria ? has become a common method for conducting peer reviews. &nbsp;For example, at the National Institutes of Health (NIH), reviewers score proposals for their overall impact as well as for their Significance, Investigator(s), Innovation, Approach, and Environment. Scoring criteria is thought to be useful because it provides applicants feedback along each dimension of evaluation and, according to psychological research, may reduce biases in overall scores by focusing judgment on merit-related dimensions.</p> <p>This project studied proposals submitted to NIH?s main funding mechanism, the Research Project Grant (R01) in 2014-2016 to evaluate whether Black-white disparities existed in funding and scoring despite NIH?s use of a scoring hierarchy.&nbsp; Using multilevel models and matching on key variables such as career stage, gender, and area of science, our peer-reviewed <em>Science Advances</em> paper reports that reviewers gave Black applicants lower preliminary overall impact scores and lower preliminary Significance, Investigator(s), Innovation, Approach, and Environment scores as compared to their white peers.&nbsp; We also discovered that disparities in the preliminary overall impact scores were not due to differences in how heavily reviewers weighted specific criteria.&nbsp; Instead, we found that Black-white disparities in preliminary criterion scores fully account for racial disparities in the preliminary overall impact scores.&nbsp; Our findings suggest that, if grant money is distributed by a partial lottery ? where winners are chosen randomly among applications that pass an initial round of peer review to cull the weakest proposals ? Black-white disparities in funding might continue, as explained in a correspondence piece published in <em>The Lancet</em>.&nbsp; Identifying specific, pervasive disparities in the grant review process has far-reaching benefits to STEM, biomedicine, and our national interests by shining a light on the concrete inequities that prevent the U.S. from leveraging the skills and talents of its full workforce.</p> <p>In addition to this research on the relationship between overall and criterion scores, we have published related papers on how shifts in evaluative context can lead to variations in scoring/valuation, methods for assessing quality of peer review scoring processes, and accounting for individual-level variability in scores in other domains of academic peer evaluation.&nbsp; These papers provide critical conceptual and methodological context for understanding how to evaluate and measure the quality of peer review scoring processes.</p><br> <p>            Last Modified: 07/20/2021<br>      Modified by: Elena&nbsp;Erosheva</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Unbiased peer review is critical for ensuring that grant agencies can fulfill their mandate to recruit, retain, and fund the best minds.  The introduction of scoring hierarchies ? where reviewers provide an overall score that is informed by scores for more specific peer review criteria ? has become a common method for conducting peer reviews.  For example, at the National Institutes of Health (NIH), reviewers score proposals for their overall impact as well as for their Significance, Investigator(s), Innovation, Approach, and Environment. Scoring criteria is thought to be useful because it provides applicants feedback along each dimension of evaluation and, according to psychological research, may reduce biases in overall scores by focusing judgment on merit-related dimensions.  This project studied proposals submitted to NIH?s main funding mechanism, the Research Project Grant (R01) in 2014-2016 to evaluate whether Black-white disparities existed in funding and scoring despite NIH?s use of a scoring hierarchy.  Using multilevel models and matching on key variables such as career stage, gender, and area of science, our peer-reviewed Science Advances paper reports that reviewers gave Black applicants lower preliminary overall impact scores and lower preliminary Significance, Investigator(s), Innovation, Approach, and Environment scores as compared to their white peers.  We also discovered that disparities in the preliminary overall impact scores were not due to differences in how heavily reviewers weighted specific criteria.  Instead, we found that Black-white disparities in preliminary criterion scores fully account for racial disparities in the preliminary overall impact scores.  Our findings suggest that, if grant money is distributed by a partial lottery ? where winners are chosen randomly among applications that pass an initial round of peer review to cull the weakest proposals ? Black-white disparities in funding might continue, as explained in a correspondence piece published in The Lancet.  Identifying specific, pervasive disparities in the grant review process has far-reaching benefits to STEM, biomedicine, and our national interests by shining a light on the concrete inequities that prevent the U.S. from leveraging the skills and talents of its full workforce.  In addition to this research on the relationship between overall and criterion scores, we have published related papers on how shifts in evaluative context can lead to variations in scoring/valuation, methods for assessing quality of peer review scoring processes, and accounting for individual-level variability in scores in other domains of academic peer evaluation.  These papers provide critical conceptual and methodological context for understanding how to evaluate and measure the quality of peer review scoring processes.       Last Modified: 07/20/2021       Submitted by: Elena Erosheva]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
