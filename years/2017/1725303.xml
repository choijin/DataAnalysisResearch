<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SPX: Collaborative Research: Mongo Graph Machine (MGM): A Flash-Based Appliance for Large Graph Analytics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2017</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>520001.00</AwardTotalIntnAmount>
<AwardAmount>520001</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>We live in the age of big data. In many problem domains such as data-mining, machine learning, scientific computing, and the study of social networks, the data deals with relationships between pairs of entities, and is represented by a data structure called a graph. Graphs of interest today may have hundreds of billions of entities, and trillions of relationships between these entities. Large-scale graph processing is typically done in data-centers which are huge clusters of power hungry computers. The proposed Mongo Graph Machine (MGM) project will explore a different solution known as out-of-core processing. In this system, graphs will be stored in flash memory, which is much cheaper, denser and cooler than DRAMs, and processed using a combination of specialized circuits called FPGAs in tandem with a conventional processor. A programming system will be developed to hide this complexity from the end-user. The resulting system will be small enough to fit under a desk and dramatically more energy-efficient while providing powerful graph processing capability.&lt;br/&gt;&lt;br/&gt;The MGM project will address the problem of storing and processing extreme-scale graphs by using in-storage acceleration based on NAND flash chips with an attached FPGA. A single machine can accommodate 1 TB to 16 TBs of flash memory using current NAND technology. This configuration provides the flash storage necessary to store very large graphs and the computational power necessary to saturate the bandwidth of the flash. To address the programming problem for this architecture, the project will develop compiler technology and FPGA accelerators that will permit developers to write applications in the high-level programming model, leaving it to the system to exploit parallelism and optimize memory accesses for the access characteristics of flash storage. The software system will be based on the Galois system, which has been shown to scale to hundreds of processors on large shared-memory machines.</AbstractNarration>
<MinAmdLetterDate>09/12/2017</MinAmdLetterDate>
<MaxAmdLetterDate>09/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1725303</AwardID>
<Investigator>
<FirstName>Professor</FirstName>
<LastName>Arvind</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Professor Arvind</PI_FULL_NAME>
<EmailAddress>arvind@mit.edu</EmailAddress>
<PI_PHON>6172536090</PI_PHON>
<NSF_ID>000386286</NSF_ID>
<StartDate>09/12/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394307</ZipCode>
<StreetAddress><![CDATA[32 Vassar Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>042Y</Code>
<Text>PPoSS-PP of Scalable Systems</Text>
</ProgramElement>
<ProgramReference>
<Code>026Z</Code>
<Text>NSCI: National Strategic Computing Initi</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~520001</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-76ae622e-7fff-ff12-0541-5872328a7bd0"> </span></p> <p dir="ltr"><span>Extremely large and sparse graphs with irregular structures (billions of vertices and hundreds of billions of edges) arise in many important problems, for example, analyses of social networks. Typically, systems for large-scale graph analytics provision the system with enough DRAM to store the entire graph data in memory. The performance of such systems falls catastrophically if the whole data (&gt; 1 TBs) doesn't fit in DRAM. Similar issues arise in other big data processing problems, for example, large key-value stores, processing analytical SQL queries on large data bases, and genomics sequencing and analysis. Our Mongo Graph Machine (MGM) project has explored an alternative architecture based on flash storage. We have shown that our approach is significantly cheaper and consumes less power, without significant loss of performance.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>There are many challenges in using flash storage instead of DRAM for computation. Flash access granularity is a page (4-8 KB) as opposed to a cache line (32-128 B) for DRAM, and flash access latency is 3-4 orders of magnitude larger than DRAM. To amortize the performance penalty, our flash-based architecture uses in-storage accelerators. However, without a modification of the algorithm, large access granularity causes an extreme read amplification. Also the large access latency of flash cannot be mitigated using caches because most of these algorithms have little locality. Thus, an effective use of flash for computation invariably requires codesign of software and hardware accelerators.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>We have demonstrated the effectiveness of our approach by building systems for four different applications.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>1. GraFBoost - a system for graph analytics:</span></p> <p dir="ltr"><span>We produced the GraFBoost system which can perform a variety of graph analytics on a single machine with a large amount of flash storage and a small amount of DRAM (~1GB). We compared our results with a wide range of existing graph analytic frameworks and showed that both of our implementations were typically 1) faster than the out-of-core frameworks and 2) competitive with many in-memory systems that use enterprise-class multicores and large DRAM (128GB). In addition, we were the only system able to complete all of the algorithms for each of the reference graphs due to the huge size of the dataset.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>2. PinK - a high-speed LSM-tree-based key-value SSD:</span></p> <p dir="ltr"><span>We produced PinK, a key-value SSD (KV-SSD) with bounded tail latency, based on the same flash-based infrastructure used in GraFBoost. Key-value store (KVS) based on a log-structured merge-tree (LSM-tree) is preferable to hash-based KVS because an LSM-tree supports a wider variety of operations and shows better performance, especially for writes. However, LSM-trees are difficult to implement in the resource constrained environment of an SSD and consequently, KV-SSDs typically use hashing. PinK improved the performance of LSM-trees running in a resource constrained storage drive by avoiding the use of Bloom filters and instead, used the freed-up DRAM to keep/pin the top levels of the LSM-tree. Also, accelerators were used to periodically merge internal data. Compared to a hash-based KV-SSD, PinK reduced the 99th-percentile tail latency by 73%, improved the average read latency by 42% and the throughput by 37%.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>3. AQUOMAN - an in-storage analytic-query offloading machine:</span></p> <p dir="ltr"><span>We produced AQUOMAN, an in-storage analytic-query offloading machine, using the hardware accelerator infrastructure of this project. AQUOMAN offloads most SQL operators, including multi-way joins, to storage. AQUOMAN executes Table Tasks, which apply a static dataflow graph of SQL operators to relational tables to produce an output table. Table Tasks use a streaming computation model, which allows AQUOMAN to process queries with a small amount of DRAM for intermediate results. AQUOMAN can be integrated in the database software stack transparently. Using the 1TB TPC-H benchmark and ignoring the inter-query page cache reuse, we have shown that MonetDB running on a 4-core, 16GB-DRAM machine with AQUOMAN-augmented SSDs performs as well as MonetDB on a 32-core, 128GB-DRAM machine with standard SSDs.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>4. SMUFIN-F - somatic mutation detection on commodity machines using flash:</span></p> <p dir="ltr"><span>Analysis of a patient's genomics data is the first step towards precision medicine. Such analyses are performed on expensive enterprise-class server machines because input data sets are large, and the intermediate data structures are even larger (TB-size) and require random-accesses. We developed a general method to perform a specific genomics problem, mutation detection, on a cheap commodity personal computer (PC) with flash storage and a small amount of DRAM. We constructed large histograms of k-mers efficiently on external SSDs and applied our technique to a state-of-the-art reference-free genomics algorithm, SMUFIN, to create SMUFIN-F. We have shown that on two PCs, SMUFIN-F achieves the same throughput at only 36% of the hardware cost and 45% of the energy compared to SMUFIN on an enterprise-class machine. To the best of our knowledge, SMUFIN-F is the first reference-free system that can detect somatic mutations on commodity PCs for whole human genomes. We believe our technique should apply to other k-mer or n-gram-based algorithms.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 07/15/2021<br>      Modified by: Professor&nbsp;Arvind</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Extremely large and sparse graphs with irregular structures (billions of vertices and hundreds of billions of edges) arise in many important problems, for example, analyses of social networks. Typically, systems for large-scale graph analytics provision the system with enough DRAM to store the entire graph data in memory. The performance of such systems falls catastrophically if the whole data (&gt; 1 TBs) doesn't fit in DRAM. Similar issues arise in other big data processing problems, for example, large key-value stores, processing analytical SQL queries on large data bases, and genomics sequencing and analysis. Our Mongo Graph Machine (MGM) project has explored an alternative architecture based on flash storage. We have shown that our approach is significantly cheaper and consumes less power, without significant loss of performance.    There are many challenges in using flash storage instead of DRAM for computation. Flash access granularity is a page (4-8 KB) as opposed to a cache line (32-128 B) for DRAM, and flash access latency is 3-4 orders of magnitude larger than DRAM. To amortize the performance penalty, our flash-based architecture uses in-storage accelerators. However, without a modification of the algorithm, large access granularity causes an extreme read amplification. Also the large access latency of flash cannot be mitigated using caches because most of these algorithms have little locality. Thus, an effective use of flash for computation invariably requires codesign of software and hardware accelerators.     We have demonstrated the effectiveness of our approach by building systems for four different applications.     1. GraFBoost - a system for graph analytics: We produced the GraFBoost system which can perform a variety of graph analytics on a single machine with a large amount of flash storage and a small amount of DRAM (~1GB). We compared our results with a wide range of existing graph analytic frameworks and showed that both of our implementations were typically 1) faster than the out-of-core frameworks and 2) competitive with many in-memory systems that use enterprise-class multicores and large DRAM (128GB). In addition, we were the only system able to complete all of the algorithms for each of the reference graphs due to the huge size of the dataset.     2. PinK - a high-speed LSM-tree-based key-value SSD: We produced PinK, a key-value SSD (KV-SSD) with bounded tail latency, based on the same flash-based infrastructure used in GraFBoost. Key-value store (KVS) based on a log-structured merge-tree (LSM-tree) is preferable to hash-based KVS because an LSM-tree supports a wider variety of operations and shows better performance, especially for writes. However, LSM-trees are difficult to implement in the resource constrained environment of an SSD and consequently, KV-SSDs typically use hashing. PinK improved the performance of LSM-trees running in a resource constrained storage drive by avoiding the use of Bloom filters and instead, used the freed-up DRAM to keep/pin the top levels of the LSM-tree. Also, accelerators were used to periodically merge internal data. Compared to a hash-based KV-SSD, PinK reduced the 99th-percentile tail latency by 73%, improved the average read latency by 42% and the throughput by 37%.    3. AQUOMAN - an in-storage analytic-query offloading machine: We produced AQUOMAN, an in-storage analytic-query offloading machine, using the hardware accelerator infrastructure of this project. AQUOMAN offloads most SQL operators, including multi-way joins, to storage. AQUOMAN executes Table Tasks, which apply a static dataflow graph of SQL operators to relational tables to produce an output table. Table Tasks use a streaming computation model, which allows AQUOMAN to process queries with a small amount of DRAM for intermediate results. AQUOMAN can be integrated in the database software stack transparently. Using the 1TB TPC-H benchmark and ignoring the inter-query page cache reuse, we have shown that MonetDB running on a 4-core, 16GB-DRAM machine with AQUOMAN-augmented SSDs performs as well as MonetDB on a 32-core, 128GB-DRAM machine with standard SSDs.    4. SMUFIN-F - somatic mutation detection on commodity machines using flash: Analysis of a patient's genomics data is the first step towards precision medicine. Such analyses are performed on expensive enterprise-class server machines because input data sets are large, and the intermediate data structures are even larger (TB-size) and require random-accesses. We developed a general method to perform a specific genomics problem, mutation detection, on a cheap commodity personal computer (PC) with flash storage and a small amount of DRAM. We constructed large histograms of k-mers efficiently on external SSDs and applied our technique to a state-of-the-art reference-free genomics algorithm, SMUFIN, to create SMUFIN-F. We have shown that on two PCs, SMUFIN-F achieves the same throughput at only 36% of the hardware cost and 45% of the energy compared to SMUFIN on an enterprise-class machine. To the best of our knowledge, SMUFIN-F is the first reference-free system that can detect somatic mutations on commodity PCs for whole human genomes. We believe our technique should apply to other k-mer or n-gram-based algorithms.          Last Modified: 07/15/2021       Submitted by: Professor Arvind]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
