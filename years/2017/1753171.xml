<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Calibrating Regularization for Enhanced Statistical Inference</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2018</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>127696</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Volumes of data that seemed unimaginable only a decade ago are now ubiquitous in scientific research. Investment decisions are based on prices, updated every microsecond, for thousands of securities; atmospheric scientists use multiresolution satellite images to understand climate change; and internet companies exploit massive music collections to infer trends in tastes and preferences. Rigorous analysis of these large datasets requires a balance between computational constraints and statistical performance, and operationalizing such tradeoffs involves a combination of algorithmic and design-based approximations. For example, decreasing the resolution of an image or subsampling high-throughput data enables faster computations but removes potentially valuable information. At the same time, elaborate explanations of the scientific process are credible because they account for real-world complexity, but estimating complex models requires both more data and larger computers. The proposed work investigates the viability of modern statistical and machine learning methodologies for answering applied scientific questions. The project will create new algorithms and open-source software for combining computational approximations with regularization for analyzing large datasets as well as providing theoretical justification for their statistical properties. A more complete picture of the interplay between statistical regularization, computational approximation, and scientific parsimony will enable fundamental scientific advancement. The PI will employ the methodologies developed in this project to facilitate novel science with large datasets in climate science, biology, music analysis, astronomy, economics, and chemistry. Furthermore, the PI will carefully integrate the research aims with educational and outreach objectives to engage elementary and high school music students, introducing them to modern statistics and computer science, as well as integrating underrepresented populations in research.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Computational tractability and statistical efficiency for large datasets necessitate approximation or regularization, either of which heuristically balances fidelity to the data with scientific goals like parsimony, smoothness, sparsity, or interpretability. The PI seeks to elucidate the dual roles of regularization and approximation as tools for better scientific understanding. Current research in computer science has focused on improving algorithms so as to enable computation with minimal approximation. Meanwhile, statisticians have developed regularization techniques in order to take advantage of simple structures - graph topologies, sparse linear models, smooth functions - that, if representative of the truth, will improve inference and prediction. The challenge is to understand the consequences of this coupling for scientific interpretability. The PI seeks to address two important issues (1) how do we select tuning parameters when computations are at a premium? and (2) how does the accuracy and stability of scientific conclusions relate to the approximation/regularization methods used to obtain those conclusions? This project seeks to enable fundamental scientific progress by understanding the connections between computational approximations and statistical regularization, thereby facilitating improved inferences. The PI will fill this gap by deriving practical algorithms with accompanying theoretical justification under more reasonable statistical assumptions. These tools will be tightly coupled with applications in neuroscience, genetics, atmospheric science, and music.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/07/2018</MinAmdLetterDate>
<MaxAmdLetterDate>10/21/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1753171</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>McDonald</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel J McDonald</PI_FULL_NAME>
<EmailAddress>dajmcdon@gmail.com</EmailAddress>
<PI_PHON>8128557828</PI_PHON>
<NSF_ID>000656759</NSF_ID>
<StartDate>06/07/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474057103</ZipCode>
<StreetAddress><![CDATA[1020 E Kirkwood Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~79994</FUND_OBLG>
<FUND_OBLG>2019~47701</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-39fe5736-7fff-3770-9db5-b051914f7af6"> </span></p> <p dir="ltr"><span>Extracting information from large data requires computational tractability and statistical optimality. These are typically achieved through approximation or regularization, either of which heuristically balances fidelity to the data with scientific goals like parsimony, smoothness, sparsity, or interpretability. For example, decreasing the resolution of an image or subsampling high-throughput data enables faster computations but removes potentially valuable information. At the same time, elaborate explanations of the scientific process are credible because they account for real-world complexity, but estimating complex models requires both more data and larger computers.</span></p> <div class="page" title="Page 1"> <div class="layoutArea"> <div class="column"> <p><span>This work investigated how statistically optimal decisions depend on the amount and the structure of regularization or approximation, both of which must be tied to the scientific questions at stake and calibrated according to the data. Research in computer science has focused on improving algorithms to enable computation with a minimum of approximation. Meanwhile, statisticians have developed regularization techniques in order to take advantage of simple structures that, if representative of the truth, will improve inference and prediction. My work bridges the gap between these perspectives.</span></p> </div> </div> </div> <p dir="ltr"><span>In this research, I have developed new statistical theories to understand properties of statistical and computational methodologies for large data and the tradeoffs involved in choosing between them. I have also created new techniques to enable inference in problems of large size. This work trained five graduate students and three undergraduates in state-of-the-art computational, statistical, and data science techniques. Through the recruitment and training of these students, this project supported diversity and promoted equity.&nbsp;This project also made domain area contributions, in collaboration with subject matter experts, in other areas of science and industry such as bioinformatics, musical analysis, neuroscience, and atmospheric science. All of the developed research has been disseminated through journal or conference papers, presented in appropriate venues, and implemented in freely available software packages.&nbsp;</span></p><br> <p>            Last Modified: 08/19/2021<br>      Modified by: Daniel&nbsp;J&nbsp;Mcdonald</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Extracting information from large data requires computational tractability and statistical optimality. These are typically achieved through approximation or regularization, either of which heuristically balances fidelity to the data with scientific goals like parsimony, smoothness, sparsity, or interpretability. For example, decreasing the resolution of an image or subsampling high-throughput data enables faster computations but removes potentially valuable information. At the same time, elaborate explanations of the scientific process are credible because they account for real-world complexity, but estimating complex models requires both more data and larger computers.     This work investigated how statistically optimal decisions depend on the amount and the structure of regularization or approximation, both of which must be tied to the scientific questions at stake and calibrated according to the data. Research in computer science has focused on improving algorithms to enable computation with a minimum of approximation. Meanwhile, statisticians have developed regularization techniques in order to take advantage of simple structures that, if representative of the truth, will improve inference and prediction. My work bridges the gap between these perspectives.    In this research, I have developed new statistical theories to understand properties of statistical and computational methodologies for large data and the tradeoffs involved in choosing between them. I have also created new techniques to enable inference in problems of large size. This work trained five graduate students and three undergraduates in state-of-the-art computational, statistical, and data science techniques. Through the recruitment and training of these students, this project supported diversity and promoted equity. This project also made domain area contributions, in collaboration with subject matter experts, in other areas of science and industry such as bioinformatics, musical analysis, neuroscience, and atmospheric science. All of the developed research has been disseminated through journal or conference papers, presented in appropriate venues, and implemented in freely available software packages.        Last Modified: 08/19/2021       Submitted by: Daniel J Mcdonald]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
