<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII:RI: Adaptive and Practical Algorithms for Personalization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2018</AwardEffectiveDate>
<AwardExpirationDate>04/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Intelligent personalization systems, such as those in news, advertising, search, online shopping, and clinical trials, are playing an increasingly important role in daily lives, bringing to us tremendous convenience as well as increasing the productivity of society. The main challenge in developing algorithmic solutions for these systems lies in the fact that only feedback for the recommended options, but not the other ones, is provided by the users. Many simple heuristics have been used in practice, and there are also some recent advances on more rigorous approaches based on the "contextual bandit" model, referring to an analogy with the objective to maximize the sum of rewards earned through a sequence of lever pulls where an encoding of past performance provides context. However, there is still great room for improvement in terms of both practicality and performance guarantees. This project seeks to develop more practical and adaptive contextual bandit algorithms for such systems. The success of this project requires developing new algorithmic techniques as well as mathematical tools from statistics, optimization, machine learning, and their combinations in an innovative way, which advances the theory and practice of the field of online decision making. Education is integrated into the project through curriculum development and student mentoring. Outreach activities include collaborations with other universities as well as with industry, and also organizing related workshops at top conferences. &lt;br/&gt;&lt;br/&gt;Specifically, the project aims at designing a family of practical contextual bandit algorithms which not only enjoy some information-theoretic worst-case guarantees but can also achieve much better performance when the problem exhibits some kind of "easiness". First, the project systematically studies different kinds of "easiness" measurements and develops and analyzes specific algorithms for each of these measurements. Second, the project further considers the question of whether it is possible to have a single algorithm that is optimal against all problem instances, where optimality is in terms of the best performance among a reasonable class of algorithms. Finally, the project implements all the developed algorithms and conducts empirical evaluation on benchmark datasets, with the goal of releasing easy-to-use and publicly available software.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>04/13/2018</MinAmdLetterDate>
<MaxAmdLetterDate>04/13/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1755781</AwardID>
<Investigator>
<FirstName>Haipeng</FirstName>
<LastName>Luo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Haipeng Luo</PI_FULL_NAME>
<EmailAddress>haipengl@usc.edu</EmailAddress>
<PI_PHON>6098278868</PI_PHON>
<NSF_ID>000753101</NSF_ID>
<StartDate>04/13/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[3720 S. Flower St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Contextual bandit algorithms have recently seen great practical success in building personalized recommendation systems, including those for news, advertising, online shopping, and others. The term "bandit" refers to a classic mathematical model where the goal is to maximize rewards via a sequence of decisions, and "contextual bandit" further makes use of contextual information to help make better decisions. The aim of this project was to develop more practical and adaptive contextual bandit algorithms that provably perform well in different environments, ranging from the easier ones where certain structures are available, to the more difficult ones where data could be non-stationary or even adversarially corrupted.</p> <p>The technical outcomes of this project include the following. First, we developed multiple new bandit and contextual bandit algorithms that can automatically adapt to the structure of different problems and outperform standard algorithms designed for the worst case. The structures we considered cover many practical scenarios, such as when the data follow a fixed distribution, when a certain pre-trained reward predictor is available, or when the data variation across time is reasonably small. Among these results, one notable finding is that the optimal results for the classic bandit problem and its contextual version are sharply different, and the latter requires a set of algorithmic innovations.</p> <p>Second, we developed the first contextual bandit algorithm for non-stationary environments that is simultaneously optimal, efficient, and parameter-free. Each of the three properties is highly desirable in practice, and yet their combination is infamously challenging to be satisfied at the same time. Through a sequence of improvements in several works, we not only managed to achieve this goal, but also accomplished so via a black-box reduction approach that automatically turns an existing algorithm designed for stationary environments into an algorithm that can handle non-stationary data. Moreover, our approach is also general enough to be applied to more challenging problems such as reinforcement learning.</p> <p>The last main technical outcome is that we initiated the study of model selection for contextual bandits. Model selection centers around a key question: given many different policy classes, which one should we choose in practice to achieve the best accuracy-complexity trade-off? While the answer is well-known for supervised machine learning, it becomes highly unclear for learning with partial information feedback such as contextual bandits. Motivated by this fact, we proposed the first model selection algorithm for a special case of linear contextual bandits, allowing one to find the best feature representation among a set of nested candidates. Our work was quickly followed up by many other improvements and generalizations, further advancing our understanding for contextual bandits.</p> <p>Besides these technical outcomes, this project also led to one related graduate course at the University of Southern California (USC). It also contributed to the training of five graduate students, three visiting students, and one undergrade student (who won the prestigious USC achievement award partially in recognition of her work done in this project). The PI also participated in the USC's Summer High School Intensive in Next-Generation Engineering (SHINE) program and had two rising seniors from local schools to spend seven weeks of the summer working with the PI and his graduate students on the basic parts of the project.</p><br> <p>            Last Modified: 06/15/2021<br>      Modified by: Haipeng&nbsp;Luo</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Contextual bandit algorithms have recently seen great practical success in building personalized recommendation systems, including those for news, advertising, online shopping, and others. The term "bandit" refers to a classic mathematical model where the goal is to maximize rewards via a sequence of decisions, and "contextual bandit" further makes use of contextual information to help make better decisions. The aim of this project was to develop more practical and adaptive contextual bandit algorithms that provably perform well in different environments, ranging from the easier ones where certain structures are available, to the more difficult ones where data could be non-stationary or even adversarially corrupted.  The technical outcomes of this project include the following. First, we developed multiple new bandit and contextual bandit algorithms that can automatically adapt to the structure of different problems and outperform standard algorithms designed for the worst case. The structures we considered cover many practical scenarios, such as when the data follow a fixed distribution, when a certain pre-trained reward predictor is available, or when the data variation across time is reasonably small. Among these results, one notable finding is that the optimal results for the classic bandit problem and its contextual version are sharply different, and the latter requires a set of algorithmic innovations.  Second, we developed the first contextual bandit algorithm for non-stationary environments that is simultaneously optimal, efficient, and parameter-free. Each of the three properties is highly desirable in practice, and yet their combination is infamously challenging to be satisfied at the same time. Through a sequence of improvements in several works, we not only managed to achieve this goal, but also accomplished so via a black-box reduction approach that automatically turns an existing algorithm designed for stationary environments into an algorithm that can handle non-stationary data. Moreover, our approach is also general enough to be applied to more challenging problems such as reinforcement learning.  The last main technical outcome is that we initiated the study of model selection for contextual bandits. Model selection centers around a key question: given many different policy classes, which one should we choose in practice to achieve the best accuracy-complexity trade-off? While the answer is well-known for supervised machine learning, it becomes highly unclear for learning with partial information feedback such as contextual bandits. Motivated by this fact, we proposed the first model selection algorithm for a special case of linear contextual bandits, allowing one to find the best feature representation among a set of nested candidates. Our work was quickly followed up by many other improvements and generalizations, further advancing our understanding for contextual bandits.  Besides these technical outcomes, this project also led to one related graduate course at the University of Southern California (USC). It also contributed to the training of five graduate students, three visiting students, and one undergrade student (who won the prestigious USC achievement award partially in recognition of her work done in this project). The PI also participated in the USC's Summer High School Intensive in Next-Generation Engineering (SHINE) program and had two rising seniors from local schools to spend seven weeks of the summer working with the PI and his graduate students on the basic parts of the project.       Last Modified: 06/15/2021       Submitted by: Haipeng Luo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
