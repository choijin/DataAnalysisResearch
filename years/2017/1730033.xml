<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>II-New: Flexible User Interaction Instrumentation for Ubiquitous and Immersive Computing Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>356657.00</AwardTotalIntnAmount>
<AwardAmount>356657</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Smart phones, networked devices, and commercially available augmented and virtual reality kits are making ubiquitous and immersive interactive systems increasingly commonplace. Designing these systems, and the applications that use them, requires the ability to study how system features and human behavior affect each other in natural contexts. This award will provide an interdisciplinary research team with equipment for capturing both individual and small group behavioral data in situ, including body motion, eye gaze, mental workload, and emotion.  The infrastructure will support projects at the team's institution in a number of domains, including autonomous vehicle use by visually impaired users, augmented reality training for emergency medical responders, and collaborative scientific discovery in virtual visualization environments. A doctoral student with related research interests will coordinate management of and training on the infrastructure, developing both technical and research skills. The team will also use the equipment to provide enhanced research opportunities for undergraduates from a number of disciplines, institutions, and backgrounds. &lt;br/&gt; &lt;br/&gt;Much of the planned infrastructure uses next-generation versions of tools that the team already has expertise with. This reduces deployment risks and allows them to augment existing projects with new capabilities while enabling new directions for research. In most cases, this is a transformation from fixed lab-based sensing to unconstrained, mobile data collection in the field. The new capabilities include five main data sources. One is body motion data, which will be collected using an industry standard infrared-based motion capture system that can flexibly capture the movements of individuals or dyads. A second is location and gait capture, for both individuals and groups, through an unobtrusive, configurable system of floor-mounted force plates. A third is eye gaze data, collected through a portable headset that captures eye fixations and pupil dilation to support monitoring visual attention. A fourth is electroencephalogram (EEG) data, collected through a portable dry sensor-based headset, that can monitor workload, affect, and facial features as well as support prototyping of brain-computer interfaces (BCIs). A fifth is physiological data including temperature, pulse, arm motion, and arousal, collected through a inconspicuous wristband that includes photoplethysmography, accelerometer, and electrodermal sensors. Each individual data stream comes with accompanying analytic software; collectively, data will be managed through a commercially available tool for analyzing events synchronized across multiple parallel data streams. Key innovations of this award this award are its novel data integration strategies and cross-disciplinary application areas.</AbstractNarration>
<MinAmdLetterDate>06/27/2017</MinAmdLetterDate>
<MaxAmdLetterDate>11/30/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1730033</AwardID>
<Investigator>
<FirstName>Wayne</FirstName>
<LastName>Lutters</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wayne G Lutters</PI_FULL_NAME>
<EmailAddress>lutters@umd.edu</EmailAddress>
<PI_PHON>3014059717</PI_PHON>
<NSF_ID>000190666</NSF_ID>
<StartDate>06/27/2017</StartDate>
<EndDate>11/30/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anita</FirstName>
<LastName>Komlodi</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anita H Komlodi</PI_FULL_NAME>
<EmailAddress>komlodi@umbc.edu</EmailAddress>
<PI_PHON>4104553212</PI_PHON>
<NSF_ID>000230004</NSF_ID>
<StartDate>06/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ravi</FirstName>
<LastName>Kuber</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ravi A Kuber</PI_FULL_NAME>
<EmailAddress>rkuber@umbc.edu</EmailAddress>
<PI_PHON>4105854118</PI_PHON>
<NSF_ID>000569993</NSF_ID>
<StartDate>06/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Helena</FirstName>
<LastName>Mentis</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Helena M Mentis</PI_FULL_NAME>
<EmailAddress>mentis@umbc.edu</EmailAddress>
<PI_PHON>4104553687</PI_PHON>
<NSF_ID>000637791</NSF_ID>
<StartDate>06/27/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrea</FirstName>
<LastName>Kleinsmith</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrea Kleinsmith</PI_FULL_NAME>
<EmailAddress>andreak@umbc.edu</EmailAddress>
<PI_PHON>5035087506</PI_PHON>
<NSF_ID>000663957</NSF_ID>
<StartDate>11/30/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrea</FirstName>
<LastName>Kleinsmith</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrea Kleinsmith</PI_FULL_NAME>
<EmailAddress>andreak@umbc.edu</EmailAddress>
<PI_PHON>5035087506</PI_PHON>
<NSF_ID>000663957</NSF_ID>
<StartDate>06/27/2017</StartDate>
<EndDate>11/30/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland Baltimore County</Name>
<CityName>Baltimore</CityName>
<ZipCode>212500002</ZipCode>
<PhoneNumber>4104553140</PhoneNumber>
<StreetAddress>1000 Hilltop Circle</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>061364808</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND BALTIMORE COUNTY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland Baltimore County]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212500002</ZipCode>
<StreetAddress><![CDATA[1000 Hilltop Circle]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~356657</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The technological advances that have occurred over the last several years have opened the door for exciting new research to understand the ways in which interactive systems are used by and are useful for people. This changing technological landscape demands new research into how individuals interact with these technologies in the wild, given that the most powerful and elegant computing solution is of little worth if it is not able to be meaningfully integrated into one?s daily life. Given the rapid rise of innovative, richly-immersive, ubiquitous computing, a new focus is viewing the person and their myriad digital devices as a coupled complex system ? a cyber-human system.</p> <p>The aim of this project was to acquire a suite of new flexible and dynamic instrumentation to engage the big questions facing human-centered computing within new interactive spaces. Thus, this project focused on instrumentation to support the capture of users? interactions with technology to facilitate researchers? pursuit and exploration of new lines of research on ubiquitous and immersive interactive environments; considering a blending of laboratory work, for requisite precision, and field work, for enhanced ecological validity.</p> <p>The new infrastructure is maintained by the Interactive Systems Research Center (ISRC) at the University of Maryland, Baltimore County?s (UMBC). Expanding our existing research infrastructure allows researchers to pursue new research and collaboration opportunities within and outside UMBC. The newly acquired instrumentation has already benefitted existing projects and collaborations. Multidisciplinary efforts are underway exploring the effectiveness of augmented and virtual reality training in diverse situations, including surgical training and mentoring, one-to-many emergency medical provider training to improve access to healthcare, and graduate student preparation for community engaged service. Other projects focus on psychophysiological understanding of stress experienced by paramedic trainees during in-situ simulations, and social and speaking anxiety experienced by students learning English as a foreign language.</p> <p>In addition to the research opportunities, the instrumentation provides hands-on educational opportunities for students at all levels through integration in graduate level courses in UMBC?s Human-Centered Computing MS and PhD programs. The Tobii Pro X3-120 eye trackers are employed in the <em>User Interface Design</em> core course to teach students how to collect and analyze eye tracking data for evaluating user interfaces. New technology is also incorporated into the HCC graduate electives of <em>Computer Supported Cooperative Work</em> and <em>Affective Human Computer Interaction</em>. In Affective HCI, students receive hands-on training and tutorials on acquiring, cleaning and analyzing data with the Empatica E4 wristbands. Armed with this experience, students are introduced to other new instrumentation, such as the Vicon motion capture system, HTC Vive Pro Eye and Magic Leap augmented reality headsets, usable for their semester-long project. &nbsp;</p> <p>With the new infrastructure in place, in January 2020 the Co-PIs held an event to showcase the new technology to current and potential collaborators across UMBC, with a view toward attracting future project partners. The well-attended event comprised short talks by doctoral students detailing their research and how they have employed the new equipment, a tour of the user studies labs with technology demonstrations by graduate students, and a dance performance by a UMBC undergraduate dance major to demonstrate the Vicon motion capture system and its capabilities.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/29/2020<br>      Modified by: Andrea&nbsp;Kleinsmith</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The technological advances that have occurred over the last several years have opened the door for exciting new research to understand the ways in which interactive systems are used by and are useful for people. This changing technological landscape demands new research into how individuals interact with these technologies in the wild, given that the most powerful and elegant computing solution is of little worth if it is not able to be meaningfully integrated into one?s daily life. Given the rapid rise of innovative, richly-immersive, ubiquitous computing, a new focus is viewing the person and their myriad digital devices as a coupled complex system ? a cyber-human system.  The aim of this project was to acquire a suite of new flexible and dynamic instrumentation to engage the big questions facing human-centered computing within new interactive spaces. Thus, this project focused on instrumentation to support the capture of users? interactions with technology to facilitate researchers? pursuit and exploration of new lines of research on ubiquitous and immersive interactive environments; considering a blending of laboratory work, for requisite precision, and field work, for enhanced ecological validity.  The new infrastructure is maintained by the Interactive Systems Research Center (ISRC) at the University of Maryland, Baltimore County?s (UMBC). Expanding our existing research infrastructure allows researchers to pursue new research and collaboration opportunities within and outside UMBC. The newly acquired instrumentation has already benefitted existing projects and collaborations. Multidisciplinary efforts are underway exploring the effectiveness of augmented and virtual reality training in diverse situations, including surgical training and mentoring, one-to-many emergency medical provider training to improve access to healthcare, and graduate student preparation for community engaged service. Other projects focus on psychophysiological understanding of stress experienced by paramedic trainees during in-situ simulations, and social and speaking anxiety experienced by students learning English as a foreign language.  In addition to the research opportunities, the instrumentation provides hands-on educational opportunities for students at all levels through integration in graduate level courses in UMBC?s Human-Centered Computing MS and PhD programs. The Tobii Pro X3-120 eye trackers are employed in the User Interface Design core course to teach students how to collect and analyze eye tracking data for evaluating user interfaces. New technology is also incorporated into the HCC graduate electives of Computer Supported Cooperative Work and Affective Human Computer Interaction. In Affective HCI, students receive hands-on training and tutorials on acquiring, cleaning and analyzing data with the Empatica E4 wristbands. Armed with this experience, students are introduced to other new instrumentation, such as the Vicon motion capture system, HTC Vive Pro Eye and Magic Leap augmented reality headsets, usable for their semester-long project.    With the new infrastructure in place, in January 2020 the Co-PIs held an event to showcase the new technology to current and potential collaborators across UMBC, with a view toward attracting future project partners. The well-attended event comprised short talks by doctoral students detailing their research and how they have employed the new equipment, a tour of the user studies labs with technology demonstrations by graduate students, and a dance performance by a UMBC undergraduate dance major to demonstrate the Vicon motion capture system and its capabilities.          Last Modified: 10/29/2020       Submitted by: Andrea Kleinsmith]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
