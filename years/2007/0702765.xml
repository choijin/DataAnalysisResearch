<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Next Generation Compilers for Emerging Multicore Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2007</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>480000.00</AwardTotalIntnAmount>
<AwardAmount>544000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The emergence of multicores as the standard machine design has created one of the most important challenges to the software industry in the history of computing. To take advantage of the additional computational power of each new generation of machines, programs must be able to profit from the most important characteristic of multicores: the presence of multiple processors. In other words, programs must be able to execute in parallel. Furthermore, for efficient execution, this parallelism must take a form that is consistent with the internal organization of the multicore machine where the program is to execute. If these programs were to be manually designed, the need to take into account machine characteristics would increase the cost of program development significantly. Also, since newer multicore designs are likely to include novel architectural features, the process of porting programs from one generation to the next will also involve significant costs. In other words, if nothing is done, the significant increases in the cost of software will be necessary for widespread acceptance of multicores.&lt;br/&gt;Our objective in this project is to develop techniques for automating the process of generating efficient parallel programs. To this end, we will extend Pivot, a prototype C++ compiler, under development at Texas A&amp;M, with techniques capable of automatically detecting the parallelism implicit in most conventional C++ programs and mapping it onto a wide range of multicore systems. That is, we will extend Pivot with automatic parallelization techniques.  We will build on static and hybrid (static and dynamic) analysis techniques developed at Illinois and Texas A&amp;M for numerical computations and extend them to handle the irregular data structures that are often used in C++ programs.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/12/2007</MinAmdLetterDate>
<MaxAmdLetterDate>07/16/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0702765</AwardID>
<Investigator>
<FirstName>Lawrence</FirstName>
<LastName>Rauchwerger</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lawrence Rauchwerger</PI_FULL_NAME>
<EmailAddress>rwerger@illinois.edu</EmailAddress>
<PI_PHON>9792550424</PI_PHON>
<NSF_ID>000468621</NSF_ID>
<StartDate>09/12/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bjarne</FirstName>
<LastName>Stroustrup</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bjarne Stroustrup</PI_FULL_NAME>
<EmailAddress>bs@cs.tamu.edu</EmailAddress>
<PI_PHON>9798621696</PI_PHON>
<NSF_ID>000197460</NSF_ID>
<StartDate>09/12/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gabriel</FirstName>
<LastName>Dos Reis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gabriel Dos Reis</PI_FULL_NAME>
<EmailAddress>gdr@cs.tamu.edu</EmailAddress>
<PI_PHON>9798621696</PI_PHON>
<NSF_ID>000493152</NSF_ID>
<StartDate>09/12/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M Engineering Experiment Station</Name>
<CityName>College Station</CityName>
<ZipCode>778454645</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy S</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>847205572</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A&amp;M ENGINEERING EXPERIMENT STATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&amp;M Engineering Experiment Station]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778454645</ZipCode>
<StreetAddress><![CDATA[400 Harvey Mitchell Pkwy S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~315455</FUND_OBLG>
<FUND_OBLG>2008~164545</FUND_OBLG>
<FUND_OBLG>2009~16000</FUND_OBLG>
<FUND_OBLG>2010~24000</FUND_OBLG>
<FUND_OBLG>2011~24000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>The goal of this research project is to help generate with minimal effort, parallel programs from a sequential specification. For example, we could transform a sequential program into a parallel one using a parallelizing compiler or, we can write an explicitly parallel programs using parallel language constructs or libraries.</p> <p>&nbsp;</p> <p>In this project we have taken a two track approach: We have introduced</p> <p>a novel and powerful approach to automatically transform serial programs into parallel ones. We have also developed C++ language features that support and simplify parallel programming.</p> <p>&nbsp;</p> <p>Our novel approach to automatic parallelization, the Hybrid Analysis, overcomes the previously unsolved static compiler analysis limitations through the use of run-time information.&nbsp; Our Hybrid Analysis framework summarizes all memory references of interest for each loop using a novel intermediate representation, the uniform set representation (USR). This set representation is then used to verify the data independence of loops, by checking the existence of a solution to the data independence equation, i.e., that the intersection of the write and read memory references sets is empty, which represents the crucial condition for loop level parallelization.</p> <p>Proving statically that the dependence equation has no solution&nbsp;&nbsp; is a very hard, sometimes impossible problem. Hence we map it into a sufficient logical (Boolean) equation which is then recursively factored and solved. If static analysis is inconclusive we generate code to evaluate the logical expressions at run-time and then select a serial or parallel version of the corresponding loop. This technique ensures that we execute only the minimal amount of run-time overhead.&nbsp; It represents a seamless integration of static and dynamic analysis not previously achieved.</p> <p>&nbsp;</p> <p>We have implemented our techniques in our compiler and reported the results in in a 2012 PLDI paper, among other venues. We show how we have obtained excellent results by parallelizing 27 benchmark codes with at least 95% sequential coverage, an unmatched result, so far.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>Complementary to our auto-parallelization effort we have designed and implemented a significant new feature for the C++ language: Concepts. To parallelize, we need information about arguments to algorithms. In ISO C++ 2014, such information is hard to obtain for algorithms (conventionally) expressed as templates because template arguments are unconstrained. We solved this problem be adding a novel and general mechanism for expressing static requirements on template arguments, called "concepts." In 2015, concepts was approved as an ISO Technical specification and our implementation is shipped as part of GCC6.0. A concept constrains the types and values passed as a set of template arguments. Concepts are general first order logic predicates and can be used to ensure that an algorithm is invoked only with arguments that meet critical, but generic, requirements such as &ldquo;the elements are part of a contagious sequence&rdquo;, &ldquo;there are 100,000 elements&rdquo;, and &ldquo;the elements must be floating-point numbers.&rdquo; To complete the effort to provide precise requirements in code, Stroustrup and Dos Reis continue work to add run-time constraints on function arguments, called &ldquo;contracts&rdquo;, to ISO C++.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/27/2016<br>      Modified by: Lawrence&nbsp;Rauchwerger</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    The goal of this research project is to help generate with minimal effort, parallel programs from a sequential specification. For example, we could transform a sequential program into a parallel one using a parallelizing compiler or, we can write an explicitly parallel programs using parallel language constructs or libraries.     In this project we have taken a two track approach: We have introduced  a novel and powerful approach to automatically transform serial programs into parallel ones. We have also developed C++ language features that support and simplify parallel programming.     Our novel approach to automatic parallelization, the Hybrid Analysis, overcomes the previously unsolved static compiler analysis limitations through the use of run-time information.  Our Hybrid Analysis framework summarizes all memory references of interest for each loop using a novel intermediate representation, the uniform set representation (USR). This set representation is then used to verify the data independence of loops, by checking the existence of a solution to the data independence equation, i.e., that the intersection of the write and read memory references sets is empty, which represents the crucial condition for loop level parallelization.  Proving statically that the dependence equation has no solution   is a very hard, sometimes impossible problem. Hence we map it into a sufficient logical (Boolean) equation which is then recursively factored and solved. If static analysis is inconclusive we generate code to evaluate the logical expressions at run-time and then select a serial or parallel version of the corresponding loop. This technique ensures that we execute only the minimal amount of run-time overhead.  It represents a seamless integration of static and dynamic analysis not previously achieved.     We have implemented our techniques in our compiler and reported the results in in a 2012 PLDI paper, among other venues. We show how we have obtained excellent results by parallelizing 27 benchmark codes with at least 95% sequential coverage, an unmatched result, so far.        Complementary to our auto-parallelization effort we have designed and implemented a significant new feature for the C++ language: Concepts. To parallelize, we need information about arguments to algorithms. In ISO C++ 2014, such information is hard to obtain for algorithms (conventionally) expressed as templates because template arguments are unconstrained. We solved this problem be adding a novel and general mechanism for expressing static requirements on template arguments, called "concepts." In 2015, concepts was approved as an ISO Technical specification and our implementation is shipped as part of GCC6.0. A concept constrains the types and values passed as a set of template arguments. Concepts are general first order logic predicates and can be used to ensure that an algorithm is invoked only with arguments that meet critical, but generic, requirements such as "the elements are part of a contagious sequence", "there are 100,000 elements", and "the elements must be floating-point numbers." To complete the effort to provide precise requirements in code, Stroustrup and Dos Reis continue work to add run-time constraints on function arguments, called "contracts", to ISO C++.             Last Modified: 03/27/2016       Submitted by: Lawrence Rauchwerger]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
