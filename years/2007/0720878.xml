<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR/PDOS: Concurrent, Direct Network Access: High-performance, Low-overhead Network I/O Virtualization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2007</AwardEffectiveDate>
<AwardExpirationDate>08/31/2011</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>461988</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Krishna Kant</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The economics of supporting a growing number of Internet-based applications has created a demand for server consolidation, and thus a resurgence of interest in virtualization.  Virtualization systems for commodity hardware virtualize processor, memory, and I/O devices in software.  Although this enables these systems to support a wide range of hardware, it also leads to significant overheads.&lt;br/&gt;&lt;br/&gt;We have developed Concurrent, Direct Network Access (CDNA), a new I/O virtualization architecture combining software and hardware that reduces the overhead of network virtualization.  This architecture provides untrusted virtual machines safe, direct access to the network interface.  While CDNA dramatically improves the efficiency of I/O virtualization, a notable gap between native and virtualized I/O performance still exists.&lt;br/&gt;&lt;br/&gt;This project's objective is to eliminate this performance gap without sacrificing the generality and manageability of software-based I/O virtualization.  This leads to three main thrusts.  First, we are working to mitigate the remaining overheads of the CDNA I/O virtualization architecture, which include memory protection and the scheduling of interrupts.  Second, we are developing mechanisms to support full virtualization, memory protection using IOMMU hardware, and protected DMA by a conventional NIC.  This will improve the generality of the CDNA I/O virtualization architecture.  Finally, we are developing mechanisms to enable migration among systems with and without CDNA and to provide mechanisms for network resource provisioning.  This will facilitate system managament for virtualized servers using the CDNA I/O virtualization architecture.  Altogether, this research will make CDNA a complete I/O virtualization solution that provides efficient, general, and manageable I/O virtualization.</AbstractNarration>
<MinAmdLetterDate>08/22/2007</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0720878</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Cox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan Cox</PI_FULL_NAME>
<EmailAddress>alc@rice.edu</EmailAddress>
<PI_PHON>7133485730</PI_PHON>
<NSF_ID>000277789</NSF_ID>
<StartDate>08/22/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Scott</FirstName>
<LastName>Rixner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Scott Rixner</PI_FULL_NAME>
<EmailAddress>rixner@rice.edu</EmailAddress>
<PI_PHON>7133486353</PI_PHON>
<NSF_ID>000102366</NSF_ID>
<StartDate>08/22/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 MAIN ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~250000</FUND_OBLG>
<FUND_OBLG>2008~200000</FUND_OBLG>
<FUND_OBLG>2009~6000</FUND_OBLG>
<FUND_OBLG>2010~5988</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In many organizations, the economics of supporting a growing number of<br />Internet-based application services has created a demand for server<br />consolidation.&nbsp; Consequently, there has been a resurgence of interest<br />in machine virtualization.&nbsp; A virtual machine monitor (VMM) enables<br />multiple virtual machines, each encapsulating one or more application<br />services, to share the same physical machine safely and fairly.&nbsp; It<br />provides isolation between the virtual machines and manages their<br />access to hardware resources.</p> <p>Modern VMMs for commodity hardware, such as VMWare and Xen, can<br />virtualize processors, memory, and I/O devices in software.&nbsp; Such<br />software-based approaches enable these VMMs to support a wide range of<br />hardware, but they also lead to significant performance overheads.&nbsp; To<br />reduce the performance overhead of processor and memory<br />virtualization, both AMD and Intel have introduced hardware<br />virtualization support that reduces the frequency and duration of<br />calls into the VMM.&nbsp; However, there has been no similar consensus on<br />how to improve the efficiency of I/O virtualization, because existing<br />hardware-based approaches sacrifice the generality and manageability<br />of software-based I/O virtualization.</p> <p>The overall objective of this project has been to advance the state-<br />of-the-art in I/O virtualization.&nbsp; To this end, we have worked both on<br />developing new approaches to I/O virtualization and on developing new<br />mechanisms to support existing approaches.&nbsp; At the same time, we have<br />looked at problems that are common to all of these approaches, such as<br />how I/O virtualization interacts with virtual machine scheduling.</p> <p>* New approaches</p> <p>Several VMMs --- including Xen, L4, and Microsoft Hyper-V --- use the<br />driver domain model to virtualize I/O devices.&nbsp; The driver domain is a<br />virtual machine that runs a largely unmodified operating system.<br />Consequently, it is able to use all of the device drivers that are<br />available for that operating system.&nbsp; This greatly simplifies the<br />complexity of providing support for a wide variety of devices in a<br />virtualized environment.&nbsp; In addition, the driver domain model<br />provides a safe execution environment for physical device drivers,<br />enabling improved fault isolation over alternative models that locate<br />device drivers in the VMM.&nbsp; However, while the driver domain provides<br />several benefits, it also incurs significant performance overheads.<br />Previous work by our collaborators at HP labs has argued that these<br />overheads can be eliminated, in part through the use of a new<br />generation of commodity NICs that support multiple transmit and<br />receive queues for packets.&nbsp; This project has fully realized that<br />vision, contributing a complete design and implementation of<br />multi-queue NIC support for the driver domain model in Xen.</p> <p>* New mechanisms</p> <p>Xen's mechanism for memory sharing and protection, called the grant<br />mechanism, is used to share I/O buffers in guest virtual machines'<br />memory with a driver domain.&nbsp; Previous studies have identified the<br />grant mechanism as a significant source of network I/O overhead in<br />Xen.&nbsp; We have developed a redesigned grant mechanism to significantly<br />reduce the associated overheads.&nbsp; Unlike the original grant mechanism,<br />the new mechanism allows guest domains to unilaterally issue and revoke<br />grants.&nbsp; As a result, the new mechanism makes it simple for the guest<br />OS to reduce the number of grant issue and revoke operations that are<br />needed for I/O by taking advantage of temporal and/or spatial locality<br />in its use of I/O buffers.</p> <p>* Common problems</p> <p>We explored the relationship be...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In many organizations, the economics of supporting a growing number of Internet-based application services has created a demand for server consolidation.  Consequently, there has been a resurgence of interest in machine virtualization.  A virtual machine monitor (VMM) enables multiple virtual machines, each encapsulating one or more application services, to share the same physical machine safely and fairly.  It provides isolation between the virtual machines and manages their access to hardware resources.  Modern VMMs for commodity hardware, such as VMWare and Xen, can virtualize processors, memory, and I/O devices in software.  Such software-based approaches enable these VMMs to support a wide range of hardware, but they also lead to significant performance overheads.  To reduce the performance overhead of processor and memory virtualization, both AMD and Intel have introduced hardware virtualization support that reduces the frequency and duration of calls into the VMM.  However, there has been no similar consensus on how to improve the efficiency of I/O virtualization, because existing hardware-based approaches sacrifice the generality and manageability of software-based I/O virtualization.  The overall objective of this project has been to advance the state- of-the-art in I/O virtualization.  To this end, we have worked both on developing new approaches to I/O virtualization and on developing new mechanisms to support existing approaches.  At the same time, we have looked at problems that are common to all of these approaches, such as how I/O virtualization interacts with virtual machine scheduling.  * New approaches  Several VMMs --- including Xen, L4, and Microsoft Hyper-V --- use the driver domain model to virtualize I/O devices.  The driver domain is a virtual machine that runs a largely unmodified operating system. Consequently, it is able to use all of the device drivers that are available for that operating system.  This greatly simplifies the complexity of providing support for a wide variety of devices in a virtualized environment.  In addition, the driver domain model provides a safe execution environment for physical device drivers, enabling improved fault isolation over alternative models that locate device drivers in the VMM.  However, while the driver domain provides several benefits, it also incurs significant performance overheads. Previous work by our collaborators at HP labs has argued that these overheads can be eliminated, in part through the use of a new generation of commodity NICs that support multiple transmit and receive queues for packets.  This project has fully realized that vision, contributing a complete design and implementation of multi-queue NIC support for the driver domain model in Xen.  * New mechanisms  Xen's mechanism for memory sharing and protection, called the grant mechanism, is used to share I/O buffers in guest virtual machines' memory with a driver domain.  Previous studies have identified the grant mechanism as a significant source of network I/O overhead in Xen.  We have developed a redesigned grant mechanism to significantly reduce the associated overheads.  Unlike the original grant mechanism, the new mechanism allows guest domains to unilaterally issue and revoke grants.  As a result, the new mechanism makes it simple for the guest OS to reduce the number of grant issue and revoke operations that are needed for I/O by taking advantage of temporal and/or spatial locality in its use of I/O buffers.  * Common problems  We explored the relationship between virtual machine scheduling and I/O performance.  Traditionally, VMM schedulers have focused on fairly sharing the processor resources among virtual machines while leaving the scheduling of I/O resources as a secondary concern.  However, this can result in poor and/or unpredictable application performance, making virtualization less desirable for applications that require efficient and consistent I/O behavior.  This project wa...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
