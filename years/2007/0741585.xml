<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SGER:  RI: Text-Based Discriminative Language Modeling</AwardTitle>
<AwardEffectiveDate>09/01/2007</AwardEffectiveDate>
<AwardExpirationDate>02/28/2009</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>105800</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Discriminatively trained conditional models have been applied with great success to language modeling problems for applications such as speech recognition and machine translation (MT), and have been demonstrated to consistently outperform generative modeling approaches.  Unfortunately, in contrast to generative approaches, discriminative modeling is an exclusively supervised approach, requiring costly manually annotated training data.  Yet there is an important difference between language modeling and other natural language processing (NLP) tasks that are modeled discriminatively. For other NLP tasks, sequences of words are the input, and some hidden structure is the output, e.g., parse trees; but for language modeling tasks, word sequences are the output given some input, such as a source language string being translated.  Large text resources that are not paired with an input of interest nevertheless provide examples of well-formed outputs, which would be 'correct' for any inputs that producing it.  The novel perspective in this exploratory proposal is recognizing this fundamental difference between typical NLP tasks, where there may be ample inputs but outputs must be manually annotated, from language modeling, where there are ample outputs with no corresponding input.  This project explores methods for simulating inputs for observed word sequences, and for using these simulated inputs with a particular MT system to produce a set of alternative (confusable) word sequences to the original observed sequence.  These sets of alternative sequences can then be used with conditional/discriminative estimation techniques, despite the fact that no supervision (manual translation of source strings) was required to produce the training data.&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/31/2007</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2008</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0741585</AwardID>
<Investigator>
<FirstName>Izhak</FirstName>
<LastName>Shafran</LastName>
<EmailAddress>zakshafran@gmail.com</EmailAddress>
<StartDate>08/31/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Roark</LastName>
<EmailAddress>roarkbr@gmail.com</EmailAddress>
<StartDate>08/31/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon Health &amp; Science University</Name>
<CityName>Portland</CityName>
<ZipCode>972393098</ZipCode>
<PhoneNumber>5034947784</PhoneNumber>
<StreetAddress>3181 S W Sam Jackson Park Rd</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9237</Code>
<Text>SMALL GRANTS-EXPLORATORY RSRCH</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
