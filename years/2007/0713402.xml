<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Collaborative Research: Discriminative Knowledge-Rich Language Modeling for Machine Translation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2007</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>325216.00</AwardTotalIntnAmount>
<AwardAmount>390214</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates a novel approach for assessing the fluency and&lt;br/&gt;grammaticality of alternative translation hypotheses that are created within&lt;br/&gt;search-based Machine Translation (MT) systems.  This task, commonly termed&lt;br/&gt;"Language Modeling" (LM), has been explored primarily in the context of speech&lt;br/&gt;recognition; however, current state-of-the-art language models (LMs) are not&lt;br/&gt;effective at distinguishing between more fluent grammatical translations and&lt;br/&gt;their poor alternatives.  In contrast, the proposed approach, "Discriminative&lt;br/&gt;Knowledge-Rich Language Modeling" (DKRLM), is explicitly designed to find the&lt;br/&gt;most fluent and grammatical translations within the search space by comparing&lt;br/&gt;the linguistic features of the translation hypotheses against very large&lt;br/&gt;"clean" monolingual corpora. The intuition is that more grammatical&lt;br/&gt;translation hypotheses should contain higher proportions of features seen in&lt;br/&gt;the large corpora.  An important contribution of the project is in exploring&lt;br/&gt;different types of linguistic features to identify those that are most&lt;br/&gt;informative for the comparisons.  Moreover, discriminative training is&lt;br/&gt;performed to incorporate the features into a system-independent scoring&lt;br/&gt;function, replacing traditional LMs in MT systems.  The broader impacts of the&lt;br/&gt;proposed work include both broader adoption for the methodology as well as&lt;br/&gt;wider use of the new DKRLM functions to other search-based NLP applications&lt;br/&gt;that aim at generating fluent grammatical text.  This includes search-based&lt;br/&gt;approaches to Speech Recognition, Natural Language Generation (NLG), Optical&lt;br/&gt;Character Recognition (OCR), Summarization, and others.</AbstractNarration>
<MinAmdLetterDate>08/29/2007</MinAmdLetterDate>
<MaxAmdLetterDate>09/21/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0713402</AwardID>
<Investigator>
<FirstName>Alon</FirstName>
<LastName>Lavie</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alon Lavie</PI_FULL_NAME>
<EmailAddress>alavie@cs.cmu.edu</EmailAddress>
<PI_PHON>4122685655</PI_PHON>
<NSF_ID>000215959</NSF_ID>
<StartDate>08/29/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>PITTSBURGH</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8010</Code>
<Text>Computing in the Cloud</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8010</Code>
<Text>Computing in the Cloud</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~107525</FUND_OBLG>
<FUND_OBLG>2008~106412</FUND_OBLG>
<FUND_OBLG>2009~111279</FUND_OBLG>
<FUND_OBLG>2010~64998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There has been dramatic progress in the accuacy and fluency of fully-automated language translation systems over the past decade. &nbsp;State-of-the-art Machine Translation (MT) has increasingly become an essential technology in the service of governments and global enterprises in recent years. &nbsp;Yet in many instances and application scenarios, the accuracy, and particularly the fluency, of MT is still far from human levels. &nbsp;The main goal of this research project &nbsp;was to explore novel ways for leveraging information about the occurance patterns and likelihood of sequences of words, as they are found in web-scale monolonigual corpora, in order to improve the accuracy and fluency of ssuch MT systems.</p> <p>The first two years of the project focused on constructing a classification framework for identifying ungrammatical and disfluent points in MT-generated translations, based on a variety of linguistic features and on occurrence statistics for 'n-grams' - sequences of words that can be observed in large-scale human-generated text corpora.</p> <p>The second half of the project focused on applying advanced n-gram statistics for improving MT system combination, and investigated the feasibility of incorporating the Microsoft Web-scale N-gram service as an additional source of information within MT system combination. In collaboration with colleagues at the University of Pittsburgh, we also explored identifying and improving translation of "difficult-to-translate" phrases within sentences.</p> <p>The results and outcomes of this project were published in several publications at premier conferences in the field of Computational Linguistics and Natural Language Processing (NLP). &nbsp;</p> <p>In a closely related project, a PhD student partially supported by this project developed KenLM - a new highly optimized language modeling software package for MT and other statsitical language applications. KenLM has quickly become the defacto standard language modeling software toolkit used by the majority of the research and commercial NLP development communities.</p><br> <p>            Last Modified: 06/05/2015<br>      Modified by: Alon&nbsp;Lavie</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There has been dramatic progress in the accuacy and fluency of fully-automated language translation systems over the past decade.  State-of-the-art Machine Translation (MT) has increasingly become an essential technology in the service of governments and global enterprises in recent years.  Yet in many instances and application scenarios, the accuracy, and particularly the fluency, of MT is still far from human levels.  The main goal of this research project  was to explore novel ways for leveraging information about the occurance patterns and likelihood of sequences of words, as they are found in web-scale monolonigual corpora, in order to improve the accuracy and fluency of ssuch MT systems.  The first two years of the project focused on constructing a classification framework for identifying ungrammatical and disfluent points in MT-generated translations, based on a variety of linguistic features and on occurrence statistics for 'n-grams' - sequences of words that can be observed in large-scale human-generated text corpora.  The second half of the project focused on applying advanced n-gram statistics for improving MT system combination, and investigated the feasibility of incorporating the Microsoft Web-scale N-gram service as an additional source of information within MT system combination. In collaboration with colleagues at the University of Pittsburgh, we also explored identifying and improving translation of "difficult-to-translate" phrases within sentences.  The results and outcomes of this project were published in several publications at premier conferences in the field of Computational Linguistics and Natural Language Processing (NLP).    In a closely related project, a PhD student partially supported by this project developed KenLM - a new highly optimized language modeling software package for MT and other statsitical language applications. KenLM has quickly become the defacto standard language modeling software toolkit used by the majority of the research and commercial NLP development communities.       Last Modified: 06/05/2015       Submitted by: Alon Lavie]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
