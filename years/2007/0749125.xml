<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Petaflops Geospace Simulations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2008</AwardEffectiveDate>
<AwardExpirationDate>12/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>1500000.00</AwardTotalIntnAmount>
<AwardAmount>1500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Daniel Katz</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>In recent years a concerted effort to develop General Circulation Models of Earth?s magnetosphere and it?s interaction with the solar wind and Earth?s upper atmosphere has been a major focus of the space sciences community.  Much progress has been made toward this goal, but it has become clear that the computational requirements are very large.  This project will develop a petaflop-capable version of the OpenGGCM (Open Geospace General Circulation Model), which is a coupled community model of the Earth?s magnetosphere, ionosphere, and thermosphere. The computationally challenging part of the OpenGGCM is the MHD magnetosphere core code which, in the current code, is under-resolved and lacks some of the necessary physics to model some of the key aspects of the magnetosphere - solar wind interaction. The petaflop OpenGGCM will overcome these limitations by including optimized code for new hardware, such as the IBM Cell Broadband Engine (CBE). The project will also develop a code library that intelligently reduces and compresses the data output from the simulation. Such I/O reduction becomes necessary for petascale simulations because I/O bandwidth and capacity does not keep pace with CPU performance. The compression technique will use wavelet compression to reduce output data before they are written to storage. This will facilitate the extraction of the optimal amount of information from the simulation. &lt;br/&gt;&lt;br/&gt;This project lays the groundwork for the next generation of geospace simulations, which promise to allow the study of the most fundamental problems of space physics. These will be the first true multiscale simulations that encompass all scales from the global scale of the magnetosphere down to the smallest scales at which the fluid approximation breaks down. The increased model resolution will dramatically reduce numerical effects and will make it possible to study dissipation sensitive phenomena such as turbulence and its effects. The development of I/O reduction algorithms introduces a new approach to mitigate the I/O bottlenecks inherent to petascale systems. &lt;br/&gt;&lt;br/&gt;All code developed during this project will be publicly available. Specifically, the petascale enabled P-OpenGGCM will be made available at the Community Coordinated Modeling Center (CCMC) for runs on demand, and the libraries developed to exploit the CBE architecture and to perform I/O compression will be made publicly available. The OpenGGCM is not just a research tool but also a candidate model for Space Weather prediction, and thus has the potential for a significant societal impact. The model improvements from this project will enhance its value in this capacity significantly. The compression libraries will be generic tools that can be utilized by any petascale simulation using time series data on three-dimensional rectilinear grids. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/04/2008</MinAmdLetterDate>
<MaxAmdLetterDate>01/04/2008</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0749125</AwardID>
<Investigator>
<FirstName>R. Daniel</FirstName>
<LastName>Bergeron</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>R. Daniel Bergeron</PI_FULL_NAME>
<EmailAddress>rdb@cs.unh.edu</EmailAddress>
<PI_PHON>6038623780</PI_PHON>
<NSF_ID>000109253</NSF_ID>
<StartDate>01/04/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joachim</FirstName>
<LastName>Raeder</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joachim Raeder</PI_FULL_NAME>
<EmailAddress>J.Raeder@unh.edu</EmailAddress>
<PI_PHON>6038623412</PI_PHON>
<NSF_ID>000466501</NSF_ID>
<StartDate>01/04/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kai</FirstName>
<LastName>Germaschewski</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kai K Germaschewski</PI_FULL_NAME>
<EmailAddress>kai.germaschewski@unh.eduu</EmailAddress>
<PI_PHON>6038622912</PI_PHON>
<NSF_ID>000488337</NSF_ID>
<StartDate>01/04/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Douglas</FirstName>
<LastName>Larson</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Douglas J Larson</PI_FULL_NAME>
<EmailAddress>douglas.larson@unh.edu</EmailAddress>
<PI_PHON>6038621234</PI_PHON>
<NSF_ID>000085472</NSF_ID>
<StartDate>01/04/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Foulks</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew Foulks</PI_FULL_NAME>
<EmailAddress>rafoulks@cs.unh.edu</EmailAddress>
<PI_PHON>6038621234</PI_PHON>
<NSF_ID>000356680</NSF_ID>
<StartDate>01/04/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of New Hampshire</Name>
<CityName>Durham</CityName>
<ZipCode>038243585</ZipCode>
<PhoneNumber>6038622172</PhoneNumber>
<StreetAddress>51 COLLEGE RD SERVICE BLDG 107</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<StateCode>NH</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NH01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>111089470</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY SYSTEM OF NEW HAMPSHIRE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001765866</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of New Hampshire]]></Name>
<CityName>Durham</CityName>
<StateCode>NH</StateCode>
<ZipCode>038243585</ZipCode>
<StreetAddress><![CDATA[51 COLLEGE RD SERVICE BLDG 107]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NH01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>5750</Code>
<Text>MAGNETOSPHERIC PHYSICS</Text>
</ProgramElement>
<ProgramElement>
<Code>7691</Code>
<Text>PetaApps</Text>
</ProgramElement>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~1500000</FUND_OBLG>
</Award>
</rootTag>
