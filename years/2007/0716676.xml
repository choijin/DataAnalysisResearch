<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CT-ISG Provably Scalable and Robust Peer-to-Peer Systems</AwardTitle>
<AwardEffectiveDate>08/01/2007</AwardEffectiveDate>
<AwardExpirationDate>07/31/2010</AwardExpirationDate>
<AwardAmount>49440</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Mohamed G. Gouda</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Proposal: 0716676&lt;br/&gt;CT-ISG: Provably Scalable and Robust Peer-to-Peer System&lt;br/&gt;Baruch Awerbuch&lt;br/&gt;Johns Hopkins University&lt;br/&gt;&lt;br/&gt;This proposal addresses the following fundamental question: how can one provably learn from the mistakes and successes of others in an extremely adversarial environment?&lt;br/&gt;&lt;br/&gt;Informally, imagine that different agents make irreversible decisions, and ``pay'' for their mistakes. It makes a lot of sense for the agents collectively to avoid making the same mistakes multiple times, rather than just individually. Clearly, leveraging trust or shared taste enables a community of users to be more productive, as it allows them to repeat each other's good decisions while avoiding unnecessary repetition of mistakes. In this case, the job of exploring different options can be ``distributed'' among cooperative users. The question is whether it is possible to design an algorithmic tool for learning from the experience of others while minimizing repeating their mistakes. This is certainly possible if all the agents that can cooperate and trust one another, namely consider the same actions to be mistakes, know about each other.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;However, in general, the above assumptions may be problematic, since reliable agents in the system must be able to divide the job of exploring different options, and the cost of mistakes associated with such exploration, among themselves, without knowing in advance whose advice can be trusted. This appears to be ``mission impossible'' if adversaries are in a majority; yet this is exactly what we will accomplish in this research effort. Notice that with adversarial majority, any attempt to use classic tools in distributed computing such as Byzantine agreement is hopeless. The main idea is that we only strive to optimize the overall performance, rather than perfectly reconstruct the preferences or discover the coalitions of honest users. Informally, this can be stated via the notion of ``regret'': one can analyze their decision in hindsight, determine the best dynamic prescient strategy that they wish they had followed, and estimate the performance gap versus this strategy.&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/01/2007</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2010</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0716676</AwardID>
<Investigator>
<FirstName>Baruch</FirstName>
<LastName>Awerbuch</LastName>
<EmailAddress>baruch@cs.jhu.edu</EmailAddress>
<StartDate>08/01/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
</Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7371</Code>
<Text>CYBER TRUST</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
