<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: CRI: IAD: Development of a Research Infrastructure for the Multithreaded Computing Community Using the Cray Eldorado Platform</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2007</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>55000.00</AwardTotalIntnAmount>
<AwardAmount>45000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Abstract &lt;br/&gt;Proposal #: CNS 07-09140 07-08307 07-08820 &lt;br/&gt;PI(s): Brockman, Jay B. Bader, David A. Gao, Guang R. &lt;br/&gt;Barabasi,Albert-Laszlo;Chawla,Nitesh;Kogge,PeterM. Vetter, Jeffrey S. &lt;br/&gt;Institution: University of Notre Dame Georgia Institute Tech U.Delaware &lt;br/&gt;Notre Dame, IN 46556-5602 Atlanta, GA 30332-0002 Newark, DE 19716-1551 &lt;br/&gt;Proposal #: CNS 07-09385 07-09111 07-09254 &lt;br/&gt;PI(s): Gilbert, John R. Upchurch, Edwin T. Yelick, Katherine A. &lt;br/&gt;Wolski, Richard. &lt;br/&gt;Institution: UC-Santa Barbara California Inst Tech UC-Berkeley &lt;br/&gt;Santa Barbara, CA 93106-2050 Pasadena, CA 91125-0600 Berkeley, CA 94704-5940 &lt;br/&gt;Title: Colla Rsch:IAD:Dev Rsch Infr. for Multithreaded Computing Community Using Cray Eldorado Platform &lt;br/&gt;&lt;br/&gt;Project Proposed: &lt;br/&gt;&lt;br/&gt;This collaborative project, developing a shared infrastructure needed to broaden its impact for developing software to run on the next generation of computer hardware, brings a diverse group of researchers from six universities in a joint effort. The work responds to the trend towards multicore processors where developers envision placing tens to hundreds of cores on a single die, each running multiple threads (in contrast to the currently dominant message-passing architectures resulting from the advent of MPI and Linux clusters). Three objectives are proposed: &lt;br/&gt;. Acquiring computer hardware as a shared community resource capable of efficiently running, in experimental and production modes, complex programs with thousands of threads in shared memory; &lt;br/&gt;. Assembling software infrastructure for developing and measuring performance of programs running on the hardware; and &lt;br/&gt;. Building stronger ties between the people themselves, creating ways for researchers at the partner institutions to collaborate and communicate their findings to the broader community. &lt;br/&gt;The Cray XMT system, scheduled for delivery in 2007 serves as an ideal platform. The second bullet includes algorithms, data sets, libraries, languages, tools, and simulators to evaluate performance of program running on the hardware focusing on applications that benefit from large numbers of threats, massively data intensive, "sparse-graph" problems that are difficult to parallelize using conventional message-passing on clusters. Each university contributes a piece to the infrastructure, using it for support of projects. Sandia National Laboratories has agreed to host the system and provide supplementary funding. Each university will use the Cray XMT system in courses. &lt;br/&gt;&lt;br/&gt;Broader Impacts: The infrastructure measures performance providing a basis for the community to improve sharin, and build strong ties for collaboration and communication. Courses will be created and materials will be made available. Workshops for dissemination of the findings are also planned. &lt;br/&gt; &lt;br/&gt; &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/16/2007</MinAmdLetterDate>
<MaxAmdLetterDate>07/23/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0709254</AwardID>
<Investigator>
<FirstName>Katherine</FirstName>
<LastName>Yelick</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katherine A Yelick</PI_FULL_NAME>
<EmailAddress>yelick@cs.berkeley.edu</EmailAddress>
<PI_PHON>5104952341</PI_PHON>
<NSF_ID>000388740</NSF_ID>
<StartDate>08/16/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>BERKELEY</CityName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress><![CDATA[Sponsored Projects Office]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~5000</FUND_OBLG>
<FUND_OBLG>2008~5000</FUND_OBLG>
<FUND_OBLG>2011~5000</FUND_OBLG>
<FUND_OBLG>2012~30000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to explore the use of <em>multithreading</em> in high performance parallel computing, where multiple threads per processor are used to mask the latency of remote accesses by running a separate thread while the remote operation completes. &nbsp; The Eldorado architecture supports this model at the hardware level, with low overhead thread creating and automatic context switching. &nbsp;The Berkeley team was exploring a particular type of programming known as Partitioned Global Address Space (PGAS) programming and how it can be used on both hardware and software implementations of multithreading. &nbsp; In collaboration with researchers at Lawrence Berkeley National Laboratory, the team explored dynamic load balancing tools for the UPC language and the use of over-provisioning processors (scheduling more than one thread per core) to hide latency on both distributed and shared memory machines. We also developed hierarchical versions of the PGAS languages to support both the hierarchical nature of future &nbsp;systems and the hierarchical parallelism that exists in some applications.&nbsp;</p> <p>The group performed several experiments to better understand the best type of threading mechanims to use on shared memory hardware, including both multicore or multichip shared memory nodes. The first version uses POSIX threads with shared memory for the runtime: this requires an explicit representation of thread-local data, e.g., variables that are local to a thread need to have a copy per processors, even if they are globally scoped, but such variables only have a single shared instance in the POSIX model. The second version uses processes rather than threads, and then adds some form of explicitly allocated shared memory, which is specific to the underlying operating system. In general, the thread version is more portable, but the process version offer better interoperability for message passing (MPI) programs that use a process per core. &nbsp;While threads are generally a lighter weight mechanism than processes, the process implementation often showed better performance than the threaded version. &nbsp;This performance effect was mostly attributable to the differences in synchronization required to access shared resources, such as a network interface: the process version has a higher message injection rate because the locking is handled at a lower level. &nbsp;In addition, processes may have better locality behavior within a shared memory node because they are treated differently by a scheduler and not moved as frequently as threads. &nbsp;This latter effect can be mitigated in the thread version by explicitly pinning threads to cores. &nbsp; Finally, the results show that a particuar form of limited multithreading called overprovisinging, namely running more than one thread per core, shows some of the same performance benefits of the multithreaded hardware.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/27/2014<br>      Modified by: Katherine&nbsp;A&nbsp;Yelick</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to explore the use of multithreading in high performance parallel computing, where multiple threads per processor are used to mask the latency of remote accesses by running a separate thread while the remote operation completes.   The Eldorado architecture supports this model at the hardware level, with low overhead thread creating and automatic context switching.  The Berkeley team was exploring a particular type of programming known as Partitioned Global Address Space (PGAS) programming and how it can be used on both hardware and software implementations of multithreading.   In collaboration with researchers at Lawrence Berkeley National Laboratory, the team explored dynamic load balancing tools for the UPC language and the use of over-provisioning processors (scheduling more than one thread per core) to hide latency on both distributed and shared memory machines. We also developed hierarchical versions of the PGAS languages to support both the hierarchical nature of future  systems and the hierarchical parallelism that exists in some applications.   The group performed several experiments to better understand the best type of threading mechanims to use on shared memory hardware, including both multicore or multichip shared memory nodes. The first version uses POSIX threads with shared memory for the runtime: this requires an explicit representation of thread-local data, e.g., variables that are local to a thread need to have a copy per processors, even if they are globally scoped, but such variables only have a single shared instance in the POSIX model. The second version uses processes rather than threads, and then adds some form of explicitly allocated shared memory, which is specific to the underlying operating system. In general, the thread version is more portable, but the process version offer better interoperability for message passing (MPI) programs that use a process per core.  While threads are generally a lighter weight mechanism than processes, the process implementation often showed better performance than the threaded version.  This performance effect was mostly attributable to the differences in synchronization required to access shared resources, such as a network interface: the process version has a higher message injection rate because the locking is handled at a lower level.  In addition, processes may have better locality behavior within a shared memory node because they are treated differently by a scheduler and not moved as frequently as threads.  This latter effect can be mitigated in the thread version by explicitly pinning threads to cores.   Finally, the results show that a particuar form of limited multithreading called overprovisinging, namely running more than one thread per core, shows some of the same performance benefits of the multithreaded hardware.           Last Modified: 04/27/2014       Submitted by: Katherine A Yelick]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
