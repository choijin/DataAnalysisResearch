<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SGER: Investigating the Utility of Affect Mechanisms in Mixed Human-Robot Teams</AwardTitle>
<AwardEffectiveDate>09/01/2007</AwardEffectiveDate>
<AwardExpirationDate>02/28/2009</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>74999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Paul Yu Oh</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Affect is deeply intertwined with cognition in humans and can influence problem solving and decision making strategies, or evaluations of social situations, among many others.  For robots working with humans in teams this means that being aware of human affect and adapting their behavior based on human expectations about how to respond to human affect might not only lead to more natural interactions, but also improve the performance of human-robot teams.  Currently, there is only one preliminary study that attempts to quantify objectively the effect of robotic affect expression on task performance in a mixed human-robot team.&lt;br/&gt;&lt;br/&gt;This project will build collect further evidence for the utility of using affect mechanisms in robotic architectures.  Specifically, the project will investigate whether selectively using affect modulations of spoken language output generated by the robot in response to human stress due to high cognitive load, detected either in the human voice or via physiological sensors attached to human subjects, can improve the performance of human-robot teams.  Moreover, it will be determined if the outcomes depend on the frequency of interactions as well as the interaction distance, comparing face-to-face interactions with remote interactions via a video link.</AbstractNarration>
<MinAmdLetterDate>08/31/2007</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2008</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0746950</AwardID>
<Investigator>
<FirstName>Matthias</FirstName>
<LastName>Scheutz</LastName>
<EmailAddress>matthias.scheutz@tufts.edu</EmailAddress>
<StartDate>08/31/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7632</Code>
<Text>HUMAN-ROBOT INTERACTION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9237</Code>
<Text>SMALL GRANTS-EXPLORATORY RSRCH</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
