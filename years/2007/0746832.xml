<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:  A Scalable Hierarchical Framework for High-Performance Data Storage</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2008</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>476000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern scientific applications, such as analyzing information from large-scale distributed sensors, climate monitoring, and forecasting environmental impacts, require powerful computing resources and entail managing an ever-growing amount of data. While high-end computer architectures comprising of tens-of-thousands or more processors are becoming a norm in modern High Performance Computing (HPC) systems supporting such applications, this growth in computational power has not been matched by a corresponding improvement in storage and I/O systems. Consequently, there is an increasing gap between storage system performance and computational power of clusters, which poses critical challenges, especially in supporting emerging petascale scientific applications. This research develops a framework for bridging the said performance gap and supporting efficient and reliable data management for HPC. Through innovation, design, development, and deployment of the framework, the investigators improve the I/O performance of modern HPC setups.&lt;br/&gt;The target HPC environments present unique research challenges, namely, maintaining I/O performance with increasing storage capacity, low-cost administration of a large number of resources, high-volume long-distance data transfers, and adapting to the varying I/O demands of applications. This research addresses these challenges in storage management by employing a Scalable Hierarchical Framework for HPC data storage. The framework provides high-performance reliable storage within HPC cluster sites via hierarchical organization of storage resources, decentralized interactions between sites to support high-speed, high-volume data exchange and strategic data placement, and system-wide I/O optimizations. The overall goal is a data storage framework attuned to the needs of modern HPC applications, which mitigates the underlying performance gap between compute resources and the I/O system. This research adopts a holistic approach where all system components interact to yield an efficient data management system for HPC.</AbstractNarration>
<MinAmdLetterDate>01/14/2008</MinAmdLetterDate>
<MaxAmdLetterDate>04/12/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0746832</AwardID>
<Investigator>
<FirstName>Ali</FirstName>
<LastName>Butt</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ali Butt</PI_FULL_NAME>
<EmailAddress>butta@cs.vt.edu</EmailAddress>
<PI_PHON>5402310489</PI_PHON>
<NSF_ID>000288467</NSF_ID>
<StartDate>01/14/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Virginia Polytechnic Institute and State University</Name>
<CityName>BLACKSBURG</CityName>
<ZipCode>240610001</ZipCode>
<PhoneNumber>5402315281</PhoneNumber>
<StreetAddress>Sponsored Programs 0170</StreetAddress>
<StreetAddress2><![CDATA[300 Turner Street NW, Suite 4200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003137015</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VIRGINIA POLYTECHNIC INSTITUTE AND STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003137015</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Virginia Polytechnic Institute and State University]]></Name>
<CityName>BLACKSBURG</CityName>
<StateCode>VA</StateCode>
<ZipCode>240610001</ZipCode>
<StreetAddress><![CDATA[Sponsored Programs 0170]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>4080</Code>
<Text>ADVANCED COMP RESEARCH PROGRAM</Text>
</ProgramElement>
<ProgramElement>
<Code>7352</Code>
<Text>COMPUTING PROCESSES &amp; ARTIFACT</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~124242</FUND_OBLG>
<FUND_OBLG>2009~85725</FUND_OBLG>
<FUND_OBLG>2010~88867</FUND_OBLG>
<FUND_OBLG>2011~92197</FUND_OBLG>
<FUND_OBLG>2012~84969</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="normal">High performance computing (HPC) systems are faced with a deluge from the vast amounts of data that is being processed by the state-of-the-art and emerging petascale scientific computing applications. The goal of this project is to address the storage and I/O challenges arising from such data-intensive operations. We have developed a framework to bridge the performance gap between storage and compute components and support efficient and reliable data management for HPC. We adopted a two-pronged approach: providing high-performance reliable storage within HPC cluster sites via hierarchical organization of distributed storage resources, and enabling decentralized interactions between sites to support high-speed, high-volume data exchange.</p> <p class="normal">&nbsp;</p> <p class="normal">A key contribution of the project is the design and development of tools to optimize the large-volume data transfers in HPC workflows. First, we developed a contributory storage based solution, which enabled HPC centers to offload data to user-provided distributed storage sites. We also developed cloud-enabled techniques for seamless data transfer between HPC centers and users, and offloading data-intensive workloads from the HPC centers to the cloud. Our off-loading approaches exploit the orthogonal bandwidth available between the users and the HPC center and relieve the center from handling I/O-intensive tasks, thus allowing the center to focus on compute-intensive components for which it is better provisioned. Evaluation of our approach using both real deployments as well as simulations demonstrates the feasibility of decentralized offloading; an improvement in the data transfer times by as much as 81.1% for typical HPC workloads was observed.</p> <p class="normal">&nbsp;</p> <p class="normal">Second, we explored the use of solid-state storage devices (SSDs) in designing a novel multi-tiered data staging area that can that then be seamlessly integrated with our offloading system, with the traditional HPC storage stack (e.g., Lustre) as the secondary storage. The novelty of our approach is that we employed SSDs in a limited number of participants that are expected to observe the peak load, thus ensuring economic feasibility.&nbsp; Our evaluation showed that the staging area absorbs application checkpoint data and seamlessly drains the data from various storage tiers to the parallel file system, thereby improving the overall I/O performance. We also extended the work to use adaptive data placement, both across various storage layers of an HPC site and with individual nodes within a site. The evaluation yielded better understanding of using the storage layers, and insights into how to incorporate SSDs into the storage hierarchy.</p> <p class="normal">&nbsp;</p> <p class="normal">Finally, we explored the use of emerging technologies such as accelerators and low-power micro-servers in supporting the HPC I/O stack operations. Specifically, we explored the use of such components in supporting I/O-intensive workloads both for HPC applications as well as the extant cloud programming model, Hadoop. To this end, we designed low-cost GPUs to achieve a flexible, fault-tolerant, and high-performance RAID-6 solution for a parallel file system. We capitalize the resources provided by the file system, such as striping individual files over multiple disks, with the computational power of a GPU to provide flexible and fast parity computation for encoding and rebuilding of degraded RAID arrays. The results demonstrate that leveraging GPUs for I/O support functions, i.e., RAID parity computation, is a feasible approach and can provide an efficient alternative to specialized-hardware-based solutions. The effect would be to reduce the cost of HPC I/O systems, and improve the overall efficiency of the system.</p> <p class="normal">&nbsp;</p> <p class="normal">The work on designing a robus...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[High performance computing (HPC) systems are faced with a deluge from the vast amounts of data that is being processed by the state-of-the-art and emerging petascale scientific computing applications. The goal of this project is to address the storage and I/O challenges arising from such data-intensive operations. We have developed a framework to bridge the performance gap between storage and compute components and support efficient and reliable data management for HPC. We adopted a two-pronged approach: providing high-performance reliable storage within HPC cluster sites via hierarchical organization of distributed storage resources, and enabling decentralized interactions between sites to support high-speed, high-volume data exchange.   A key contribution of the project is the design and development of tools to optimize the large-volume data transfers in HPC workflows. First, we developed a contributory storage based solution, which enabled HPC centers to offload data to user-provided distributed storage sites. We also developed cloud-enabled techniques for seamless data transfer between HPC centers and users, and offloading data-intensive workloads from the HPC centers to the cloud. Our off-loading approaches exploit the orthogonal bandwidth available between the users and the HPC center and relieve the center from handling I/O-intensive tasks, thus allowing the center to focus on compute-intensive components for which it is better provisioned. Evaluation of our approach using both real deployments as well as simulations demonstrates the feasibility of decentralized offloading; an improvement in the data transfer times by as much as 81.1% for typical HPC workloads was observed.   Second, we explored the use of solid-state storage devices (SSDs) in designing a novel multi-tiered data staging area that can that then be seamlessly integrated with our offloading system, with the traditional HPC storage stack (e.g., Lustre) as the secondary storage. The novelty of our approach is that we employed SSDs in a limited number of participants that are expected to observe the peak load, thus ensuring economic feasibility.  Our evaluation showed that the staging area absorbs application checkpoint data and seamlessly drains the data from various storage tiers to the parallel file system, thereby improving the overall I/O performance. We also extended the work to use adaptive data placement, both across various storage layers of an HPC site and with individual nodes within a site. The evaluation yielded better understanding of using the storage layers, and insights into how to incorporate SSDs into the storage hierarchy.   Finally, we explored the use of emerging technologies such as accelerators and low-power micro-servers in supporting the HPC I/O stack operations. Specifically, we explored the use of such components in supporting I/O-intensive workloads both for HPC applications as well as the extant cloud programming model, Hadoop. To this end, we designed low-cost GPUs to achieve a flexible, fault-tolerant, and high-performance RAID-6 solution for a parallel file system. We capitalize the resources provided by the file system, such as striping individual files over multiple disks, with the computational power of a GPU to provide flexible and fast parity computation for encoding and rebuilding of degraded RAID arrays. The results demonstrate that leveraging GPUs for I/O support functions, i.e., RAID parity computation, is a feasible approach and can provide an efficient alternative to specialized-hardware-based solutions. The effect would be to reduce the cost of HPC I/O systems, and improve the overall efficiency of the system.   The work on designing a robust and scalable data storage framework attuned to the needs of modern HPC applications is complete and has resulted in improving the efficiency of HPC storage systems (as discussed above). With the help of our collaborators, some of the services are being adopted in production cl...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
