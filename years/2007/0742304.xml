<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Safety, Security, Rescue, and First Response</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2007</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>465000.00</AwardTotalIntnAmount>
<AwardAmount>515263</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Shashank Priya</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The University of Pennsylvania has joined the multi-university Industry/University Cooperative Research Center for Safety, Security and Rescue Research located at the University of South Florida and the University of Minnesota.  The I/UCRC  will bring together industry, academe, and public sector users together to provide integrative robotics and artificial intelligence solutions in robotics for activities conducted by the police, FBI, FEMA, firefighting, transportation safety, and emergency response to mass casualty-related activities. &lt;br/&gt;&lt;br/&gt;The need for safety, security, and rescue technologies has accelerated in the aftermath of 9/11 and a new research community is forming, as witnessed by the first IEEE Workshop on Safety, Security and Rescue Robotics.   The Center is built upon the knowledge and expertise of multi-disciplinary researchers in computer science, engineering, industrial organization, psychology, public health, and marine sciences at member institutions.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/14/2007</MinAmdLetterDate>
<MaxAmdLetterDate>09/14/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0742304</AwardID>
<Investigator>
<FirstName>R. Vijay</FirstName>
<LastName>Kumar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>R. Vijay Kumar</PI_FULL_NAME>
<EmailAddress>Kumar@seas.upenn.edu</EmailAddress>
<PI_PHON>2158983630</PI_PHON>
<NSF_ID>000280506</NSF_ID>
<StartDate>08/14/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Camillo</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Camillo J Taylor</PI_FULL_NAME>
<EmailAddress>cjtaylor@central.cis.upenn.edu</EmailAddress>
<PI_PHON>2158980376</PI_PHON>
<NSF_ID>000097685</NSF_ID>
<StartDate>08/14/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>08/14/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>George</FirstName>
<LastName>Pappas</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George J Pappas</PI_FULL_NAME>
<EmailAddress>pappasg@seas.upenn.edu</EmailAddress>
<PI_PHON>2158989780</PI_PHON>
<NSF_ID>000156545</NSF_ID>
<StartDate>08/14/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Yim</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Yim</PI_FULL_NAME>
<EmailAddress>yim@grasp.upenn.edu</EmailAddress>
<PI_PHON>2158985269</PI_PHON>
<NSF_ID>000230349</NSF_ID>
<StartDate>08/14/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress><![CDATA[Research Services]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0400000</Code>
<Name>Industry University - Co-op</Name>
</FoaInformation>
<ProgramElement>
<Code>5761</Code>
<Text>IUCRC-Indust-Univ Coop Res Ctr</Text>
</ProgramElement>
<ProgramElement>
<Code>J418</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>K611</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>K650</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1049</Code>
<Text>INDUSTRY-UNIV COOPERATIVE RSCH PROJECTS</Text>
</ProgramReference>
<ProgramReference>
<Code>122E</Code>
<Text>CENTERS: OTHER INDUSTRY-UNIV</Text>
</ProgramReference>
<ProgramReference>
<Code>170E</Code>
<Text>Interagency Agreements</Text>
</ProgramReference>
<ProgramReference>
<Code>5761</Code>
<Text>INDUSTRY/UNIV COOP RES CENTERS</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~70000</FUND_OBLG>
<FUND_OBLG>2008~75000</FUND_OBLG>
<FUND_OBLG>2009~210000</FUND_OBLG>
<FUND_OBLG>2010~23763</FUND_OBLG>
<FUND_OBLG>2011~66500</FUND_OBLG>
<FUND_OBLG>2012~70000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Visual Breadcrumbs for Rescuers:</strong></p> <p>Research on metric or topological mapping has flourished in the past decade producing maps in terms of point clouds, occupancy grids, or graphs of poses and landmarks. While such maps could be used for a robot to traverse the same space they are not appropriate for human-robot interaction or spatial reasoning. Semantic information about objects in the scene is a first step towards enabling reasoning and communication about space.</p> <p>We introduced a new localization method that is achievable using &ldquo;soft&rdquo; detections of objects; these represent the presence of specific objects as a dense likelihood across the entire image without making hard decisions regarding object detection. Such soft detections provide probabilities about being at a specific location on a map. Given a map annotated with objects, we can then establish a particle filter procedure that provides the location of a user based just on soft association of visible objects. We tested it in the 30<sup>th</sup> St Station in Philadelphia from where we show an initial map of probabilities (Fig. 1) about where the user or a robot can be based on RGB camera image input only.</p> <p>&nbsp;</p> <p><strong>Autonomous Robots in Disaster Situations:</strong></p> <p>In collaboration with the Tohuku University at Japan, we designed a field experiment that highlighted the need for heterogeneity. Ground robots do not have the same payload limitations as quadrotors, and they are therefore able to carry larger sensor payloads, maintain tethered communication links, and operate for longer periods of time. However, quadrotors provide mobility and observational capabilities unavailable to ground robots. Hence, to build a rich 3D representation of the environment, we leveraged the advantages of each platform, and in doing so we mitigated the platform limitations.</p> <p>During the experiment, we used three different research platforms (Fig. 2). The first platform is a ground robot equipped with an onboard sensing suite that enables the generation of dense 3D maps. The vehicle is teleoperated through the multifloor environment while simultaneously collecting sensor data. After the operators identify locations in the environment that are inaccessible to the ground platform, a second ground platform equipped with an automated helipad is teleoperated to these locations and carries a quadrotor robot equipped with onboard sensing that is able to remotely open and close the helipad and autonomously take off and land from the helipad.</p> <p>The aerial robot is equipped with a laser scanner and onboard computing for online mapping. It is physically transported by the ground robot to each location of interest, where it autonomously takes off before an operator is able to guide the robot to map or observe these inaccessible regions. Upon completion of the mapping and observation phase, the aerial robot is remotely signaled to autonomously land and close the helipad. The quadrotor is then guided to the next location of interest via the teleoperated ground robot.</p> <p>On-site, we realized that in complex environments like the earthquake-damaged buildings in Sendai, the appearance of the environment drastically changed from its original structure. Our earlier aerial navigation approach that utilized certain assumptions that are specific to man-made indoor environments would have failed and hence we designed a new quadrotor platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer and GPS receiver (Fig. 3). The motivation was to utilize the information from multiple sensors such that even if a subset of the sensors were to fail, the performance of the overall system would not be seriously compromised.</p> <p>We proposed a novel modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors tha...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Visual Breadcrumbs for Rescuers:  Research on metric or topological mapping has flourished in the past decade producing maps in terms of point clouds, occupancy grids, or graphs of poses and landmarks. While such maps could be used for a robot to traverse the same space they are not appropriate for human-robot interaction or spatial reasoning. Semantic information about objects in the scene is a first step towards enabling reasoning and communication about space.  We introduced a new localization method that is achievable using "soft" detections of objects; these represent the presence of specific objects as a dense likelihood across the entire image without making hard decisions regarding object detection. Such soft detections provide probabilities about being at a specific location on a map. Given a map annotated with objects, we can then establish a particle filter procedure that provides the location of a user based just on soft association of visible objects. We tested it in the 30th St Station in Philadelphia from where we show an initial map of probabilities (Fig. 1) about where the user or a robot can be based on RGB camera image input only.     Autonomous Robots in Disaster Situations:  In collaboration with the Tohuku University at Japan, we designed a field experiment that highlighted the need for heterogeneity. Ground robots do not have the same payload limitations as quadrotors, and they are therefore able to carry larger sensor payloads, maintain tethered communication links, and operate for longer periods of time. However, quadrotors provide mobility and observational capabilities unavailable to ground robots. Hence, to build a rich 3D representation of the environment, we leveraged the advantages of each platform, and in doing so we mitigated the platform limitations.  During the experiment, we used three different research platforms (Fig. 2). The first platform is a ground robot equipped with an onboard sensing suite that enables the generation of dense 3D maps. The vehicle is teleoperated through the multifloor environment while simultaneously collecting sensor data. After the operators identify locations in the environment that are inaccessible to the ground platform, a second ground platform equipped with an automated helipad is teleoperated to these locations and carries a quadrotor robot equipped with onboard sensing that is able to remotely open and close the helipad and autonomously take off and land from the helipad.  The aerial robot is equipped with a laser scanner and onboard computing for online mapping. It is physically transported by the ground robot to each location of interest, where it autonomously takes off before an operator is able to guide the robot to map or observe these inaccessible regions. Upon completion of the mapping and observation phase, the aerial robot is remotely signaled to autonomously land and close the helipad. The quadrotor is then guided to the next location of interest via the teleoperated ground robot.  On-site, we realized that in complex environments like the earthquake-damaged buildings in Sendai, the appearance of the environment drastically changed from its original structure. Our earlier aerial navigation approach that utilized certain assumptions that are specific to man-made indoor environments would have failed and hence we designed a new quadrotor platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer and GPS receiver (Fig. 3). The motivation was to utilize the information from multiple sensors such that even if a subset of the sensors were to fail, the performance of the overall system would not be seriously compromised.  We proposed a novel modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at different and varying time intervals, and to provide smooth and globally consistent estimates of position in real time for aut...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
