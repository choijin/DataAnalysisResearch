<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MAJOR:    Collaborative Research:   Modeling Creative and Emotive Improvisation in Theatre Performance</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2008</AwardEffectiveDate>
<AwardExpirationDate>06/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>378364.00</AwardTotalIntnAmount>
<AwardAmount>529894</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project studies creativity in improvisation in both standard theatrical techniques where a script's interpretation, including the physical performance, is improvised by an actor, and "improv theatre" where entire scenes are created by actors in real-time through improvisation.   This research will increase the state of knowledge about improvisation, creativity, and intelligent agent design, as well as contribute meaningfully to theoretical and academic understanding of creative practice in theatre.  In addition to integrating engineering scientific methods with the theory and practice of acting, this research will contribute cognitive and computational models of improvisation, emotion, acting styles, and problem-solving in the context of theatre.  These models will pave the way to the development of more sophisticated synthetic characters, virtual humans, and other intelligent autonomous agents that can interact with humans and with each other for purposes of entertainment, education, and training.</AbstractNarration>
<MinAmdLetterDate>05/12/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/07/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0757567</AwardID>
<Investigator>
<FirstName>Celia</FirstName>
<LastName>Pearce</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Celia Pearce</PI_FULL_NAME>
<EmailAddress>c.pearce@neu.edu</EmailAddress>
<PI_PHON>3108668014</PI_PHON>
<NSF_ID>000077176</NSF_ID>
<StartDate>05/12/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Magerko</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian S Magerko</PI_FULL_NAME>
<EmailAddress>magerko@gatech.edu</EmailAddress>
<PI_PHON>4043850866</PI_PHON>
<NSF_ID>000077563</NSF_ID>
<StartDate>05/12/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Riedl</LastName>
<PI_MID_INIT>O</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark O Riedl</PI_FULL_NAME>
<EmailAddress>riedl@cc.gatech.edu</EmailAddress>
<PI_PHON>4043856450</PI_PHON>
<NSF_ID>000077574</NSF_ID>
<StartDate>05/12/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 NORTH AVE NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7788</Code>
<Text>CreativeIT</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7655</Code>
<Text>ITR-CreativeIT</Text>
</ProgramReference>
<ProgramReference>
<Code>7788</Code>
<Text>CreativeIT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~394614</FUND_OBLG>
<FUND_OBLG>2009~16000</FUND_OBLG>
<FUND_OBLG>2010~87280</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<FUND_OBLG>2012~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project, called <em>The Digital Improv Project</em>, has focused on the study of human co-creativity and improvisation. Improvisational stories are a highly interesting example of human creativity, with performers constantly making creative, collaborative decisions in real-time under severe constraints.&nbsp; We have studied improvisational actors over the course of several years to better understand the socio-cognitive processes that they employ in co-creating stories. We have applied this understanding to the construction of formal artificial intelligence programs that represent our formal understanding.&nbsp; The rationale is that if we can build formal models of the socio-cognitive phenomenon we are examining, then we have a solid understanding of it.</p> <p>Our findings have resulted in better understanding improvisational narrative construction and shared meaning making.&nbsp;&nbsp; The process of building shared meaning in a group is called building a &ldquo;shared mental model&rdquo;.&nbsp; This process is broadly ubiquitous in human co-creative settings.&nbsp; It involves the continuous process of each individual recognizing differences in mental models in a problem solving environment, attempting to reconcile those differences, and monitoring to see if the repair was successful.</p> <p>Our socio-cognitive studies have resulted in the development of multiple AI-based improvisational theatre applications.&nbsp; We have created the &ldquo;Party Quirks&rdquo; installation as a means of representing the process of performing characters in an improvised scene.&nbsp; Users would interact with improvisational characters on a virtual stage via an iPad interface.&nbsp; They would play as the host of a party, trying to understand the quirky characters that each AI actor was portraying by interating with them via share mental models moves.&nbsp; This work was accepted into the 2011 Chicago Improv Festival as a normal &ldquo;troupe entry&rdquo; and demoed for hundreds of attendees at the festival.&nbsp;</p> <p>Our second piece, &ldquo;Three Line Scene,&rdquo; examined how to collaboratively construct the introductory details for a scene with AI actors through movement-based interaction using a Microsoft Kinect.&nbsp; We created an open-ended user experience where the user could suggest different characters and situations with body gestures.&nbsp; The AI actor would sense the movement, reason about its relation to things it knows about the story world (in this case, the Old West), and decide how to proceed to create a shared mental model about the scene.&nbsp; The AI may decide on a single interpretation of user behavior, decide to pick a particular interpretation within an ambiguous set of possible interpretations, or execute a shared mental model move to try to get on the same page with the user.</p> <p><em>The Digital Improv Project</em> has contributed to a better understanding of us creative entities.&nbsp; We have formalized our findings in the form of interactive AI-based installations, providing both a fertile ground for exploring co-creation using different human-computer interfaces (e.g. the iPad and Kinect) and the generation of creative content by human / computer teams. &nbsp;More information about this work, and our subsequent research on human co-creativity, can be found at http://adam.cc.gatech.edu.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/05/2013<br>      Modified by: Brian&nbsp;S&nbsp;Magerko</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNaviga...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project, called The Digital Improv Project, has focused on the study of human co-creativity and improvisation. Improvisational stories are a highly interesting example of human creativity, with performers constantly making creative, collaborative decisions in real-time under severe constraints.  We have studied improvisational actors over the course of several years to better understand the socio-cognitive processes that they employ in co-creating stories. We have applied this understanding to the construction of formal artificial intelligence programs that represent our formal understanding.  The rationale is that if we can build formal models of the socio-cognitive phenomenon we are examining, then we have a solid understanding of it.  Our findings have resulted in better understanding improvisational narrative construction and shared meaning making.   The process of building shared meaning in a group is called building a "shared mental model".  This process is broadly ubiquitous in human co-creative settings.  It involves the continuous process of each individual recognizing differences in mental models in a problem solving environment, attempting to reconcile those differences, and monitoring to see if the repair was successful.  Our socio-cognitive studies have resulted in the development of multiple AI-based improvisational theatre applications.  We have created the "Party Quirks" installation as a means of representing the process of performing characters in an improvised scene.  Users would interact with improvisational characters on a virtual stage via an iPad interface.  They would play as the host of a party, trying to understand the quirky characters that each AI actor was portraying by interating with them via share mental models moves.  This work was accepted into the 2011 Chicago Improv Festival as a normal "troupe entry" and demoed for hundreds of attendees at the festival.   Our second piece, "Three Line Scene," examined how to collaboratively construct the introductory details for a scene with AI actors through movement-based interaction using a Microsoft Kinect.  We created an open-ended user experience where the user could suggest different characters and situations with body gestures.  The AI actor would sense the movement, reason about its relation to things it knows about the story world (in this case, the Old West), and decide how to proceed to create a shared mental model about the scene.  The AI may decide on a single interpretation of user behavior, decide to pick a particular interpretation within an ambiguous set of possible interpretations, or execute a shared mental model move to try to get on the same page with the user.  The Digital Improv Project has contributed to a better understanding of us creative entities.  We have formalized our findings in the form of interactive AI-based installations, providing both a fertile ground for exploring co-creation using different human-computer interfaces (e.g. the iPad and Kinect) and the generation of creative content by human / computer teams.  More information about this work, and our subsequent research on human co-creativity, can be found at http://adam.cc.gatech.edu.          Last Modified: 11/05/2013       Submitted by: Brian S Magerko]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
