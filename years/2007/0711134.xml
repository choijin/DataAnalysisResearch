<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>A National Institute for Computational Sciences to Provide Leading-Edge Computational Support for Breakthrough Science and Engineering Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2007</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>64442171.00</AwardTotalIntnAmount>
<AwardAmount>84527695</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Edward Walker</SignBlockName>
<PO_EMAI>edwalker@nsf.gov</PO_EMAI>
<PO_PHON>7032924863</PO_PHON>
</ProgramOfficer>
<AbstractNarration>0711134: University of Tennessee - Knoxville&lt;br/&gt;PI: Thomas Zacharia&lt;br/&gt;&lt;br/&gt;ABSTRACT&lt;br/&gt;&lt;br/&gt;In this project, the University of Tennessee at Knoxville (UTK) will provide a significant new computing capability to the research community.   It will provide the capability for researchers to tackle large and complex research challenges in a wide range of areas.  In partnership with the Oak Ridge National Laboratory (ORNL), UTK will acquire, deploy and operate a sequence of large, well balanced, high-performance computational resources on behalf of the science and engineering research community.  Initially, a large teraflop/s Cray XT4 system will be deployed.  This will subsequently be upgraded to a Cray Baker system with a peak performance of over one petaflop/s and a large amount of main memory.  These systems will be sited at the Joint Institute for Computational Sciences, a center established by the University of Tennessee and the Oak Ridge National Laboratory (ORNL), housed in a building constructed by the state of Tennessee on the ORNL campus.  The new systems will form part of NSF's TeraGrid high-performance cyberinfrastructure, doubling the computational capacity of the TeraGrid in one year.&lt;br/&gt;&lt;br/&gt;This award will permit investigators across the country to conduct innovative research in a number of areas.  Examples of recent impacts of the TeraGrid's high performance computing resources in research, taken from NSF's "Highlights" database, include:&lt;br/&gt;&lt;br/&gt;. The first atomic level simulation of a life form. These simulations of the satellite tobacco mosaic virus will help scientists determine what factors are important to the virus' structural integrity and how those factors might influence assembly of the virus inside host cells.&lt;br/&gt;. Avalanches, oil spills, thunderstorm fronts, and the dust cloud following a building collapse all generate heavier fluid intrusions into a lighter environment. Mathematical modeling and large scale simulations give engineers the means to study these threedimensional flows, which are frequently immeasurable due to their destructive power.&lt;br/&gt;. Over the last year, scientists have used more than one million CPU hours on TeraGrid systems to optimize the process by which the signals generated by Higgs decay are separated from the potentially overwhelming background noise.&lt;br/&gt;. One of the best tools economists have to account for the vagaries of human decision-making as it affects economic forecasting is the life cycle model. Using a process called "backward induction," for the first time, made it possible to apply massively parallel computing to the life cycle model. TeraGrid systems were used to solve the largest, most realistically specified versions of the life cycle model ever attempted. &lt;br/&gt;. Proteins are the building blocks of the body, and biologists have learned that the myriad ways they function  from fighting off infection and building new bones to storing a memory  depend on the precise details of their 3 D shapes. But determining the shapes of proteins has been a slow and exacting process.  By dramatically accelerating scientific research, modern supercomputers are opening the door to medical advances such as rational drug design.&lt;br/&gt;&lt;br/&gt;This project manifests broader impacts in a number of areas.  &lt;br/&gt;. The project enhances the infrastructure for research and education through the provision of facilities for high-end computing integrated into the TeraGrid.  Integration into the TeraGrid permits the facility to be used, relatively transparently, to provide back-end services to the education portals within the TeraGrid Science and Engineering Gateways. &lt;br/&gt;. The project partners at Oak Ridge National Laboratory, the Texas Advanced Computing Center, and the National Center for Atmospheric Research will collaborate to develop and offer advanced training in high-end computing topics including scaling and performance optimization.  This training program will include in-person training at remote institutions where there are at least ten attendees.  Training sessions will also be provided at large science and engineering workshops and conferences.&lt;br/&gt;. Leveraging funding from a variety of sources, the University of Tennessee at Knoxville will launch a multidisciplinary Intercollegiate Graduate Program in Computational Science that will offer training in computational science to students in a wide range of disciplines.&lt;br/&gt;. Many of the users of the proposed system will be graduate students and post-doctoral researchers working in research groups that use high-end computing in their investigations.  It is anticipated that the computational resources provided by this project will play a role in over one hundred graduate theses.&lt;br/&gt;. The project will work with the Oak Ridge Associated Universities' Council of Minority-Serving Institutions to recruit a diverse group of users from underrepresented groups.&lt;br/&gt;&lt;br/&gt;One of the primary partners in this project, ORNL, operates a portfolio of existing programs aimed at introducing pre-college and college students to the science and engineering uses of highend computing and at broadening the participation of underrepresented groups in science and engineering.  The types of research conducted with the new system will be integrated into these programs.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/28/2007</MinAmdLetterDate>
<MaxAmdLetterDate>09/20/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0711134</AwardID>
<Investigator>
<FirstName>Gregory</FirstName>
<LastName>Peterson</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregory D Peterson</PI_FULL_NAME>
<EmailAddress>gdp@utk.edu</EmailAddress>
<PI_PHON>8659746352</PI_PHON>
<NSF_ID>000473243</NSF_ID>
<StartDate>11/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Zacharia</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Zacharia</PI_FULL_NAME>
<EmailAddress>zachariat@ornl.gov</EmailAddress>
<PI_PHON>8655744897</PI_PHON>
<NSF_ID>000354743</NSF_ID>
<StartDate>09/28/2007</StartDate>
<EndDate>11/27/2012</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName>Knoxville</CityName>
<StateCode>TN</StateCode>
<ZipCode>379163801</ZipCode>
<StreetAddress><![CDATA[1331 CIR PARK DR]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>5740</Code>
<Text>Climate &amp; Large-Scale Dynamics</Text>
</ProgramElement>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramElement>
<Code>7476</Code>
<Text>XD-Extreme Digital</Text>
</ProgramElement>
<ProgramElement>
<Code>7619</Code>
<Text>Innovative HPC</Text>
</ProgramElement>
<ProgramElement>
<Code>7781</Code>
<Text>Leadership-Class Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7476</Code>
<Text>ETF</Text>
</ProgramReference>
<ProgramReference>
<Code>7619</Code>
<Text>EQUIPMENT ACQUISITIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~38131952</FUND_OBLG>
<FUND_OBLG>2008~7788584</FUND_OBLG>
<FUND_OBLG>2009~18528956</FUND_OBLG>
<FUND_OBLG>2011~6497365</FUND_OBLG>
<FUND_OBLG>2012~6725518</FUND_OBLG>
<FUND_OBLG>2014~3874530</FUND_OBLG>
<FUND_OBLG>2015~2980790</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In 2009, the National Institute for Computational Sciences (NICS) delivered the first academic petaflop computer to the NSF community&mdash;a Cray XT5 called <em>Kraken</em>. By the end of 2010, two Cray systems at NICS, <em>Kraken</em> and the 166 TF Cray XT4 <em>Athena</em>, were primary providers of computer time to the TeraGrid, delivering more than 70% of all NSF compute cycles. In 2011 NICS decommissioned <em>Athena</em>, after providing 99% availability and 93% system utilization. In 2014 NICS decommissioned <em>Kraken</em>, and began providing the NSF community access to <em>Darter</em>, a 250 TF Cray XC-30.</p> <p>Kraken was a Cray XT5 consisting of 9,408 compute nodes, each containing two 6-core AMD Istanbul Opteron processors and 16 GB of on-node memory. The resulting 112,896 compute cores delivered 1.17 PF at peak performance with 147 TB of memory. Communications took place over the Cray SeaStar2+ interconnect. A parallel Lustre file system provided 3.3 PB (raw) of short-term data storage.</p> <p>Athena was a Cray XT4 with 4512 compute nodes interconnected with SeaStar, a 3D torus. Each compute node had one four-core AMD Opteron for a total of 18,048 cores. All nodes had 4 Gbytes of memory: 1 Gbyte of memory per core. A parallel Lustre file system provided 100 TB (raw) of short-term data storage.</p> <p>The High Performance Storage System (HPSS) is capable of archiving hundreds of petabytes of data and can be accessed by all major leadership computing platforms. Incoming data was written to disk and later migrated to tape for long term archiving. NICS users stored over 14PB of data in HPSS by the end of the Kraken project.</p> <p>From April 2008 until its decommissioning, <em>Kraken</em> delivered more than 4 billion core-hours of computing to 3.8 million jobs and maintained an average uptime (availability) of 96 percent.</p> <p>&nbsp;The U.S. was world leader in computer simulations from the expansion of the NSF's open-science capability in the mid-1980s until the early 2000s when Japan came forth with a simulator that was an order of magnitude greater than anything the U.S. had installed at the time. However, NSF's <em>Kraken</em>, in combination with the Department of Energy's former <em>Jaguar</em> supercomputer, helped restore the global preeminence of the U.S. in computer simulations. <em>Kraken</em>, together with <em>Jaguar</em>, then the world's fastest computer for open scientific research, made ORNL the most powerful computing complex on the planet, with more than two petaflops of power under one roof.</p> <p>&nbsp;For a period of time, <em>Kraken</em> provided more than 60 percent of the allocated compute cycles in the TeraGrid portfolio of approximately a dozen resource providers. Even to retirement, <em>Kraken</em> remained one of NSF's most highly used systems&mdash;from August 2008 through March 2014, it supplied an average of 43 percent of all allocated compute cycles for TeraGrid/XSEDE. Moreover, users consistently were able to run across the entire machine with high efficiency.</p> <p>&nbsp;Before the deployment of the <em>Blue Waters</em> supercomputer at the University of Illinois, <em>Kraken</em> fulfilled the role of capability computing resource. Capability users are those who effectively use a system up to its limits.</p> <p>&nbsp;NICS, <em>Kraken's</em> managing organization, instituted an innovation to enable the optimal performance of its operational mission of providing the maximum number of total compute cycles to the scientific community while enabling full machine runs for capability users. The innovation, called <em>bimodal scheduling</em>, entails a forced &ldquo;draining&rdquo; of the system on a weekly basis, followed by consecutive full machine runs. Implementation of bimodal scheduling led to utilization of more than 90 percent&mdash;the equivalent of a 300-plus teraflop supercomputer, or several million dollars of compute time a year. Average utilization during the course of <em>Kraken's</em> life was an exceptional 92 percent. Bimodal scheduling was the brainchild of the late Phil Andrews, the first director of NICS.</p> <p><em>Kraken</em> was a series of Cray XT systems that culminated in a 112,896-core XT5 system with a peak performance of 1.17 petaflops (1,174 teraflops), 147 terabytes of memory, and 2.4 petabytes of dedicated formatted disk space. Each evolutionary milestone was delivered on scope, schedule, and budget.</p> <p><em>Kraken</em> entered full production mode on Feb. 2, 2009, with a speed of 607 teraflops, or 607 trillion calculations per second. In the latter part of that year, <em>Kraken</em> became only the fourth open supercomputer ever to perform a petaflop, or 1,000 trillion calculations per second.</p> <p>&nbsp;<em>Kraken</em> held the distinction of being the world's most powerful computer managed by academia and was the third fastest on the Top500 list in November 2009. As of November 2013, it was still ranked number 35.</p> <p>&nbsp;The first academic computer to break the petaflop barrier of more than a quadrillion floating-point operations per second, <em>Kraken</em> enabled researchers in myriad scientific and engineering domains&mdash;from physics to molecular biology, atmospheric sciences, climate, and many others&mdash;to achieve advances that prior academic computing resources lacked the power to support.</p><br> <p>            Last Modified: 12/06/2017<br>      Modified by: Gregory&nbsp;D&nbsp;Peterson</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512581919966_kraken-picture--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512581919966_kraken-picture--rgov-800width.jpg" title="Kraken: World's First Academic Petascale Supercomputer"><img src="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512581919966_kraken-picture--rgov-66x44.jpg" alt="Kraken: World's First Academic Petascale Supercomputer"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Kraken, a Cray XT5 supercomputer, delivered 1.17PF</div> <div class="imageCredit">NICS</div> <div class="imageSubmitted">Gregory&nbsp;D&nbsp;Peterson</div> <div class="imageTitle">Kraken: World's First Academic Petascale Supercomputer</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582135353_Kraken-configuration--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582135353_Kraken-configuration--rgov-800width.jpg" title="Kraken System Configuration Over Time"><img src="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582135353_Kraken-configuration--rgov-66x44.jpg" alt="Kraken System Configuration Over Time"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Kraken Grew from a 40-cabinet 38.6TF Cray XT-3 to a 100-cabinet 1.17PF Cray XT-5</div> <div class="imageCredit">NICS</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Gregory&nbsp;D&nbsp;Peterson</div> <div class="imageTitle">Kraken System Configuration Over Time</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582286576_Kraken-overview--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582286576_Kraken-overview--rgov-800width.jpg" title="Kraken Enabled Leadership Computing to the NSF Community"><img src="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582286576_Kraken-overview--rgov-66x44.jpg" alt="Kraken Enabled Leadership Computing to the NSF Community"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Kraken System Provided a Significant Fraction of the Available Computing Cycles to the National Computational Science Community</div> <div class="imageCredit">NICS</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Gregory&nbsp;D&nbsp;Peterson</div> <div class="imageTitle">Kraken Enabled Leadership Computing to the NSF Community</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582411022_Kraken-usage--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582411022_Kraken-usage--rgov-800width.jpg" title="Kraken Impacted Science"><img src="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582411022_Kraken-usage--rgov-66x44.jpg" alt="Kraken Impacted Science"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Kraken Delivered Over 4 Billion Hours to NSF Science Research</div> <div class="imageCredit">NICS</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Gregory&nbsp;D&nbsp;Peterson</div> <div class="imageTitle">Kraken Impacted Science</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582538754_kraken-productivity--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582538754_kraken-productivity--rgov-800width.jpg" title="Kraken Enabled Scientific Productivity"><img src="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512582538754_kraken-productivity--rgov-66x44.jpg" alt="Kraken Enabled Scientific Productivity"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Kraken Impacted a Broad Spectrum of Users and Projects by Delivering 4 Billion Compute Hours to 3.8 Million Jobs</div> <div class="imageCredit">NICS</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Gregory&nbsp;D&nbsp;Peterson</div> <div class="imageTitle">Kraken Enabled Scientific Productivity</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512583096174_kraken-operations--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512583096174_kraken-operations--rgov-800width.jpg" title="Kraken Operational Excellence"><img src="/por/images/Reports/POR/2017/0711134/0711134_10136505_1512583096174_kraken-operations--rgov-66x44.jpg" alt="Kraken Operational Excellence"></a> <div class="imageCaptionContainer"> <div class="imageCaption">NICS Operational Excellence Resulted in Outstanding Kraken Uptime and Utilization</div> <div class="imageCredit">NICS</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Gregory&nbsp;D&nbsp;Peterson</div> <div class="imageTitle">Kraken Operational Excellence</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In 2009, the National Institute for Computational Sciences (NICS) delivered the first academic petaflop computer to the NSF community&mdash;a Cray XT5 called Kraken. By the end of 2010, two Cray systems at NICS, Kraken and the 166 TF Cray XT4 Athena, were primary providers of computer time to the TeraGrid, delivering more than 70% of all NSF compute cycles. In 2011 NICS decommissioned Athena, after providing 99% availability and 93% system utilization. In 2014 NICS decommissioned Kraken, and began providing the NSF community access to Darter, a 250 TF Cray XC-30.  Kraken was a Cray XT5 consisting of 9,408 compute nodes, each containing two 6-core AMD Istanbul Opteron processors and 16 GB of on-node memory. The resulting 112,896 compute cores delivered 1.17 PF at peak performance with 147 TB of memory. Communications took place over the Cray SeaStar2+ interconnect. A parallel Lustre file system provided 3.3 PB (raw) of short-term data storage.  Athena was a Cray XT4 with 4512 compute nodes interconnected with SeaStar, a 3D torus. Each compute node had one four-core AMD Opteron for a total of 18,048 cores. All nodes had 4 Gbytes of memory: 1 Gbyte of memory per core. A parallel Lustre file system provided 100 TB (raw) of short-term data storage.  The High Performance Storage System (HPSS) is capable of archiving hundreds of petabytes of data and can be accessed by all major leadership computing platforms. Incoming data was written to disk and later migrated to tape for long term archiving. NICS users stored over 14PB of data in HPSS by the end of the Kraken project.  From April 2008 until its decommissioning, Kraken delivered more than 4 billion core-hours of computing to 3.8 million jobs and maintained an average uptime (availability) of 96 percent.   The U.S. was world leader in computer simulations from the expansion of the NSF's open-science capability in the mid-1980s until the early 2000s when Japan came forth with a simulator that was an order of magnitude greater than anything the U.S. had installed at the time. However, NSF's Kraken, in combination with the Department of Energy's former Jaguar supercomputer, helped restore the global preeminence of the U.S. in computer simulations. Kraken, together with Jaguar, then the world's fastest computer for open scientific research, made ORNL the most powerful computing complex on the planet, with more than two petaflops of power under one roof.   For a period of time, Kraken provided more than 60 percent of the allocated compute cycles in the TeraGrid portfolio of approximately a dozen resource providers. Even to retirement, Kraken remained one of NSF's most highly used systems&mdash;from August 2008 through March 2014, it supplied an average of 43 percent of all allocated compute cycles for TeraGrid/XSEDE. Moreover, users consistently were able to run across the entire machine with high efficiency.   Before the deployment of the Blue Waters supercomputer at the University of Illinois, Kraken fulfilled the role of capability computing resource. Capability users are those who effectively use a system up to its limits.   NICS, Kraken's managing organization, instituted an innovation to enable the optimal performance of its operational mission of providing the maximum number of total compute cycles to the scientific community while enabling full machine runs for capability users. The innovation, called bimodal scheduling, entails a forced "draining" of the system on a weekly basis, followed by consecutive full machine runs. Implementation of bimodal scheduling led to utilization of more than 90 percent&mdash;the equivalent of a 300-plus teraflop supercomputer, or several million dollars of compute time a year. Average utilization during the course of Kraken's life was an exceptional 92 percent. Bimodal scheduling was the brainchild of the late Phil Andrews, the first director of NICS.  Kraken was a series of Cray XT systems that culminated in a 112,896-core XT5 system with a peak performance of 1.17 petaflops (1,174 teraflops), 147 terabytes of memory, and 2.4 petabytes of dedicated formatted disk space. Each evolutionary milestone was delivered on scope, schedule, and budget.  Kraken entered full production mode on Feb. 2, 2009, with a speed of 607 teraflops, or 607 trillion calculations per second. In the latter part of that year, Kraken became only the fourth open supercomputer ever to perform a petaflop, or 1,000 trillion calculations per second.   Kraken held the distinction of being the world's most powerful computer managed by academia and was the third fastest on the Top500 list in November 2009. As of November 2013, it was still ranked number 35.   The first academic computer to break the petaflop barrier of more than a quadrillion floating-point operations per second, Kraken enabled researchers in myriad scientific and engineering domains&mdash;from physics to molecular biology, atmospheric sciences, climate, and many others&mdash;to achieve advances that prior academic computing resources lacked the power to support.       Last Modified: 12/06/2017       Submitted by: Gregory D Peterson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
