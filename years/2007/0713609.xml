<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC:  Egocentric Depth Perception in Augmented Reality</AwardTitle>
<AwardEffectiveDate>10/01/2007</AwardEffectiveDate>
<AwardExpirationDate>09/30/2011</AwardExpirationDate>
<AwardTotalIntnAmount>392268.00</AwardTotalIntnAmount>
<AwardAmount>392268</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Augmented reality (AR) is a technology where computer displays add (superimpose) computer-generated, graphical objects to a user's view of the physical (real) world.  AR is distinguished from the better-known virtual reality (VR), wherein an observer sees an entirely computer-generated graphical scene.  AR makes possible visualization techniques that have no real-world equivalent; one such technique is x-ray vision, where AR users perceive objects which are located behind solid, opaque surfaces.  In this project the PI will empirically study how egocentric depth perception (the distance from an observer to an object) operates in AR.  The PI will conduct a series of experiments, in which observers judge the depth of AR-presented virtual objects.  These experiments will use two different categories of dependent measures: visually directed actions, and application-based tasks.&lt;br/&gt;&lt;br/&gt;A commonly used visually directed action is blind walking, where observers view a target, cover their eyes, and then walk to the target location without sight.  There are good theoretical arguments that visually directed actions measure a relatively pure percept of egocentric distance, uncontaminated by observers' cognitive knowledge.  Furthermore, there is a substantial body of empirical data that describes visually directed action distance judgments of both real-world objects, and virtual objects viewed with VR display devices.  These data indicate that, under full-cue conditions, real-world objects are judged without systematic error up to ~20 meters, while the distance of VR objects is systematically underestimated (a phenomenon which has been studied extensively but not yet fully explained).&lt;br/&gt;&lt;br/&gt;The application-based tasks are motivated by compelling applications of AR technology. One such task is perceptual matching, where the depth of a virtual and a real object are matched; this task is an important component of AR applications in medicine and image-assisted surgery, AR situation awareness in urban settings and buildings, and others.  Another such task is forced choice, where the depth of a virtual object is placed into one of a small number of categories relative to other objects.  This task is motivated by applications such as an AR airport control tower and an AR urban situation awareness system, where observers must make decisions based on the gross spatial arrangement of virtual and real objects.&lt;br/&gt;&lt;br/&gt;Although the egocentric depth perception of real-world objects and VR-presented virtual objects has been widely studied, currently there exists very little empirical data on the issue, an absence this research will correct.  Furthermore, because the present studies will use two complimentary categories of dependent measures, they will allow measuring the degree to which phenomena such as the VR underestimation effect, which has been found by visually directed action tasks, is also present in qualitatively different dependent measures.  This will help resolve controversial questions regarding the degree to which such phenomena arise from the choice of dependent measure versus deeper perceptual mechanisms.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  In an applied context, a better understanding of how AR depth perception operates is necessary for many compelling AR applications to be realized, and the empirical data gathered through this activity will hasten AR application development.  In addition, through this activity a series of students will receive a blend of experience in both computer graphics and human-subject empirical methods; upon graduation these students will be well-positioned to contribute to the important emerging research area of applied perception in computer graphics.</AbstractNarration>
<MinAmdLetterDate>08/06/2007</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2009</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0713609</AwardID>
<Investigator>
<FirstName>J. Edward</FirstName>
<LastName>Swan II</LastName>
<EmailAddress>swan@cse.msstate.edu</EmailAddress>
<StartDate>08/06/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Mississippi State University</Name>
<CityName>MISSISSIPPI STATE</CityName>
<ZipCode>397629662</ZipCode>
<PhoneNumber>6623257404</PhoneNumber>
<StreetAddress>PO Box 6156</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Mississippi</StateName>
<StateCode>MS</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
