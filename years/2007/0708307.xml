<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: CRI: IAD: Development of a Research Infrastructure</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2007</AwardEffectiveDate>
<AwardExpirationDate>07/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Abstract &lt;br/&gt;Proposal #: CNS 07-09140 07-08307 07-08820 &lt;br/&gt;PI(s): Brockman, Jay B. Bader, David A. Gao, Guang R. &lt;br/&gt;Barabasi,Albert-Laszlo;Chawla,Nitesh;Kogge,PeterM. Vetter, Jeffrey S. &lt;br/&gt;Institution: University of Notre Dame Georgia Institute Tech U.Delaware &lt;br/&gt;Notre Dame, IN 46556-5602 Atlanta, GA 30332-0002 Newark, DE 19716-1551 &lt;br/&gt;Proposal #: CNS 07-09385 07-09111 07-09254 &lt;br/&gt;PI(s): Gilbert, John R. Upchurch, Edwin T. Yelick, Katherine A. &lt;br/&gt;Wolski, Richard. &lt;br/&gt;Institution: UC-Santa Barbara California Inst Tech UC-Berkeley &lt;br/&gt;Santa Barbara, CA 93106-2050 Pasadena, CA 91125-0600 Berkeley, CA 94704-5940 &lt;br/&gt;Title: Colla Rsch:IAD:Dev Rsch Infr. for Multithreaded Computing Community Using Cray Eldorado Platform &lt;br/&gt;&lt;br/&gt;Project Proposed: &lt;br/&gt;&lt;br/&gt;This collaborative project, developing a shared infrastructure needed to broaden its impact for developing software to run on the next generation of computer hardware, brings a diverse group of researchers from six universities in a joint effort. The work responds to the trend towards multicore processors where developers envision placing tens to hundreds of cores on a single die, each running multiple threads (in contrast to the currently dominant message-passing architectures resulting from the advent of MPI and Linux clusters). Three objectives are proposed: &lt;br/&gt;. Acquiring computer hardware as a shared community resource capable of efficiently running, in experimental and production modes, complex programs with thousands of threads in shared memory; &lt;br/&gt;. Assembling software infrastructure for developing and measuring performance of programs running on the hardware; and &lt;br/&gt;. Building stronger ties between the people themselves, creating ways for researchers at the partner institutions to collaborate and communicate their findings to the broader community. &lt;br/&gt;The Cray XMT system, scheduled for delivery in 2007 serves as an ideal platform. The second bullet includes algorithms, data sets, libraries, languages, tools, and simulators to evaluate performance of program running on the hardware focusing on applications that benefit from large numbers of threats, massively data intensive, "sparse-graph" problems that are difficult to parallelize using conventional message-passing on clusters. Each university contributes a piece to the infrastructure, using it for support of projects. Sandia National Laboratories has agreed to host the system and provide supplementary funding. Each university will use the Cray XMT system in courses. &lt;br/&gt;&lt;br/&gt;Broader Impacts: The infrastructure measures performance providing a basis for the community to improve sharin, and build strong ties for collaboration and communication. Courses will be created and materials will be made available. Workshops for dissemination of the findings are also planned. &lt;br/&gt; &lt;br/&gt; &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/16/2007</MinAmdLetterDate>
<MaxAmdLetterDate>05/06/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0708307</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Bader</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David A Bader</PI_FULL_NAME>
<EmailAddress>bader@njit.edu</EmailAddress>
<PI_PHON>9735962654</PI_PHON>
<NSF_ID>000206826</NSF_ID>
<StartDate>08/16/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Vetter</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey S Vetter</PI_FULL_NAME>
<EmailAddress>vetter@tennessee.edu</EmailAddress>
<PI_PHON>8659743461</PI_PHON>
<NSF_ID>000246878</NSF_ID>
<StartDate>08/16/2007</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 NORTH AVE NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2007~5000</FUND_OBLG>
<FUND_OBLG>2008~5000</FUND_OBLG>
<FUND_OBLG>2009~5000</FUND_OBLG>
<FUND_OBLG>2010~5000</FUND_OBLG>
<FUND_OBLG>2011~30000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div><span style="font-family: Times New Roman; font-size: x-small;"><span style="font-family: Times New Roman; font-size: x-small;"> <div>On this research project, we acquired a Cray XMT massively multithreaded supercomputing platform, and performed basic research on developing algorithms for big data challenges.</div> <div>Generalizing k-Betweenness Centrality Using Short Paths and a Parallel Multithreaded Implementation:</div> <div>We present a new parallel algorithm that extends and generalizes the traditional graph analysis metric of betweenness centrality to include additional non-shortest paths according to an input parameter k. Betweenness centrality is a useful kernel for analyzing the importance of vertices or edges in a graph and has found uses in social networks, biological networks, and power grids, among others. k-betweenness centrality captures the additional information provided by paths whose length is within k units of the shortest path length. These additional paths provide robustness that is not captured in traditional betweenness centrality computations, and they may become important shortest paths if key edges are missing in the data. We implement our parallel algorithm using lock-free methods on a massively multithreaded Cray XMT. We apply this implementation to a real-world data set of pages on the World Wide Web and show the importance of the additional data incorporated by our algorithm.</div> <div>Massive Streaming Data Analytics: A Case Study with Clustering Coefficients:</div> <div>We present a new approach for parallel massive graph analysis of streaming, temporal data with a dynamic and extensible representation. Handling the constant stream of new data from health care, security, business, and social network applications requires new algorithms and data structures. We examine data structure and algorithm trade-offs that extract the parallelism necessary for high-performance updating analysis of massive graphs. Static analysis kernels often rely on storing input data in a specific structure. Maintaining these structures for each possible kernel with high data rates incurs a significant performance cost. A case study computing clustering coefficients on a general-purpose data structure demonstrates incremental updates can be more efficient than global recomputation. Within this kernel, we compare three methods for dynamically updating local clustering coefficients: a brute-force local recalculation, a sorting algorithm, and our new approximation method using a Bloom filter. On 32 processors of a Cray XMT with a synthetic scale-free graph of&nbsp;16 million vertices and 537 million edges, the brute-force method processes a mean of over 50,000 updates per second and our Bloom filter approaches 200,000 updates per second.</div> </span></span></div><br> <p>            Last Modified: 08/13/2012<br>      Modified by: David&nbsp;A&nbsp;Bader</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ On this research project, we acquired a Cray XMT massively multithreaded supercomputing platform, and performed basic research on developing algorithms for big data challenges. Generalizing k-Betweenness Centrality Using Short Paths and a Parallel Multithreaded Implementation: We present a new parallel algorithm that extends and generalizes the traditional graph analysis metric of betweenness centrality to include additional non-shortest paths according to an input parameter k. Betweenness centrality is a useful kernel for analyzing the importance of vertices or edges in a graph and has found uses in social networks, biological networks, and power grids, among others. k-betweenness centrality captures the additional information provided by paths whose length is within k units of the shortest path length. These additional paths provide robustness that is not captured in traditional betweenness centrality computations, and they may become important shortest paths if key edges are missing in the data. We implement our parallel algorithm using lock-free methods on a massively multithreaded Cray XMT. We apply this implementation to a real-world data set of pages on the World Wide Web and show the importance of the additional data incorporated by our algorithm. Massive Streaming Data Analytics: A Case Study with Clustering Coefficients: We present a new approach for parallel massive graph analysis of streaming, temporal data with a dynamic and extensible representation. Handling the constant stream of new data from health care, security, business, and social network applications requires new algorithms and data structures. We examine data structure and algorithm trade-offs that extract the parallelism necessary for high-performance updating analysis of massive graphs. Static analysis kernels often rely on storing input data in a specific structure. Maintaining these structures for each possible kernel with high data rates incurs a significant performance cost. A case study computing clustering coefficients on a general-purpose data structure demonstrates incremental updates can be more efficient than global recomputation. Within this kernel, we compare three methods for dynamically updating local clustering coefficients: a brute-force local recalculation, a sorting algorithm, and our new approximation method using a Bloom filter. On 32 processors of a Cray XMT with a synthetic scale-free graph of 16 million vertices and 537 million edges, the brute-force method processes a mean of over 50,000 updates per second and our Bloom filter approaches 200,000 updates per second.        Last Modified: 08/13/2012       Submitted by: David A Bader]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
