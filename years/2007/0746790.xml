<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Co-analysis of Signal and Sense for Understanding Non-verbal Communications and their Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2008</AwardEffectiveDate>
<AwardExpirationDate>04/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>494919.00</AwardTotalIntnAmount>
<AwardAmount>494919</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The objective of this research is to advance our understanding of prosodic relationships and their synchronizations between verbal and nonverbal communication modes.  Prosody and kinesics such as hand gestures, head nods, facial expressions, and posture, play a crucial role in everyday communication by adding expressiveness, as well as by structuring information.  Although there are different types and various levels of synchronization across these modalities, their exact mapping remains unclear.  Whereas cross-modal synchronization between verbal and nonverbal modalities has been explored mostly at the semantic and discourse levels to date, in this work the PI will focus on the interplay between them at various levels of granularity.  In particular, co-analyses of speech, using language and discourse models, with kinesics will be used to uncover prosodic correspondences which, in turn, will be used to develop novel algorithms for modeling dialog acts, emotions and other kinesics.  The two primary goals of the project are to develop computational methods and software tools to iteratively uncover prosodic relationships between nonverbal and verbal behaviors, and to use derived prosodic relationships and their synchronizations to develop novel computational methods and software tools for the robust recognition of gestures, facial expressions, emotions, head nods, and dialog acts.  The PI hopes to thereby lay the foundation for a framework for co-analysis of multimodal articulations to obtain a deeper understanding of (a) how the nucleus of an utterance and visual prosody interact to render the intent of the utterance, and (b) how synchronization with other modalities affects the production of multimodal co-articulation.  To further improve the robustness and recognition accuracies, a set of classifiers will be designed and fused by taking into account the diversity among them. Systematic methods will be developed to evaluate the classifiers using various performance metrics (i.e., precision, recall, F-measures), graphical analyses and measure functions.  The outcomes of the research, together with the PI's prior work, will ultimately enable the development of a perceptual interface for AutoTutor (an artificially intelligent web-based tutoring system), providing a natural means to interact with multimedia contents for instruction.&lt;br/&gt;&lt;br/&gt;Broader Impact:  The results of this research will have profound impact on the understanding and tracking of multimodal communications in humans and agents.  The interplay between the complementary modalities and prosodic manifestations of their synchronization will also broaden the understanding of multi-channel communications in cognitive science, discourse processing, linguistics, and human-machine interaction, which will enable the development of innovative applications such as collaborative environments for agents and humans, and assistive technologies for the elderly and disabled.  The long-term vision of the proposed research is to develop a perceptual interface for web-based tutoring systems such as AutoTutor.  Use of an enhanced artificially intelligent web-based tutor offers significant opportunities for improving the math and science preparation of incoming engineering and science undergraduates of the Memphis City Schools and other regional or national clients.  The PI will also create an online collaborative learning environment, using newer frameworks such as Web 2.0, to organize a massive amount of digital contents in such a way that communities of learners can effectively share and co-manage the information.  The software and databases developed as part of this project will be made available to other researchers through the project website.</AbstractNarration>
<MinAmdLetterDate>05/02/2008</MinAmdLetterDate>
<MaxAmdLetterDate>04/30/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0746790</AwardID>
<Investigator>
<FirstName>Mohammed</FirstName>
<LastName>Yeasin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mohammed Yeasin</PI_FULL_NAME>
<EmailAddress>myeasin@memphis.edu</EmailAddress>
<PI_PHON>9016784078</PI_PHON>
<NSF_ID>000484375</NSF_ID>
<StartDate>05/02/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Memphis</Name>
<CityName>Memphis</CityName>
<ZipCode>381523370</ZipCode>
<PhoneNumber>9016783251</PhoneNumber>
<StreetAddress>Administration 315</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>055688857</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MEMPHIS, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>878135631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Memphis]]></Name>
<CityName>Memphis</CityName>
<StateCode>TN</StateCode>
<ZipCode>381523370</ZipCode>
<StreetAddress><![CDATA[Administration 315]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~94751</FUND_OBLG>
<FUND_OBLG>2009~97214</FUND_OBLG>
<FUND_OBLG>2010~99761</FUND_OBLG>
<FUND_OBLG>2011~102274</FUND_OBLG>
<FUND_OBLG>2012~100919</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Text"><span style="white-space: pre;"> </span>This report highlights the major activities, specific objectives significant results, and key achievements of the Year 2013-2014.&nbsp; A number of new initiatives were taken to solve some of the critical problems in meeting the stated objective. In particular, we have strived to: (<em>i) Assistive Technology for People who are Blind or Visually Impaired, (ii) Modeling of Cognitive Ability-Demand Gaps in Collaborative Sense-making to develop Assistive Technology Solutions, (iii) Semi-automated system to annotate large audio-visual database, (iii) develop software system to co-analyze features for both the audio and video data, (iv) Modality and Task Independent Cognitive Load for Assistive Technology and (v) Tool for Integration and Mining of Big Data in Biomedicine.</em> Finding of this research is being published in well-reputed journals and conferences and many of the research outcomes are directly applied to the NSF RESSEE proposal entitled &ldquo;Contextual Research-Empirical Research--Detecting, Tracking, and Modeling Cognitive, Affective, and Meta-cognitive Regulatory Processes to Optimize Learning with MetaTutor, where Dr. Yeasin is Co-PI.</p> <p class="Text"><span style="white-space: pre;"> </span>During the period (05/01/2013 to 04/30/2014) several new initiatives were taken to build on top of the prior years research accomplishments. The PI and the research team made significant effort in developing assistive technology solutions for people who are blind or visually impaired. Culminating research effort in this period produces: (i) Five (5) Journal paper, (ii) Ten (10) Peer reviewed conference papers, (iii) Two (2) Journal abstract, (iv) Two (2) Ph.D. Dissertation and (v) Two (2) MS Thesis.&nbsp;</p> <p><strong>List of Publications during the reporting period (May 2013 &ndash;April 2014)</strong>:</p> <p><strong>Journal Papers:</strong></p> <ol> <li>AKM Mahbubur Rahman, ASM Iftekhar Anam and Mohammed Yeasin, &ldquo;EmoAssist: Emotion Enabled Assistive Tool to Enhance Dyadic Conversation for the Visually Impaired, IEEE Transaction on Affective Computing, 2014 (in Press).</li> <li>AKM Mahbubur Rahman, and Mohammed Yeasin,"A Unified Framework for Dividing and Predicting a Large Set of Action Units", IEEE Transaction on Affective Computing, 2014 (In Press).</li> <li>&nbsp;M. Iftekhar Tanveer, A.S.M. Iftekhar Anam and Mohammed Yeasin, &ldquo;Designing a Technology for the Blind Users to Understand Others Facial Expressions,&rdquo; ACM Transaction of Accessible Computing (TACCESS), 2014 (in Press).</li> <li>G. Hossain and M. Yeasin,"Cognitive Ability-Demand Gap Analysis with Latent Response Models", IEEE Transactions on ACCESS, EEE (DOI) - 10.1109/ACCESS.2014.2339328, 2014. </li> <li>G. Hossain and M. Yeasin,"Assistive Thinking: Integrating System Dynamics into Design Thinking Approach", IEEE Journal of Systems, 2014 (<em>in Press</em>).</li> </ol> <p><strong>Conference Papers</strong>:</p> <ol> <li>Expression: A Dyadic ConversationAid using Google Glass for Peoplewith Visual Impairments, Anam, ASM Iftekhar and Alam, Shahinur and Yeasin, To appear in ACM Ubiquitous computing 2014 Adjunct Publicaiton, Seattle, WA, Sept 13-17</li> <li>M. Iftekhar Tanveer, A.S.M. Iftekhar Anam, Mohammed Yeasin and Majid Khan, &ldquo;Do You See What I See? Designing a Sensory Substitution Device to Access Non-verbal Modes of Communication&rdquo;, To appear in proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS &rsquo;13), Bellevue, Washington, 2013.</li> <li>A K M M. Rahman, ASM. I. Anam, M. I. Tanveer, and M. Yeasin, &ldquo;EmoAssist: A Real-time Social Interaction Tool to assist the Visually Impaired", In Proceedings of the 15th &nbsp;International Conference on Human- &nbsp; &nbsp; &nbsp; Computer Interaction (HCII 2013), Las Vegas, NV.</li> <li>G. Hossain and M. Yeasin,"Under...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This report highlights the major activities, specific objectives significant results, and key achievements of the Year 2013-2014.  A number of new initiatives were taken to solve some of the critical problems in meeting the stated objective. In particular, we have strived to: (i) Assistive Technology for People who are Blind or Visually Impaired, (ii) Modeling of Cognitive Ability-Demand Gaps in Collaborative Sense-making to develop Assistive Technology Solutions, (iii) Semi-automated system to annotate large audio-visual database, (iii) develop software system to co-analyze features for both the audio and video data, (iv) Modality and Task Independent Cognitive Load for Assistive Technology and (v) Tool for Integration and Mining of Big Data in Biomedicine. Finding of this research is being published in well-reputed journals and conferences and many of the research outcomes are directly applied to the NSF RESSEE proposal entitled "Contextual Research-Empirical Research--Detecting, Tracking, and Modeling Cognitive, Affective, and Meta-cognitive Regulatory Processes to Optimize Learning with MetaTutor, where Dr. Yeasin is Co-PI.  During the period (05/01/2013 to 04/30/2014) several new initiatives were taken to build on top of the prior years research accomplishments. The PI and the research team made significant effort in developing assistive technology solutions for people who are blind or visually impaired. Culminating research effort in this period produces: (i) Five (5) Journal paper, (ii) Ten (10) Peer reviewed conference papers, (iii) Two (2) Journal abstract, (iv) Two (2) Ph.D. Dissertation and (v) Two (2) MS Thesis.   List of Publications during the reporting period (May 2013 &ndash;April 2014):  Journal Papers:  AKM Mahbubur Rahman, ASM Iftekhar Anam and Mohammed Yeasin, "EmoAssist: Emotion Enabled Assistive Tool to Enhance Dyadic Conversation for the Visually Impaired, IEEE Transaction on Affective Computing, 2014 (in Press). AKM Mahbubur Rahman, and Mohammed Yeasin,"A Unified Framework for Dividing and Predicting a Large Set of Action Units", IEEE Transaction on Affective Computing, 2014 (In Press).  M. Iftekhar Tanveer, A.S.M. Iftekhar Anam and Mohammed Yeasin, "Designing a Technology for the Blind Users to Understand Others Facial Expressions," ACM Transaction of Accessible Computing (TACCESS), 2014 (in Press). G. Hossain and M. Yeasin,"Cognitive Ability-Demand Gap Analysis with Latent Response Models", IEEE Transactions on ACCESS, EEE (DOI) - 10.1109/ACCESS.2014.2339328, 2014.  G. Hossain and M. Yeasin,"Assistive Thinking: Integrating System Dynamics into Design Thinking Approach", IEEE Journal of Systems, 2014 (in Press).   Conference Papers:  Expression: A Dyadic ConversationAid using Google Glass for Peoplewith Visual Impairments, Anam, ASM Iftekhar and Alam, Shahinur and Yeasin, To appear in ACM Ubiquitous computing 2014 Adjunct Publicaiton, Seattle, WA, Sept 13-17 M. Iftekhar Tanveer, A.S.M. Iftekhar Anam, Mohammed Yeasin and Majid Khan, "Do You See What I See? Designing a Sensory Substitution Device to Access Non-verbal Modes of Communication", To appear in proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS Æ13), Bellevue, Washington, 2013. A K M M. Rahman, ASM. I. Anam, M. I. Tanveer, and M. Yeasin, "EmoAssist: A Real-time Social Interaction Tool to assist the Visually Impaired", In Proceedings of the 15th  International Conference on Human-       Computer Interaction (HCII 2013), Las Vegas, NV. G. Hossain and M. Yeasin,"Understanding Effects of Cognitive Load from Pupillary Responses Using Hilbert Analytic Phase," in Proc. IEEE Workshop on Vision Meets Cognition (in conjunction with CVPR 2014), June 23, 2014. G. Hossain and M. Yeasin, "Assistive Thinking: A New Approach in Assistive Technology Design in Disability Management", IEEE 1st International Conference on Technology for Helping People with Special Needs (ICTHP-2013), February 18-20, 2013, Riyadh,...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
