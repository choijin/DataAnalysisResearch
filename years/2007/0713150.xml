<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Robot developmental learning of objects, actions, and tools</AwardTitle>
<AwardEffectiveDate>09/15/2007</AwardEffectiveDate>
<AwardExpirationDate>08/31/2011</AwardExpirationDate>
<AwardTotalIntnAmount>449999.00</AwardTotalIntnAmount>
<AwardAmount>449999</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Todd Leen</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Planning to achieve a goal requires knowledge of objects, actions, preconditions, and consequences.  These abstract concepts are at a much higher level than the ''pixel-level'' sensory and motor interfaces between an embodied robot and the continuous world.  Our goal is to show how high-level concepts of object and action can be learned autonomously from experience with low-level sensorimotor interaction.  &lt;br/&gt;&lt;br/&gt;We hypothesize that these concepts are part of a larger package of foundational concepts that can be learned in approximately the following sequence:  using motion to discriminate objects from background; detecting tight, reliable control loops to distinguish self from non-self objects; learning preconditions and consequences of actions applied to objects; identifying ''grasp'' actions that temporarily transform a non-self object to a self object; learning actions and effects that are achievable only with such an object (a tool!).&lt;br/&gt;&lt;br/&gt;The learning process depends on representing sensorimotor interaction with the world as a stochastic dynamical system.  A ''curiosity'' drive rewards improvements in prediction reliability.  Evaluation uses a simulated robot child with two arms, stereo vision, and a tray of blocks and other objects.  This research will help robots learn their own high-level concepts, and could provide insights into human learning disabilities.</AbstractNarration>
<MinAmdLetterDate>09/07/2007</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2008</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0713150</AwardID>
<Investigator>
<FirstName>Benjamin</FirstName>
<LastName>Kuipers</LastName>
<EmailAddress>kuipers@umich.edu</EmailAddress>
<StartDate>09/07/2007</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
