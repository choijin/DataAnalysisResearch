<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>High-Bandwidth Memory Pipeline for Wide-Issue Processors</AwardTitle>
<AwardEffectiveDate>07/01/2000</AwardEffectiveDate>
<AwardExpirationDate>02/28/2002</AwardExpirationDate>
<AwardAmount>206130</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010300</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Pratibha Varma-Nelson</SignBlockName>
</ProgramOfficer>
<AbstractNarration>ABSTRACT&lt;br/&gt;Proposal: 0073259&lt;br/&gt;PI: Gyungho Lee&lt;br/&gt;Title: High-Bandwidth Memory Pipeline for Wide-Issue Processors &lt;br/&gt; &lt;br/&gt;Technological and architectural innovations have enabled development of powerful microprocessors that can issue several instructions concurrently at a very high clock rate. In future processors with aggressive speculation techniques, an even larger number of instruction issues per cycle are expected. Efficient handling of memory references for data access is one of the keys to achieving high performance in future processors. This research addresses methods to provide sufficient bandwidth at fast latency for data access in wide-issue processors issuing tens of instructions per cycle. The methods are based on the concept of "data decoupling". Data decoupling divides the memory reference instructions into multiple independent streams before the actual addresses of the data they access are known. Partitioned memory reference instructions are then fed into a separate memory pipeline. This research investigates the issues in effective and efficient hardware and software support for multiple memory pipelines based on data decoupling. The data decoupling approach of providing multiple memory pipelines can provide two crucial advantages over a conventional design. First, the cost and the complexity of building a large cache with many ports are reduced. Second, partitioning memory references can facilitate more specialized handling of each partitioned stream. &lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/10/2000</MinAmdLetterDate>
<MaxAmdLetterDate>07/10/2000</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0073259</AwardID>
<Investigator>
<FirstName>Gyungho</FirstName>
<LastName>Lee</LastName>
<EmailAddress>ghlee@ece.uic.edu</EmailAddress>
<StartDate>07/10/2000</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Iowa State University</Name>
<CityName>AMES</CityName>
<ZipCode>500112207</ZipCode>
<PhoneNumber>5152945225</PhoneNumber>
<StreetAddress>1138 Pearson</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
</Institution>
<ProgramElement>
<Code>4715</Code>
<Text>COMPUTER SYSTEMS ARCHITECTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
