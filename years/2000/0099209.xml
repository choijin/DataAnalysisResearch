<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Judgement Under Uncertainty: Ad Hoc Partitions and Insufficient Reason</AwardTitle>
    <AwardEffectiveDate>05/01/2001</AwardEffectiveDate>
    <AwardExpirationDate>10/31/2004</AwardExpirationDate>
    <AwardAmount>163114</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04050100</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Divn Of Social and Economic Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Robert E. O'Connor</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Most decisions require an assessment of the likelihood of possible consequences. For instance, the decision whether or not to undergo surgery requires an estimation of the likelihood that the operation will succeed. Not only do people judge probabilities in service to making decisions, but they also use these numbers as a currency of communication (e.g., between doctor and patient). A better understanding of the intuitive processes underlying probability assessment can lead to prescriptions for improving intuitive decision making, strategies for debiasing formal decision analysis, and more effective communication.&lt;br/&gt;&lt;br/&gt;People rarely know the precise probability of an event except when it entails a simple game of chance (e.g., "a fair coin lands heads") or stable historic frequencies are available (e.g., "1 in 40,000 people inoculated for measles experiences an allergic reaction"). More often, people judge probabilities intuitively and with some degree of imprecision. Past research has identified mental shortcuts or "heuristics" that govern this process (Kahneman, Slovic &amp; Tversky, 1982), which has recently been formalized in support theory (Tversky &amp; Koehler, 1994; Rottenstreich &amp; Tversky, 1997). In this model, judged probability is viewed as the balance of evidence or "support" that can be recruited for the target hypothesis (e.g. "rain today") relative to its complement ("no rain today"). If the balance of evidence strongly favors the focal hypothesis, the judged probability will be close to one; if the balance favors the complementary hypothesis the judged probability will be close to zero; if the balance is equal a the judged probability will be one-half.&lt;br/&gt;&lt;br/&gt;The Principal Investigator (PI) observes that when people have no evidence available they typically apply the "principle of insufficient reason," assigning equal credence to the elementary events into which the event space is subjectively partitioned. For instance, if five unfamiliar horses are entered in a race, a person will typically assign equal belief in the proposition that each horse will win, yielding an "ignorance prior" probability of 1/5 for each horse. Assuming one holds the partition constant, this mode of judgment yields probabilities that are perfectly calibrated. Unfortunately, subjective partitions usually shift over time. For instance, a person may judge the probability that the Jakarta stock index (JAX) closes above 1,000 to be 0.5 (assigning equal belief to the events "below 1,000" and "above 1,000") but may likewise judge the probability that the JAX closes above 5,000 or even 50,000 to be 0.5. Hence, judged probabilities seem to depend crucially on the subjective partition that is adopted by the decision maker.&lt;br/&gt;&lt;br/&gt;The PI hypothesizes that judgment under uncertainty typically reflects a compromise between the balance of evidential support and this "ignorance prior," and advances and tests a theoretical model that extends support theory to accommodate this pattern.</AbstractNarration>
    <MinAmdLetterDate>05/15/2001</MinAmdLetterDate>
    <MaxAmdLetterDate>04/30/2004</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0099209</AwardID>
    <Investigator>
      <FirstName>Craig</FirstName>
      <LastName>Fox</LastName>
      <EmailAddress>cfox@mail.duke.edu</EmailAddress>
      <StartDate>05/15/2001</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Duke University</Name>
      <CityName>Durham</CityName>
      <ZipCode>277054010</ZipCode>
      <PhoneNumber>9196843030</PhoneNumber>
      <StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>North Carolina</StateName>
      <StateCode>NC</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0116000</Code>
      <Name>Human Subjects</Name>
    </FoaInformation>
    <ProgramElement>
      <Code>1321</Code>
      <Text>DECISION RISK &amp; MANAGEMENT SCI</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>0000</Code>
      <Text>UNASSIGNED</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>OTHR</Code>
      <Text>OTHER RESEARCH OR EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>
