<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Feature Construction for Large Discrete Domains</AwardTitle>
<AwardEffectiveDate>08/01/2001</AwardEffectiveDate>
<AwardExpirationDate>06/30/2005</AwardExpirationDate>
<AwardTotalIntnAmount>353106.00</AwardTotalIntnAmount>
<AwardAmount>353106</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James French</SignBlockName>
</ProgramOfficer>
<AbstractNarration>&lt;br/&gt;IIS-0097218&lt;br/&gt;Paul E. Utgoff&lt;br/&gt;University of Massachusetts at Amherst&lt;br/&gt;$119,407 - 12 mos&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Feature Construction for Large Discrete Domains&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is the first year funding of a three year continuing award. This project focuses on the problem of learning the representations on which more widely studied data fitting programs depend.  Thus, instead of providing a representation and watching the fitting algorithm run its course, the goal is to attack the more fundamental problem of learning the representation itself.  The objective is to produce an agent that can identify all of the useful features of a large discrete domain.   Artificial neural networks of the typical one or two layers of hidden units cannot scale to large problems of the kind that the PI wants to solve (learning in large discrete domains), because this kind of `few-layered learning' suffers not only from local minima and shallow gradients, but more fundamentally from inappropriate bases and the inherent need for exponentially many features (hidden units) imposed by the constraint of so few layers of features.  The PI will pursue scalable methods that he characterizes as `many-layered learning', which will move the state-of-the-art past the current nonscalable practices of feature construction, which in turn will affect much of the work in function approximation, including the current nonscalable use of few-layered artificial neural networks.  The project will produce a variety of algorithms for many-layered learning. Among them will be one for building a nested feature representation based on problem-solving experience.  A second will demonstrate that knowledge layering and decomposition follow naturally from using only simple learning mechanisms to learn the next most easily learned features based on the representation learned thus far.   The larger implications for many-layered learning on intelligence will be investigated.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/31/2001</MinAmdLetterDate>
<MaxAmdLetterDate>02/01/2005</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0097218</AwardID>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Utgoff</LastName>
<EmailAddress>utgoff@cs.umass.edu</EmailAddress>
<StartDate>07/31/2001</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>6856</Code>
<Text>ARTIFICIAL INTELL &amp; COGNIT SCI</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
