<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Trends And Empirical Econometric Limits</AwardTitle>
<AwardEffectiveDate>05/01/2001</AwardEffectiveDate>
<AwardExpirationDate>04/30/2005</AwardExpirationDate>
<AwardAmount>226927</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>04050100</Code>
<Directorate>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Daniel H. Newlon</SignBlockName>
</ProgramOfficer>
<AbstractNarration>&lt;br/&gt;&lt;br/&gt;An important issue that bears on all practical economic analysis is the extent to which&lt;br/&gt;we can expect to understand economic phenomena by the process of developing a theory,&lt;br/&gt;taking observations and fitting a model. An especially relevant question in practice is&lt;br/&gt;whether there are limits on how well we can predict future observations using empirical&lt;br/&gt;models that are obtained by such processes. Finding quantitative expression for these&lt;br/&gt;limits is the main subject of the project.&lt;br/&gt;A primary limitation on empirical knowledge is that the true model for any given data&lt;br/&gt;is unknown and, in all practical cases, unknowable. This is because even if the formulated&lt;br/&gt;model were correct it would still depend on parameters that need to be estimated from&lt;br/&gt;data. Often, the data is scarce relative to the number of parameters that need to be&lt;br/&gt;estimated, and this is especially so in models that have some functional representation&lt;br/&gt;that necessitates the use of nonparametric or semiparametric methods. In such situations&lt;br/&gt;one might expect that the empirical limitations on modeling are greater than in finite&lt;br/&gt;parameter models. Using reasoning that was pioneered by Jorma Rissanen in 1987, the&lt;br/&gt;author has shown in collaborative work with Werner Ploberger in 1999 that there is a&lt;br/&gt;quantitative bound on how close an empirical model can get (in terms of its log likelihood&lt;br/&gt;ratio) to the true model. This bound depends on the data itself as well as the model that&lt;br/&gt;is being used. A discovery that seems important in applications to economic data is that&lt;br/&gt;the magnitude of the bound depends on the presence and nature of trends in the data.&lt;br/&gt;In particular, the achievable distance is greater for trending data than when the data are&lt;br/&gt;stationary. This result gives quantitative expresssion to the intuitively appealing notion&lt;br/&gt;that trending data is harder to predict than data that does not trend. The project develops&lt;br/&gt;and extends limitation results of this type to models where there are local and gross&lt;br/&gt;errors of specification, to nonparametric situations where the dimension of the parameter&lt;br/&gt;space is infinite or where it may grow with the sample size, that is, in situations where&lt;br/&gt;modeling becomes more ambitious as more data becomes available. The project also seeks&lt;br/&gt;to develop explicit representations of the forecast error divergence so that the limits on&lt;br/&gt;empirical forecasting capability are quantifed. The intent of this project is to develop&lt;br/&gt;the theory to a stage where the limits will be useful to empirical researchers, especially in&lt;br/&gt;terms of the implementation of model determination criteria that are designed to achieve&lt;br/&gt;the empirical bounds.&lt;br/&gt;In subsidiary wings of research that relate to this main theme, the project studies&lt;br/&gt;more explicit issues of trend regression, where the order of magnitude of the trend is&lt;br/&gt;not specifed but has to be estimated, where there is long memory in the data which is&lt;br/&gt;possibly nonstationary and the memory parameter must be estimated semiparametrically,&lt;br/&gt;and where there is nonstationary explanatory data but a limited dependent variable.&lt;br/&gt;The latter study is relevant to market intervention policy by the Federal Reserve and&lt;br/&gt;Treasury. Thus, monetary policy intervention is a binary decision (intervene or not), yet&lt;br/&gt;the explanatory variables that determine it involve a host of economic data, much of which&lt;br/&gt;has nonstationary features, like the growth characteristics of industrial production and the&lt;br/&gt;random wandering behavior of stock prices. We seek to learn how various characteristics in&lt;br/&gt;the explanatory data translate into the probability law for the binary variable and, hence,&lt;br/&gt;market intervention. Can these probability laws explain, for instance, the tendency of&lt;br/&gt;market intervention to lapse into long periods of little intervention broken by periods of&lt;br/&gt;regular intervention?</AbstractNarration>
<MinAmdLetterDate>03/27/2001</MinAmdLetterDate>
<MaxAmdLetterDate>03/03/2003</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0092509</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Phillips</LastName>
<EmailAddress>peter.phillips@yale.edu</EmailAddress>
<StartDate>03/27/2001</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Yale University</Name>
<CityName>New Haven</CityName>
<ZipCode>065208327</ZipCode>
<PhoneNumber>2037854689</PhoneNumber>
<StreetAddress>Office of Sponsored Projects</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
</Institution>
<ProgramElement>
<Code>1320</Code>
<Text>ECONOMICS</Text>
</ProgramElement>
<ProgramElement>
<Code>1333</Code>
<Text>METHOD, MEASURE &amp; STATS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
