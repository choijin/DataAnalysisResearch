<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I: Video-based Head and Face Gesture Recognition System for Hands-Free Control</AwardTitle>
<AwardEffectiveDate>01/01/2001</AwardEffectiveDate>
<AwardExpirationDate>06/30/2001</AwardExpirationDate>
<AwardAmount>99999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sara B. Nerlove</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This Small Business Innovation Research (SBIR) Phase I project from Future of Technology and Health, (FUTH), LC, will develop video-based gesture recognition technology to provide an effective new type of computer access for people who have difficulty using a standard keyboard or mouse due to disabilities including cerebral palsy, Lou Gerhig's disease (ALS), stroke, spinal cord injury, or repetitive stress injury.  This technology is also expected to provide the capability for 'hands-free' control of computers and other electronic equipment for all users who may be using their hands for other tasks (such as typing, driving a motor vehicle, operating test equipment, etc.).  This project focuses on recognition of multiple head and face gestures using standard low cost digital video cameras (under $100) and standard personal computers.  The system may also be implemented on pocket computers for mobile and in-vehicle applications.  Face and head gestures are used to generate mouse or keyboard actions to control a computer or to control signals that in turn control other types of electronic devices.  For example, one application is the capability to 'surf the web' hands-free using head gestures to navigate web pages, including selecting and activating desired links.  Gesture recognition can be used to replace or augment existing switch interfaces or expensive eye/head tracking systems for people with disabilities, and  it has a number of advantages over voice recognition in many applications.&lt;br/&gt;&lt;br/&gt;Early customers for this technology include computer users with mobility impairments who cannot effectively use a standard keyboard or mouse.  Other commercial applications include hands-free control of desktop computer software such as web browsers or text-to-speech, and hands-free control of in-vehicle information systems and personal digital assistants (PDA's).  The technology may also be applied for highly reliable hands-free control of industrial, scientific, or military equipment.&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>12/06/2000</MinAmdLetterDate>
<MaxAmdLetterDate>12/06/2000</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0060372</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Bishop</LastName>
<EmailAddress>research@futh.com</EmailAddress>
<StartDate>12/06/2000</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Future of Technology and Health, LC</Name>
<CityName>Iowa City</CityName>
<ZipCode>522441233</ZipCode>
<PhoneNumber>3196443787</PhoneNumber>
<StreetAddress>PO Box 1233</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
</Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1545</Code>
<Text>RES IN DISABILITIES ED</Text>
</ProgramElement>
<ProgramReference>
<Code>1545</Code>
<Text>RES IN DISABILITIES ED</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9180</Code>
<Text>PUBLIC SCIENCE LITERACY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
