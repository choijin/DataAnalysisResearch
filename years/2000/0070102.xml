<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Lyapunov Methods for Reinforcement Learning</AwardTitle>
<AwardEffectiveDate>07/01/2000</AwardEffectiveDate>
<AwardExpirationDate>12/31/2002</AwardExpirationDate>
<AwardTotalIntnAmount>119405.00</AwardTotalIntnAmount>
<AwardAmount>119405</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Paul Werbos</SignBlockName>
</ProgramOfficer>
<AbstractNarration>0070102&lt;br/&gt;Barto&lt;br/&gt;&lt;br/&gt;Reinforcement learning is a summary term for a collection of methods for approximating solutions to stochastic optimal control problems.  RL methods leave been successfully applied to a large array of such problems in a diversity of domains, including finance, logistics, telecommunications, and robot control.  Although similar problems have been studied intensively for many years in control engineering and operations research, the methods developed by RL researchers have added new elements to the classical solution methods.  RL methods offer novel ways to approximate solutions to problems that are too large or ill-defined for the classical solution methods to be feasible.&lt;br/&gt;&lt;br/&gt;A significant part of RL research is directed at increasing on-line performance and speed of convergence by providing RL systems with domain knowledge.  This project is concerned with knowledge related to the design of stabilizing controllers for complex dynamical systems.  It will try to develop a general theory for incorporating this knowledge into RL systems.  The basic idea is to mathematically define policy subspaces that have certain known stability and safety properties and to focus exploration on control laws that lie within these policy subspaces.  The means by which this is achieved are based on the theory of Lyapunov stability and the associated methods of Lyapunov control design.&lt;br/&gt;***&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/03/2000</MinAmdLetterDate>
<MaxAmdLetterDate>07/03/2000</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0070102</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Barto</LastName>
<EmailAddress>barto@cs.umass.edu</EmailAddress>
<StartDate>07/03/2000</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sascha</FirstName>
<LastName>Engelbrecht</LastName>
<EmailAddress>sascha@io.cs.umass.edu</EmailAddress>
<StartDate>07/03/2000</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
</Institution>
<FoaInformation>
<Code>0206000</Code>
<Name>Telecommunications</Name>
</FoaInformation>
<ProgramElement>
<Code>1518</Code>
<Text>CONTROL, NETWORKS, &amp; COMP INTE</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
