<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: SHF: Design and Analysis of Processing-Near-Memory Enabled GPU Architecture</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2017</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Graphics Processing Units (GPUs) are becoming an inevitable part of every computing system because of their ability to enable orders of magnitude faster and energy-efficient execution. However, the necessary and continuous scaling of GPUs in terms of performance and energy efficiency will not be an easy task. Prior works have shown that two biggest impediments towards this scaling are the limited memory bandwidth and the excessive data movement across different levels of the memory hierarchy. In order to alleviate these two issues, die-stacking technology is gaining momentum in the realm of high-performance energy-efficient GPU computing. This technology not only enables very high memory bandwidth for better performance but also provides support for processing-near-memory (PNM) to reduce data movement, access latencies, and energy consumption. Although these technologies seem promising, the architectural support and execution models for PNM-based GPUs and their implications on the entire system design have largely been unexplored. This project takes a fresh look at the design and execution model of a PNM-enabled GPU, which consists of multiple memory stacks and each memory stack incorporates a 3D-stacked logic layer that can consist of multiple PNM GPU cores and other uncore components. Considering that GPUs are becoming an inevitable part of every computing system ranging from warehouse-scale computers to wearable devices, the insights resulting from this research can have a long-term positive impact on the GPU-based computing. The findings of this research will be incorporated to existing and new undergraduate and graduate courses, which will directly help in educating and training students, including women and students from diverse backgrounds and minority groups.&lt;br/&gt;&lt;br/&gt;First, a detailed design space exploration will be performed, which will involve the study of the impact and interactions of different design choices related to PNM cores (e.g., register file, SIMD width, pipeline components, warp occupancy), uncore components at the logic layer (e.g., caches) and stacked memory (e.g., number of stacked memories). Second, a computation distribution framework (CDF) will be developed that will answer: a) when is it preferable to map computations to PNM cores, b) which PNM cores and computations they should be?, and c) how can we effectively take advantage of both PNM and regular GPU cores? The CDF will leverage different static and runtime strategies to address many of such similar questions to push the envelopes of energy efficiency and performance even further. The proposed research components will be evaluated via a wide-range of GPGPU applications.  If successful, the findings of this research would better equip PNM-enabled GPUs to effectively alleviate the two major bottlenecks: memory bandwidth and energy.</AbstractNarration>
<MinAmdLetterDate>01/25/2017</MinAmdLetterDate>
<MaxAmdLetterDate>01/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657336</AwardID>
<Investigator>
<FirstName>Adwait</FirstName>
<LastName>Jog</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adwait Jog</PI_FULL_NAME>
<EmailAddress>ajog@wm.edu</EmailAddress>
<PI_PHON>7572211434</PI_PHON>
<NSF_ID>000702344</NSF_ID>
<StartDate>01/25/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>College of William and Mary</Name>
<CityName>Williamsburg</CityName>
<ZipCode>231878795</ZipCode>
<PhoneNumber>7572213966</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 8795]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>074762238</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLLEGE OF WILLIAM &amp; MARY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>074762238</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[College of William and Mary]]></Name>
<CityName>Williamsburg</CityName>
<StateCode>VA</StateCode>
<ZipCode>231878795</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-ccfb695a-7fff-5788-0676-323e6e816247"> </span></p> <p dir="ltr"><span>Intellectual Merit</span></p> <p dir="ltr"><span>Graphics Processing Units (GPUs) are becoming an inevitable part of every computing system because of their ability to enable orders of magnitude faster and energy-efficient execution. However, the necessary and continuous scaling of GPUs in terms of performance and energy efficiency is not an easy task. Prior works have shown that the two biggest impediments towards this scaling are the limited memory bandwidth and the excessive data movement across different levels of the memory hierarchy. This project took a global view of this problem by researching on three related topics: near data computing, bandwidth management, and memory systems. All these topics were considered in light of emerging applications. In terms of near data computing, the major works appeared in top research venues (ISCA 2019 and MICRO 2018) where the PI considered near-data computing in GPU interconnect and emerging automata accelerators. In terms of bandwidth management, the major works appeared in top research venues (HPCA 2018 and ICS 2019) where PI considered bandwidth management for performance, fairness, and energy efficiency. Finally, PI also explored novel memory architectures in ISVLSI 2017 and VLSID 2017 papers.&nbsp; Overall, this research initiation grant helped in generating several novel insights and was instrumental in building PI's independent research.</span></p> <p dir="ltr"><span><br /></span></p> <p dir="ltr"><span>Broader Impact</span></p> <p dir="ltr"><span>On the topics related to this grant, PI successfully advised the thesis of multiple Ph.D. students, one female master's student, and one undergraduate student. Several supported students also did internships at AMD Research and Paci&#64257;c Northwest National Laboratory. For outreach, the PI co-organized workshops on minimizing data movement (Min-Move) and gave a keynote talk in the School of Education at William &amp; Mary to attract middle and high school students to careers related to computer science. The topics of this grant have also been incorporated into the new course on GPU Architecture and Programming that PI Jog developed at William &amp; Mary. </span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/17/2020<br>      Modified by: Adwait&nbsp;Jog</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Intellectual Merit Graphics Processing Units (GPUs) are becoming an inevitable part of every computing system because of their ability to enable orders of magnitude faster and energy-efficient execution. However, the necessary and continuous scaling of GPUs in terms of performance and energy efficiency is not an easy task. Prior works have shown that the two biggest impediments towards this scaling are the limited memory bandwidth and the excessive data movement across different levels of the memory hierarchy. This project took a global view of this problem by researching on three related topics: near data computing, bandwidth management, and memory systems. All these topics were considered in light of emerging applications. In terms of near data computing, the major works appeared in top research venues (ISCA 2019 and MICRO 2018) where the PI considered near-data computing in GPU interconnect and emerging automata accelerators. In terms of bandwidth management, the major works appeared in top research venues (HPCA 2018 and ICS 2019) where PI considered bandwidth management for performance, fairness, and energy efficiency. Finally, PI also explored novel memory architectures in ISVLSI 2017 and VLSID 2017 papers.  Overall, this research initiation grant helped in generating several novel insights and was instrumental in building PI's independent research.   Broader Impact On the topics related to this grant, PI successfully advised the thesis of multiple Ph.D. students, one female master's student, and one undergraduate student. Several supported students also did internships at AMD Research and Paci&#64257;c Northwest National Laboratory. For outreach, the PI co-organized workshops on minimizing data movement (Min-Move) and gave a keynote talk in the School of Education at William &amp; Mary to attract middle and high school students to careers related to computer science. The topics of this grant have also been incorporated into the new course on GPU Architecture and Programming that PI Jog developed at William &amp; Mary.                Last Modified: 03/17/2020       Submitted by: Adwait Jog]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
