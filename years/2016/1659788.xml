<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>REU Site: Machine Learning in Natural Language Processing and Computer Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>379853.00</AwardTotalIntnAmount>
<AwardAmount>379853</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The proposal seeks to increase the number of American citizens and permanent resident undergraduates who are attracted to careers in research and advanced studies in Computer Science. Training in theoretical and empirical machine learning will enable participants to contribute to ubiquitous software-based technologies at the highest levels in innovative ways. The proposal will also focus almost exclusively on training future computer scientists from institutions with limited research opportunities, women and under-represented minorities. The research experience will encourage these students to be productive researchers in academic and non-academic environments during their future careers. Computer vision, through applications such as face recognition and autonomous vehicles, has profound implications for society in terms of future of national security and transportation. Natural language processing, through applications such as efficient large volume semantic analysis and summarization of textual documents from disparate sources, also has implications for national security in addition to numerous other possibilities that can be exploited by industry.  &lt;br/&gt;&lt;br/&gt;The objective of this proposal is to expose bright and motivated undergraduates who want to pursue advanced careers in Computer Science to active research experience early in their careers. This proposal seeks to develop an REU site to broaden the intellectual horizon of participants through exposure to opportunities available in university research.  Students will be involved in research projects in machine learning techniques and in emerging applications that exploit machine learning.  The applications of interest are in the fields of natural language processing and computer vision.  Students will have the opportunity to utilize a combination of theoretical reading, analysis, and research within laboratory environments.  The students will be introduced to the topics and helped to obtain in-depth understanding of selected topics through introductory presentations. Then, they will perform hands-on research on novel problems, conduct experiments, and communicate their results through written papers and presentations. The REU students will be involved in research in machine learning and its applications in diverse and emergent areas alongside faculty mentors and graduate students in a university environment. The research will involve undergraduate students in cutting-edge research where they will write software to solve interesting and timely problems and write papers for publication.</AbstractNarration>
<MinAmdLetterDate>03/14/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/14/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1659788</AwardID>
<Investigator>
<FirstName>Jugal</FirstName>
<LastName>Kalita</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jugal Kalita</PI_FULL_NAME>
<EmailAddress>jkalita@uccs.edu</EmailAddress>
<PI_PHON>7192553432</PI_PHON>
<NSF_ID>000221568</NSF_ID>
<StartDate>03/14/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Ventura</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan Ventura</PI_FULL_NAME>
<EmailAddress>jventu09@calpoly.edu</EmailAddress>
<PI_PHON>8057565624</PI_PHON>
<NSF_ID>000677310</NSF_ID>
<StartDate>03/14/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Colorado Springs</Name>
<CityName>Colorado Springs</CityName>
<ZipCode>809183733</ZipCode>
<PhoneNumber>7192553153</PhoneNumber>
<StreetAddress>1420, Austin Bluffs Parkway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>186192829</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Colorado Springs]]></Name>
<CityName>Colorado Springs</CityName>
<StateCode>CO</StateCode>
<ZipCode>809183733</ZipCode>
<StreetAddress><![CDATA[1420 Austin Bluffs Parkway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1139</Code>
<Text>RSCH EXPER FOR UNDERGRAD SITES</Text>
</ProgramElement>
<ProgramReference>
<Code>9250</Code>
<Text>REU SITE-Res Exp for Ugrd Site</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~379853</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The REU program at the University &nbsp;of Colorado, Colorado Springs, brought 30 undergraduates for summer programs in 2017, 2018 and 2019, out of a total of 170 applicants from around the country. The overall acceptance rate was 18%.&nbsp;</p> <p>&nbsp;</p> <p><strong>Intellectual Merit:</strong> All interns in the three years were trained on machine learning and deep learning, through class-style group meetings, &nbsp;discussions and presentations. In addition, three faculty mentors and several graduate mentors (only one was paid by the &nbsp;grant) were available &nbsp;regularly, for one-on-one discussions.&nbsp;</p> <p>The students attempted to solve cutting-edge research problems in natural language processing and computer vision, achieving various levels of success. A total of 17 papers were published based on the research performed by the REU students, attesting to the high quality of research at the REU site. Below are &nbsp;a couple of examples of papers published.</p> <p>A paper by Tiffany Chien, based on her &nbsp;work during the REU summer, was titled <em>Adversarial Analysis of Natural Language Inference Systems,</em>&nbsp;<span style="font-size: 12px;">published&nbsp;</span><span style="font-size: 12px;">at the&nbsp;</span><span style="font-size: 12px;">2020 IEEE 14th International Conference on Semantic Computing (ICSC). In this work, Tiffany examined carefully how state-of-the-art deep learning systems that perform inference can be attacked in various ways.&nbsp;</span></p> <p><span style="font-size: 12px;">&nbsp;</span></p> <div id="gsc_vcd_title"><span style="font-size: 12px;">This work evaluates the failures of state-of-the-art models on existing adversarial datasets that test different linguistic phenomena, and find that even though the models perform similarly on MNLI, they differ greatly in their robustness to these attacks. In particular, we find syntax-related attacks to be particularly effective across all models, so we provide a fine-grained analysis and comparison of model performance on those examples. We draw conclusions about the value of model size and multi-task learning (beyond comparing their standard test set performance), and provide suggestions for more effective training data.</span></div> <div><span style="font-size: 12px;">Another paper by Kaden Griffith, titled&nbsp;<span><em>&nbsp;Automatically Using Transformer and Unambiguous Representations</em>, was published at the IEEE&nbsp;</span>International Conference on Computational Science and Computational Intelligence (CSCI), 2019. &nbsp;</span><span style="font-size: 12px;">Prior attempts using machine learning have been trained on corpora specific to math word problems to produce arithmetic expressions in infix notation before answer computation. We find that custom-built neural networks have struggled to generalize well. This work outlines the use of Transformer networks trained to translate math word problems to equivalent arithmetic expressions in infix, prefix, and postfix notations. In addition to training directly on domain-specific corpora, we use an approach that pre-trains on a general text corpus to provide foundational language abilities to explore if it improves performance. We compare results produced by a large number of neural configurations and find that most configurations outperform previously reported approaches on three of four datasets with significant increases in accuracy of over 20 percentage points. The best neural approaches boost accuracy by almost 10% on average when compared to the previous state of the art.</span></div> <div><strong>Broader Impacts</strong>: During the three year period, 35% of the summer REU interns were female and under-represented minorities. 68% of the &nbsp;students were from colleges with limited or no research opportunities. We had 1 African-American female, 1 Hispanic male, and a total of 9 women in the program during the period. Of the students who have gone through the program, one is currently &nbsp;a graduate student at the University of Oregon, one at NYU, one at Notre Dame, and another at Georgia Tech. Several others are working in deep learning in &nbsp;software companies, and others are in the process of applying to graduate school.&nbsp;</div> <p>&nbsp;</p> <p><span style="font-size: 12px;">&nbsp;</span></p> <div> <div id="gsc_vcd_table"></div> </div> <p>&nbsp;</p><br> <p>            Last Modified: 09/01/2020<br>      Modified by: Jugal&nbsp;Kalita</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The REU program at the University  of Colorado, Colorado Springs, brought 30 undergraduates for summer programs in 2017, 2018 and 2019, out of a total of 170 applicants from around the country. The overall acceptance rate was 18%.      Intellectual Merit: All interns in the three years were trained on machine learning and deep learning, through class-style group meetings,  discussions and presentations. In addition, three faculty mentors and several graduate mentors (only one was paid by the  grant) were available  regularly, for one-on-one discussions.   The students attempted to solve cutting-edge research problems in natural language processing and computer vision, achieving various levels of success. A total of 17 papers were published based on the research performed by the REU students, attesting to the high quality of research at the REU site. Below are  a couple of examples of papers published.  A paper by Tiffany Chien, based on her  work during the REU summer, was titled Adversarial Analysis of Natural Language Inference Systems, published at the 2020 IEEE 14th International Conference on Semantic Computing (ICSC). In this work, Tiffany examined carefully how state-of-the-art deep learning systems that perform inference can be attacked in various ways.     This work evaluates the failures of state-of-the-art models on existing adversarial datasets that test different linguistic phenomena, and find that even though the models perform similarly on MNLI, they differ greatly in their robustness to these attacks. In particular, we find syntax-related attacks to be particularly effective across all models, so we provide a fine-grained analysis and comparison of model performance on those examples. We draw conclusions about the value of model size and multi-task learning (beyond comparing their standard test set performance), and provide suggestions for more effective training data. Another paper by Kaden Griffith, titled  Automatically Using Transformer and Unambiguous Representations, was published at the IEEE International Conference on Computational Science and Computational Intelligence (CSCI), 2019.  Prior attempts using machine learning have been trained on corpora specific to math word problems to produce arithmetic expressions in infix notation before answer computation. We find that custom-built neural networks have struggled to generalize well. This work outlines the use of Transformer networks trained to translate math word problems to equivalent arithmetic expressions in infix, prefix, and postfix notations. In addition to training directly on domain-specific corpora, we use an approach that pre-trains on a general text corpus to provide foundational language abilities to explore if it improves performance. We compare results produced by a large number of neural configurations and find that most configurations outperform previously reported approaches on three of four datasets with significant increases in accuracy of over 20 percentage points. The best neural approaches boost accuracy by almost 10% on average when compared to the previous state of the art. Broader Impacts: During the three year period, 35% of the summer REU interns were female and under-represented minorities. 68% of the  students were from colleges with limited or no research opportunities. We had 1 African-American female, 1 Hispanic male, and a total of 9 women in the program during the period. Of the students who have gone through the program, one is currently  a graduate student at the University of Oregon, one at NYU, one at Notre Dame, and another at Georgia Tech. Several others are working in deep learning in  software companies, and others are in the process of applying to graduate school.                    Last Modified: 09/01/2020       Submitted by: Jugal Kalita]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
