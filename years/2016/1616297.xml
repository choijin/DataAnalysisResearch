<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: Entropy Maximization in Approximation, Learning, and Complexity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>466000.00</AwardTotalIntnAmount>
<AwardAmount>466000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Entropy plays a distinguished role in the world.  The second law of thermodynamics tell us that, in closed systems, entropy always increases; it is maximized at thermodynamic equilibrium.  Given a collection of data, the "principle of maximum entropy" asserts that, among all hypothetical probability distributions that agree with the data, the one of maximum entropy best represents the current state of knowledge.&lt;br/&gt;&lt;br/&gt;Moreover, if one considers a convex set of probability distributions, the problem of maximizing a strongly concave function (like the Shannon entropy) over this set is computationally tractable and has a unique optimal solution.  This project is concerned with the structure and computational utility of entropy maximizers in algorithm design, machine learning, complexity theory, and related areas of discrete mathematics.  In particular, the project will study the role of entropy maximization in encouraging simplicity in the optimum solution.  This property stands to reason:  The entropy maximizer should intuitively contain only the information implied by the constraints and nothing more.&lt;br/&gt;&lt;br/&gt;The scope of the project includes not only classical entropy functionals like the Shannon entropy and Kullback-Leibler divergence, but also the analogous notions for quantum states (von Neumann entropy).  The study of quantum entropy maximizers has far-reaching applications in semi-definite programming and communication complexity.  Moreover, much of the theory extends to other Bregman divergences, and this is particularly relevant for applications in online algorithms where certain smoothed entropy functionals become relevant.  A portion of the project concerns entropy optimality on path spaces.  This perspective provides a novel view of Markov processes on discrete and continuous spaces.  The PI will employ this viewpoint to study rapid mixing of Markov chains, as well smoothing properties of the noise operator on the discrete hypercube (a topic with remarkable applications in complexity theory and hardness of approximation).&lt;br/&gt;&lt;br/&gt;Finally, it should be mentioned that iterative algorithms for finding entropy maximizers can be viewed in the framework of entropy-regularized gradient descent; such algorithms are fundamental in machine learning (boosting) and online convex optimization (multiplicative weights update).  This provides a powerful connection to large bodies of work, and a substantial motivation for the project is to create a bridge of ideas and techniques between the two perspectives.&lt;br/&gt;&lt;br/&gt;Broader impact of the project includes training of the next generation of scientists, including at the undergraduate level.  This project presents a number of opportunities for undergraduate researchers to contribute in a meaningful and substantial way, while at the same time receiving valuable mentoring and experience as developing scientists.</AbstractNarration>
<MinAmdLetterDate>05/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/17/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1616297</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James R Lee</PI_FULL_NAME>
<EmailAddress>jrl@cs.washington.edu</EmailAddress>
<PI_PHON>2066164368</PI_PHON>
<NSF_ID>000482524</NSF_ID>
<StartDate>05/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7927</Code>
<Text>COMPLEXITY &amp; CRYPTOGRAPHY</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~466000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Entropy is a rather special quantity. The second law of thermodynamics tells us that, in closed systems, entropy always increases; it is maximized at thermodynamic equilibrium. Given a collection of data, the ?principle of maximum entropy? asserts that, among all hypothetical probability distributions that agree with the data, the one of maximum entropy best represents the current state of knowledge.</p> <p>This project aimed to develop a theory of entropy maximization with a particular focus on its applications to centrally important problems in theoretical computer science.&nbsp; The theory was applied fruitfully to a range of problems in algorithms and complexity theory.&nbsp; For instance, we used entropy maximization to understand the regularizing properties of noise on Boolean functions, the additive structure of subsets of integers, and the rate of convergence of Markov chains to their equilibrium measure.&nbsp; This theory was also applied to related phenomena in maximizers for the von Neumann entropy of quantum states.</p> <p>Perhaps the most striking application was in the area of online algorithms and competitive analysis, where we showed that one can derive algorithms directly from a convex representation along with a convex regularizer that deforms the geometry of the underlying space.&nbsp; Employing an appropriate <em>entropic</em> regularizer allows one to design algorithms that make near-optimal decisions as inputs arrive over time, while also deftly hedging against future uncertainty.&nbsp; This leads to breakthroughs in algorithm design for the k-server and metrical task system problems, basic online computational tasks whose structure had remained elusive despite decades of study.</p><br> <p>            Last Modified: 01/27/2021<br>      Modified by: James&nbsp;R&nbsp;Lee</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1616297/1616297_10425052_1611809892094_kserver--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1616297/1616297_10425052_1611809892094_kserver--rgov-800width.jpg" title="HST"><img src="/por/images/Reports/POR/2021/1616297/1616297_10425052_1611809892094_kserver--rgov-66x44.jpg" alt="HST"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Entropy flow on a tree</div> <div class="imageCredit">James R Lee</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">James&nbsp;R&nbsp;Lee</div> <div class="imageTitle">HST</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Entropy is a rather special quantity. The second law of thermodynamics tells us that, in closed systems, entropy always increases; it is maximized at thermodynamic equilibrium. Given a collection of data, the ?principle of maximum entropy? asserts that, among all hypothetical probability distributions that agree with the data, the one of maximum entropy best represents the current state of knowledge.  This project aimed to develop a theory of entropy maximization with a particular focus on its applications to centrally important problems in theoretical computer science.  The theory was applied fruitfully to a range of problems in algorithms and complexity theory.  For instance, we used entropy maximization to understand the regularizing properties of noise on Boolean functions, the additive structure of subsets of integers, and the rate of convergence of Markov chains to their equilibrium measure.  This theory was also applied to related phenomena in maximizers for the von Neumann entropy of quantum states.  Perhaps the most striking application was in the area of online algorithms and competitive analysis, where we showed that one can derive algorithms directly from a convex representation along with a convex regularizer that deforms the geometry of the underlying space.  Employing an appropriate entropic regularizer allows one to design algorithms that make near-optimal decisions as inputs arrive over time, while also deftly hedging against future uncertainty.  This leads to breakthroughs in algorithm design for the k-server and metrical task system problems, basic online computational tasks whose structure had remained elusive despite decades of study.       Last Modified: 01/27/2021       Submitted by: James R Lee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
