<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Scalable Crowdsourced Reinforcement of Robot Behavior</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>123136.00</AwardTotalIntnAmount>
<AwardAmount>123136</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dan Cosley</SignBlockName>
<PO_EMAI>dcosley@nsf.gov</PO_EMAI>
<PO_PHON>7032928832</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will develop the groundwork for systems that allow large numbers of volunteer contributors to help train robots with different movement capabilities to respond correctly to commands, answering research questions both about how to design the systems to motivate people to contribute and about how crowds can help train robots.  This is an important robotics question because training these robots requires large, labeled datasets in which the robots are moving and their motions are appropriately labeled; collecting volunteer contributions ("crowdsourcing") is a potentially cost- and time-effective way to generate these kinds of datasets.  To test this, the research team will extend an existing platform that allows people to talk and collaborate around video streams in order to attract and retain participants interested in helping train robots.  They will present volunteers watching their stream with simulated robots that are increasingly complex over time, asking volunteers both to suggest appropriate commands that might work well with a particular robot's structure and to give the robots feedback about how well they carry out the commands.  Although at first the robots will simply try different possible motions, volunteers' feedback will help the robots learn which motions help them carry out commands, while providing the volunteers direct experience with robotics research that might increase their interest in and acceptance of robots in the future.  To encourage long-term participation, the team will also add a number of interface features to the stream that allow volunteers more control over the robots being displayed as well as information about the effectiveness of their training efforts.  The research  will result in insights into robot learning, the production of large datasets that could be used in other robotics research, and the validation of strategies (such as gradually increasing the complexity of tasks and the demonstration of progress) for encouraging people to participate in crowdsourcing systems.  This, too, is an important general question, as crowdsourcing is useful in many socially relevant domains, including building open source software, constructing information resources, and supporting citizen science.  Finally, the team will make the supporting software and data available to researchers in domains such as linguistics and developmental psychology that are also interested in language acquisition.&lt;br/&gt; &lt;br/&gt;To do this, the team will leverage an existing prototype system that is based on a live video streaming platform.  This system presents a robot, then cycles through a command selection period where the content people type into the chat window is interpreted as potential commands, and a command response period where the robot performs some action and chat input is interpreted as positive or negative reinforcement for robot learning.  An initial trial generated much interest and useful data for learning, but contributions rapidly dropped because the robots did not learn in real-time and did not change.  To improve learning, the team will integrate robot control algorithms that leverage crowd input to learn appropriate per-command behaviors online, as well as visual indicators of the amount of learning that might incentivize participation through showing the value of volunteers' contributions.  To improve variety, the team will slowly evolve the robot's morphology, adding additional appendages and sensors that support the exploration and learning of new commands while helping maintain volunteers' interest.  In addition to evaluating the effectiveness of the data itself in supporting robot learning of commands, the team will measure the levels, frequency, and appropriateness of contributions to evaluate whether changes in responsiveness and robot complexity in fact increase people's willingness to participate, as well as hypotheses about how the apparent capabilities of robots affect the ways people choose to interact with them.  The long-term goal is to build a large, sustained community around robot teaching that can help evolutionary biologists, social scientists, cognitive scientists, neuroscientists, linguists, and roboticists all pose and answer novel hypotheses around the co-evolution of humans and robots.</AbstractNarration>
<MinAmdLetterDate>07/25/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/25/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1649175</AwardID>
<Investigator>
<FirstName>Joshua</FirstName>
<LastName>Bongard</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joshua Bongard</PI_FULL_NAME>
<EmailAddress>jbongard@uvm.edu</EmailAddress>
<PI_PHON>8026564665</PI_PHON>
<NSF_ID>000177438</NSF_ID>
<StartDate>07/25/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Vermont &amp; State Agricultural College</Name>
<CityName>Burlington</CityName>
<ZipCode>054050160</ZipCode>
<PhoneNumber>8026563660</PhoneNumber>
<StreetAddress>85 South Prospect Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Vermont</StateName>
<StateCode>VT</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VT00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066811191</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF VERMONT &amp; STATE AGRICULTURAL COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>066811191</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Vermont & State Agricultural College]]></Name>
<CityName/>
<StateCode>VT</StateCode>
<ZipCode>054050160</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Vermont</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VT00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~123136</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary outcome of this award is a website that allows participants to teach simulated robots to perform tasks of their choosing. The website is available at twitch.tv slash twitchplaysrobotics. In brief, participants type in, in plain English, actions that the robots should perform, such as`walk to the left' or 'move toward the red cube'. These English commands are encoded in a form that the robots can hear. When they hear one of these commands, they perform some action, and participants then indicate whether the robot obeyed or disobeyed that command. This crowdsourced feedback is used to evolve populations of simulated robots that, over time, evolve to obey an increasing number of the crowd-issued commands.Importantly, the website is now self-sustaining and will continue to allow participants to interact with the evolving robots without further financial or human resources input.The intellectual merit of this outcome is that we found that participants are happy to participate without financial incentive and, most of the time, they provide truthful feedback to the robots. Moreover, we have found that the body plan of the robot influenced how well they are able to evolve to obey the crowd's commands. This indicates a previously unknown relationship between body plan and evolvability.The broader impact of this work is that it demonstrate one possible path to creating safe AI: robots that `obey' a given command but not in a way that humans approve of receive negative reinforcement from the crowd and are thus likely to be deleted. An example of this is if the crowd issues the command to not touch an object representing a human in the simulation, but the robot pushes another object at high speed into the simulated human, the crowd is likely to punish this robot with negative feedback.</p><br> <p>            Last Modified: 08/13/2018<br>      Modified by: Joshua&nbsp;Bongard</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary outcome of this award is a website that allows participants to teach simulated robots to perform tasks of their choosing. The website is available at twitch.tv slash twitchplaysrobotics. In brief, participants type in, in plain English, actions that the robots should perform, such as`walk to the left' or 'move toward the red cube'. These English commands are encoded in a form that the robots can hear. When they hear one of these commands, they perform some action, and participants then indicate whether the robot obeyed or disobeyed that command. This crowdsourced feedback is used to evolve populations of simulated robots that, over time, evolve to obey an increasing number of the crowd-issued commands.Importantly, the website is now self-sustaining and will continue to allow participants to interact with the evolving robots without further financial or human resources input.The intellectual merit of this outcome is that we found that participants are happy to participate without financial incentive and, most of the time, they provide truthful feedback to the robots. Moreover, we have found that the body plan of the robot influenced how well they are able to evolve to obey the crowd's commands. This indicates a previously unknown relationship between body plan and evolvability.The broader impact of this work is that it demonstrate one possible path to creating safe AI: robots that `obey' a given command but not in a way that humans approve of receive negative reinforcement from the crowd and are thus likely to be deleted. An example of this is if the crowd issues the command to not touch an object representing a human in the simulation, but the robot pushes another object at high speed into the simulated human, the crowd is likely to punish this robot with negative feedback.       Last Modified: 08/13/2018       Submitted by: Joshua Bongard]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
