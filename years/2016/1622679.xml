<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SCH: INT: Collaborative Research: Assistive Integrative Support Tool for Retinopathy of Prematurity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2016</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>407404.00</AwardTotalIntnAmount>
<AwardAmount>407404</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Retinopathy of prematurity (ROP) is a leading cause of childhood visual loss worldwide, and the social burdens of infancy-acquired blindness are enormous. Early diagnosis is critically important for successful treatment, and can prevent most cases of blindness. However, lack of access to expert medical diagnosis and care, especially in rural areas, remains a growing healthcare challenge. In addition, clinical expertise in ROP is lacking, and medical professionals are struggling to meet the increasing need for ROP care. As point-of-care technologies for diagnosis and intervention are rapidly expanding, the potential ability to assess ROP severity from any location with an internet connection and a camera, even without immediate ophthalmologic consultation available, could significantly improve delivery of ROP care by identifying infants who are in most urgent need for referral and treatment. This would dramatically reduce the incidence of blindness without a proportionate increase in the need for human resources, which take many years to develop. &lt;br/&gt;&lt;br/&gt;This project develops a prototype assistive integrative support tool for ROP, featuring a modular design comprising: (a) image analysis, (b) information fusion of clinical, imaging, and diagnostic data, and (c) generative probabilistic and regression models with associated computationally efficient machine learning algorithms. The outcomes of the project include disease severity metrics and diagnostic estimates obtained through clinical evidence classifiers trained jointly over expert-generated labels. These labels consist of discrete diagnostic labels, as well as comparison outcomes of relative severity between pairs of images. Random process models for vessel tortuosity and diameter distributions over the retina, as well as patch-based vessel-free image analysis through the use of convolutional neural networks on the entire image, enhance and augment feature extraction. Moreover, incorporating severity comparison outcomes through novel hard and soft constraint methods force inferred severity to agree with ordinal information provided by experts and address inherent uncertainty in expert ground-truth labels. The above severity inference methods are evaluated and fine-tuned over a broad array of generative models, both through retrospective analysis, including cross-validation, longitudinal tests, and tests across multiple sites, as well as through prospective analysis, evaluating its real-world clinical impact.</AbstractNarration>
<MinAmdLetterDate>07/27/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1622679</AwardID>
<Investigator>
<FirstName>Kemal</FirstName>
<LastName>Sonmez</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kemal Sonmez</PI_FULL_NAME>
<EmailAddress>sonmezk@ohsu.edu</EmailAddress>
<PI_PHON>5033463763</PI_PHON>
<NSF_ID>000517960</NSF_ID>
<StartDate>07/27/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Chiang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Chiang</PI_FULL_NAME>
<EmailAddress>chiangm@ohsu.edu</EmailAddress>
<PI_PHON>5034947784</PI_PHON>
<NSF_ID>000712802</NSF_ID>
<StartDate>07/27/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Campbell</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John Campbell</PI_FULL_NAME>
<EmailAddress>campbelp@ohsu.edu</EmailAddress>
<PI_PHON>5034947784</PI_PHON>
<NSF_ID>000712803</NSF_ID>
<StartDate>07/27/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robison</FirstName>
<LastName>Chan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robison Chan</PI_FULL_NAME>
<EmailAddress>rvpchan@uic.edu</EmailAddress>
<PI_PHON>3129962862</PI_PHON>
<NSF_ID>000713682</NSF_ID>
<StartDate>07/27/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon Health &amp; Science University</Name>
<CityName>Portland</CityName>
<ZipCode>972393098</ZipCode>
<PhoneNumber>5034947784</PhoneNumber>
<StreetAddress>3181 S W Sam Jackson Park Rd</StreetAddress>
<StreetAddress2><![CDATA[Mail Code L106OPAM]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>096997515</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OREGON HEALTH &amp; SCIENCE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>096997515</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Oregon Health & Science University]]></Name>
<CityName>Portland</CityName>
<StateCode>OR</StateCode>
<ZipCode>972393098</ZipCode>
<StreetAddress><![CDATA[3181 SW Sam Jackson Park Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramElement>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8062</Code>
<Text>SCH Type II: INT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~407404</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Over the past 4 years, we have explored the use of deep neural networks for retinal vessel segmentation and direct classification of retinal images into three categories (plus, preplus, normal) using absolute labels (with the consensus label of five experts as target). We created a probabilistic model (based on linear logistic classifier parametrization) for learning from both absolute and comparison labels, transition of this model to use neural networks in the next iteration, active learning algorithms, tracing, segmentation &amp; feature extraction, as well as continuous-valued disease severity assessment in concordance with classification and comparison labels. We conducted analysis of automatically learned features (as opposed to extraction of features from manually segmented vessels) using occlusion and t-SNE embedding. We then tracked the disease over time in patient cohorts using a simple severity score. We ultimately developed a fully-automated disease severity assessment system for ROP diagnosis using convolutional neural networks (&ldquo;deep learning&rdquo;). This was developed and validated using the entire data set of images, for which we have developed rigorous reference standards using previously-published methods (Ryan et al, 2014). We have developed a convolutional neural network for detecting image quality which had AUC 0.960 for classifying &ldquo;acceptable quality&rdquo; images vs &ldquo;not acceptable&rdquo; images. For real-world use, it is important to balance imageability (percent of images acceptable for analysis) with diagnostic performance (sensitivity and specificity). We have explored methods of automated view detection, montaging images for analysis of larger fields of view, impact of the individual photographer, and impact of image field of view on quality and diagnosis. We utilized deep learning methods to examine clinically-relevant factors involving disease progression (e.g. how to identify infants at risk for progression, how to quantify these changes, how to identify improvement post-treatment for severe ROP, whether there is quantitative evidence that infants treated with different methods improve at different rates). We developed and validated a quantitative ROP vascular severity scale (1-9) to examine clinically-relevant factors involving disease progression (e.g. how to identify infants at risk for progression, how to quantify these changes, how to identify improvement post-treatment for severe ROP, whether there is quantitative evidence that infants treated with different methods improve at different rates) and disease regression after treatment (e.g. how to recognize improvement, how to identify deviation from expected improvement after treatment).</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>We completed FDA pre-submission paperwork for approval as an assistive medical device and recently have been granted designation as a Breakthrough Device for our proposed intended use by trained health care providers in infants who are currently undergoing screening examination for ROP. Real-world adoption of these systems is limited because there has never been prospective validation of an artificial intelligence system for ROP and because of challenges in automatically identifying images of adequate diagnostic quality. We have continued to prospectively recruit subjects and obtain images during routine ROP exams and these are entered into the system to produce a quantitative ROP vascular severity score (1-9). These scores are then compared with a reference standard diagnosis (based on clinical exam diagnosis and remote image diagnosis from multiple ROP experts). We are currently comparing the prospectively gathered images with international image data sets and demonstrating high sensitivity not just for the detection of plus disease, but for any baby with treatment-requiring disease in a real-world ROP screening population with the aim of eventually being able to detect predictive features of severe ROP in infants.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/27/2021<br>      Modified by: John&nbsp;Campbell</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Over the past 4 years, we have explored the use of deep neural networks for retinal vessel segmentation and direct classification of retinal images into three categories (plus, preplus, normal) using absolute labels (with the consensus label of five experts as target). We created a probabilistic model (based on linear logistic classifier parametrization) for learning from both absolute and comparison labels, transition of this model to use neural networks in the next iteration, active learning algorithms, tracing, segmentation &amp; feature extraction, as well as continuous-valued disease severity assessment in concordance with classification and comparison labels. We conducted analysis of automatically learned features (as opposed to extraction of features from manually segmented vessels) using occlusion and t-SNE embedding. We then tracked the disease over time in patient cohorts using a simple severity score. We ultimately developed a fully-automated disease severity assessment system for ROP diagnosis using convolutional neural networks ("deep learning"). This was developed and validated using the entire data set of images, for which we have developed rigorous reference standards using previously-published methods (Ryan et al, 2014). We have developed a convolutional neural network for detecting image quality which had AUC 0.960 for classifying "acceptable quality" images vs "not acceptable" images. For real-world use, it is important to balance imageability (percent of images acceptable for analysis) with diagnostic performance (sensitivity and specificity). We have explored methods of automated view detection, montaging images for analysis of larger fields of view, impact of the individual photographer, and impact of image field of view on quality and diagnosis. We utilized deep learning methods to examine clinically-relevant factors involving disease progression (e.g. how to identify infants at risk for progression, how to quantify these changes, how to identify improvement post-treatment for severe ROP, whether there is quantitative evidence that infants treated with different methods improve at different rates). We developed and validated a quantitative ROP vascular severity scale (1-9) to examine clinically-relevant factors involving disease progression (e.g. how to identify infants at risk for progression, how to quantify these changes, how to identify improvement post-treatment for severe ROP, whether there is quantitative evidence that infants treated with different methods improve at different rates) and disease regression after treatment (e.g. how to recognize improvement, how to identify deviation from expected improvement after treatment).        We completed FDA pre-submission paperwork for approval as an assistive medical device and recently have been granted designation as a Breakthrough Device for our proposed intended use by trained health care providers in infants who are currently undergoing screening examination for ROP. Real-world adoption of these systems is limited because there has never been prospective validation of an artificial intelligence system for ROP and because of challenges in automatically identifying images of adequate diagnostic quality. We have continued to prospectively recruit subjects and obtain images during routine ROP exams and these are entered into the system to produce a quantitative ROP vascular severity score (1-9). These scores are then compared with a reference standard diagnosis (based on clinical exam diagnosis and remote image diagnosis from multiple ROP experts). We are currently comparing the prospectively gathered images with international image data sets and demonstrating high sensitivity not just for the detection of plus disease, but for any baby with treatment-requiring disease in a real-world ROP screening population with the aim of eventually being able to detect predictive features of severe ROP in infants.          Last Modified: 01/27/2021       Submitted by: John Campbell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
