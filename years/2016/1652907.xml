<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: A New Interaction Model for Eyes-Free Exploration of Touch Screens</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2017</AwardEffectiveDate>
<AwardExpirationDate>02/28/2022</AwardExpirationDate>
<AwardTotalIntnAmount>550000.00</AwardTotalIntnAmount>
<AwardAmount>550000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Touch-based user interfaces, which have become ubiquitous in personal computers and mobile devices, typically offer only limited accessibility to blind and visually impaired users because they rely heavily upon visual interaction and provide little if any audio or haptic feedback.  This creates an accessibility barrier that affects millions of people in the United States.  The problem is compounded by the interaction model supported by most touch interfaces, which assumes that a blind user will explore via a single finger and interact with only one on-screen item at a time; as a consequence, blind users cannot easily browse or skim through large amounts of information or interact with on-screen content using gestures.  These problems are further exacerbated by a lack of coordination between the touch input framework and the speech output engine.  This research will substantially improve the accessibility and usability of touch-based interfaces for blind users through the creation of a new model that supports robust eyes-free interaction with touch screen applications and tangible user interfaces, which will significantly improve touch exploration of complex data by blind users and also by sighted people (e.g. to avoid distractions while driving).  Students with disabilities will be recruited as members of the research team.  Additional broad impact will be achieved through educational activities that engage computing students in the design of accessible user interfaces; to this end, modules on universal design of eyes-free user interfaces for courses on user-centered design and tangible computing will be created.  Outreach activities will include a series of inclusive design workshops in which persons with disabilities will be invited to participate.&lt;br/&gt;&lt;br/&gt;The research will proceed in several stages.  First, to identify benchmarks and metrics for natural eyes-free exploration, formative studies will be conducted with blind individuals and teachers of the visually impaired.  A new interaction model that supports eyes-free exploration of, and interaction with, touch screen user interfaces will then be developed that supports the hand poses, movements, gestures and navigation strategies preferred by blind users, and will be implemented as an input framework that can be embedded in existing touch-based applications.  As part of this work, new gesture recognizers and a new text-to-speech engine will be developed and evaluated.  Finally, authoring tools that enable scientists, artists, teachers, and others to adapt their existing data to support non-visual touch interaction with both touch screens and 3D-printed tactile graphics will be developed through iterative testing with blind and sighted users, and their teachers, and through the public deployment of reference applications including an interactive map for learning public transit routes in a large urban area and a tool for exploring interactive museum exhibits.</AbstractNarration>
<MinAmdLetterDate>03/01/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/26/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1652907</AwardID>
<Investigator>
<FirstName>Shaun</FirstName>
<LastName>Kane</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shaun Kane</PI_FULL_NAME>
<EmailAddress>shaun.kane@colorado.edu</EmailAddress>
<PI_PHON>3037357209</PI_PHON>
<NSF_ID>000611397</NSF_ID>
<StartDate>03/01/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090572</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, Room 481]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~98287</FUND_OBLG>
<FUND_OBLG>2018~94644</FUND_OBLG>
<FUND_OBLG>2019~97607</FUND_OBLG>
<FUND_OBLG>2020~127601</FUND_OBLG>
<FUND_OBLG>2021~131861</FUND_OBLG>
</Award>
</rootTag>
