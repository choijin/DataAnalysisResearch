<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research in DRMS: On the Evaluation of Beliefs: A Method for Assessing Credibility in Subjective Probability Judgment</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>15915.00</AwardTotalIntnAmount>
<AwardAmount>15915</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert O'Connor</SignBlockName>
<PO_EMAI>roconnor@nsf.gov</PO_EMAI>
<PO_PHON>7032927263</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Uncertainty is a pervasive feature of the world we live in. In the face of uncertainty, decision makers cannot observe probabilities directly and must instead base their choices on subjective beliefs. These beliefs are often expressed as subjective probability judgments (SPJs), and their quality is constrained by the knowledge of the individuals who hold them. Consequently, decision making under uncertainty requires that decision makers account for the quality of the judgments at their disposal. In many of the areas that concern us the most (e.g., health, safety, and the protection of the environment), however, decisions often hinge on the likelihood of single events such as the success of a surgery, the growth of a mutual fund, or the melting of the polar ice caps. As a result, evaluating SPJs is either impractical or impossible in many areas of decision making. To circumvent this issue, research on judgment under uncertainty has often been forced to evaluate SPJs relative to (a) laboratory situations in which base rates or relative frequencies are known; (b) sets of judgments for which probability theory demands some qualitative ordering or relationship; or (c) the observed outcomes of uncertain events. Thus, while informative, the extensive body of research on judgment under uncertainty is often of little use to real-world decision makers when attempting to identify their best course of action, ex ante. This dissertation develops a method for assessing the quality of SPJs when benchmarks (e.g., base rates, outcomes) are unknown. &lt;br/&gt;&lt;br/&gt;Specifically, the present research develops a method for measuring the degree to which an individual's SPJs tend to agree with the optimized, aggregate "wisdom of the crowds", also known as credibility. To do this, we will conduct several probability forecasting tournaments and regress each individual's SPJs on optimized aggregates, calculated using methods developed by the Good Judgment Project. The estimated parameters of these models are indices of an individual's bias; expertise; and acuity (measured as the standard error of the regression) in subjective probability judgment. This work improves upon previous methods for assessing uncertainty in at least two ways. First, the credibility framework developed here can be used to evaluate SPJs when normative benchmarks (e.g., base rates, outcomes, repeated measurements) are unavailable or unknown. Second, preliminary evidence suggests that modeling judgments in this way can provide decision makers with an empirical method for correcting "errors" and "biases" in subjective probability judgment. Additionally, because predictions in domains such as military intelligence, climate science, and epidemiology tend be highly uncertain and errors prohibitively costly, decision makers in these domains may benefit from even marginal gains in their ability to evaluate SPJs. The present research facilitates this evaluation by providing straightforward measures of a source's quality, independent of empirical outcomes. In doing so, this research improves the quality and evaluability of decision making across a wide variety of domains.</AbstractNarration>
<MinAmdLetterDate>02/01/2017</MinAmdLetterDate>
<MaxAmdLetterDate>02/01/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1658685</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Baron</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan Baron</PI_FULL_NAME>
<EmailAddress>baron@upenn.edu</EmailAddress>
<PI_PHON>0000000000</PI_PHON>
<NSF_ID>000108252</NSF_ID>
<StartDate>02/01/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joshua</FirstName>
<LastName>Baker</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joshua D Baker</PI_FULL_NAME>
<EmailAddress>ibak@sas.upenn.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000728647</NSF_ID>
<StartDate>02/01/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Trustees of the University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046204</ZipCode>
<StreetAddress><![CDATA[3720 Walnut Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1321</Code>
<Text>Decision, Risk &amp; Mgmt Sci</Text>
</ProgramElement>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~15915</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Effective decision making often depends on the probability of uncertain events such as the success of a medical procedure or the outcome of an election. In many cases, however, it is impossible to calculate or observe these probabilities because the event in question will only ever happen once. When faced with this type of situation, decision makers often have to rely on subjective probability judgments, which are statements like &ldquo;there&rsquo;s an 80% chance that it will happen.&rdquo; In general, these statements are usually understood to reflect someone's&nbsp;<em>beliefs</em>&nbsp;about how likely the event is to occur.</p> <p>The problem, of course, is that beliefs can sometimes be wrong. As a result, decision makers generally do not know how much they should trust subjective probability judgments. To address this problem, the goal of our research was to develop a statistical tool that could be used to assess the &ldquo;quality&rdquo; of someone&rsquo;s beliefs. To do this, our tool would use what we already know about crowdsourcing (pooling judgments or beliefs across a large number of people) to come up with something similar to the &ldquo;best possible&rdquo; subjective probability judgment that someone could make. Our tool would then compare these &ldquo;best possible&rdquo; judgments with the judgments of ordinary people to see if there were any patterns that could explain why a given person&rsquo;s judgments were less accurate than they could be. Once our tool had identified these patterns, we could then (a) evaluate the relative &ldquo;quality&rdquo; of different people&rsquo;s judgments; and (b) &ldquo;undo&rdquo; or &ldquo;reverse&rdquo; the negative aspects of each person&rsquo;s pattern to make their judgments better.</p> <p>In support of these goals, funding from the National Science Foundation was used to fund two studies about probabilistic prediction. In the first study, participants made predictions about games in the 2017 March Madness basketball tournament, and in the second study, participants predicted whether the prices of certain stocks would go up over a given period of time. In both studies, money from the present NSF award was used to pay people for their participation.</p> <p>Unfortunately, the results of both studies were inconclusive. In the March Madness study, there was evidence to suggest that our tool was working correctly, but in the end, it did not help improve people&rsquo;s judgments. The reason for this is that the outcomes of March Madness games were so hard to predict that most of our participants were very inaccurate. As a result, our crowdsourcing method for calculating the &ldquo;best possible&rdquo; judgments didn&rsquo;t do much better than ordinary people. When we used our tool to make participants&rsquo; judgments look more like the &ldquo;best possible&rdquo; estimates, therefore, it didn&rsquo;t help very much. In other words, the results of this study showed that our tool might have worked correctly on a different data-set, but it didn&rsquo;t work here because it was too difficult to tell what &ldquo;good judgment&rdquo; was supposed to look like.</p> <p>In the stock prediction study, we were not able to use our tool because we did not have enough data. Despite recruiting almost 100 people to participate in this study, only 12 people completed the whole thing. As a result, we could not use the data we collected to test the effectiveness of our tool.</p> <p>Although these two studies were the only two that were funded by the current NSF award, it is useful to note that other studies we have run suggest that our tool can be very effective. Though the studies described here did not turn out the way we hoped, they were still important because they taught us about when our tool does not work, and why. In real-world decision making, where wealth, wellbeing, and human lives are sometimes on the line, it is often important to get decisions &ldquo;right.&rdquo; As a result, it is very important to know when a tool will work and when it will not. Even though these two studies happened to be cases where our tool did not work, they were still very helpful because they highlighted the limitations of a tool that can often be useful, but that can also be risky in the wrong situation.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/05/2018<br>      Modified by: Joshua D.&nbsp;D&nbsp;Baker</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Effective decision making often depends on the probability of uncertain events such as the success of a medical procedure or the outcome of an election. In many cases, however, it is impossible to calculate or observe these probabilities because the event in question will only ever happen once. When faced with this type of situation, decision makers often have to rely on subjective probability judgments, which are statements like "there?s an 80% chance that it will happen." In general, these statements are usually understood to reflect someone's beliefs about how likely the event is to occur.  The problem, of course, is that beliefs can sometimes be wrong. As a result, decision makers generally do not know how much they should trust subjective probability judgments. To address this problem, the goal of our research was to develop a statistical tool that could be used to assess the "quality" of someone?s beliefs. To do this, our tool would use what we already know about crowdsourcing (pooling judgments or beliefs across a large number of people) to come up with something similar to the "best possible" subjective probability judgment that someone could make. Our tool would then compare these "best possible" judgments with the judgments of ordinary people to see if there were any patterns that could explain why a given person?s judgments were less accurate than they could be. Once our tool had identified these patterns, we could then (a) evaluate the relative "quality" of different people?s judgments; and (b) "undo" or "reverse" the negative aspects of each person?s pattern to make their judgments better.  In support of these goals, funding from the National Science Foundation was used to fund two studies about probabilistic prediction. In the first study, participants made predictions about games in the 2017 March Madness basketball tournament, and in the second study, participants predicted whether the prices of certain stocks would go up over a given period of time. In both studies, money from the present NSF award was used to pay people for their participation.  Unfortunately, the results of both studies were inconclusive. In the March Madness study, there was evidence to suggest that our tool was working correctly, but in the end, it did not help improve people?s judgments. The reason for this is that the outcomes of March Madness games were so hard to predict that most of our participants were very inaccurate. As a result, our crowdsourcing method for calculating the "best possible" judgments didn?t do much better than ordinary people. When we used our tool to make participants? judgments look more like the "best possible" estimates, therefore, it didn?t help very much. In other words, the results of this study showed that our tool might have worked correctly on a different data-set, but it didn?t work here because it was too difficult to tell what "good judgment" was supposed to look like.  In the stock prediction study, we were not able to use our tool because we did not have enough data. Despite recruiting almost 100 people to participate in this study, only 12 people completed the whole thing. As a result, we could not use the data we collected to test the effectiveness of our tool.  Although these two studies were the only two that were funded by the current NSF award, it is useful to note that other studies we have run suggest that our tool can be very effective. Though the studies described here did not turn out the way we hoped, they were still important because they taught us about when our tool does not work, and why. In real-world decision making, where wealth, wellbeing, and human lives are sometimes on the line, it is often important to get decisions "right." As a result, it is very important to know when a tool will work and when it will not. Even though these two studies happened to be cases where our tool did not work, they were still very helpful because they highlighted the limitations of a tool that can often be useful, but that can also be risky in the wrong situation.          Last Modified: 11/05/2018       Submitted by: Joshua D. D Baker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
