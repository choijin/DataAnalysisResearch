<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EXP: Collaborative Research: Perception and Production in Second Language: The Roles of Voice Variability and Familiarity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>276647.00</AwardTotalIntnAmount>
<AwardAmount>276647</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The U.S. economy is boosted by many highly skilled professionals from non-English speaking countries, who work in higher education, healthcare, and technology. These professionals typically have solid knowledge of English but age-related non-native pronunciations that can impair their intelligibility. Highly-skilled professions require advanced oral communication skills, including highly intelligible pronunciation; because such professionals come from a variety of first languages, their pronunciation needs are highly individualized. Thus, traditional classroom instruction cannot easily meet the needs of diverse learners nor allow for significant amounts of one-on-one instruction.  To address this issue, the investigators propose an innovative genre of technology for computer-assisted pronunciation training (CAPT) that allows learners to practice at their own pace, and as often as they wish, with a personalized voice model, increasing the likelihood of improved intelligibility.&lt;br/&gt;&lt;br/&gt;Specifically, the approach builds on research showing that second-language (L2) learners are more likely to succeed when imitating a similar voice, a so-called golden speaker. The proposed CAPT technology provides an ideal golden speaker for each learner: their own voice but with a native accent through a digital learning environment that allows L2 learners to build their golden speakers incrementally and interactively by selecting among different regional US accents.  This vision is enabled by on-going NSF-sponsored research on accent-conversion algorithms to digitally alter L2 utterances with suprasegmental and segmental features from a reference native speaker.  This Cyberlearning project brings accent-conversion techniques into pronunciation instruction in the form of a web-based interactive learning tool. The project uses a design-based research paradigm to (1) gain knowledge of how people learn L2 pronunciation skills while interacting with computational golden speakers; and (2) identify efficient ways of implementing this technology in L2 learning contexts. The project focuses initially on Spanish learners of English, later generalizing the approach to other first language speakers.</AbstractNarration>
<MinAmdLetterDate>07/27/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1623622</AwardID>
<Investigator>
<FirstName>Evgeny</FirstName>
<LastName>Chukharev-Hudilainen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Evgeny Chukharev-Hudilainen</PI_FULL_NAME>
<EmailAddress>evgeny@iastate.edu</EmailAddress>
<PI_PHON>5152946958</PI_PHON>
<NSF_ID>000687921</NSF_ID>
<StartDate>07/27/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Levis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John Levis</PI_FULL_NAME>
<EmailAddress>jlevis@iastate.edu</EmailAddress>
<PI_PHON>5152945225</PI_PHON>
<NSF_ID>000710398</NSF_ID>
<StartDate>07/27/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Iowa State University</Name>
<CityName>AMES</CityName>
<ZipCode>500112207</ZipCode>
<PhoneNumber>5152945225</PhoneNumber>
<StreetAddress>1138 Pearson</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IA04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005309844</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>IOWA STATE UNIVERSITY OF SCIENCE AND TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005309844</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Iowa State University]]></Name>
<CityName>Ames</CityName>
<StateCode>IA</StateCode>
<ZipCode>500112207</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IA04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<ProgramReference>
<Code>8841</Code>
<Text>Exploration Projects</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~276647</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-d7863bbb-7fff-d290-8702-0306651886dd"> </span></p> <p dir="ltr"><span>This award resulted in the interdisciplinary development of an approach to pronunciation training for nonnative speakers of English using Golden Speaker voices. Golden speaker voices are synthesized voices developed using specially-developed speech training algorithms that combine the language learner's own voice with the pronunciation features of a native speaker voice. This creates a new voice that sounds like the language learner's voice but has native pronunciation features. Golden speaker voices have advantages over unsynthesized native voices because the language learner is listening to a very familiar voice (their own) with native-sounding vowels, consonants, rhythm and melody. Using such a familiar voice for language learning creates greater self-awareness of the learner's typical errors and provides the input for more accurate imitation. This award, a collaboration between computer scientists at Texas A&amp;M and applied linguists at Iowa State University, resulted in several publications of note as well as a publicly available Golden Speaker Builder tool (see image for GSB Public) that language learners around the world can use for self-study (https://goldenspeaker.engl.iastate.edu/speech/). Among these publications was a major 2019 article in the Interdisciplinary journal Speech Communication titled "Golden speaker builder?An interactive tool for pronunciation training". In this article we report on the development of the tool and its success in improving the spoken comprehensibility and fluency of 15 Korean learners of English. The 15 learners recorded a number of sentences and pronounced native-like versions of all the phonemes of English in order to provide the input to build their Golden Speaker voice (see image of GSB Speech Communication). Then they took place in nine 30-minute training sessions, and their initial and final productions of trained sentences were rated by native English-speaking listeners who rated how easy it was to understand their pronunciation (comprehensibility) and how smoothly they produced their speech (fluency). The ratings confirmed that they significantly improved in both areas from before they received the training. The participants in the study also enjoyed the training and became more aware of how the Golden Speaker voice included both greater speed as compared to how they usually spoke and how the Golden Speaker voice blended sounds together in ways that they were not aware happened in English.</span></p> <p dir="ltr"><span>A second major study (?Effects of voice type and task on L2 learners? awareness of pronunciation errors?) is published in the proceedings of the annual Interspeech Conference, a major interdisciplinary international conference focused on the learning of second language speech from many different perspectives. In this study, we examined in two experiments whether language learners were more aware of pronunciation errors when listening to two model voices: a Golden Speaker voice and a Silver Speaker voice (a synthesized voice that does not make use of the language learner's voice). In the first experiment, we found that there was no difference in awareness due to the use of a Golden Speaker voice and also that language learners had greater difficulty in "spotting the differences" isn pronunciation between their own voice and mode voices, but in a second experiment we found that spotting the difference became easier when listeners were told how many differences to spot. Surprisingly, we again found no effect of voice type, suggesting that the use of synthesized voices may require explicit training before learners are able to notice their own errors.</span></p> <p dir="ltr"><span>One of the most promising outcomes of this project&nbsp; was the interdisciplinary collaboration between those trained in computer science (the Texas A&amp;M group) and those trained in applied linguistics and language learning (Iowa State's group). Both fields address the pronunciation needs of second language learners, but because both fields study spoken language using different tools, approaches and assumptions, there was unexpected learning from both sides as we confronted our assumptions and learned to approach common problems from very different perspectives. While this was valuable for the primary investigators, it was more valuable for the graduate students who developed flexibility and expertise in looking at approaches to spoken language.</span></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/23/2021<br>      Modified by: John&nbsp;Levis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1623622/1623622_10444192_1624468779311_GSBPublic--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1623622/1623622_10444192_1624468779311_GSBPublic--rgov-800width.jpg" title="GSB Public"><img src="/por/images/Reports/POR/2021/1623622/1623622_10444192_1624468779311_GSBPublic--rgov-66x44.jpg" alt="GSB Public"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Golden Speaker Builder (Publicly available)</div> <div class="imageCredit">John Levis</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">John&nbsp;Levis</div> <div class="imageTitle">GSB Public</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1623622/1623622_10444192_1624468898039_GSBSpeechCommunication--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1623622/1623622_10444192_1624468898039_GSBSpeechCommunication--rgov-800width.jpg" title="GSB Speech Communication"><img src="/por/images/Reports/POR/2021/1623622/1623622_10444192_1624468898039_GSBSpeechCommunication--rgov-66x44.jpg" alt="GSB Speech Communication"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Picture of Golden Speaker Builder used for research studies</div> <div class="imageCredit">John Levis</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">John&nbsp;Levis</div> <div class="imageTitle">GSB Speech Communication</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   This award resulted in the interdisciplinary development of an approach to pronunciation training for nonnative speakers of English using Golden Speaker voices. Golden speaker voices are synthesized voices developed using specially-developed speech training algorithms that combine the language learner's own voice with the pronunciation features of a native speaker voice. This creates a new voice that sounds like the language learner's voice but has native pronunciation features. Golden speaker voices have advantages over unsynthesized native voices because the language learner is listening to a very familiar voice (their own) with native-sounding vowels, consonants, rhythm and melody. Using such a familiar voice for language learning creates greater self-awareness of the learner's typical errors and provides the input for more accurate imitation. This award, a collaboration between computer scientists at Texas A&amp;M and applied linguists at Iowa State University, resulted in several publications of note as well as a publicly available Golden Speaker Builder tool (see image for GSB Public) that language learners around the world can use for self-study (https://goldenspeaker.engl.iastate.edu/speech/). Among these publications was a major 2019 article in the Interdisciplinary journal Speech Communication titled "Golden speaker builder?An interactive tool for pronunciation training". In this article we report on the development of the tool and its success in improving the spoken comprehensibility and fluency of 15 Korean learners of English. The 15 learners recorded a number of sentences and pronounced native-like versions of all the phonemes of English in order to provide the input to build their Golden Speaker voice (see image of GSB Speech Communication). Then they took place in nine 30-minute training sessions, and their initial and final productions of trained sentences were rated by native English-speaking listeners who rated how easy it was to understand their pronunciation (comprehensibility) and how smoothly they produced their speech (fluency). The ratings confirmed that they significantly improved in both areas from before they received the training. The participants in the study also enjoyed the training and became more aware of how the Golden Speaker voice included both greater speed as compared to how they usually spoke and how the Golden Speaker voice blended sounds together in ways that they were not aware happened in English. A second major study (?Effects of voice type and task on L2 learners? awareness of pronunciation errors?) is published in the proceedings of the annual Interspeech Conference, a major interdisciplinary international conference focused on the learning of second language speech from many different perspectives. In this study, we examined in two experiments whether language learners were more aware of pronunciation errors when listening to two model voices: a Golden Speaker voice and a Silver Speaker voice (a synthesized voice that does not make use of the language learner's voice). In the first experiment, we found that there was no difference in awareness due to the use of a Golden Speaker voice and also that language learners had greater difficulty in "spotting the differences" isn pronunciation between their own voice and mode voices, but in a second experiment we found that spotting the difference became easier when listeners were told how many differences to spot. Surprisingly, we again found no effect of voice type, suggesting that the use of synthesized voices may require explicit training before learners are able to notice their own errors. One of the most promising outcomes of this project  was the interdisciplinary collaboration between those trained in computer science (the Texas A&amp;M group) and those trained in applied linguistics and language learning (Iowa State's group). Both fields address the pronunciation needs of second language learners, but because both fields study spoken language using different tools, approaches and assumptions, there was unexpected learning from both sides as we confronted our assumptions and learned to approach common problems from very different perspectives. While this was valuable for the primary investigators, it was more valuable for the graduate students who developed flexibility and expertise in looking at approaches to spoken language.                Last Modified: 06/23/2021       Submitted by: John Levis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
