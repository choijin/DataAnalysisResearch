<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Accurate Preconditioing for Computing Eigenvalues of Large and Extremely Ill-conditioned Matrices</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computations of eigenvalues of large matrices arise in a wide range of scientific and engineering applications, including, for example, page ranking of the Google Search Engine. Large scale eigenvalue problems are often inherently ill-conditioned which implies that their eigenvalues differ vastly in  magnitude. This poses a significant challenge to the existing eigenvalue algorithms in the sense that smaller eigenvalues computed may have a poor accuracy, caused by roundoff errors in computer arithmetic. This project will develop new algorithms to address this numerical difficulty. The research results will have applications in a variety of problems where extreme ill-conditioning arises. In particular, a notable ill-conditioning problem is the biharmonic differential operator, which has been used in modeling and design of rigid elastic structures such as beams, plates, or solids, in constructions of multivariate splines, as well as in geometric modeling and computer graphics. A discrete version of the biharmonic operator has also found applications in circuits, image processing, mesh deformation, and manifold learning. With the discretized biharmonic operators easily becoming extremely ill-conditioned, this research will resolve the numerical accuracy issues of  the existing algorithms for these applications.&lt;br/&gt;&lt;br/&gt;Computing smaller eigenvalues of large and extremely ill-conditioned matrices is an important and intellectually challenging task. Indeed, the effect of ill-conditioning on accuracy is often regarded as an unsolvable problem that is attributable to the formulation of the eigenvalue problem itself. While recent research results have shown that this may be mitigated by exploring structures of matrices, the main objective of this project is to propose an innovative use of preconditioning as a new general methodology to solve the accuracy issue caused by ill-conditioning. We will develop new methods that combine preconditioning with accurate structured inversion methods to accurately compute smaller eigenvalues of an extremely ill-conditioned matrix. As an application, we will also study various discretization schemes and derive suitable structured preconditioners for biharmonic differential operators.</AbstractNarration>
<MinAmdLetterDate>06/15/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/19/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1620082</AwardID>
<Investigator>
<FirstName>Qiang</FirstName>
<LastName>Ye</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qiang Ye</PI_FULL_NAME>
<EmailAddress>qye3@uky.edu</EmailAddress>
<PI_PHON>8592574653</PI_PHON>
<NSF_ID>000482987</NSF_ID>
<StartDate>06/15/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Kentucky Research Foundation</Name>
<CityName>Lexington</CityName>
<ZipCode>405260001</ZipCode>
<PhoneNumber>8592579420</PhoneNumber>
<StreetAddress>109 Kinkead Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<StateCode>KY</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>KY06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>939017877</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF KENTUCKY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007400724</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Kentucky Research Foundation]]></Name>
<CityName>Lexington</CityName>
<StateCode>KY</StateCode>
<ZipCode>405260001</ZipCode>
<StreetAddress><![CDATA[500 S Limestone 109 Kinkead Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>KY06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~58944</FUND_OBLG>
<FUND_OBLG>2017~81684</FUND_OBLG>
<FUND_OBLG>2018~84372</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Intellectual Merit: <br /><br />Accuracy of traditional numerical algorithms for solving a linear system or computing eigenvalues of a matrix is limited by the condition number of the matrix. In this project, we have developed an accurate preconditioning algorithm together with an error analysis to solve an ill-conditioned linear algebra problem with an accuracy independent of the condition number, provided there is an accurate inversion algorithm for the preconditioner. The algorithm allows us to solve some ill-conditioned linear systems to the accuracy of machine precision regardless of the condition number. The accurate solution of linear systems are used to compute eigenvalues to high relative accuracy. Numerical tests have confirmed the stability of the method developed. As part of the analysis, we have introduced a new stability concept, called inverse-equivalent accuracy, that is fundamental in precisely characterizing the stability gained.<br /><br />We have studied applications of preconditioning to neural network optimization by developing a novel preconditioning method, called batch normalization preconditioning, to accelerate convergence of neural network training. Our algorithm is rooted in a very effective neural network training method called Batch Normalization (BN), but is derived from a well-established theoretical framework of preconditioning. We have developed a theory to show the batch normalization preconditioning reduces the condition number of Hessian matrix. Our theory is also used to derive a BN method for Convolutional Neural Networks. Extensive experiments have been carried out that demonstrate that our method is highly competitive to BN and, in particular, works with small mini-batches whereas BN does not. We have also developed a new optimization algorithm with a convergence theory that combines the classical nonlinear conjugate gradient with the momentum methods, called adaptive momentum method. Our method has the convergence characteristic of the nonlinear conjugate gradient without the need for line search at each step, which is important in large scale optimization problems such as neural networks. We have applied the adaptive momentum method to convolutional neural network training for the image classification problems, which shows significant improvements over the classical momentum method as well as the Nesterov momentum method.<br /><br />We have studied stable constructions of orthogonal or unitary matrices in recurrent neural networks (RNNs). We have developed two new methods to construct a recurrent matrix that is orthogonal/unitary to mitigate the exploding/vanishing gradient problem. One method is for real orthogonal matrix and the other is for complex unitary matrix. Both of our methods have outperformed the standard RNNs and their variants on a variety of benchmark testing problems. We have further extended the methods to allow construction of a recurrent matrix with a 2x2 block upper triangular matrix with the diagonal blocks consisting of an orthogonal matrix and a matrix with the spectral radius less than 1. This remedies some disadvantages of an orthogonal recurrent matrix and has led to much improved performance. Our methods have produced state-of-the-art results in several benchmark datasets.<br /><br />We have proposed a new optimization algorithm for generative adversarial networks (GANs) by introducing an adaptive weighting scheme to calibrate simultaneous optimizations of two losses in GAN's discriminator. The methods have produced some very competitive results, including state-of-the-arts in inception scores for several benchmark datasets.<br /><br />We have derived conditions on a convolution kernel that can generate or preserve symmetric structures in features. We have applied our method to the sequential recommendation problem and to the RNA secondary structure inference problem. Our experimental results demonstrate that our approaches can improve inference results while using equal or smaller numbers of machine parameters. <br /><br />Broader Impact:<br /><br />We have applied the accurate preconconditioning method to compute a few smallest eigenvalues of a finite difference discretization of the biharmonic operator with an accuracy in the order of machine precision. This was not possible without using higher precision by existing methods. The biharmonic eigenvalue problems arise in many applications.<br /><br />We have collaborated with colleagues in mathematical biology and bioinformatics to study two applications of RNNs. First, we have successfully derived an LSTM method (a variation of RNN) for the state inference of RNA secondary structure that significantly outperforms traditional methods in classification accuracy. Second, we have derived an LSTM implementation for automatic detections of epileptic seizures through analysis of the electroencephalography (EEG) signals and our method has better performance compared with the current state-of-the-art methods and is more robust across patients. Finally, four computer codes derived from this project have been freely distributed at the open source platform GitHub.</p><br> <p>            Last Modified: 10/01/2020<br>      Modified by: Qiang&nbsp;Ye</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit:   Accuracy of traditional numerical algorithms for solving a linear system or computing eigenvalues of a matrix is limited by the condition number of the matrix. In this project, we have developed an accurate preconditioning algorithm together with an error analysis to solve an ill-conditioned linear algebra problem with an accuracy independent of the condition number, provided there is an accurate inversion algorithm for the preconditioner. The algorithm allows us to solve some ill-conditioned linear systems to the accuracy of machine precision regardless of the condition number. The accurate solution of linear systems are used to compute eigenvalues to high relative accuracy. Numerical tests have confirmed the stability of the method developed. As part of the analysis, we have introduced a new stability concept, called inverse-equivalent accuracy, that is fundamental in precisely characterizing the stability gained.  We have studied applications of preconditioning to neural network optimization by developing a novel preconditioning method, called batch normalization preconditioning, to accelerate convergence of neural network training. Our algorithm is rooted in a very effective neural network training method called Batch Normalization (BN), but is derived from a well-established theoretical framework of preconditioning. We have developed a theory to show the batch normalization preconditioning reduces the condition number of Hessian matrix. Our theory is also used to derive a BN method for Convolutional Neural Networks. Extensive experiments have been carried out that demonstrate that our method is highly competitive to BN and, in particular, works with small mini-batches whereas BN does not. We have also developed a new optimization algorithm with a convergence theory that combines the classical nonlinear conjugate gradient with the momentum methods, called adaptive momentum method. Our method has the convergence characteristic of the nonlinear conjugate gradient without the need for line search at each step, which is important in large scale optimization problems such as neural networks. We have applied the adaptive momentum method to convolutional neural network training for the image classification problems, which shows significant improvements over the classical momentum method as well as the Nesterov momentum method.  We have studied stable constructions of orthogonal or unitary matrices in recurrent neural networks (RNNs). We have developed two new methods to construct a recurrent matrix that is orthogonal/unitary to mitigate the exploding/vanishing gradient problem. One method is for real orthogonal matrix and the other is for complex unitary matrix. Both of our methods have outperformed the standard RNNs and their variants on a variety of benchmark testing problems. We have further extended the methods to allow construction of a recurrent matrix with a 2x2 block upper triangular matrix with the diagonal blocks consisting of an orthogonal matrix and a matrix with the spectral radius less than 1. This remedies some disadvantages of an orthogonal recurrent matrix and has led to much improved performance. Our methods have produced state-of-the-art results in several benchmark datasets.  We have proposed a new optimization algorithm for generative adversarial networks (GANs) by introducing an adaptive weighting scheme to calibrate simultaneous optimizations of two losses in GAN's discriminator. The methods have produced some very competitive results, including state-of-the-arts in inception scores for several benchmark datasets.  We have derived conditions on a convolution kernel that can generate or preserve symmetric structures in features. We have applied our method to the sequential recommendation problem and to the RNA secondary structure inference problem. Our experimental results demonstrate that our approaches can improve inference results while using equal or smaller numbers of machine parameters.   Broader Impact:  We have applied the accurate preconconditioning method to compute a few smallest eigenvalues of a finite difference discretization of the biharmonic operator with an accuracy in the order of machine precision. This was not possible without using higher precision by existing methods. The biharmonic eigenvalue problems arise in many applications.  We have collaborated with colleagues in mathematical biology and bioinformatics to study two applications of RNNs. First, we have successfully derived an LSTM method (a variation of RNN) for the state inference of RNA secondary structure that significantly outperforms traditional methods in classification accuracy. Second, we have derived an LSTM implementation for automatic detections of epileptic seizures through analysis of the electroencephalography (EEG) signals and our method has better performance compared with the current state-of-the-art methods and is more robust across patients. Finally, four computer codes derived from this project have been freely distributed at the open source platform GitHub.       Last Modified: 10/01/2020       Submitted by: Qiang Ye]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
