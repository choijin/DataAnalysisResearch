<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Sketching Geometry and Physics Informed Inference for Mobile Robot Manipulation in Cluttered Scenes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>400857.00</AwardTotalIntnAmount>
<AwardAmount>400857</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to improve the ability of robots to manipulate and interact with objects, such as when assisting people to support their daily activities.  The key idea is that people can provide robots with important information about their environment and the objects within their environment. In particular, people can use their cognitive skills to name objects, provide an understanding of the geometrical structure of objects, and describe an object's behavior in relation to other objects. Specifically, the project will develop a natural user interface that enables people to provide such information by drawing and sketching on top of the robot's view of the world.  Physical simulation will then be used to fill in the missing gaps needed for a robot to complete autonomous manipulation tasks. Thus, the project aims to combine object sketching and physical simulation to better support mobile manipulation tasks as well as learn to perform new manipulation tasks when encountered.  The project will support a "Put That There" task, where a user can simply give high-level manipulation commands, with the robot filling in the details necessary to complete the task in a cluttered environment.&lt;br/&gt;&lt;br/&gt;This project aims to improve goal-directed dexterous robotic manipulation in cluttered and unstructured environments through sketching and physical simulation. Robots operating in human environments face considerable uncertainty in perception due to physical contact and occlusions between objects. This project will address such perceptual uncertainty by combining methods for probabilistic inference with natural sketch-based interfaces to extract, label, and automatically infer the geometry, pose, and behavior of objects in complicated scenes.  From a human usability perspective, the project addresses how to best create a sketching language and interfaces for intuitive human-in-the-loop extraction of object geometries and behavior from robot sensing.  The planned exploration into sketching methods will also explore what underlying representations, raw point clouds, RGB images and video, or RGBD images will be most conducive to supporting accurate geometry extraction and grasp location identification.  Given sketched objects, the project will develop probabilistic physically plausible methods for scene estimation that will enable perception for manipulation in cluttered environments.  These methods build upon advances in physical simulation to constrain scene estimates to only plausible configurations to both improve estimation accuracy and enable computational tractability.  The project will also develop a "Put That There" testbed using a tablet-based web application to support exploration of these concepts as well as act as user studies to evaluate geometry extraction accuracy and the robustness of physics-based scene estimation algorithms.</AbstractNarration>
<MinAmdLetterDate>08/10/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1638047</AwardID>
<Investigator>
<FirstName>Odest</FirstName>
<LastName>Jenkins</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Odest C Jenkins</PI_FULL_NAME>
<EmailAddress>ocj@umich.edu</EmailAddress>
<PI_PHON>7347636985</PI_PHON>
<NSF_ID>000059675</NSF_ID>
<StartDate>08/10/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName>Ann Arbor</CityName>
<StateCode>MI</StateCode>
<ZipCode>481092121</ZipCode>
<StreetAddress><![CDATA[2260 Hayward, 3644 Beyster Bldg.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~400857</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This project has led to the development of Semantic Robotic Programming as a new declarative approach to robot programming from human demonstration.&nbsp; Declarative forms of robot programming by demonstration, including Semantic Robot Programming, offers a much more intuitive and fluent means to customize and adapt robots by real people in the real world for real tasks.&nbsp; Semantic Robot Programming aims to enable people to teach tasks to robots declaratively as ?what to do? as we might for teaching another person, instead of procedurally ?how to do? through specific prescribed motions.&nbsp; More broadly, work for this project could open new possibilities that better bridge the wealth of methods in Artificial Intelligence with advances in modern Robotics.&nbsp; To enable Semantic Robot Programming, this project has addressed a critical need for semantic perception, as a scene graph, of a physical environment by a robot through generative-discriminative inference. This project found that probabilistic methods for inference in artificial intelligence has the potential to address the shortcomings of neural networks and deep learning with respect to accuracy and robustness.&nbsp; This project has been a good step towards revisiting probabilistic inference with insights into its efficient implementation, where energy consumption by modern neural networks pose a considerable scalability and climate challenge. Efficient probabilistic inference could have a renaissance back into the mainstream of AI, similar to how big data and GPUs enabled the comeback of neural networks over the last decade.</span></p><br> <p>            Last Modified: 01/18/2020<br>      Modified by: Odest&nbsp;C&nbsp;Jenkins</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1638047/1638047_10449108_1579376775032_srp_illustration--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1638047/1638047_10449108_1579376775032_srp_illustration--rgov-800width.jpg" title="Semantic Robot Programming"><img src="/por/images/Reports/POR/2020/1638047/1638047_10449108_1579376775032_srp_illustration--rgov-66x44.jpg" alt="Semantic Robot Programming"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Semantic Robot Programming allows for human users to teach robots tasks intuitively through  demonstration.  Robots learn the goal of the demonstrated task, which it can then robustly perform in a wide variety of conditions and scenarios.</div> <div class="imageCredit">Zhen Zeng, Odest Chadwicke Jenkins</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Odest&nbsp;C&nbsp;Jenkins</div> <div class="imageTitle">Semantic Robot Programming</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1638047/1638047_10449108_1579375941748_SRP_robot_perception--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1638047/1638047_10449108_1579375941748_SRP_robot_perception--rgov-800width.jpg" title="Robot perception of articulated objects enabled by nonparametric belief propagation methods developed"><img src="/por/images/Reports/POR/2020/1638047/1638047_10449108_1579375941748_SRP_robot_perception--rgov-66x44.jpg" alt="Robot perception of articulated objects enabled by nonparametric belief propagation methods developed"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Robot perception of articulated objects enabled by nonparametric belief propagation methods developed</div> <div class="imageCredit">Karthik Desingh, Odest Chadwicke Jenkins</div> <div class="imageSubmitted">Odest&nbsp;C&nbsp;Jenkins</div> <div class="imageTitle">Robot perception of articulated objects enabled by nonparametric belief propagation methods developed</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has led to the development of Semantic Robotic Programming as a new declarative approach to robot programming from human demonstration.  Declarative forms of robot programming by demonstration, including Semantic Robot Programming, offers a much more intuitive and fluent means to customize and adapt robots by real people in the real world for real tasks.  Semantic Robot Programming aims to enable people to teach tasks to robots declaratively as ?what to do? as we might for teaching another person, instead of procedurally ?how to do? through specific prescribed motions.  More broadly, work for this project could open new possibilities that better bridge the wealth of methods in Artificial Intelligence with advances in modern Robotics.  To enable Semantic Robot Programming, this project has addressed a critical need for semantic perception, as a scene graph, of a physical environment by a robot through generative-discriminative inference. This project found that probabilistic methods for inference in artificial intelligence has the potential to address the shortcomings of neural networks and deep learning with respect to accuracy and robustness.  This project has been a good step towards revisiting probabilistic inference with insights into its efficient implementation, where energy consumption by modern neural networks pose a considerable scalability and climate challenge. Efficient probabilistic inference could have a renaissance back into the mainstream of AI, similar to how big data and GPUs enabled the comeback of neural networks over the last decade.       Last Modified: 01/18/2020       Submitted by: Odest C Jenkins]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
