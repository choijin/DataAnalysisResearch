<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Desktop Co-robotic Assistant for Graphics and Text Access and Creation for Individuals who are Blind or Visually Impaired</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2017</AwardEffectiveDate>
<AwardExpirationDate>01/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>303004.00</AwardTotalIntnAmount>
<AwardAmount>322004</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07020000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CBET</Abbreviation>
<LongName>Div Of Chem, Bioeng, Env, &amp; Transp Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Grace Hwang</SignBlockName>
<PO_EMAI>ghwang@nsf.gov</PO_EMAI>
<PO_PHON>7032924271</PO_PHON>
</ProgramOfficer>
<AbstractNarration>1605226 - Pawluk&lt;br/&gt;&lt;br/&gt;In the U.S. alone, there are approximately 8 million individuals who are blind or visually impaired. In order for these individuals to reach their potential and be competitive with their peers, it is important that they have equivalent access to information to ensure timely completion of education and work tasks. Computers have become essential in these environments, creating information and collaborating with others, whether in the same room or remotely over the web. A lot of this interaction is highly spatial in nature, whether in the use and creation of graphics, interacting with others through a mouse or cursor, or searching for information in the spatial layout of text on a web page. For individuals who are blind or visually impaired, this spatial information is largely inaccessible. This project is intended to address this issue by developing a robotic assistant to provide haptic and Braille input and output to a computer, as well as aid an individual in exploration of the computer window.&lt;br/&gt;&lt;br/&gt;The main goal of this grant is to provide a coherent haptic system that provides effective input and output of graphics and Braille on a full haptic page/window through a single device that can be used in a single hand posture. The objectives are to first develop a desktop co-robot to provide shared control with the user over a haptic interface to aid in exploring graphical and full page Braille information.  A mouse-shaped tactile input/output component will be mounted on the co-robot to provide spatially distributed tactile feedback to the second and third fingers of the hand.  Combined with a stationary version of the mouse, this will also allow simultaneous input of Braille text through button inputs and, in the future, the creation of graphical information.  In developing this system, we will use a Participatory Action Design approach involving all stakeholders. The created system will then be validated in accessing graphical and full page text information, as well as editing full page text compared to current commercial and/or research devices.  In addition, this system will be furthered by the development of techniques to effectively communicate the focus of attention between the user and collaborators.</AbstractNarration>
<MinAmdLetterDate>01/18/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/07/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1605226</AwardID>
<Investigator>
<FirstName>Dianne</FirstName>
<LastName>Pawluk</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dianne Pawluk</PI_FULL_NAME>
<EmailAddress>dtpawluk@vcu.edu</EmailAddress>
<PI_PHON>8048289491</PI_PHON>
<NSF_ID>000090237</NSF_ID>
<StartDate>01/18/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Virginia Commonwealth University</Name>
<CityName>RICHMOND</CityName>
<ZipCode>232980568</ZipCode>
<PhoneNumber>8048286772</PhoneNumber>
<StreetAddress>P.O. Box 980568</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>105300446</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VIRGINIA COMMONWEALTH UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>105300446</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Virginia Commonwealth University Department of Biomedical Engin]]></Name>
<CityName>Richmond</CityName>
<StateCode>VA</StateCode>
<ZipCode>232843067</ZipCode>
<StreetAddress><![CDATA[401 West Main Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>093Z</Code>
<Text>AI Education/Workforce Develop</Text>
</ProgramReference>
<ProgramReference>
<Code>5342</Code>
<Text>RESEARCH TO AID THE DISABLED</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~319004</FUND_OBLG>
<FUND_OBLG>2021~3000</FUND_OBLG>
</Award>
</rootTag>
