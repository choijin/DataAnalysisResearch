<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STEM Evaluation Community Project</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>484727.00</AwardTotalIntnAmount>
<AwardAmount>611600</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>01060300</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OIA</Abbreviation>
<LongName>Office of Integrative Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cynthia Phillips</SignBlockName>
<PO_EMAI>cphillip@nsf.gov</PO_EMAI>
<PO_PHON>7032922235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The primary goal of the STEM Evaluation Community project is to increase the capacity of evaluators to produce high quality, conceptually sound, methodologically appropriate evaluations of NSF programs and projects. The project will contribute to the exploration of innovative new approaches for determining the impact of NSF programs and projects, promote the usefulness of NSF program and project evaluations, and help to expand the theoretical foundations for evaluating STEM education, workforce, and outreach initiatives.&lt;br/&gt;&lt;br/&gt;The STEM Evaluation Community will serve as a focal point for evaluation capacity building with and across the NSF programs that fund STEM education, workforce and outreach activities, bringing people and ideas together, facilitating dialogue, sharing resources, offering opportunities for dissemination and more. The resulting connected system of evaluators will promote social innovation through a lively, dynamic evaluation professional community in which NSF program and project evaluators will share their work, learn from each other, and, ultimately, leverage and enhance the existing evaluation capacity building infrastructure that has been developed for specific NSF programs and audiences, thus charting a course to build further evaluation capacity across NSF.</AbstractNarration>
<MinAmdLetterDate>09/07/2016</MinAmdLetterDate>
<MaxAmdLetterDate>01/24/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.083</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1650215</AwardID>
<Investigator>
<FirstName>Leslie</FirstName>
<LastName>Goodyear</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leslie Goodyear</PI_FULL_NAME>
<EmailAddress>lgoodyear@edc.org</EmailAddress>
<PI_PHON>6176182354</PI_PHON>
<NSF_ID>000403099</NSF_ID>
<StartDate>09/07/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Education Development Center</Name>
<CityName>Waltham</CityName>
<ZipCode>024538313</ZipCode>
<PhoneNumber>6176182227</PhoneNumber>
<StreetAddress>43 Foundry Avenue</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>076583830</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EDUCATION DEVELOPMENT CENTER, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>076583830</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Education Development Center]]></Name>
<CityName>Waltham</CityName>
<StateCode>MA</StateCode>
<ZipCode>024538313</ZipCode>
<StreetAddress><![CDATA[43 Foundry Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1515</Code>
<Text>AGEP</Text>
</ProgramElement>
<ProgramElement>
<Code>7550</Code>
<Text>Eval &amp; Assessment Capabilites</Text>
</ProgramElement>
<ProgramReference>
<Code>110E</Code>
<Text>EDUCATION RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>1515</Code>
<Text>MINORITY GRADUATE EDUC ACTIVIT</Text>
</ProgramReference>
<ProgramReference>
<Code>5299</Code>
<Text>EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7263</Code>
<Text>NETWORKING INFRASTRUCT-EDUCAT</Text>
</ProgramReference>
<ProgramReference>
<Code>8212</Code>
<Text>Broaden Particip STEM Resrch</Text>
</ProgramReference>
<ProgramReference>
<Code>9152</Code>
<Text>EDUCATION PROGRAM EVALUATION</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0418</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~484727</FUND_OBLG>
<FUND_OBLG>2018~126873</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In 2017, the Education Development Center (EDC) received a grant from the National Science Foundation (NSF) to convene the STEM Evaluation Community as a way to better understand how to increase the capacity of evaluators to produce high quality, conceptually sound, methodologically appropriate evaluations of NSF programs and projects, specifically in the area of STEM education and outreach. The project hosted three community meetings, which included representatives of NSF-funded program resource centers, evaluators of NSF STEM projects and programs, and evaluation experts and advisors, and NSF representatives. In addition, with input from stakeholders, the project designed and implemented two studies: a landscape study of NSF-funded STEM evaluation resources and guidance, and an evaluator survey focused on needs, capacities, career trajectories, and networks.</p> <p>Findings from the project include:</p> <ul> <li>Evaluation requirements set forth by NSF vary greatly across Programs, Divisions, and Directorates.&#8203; Most program solicitations include an evaluation requirement, but only 23% require an external evaluator.</li> <li>NSF-funded Resource Centers provide an incredible number of resources that support evaluation and they offer some opportunities for interaction among projects and evaluators.</li> <li>NSF-funded evaluation resources target different audiences, i.e. by role or by program, and are tailored with program-specific guidance.&#8203; The majority of resources focus on the early stages of evaluation, including planning and methods.&#8203; Resources include guides, webinars, checklists, research articles, and databases of instruments and reports, and other formats. However, there is no central repository or documentation of STEM evaluation resources across NSF programs.</li> <li>NSF STEM evaluators come from diverse backgrounds and there was considerable evaluation experience, especially across NSF Programs, represented among the survey respondents</li> <li>Although STEM evaluators know where to find and how to access existing resources, there are challenges evaluators face that are not met with existing resources.&#8203;</li> <li>Survey respondents indicated that there are multiple pathways to becoming a STEM evaluator. The main pathways are to start as a more general evaluator and move into evaluating STEM programs and projects, or to start as a STEM professional/STEM educator/ or STEM project grants administrator and move into evaluation. </li> <li>Situations that ranked high in terms of when STEM evaluators sought resources were when&nbsp;&#8203;<em>Creating and selecting instruments </em>(73%),&nbsp;&#8203;<em>Learning about requirements related to an RFP or solicitation </em>(71%), and&nbsp;&#8203;<em>Writing a proposal </em>(64%).&nbsp;&#8203;&#8203;</li> <li>We also learned from the evaluator survey that interactions with colleagues, professionals, and friends were just as important as static resources and guides when looking for support with STEM evaluation work.&nbsp;&#8203;</li> <li>Challenges STEM evaluators face when conducting STEM evaluations include limited time and funding, PIs? lack of understanding of the value of STEM evaluation, and identifying common definitions, measures or interpretations of STEM and STEM education outcomes.&nbsp;&#8203; Some of these cannot be remedied by additional resources or evaluator capacity building. </li> </ul> <p class="paragraph">These findings informed the content for the STEM Evaluation Community discussions. Based on our findings and meeting discussions, we came to the conclusion that evaluation capacity building is not ?one-size-fits-all,? and that evaluation does not happen in isolation. Evaluation is part of a system, heavily shaped by both evaluator needs and the needs, interests, and capacity of funders and PIs. Essentially, we learned there is more that we need to consider when doing evaluation capacity building.&#8203; Given these external factors that influence evaluations, the many audiences and approaches that can, need, or should be targeted with respect to ECB, we are left to ask the questions: Is evaluator capacity in fact the issue that needs to be addressed at this time? And, are ECB efforts the best way to improve evaluation quality of NSF projects and programs?&nbsp;&#8203;</p> <p class="paragraph">In conclusion, we believe that a focus on the demand side of evaluation ? building the capacity of those who commission and use evaluations ? will contributes to increasing evaluations? value and utility.&nbsp;&#8203;Possible actions include:</p> <ul> <li>Challenging evaluators to shift from talking about ?what we do in evaluation? to ?why we do it.? </li> <li>Exploring the ?habits of mind? of successful evaluators.</li> <li>Investigating what high-quality evaluation looks like, who does one, and how it is used. </li> <li>Helping evaluators shift PIs? attitudes about evaluation to learning activities rather than compliance activities.&#8203;</li> <li>And, with all these in mind, considering the need to influence NSF and others? vision of evaluation. </li> </ul> <p class="paragraph">Given that evaluations do not take place in a vacuum, we need to think about how to build evaluation capacity across the system. Building capacity of evaluators alone is unlikely to result in higher quality evaluations and better evaluation use. And, evaluation capacity building of a variety of players is necessary in order to both focus on evaluation demand and reframe the role and purpose of evaluation in supporting and advancing STEM education and the STEM fields.&nbsp;&#8203;</p> <p class="paragraph">&nbsp;</p> <p>&#8203;</p> <p class="paragraph">&nbsp;</p> <p>&#8203;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/02/2020<br>      Modified by: Leslie&nbsp;Goodyear</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1650215/1650215_10459446_1601659440917_STEMECphoto--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1650215/1650215_10459446_1601659440917_STEMECphoto--rgov-800width.jpg" title="STEM EC Photo"><img src="/por/images/Reports/POR/2020/1650215/1650215_10459446_1601659440917_STEMECphoto--rgov-66x44.jpg" alt="STEM EC Photo"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The STEM Evaluation Community stakeholders at the final project meeting</div> <div class="imageCredit">Leslie Goodyear</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Leslie&nbsp;Goodyear</div> <div class="imageTitle">STEM EC Photo</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In 2017, the Education Development Center (EDC) received a grant from the National Science Foundation (NSF) to convene the STEM Evaluation Community as a way to better understand how to increase the capacity of evaluators to produce high quality, conceptually sound, methodologically appropriate evaluations of NSF programs and projects, specifically in the area of STEM education and outreach. The project hosted three community meetings, which included representatives of NSF-funded program resource centers, evaluators of NSF STEM projects and programs, and evaluation experts and advisors, and NSF representatives. In addition, with input from stakeholders, the project designed and implemented two studies: a landscape study of NSF-funded STEM evaluation resources and guidance, and an evaluator survey focused on needs, capacities, career trajectories, and networks.  Findings from the project include:  Evaluation requirements set forth by NSF vary greatly across Programs, Divisions, and Directorates.&#8203; Most program solicitations include an evaluation requirement, but only 23% require an external evaluator. NSF-funded Resource Centers provide an incredible number of resources that support evaluation and they offer some opportunities for interaction among projects and evaluators. NSF-funded evaluation resources target different audiences, i.e. by role or by program, and are tailored with program-specific guidance.&#8203; The majority of resources focus on the early stages of evaluation, including planning and methods.&#8203; Resources include guides, webinars, checklists, research articles, and databases of instruments and reports, and other formats. However, there is no central repository or documentation of STEM evaluation resources across NSF programs. NSF STEM evaluators come from diverse backgrounds and there was considerable evaluation experience, especially across NSF Programs, represented among the survey respondents Although STEM evaluators know where to find and how to access existing resources, there are challenges evaluators face that are not met with existing resources.&#8203; Survey respondents indicated that there are multiple pathways to becoming a STEM evaluator. The main pathways are to start as a more general evaluator and move into evaluating STEM programs and projects, or to start as a STEM professional/STEM educator/ or STEM project grants administrator and move into evaluation.  Situations that ranked high in terms of when STEM evaluators sought resources were when &#8203;Creating and selecting instruments (73%), &#8203;Learning about requirements related to an RFP or solicitation (71%), and &#8203;Writing a proposal (64%). &#8203;&#8203; We also learned from the evaluator survey that interactions with colleagues, professionals, and friends were just as important as static resources and guides when looking for support with STEM evaluation work. &#8203; Challenges STEM evaluators face when conducting STEM evaluations include limited time and funding, PIs? lack of understanding of the value of STEM evaluation, and identifying common definitions, measures or interpretations of STEM and STEM education outcomes. &#8203; Some of these cannot be remedied by additional resources or evaluator capacity building.   These findings informed the content for the STEM Evaluation Community discussions. Based on our findings and meeting discussions, we came to the conclusion that evaluation capacity building is not ?one-size-fits-all,? and that evaluation does not happen in isolation. Evaluation is part of a system, heavily shaped by both evaluator needs and the needs, interests, and capacity of funders and PIs. Essentially, we learned there is more that we need to consider when doing evaluation capacity building.&#8203; Given these external factors that influence evaluations, the many audiences and approaches that can, need, or should be targeted with respect to ECB, we are left to ask the questions: Is evaluator capacity in fact the issue that needs to be addressed at this time? And, are ECB efforts the best way to improve evaluation quality of NSF projects and programs? &#8203; In conclusion, we believe that a focus on the demand side of evaluation ? building the capacity of those who commission and use evaluations ? will contributes to increasing evaluations? value and utility. &#8203;Possible actions include:  Challenging evaluators to shift from talking about ?what we do in evaluation? to ?why we do it.?  Exploring the ?habits of mind? of successful evaluators. Investigating what high-quality evaluation looks like, who does one, and how it is used.  Helping evaluators shift PIs? attitudes about evaluation to learning activities rather than compliance activities.&#8203; And, with all these in mind, considering the need to influence NSF and others? vision of evaluation.   Given that evaluations do not take place in a vacuum, we need to think about how to build evaluation capacity across the system. Building capacity of evaluators alone is unlikely to result in higher quality evaluations and better evaluation use. And, evaluation capacity building of a variety of players is necessary in order to both focus on evaluation demand and reframe the role and purpose of evaluation in supporting and advancing STEM education and the STEM fields. &#8203;    &#8203;    &#8203;             Last Modified: 10/02/2020       Submitted by: Leslie Goodyear]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
