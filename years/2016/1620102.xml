<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Fitting Manifolds to Noisy Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>140000.00</AwardTotalIntnAmount>
<AwardAmount>140000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In recent years, high dimensional statistics has focused on methods of alleviating the curse of dimensionality, which refers to various inconvenient phenomena that arise when analyzing data in high-dimensional spaces. One assumption that facilitates this is the Manifold hypothesis: that data lie in the vicinity of a low dimensional manifold. For example, if we randomly photograph a large number of objects, randomly choose three-by-three pixel patches, and plot the corresponding points in nine-dimensional space, we approximately see a Klein bottle, which is a two dimensional manifold. This could be due to the generating process possessing only a few essential degrees of freedom. Manifold learning is a collection of methodologies for analyzing high dimensional data based on the Manifold hypothesis. This has been an area of intense activity over the past two decades. In work with Fefferman and Mitter, the foundation for a novel approach to Manifold learning has been established. The present project is concerned with using this approach for the building of practically and theoretically efficient algorithms to fit a manifold to data, given that the data lie near a manifold.&lt;br/&gt;&lt;br/&gt;Current approaches which attempt to build a manifold with some guarantees of success, either forgo the smoothness assumption or are restrictive in some way. In the work with Fefferman and Mitter new techniques based on fiber bundles have been developed and formally justified to verify in finite time whether there is a manifold of bounded reach lying near the data. Since this is an exhaustive process, the algorithm is extremely slow. However, when the data is actually from a manifold with perhaps some corruption by noise, our approach has sufficiently powerful techniques which can be modified and adopted to construct an approximation of the manifold in near linear time on the size of the data. The feasibility of the method has been tested experimentally through implementation of the algorithm on simple examples. Theoretical guarantees of speed will be provided for these algorithms as part of this project and the algorithms will be shown to be practical. At a broader level, we would like to introduce ideas from Whitney interpolation to statistics and engineering.</AbstractNarration>
<MinAmdLetterDate>09/08/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/25/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1620102</AwardID>
<Investigator>
<FirstName>Hariharan</FirstName>
<LastName>Narayanan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hariharan Narayanan</PI_FULL_NAME>
<EmailAddress>harin@uw.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000657865</NSF_ID>
<StartDate>09/08/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~78288</FUND_OBLG>
<FUND_OBLG>2017~30502</FUND_OBLG>
<FUND_OBLG>2018~31210</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Machine learning can be broadly defined as a discipline whose goal is to enable a computer to make inferences from observed data about future observations. There are two directions in which progress is crucial. The first is making inferences from very few observations. The second is dealing with data with increasing dimensionality, which has come to prominence through recent applications in vision, imaging (such as cryo-EM) and the world wide web. A hypothesis whose validity would mitigate both of these problems is that high dimensional data tend to lie in the vicinity of a low dimensional embedded manifold. This hypothesis, henceforth referred to as the ?manifold hypothesis? has become influential over the past decade and has given rise to the field known as manifold learning. Whether the hypothesis that data lie near a manifold be tested efficiently is a basic question. In [5], it was shown that the number of samples and the number of computations, (in a model where computing an inner product requires a unit cost,) needed to test whether a probability distribution supported in a unit ball in R^n for large n has a mean-squared distance of less than epsilon from a low dimensional manifold is independent of n, where the test is for manifolds that have bounded curvature and volume. The algorithm uses the implicit function theorem to define an appropriate vector bundle from each member of an exhaustive set of ?approximate-squared-distance-functions? , and an optimization is then performed over certain sections of this vector bundle. The size of the aforementioned exhaustive set of approximatesquared-distance functions is doubly exponential in the intrinsic dimension of the manifold of interest. In order to improve this bound, a more restrictive setting was considered in [3]. The setting is as follows. Suppose data belonging to a high dimensional Euclidean space is drawn independently, identically distributed from a measure supported on a low dimensional twice differentiable embedded manifold M, and corrupted by a small amount of gaussian noise. How can we produce a manifold Mo whose Hausdorff distance to M is small and whose reach is not much smaller than the reach of M? In [3] we show that a Hausdorff distance of O(&sigma;) can be achieved using a number of computations that depends only singly exponentially on the intrinsic dimension, where &sigma; is the standard deviation of the Gaussian. This has recently been improved to O(&sigma;^2 ) in [4]. In the setting where the manifold is C^k , whether a bound of O(&sigma;^k) on the Hausdorff distance can be achieved is an interesting question. In [1], the problem of reconstructing a Riemannian manifold from pairwise distance measurements on a fine net of a manifold was considered. Such questions are encountered in differential geometry and in many inverse problems encountered in applications. The determination of a Riemannian manifold includes the construction of its topology, differentiable structure, and metric. Constructive solutions have been given to the above problems. Algorithmic procedures have been developed that solve the geometric Whitney problem for a metric space and the manifold reconstruction problem in Euclidean space, and estimate the computational complexity of these procedures. The results for Riemannian manifolds are based on a generalisation of the Whitney embedding construction where approximative coordinate charts are embedded in R m and interpolated to a smooth submanifold. In subsequent work [2] the case of noisy (random) distance measurements, with significantly larger errors than in [1] was considered. Through an averaging scheme, the problem of Riemannian manifold recovery was reduced to the question solved in [1].</p> <p>References</p> <p>[1] C. Fefferman, S. Ivanov, Y. Kurylev, M. Lassas, H. Narayanan Reconstruction and interpolation of manifolds I: The geometric Whitney problem, Arxiv preprint: http://arxiv.org/abs/1508.00674 To appear in Foundations of Computational Mathematics</p> <p>[2] C. Fefferman, S. Ivanov, M. Lassas, H. Narayanan Reconstruction of a Riemannian manifold from noisy intrinsic distances, ArXiv preprint: arXiv:1905.07182 Accepted modulo revision in SIAM journal on Mathematics of Data Science</p> <p>[3] C. Fefferman, S. Ivanov, Y. Kurylev, M. Lassas, H. Narayanan Fitting a putative manifold to noisy data, 31sth Annual Conference on Learning Theory (COLT), June 2018.</p> <p>[4] C. Fefferman, S. Ivanov, M. Lassas, H. Narayanan Fitting a manifold of large reach to noisy data, Arxiv preprint: http://arxiv.org/abs/1905.07182, 2019</p> <p>[5] C. Fefferman, S. Mitter and H. Narayanan, Testing the Manifold Hypothesis, Online February 9, 2016, Journal of the American Mathematical Society, Volume 29 (2016), 983-1049</p><br> <p>            Last Modified: 09/01/2020<br>      Modified by: Hariharan&nbsp;Narayanan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Machine learning can be broadly defined as a discipline whose goal is to enable a computer to make inferences from observed data about future observations. There are two directions in which progress is crucial. The first is making inferences from very few observations. The second is dealing with data with increasing dimensionality, which has come to prominence through recent applications in vision, imaging (such as cryo-EM) and the world wide web. A hypothesis whose validity would mitigate both of these problems is that high dimensional data tend to lie in the vicinity of a low dimensional embedded manifold. This hypothesis, henceforth referred to as the ?manifold hypothesis? has become influential over the past decade and has given rise to the field known as manifold learning. Whether the hypothesis that data lie near a manifold be tested efficiently is a basic question. In [5], it was shown that the number of samples and the number of computations, (in a model where computing an inner product requires a unit cost,) needed to test whether a probability distribution supported in a unit ball in R^n for large n has a mean-squared distance of less than epsilon from a low dimensional manifold is independent of n, where the test is for manifolds that have bounded curvature and volume. The algorithm uses the implicit function theorem to define an appropriate vector bundle from each member of an exhaustive set of ?approximate-squared-distance-functions? , and an optimization is then performed over certain sections of this vector bundle. The size of the aforementioned exhaustive set of approximatesquared-distance functions is doubly exponential in the intrinsic dimension of the manifold of interest. In order to improve this bound, a more restrictive setting was considered in [3]. The setting is as follows. Suppose data belonging to a high dimensional Euclidean space is drawn independently, identically distributed from a measure supported on a low dimensional twice differentiable embedded manifold M, and corrupted by a small amount of gaussian noise. How can we produce a manifold Mo whose Hausdorff distance to M is small and whose reach is not much smaller than the reach of M? In [3] we show that a Hausdorff distance of O(&sigma;) can be achieved using a number of computations that depends only singly exponentially on the intrinsic dimension, where &sigma; is the standard deviation of the Gaussian. This has recently been improved to O(&sigma;^2 ) in [4]. In the setting where the manifold is C^k , whether a bound of O(&sigma;^k) on the Hausdorff distance can be achieved is an interesting question. In [1], the problem of reconstructing a Riemannian manifold from pairwise distance measurements on a fine net of a manifold was considered. Such questions are encountered in differential geometry and in many inverse problems encountered in applications. The determination of a Riemannian manifold includes the construction of its topology, differentiable structure, and metric. Constructive solutions have been given to the above problems. Algorithmic procedures have been developed that solve the geometric Whitney problem for a metric space and the manifold reconstruction problem in Euclidean space, and estimate the computational complexity of these procedures. The results for Riemannian manifolds are based on a generalisation of the Whitney embedding construction where approximative coordinate charts are embedded in R m and interpolated to a smooth submanifold. In subsequent work [2] the case of noisy (random) distance measurements, with significantly larger errors than in [1] was considered. Through an averaging scheme, the problem of Riemannian manifold recovery was reduced to the question solved in [1].  References  [1] C. Fefferman, S. Ivanov, Y. Kurylev, M. Lassas, H. Narayanan Reconstruction and interpolation of manifolds I: The geometric Whitney problem, Arxiv preprint: http://arxiv.org/abs/1508.00674 To appear in Foundations of Computational Mathematics  [2] C. Fefferman, S. Ivanov, M. Lassas, H. Narayanan Reconstruction of a Riemannian manifold from noisy intrinsic distances, ArXiv preprint: arXiv:1905.07182 Accepted modulo revision in SIAM journal on Mathematics of Data Science  [3] C. Fefferman, S. Ivanov, Y. Kurylev, M. Lassas, H. Narayanan Fitting a putative manifold to noisy data, 31sth Annual Conference on Learning Theory (COLT), June 2018.  [4] C. Fefferman, S. Ivanov, M. Lassas, H. Narayanan Fitting a manifold of large reach to noisy data, Arxiv preprint: http://arxiv.org/abs/1905.07182, 2019  [5] C. Fefferman, S. Mitter and H. Narayanan, Testing the Manifold Hypothesis, Online February 9, 2016, Journal of the American Mathematical Society, Volume 29 (2016), 983-1049       Last Modified: 09/01/2020       Submitted by: Hariharan Narayanan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
