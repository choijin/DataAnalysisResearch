<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Representing, Understanding, and Enhancing Scenes at the Internet-Scale</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2016</AwardEffectiveDate>
<AwardExpirationDate>12/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>95303.00</AwardTotalIntnAmount>
<AwardAmount>95303</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>CAREER: Understanding, Representing, and Enhancing Scenes at the Internet-scale&lt;br/&gt;&lt;br/&gt;Photography has an enormous impact on society -- it is our primary visual history and a medium for storytelling, entertainment, and art. But our visual world is extraordinarily complex which makes it difficult for computer vision to understand photos and for computer graphics to synthesize visual content. However, the emergence of Internet-scale photo collections in recent years enables new research directions. We use scene-based representations to leverage Internet-scale data. Scenes (places or environments) are the context in which all other visual phenomena exist and it seems possible to brute-force the space of scenes -- with millions of scenes, we find qualitatively similar scenes and create massively data-driven algorithms with capabilities that are complementary to typical bottom-up graphics and vision pipelines. The underlying principle of this study is that joint investigations of scene representations and large image databases will advance the state-of-the-art in graphics and vision. &lt;br/&gt;&lt;br/&gt;First, we are investigating detail synthesis tasks which alleviate camera shake, motion blur, defocus, atmospheric scattering, or low resolution. Scene representations are robust enough to find matching scenes in Internet-scale photo collections even in the presence of dramatic blurring. These matching scenes provide a context-specific statistical model which can be used to insert convincing texture and object detail. Second, we are studying attribute-based representations of scenes. We use crowdsourcing to discover attributes and build large databases for the community. Attributes are a powerful intermediate representation for the next generation of big data imaging research which can have broad societal impact through applications such as robotics, security, assistance to vision-impaired, and vehicle safety. The investigators also are developing a new introductory course for Brown students to explore big data computing across scientific disciplines and are creating an online community for visual computing education to benefit students interested in photography and programming.</AbstractNarration>
<MinAmdLetterDate>05/16/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/16/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1641340</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Hays</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James H Hays</PI_FULL_NAME>
<EmailAddress>hays@gatech.edu</EmailAddress>
<PI_PHON>4014020486</PI_PHON>
<NSF_ID>000561883</NSF_ID>
<StartDate>05/16/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~95303</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This award funded my lab's research on "big data" computer vision. It funded three PhD students through significant portions of their PhD and led to roughly a dozen publicationsin top tier computer vision and computer graphics venues.</p> <p>The first goal of this project was "big data image restoration". This work was led by then-PhD student Libin Sun (PhD Brown University 2016). Through many publications, we showed how to use machine learning and image databases to restore various image defects such as camera shake, motion blur, defocus, atmospheric scattering, and low resolution. Our most influential work, in collaboration with Adobe, was "Edge-based blur kernel estimation using patch priors" published in the international conference on computational photography. It has 149 citations as of spring 2018. It addresses the problem of repairing a photograph which is blurry because the camera moved during an exposure. It shows that the camera motion (or blur kernal) of a photograph can be estimated with a simple model of what edge patches (tiny image blocks) should look like. It was near the state of the art in the field for many years.</p> <p>The second goal of this project was to examine attribute-based representations of scenes and other visual phenomena. This work was led by then-PhD student Genevieve Patterson (PhD Brown University, 2016). Attributes are an alternative to the more common classification of visual items (objects, scenes, materials, actions, etc) into non-overlapping categories. Influential image databases and machine learning benchmarks like ImageNet or COCO group objects into such categories (dog, car, television) and this is natural. But for scenes, such groupings (e.g. kitchen, living room, forest) are more problematic because the boundaries between scenes are less clear cut than the boundaries between objects. Genevieve's work on the "Sun Attribute Database" built the first large database of scenes with attribute labels. Attribute labels encompass functions (e.g. eating, socializing), materials (tile, wood, concrete), surface properties (dry, worn), emotational responses (stressful), etc. A single scene can have many such attributes. Our database is described in two publications, one in the 2012 conference on Comptuer Vision and Pattern Recognition and one in the International Journal of Computer Vision in 2014. Together these manuscripts have more than 400 citations as of spring 2018. Another influential work in this space was our "Tropel" system for the crowdsourced creation of fine-grained object detection classifiers which one the best-paper runner up award at HCOMP 2015.</p> <p>The computer vision community has undergone and dramatic disruption during the course of this project: transitioning from more classical image-processing approaches to deep learning. Accordingly our project has shfited to build on deep learning methods. In the later years of this project we addressed some of the same tasks (e.g. image super-resolution) with deep learning approaches instead of the classical retrieval approaches used initially. This includes the work of Nam Vo (PhD expected from Georgia Tech in 2018) who revisited the "big data" work im2gps that I did during my PhD thesis but used a deep representation instead of hand crafted features. In work published in the International Conference on Computer Vision in 2017 Nam showed that switching to deep features dramatically improves the capability of global image geolocalization techniques over the original im2gps project. Furthermore, Nam's simple deep retrieval approach outperformed other high-profile image geolocalization techniques from Google.&nbsp;</p> <p>Beyond the intellectual impact of these and other publications, the broader impact of this work was to train three PhD students (including one under-represented minority). PhD students Libin Sun and Genevieve Patterson have both graduated and are both pursuing research and development related to their thesis topics. As part of this project, we also mentored undegraduate researchers to help Libin and Genevieve with their theses. We also mentored underrepresented students who have since gone on to do PhDs at other instutitions such as Sirion Vittayakorn (PhD University of North Carolina).</p><br> <p>            Last Modified: 05/22/2018<br>      Modified by: James&nbsp;H&nbsp;Hays</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This award funded my lab's research on "big data" computer vision. It funded three PhD students through significant portions of their PhD and led to roughly a dozen publicationsin top tier computer vision and computer graphics venues.  The first goal of this project was "big data image restoration". This work was led by then-PhD student Libin Sun (PhD Brown University 2016). Through many publications, we showed how to use machine learning and image databases to restore various image defects such as camera shake, motion blur, defocus, atmospheric scattering, and low resolution. Our most influential work, in collaboration with Adobe, was "Edge-based blur kernel estimation using patch priors" published in the international conference on computational photography. It has 149 citations as of spring 2018. It addresses the problem of repairing a photograph which is blurry because the camera moved during an exposure. It shows that the camera motion (or blur kernal) of a photograph can be estimated with a simple model of what edge patches (tiny image blocks) should look like. It was near the state of the art in the field for many years.  The second goal of this project was to examine attribute-based representations of scenes and other visual phenomena. This work was led by then-PhD student Genevieve Patterson (PhD Brown University, 2016). Attributes are an alternative to the more common classification of visual items (objects, scenes, materials, actions, etc) into non-overlapping categories. Influential image databases and machine learning benchmarks like ImageNet or COCO group objects into such categories (dog, car, television) and this is natural. But for scenes, such groupings (e.g. kitchen, living room, forest) are more problematic because the boundaries between scenes are less clear cut than the boundaries between objects. Genevieve's work on the "Sun Attribute Database" built the first large database of scenes with attribute labels. Attribute labels encompass functions (e.g. eating, socializing), materials (tile, wood, concrete), surface properties (dry, worn), emotational responses (stressful), etc. A single scene can have many such attributes. Our database is described in two publications, one in the 2012 conference on Comptuer Vision and Pattern Recognition and one in the International Journal of Computer Vision in 2014. Together these manuscripts have more than 400 citations as of spring 2018. Another influential work in this space was our "Tropel" system for the crowdsourced creation of fine-grained object detection classifiers which one the best-paper runner up award at HCOMP 2015.  The computer vision community has undergone and dramatic disruption during the course of this project: transitioning from more classical image-processing approaches to deep learning. Accordingly our project has shfited to build on deep learning methods. In the later years of this project we addressed some of the same tasks (e.g. image super-resolution) with deep learning approaches instead of the classical retrieval approaches used initially. This includes the work of Nam Vo (PhD expected from Georgia Tech in 2018) who revisited the "big data" work im2gps that I did during my PhD thesis but used a deep representation instead of hand crafted features. In work published in the International Conference on Computer Vision in 2017 Nam showed that switching to deep features dramatically improves the capability of global image geolocalization techniques over the original im2gps project. Furthermore, Nam's simple deep retrieval approach outperformed other high-profile image geolocalization techniques from Google.   Beyond the intellectual impact of these and other publications, the broader impact of this work was to train three PhD students (including one under-represented minority). PhD students Libin Sun and Genevieve Patterson have both graduated and are both pursuing research and development related to their thesis topics. As part of this project, we also mentored undegraduate researchers to help Libin and Genevieve with their theses. We also mentored underrepresented students who have since gone on to do PhDs at other instutitions such as Sirion Vittayakorn (PhD University of North Carolina).       Last Modified: 05/22/2018       Submitted by: James H Hays]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
