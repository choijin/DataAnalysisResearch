<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Collaborative Research: F: Algorithmic Fairness: A Systemic and Foundational Treatment of Nondiscriminatory Data Mining</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>296616.00</AwardTotalIntnAmount>
<AwardAmount>296616</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Data-driven modeling has moved beyond the realm of consumer predictions and recommendations into areas of policy and planning that have a profound impact on our daily lives. The tools of data analysis are being harnessed to predict crime, select candidates for jobs, identify security threats, determine credit risk, and even decide treatment plans and interventions for patients. Automated learning and mining tools can crunch incredible amounts and variety of data in order to detect patterns and make predictions. As is rapidly becoming clear, these tools can also introduce discriminatory behavior and amplify biases in the systems they are trained on. In this project, the PIs will study the problems of discrimination and bias in algorithmic decision-making. By studying all aspects of the data pipeline (from data preparation to learning, evaluation, and feedback), they will develop tools for analyzing, auditing, and designing automated decision-making systems that will be fair, accountable, and transparent. As specific goals to broaden the impact of this research, the PIs will develop a course curriculum to educate the next generation of data scientists on the ethical, legal, and societal implications of algorithmic decision-making, with the intent that they will then take this understanding into their jobs as they enter the workforce. Initial efforts by the PIs have attracted students from underrepresented groups in computer science, and they will continue these efforts. The PIs will also explore the legal and policy ramifications of this research, and develop best practice guidelines for the use of their tools by policy makers, lawyers, journalists, and other practitioners.&lt;br/&gt;&lt;br/&gt;The PIs will explore the technical subject of this project in three ways. Firstly, they will develop a sound theoretical framework for reasoning about algorithmic fairness. This framework carefully separates mechanisms, beliefs, and assumptions in order to make explicit implicitly held assumptions about the nature of fairness in learning. Secondly, by examining the entire pipeline of tasks associated with learning, they will identify hitherto unexplored areas where bias may be unintentionally introduced into learning as well as novel problems associated with ensuring fairness. These include the initial stages of data preparation, various kinds of fairness-aware learning, and evaluation. They will also investigate the problem of feedback: when actions based on a biased learned model might cause a feedback loop that changes reality and leads to more bias.</AbstractNarration>
<MinAmdLetterDate>08/31/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1633400</AwardID>
<Investigator>
<FirstName>danah</FirstName>
<LastName>boyd</LastName>
<PI_MID_INIT>m</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>danah m boyd</PI_FULL_NAME>
<EmailAddress>danah@datasociety.net</EmailAddress>
<PI_PHON>6468322038</PI_PHON>
<NSF_ID>000654356</NSF_ID>
<StartDate>08/31/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Data &amp; Society Research Institute</Name>
<CityName>New York</CityName>
<ZipCode>100031502</ZipCode>
<PhoneNumber>6468322038</PhoneNumber>
<StreetAddress>228 Park Ave S PMB 83075</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079141716</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DATA AND SOCIETY RESEARCH INSTITUTE, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Data & Society Research Institute]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>100114241</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~296616</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Postdoc Selbst edited and published &ldquo;The Intuitive Appeal of Explainable Machines&rdquo; with Solon Barocas, accepted for publication in Fordham Law Review, and wrote &ldquo;Negligence and AI&rsquo;s Human Users,&rdquo; now accepted for publication in Boston University Law Review.</p> <p>Postdoc Selbst published an op-ed in Slate regarding the noticed of proposed rulemaking released by Housing and Urban Development on the disparate impact doctrine. The NPRM is the federal government&rsquo;s first official policy action regarding algorithmic discrimination, and the op-ed led to many conversations with attorneys, activists and academics, who filed comments explaining the research on algorithmic discrimination to HUD. Along with clinical law professor Michele Gilman, a fellow at Data &amp; Society, Selbst also filed a comment explaining how some of the field&rsquo;s research suggests that the final rule should be altered.</p> <p>PI boyd and Michael Golebiewski produced a new Data &amp; Society report on &ldquo;Data Voids: Where Missing Data Can Easily Be Exploited.&rdquo; https://datasociety.net/output/data-voids/ This work has attracted widespread attention, resulting in conversations with Facebook, Google, YouTube, Twitter, and LinkedIn. It has also prompted those who are trying to deal with vulnerabilities to start using the production of data as a tool for addressing this problem, including the Census Bureau and a range of civil rights groups.</p> <p>PI boyd, Moss, and Metcalf published &ldquo;Owning Ethics: Corporate Logics, Silicon Valley, and the Institutionalization of Ethics&rdquo; in the journal &ldquo;Social Research.&rdquo; Metcalf and Moss built on this work to produce a Harvard Business Review article on &ldquo;The Ethical Dilemma at the Heart of Big Tech Companies.&rdquo;</p> <p>PIs Friedler, Venkatasubramanian, and boyd also published &ldquo;Gaps in Information Access in Social Networks&rdquo; at WWW&rsquo;19 with other co-authors.</p> <p>PI boyd was recognized for her work in this area by the Electronic Frontier Foundation, who honored her with the 2019 Pioneer/Barlow prize.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/05/2019<br>      Modified by: Danah&nbsp;M&nbsp;Boyd</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Postdoc Selbst edited and published "The Intuitive Appeal of Explainable Machines" with Solon Barocas, accepted for publication in Fordham Law Review, and wrote "Negligence and AI’s Human Users," now accepted for publication in Boston University Law Review.  Postdoc Selbst published an op-ed in Slate regarding the noticed of proposed rulemaking released by Housing and Urban Development on the disparate impact doctrine. The NPRM is the federal government’s first official policy action regarding algorithmic discrimination, and the op-ed led to many conversations with attorneys, activists and academics, who filed comments explaining the research on algorithmic discrimination to HUD. Along with clinical law professor Michele Gilman, a fellow at Data &amp; Society, Selbst also filed a comment explaining how some of the field’s research suggests that the final rule should be altered.  PI boyd and Michael Golebiewski produced a new Data &amp; Society report on "Data Voids: Where Missing Data Can Easily Be Exploited." https://datasociety.net/output/data-voids/ This work has attracted widespread attention, resulting in conversations with Facebook, Google, YouTube, Twitter, and LinkedIn. It has also prompted those who are trying to deal with vulnerabilities to start using the production of data as a tool for addressing this problem, including the Census Bureau and a range of civil rights groups.  PI boyd, Moss, and Metcalf published "Owning Ethics: Corporate Logics, Silicon Valley, and the Institutionalization of Ethics" in the journal "Social Research." Metcalf and Moss built on this work to produce a Harvard Business Review article on "The Ethical Dilemma at the Heart of Big Tech Companies."  PIs Friedler, Venkatasubramanian, and boyd also published "Gaps in Information Access in Social Networks" at WWW’19 with other co-authors.  PI boyd was recognized for her work in this area by the Electronic Frontier Foundation, who honored her with the 2019 Pioneer/Barlow prize.          Last Modified: 12/05/2019       Submitted by: Danah M Boyd]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
