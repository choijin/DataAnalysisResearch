<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Reasoning Geometric Commonsense for 3D Image/Video Parsing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2017</AwardEffectiveDate>
<AwardExpirationDate>04/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>115062.00</AwardTotalIntnAmount>
<AwardAmount>115062</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Commonsense reasoning studies the consensus reality, knowledge, causality, and rationales available to the overwhelming majority of people, and can be used to enhance all aspects of Artificial Intelligence (AI). This project develops representations of geometric commonsense as well as computing principles of commonsense reasoning for computer vision applications. The project systemically studies commonsense knowledge over geometric dimensions of scene entities, e.g., the length of a sedan is shorter than that of a bus; or that window edges on the same fa√ßade are parallel to each other and are orthogonal to the edges on the ground. These first-order and second-order knowledges, once extracted, are fairly stable across different types of scenes, and are informative enough for enhancing the understanding of images or videos in both 2D and 3D. The project integrates research with education by supporting graduate students, and outreaches to computer vision and AI research communities by organizing workshops in the relevant conferences.&lt;br/&gt;&lt;br/&gt;This research studies geometric commonsense reasoning for 3D scene parsing in images or videos, and contributes a unified probabilistic approach that is capable of reconstructing a wide variety of scene categories (e.g., suburb, urban, campus) from a single input image or a monocular video sequence.  The project approaches the problem from two aspects. First, a new attributed grammar model is developed to represent both images and the associated geometric commonsense knowledge using a hierarchical graphical structure. With this grammar model, the segmentation of semantic regions, the reconstruction of scene entities, and the reasoning of geometric commonsense can be all solved through creating a valid parse graph from images or videos. Second, a new computing framework is introduced so that the inference of image parsing can be conducted in the joint space of discrete semantic labels and continuous geometric labels, and the learning of grammar models can be conducted over training images with weak supervision. The developed techniques enable a state-of-the-art computer vision system that can robustly estimate semantic and geometric scene structures from images or videos.</AbstractNarration>
<MinAmdLetterDate>05/03/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/03/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657600</AwardID>
<Investigator>
<FirstName>Xiaobai</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaobai Liu</PI_FULL_NAME>
<EmailAddress>xiaobai.liu@mail.sdsu.edu</EmailAddress>
<PI_PHON>3109945313</PI_PHON>
<NSF_ID>000712345</NSF_ID>
<StartDate>05/03/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>San Diego State University Foundation</Name>
<CityName>San Diego</CityName>
<ZipCode>921822190</ZipCode>
<PhoneNumber>6195945731</PhoneNumber>
<StreetAddress>5250 Campanile Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>53</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA53</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073371346</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SAN DIEGO STATE UNIVERSITY FOUNDATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[San Diego State University Foundation]]></Name>
<CityName>San Diego</CityName>
<StateCode>CA</StateCode>
<ZipCode>921822190</ZipCode>
<StreetAddress><![CDATA[5500 Campanile Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>53</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA53</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~115062</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Major goals</strong></p> <p>During the third-year of this grant, the investigation team has been working on two aspects.</p> <ol> <li>&nbsp;Developing end-to-end unsupervised learning framework for solving 3D camera calibration and parameter estimation problems.&nbsp; </li> <li>&nbsp;Integrating stochastic image grammars with the powerful deep representation methods in order to better ground grammatical models on visual inputs. &nbsp;</li> </ol> <p>&nbsp;</p> <p>&nbsp;</p> <p><strong>Major Activities</strong></p> <p>The investigation team has undergone the following major research activities.</p> <p><strong>&nbsp;</strong></p> <p><strong>Learning Sequential Model for Planar Homography Estimation in Aerial Video </strong></p> <p><strong>&nbsp;</strong></p> <p>&nbsp;Estimating planar homographic transformations between consecutive video frames is a fundamental image task and is an essential part of many multimedia applications, including video management, robot navigation, content-based video retrieval, intelligent drones, self-driving vehicles, etc. The matrix can also be used to approximate camera poses in aerial videos where facing-downward cameras are positioned far away from the ground and objects on the ground could be reasonably assumed to be from the same planar. In the past literature, classical estimators usually build feature-level or pixel-level correspondences between camera views and employ perspective geometry to recover their planar transformations. Each feature or pixel is represented as a feature descriptor (e.g., histograms) and is matched across camera views. Robust matching methods, e.g., RANSAC, might be used for eliminating incorrect or noisy correspondences. The feature-based methods are robust to some extent because the detected feature points and feature descriptors are invariant to rotation and scaling, and the RANSAC algorithm can potentially prune outlier correspondences. However, such a stage-wise pipeline involves many hand-crafted parameters, including thresholds for detecting feature points, choices of designing feature descriptors, and algorithms for matching feature descriptors. Those hyper-parameters could be tricky to calibrate in real-world multimedia applications. In this work, we develop a knowledge-rich sequence-to-sequence model to directly regress homography parameters for aerial videos. Our method takes a sequence of video frames as inputs and generates a stitched image by a sequence of estimated homography matrices between consecutive video frames. For each pair of input images, our model employs a deep neural network to extract the feature representation of video frame pairs, regresses homography parameters, and warps one input image into the other image. A Long Short-Term Memory (LSTM) model is also employed to directly incorporate the temporal dependencies while estimating the sequence of homography matrices. Our method does not require any annotations and can be trained from raw video frames using standard gradient-based methods.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p><strong>Learning Stochastic Image Grammar for Monocular 3D Pose Estimation </strong></p> <p>In this work, we developed learning-based stochastic image grammar to tackle the problem of 3D human pose estimation from a monocular RGB image. Our grammar model takes estimated 2D pose as the input and learns a generalized 2D-3D mapping function to leverage into 3D pose.</p> <p>The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNNs) on the top to explicitly incorporate a set of knowledge regarding human body configuration (i.e., kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a data augmentation algorithm to further improve model robustness against appearance variations and cross-view generalization ability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.</p> <p>&nbsp;</p> <p><strong>Significant Results</strong></p> <p>We have submitted the above findings to one international conference and one journal, both of which are under reviewing. The research focus and significances of these papers are summarized as follows:&nbsp;</p> <p><strong>Manuscript 1</strong>: Title: &ldquo;Learning Knowledge-Rich Sequential Model for Planar Homography Estimation in Aerial Video&rdquo;, Submitted to the 2020 International Conference on Pattern Recognition</p> <p><strong>Significance</strong>: Firstly, we reformulate the homography estimation of aerial videos to be a sequence-to-sequence task and develop a LSTM network to estimate the sequence of homography parameters, which is the first of its catalog in the literature. Secondly, we employ a set of spatial-scale-temporal knowledge to regularize training of the LSTM model and empirically validate its superior performance over alternative methods on challenging aerial videos.</p> <p>&nbsp;<br /> &nbsp;<strong>Manuscript 2</strong>: Title: &ldquo;Monocular 3D Pose Estimation via Pose Grammar and Data Augmentation&rdquo;, Accepted with Major Revision, IEEE Pattern Analysis and Machine Intelligence (PAMI), Submitted in 2019, Revised in Jan 2020.</p> <p>&nbsp;<strong>Significance: </strong>&nbsp;(I) A deep grammar network that incorporates both powerful encoding capabilities of deep neural networks and high-level dependencies and relations of human body. (II) A data augmentation algorithm that improves model robustness against appearance variations and cross-view generalization ability. (III) A new evaluation protocol for validating model cross-view generalization abilities on Human 3.6M dataset.</p> <p>&nbsp;</p> <p><strong>&nbsp;</strong></p> <p>&nbsp;</p><br> <p>            Last Modified: 08/05/2020<br>      Modified by: Xiaobai&nbsp;Liu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Major goals  During the third-year of this grant, the investigation team has been working on two aspects.   Developing end-to-end unsupervised learning framework for solving 3D camera calibration and parameter estimation problems.    Integrating stochastic image grammars with the powerful deep representation methods in order to better ground grammatical models on visual inputs.           Major Activities  The investigation team has undergone the following major research activities.     Learning Sequential Model for Planar Homography Estimation in Aerial Video       Estimating planar homographic transformations between consecutive video frames is a fundamental image task and is an essential part of many multimedia applications, including video management, robot navigation, content-based video retrieval, intelligent drones, self-driving vehicles, etc. The matrix can also be used to approximate camera poses in aerial videos where facing-downward cameras are positioned far away from the ground and objects on the ground could be reasonably assumed to be from the same planar. In the past literature, classical estimators usually build feature-level or pixel-level correspondences between camera views and employ perspective geometry to recover their planar transformations. Each feature or pixel is represented as a feature descriptor (e.g., histograms) and is matched across camera views. Robust matching methods, e.g., RANSAC, might be used for eliminating incorrect or noisy correspondences. The feature-based methods are robust to some extent because the detected feature points and feature descriptors are invariant to rotation and scaling, and the RANSAC algorithm can potentially prune outlier correspondences. However, such a stage-wise pipeline involves many hand-crafted parameters, including thresholds for detecting feature points, choices of designing feature descriptors, and algorithms for matching feature descriptors. Those hyper-parameters could be tricky to calibrate in real-world multimedia applications. In this work, we develop a knowledge-rich sequence-to-sequence model to directly regress homography parameters for aerial videos. Our method takes a sequence of video frames as inputs and generates a stitched image by a sequence of estimated homography matrices between consecutive video frames. For each pair of input images, our model employs a deep neural network to extract the feature representation of video frame pairs, regresses homography parameters, and warps one input image into the other image. A Long Short-Term Memory (LSTM) model is also employed to directly incorporate the temporal dependencies while estimating the sequence of homography matrices. Our method does not require any annotations and can be trained from raw video frames using standard gradient-based methods.        Learning Stochastic Image Grammar for Monocular 3D Pose Estimation   In this work, we developed learning-based stochastic image grammar to tackle the problem of 3D human pose estimation from a monocular RGB image. Our grammar model takes estimated 2D pose as the input and learns a generalized 2D-3D mapping function to leverage into 3D pose.  The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNNs) on the top to explicitly incorporate a set of knowledge regarding human body configuration (i.e., kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a data augmentation algorithm to further improve model robustness against appearance variations and cross-view generalization ability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.     Significant Results  We have submitted the above findings to one international conference and one journal, both of which are under reviewing. The research focus and significances of these papers are summarized as follows:   Manuscript 1: Title: "Learning Knowledge-Rich Sequential Model for Planar Homography Estimation in Aerial Video", Submitted to the 2020 International Conference on Pattern Recognition  Significance: Firstly, we reformulate the homography estimation of aerial videos to be a sequence-to-sequence task and develop a LSTM network to estimate the sequence of homography parameters, which is the first of its catalog in the literature. Secondly, we employ a set of spatial-scale-temporal knowledge to regularize training of the LSTM model and empirically validate its superior performance over alternative methods on challenging aerial videos.      Manuscript 2: Title: "Monocular 3D Pose Estimation via Pose Grammar and Data Augmentation", Accepted with Major Revision, IEEE Pattern Analysis and Machine Intelligence (PAMI), Submitted in 2019, Revised in Jan 2020.   Significance:  (I) A deep grammar network that incorporates both powerful encoding capabilities of deep neural networks and high-level dependencies and relations of human body. (II) A data augmentation algorithm that improves model robustness against appearance variations and cross-view generalization ability. (III) A new evaluation protocol for validating model cross-view generalization abilities on Human 3.6M dataset.                Last Modified: 08/05/2020       Submitted by: Xiaobai Liu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
