<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Enabling Manipulation of Object Collections via Self-Supervised Robot Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2017</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>183000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>While manipulation of individual objects in cluttered, real-world settings has received substantial attention, the problem of directly manipulating collections of objects has been left unexplored. This project investigates to what extent robots can autonomously manipulate such object collections. This project facilitates autonomous manipulation methods suitable for use in home robotic assistants. Such assistive robots stand to make a substantial impact in increasing the quality of life of older adults and persons with certain degenerative diseases. Additionally, the robot skills investigated in this project are suitable for manipulation in areas damaged in natural or man-made disasters, where building rubble and other debris need to be cleared. The project supports the development and presentation of an interactive robotics lecture for low-income school children, teaching them fundamentals of computer programming.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The research goal of this project is to enable robots to manipulate and reason about groups of objects en masse. The hypothesis of this project is that treating object collections as single entities enables data-efficient, self-supervised learning of contact locations for pushing and grasping grouped objects. This project investigates a novel neural network architecture for self-supervised manipulation learning. The convolutional neural network model takes as input sensory data and a robot hand configuration. The network learns to predict as output a manipulation quality score for the given inputs. When presented with a novel scene, the robot can perform manipulation inference by evaluating the current sensory data, while directly optimizing the manipulation score predicted by the network over different hand configurations. The project supports the development of experimental protocols and the collection of associated data for dissemination to stimulate research activity in manipulation of object collections.</AbstractNarration>
<MinAmdLetterDate>02/23/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/27/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657596</AwardID>
<Investigator>
<FirstName>Tucker</FirstName>
<LastName>Hermans</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tucker Hermans</PI_FULL_NAME>
<EmailAddress>thermans@cs.utah.edu</EmailAddress>
<PI_PHON>8015818122</PI_PHON>
<NSF_ID>000701391</NSF_ID>
<StartDate>02/23/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName/>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~175000</FUND_OBLG>
<FUND_OBLG>2018~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p dir="ltr">This project developed a novel approach to robot grasp planning; the problem of a robot selecting the correct hand shape and location to hold and lift an object without dropping it. In particular this project examined the problem of a robot lifting an object about which the robot has no previous knowledge or experience. In order to achieve this we focused on taking a data-driven approach, where the robot autonomously attempts grasps on various objects and uses its own sensors to measure if the object was successfully lifted and held. The robot then uses this data to learn a function that takes a camera image of an object and the associated hand shape and pose as input and predicts if the grasp will be successful. We learn a neural network as the function enabling the robot to generalize its predictions to different images of objects either previously seen or unseen, and different grasp configurations.</p> <p>&nbsp;</p> <p dir="ltr">We show how the robot can then use this learned function to plan grasps for new objects. We encode this planning problem as a continuous optimization, where the learned function acts as the cost to be maximized and additional constraints can be added to ensure the planner respects the position limits imposed by the robot's joints. We demonstrated that this approach outperforms alternative grasp planning formulations commonly used in learning. Our results further show that by learning separate functions for grasps of qualitatively different types, fingertip grasps versus wrap grasps, the robot can further improve performance in terms of successful grasp execution on previously unseen objects.</p> <p>&nbsp;</p> <p dir="ltr">This project additionally examined robot in-hand manipulation, the problem of repositioning an already grasped object relative to the robot's palm, without first placing the object down. Our approach defines a novel optimization problem enabling the robot to plan in-hand manipulation sequences using only knowledge of an object's shape, not requiring mass, friction, or inertia knowledge required by previously proposed approaches.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/01/2019<br>      Modified by: Tucker&nbsp;Hermans</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[This project developed a novel approach to robot grasp planning; the problem of a robot selecting the correct hand shape and location to hold and lift an object without dropping it. In particular this project examined the problem of a robot lifting an object about which the robot has no previous knowledge or experience. In order to achieve this we focused on taking a data-driven approach, where the robot autonomously attempts grasps on various objects and uses its own sensors to measure if the object was successfully lifted and held. The robot then uses this data to learn a function that takes a camera image of an object and the associated hand shape and pose as input and predicts if the grasp will be successful. We learn a neural network as the function enabling the robot to generalize its predictions to different images of objects either previously seen or unseen, and different grasp configurations.    We show how the robot can then use this learned function to plan grasps for new objects. We encode this planning problem as a continuous optimization, where the learned function acts as the cost to be maximized and additional constraints can be added to ensure the planner respects the position limits imposed by the robot's joints. We demonstrated that this approach outperforms alternative grasp planning formulations commonly used in learning. Our results further show that by learning separate functions for grasps of qualitatively different types, fingertip grasps versus wrap grasps, the robot can further improve performance in terms of successful grasp execution on previously unseen objects.    This project additionally examined robot in-hand manipulation, the problem of repositioning an already grasped object relative to the robot's palm, without first placing the object down. Our approach defines a novel optimization problem enabling the robot to plan in-hand manipulation sequences using only knowledge of an object's shape, not requiring mass, friction, or inertia knowledge required by previously proposed approaches.          Last Modified: 04/01/2019       Submitted by: Tucker Hermans]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
