<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Robots that Learn to Communicate through Natural Human Dialog</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>936906.00</AwardTotalIntnAmount>
<AwardAmount>936906</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Robots are increasingly capable and are on the threshold of becoming a ubiquitous technology. For robots to be truly useful, people must be able to effectively communicate their needs in everyday human language. Although there is a growing body of research on natural-language processing for human-robot interaction, it typically requires some form of explicit supervision provided by an engineering expert and involves unnatural, laborious training to obtain robustness and coverage. This project involves the development of human-robot dialog systems that learn to communicate with users through natural dialog, learning from repeated normal user interactions to become more robust and capable. The project supports the education of students in the areas of natural-language processing, human-robot interaction, and machine learning, where there is significant demand for educated personnel.  It is integrated with the university's  Freshman Research Initiative, which gets undergraduate students involved in research in their first year.&lt;br/&gt;&lt;br/&gt;In order to develop human-robot dialog systems that learn to improve their communication skills through normal user interactions, the project integrates and adapts learning techniques from three currently disparate technical areas: semantic parsing, spoken dialog management, and perceptual language grounding.  The project adapts and integrates techniques for semantic-parser learning using combinatory categorial grammar (CCG), dialog management using Partially Observable Markov Decision Processes (POMDPs), and multi-modal language grounding using both visual and haptic sensors, in order to develop a dialog system for communicating with robots that comprise the Building Wide Intelligence (BWI) system being developed at the University of Texas at Austin. The research integrates the PI's expertise in semantic parsing and language grounding with the co-PI's expertise in robotics and reinforcement learning, forming a unique interdisciplinary team for developing novel and effective systems for human-robot interaction. The project includes rigorous evaluations using controlled experiments on a range of tasks using both on-line simulations with crowdsourced users, and natural user interaction with a mobile robot platform consisting of a wheeled Segway base and a Kinova robot arm being developed for the BWI system.</AbstractNarration>
<MinAmdLetterDate>08/01/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1637736</AwardID>
<Investigator>
<FirstName>Raymond</FirstName>
<LastName>Mooney</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raymond J Mooney</PI_FULL_NAME>
<EmailAddress>mooney@cs.utexas.edu</EmailAddress>
<PI_PHON>5124719558</PI_PHON>
<NSF_ID>000308265</NSF_ID>
<StartDate>08/01/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Stone</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter H Stone</PI_FULL_NAME>
<EmailAddress>pstone@cs.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000156504</NSF_ID>
<StartDate>08/01/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName/>
<StateCode>TX</StateCode>
<ZipCode>787121757</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~936906</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In order for robots to be useful for many tasks in industry, health  care, elder care, emergency response, and home maintenance, humans need  to be able to control and command them using ordinary human language.  This project has developed human-robot dialog systems that learn to  communicate with users through   natural dialog, learning from repeated  user interactions to become  more  robust and capable through normal,  everyday use. To achieve this goal, we integrated and adapted machine  learning techniques from three  disparate  areas: semantic parsing,  spoken dialog management, and  perceptual  language grounding. Semantic  parsing is the task of mapping human language to computer language the  robot can understand. Dialog  management  concerns controlling  multi-turn linguistic  interaction to aid  comprehension and task  completion. Particularly, when does a robot ask its human users  questions that clarify their goals and intentions and help the robot  learn new concepts needed to fulfill them?&nbsp; Perceptual  grounding  concerns  associating words and phrases in language to  objects,  properties and  relations in the world as perceived by the  robot&rsquo;s  sensors.This allows the robot to connect its understanding of language  to its perceptions and therefore achieve users' goals by acting in  the world. This project has developed new techniques for each of these  components, and integrated them to produce an effective, overall system that learns  to communicate with users better and accomplish their goals.&nbsp; The  system has been implemented on real robots based on a Segway base  developed at the University of Texas at Austin as part of the Building  Wide Intelligence project.&nbsp; It has been tested and evaluated on simple  tasks such as moving a particular described object from one place in a  building to another. Experiments both in simulation and with real users  demonstrated that, simply by interacting with users and learning from  these dialogs, the system improves its abiity to understand human  commands and successfully deliver objects described in natural  language. A video of the system interpretting and obeying a sample delivery command can be seen at https://youtu.be/PbOfteZ_CJc</p><br> <p>            Last Modified: 01/03/2021<br>      Modified by: Raymond&nbsp;J&nbsp;Mooney</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In order for robots to be useful for many tasks in industry, health  care, elder care, emergency response, and home maintenance, humans need  to be able to control and command them using ordinary human language.  This project has developed human-robot dialog systems that learn to  communicate with users through   natural dialog, learning from repeated  user interactions to become  more  robust and capable through normal,  everyday use. To achieve this goal, we integrated and adapted machine  learning techniques from three  disparate  areas: semantic parsing,  spoken dialog management, and  perceptual  language grounding. Semantic  parsing is the task of mapping human language to computer language the  robot can understand. Dialog  management  concerns controlling  multi-turn linguistic  interaction to aid  comprehension and task  completion. Particularly, when does a robot ask its human users  questions that clarify their goals and intentions and help the robot  learn new concepts needed to fulfill them?  Perceptual  grounding  concerns  associating words and phrases in language to  objects,  properties and  relations in the world as perceived by the  robotâ€™s  sensors.This allows the robot to connect its understanding of language  to its perceptions and therefore achieve users' goals by acting in  the world. This project has developed new techniques for each of these  components, and integrated them to produce an effective, overall system that learns  to communicate with users better and accomplish their goals.  The  system has been implemented on real robots based on a Segway base  developed at the University of Texas at Austin as part of the Building  Wide Intelligence project.  It has been tested and evaluated on simple  tasks such as moving a particular described object from one place in a  building to another. Experiments both in simulation and with real users  demonstrated that, simply by interacting with users and learning from  these dialogs, the system improves its abiity to understand human  commands and successfully deliver objects described in natural  language. A video of the system interpretting and obeying a sample delivery command can be seen at https://youtu.be/PbOfteZ_CJc       Last Modified: 01/03/2021       Submitted by: Raymond J Mooney]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
