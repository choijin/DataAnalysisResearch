<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Shape Processing with Deep Architectures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>499935.00</AwardTotalIntnAmount>
<AwardAmount>499935</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Digital representations of three-dimensional shapes are becoming an integral part of many scientific and engineering fields.  And 3D printers are becoming increasingly popular for transforming these digital representations into real objects for industrial or home use as tools, mechanical components, or household items.  Virtual environments for simulation, education, and entertainment require large numbers of diverse 3D models to maintain realism and operability.  In computer vision, several recent object recognition algorithms are trained on large synthetic datasets originating from 3D shape repositories.  All these fields and several others increasingly depend on the availability of digital 3D shapes.  However, it is still a challenge to develop tools that allow users to easily produce new 3D shapes, or even to retrieve and process existing ones from online repositories.  Current 3D modeling tools often require laborious user interaction via low-level selection and editing commands, while existing search engines for retrieving 3D shapes largely depend on manually entered tags and hand-tuned feature representations which results in unsatisfactory retrieval performance.  The PI's goal in this research is to create algorithms based on "deep" architectures that automatically learn from data how to reliably analyze and synthesize 3D shapes that are optimized for 3D shape processing and synthesis performance (so that, for example, when these shapes are 3D printed they retain desirable physical properties such as reduced mechanical stress).  Project outcomes will be released as open source code and will have broad impact not only on computer graphics and computer vision but also on industry, computer-aided design and mechanical engineering pipelines, and architectural engineering software for buildings and indoor environments.&lt;br/&gt;&lt;br/&gt;This research will make three major contributions in terms of intellectual merit: 1) Deep architectures and algorithms for learning 3D shape feature representations optimized for retrieval and processing performance; the algorithms will be trained on massive 2D image datasets as well as 3D model repositories, and the learned representations will be used for accurate shape categorization, segmentation, correspondence, style analysis, and texturing.  2) Deep architectures and algorithms for learning to reliably generate new 3D shapes based on intuitive user input, such as sketches and textual descriptions.  3) Algorithms for learning to optimize the underlying geometry of 3D shapes such that they acquire desired physical properties for 3D printing and manufacturing.</AbstractNarration>
<MinAmdLetterDate>07/11/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617333</AwardID>
<Investigator>
<FirstName>Evangelos</FirstName>
<LastName>Kalogerakis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Evangelos Kalogerakis</PI_FULL_NAME>
<EmailAddress>kalo@cs.umass.edu</EmailAddress>
<PI_PHON>4135450698</PI_PHON>
<NSF_ID>000630222</NSF_ID>
<StartDate>07/11/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>AMHERST</CityName>
<StateCode>MA</StateCode>
<ZipCode>010039242</ZipCode>
<StreetAddress><![CDATA[70 Butterfield Terrace]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~197475</FUND_OBLG>
<FUND_OBLG>2017~302460</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Digital representations of three-dimensional (3D) shapes have become an integral part of several scientific and engineering fields. Virtual environments for simulation, education, and entertainment require large amounts of diverse 3D models of shapes to maintain certain realism and operability. In the computer vision field, several object recognition algorithms are trained on large synthetic datasets originating from 3D shape repositories. 3D printers are becoming increasingly popular for transforming digital representations of shapes into real, usable objects for either industrial or home use, such as tools, mechanical components, household items, to name a few.&nbsp; All these fields and several others increasingly depend on the availability of digital 3D shapes. However, it has still been a tough challenge to develop tools that allow users to easily produce new 3D shapes, or even retrieve and process existing ones from online repositories. Traditional 3D modeling tools often require laborious user interaction, such as low-level selection and editing commands. Traditional methods for 3D shape processing, such as retrieval, segmentation, and correspondences, largely depend on hand-tuned feature representations and hand-engineered pipelines causing unsatisfactory retrieval performance.<br /><br />This proposal aimed to create algorithms that automatically learn from data how to reliably analyze and synthesize 3D shapes. In contrast to prior machine learning algorithms that performed shape analysis and synthesis by relying on "shallow" architectures with limited generalization performance and hand-tuned feature representations, the algorithms developed in this project were based on new "deep" architectures. The key idea of these architectures is to replace hand-crafted shape feature representations with learned, compositional representations that are automatically optimized for 3D shape processing and synthesis performance. These new 3D deep learning algorithms and architectures transformed the fields of 3D computer graphics and vision by introducing powerful 3D shape, point cloud, and scene representations optimized for shape/scene analysis, processing and synthesis tasks Specifically, we made progress in the following areas:<br /><br />(a) 3D shape segmentation: the ShapePFCN architecture introduced at CVPR 2017 (oral presentation) was one of the first approaches to 3D shape segmentation with deep learning (Figure 1). It demonstrated significant improvements over prior work that relied on hand-engineered feature representations or shallow architectures. It inspired several other works, and has been cited ~200 times according to Google Scholar 3 years after its original publication.<br /><br />(b) 3D point cloud processing: the SplatNet architecture introduced at CVPR 2018 (oral presentation, best paper honorable mention award) was the first method to demonstrate the power of 3D sparse convolutions in a lattice, significantly improving the state-of-the-art in 3D point cloud processing (Figure 2). The paper heavily influenced follow-up work and has been cited ~300 times according to Google Scholar 2 years after its original publication.<br /><br />(c) 3D shape reconstruction and auto-completion: we introduced a 3D fully convolutional architecture at ICCV 2017 that was able to progressively produce a high-resolution, complete surface from a partial point cloud through a volumetric encoder-decoder architecture (Figure 3). Again, this approach was one of the first of its kind, and was cited ~100 times 3 years since its original publication.<br /><br />(d) 3D correspondences: the LMVCNN architecture (TOG 2018) was able to compute 3D point correspondences (Figure 4) in a weakly supervised manner, a precursor of several other weak or few-shot learning approaches for 3D shape analysis that have appeared more recently. It has been cited ~80 times according to Google Scholar 2 years after its original publication.<br /><br />(e) 3D sketch-based modeling: the ShapeMVD architecture (3DV 2017) was the first to propose a deep learning architecture to reconstruct 3D models from 2D line drawings (Figure 5). It inspired follow-up work in graphics and vision and has been cited ~80 times according to Google Scholar 3 years after its original publication.<br /><br />(f) 3D scene synthesis and shape recognition in scenes: the SceneGraphNet architecture (ICCV 2019) proposed a graph neural network for iterative scene synthesis and context-based shape recognition significantly outperforming prior work in the field (Figure 6).<br /><br />(g) 3D shape parsing: the CSGNet architecture (CVPR 2018) was the first deep learning architecture to reverse engineer a 2D image or 3D shape into CSG modeling instructions. Furthermore, the ParseNet architecture (ECCV 2020) proposed an architecture that reverse engineers point clouds into a collection of primitives, including B-spline patches, popular in 3D geometric modeling.<br /><br />(h) artistic rendering: our Neural Contours paper (CVPR 2020) introduced the first deep learning architecture to generate line drawings from 3D models, again demonstrated significant gains compared to traditional approaches.<br /><br />(i) few-shot shape understanding: finally, we made progress in learning shape representations from few training examples, which is an increasingly important topic in the area of 3D deep learning (3DV 2019 and ECCV 2020).<br /><br />In terms of educational impacts, the PI and collaborators created tutorials on 3D deep learning presented at international conferences, including CVPR 2017, SIGGRAPH ASIA 2016, and EG 2016. Students from several universities around the world learned how to incorporate deep learning techniques in 3D shape/scene modeling and processing pipelines.</p><br> <p>            Last Modified: 12/07/2020<br>      Modified by: Evangelos&nbsp;Kalogerakis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607368438162_ShapePFCN--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607368438162_ShapePFCN--rgov-800width.jpg" title="Figure 1: ShapePFCN results"><img src="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607368438162_ShapePFCN--rgov-66x44.jpg" alt="Figure 1: ShapePFCN results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Automatic 3D shape segmentation and labeling of parts produced by the ShapePFCN architecture</div> <div class="imageCredit">Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, Siddhartha Chaudhuri</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">Figure 1: ShapePFCN results</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369127172_SplatNet--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369127172_SplatNet--rgov-800width.jpg" title="Figure 2: SplatNet architecture"><img src="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369127172_SplatNet--rgov-66x44.jpg" alt="Figure 2: SplatNet architecture"></a> <div class="imageCaptionContainer"> <div class="imageCaption">SPLATNet3D can directly process 3D point  clouds as input and predict part labels for each point (top).   SPLATNet2D-3D can jointly process both point cloud and corresponding multi-view images for better 2D and 3D predictions of part labels (bottom).</div> <div class="imageCredit">Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, Jan Kautz</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">Figure 2: SplatNet architecture</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369370016_ShapeCompletion--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369370016_ShapeCompletion--rgov-800width.jpg" title="Figure 3: 3D shape auto-completion"><img src="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369370016_ShapeCompletion--rgov-66x44.jpg" alt="Figure 3: 3D shape auto-completion"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Pipeline of our high-resolution shape completion method (ICCV 2017). Given a 3D shape with large missing regions, our deep architecture outputs a complete shape through global structure inference and local geometry refinement.</div> <div class="imageCredit">Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis, Yizhou Yu</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">Figure 3: 3D shape auto-completion</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369696883_LMVCNN--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369696883_LMVCNN--rgov-800width.jpg" title="Figure 4: LMVCNN results and applications"><img src="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607369696883_LMVCNN--rgov-66x44.jpg" alt="Figure 4: LMVCNN results and applications"></a> <div class="imageCaptionContainer"> <div class="imageCaption">LMVCNN produces local 3D shape descriptors. The network is trained such that geometrically and semantically similar points across different shapes are embedded close to each other in descriptor space (left). Our descriptors are used for shape matching, human affordance prediction, and segmentation.</div> <div class="imageCredit">Haibin Huang, Evangelos Kalogerakis, Siddhartha Chaudhuri, Duygu Ceylan, Vladimir Kim, Ersin Yumer</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">Figure 4: LMVCNN results and applications</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607370028981_ShapeMVD--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607370028981_ShapeMVD--rgov-800width.jpg" title="Figure 5: ShapeMVD results"><img src="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607370028981_ShapeMVD--rgov-66x44.jpg" alt="Figure 5: ShapeMVD results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our deep architecture is able to reconstruct 3D shapes (in blue) from 2D input line drawings based on a deep architecture.</div> <div class="imageCredit">Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji, Rui Wang</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">Figure 5: ShapeMVD results</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607371051007_Scene_Graph_Net--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607371051007_Scene_Graph_Net--rgov-800width.jpg" title="Figure 6: SceneGraphNet result"><img src="/por/images/Reports/POR/2020/1617333/1617333_10439629_1607371051007_Scene_Graph_Net--rgov-66x44.jpg" alt="Figure 6: SceneGraphNet result"></a> <div class="imageCaptionContainer"> <div class="imageCaption">SceneGraphNet can iteratively synthesize 3D indoor scenes. Given an incomplete scene, our method is used to populate it progressively with more objects at their most likely locations predicted from SceneGraphNet.</div> <div class="imageCredit">Yang Zhou, Zachary While, Evangelos Kalogerakis</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Evangelos&nbsp;Kalogerakis</div> <div class="imageTitle">Figure 6: SceneGraphNet result</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Digital representations of three-dimensional (3D) shapes have become an integral part of several scientific and engineering fields. Virtual environments for simulation, education, and entertainment require large amounts of diverse 3D models of shapes to maintain certain realism and operability. In the computer vision field, several object recognition algorithms are trained on large synthetic datasets originating from 3D shape repositories. 3D printers are becoming increasingly popular for transforming digital representations of shapes into real, usable objects for either industrial or home use, such as tools, mechanical components, household items, to name a few.  All these fields and several others increasingly depend on the availability of digital 3D shapes. However, it has still been a tough challenge to develop tools that allow users to easily produce new 3D shapes, or even retrieve and process existing ones from online repositories. Traditional 3D modeling tools often require laborious user interaction, such as low-level selection and editing commands. Traditional methods for 3D shape processing, such as retrieval, segmentation, and correspondences, largely depend on hand-tuned feature representations and hand-engineered pipelines causing unsatisfactory retrieval performance.  This proposal aimed to create algorithms that automatically learn from data how to reliably analyze and synthesize 3D shapes. In contrast to prior machine learning algorithms that performed shape analysis and synthesis by relying on "shallow" architectures with limited generalization performance and hand-tuned feature representations, the algorithms developed in this project were based on new "deep" architectures. The key idea of these architectures is to replace hand-crafted shape feature representations with learned, compositional representations that are automatically optimized for 3D shape processing and synthesis performance. These new 3D deep learning algorithms and architectures transformed the fields of 3D computer graphics and vision by introducing powerful 3D shape, point cloud, and scene representations optimized for shape/scene analysis, processing and synthesis tasks Specifically, we made progress in the following areas:  (a) 3D shape segmentation: the ShapePFCN architecture introduced at CVPR 2017 (oral presentation) was one of the first approaches to 3D shape segmentation with deep learning (Figure 1). It demonstrated significant improvements over prior work that relied on hand-engineered feature representations or shallow architectures. It inspired several other works, and has been cited ~200 times according to Google Scholar 3 years after its original publication.  (b) 3D point cloud processing: the SplatNet architecture introduced at CVPR 2018 (oral presentation, best paper honorable mention award) was the first method to demonstrate the power of 3D sparse convolutions in a lattice, significantly improving the state-of-the-art in 3D point cloud processing (Figure 2). The paper heavily influenced follow-up work and has been cited ~300 times according to Google Scholar 2 years after its original publication.  (c) 3D shape reconstruction and auto-completion: we introduced a 3D fully convolutional architecture at ICCV 2017 that was able to progressively produce a high-resolution, complete surface from a partial point cloud through a volumetric encoder-decoder architecture (Figure 3). Again, this approach was one of the first of its kind, and was cited ~100 times 3 years since its original publication.  (d) 3D correspondences: the LMVCNN architecture (TOG 2018) was able to compute 3D point correspondences (Figure 4) in a weakly supervised manner, a precursor of several other weak or few-shot learning approaches for 3D shape analysis that have appeared more recently. It has been cited ~80 times according to Google Scholar 2 years after its original publication.  (e) 3D sketch-based modeling: the ShapeMVD architecture (3DV 2017) was the first to propose a deep learning architecture to reconstruct 3D models from 2D line drawings (Figure 5). It inspired follow-up work in graphics and vision and has been cited ~80 times according to Google Scholar 3 years after its original publication.  (f) 3D scene synthesis and shape recognition in scenes: the SceneGraphNet architecture (ICCV 2019) proposed a graph neural network for iterative scene synthesis and context-based shape recognition significantly outperforming prior work in the field (Figure 6).  (g) 3D shape parsing: the CSGNet architecture (CVPR 2018) was the first deep learning architecture to reverse engineer a 2D image or 3D shape into CSG modeling instructions. Furthermore, the ParseNet architecture (ECCV 2020) proposed an architecture that reverse engineers point clouds into a collection of primitives, including B-spline patches, popular in 3D geometric modeling.  (h) artistic rendering: our Neural Contours paper (CVPR 2020) introduced the first deep learning architecture to generate line drawings from 3D models, again demonstrated significant gains compared to traditional approaches.  (i) few-shot shape understanding: finally, we made progress in learning shape representations from few training examples, which is an increasingly important topic in the area of 3D deep learning (3DV 2019 and ECCV 2020).  In terms of educational impacts, the PI and collaborators created tutorials on 3D deep learning presented at international conferences, including CVPR 2017, SIGGRAPH ASIA 2016, and EG 2016. Students from several universities around the world learned how to incorporate deep learning techniques in 3D shape/scene modeling and processing pipelines.       Last Modified: 12/07/2020       Submitted by: Evangelos Kalogerakis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
