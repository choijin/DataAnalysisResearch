<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAPSI: Generating Word Embeddings using Extreme Learning Machines for Classifying Clinical Texts</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>5400.00</AwardTotalIntnAmount>
<AwardAmount>5400</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In 1950, the computer scientist Alan Turing proposed a test for true artificial intelligence. In Turing's view, a computer must be considered intelligent if it could understand human language.  After more than 60 years of research, this is still an ongoing effort. Recent methods, including neural language models that use advanced statistics, have made great strides towards realizing Turing's vision.  This study builds on existing research to explore methods for improving computer-based natural language understanding.  The research will be conducted under the mentorship of Professor Guang-bin Huang, a noted expert on machine learning, of Nanyang Technological University.&lt;br/&gt;&lt;br/&gt;Natural language processing (NLP) involves the development of computer-based algorithms to understand natural language. Statistical language models are typically used for various NLP tasks, including machine translation and text categorization.  Language models based on neural networks, also known as neural embeddings, map words (or phrases) to a numerical representation in a low-dimensional space. Typically, neural networks use back-propagation for training a neural network, which results in slow training. Extreme Learning Machines (ELM) is a type of neural network, where hidden neurons are randomly generated hidden nodes. This study involves the use of ELM for faster training in the generation of neural embeddings.&lt;br/&gt;&lt;br/&gt;This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the National Research Foundation of Singapore.</AbstractNarration>
<MinAmdLetterDate>07/08/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1614024</AwardID>
<Investigator>
<FirstName>Paula</FirstName>
<LastName>Lauren</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paula Lauren</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>2487955019</PI_PHON>
<NSF_ID>000710991</NSF_ID>
<StartDate>07/08/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Lauren                  Paula</Name>
<CityName>Royal Oak</CityName>
<ZipCode>480735310</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Lauren                  Paula]]></Name>
<CityName>Royal Oak</CityName>
<StateCode>MI</StateCode>
<ZipCode>480735310</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5927</Code>
<Text>EAST ASIA, OTHER</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~5400</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The outcome of this research entailed the development of a low-dimensional vector representation of words using an Extreme Learning Machine (ELM) for eventual use in Natural Language Processing (NLP) tasks such as classification and summarization.&nbsp; An auto-encoder based on an ELM architecture had been utilized for this research.&nbsp;&nbsp;&nbsp;An auto-encoder is a type of neural network that performs feature learning with the capability of reducing the dimensionality of the vectors in the hidden layer of the neural network.&nbsp; The latter is especially valuable in rendering low-dimensional word vectors or word embeddings.&nbsp;&nbsp;In the&nbsp;published literature on ELM, the weight matrix is&nbsp;referred to as the beta matrix.&nbsp; After training of the network is complete,&nbsp; the extraction of the beta matrix are the word vectors.&nbsp;</p> <p>The preliminary evaluation of the results involved assessing the semantic relatedness of various words using the cosine similarity measure, a common method&nbsp;in&nbsp;the intrinsic evaluation of word embeddings.&nbsp;&nbsp;The evaluation also included comparison to state-of-the-art methods.&nbsp; The ELM-based approach performed competitively and complimentary with these other approaches.&nbsp;&nbsp; Further work entails the application of this novel approach with generating word embeddings&nbsp;for NLP tasks such as classification and summarization with publicly available datasets, and&nbsp;finally&nbsp;reporting on the results for publication.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/14/2017<br>      Modified by: Paula&nbsp;Lauren</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The outcome of this research entailed the development of a low-dimensional vector representation of words using an Extreme Learning Machine (ELM) for eventual use in Natural Language Processing (NLP) tasks such as classification and summarization.  An auto-encoder based on an ELM architecture had been utilized for this research.   An auto-encoder is a type of neural network that performs feature learning with the capability of reducing the dimensionality of the vectors in the hidden layer of the neural network.  The latter is especially valuable in rendering low-dimensional word vectors or word embeddings.  In the published literature on ELM, the weight matrix is referred to as the beta matrix.  After training of the network is complete,  the extraction of the beta matrix are the word vectors.   The preliminary evaluation of the results involved assessing the semantic relatedness of various words using the cosine similarity measure, a common method in the intrinsic evaluation of word embeddings.  The evaluation also included comparison to state-of-the-art methods.  The ELM-based approach performed competitively and complimentary with these other approaches.   Further work entails the application of this novel approach with generating word embeddings for NLP tasks such as classification and summarization with publicly available datasets, and finally reporting on the results for publication.              Last Modified: 03/14/2017       Submitted by: Paula Lauren]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
