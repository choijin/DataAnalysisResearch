<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Acquisition of a Shared Scalable Research Storage System</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project, acquiring a high-end scalable network storage system, aims to integrate an innovative storage architecture (MEMORI) into the existing research computing infrastructure at the institution to address the needs of both traditional and non-traditional HPC researchers. The proponents will then instantiate that architecture with the acquisition of the high-end scalable network storage system. The instrumentation will be managed centrally, but shared with the entire research community. The instrumentation enables a variety of projects in cyber-security, mental health, climate dynamics and social systems, all of which positively benefit the health and economic well-being of the country. In particular, the storage system is designed to support shared access by sister institutions, since GMU is a member of the 4-VA university consortium that provides state-level infrastructure promoting resource sharing. Data access by potential outside stakeholders will also be enhanced. &lt;br/&gt;&lt;br/&gt;The current research computing infrastructure is unable to support the growing storage needs for the wide variety of current research projects involving elements of "big data." As a result, individual projects are limited in their ability to store, mine, visualize and share large data sets. Rather than forcing research projects to deal with these limitations on an individual and ad hoc basis, the researchers will take advantage of recent developments in storage technologies to provide a shared storage server capable of supporting current storage needs and scalable in a cost-effective manner for future needs. The proposed architecture is specifically designed to support research projects in the humanities and social sciences, as well as the more traditional science and engineering disciplines. Furthermore, centrally managing the instrumentation creates a secure environment for multi-university shared access, appropriate control of sensitive data, and a reliable framework for data management plans, backups, and archiving.  In addition, a variety of departments offer courses in data mining, data analytics, and data visualization. These courses are currently limited in their ability to support datasets of significant size, and will greatly benefit from the proposed system. The library services will be significantly enhanced by the instrumentation. Fast network access to the system provides the ability to store local copies of large databases for text mining research by faculty and students, as well as visualization capabilities.</AbstractNarration>
<MinAmdLetterDate>09/13/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/13/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1625039</AwardID>
<Investigator>
<FirstName>Kenneth</FirstName>
<LastName>De Jong</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kenneth A De Jong</PI_FULL_NAME>
<EmailAddress>kdejong@gmu.edu</EmailAddress>
<PI_PHON>7039931553</PI_PHON>
<NSF_ID>000329301</NSF_ID>
<StartDate>09/13/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Huzefa</FirstName>
<LastName>Rangwala</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Huzefa Rangwala</PI_FULL_NAME>
<EmailAddress>hrangwal@gmu.edu</EmailAddress>
<PI_PHON>7039933826</PI_PHON>
<NSF_ID>000515447</NSF_ID>
<StartDate>09/13/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Kinter</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James Kinter</PI_FULL_NAME>
<EmailAddress>ikinter@gmu.edu</EmailAddress>
<PI_PHON>3015957000</PI_PHON>
<NSF_ID>000567815</NSF_ID>
<StartDate>09/13/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Mason University</Name>
<CityName>FAIRFAX</CityName>
<ZipCode>220304422</ZipCode>
<PhoneNumber>7039932295</PhoneNumber>
<StreetAddress>4400 UNIVERSITY DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA11</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>077817450</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE MASON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>077817450</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Mason University]]></Name>
<CityName>Fairfax</CityName>
<StateCode>VA</StateCode>
<ZipCode>220304422</ZipCode>
<StreetAddress><![CDATA[4400 University Dr.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA11</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>The rapid growth of research involving large data sets is generating computing infrastructure needs beyond the traditional HPC cluster models. In particular, there is a significant need for scalable and cost-effective storage systems and enhanced visualization tools. This project involved the design and implementation of an innovative storage architecture (MEMORI) to be integrated into the existing research computing infrastructure at George Mason University (GMU), and addressed these needs for both the traditional and non-traditional HPC researchers. &nbsp;That architecture was successfully instantiated via the acquisition and installation of a high-end scalable network storage system, managed centrally, but shared with the entire research community.</span></p> </div> </div> </div> </div> <div class="page" title="Page 1"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>The MEMORI system was installed in July 2017 with a set of 12 initial users. &nbsp;Over the next two years the user base has increased to more than 150.</span></p> <div class="page" title="Page 4"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>Some examples of existing GMU use cases:</span></p> <p><span>&nbsp;- A proteomics group is using MEMORI for sharing data and for analysis.</span></p> <p><span>&nbsp;- MRI imaging data is now uploaded to MEMORI and shared among the researchers without having to burn a DVD or copy data to a USB device.</span></p> <p><span>&nbsp;- Data generated from Nuclear Magnetic Resonance spectroscopy is stored on MEMORI and shared for analysis.</span></p> <p><span>&nbsp;- A group is using MEMORI for their DDOT (DC Dept. of Transportation) project "Design and Installation of a traffic garden", storing research data and documents, and sharing them with non-GMU collaborators.</span></p> <p><span>&nbsp;- A group is using MEMORI for their "<span>Pompeii Food and Drink" project, storing&nbsp;<span>data, videos, pictures and documents, and sharing theem with their international collaborators.</span></span></span></p> <p>In addition, being centrally managed, MEMORI provides a secure environment for multi-university shared access, appropriate control of sensitive data, and a reliable framework for data management plans, backups and archiving.</p> <p>GMU's library services have also been significantly enhanced by the system. They are now able to store local copies of large databases for text mining research by faculty and students in the humanities, as well as visualization capabilities.</p> <p><span>MEMORI also benefits the educational mission of the university, particularly with respect to courses in data mining, data analytics, and data visualization of datasets of significant size.</span></p> </div> </div> </div> </div> <p><span><br /></span></p> </div> </div> </div> </div> <p>&nbsp;</p><br> <p>            Last Modified: 10/25/2019<br>      Modified by: Kenneth&nbsp;A&nbsp;Dejong</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     The rapid growth of research involving large data sets is generating computing infrastructure needs beyond the traditional HPC cluster models. In particular, there is a significant need for scalable and cost-effective storage systems and enhanced visualization tools. This project involved the design and implementation of an innovative storage architecture (MEMORI) to be integrated into the existing research computing infrastructure at George Mason University (GMU), and addressed these needs for both the traditional and non-traditional HPC researchers.  That architecture was successfully instantiated via the acquisition and installation of a high-end scalable network storage system, managed centrally, but shared with the entire research community.          The MEMORI system was installed in July 2017 with a set of 12 initial users.  Over the next two years the user base has increased to more than 150.      Some examples of existing GMU use cases:   - A proteomics group is using MEMORI for sharing data and for analysis.   - MRI imaging data is now uploaded to MEMORI and shared among the researchers without having to burn a DVD or copy data to a USB device.   - Data generated from Nuclear Magnetic Resonance spectroscopy is stored on MEMORI and shared for analysis.   - A group is using MEMORI for their DDOT (DC Dept. of Transportation) project "Design and Installation of a traffic garden", storing research data and documents, and sharing them with non-GMU collaborators.   - A group is using MEMORI for their "Pompeii Food and Drink" project, storing data, videos, pictures and documents, and sharing theem with their international collaborators.  In addition, being centrally managed, MEMORI provides a secure environment for multi-university shared access, appropriate control of sensitive data, and a reliable framework for data management plans, backups and archiving.  GMU's library services have also been significantly enhanced by the system. They are now able to store local copies of large databases for text mining research by faculty and students in the humanities, as well as visualization capabilities.  MEMORI also benefits the educational mission of the university, particularly with respect to courses in data mining, data analytics, and data visualization of datasets of significant size.                     Last Modified: 10/25/2019       Submitted by: Kenneth A Dejong]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
