<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Collaborative Research: F: Foundations of Nonconvex Problems in BigData Science and Engineering: Models, Algorithms, and Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>400687.00</AwardTotalIntnAmount>
<AwardAmount>400687</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Victor Roytburd</SignBlockName>
<PO_EMAI>vroytbur@nsf.gov</PO_EMAI>
<PO_PHON>7032928584</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In today's digital world, huge amounts of data, i.e., big data, can be found in almost every aspect of scientific research and human activity.  These data need to be managed effectively for reliable prediction and inference to improve decision making.  Statistical learning is an emergent scientific discipline wherein mathematical modeling, computational algorithms, and statistical analysis are jointly employed to address these challenging data management problems.  Invariably, quantitative criteria need to be introduced for the overall learning process in order to gauge the quality of the solutions obtained. This research focuses on two important criteria: data fitness and sparsity representation of the underlying learning model.  Potential applications of the results can be found in computational statistics, compressed sensing, imaging, machine learning, bio-informatics, portfolio selection, and decision making under uncertainty, among many areas involving big data.&lt;br/&gt;&lt;br/&gt;Till now, convex optimization has been the dominant methodology for statistical learning in which the two criteria employed are expressed by convex functions either to be optimized and/or set as constraints of the variables being sought.  Recently, non-convex functions of the difference-of-convex (DC) type and the difference-of-convex algorithm (DCA) have been shown to yield superior results in many contexts and serve as the motivation for this project.  The goal is to develop a solid foundation and a unified framework to address many fundamental issues in big data problems in which non-convexity and non-differentiability are present in the optimization problems to be solved. These two non-standard features in computational statistical learning are challenging and their rigorous treatment requires the fusion of expertise from different domains of mathematical sciences.  Technical issues to be investigated will cover the optimality, sparsity, and statistical properties of computable solutions to the non-convex, non-smooth optimization problems arising from statistical learning and its many applications.  Novel algorithms will be developed and tested first on synthetic data sets for preliminary experimentation and then on publicly available data sets for realism; comparisons will be made among different formulations of the learning problems.</AbstractNarration>
<MinAmdLetterDate>08/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/17/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1632971</AwardID>
<Investigator>
<FirstName>Jong-Shi</FirstName>
<LastName>Pang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jong-Shi Pang</PI_FULL_NAME>
<EmailAddress>jongship@usc.edu</EmailAddress>
<PI_PHON>2137407436</PI_PHON>
<NSF_ID>000448307</NSF_ID>
<StartDate>08/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890193</ZipCode>
<StreetAddress><![CDATA[3720 S. Flower St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~400687</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="text-align: left;"><span>The principal goal is to develop a solid foundation and a unified framework to address many fundamental issues in emergent big-data problems that can be formulated as non-convex and non-differentiable optimization problems.&nbsp; &nbsp;We analyze a host of nonconvex problems arising from big-data science either directly or via their polyhedral approximations, and compare the various formulations and their approximations in specific tasks such as sparsity minimization in signal recovery and portfolio revision, structured sparsity representation, low-rank matrix construction, multi-category classication, support vector machines, and deep neural networks.&nbsp;&nbsp; We also investigate new domains of application of non-convexity.&nbsp; On the education side, we supervise doctoral students, mentor postdoctoral associates, and offer courses for graduate students.</span></p> <p style="text-align: left;"><strong>Intellectual merits</strong>:&nbsp;&nbsp;Non-convexity and non-differentiability of the objective and constraint functions involved give rise to a host of technically challenging issues that require innovative treatment, including (1) understanding different types of stationary solutions and their (locally) minimizing properties; (2) computations of such solutions and convergence analysis of algorithms; (3) deterministic and probabilistic guarantees of the quality of the computed solutions, such as certicate of optimality, asymptotic analysis of sample approximations, and bounds on sparsity; and (4) statistical properties of the computed solutions such as consistency, robustness, prediction estimates, and sparsity recovery.&nbsp; &nbsp;The intellectual merits of the project lie in the rigorous study of these issues and advance the science and engineering associated with big-data problems.</p> <p style="text-align: left;"><strong>Broader Impacts:&nbsp;&nbsp;</strong>Statistical learning problems are at the heart of knowledge discovery with applications in many fields of engineering, computer science, image science, health science, precision medicine, and social networks. The broader impacts of this project are: facilitate the formulation of enhanced and more realistic mathematical models, enable their rigorous treamtent, and&nbsp; resultin improved operations, and planning, and decision making in these individual domains.&nbsp; The project has trained the next-generation professionals and experts in this field of big-data science and placed them for gainful employment in industry and academia.&nbsp;</p> <p style="text-align: left;"><strong>Project outcomes:</strong>&nbsp;&nbsp;<span>The PI has made great strides in understanding, advancing, and applying the theory and methods of modern nonconvex nondifferentiable optimization for big-data problems. The bulk of the advances lies in the in-depth analytical treatment and design of efficient solution methods for solving non-tradtional optimization problems that arise in diverse domains where these problems are of central importance, including statistics, operations research, risk optimization, and game theory.&nbsp; The main challenge in these problems is that nonconvexity and nonsmoothness are coupled, making them fail a key regularity property that has traditionally played a central role in nonsmooth analysis. With the applied big-data problems in the backdrop, the research has allowed us to push forward the field of optimization with these ``non"-properties to a new frontier, equipped it with new technology and understanding for solving the problems on hand, and preparing the field for a new wave of advanced applications in the coming years.</span></p><br> <p>            Last Modified: 10/01/2020<br>      Modified by: Jong-Shi&nbsp;Pang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The principal goal is to develop a solid foundation and a unified framework to address many fundamental issues in emergent big-data problems that can be formulated as non-convex and non-differentiable optimization problems.   We analyze a host of nonconvex problems arising from big-data science either directly or via their polyhedral approximations, and compare the various formulations and their approximations in specific tasks such as sparsity minimization in signal recovery and portfolio revision, structured sparsity representation, low-rank matrix construction, multi-category classication, support vector machines, and deep neural networks.   We also investigate new domains of application of non-convexity.  On the education side, we supervise doctoral students, mentor postdoctoral associates, and offer courses for graduate students. Intellectual merits:  Non-convexity and non-differentiability of the objective and constraint functions involved give rise to a host of technically challenging issues that require innovative treatment, including (1) understanding different types of stationary solutions and their (locally) minimizing properties; (2) computations of such solutions and convergence analysis of algorithms; (3) deterministic and probabilistic guarantees of the quality of the computed solutions, such as certicate of optimality, asymptotic analysis of sample approximations, and bounds on sparsity; and (4) statistical properties of the computed solutions such as consistency, robustness, prediction estimates, and sparsity recovery.   The intellectual merits of the project lie in the rigorous study of these issues and advance the science and engineering associated with big-data problems. Broader Impacts:  Statistical learning problems are at the heart of knowledge discovery with applications in many fields of engineering, computer science, image science, health science, precision medicine, and social networks. The broader impacts of this project are: facilitate the formulation of enhanced and more realistic mathematical models, enable their rigorous treamtent, and  resultin improved operations, and planning, and decision making in these individual domains.  The project has trained the next-generation professionals and experts in this field of big-data science and placed them for gainful employment in industry and academia.  Project outcomes:  The PI has made great strides in understanding, advancing, and applying the theory and methods of modern nonconvex nondifferentiable optimization for big-data problems. The bulk of the advances lies in the in-depth analytical treatment and design of efficient solution methods for solving non-tradtional optimization problems that arise in diverse domains where these problems are of central importance, including statistics, operations research, risk optimization, and game theory.  The main challenge in these problems is that nonconvexity and nonsmoothness are coupled, making them fail a key regularity property that has traditionally played a central role in nonsmooth analysis. With the applied big-data problems in the backdrop, the research has allowed us to push forward the field of optimization with these ``non"-properties to a new frontier, equipped it with new technology and understanding for solving the problems on hand, and preparing the field for a new wave of advanced applications in the coming years.       Last Modified: 10/01/2020       Submitted by: Jong-Shi Pang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
