<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:   Income Learning:   A New Model for Behavior-Analysis-Inspired Learning from Human Feedback</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>70000.00</AwardTotalIntnAmount>
<AwardAmount>70000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As virtual agents and physical robots become more common, there is an increasing number of complex tasks they can usefully perform to assist humans. These tasks are typically formalized as sequential decision tasks, where robots and agents perceive states, take actions, and receive a reward feedback signal. In practice, there is a critical need to learn directly from human users if such machines are to accomplish tasks outside of those pre-specified by the original developments. Machine reinforcement learning (RL), a paradigm often used for solving sequential decision making tasks, was originally developed with inspiration from animal learning research from the applied behavior analysis (ABA) community. Existing RL approaches operationalize a limited set of ABA principles effectively; however, there are additional principles and properties from ABA research that are not well encapsulated in the existing RL formalisms, and that are likely sources of new inspiration for designing more effective RL techniques capable of learning from human teachers. This project will (1) take combine principles from ABA and RL to produce algorithms that can learn more effectively from humans, (2) evaluate these algorithms in both virtual agents and on robot platforms, and (3) investigate whether and how non-expert humans can construct sequences of tasks of increasing difficulty, similar to how expert animal trainers shape tasks. Insights from these user studies will be leveraged to further improve our algorithms' abilities to learn from human trainers. Once successful, this project will make critical progress towards allowing non-technical users to be able to teach virtual and physical agents to perform complex tasks in a natural setting, familiar to many from previous experience in training household pets.&lt;br/&gt;&lt;br/&gt;This project is a part of a larger effort between Washington State University (WSU), North Carolina State University, and Brown University. The WSU effort will focus on implementing the proposed family of machine learning algorithms, called Income Learning (I-Learning). As these algorithms are co-developed by the three universities, WSU will design user studies to evaluate when and how the principles behind I-Learning allow it to outperform other existing algorithms at learning from human feedback. WSU will primarily focus on 1) virtual agents, allowing test learning via crowdsourcing, as well as testing on 2) physical robots and study if embodiment changes user's perceptions and actions, or the algorithms' learning efficacy. Additionally, WSU will investigate 3) human curricula design. Expert trainers can shape the behavior of animals, increasing task complexity over time, so that the animals can learn a sequence of tasks much faster than if they trained directly on the final, difficult task. WSU will run user studies on crowdsourcing platforms to better understand how non-expert humans design curricula for machine learning algorithms in sequential decision tasks, and investigate how these design decisions can inform algorithm design.</AbstractNarration>
<MinAmdLetterDate>08/08/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1643614</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Taylor</PI_FULL_NAME>
<EmailAddress>taylorm@eecs.wsu.edu</EmailAddress>
<PI_PHON>5093356602</PI_PHON>
<NSF_ID>000560224</NSF_ID>
<StartDate>08/08/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Washington State University</Name>
<CityName>PULLMAN</CityName>
<ZipCode>991641060</ZipCode>
<PhoneNumber>5093359661</PhoneNumber>
<StreetAddress>280 Lighty</StreetAddress>
<StreetAddress2><![CDATA[PO BOX 641060]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041485301</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WASHINGTON STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041485301</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Washington State University]]></Name>
<CityName/>
<StateCode>WA</StateCode>
<ZipCode>991642752</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~70000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Our core hypothesis was that a new model of machine learning that more closely tracks the lawful-ness studied in the applied behavior analysis community is the key to unlocking more efficient, natural, and effective interactions between learning machines and their human teachers. Our major goal was to operationalize the known laws of behavioral analysis into learning algorithms and to evaluate the impact they have when real users interact with them.</span><br /><br /><span>The team focused on curriculum design, where a series of tasks can be specified and learned in a sequential manner. In particular, we investigated how&nbsp;</span>naive trainers design curricula, which curricula are the most effective for training our learning agents, and how to encourage naive trainers to produce more effective curricula. Our main result is that, compared to directly learning the target task, less feedback was required for the agent to 1) master the intended task, and 2) learn all tasks within the curricula (including the target task) after training on curricula designed by participants. The experiment demonstrates that non-expert humans can successfully design curricula that result in better overall agent performance than learning from scratch, even in the absence of relative curricula evaluation.</p> <p>The long-term implications of this work is that non-technical people may be able to teach robots or virtual agents to perform complex tasks without requiring them to explicitly program the desired behavior (as is currently done).</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/21/2017<br>      Modified by: Matthew&nbsp;Taylor</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our core hypothesis was that a new model of machine learning that more closely tracks the lawful-ness studied in the applied behavior analysis community is the key to unlocking more efficient, natural, and effective interactions between learning machines and their human teachers. Our major goal was to operationalize the known laws of behavioral analysis into learning algorithms and to evaluate the impact they have when real users interact with them.  The team focused on curriculum design, where a series of tasks can be specified and learned in a sequential manner. In particular, we investigated how naive trainers design curricula, which curricula are the most effective for training our learning agents, and how to encourage naive trainers to produce more effective curricula. Our main result is that, compared to directly learning the target task, less feedback was required for the agent to 1) master the intended task, and 2) learn all tasks within the curricula (including the target task) after training on curricula designed by participants. The experiment demonstrates that non-expert humans can successfully design curricula that result in better overall agent performance than learning from scratch, even in the absence of relative curricula evaluation.  The long-term implications of this work is that non-technical people may be able to teach robots or virtual agents to perform complex tasks without requiring them to explicitly program the desired behavior (as is currently done).          Last Modified: 12/21/2017       Submitted by: Matthew Taylor]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
