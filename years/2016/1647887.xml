<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps: Semantic Video - from Video to Descriptions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Steven Konsek</SignBlockName>
<PO_EMAI>skonsek@nsf.gov</PO_EMAI>
<PO_PHON>7032927021</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this I-Corps project involves computer vision analysis of video, using both visual and auditory cues, to create descriptions of the content. The technology has a large variety of potential applications from law enforcement to surveillance to consumer applications. These include enabling the efficient storage and retrieval of large volumes of camera data.  Smart surveillance systems can be enhanced with features that allows for summarization of daylong video footages as a list of security-relevant events.  The technology can also allow automated organization of large collections of multimedia data.&lt;br/&gt;&lt;br/&gt;This I-Corps project involves commercialization feasibility research for a computer vision technology for expressing video content in terms of natural language text and grammar, i.e. semantics. This project builds on a video analysis framework that leverages state-of-the-art methods for object detection and action recognition in a unified formalism encoded in terms of a mathematical and statistical approach known as pattern theory. The video analysis approach can (i) handle structural variability of complex events without requiring large training data while exploiting easily available ontological information, (ii) overcome classification errors of machine learning classifiers of actions and objects, (iii) accommodate scene clutter, i.e. extraneous objects that do not in the activity present in the scene, (iv) and manage sequences of elementary events, all without retraining. The formalism allows for the easy incorporation of temporal, spatial, and logical constraints. This team has demonstrated this system on standard datasets used to benchmark performance in computer vision for human activity recognition tasks.</AbstractNarration>
<MinAmdLetterDate>08/09/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/09/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1647887</AwardID>
<Investigator>
<FirstName>Sudeep</FirstName>
<LastName>Sarkar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sudeep Sarkar</PI_FULL_NAME>
<EmailAddress>sarkar@usf.edu</EmailAddress>
<PI_PHON>8139742113</PI_PHON>
<NSF_ID>000285699</NSF_ID>
<StartDate>08/09/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of South Florida</Name>
<CityName>Tampa</CityName>
<ZipCode>336172008</ZipCode>
<PhoneNumber>8139742897</PhoneNumber>
<StreetAddress>4019 E. Fowler Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 100]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL14</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>069687242</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTH FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>069687242</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of South Florida]]></Name>
<CityName>Tampa</CityName>
<StateCode>FL</StateCode>
<ZipCode>336129446</ZipCode>
<StreetAddress><![CDATA[3702 Spectrum Blvd.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>14</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL14</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The team consisted of the Fillipe de Souza, the entrepreneurial lead, Sudeep Sarkar, the principal investigator, and Matthew Galvin, the business mentor.&nbsp; The team's core technical idea was in the space of video content understanding technology.&nbsp; Pattern theory based technology enables computers, and future robots, with the ability to analyze and understand the semantics of video events. The associated software can translate a video into words and phrases that describe the actions and objects involved in the activity captured in the video.</p> <p>The intellectual merit of the approach lies in its simplicity and power of representation. With only a few assumptions and mathematical constructs, built out of Grenander's Pattern Theory model, we can model a broad and diverse variety of actions and activities. The primary advantage from an implementation point of view is that the approach does not rely on the enormous amount of training data, which is the mainstay of current deep machine learning methods.</p> <p>The broader impact/commercial potential of this I-Corps project involves computer vision analysis of video, using both visual and auditory cues, to create descriptions of the content.</p> <p>At the NSF I-Corps Bay Area Summer Cohort 2016, we were given the opportunity to investigate the commercialization opportunities of this technology for immediate use. We fully immersed and engaged in the process of customer discovery, as systematically put forward and promoted by Steve Blank and his colleagues, performing more than 105 customer interviews exploring different customer segments.</p> <p>From these interviews, we realized that we had to go back to the drawing boards. Although there was potential product-market fit for the profitable advertising market, we found out that our technology was not technically ready. We had algorithms and software for handling specific domains. We researched and developed algorithms code to expand the capability of the technology to accommodate the needs for handling general domains.</p> <p>Based on this new theory and our customer discovery findings we have also sketched out a minimal viable product (MVP) that captures the use cases and requirements of the discovered commercial applications learned with the customer interviews.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/23/2017<br>      Modified by: Sudeep&nbsp;Sarkar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The team consisted of the Fillipe de Souza, the entrepreneurial lead, Sudeep Sarkar, the principal investigator, and Matthew Galvin, the business mentor.  The team's core technical idea was in the space of video content understanding technology.  Pattern theory based technology enables computers, and future robots, with the ability to analyze and understand the semantics of video events. The associated software can translate a video into words and phrases that describe the actions and objects involved in the activity captured in the video.  The intellectual merit of the approach lies in its simplicity and power of representation. With only a few assumptions and mathematical constructs, built out of Grenander's Pattern Theory model, we can model a broad and diverse variety of actions and activities. The primary advantage from an implementation point of view is that the approach does not rely on the enormous amount of training data, which is the mainstay of current deep machine learning methods.  The broader impact/commercial potential of this I-Corps project involves computer vision analysis of video, using both visual and auditory cues, to create descriptions of the content.  At the NSF I-Corps Bay Area Summer Cohort 2016, we were given the opportunity to investigate the commercialization opportunities of this technology for immediate use. We fully immersed and engaged in the process of customer discovery, as systematically put forward and promoted by Steve Blank and his colleagues, performing more than 105 customer interviews exploring different customer segments.  From these interviews, we realized that we had to go back to the drawing boards. Although there was potential product-market fit for the profitable advertising market, we found out that our technology was not technically ready. We had algorithms and software for handling specific domains. We researched and developed algorithms code to expand the capability of the technology to accommodate the needs for handling general domains.  Based on this new theory and our customer discovery findings we have also sketched out a minimal viable product (MVP) that captures the use cases and requirements of the discovered commercial applications learned with the customer interviews.          Last Modified: 09/23/2017       Submitted by: Sudeep Sarkar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
