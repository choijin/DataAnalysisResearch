<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: Collaborative Research: MRI Powered &amp; Guided Tetherless Effectors for Localized Therapeutic Interventions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2017</AwardEffectiveDate>
<AwardExpirationDate>12/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>607937.00</AwardTotalIntnAmount>
<AwardAmount>623937</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Magnetic Resonance Imaging (MRI) scanners use strong magnetic fields to safely image soft tissues deep inside the body. They offer a unique tool for guiding therapies: images while patient is inside the scanner can localize diseased tissue and guide an intervention with high accuracy. This research controls MRI magnetic fields to wirelessly push millimeter-scale robots through vessels in the body, assemble them into tools, and provide targeted drug delivery or pierce tissue. This will directly impact healthcare, improving patient outcome by enabling unparalleled minimal invasiveness resulting in faster recovery, fewer side effects, and cost-effectiveness. This transformative toolset for multi-agent control will set the foundation for a wealth of medical therapies and surgical interventions. &lt;br/&gt;&lt;br/&gt;Using magnetic forces of clinical MRI scanners to steer miniature tetherless effectors through human bodies and combining with real-time imaging and operator immersion could transform the practice of minimally invasive interventions. This CPS will seamlessly integrate physical (scanner sensor/actuator, effectors, patient, operator) and cyber (world modeling, combined sensor and effector control, operator immersion). Work entails: (1) Portfolio of parametric effector designs that can be optimized to exploit the constraints of a given clinical procedure. (2) Toolbox of automatic controllers for MRI-based powering and steering of tetherless effectors in the body lumen, self-assembling them into tools, and precision therapy delivery or to pierce tissue. (3) Real-time MRI-based sensing of the physical world for imaging and tracking effectors and tissue. (4) Linked effector and MRI scanner control on-the-fly. (5) Visual/force-feedback human-robot interfacing. The work focuses on two effector classes: an MRI Gauss gun that stores magnetic potential energy released by a chain reaction when robots self-assemble, and an MRI pile-driver that converts kinetic energy from an enclosed sphere into impulses to tunnel into tissue. These approaches will be validated through analytical modeling, scaled hardware experiments, and experiments in clinical MRI scanners.</AbstractNarration>
<MinAmdLetterDate>09/02/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/17/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1646566</AwardID>
<Investigator>
<FirstName>Nikolaos</FirstName>
<LastName>Tsekos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nikolaos Tsekos</PI_FULL_NAME>
<EmailAddress>nvtsekos@central.uh.edu</EmailAddress>
<PI_PHON>3142760509</PI_PHON>
<NSF_ID>000517579</NSF_ID>
<StartDate>09/02/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Aaron</FirstName>
<LastName>Becker</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aaron T Becker</PI_FULL_NAME>
<EmailAddress>atbecker@uh.edu</EmailAddress>
<PI_PHON>7137439240</PI_PHON>
<NSF_ID>000668782</NSF_ID>
<StartDate>09/02/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036837920</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042916627</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770044005</ZipCode>
<StreetAddress><![CDATA[4726 Calhoun Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7918</Code>
<Text>CPS-Cyber-Physical Systems</Text>
</ProgramElement>
<ProgramElement>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramElement>
<ProgramReference>
<Code>7918</Code>
<Text>CYBER-PHYSICAL SYSTEMS (CPS)</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8235</Code>
<Text>CPS-Synergy</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~607937</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-9a1881f2-7fff-95c7-8404-4245c3ab54f5"> </span></p> <p><span id="docs-internal-guid-fb86ecf1-7fff-f47e-1ff9-98e4c3439714"> </span></p> <p><span id="docs-internal-guid-c4428c21-7fff-26e7-5590-bacd045abfa1"> </span></p> <p dir="ltr"><span>Magnetic Resonance Imaging (MRI) scanners use strong magnetic fields to safely image soft tissues deep inside the body. They offer a unique tool for guiding therapies: images taken while the patient is inside the scanner can both localize diseased tissue and be used to guide an intervention with high accuracy. This research controls MRI magnetic fields to wirelessly push millimeter-scale robots through vessels in the body, assemble them into tools, and provide targeted drug delivery or pierce tissue. In a collaboration between the electrical engineering and computer science departments at the University of Houston and the Division of Cardiovascular Imaging at Houston Methodist Hospital, scientists, engineers and surgeons worked on methods to improve imaging and access in a cyber physical medical system.</span></p> <p dir="ltr"><span>&nbsp;</span></p> <p dir="ltr"><span>Our research has built families of miniature magnetically-propelled robots (effectors) and a control apparatus able to perform both 3-D path following and blood clot removal. These include a magnetic-hammer type actuator capable of propelling a milli-scale robot through tissue, magnetic robots that self-assemble, and helical swimmers propelled by magnetic fields at speeds of up to 100 mm/s along 3D complex paths. We developed the world's first robotic magnetic manipulator cooled by liquid nitrogen</span></p> <p dir="ltr"><span>&nbsp;</span></p> <p dir="ltr"><span>In synergy with these physical robots, our team has focused on the computational core to help physicians partner with the MRI, use the 3D data, plan the path and guide the miniature magnetically-propelled robots.&nbsp; We developed a platform that integrates a customized MRI data acquisition scheme with reconstruction and three-dimensional (3D) visualization modules along with a module for controlling an MRI-compatible robotic device to facilitate the performance of robot-assisted, MRI-guided interventional procedures. We provided a framework for exploring dynamically acquired MRI data for interactive immersion with data, integration of image processing and analytics, and rendering and fusion with an augmented reality (AR) interface. Our work in human-machine interfacing included the wireless head-mounted displays (HMD) to enhance 3D image visualization by immersing the physician into the 3D morphology as this was rendered from MRI data on the fly as they were collected.&nbsp;&nbsp;</span></p> <p dir="ltr"><span>&nbsp;</span></p> <p dir="ltr"><span>An important part of the CPS is the interaction with the human elements.&nbsp; User evaluation studies assessed the effect of information rendered by an interventional planning software on the operator's ability to plan transrectal magnetic resonance (MR)-guided prostate biopsies using actuated robotic manipulators. This project also benefited from recent advances in machine learning. We trained convolutional neural nets (CNN)s to learn hierarchies of features automatically and robustly perform classification. This enables automatic&nbsp; feature extraction and precise localization.</span></p> <p dir="ltr"><span>&nbsp;</span></p> <p dir="ltr"><span>Research for this project was used in the dissertations of five Ph.D. students, the theses of seven MS students, introduced 12 undergraduates to university research, and led to 26 peer-reviewed publications.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p dir="ltr">&nbsp;</p><br> <p>            Last Modified: 04/28/2021<br>      Modified by: Aaron&nbsp;T&nbsp;Becker</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1646566/1646566_10458115_1619577583635_MagHammer--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1646566/1646566_10458115_1619577583635_MagHammer--rgov-800width.jpg" title="Magnetic Hammer"><img src="/por/images/Reports/POR/2021/1646566/1646566_10458115_1619577583635_MagHammer--rgov-66x44.jpg" alt="Magnetic Hammer"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A prototype containing a steel bead rests in front of two magnetic coils</div> <div class="imageCredit">Julien Leclerc</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Aaron&nbsp;T&nbsp;Becker</div> <div class="imageTitle">Magnetic Hammer</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1646566/1646566_10458115_1619641960114_imageMRI--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1646566/1646566_10458115_1619641960114_imageMRI--rgov-800width.jpg" title="Hololens MRI data"><img src="/por/images/Reports/POR/2021/1646566/1646566_10458115_1619641960114_imageMRI--rgov-66x44.jpg" alt="Hololens MRI data"></a> <div class="imageCaptionContainer"> <div class="imageCaption">MRI data is 3D.  Recent advances in augmented reality allowed us to share reconstructed vasculature from MRI scans (and to identify tumors), and display this information in 3D to surgeons, who could then interact with the data through voice commands and gestures.</div> <div class="imageCredit">Daniel Velazco</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Aaron&nbsp;T&nbsp;Becker</div> <div class="imageTitle">Hololens MRI data</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[         Magnetic Resonance Imaging (MRI) scanners use strong magnetic fields to safely image soft tissues deep inside the body. They offer a unique tool for guiding therapies: images taken while the patient is inside the scanner can both localize diseased tissue and be used to guide an intervention with high accuracy. This research controls MRI magnetic fields to wirelessly push millimeter-scale robots through vessels in the body, assemble them into tools, and provide targeted drug delivery or pierce tissue. In a collaboration between the electrical engineering and computer science departments at the University of Houston and the Division of Cardiovascular Imaging at Houston Methodist Hospital, scientists, engineers and surgeons worked on methods to improve imaging and access in a cyber physical medical system.   Our research has built families of miniature magnetically-propelled robots (effectors) and a control apparatus able to perform both 3-D path following and blood clot removal. These include a magnetic-hammer type actuator capable of propelling a milli-scale robot through tissue, magnetic robots that self-assemble, and helical swimmers propelled by magnetic fields at speeds of up to 100 mm/s along 3D complex paths. We developed the world's first robotic magnetic manipulator cooled by liquid nitrogen   In synergy with these physical robots, our team has focused on the computational core to help physicians partner with the MRI, use the 3D data, plan the path and guide the miniature magnetically-propelled robots.  We developed a platform that integrates a customized MRI data acquisition scheme with reconstruction and three-dimensional (3D) visualization modules along with a module for controlling an MRI-compatible robotic device to facilitate the performance of robot-assisted, MRI-guided interventional procedures. We provided a framework for exploring dynamically acquired MRI data for interactive immersion with data, integration of image processing and analytics, and rendering and fusion with an augmented reality (AR) interface. Our work in human-machine interfacing included the wireless head-mounted displays (HMD) to enhance 3D image visualization by immersing the physician into the 3D morphology as this was rendered from MRI data on the fly as they were collected.     An important part of the CPS is the interaction with the human elements.  User evaluation studies assessed the effect of information rendered by an interventional planning software on the operator's ability to plan transrectal magnetic resonance (MR)-guided prostate biopsies using actuated robotic manipulators. This project also benefited from recent advances in machine learning. We trained convolutional neural nets (CNN)s to learn hierarchies of features automatically and robustly perform classification. This enables automatic  feature extraction and precise localization.   Research for this project was used in the dissertations of five Ph.D. students, the theses of seven MS students, introduced 12 undergraduates to university research, and led to 26 peer-reviewed publications.              Last Modified: 04/28/2021       Submitted by: Aaron T Becker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
