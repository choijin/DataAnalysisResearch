<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Index Coding and Matrix Factorizations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>449094.00</AwardTotalIntnAmount>
<AwardAmount>449094</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many communication systems, content delivery systems, and machine learning algorithms rely on algorithms that use matrix factorizations with some structure constraints. A specific type of matrix factorization of significant theoretical and practical interest is called "Index Coding". The research in this project is centered around studying the foundations of index coding and the development of algorithms for structured matrix factorizations. Beyond the development of theoretical foundations, this research is expected to lead to better algorithms for caching in wireless networks and for designing distributed storage codes. The project also includes plans for curriculum development, student training, and tutorial presentations.&lt;br/&gt;&lt;br/&gt;The specific focus of this research program consists of three thrusts: 1) Developing novel algorithms and bounds for index coding, 2) Understanding the connections of index coding with distributed storage codes other factorization problems and 3) Using index coding to obtain bounds and achievable schemes for information flow problems. Preliminary work indicates that this is possible for multiple unicasts and more possibly more generally. Connections with semidefinite programming relaxations and graph theory are also considered in this project.</AbstractNarration>
<MinAmdLetterDate>06/27/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618689</AwardID>
<Investigator>
<FirstName>Georgios-Alex</FirstName>
<LastName>Dimakis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Georgios-Alex Dimakis</PI_FULL_NAME>
<EmailAddress>dimakis@austin.utexas.edu</EmailAddress>
<PI_PHON>5124713068</PI_PHON>
<NSF_ID>000515168</NSF_ID>
<StartDate>06/27/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121532</ZipCode>
<StreetAddress><![CDATA[101 East 27th St., Suite 5.300]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7935</Code>
<Text>COMM &amp; INFORMATION THEORY</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~449094</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p><br />Datasets needed for training current machine learning models are becoming increasingly larger and distributed storage and training are needed for efficiency and robustness. This project studied the uses of coding theoretic methods to distributed information processing and storage. Also we studied how the problem of index coding is deeply connected to distributed storage and graph theory questions.&nbsp;<br /><br />One important problem is designing codes to help distributed machine learning training in the presence of stragglers: These correspond to a few machines that participate in distributed training but can be significantly slower compared to most other workers. We showed (by measuring training on cloud infrastructure) that a few machines can be significantly slower compared to all others hence creating a bottleneck for training time. This project also tackled this challenge by introducing a novel technique called gradient coding to mitigate straggler delays. Gradient coding techniques introduce redundancy&nbsp; in storage but also gradient computation, during stochastic gradient descent, or batch gradient descent. This redundancy allows distributed training to continue even if a few machines fail to complete their assigned computations.&nbsp;<br />We also introduced an approximate variant of the gradient coding problem. In this variant, the requirement for exact computation of the full gradient is traded by an approximate one, where the deviation of the generated vector from the true gradient is guaranteed to have a small error norm. This problem creates a new set of achievable solutions on the tradeoff between robustness to stragglers and computational efficiency.&nbsp;<br />This project developed novel coding theoretic techniques that can be used for fast distributed training of machine learning models. Further, our techniques can be useful for&nbsp; federated learning and distributed model selection.&nbsp;</p><br> <p>            Last Modified: 06/13/2020<br>      Modified by: Georgios-Alex&nbsp;Dimakis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     Datasets needed for training current machine learning models are becoming increasingly larger and distributed storage and training are needed for efficiency and robustness. This project studied the uses of coding theoretic methods to distributed information processing and storage. Also we studied how the problem of index coding is deeply connected to distributed storage and graph theory questions.   One important problem is designing codes to help distributed machine learning training in the presence of stragglers: These correspond to a few machines that participate in distributed training but can be significantly slower compared to most other workers. We showed (by measuring training on cloud infrastructure) that a few machines can be significantly slower compared to all others hence creating a bottleneck for training time. This project also tackled this challenge by introducing a novel technique called gradient coding to mitigate straggler delays. Gradient coding techniques introduce redundancy  in storage but also gradient computation, during stochastic gradient descent, or batch gradient descent. This redundancy allows distributed training to continue even if a few machines fail to complete their assigned computations.  We also introduced an approximate variant of the gradient coding problem. In this variant, the requirement for exact computation of the full gradient is traded by an approximate one, where the deviation of the generated vector from the true gradient is guaranteed to have a small error norm. This problem creates a new set of achievable solutions on the tradeoff between robustness to stragglers and computational efficiency.  This project developed novel coding theoretic techniques that can be used for fast distributed training of machine learning models. Further, our techniques can be useful for  federated learning and distributed model selection.        Last Modified: 06/13/2020       Submitted by: Georgios-Alex Dimakis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
