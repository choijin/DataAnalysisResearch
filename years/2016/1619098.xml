<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TWC: Small: Automatic Techniques for Evaluating and Hardening Machine Learning Classifiers in the Presence of Adversaries</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>494884.00</AwardTotalIntnAmount>
<AwardAmount>494884</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei-Shinn Ku</SignBlockName>
<PO_EMAI>weiku@nsf.gov</PO_EMAI>
<PO_PHON>7032928318</PO_PHON>
</ProgramOfficer>
<AbstractNarration>New security exploits emerge far faster than manual analysts can analyze them, driving growing interest in automated machine learning tools for computer security. Classifiers based on machine learning algorithms have shown promising results for many security tasks including malware classification and network intrusion detection, but classic machine learning algorithms are not designed to operate in the presence of adversaries. Intelligent and adaptive adversaries may actively manipulate the information they present in attempts to evade a trained classifier, leading to a competition between the designers of learning systems and attackers who wish to evade them. This project is developing automated techniques for predicting how well classifiers will resist the evasions of adversaries, along with general methods to automatically harden machine-learning classifiers against adversarial evasion attacks.&lt;br/&gt;&lt;br/&gt;At the junction between machine learning and computer security, this project involves two main tasks: (1) developing a framework that can automatically assess the robustness of a classifier by using evolutionary techniques to simulate an adversary's efforts to evade that classifier; and (2) improving the robustness of classifiers by developing generic machine learning architectures that employ randomized models and co-evolution to automatically harden machine-learning classifiers against adversaries. Our system aims to allow a classifier designer to understand how the classification performance of a model degrades under evasion attacks, enabling better-informed and more secure design choices. The framework is general and scalable, and takes advantage of the latest advances in machine learning and computer security.</AbstractNarration>
<MinAmdLetterDate>09/01/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/11/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1619098</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Evans</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David E Evans</PI_FULL_NAME>
<EmailAddress>evans@cs.virginia.edu</EmailAddress>
<PI_PHON>4349822218</PI_PHON>
<NSF_ID>000319935</NSF_ID>
<StartDate>09/01/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Westley</FirstName>
<LastName>Weimer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Westley Weimer</PI_FULL_NAME>
<EmailAddress>weimerw@umich.edu</EmailAddress>
<PI_PHON>7346159916</PI_PHON>
<NSF_ID>000205470</NSF_ID>
<StartDate>09/01/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yanjun</FirstName>
<LastName>Qi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yanjun Qi</PI_FULL_NAME>
<EmailAddress>yq2h@virginia.edu</EmailAddress>
<PI_PHON>4349822225</PI_PHON>
<NSF_ID>000663606</NSF_ID>
<StartDate>09/01/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia]]></Name>
<CityName>Charlottesville</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044195</ZipCode>
<StreetAddress><![CDATA[P. O. Box 400195]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~494884</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-837b968a-7fff-c8db-ba0f-f1af775da5f2"> <p dir="ltr"><span>At the junction between machine learning and computer security, this project&nbsp; developed automated techniques for predicting how well classifiers will resist the evasions of adversaries, along with general methods to automatically harden machine-learning classifiers against adversarial evasion attacks.</span></p> <p dir="ltr"><span>This project has made contributions via four fronts:&nbsp;</span></p> <p dir="ltr"><span>(1) Improving the robustness of classifiers by developing methods to automatically harden machine-learning classifiers against adversaries. Our main contribution is the &ldquo;feature squeezing&rdquo; techniques that reduce the search space for adversaries by coalescing inputs.&nbsp;</span></p> <p dir="ltr"><span>(2) Developing techniques to automatically assess the robustness of a classifier.&nbsp; Our main contribution is a genetic-programming based evolutionary framework to automatically find variants that preserve malicious behavior but evade a target classifier.</span></p> <p dir="ltr"><span>(3) </span><span>Understanding how deep learning classification performance degrades under evasion attacks, enabling better-informed and more secure design choices. Our contribution is a so-called &ldquo;deepWordBug&rdquo; framework generating natural language adversarial examples via greedy search under the black-box setting. We later revised deepWordBug via Monte Carlo Tree Search and Homoglyph Attack.&nbsp;</span></p> <p dir="ltr"><span>(4) Understanding how deep learning classification performance degrades under evasion attacks and consider a budget. Our main contribution is a new suite of hybrid batch attacks finding black-box adversarial examples with limited queries.</span></p> <p dir="ltr"><span>Our systems will allow a classifier designer to understand how the classification performance of a model degrades under evasion attacks, enabling better-informed and more secure design choices. The developed methods are general and scalable, taking advantage of the latest advances in machine learning and computer security. Computational methods, tools, and datasets developed in this proposal are available via url{https://qdata.github.io/secureml-web/}. </span></p> </span></p> <p>&nbsp;</p> <div></div><br> <p>            Last Modified: 12/18/2019<br>      Modified by: Yanjun&nbsp;Qi</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1619098/1619098_10457751_1576693833644_squeezing--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1619098/1619098_10457751_1576693833644_squeezing--rgov-800width.jpg" title="Feature Squeezing"><img src="/por/images/Reports/POR/2019/1619098/1619098_10457751_1576693833644_squeezing--rgov-66x44.jpg" alt="Feature Squeezing"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Reduce search space for adversaries by coalescing inputs. (Top row shows &#8467;0 adversarial examples, squeezed by median smoothing.)</div> <div class="imageCredit">Weilin Xu</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Yanjun&nbsp;Qi</div> <div class="imageTitle">Feature Squeezing</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  At the junction between machine learning and computer security, this project  developed automated techniques for predicting how well classifiers will resist the evasions of adversaries, along with general methods to automatically harden machine-learning classifiers against adversarial evasion attacks. This project has made contributions via four fronts:  (1) Improving the robustness of classifiers by developing methods to automatically harden machine-learning classifiers against adversaries. Our main contribution is the "feature squeezing" techniques that reduce the search space for adversaries by coalescing inputs.  (2) Developing techniques to automatically assess the robustness of a classifier.  Our main contribution is a genetic-programming based evolutionary framework to automatically find variants that preserve malicious behavior but evade a target classifier. (3) Understanding how deep learning classification performance degrades under evasion attacks, enabling better-informed and more secure design choices. Our contribution is a so-called "deepWordBug" framework generating natural language adversarial examples via greedy search under the black-box setting. We later revised deepWordBug via Monte Carlo Tree Search and Homoglyph Attack.  (4) Understanding how deep learning classification performance degrades under evasion attacks and consider a budget. Our main contribution is a new suite of hybrid batch attacks finding black-box adversarial examples with limited queries. Our systems will allow a classifier designer to understand how the classification performance of a model degrades under evasion attacks, enabling better-informed and more secure design choices. The developed methods are general and scalable, taking advantage of the latest advances in machine learning and computer security. Computational methods, tools, and datasets developed in this proposal are available via url{https://qdata.github.io/secureml-web/}.             Last Modified: 12/18/2019       Submitted by: Yanjun Qi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
