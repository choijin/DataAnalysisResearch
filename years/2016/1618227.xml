<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: Structured Inference for Low-Level Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>305000.00</AwardTotalIntnAmount>
<AwardAmount>305000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Vision is a valuable sensing modality because it is versatile. It lets humans navigate through unfamiliar environments, discover assets, grasp and manipulate tools, react to projectiles, track targets through clutter, interpret body language, and recognize familiar objects and people. This versatility stems from low-level visual processes that somehow produce, from ambiguous retinal measurements, useful intermediate representations of depth, surface orientation, motion, and other intrinsic scene properties. This project establishes a mathematical and computational foundation for similar low-level processing in machines. The key challenge it addresses is how to usefully encode and exploit the fact that, visually, the world exhibits substantial intrinsic structure. By advancing understanding of low-level vision in machines, this project makes progress toward computer vision systems that can compare to vision in humans, in terms of accuracy, reliability, speed, and power-efficiency.&lt;br/&gt;&lt;br/&gt;This research revisits low-level vision, and develops a comprehensive framework that possesses a common abstraction for information from different optical cues; the ability to encode scene structure across large regions and at multiple scales; implementation as parallel and distributed processing; and large-scale end-to-end learnability. The project approaches low-level vision as a structured prediction task, with ambiguous local predictions from many overlapping receptive fields being combined to produce a consistent global scene map that spans the visual field. The structured prediction models are different from those used for categorical tasks such as semantic segmentation, because they are specifically designed to accommodate the distinctive requirements and properties of low-level vision: continuous-valued output spaces; ambiguities that may form equiprobable manifolds; extreme scale variations; and global scene maps with higher-order piecewise smoothness. By strengthening the computational foundations of low-level vision, this project strives to enable many kinds of vision systems that are more efficient and more versatile, and it strives to have impacts across the breadth of computer vision.</AbstractNarration>
<MinAmdLetterDate>06/07/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618227</AwardID>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Zickler</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd Zickler</PI_FULL_NAME>
<EmailAddress>zickler@eecs.harvard.edu</EmailAddress>
<PI_PHON>6174954390</PI_PHON>
<NSF_ID>000118883</NSF_ID>
<StartDate>06/07/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021382933</ZipCode>
<StreetAddress><![CDATA[33 Oxford Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~305000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-7eab8384-7fff-af01-c430-0b7218f846f2" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The principal disciplinary field of the project is computer vision, which broadly speaking has the goal of enabling machines to "see". In particular, the project concerns low-level vision, which focuses on processing pixel values into useful information about 3D scene structure, including depth, motion, surface orientation, and surface color and material properties. The project's impact on this field can be described in three main categories:</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">1. Local predictors. These are computational units that can process the image data within a small, local image region (a receptive field) to produce a concise description of the set of likely scene values in that region, such as the local scene shape, motion, or material properties. We discovered new and powerful local predictors for: real-time depth from defocus (Guo et al., ICCV 2017); stereo occlusion boundaries (Wang et al., CVPR 2019); lighting-invariant shape from shading (Heal et al., CVPR 2020); shape from texture (Verbin et al., CVPR 2020); shape from RGB photometric stereo (Chakrabarti and Sunkavalli, 3DV 2016); monocular depth as derivative distributions (Shao et al., NeurIPS 2016) and patch samples (Xia et al., CVPR 2020a); and surface chromaticity under mixed unknown lighting (Hui et al., CVPR 2019).</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">2. Globalization schemes. These are distributed, convergent algorithms for incorporating the sets of local predictions at many locations in an image into consistent global scene maps such as depth maps, material maps, or optical flow fields. Our works above in (Chakrabarti and Sunkavalli, 3DV 2016) and (Shao et al., NeurIPS 2016) included such globalization methods, while our method in (Xia et al., CVPR 2020a) was able to form global maps based not only from our local predictors but also from external sources of information. We also invented new globalization algorithms for: depth from stereo via occlusion-aware dynamic programming (Wang et al., 2017), and shape from texture (Verbin et al., CVPR 2020; Verbin et al., IEEE T-PAMI). Moreover, we explored the use of global ?non-local? image structure for applications in single image denoising (Xia and Chakrabarti; WACV 2020); burst denoising (Xia et al., CVPR 2020b); and processing of flash no-flash pairs (Xia et al., CVPR 2021).</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">3. End-to-end Learning. These are structured learning techniques that allow the simultaneous training of parameters for globalization processes and local predictors, via end-to-end, loss-descent learning from training data. We introduced a fast stereo method in (Yee and Chakrabarti, WACV 2020) that trained a globalization network with traditional fast local predictors, while our works in (Hui et al., CVPR 2019), (Xia et al., WACV 2020), (Xia et al., CVPR 2020b), and (Xia et al., CVPR 2021) trained local and globalization portions of the pipeline together. We invented a new scheme for end-to-end learning from a single image, where the vision task is formulated as a sort of three-player game with an equilibrium that yields both shape and material properties (Verbin and Zickler, CVPR 2020). We also introduced new learning algorithms specifically directed towards high dimensional scene-map outputs to improve stability (Neyshabur et al., arXiv:1705.07831), data efficiency (Xia and Chakrabarti; NeurIPS 2019) and memory efficiency (Chakrabarti and Moseley; NeurIPS 2019).</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Together, these new forms of visual processing provide a foundation for robotic vision systems that come closer to mimicking the visual capabilities of humans. They may also help us understand the visual processing that occurs in humans.</span></p><br> <p>            Last Modified: 06/22/2021<br>      Modified by: Todd&nbsp;Zickler</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The principal disciplinary field of the project is computer vision, which broadly speaking has the goal of enabling machines to "see". In particular, the project concerns low-level vision, which focuses on processing pixel values into useful information about 3D scene structure, including depth, motion, surface orientation, and surface color and material properties. The project's impact on this field can be described in three main categories:    1. Local predictors. These are computational units that can process the image data within a small, local image region (a receptive field) to produce a concise description of the set of likely scene values in that region, such as the local scene shape, motion, or material properties. We discovered new and powerful local predictors for: real-time depth from defocus (Guo et al., ICCV 2017); stereo occlusion boundaries (Wang et al., CVPR 2019); lighting-invariant shape from shading (Heal et al., CVPR 2020); shape from texture (Verbin et al., CVPR 2020); shape from RGB photometric stereo (Chakrabarti and Sunkavalli, 3DV 2016); monocular depth as derivative distributions (Shao et al., NeurIPS 2016) and patch samples (Xia et al., CVPR 2020a); and surface chromaticity under mixed unknown lighting (Hui et al., CVPR 2019).    2. Globalization schemes. These are distributed, convergent algorithms for incorporating the sets of local predictions at many locations in an image into consistent global scene maps such as depth maps, material maps, or optical flow fields. Our works above in (Chakrabarti and Sunkavalli, 3DV 2016) and (Shao et al., NeurIPS 2016) included such globalization methods, while our method in (Xia et al., CVPR 2020a) was able to form global maps based not only from our local predictors but also from external sources of information. We also invented new globalization algorithms for: depth from stereo via occlusion-aware dynamic programming (Wang et al., 2017), and shape from texture (Verbin et al., CVPR 2020; Verbin et al., IEEE T-PAMI). Moreover, we explored the use of global ?non-local? image structure for applications in single image denoising (Xia and Chakrabarti; WACV 2020); burst denoising (Xia et al., CVPR 2020b); and processing of flash no-flash pairs (Xia et al., CVPR 2021).    3. End-to-end Learning. These are structured learning techniques that allow the simultaneous training of parameters for globalization processes and local predictors, via end-to-end, loss-descent learning from training data. We introduced a fast stereo method in (Yee and Chakrabarti, WACV 2020) that trained a globalization network with traditional fast local predictors, while our works in (Hui et al., CVPR 2019), (Xia et al., WACV 2020), (Xia et al., CVPR 2020b), and (Xia et al., CVPR 2021) trained local and globalization portions of the pipeline together. We invented a new scheme for end-to-end learning from a single image, where the vision task is formulated as a sort of three-player game with an equilibrium that yields both shape and material properties (Verbin and Zickler, CVPR 2020). We also introduced new learning algorithms specifically directed towards high dimensional scene-map outputs to improve stability (Neyshabur et al., arXiv:1705.07831), data efficiency (Xia and Chakrabarti; NeurIPS 2019) and memory efficiency (Chakrabarti and Moseley; NeurIPS 2019).    Together, these new forms of visual processing provide a foundation for robotic vision systems that come closer to mimicking the visual capabilities of humans. They may also help us understand the visual processing that occurs in humans.       Last Modified: 06/22/2021       Submitted by: Todd Zickler]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
