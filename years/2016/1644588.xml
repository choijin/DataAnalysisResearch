<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:   Theoretic Structures of High Dimensional Data Decomposition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>akbar sayeed</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project aims at developing an information theoretic view of the problems of dimension reduction and feature selection. The problem of feature selection is a core issue in processing high dimensional data. It is difficult from an information processing point-of-view mainly because when reducing high dimensional data into lower dimensional feature space, it is in general inevitable to incur irreversible information losses. In this work, the problem is formulated as a general lossy information processing problem. The solutions to this problem is efficient algorithms that can be used to choose informative features that are relevant universally to a family of inference tasks.&lt;br/&gt;&lt;br/&gt;The goal of a general theoretic framework to this problem is to develop systematic understanding and uniform performance comparisons to the existing wide variety of practical solutions. The main technical merit lies in a new operational meaning of information metrics, which connects a large body of research on information theory to the challenges of high dimensional data analytics. A new geometric analysis approach is used in this work, which helps to visualize the problem of feature selections, and link the problem to the well-studied concept of the Hirschfeld-Gebelein-Renyi maximal correlation.&lt;br/&gt;&lt;br/&gt;The key advantage of the proposed approach is its generality. It can be applied to any type of data, incorporate prior knowledge and side information, connect multiple platforms, follow computation and storage constraints, adapt to time-variations, etc., all based on the same theoretic principle. It is envisioned that such universality would lead to architectural changes in the area of data analysis, with a universal interface that separates the task of a data scientist, in information extraction, from the task of a specialist with domain knowledge, in collecting the data, providing the models, and interpreting the result.</AbstractNarration>
<MinAmdLetterDate>09/07/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1644588</AwardID>
<Investigator>
<FirstName>Lizhong</FirstName>
<LastName>Zheng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lizhong Zheng</PI_FULL_NAME>
<EmailAddress>lizhong@mit.edu</EmailAddress>
<PI_PHON>6174522941</PI_PHON>
<NSF_ID>000489589</NSF_ID>
<StartDate>09/07/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394307</ZipCode>
<StreetAddress><![CDATA[77 Mass Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7564</Code>
<Text>CCSS-Comms Circuits &amp; Sens Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project is to develop better understanding of some of the popular algorithms used in data analysis, from the aspect of information theory and feature selections. This is conceptually important as many processing modules of high dimensional data, including PCA, deep neural networks, as well as many applications specific preprocessing in the areas of processing image, video, natural language etc., are feature selection algorithms. It is however not always clear under what principles/criteria are features selected and what implicit assumptions are made for these algorithms to be useful. In this project, we try to develop a general theoretic framework about feature selections by quantitatively describing what information content is carried by each feature and how relevant it is, and based on that a general notion of feature quality and a fundamental performance limit for all feature selection algorithms.&nbsp;</p> <p>In this project, we formulated a new information theoretic problem which we call the &ldquo;universal feature selection&rdquo; problem: we choose features of high dimensional data to answer not one specific query but to solve a family of inference problems. We use the average of the performance over this family of potential inference problems as a measure of the quality of the features, and solve the optimization problem to find the best features. Through this process, we have the following findings:</p> <ol> <li>We found that a number of learning algorithms, including deep neural networks, are in fact numerical methods to solve this problem. This gives interpretations of the behavior and results of neural networks;</li> <li>We found that several variations of neural networks, such as regulators in the loss functions, different network connectivity structures, dropouts, can be interpreted as solving the universal feature selection problem for different family of potential tasks;</li> <li>We found that the optimal solution to the universal feature selection problem is connected to a number of well-studied problems in information theory and statistics, which connects these fields of study to the current practice of deep learning;</li> <li>We practiced designs of new learning algorithms based on our theory, which i) connects modules of different learning algorithms to jointly solve the feature selection problem, ii) utilizes external knowledge such as the regularity and structure in the data or physical laws and prior knowledge, and iii) in transfer learning applications. We tested these algorithms with some real-world datasets and obtained good performance in general. &nbsp;</li> </ol> <p>Overall, we believe the project has the intellectual merit of establishing a theoretic framework to unify the understanding and designs of a wide variety of learning algorithms. It helps to bring some well-understood results in information theory and statistic to the area of learning, which allows for more systematic design procedures and more versatile and more targeted algorithms.&nbsp;</p> <p>The project has broader impacts in that it establishes a quantitative analysis method the describe how knowledge of different forms can be combined with the information extracted directly from the data. This is a critical building block to allow machine learning to be used in different engineering fields, where utilizing the domain knowledge in data processing is a major challenge. The project also has impacts on how we understand and teach machine learning in general by introducing methods to visualize and intuit the information flows in learning algorithms.&nbsp;</p> <p>This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2018<br>      Modified by: Lizhong&nbsp;Zheng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project is to develop better understanding of some of the popular algorithms used in data analysis, from the aspect of information theory and feature selections. This is conceptually important as many processing modules of high dimensional data, including PCA, deep neural networks, as well as many applications specific preprocessing in the areas of processing image, video, natural language etc., are feature selection algorithms. It is however not always clear under what principles/criteria are features selected and what implicit assumptions are made for these algorithms to be useful. In this project, we try to develop a general theoretic framework about feature selections by quantitatively describing what information content is carried by each feature and how relevant it is, and based on that a general notion of feature quality and a fundamental performance limit for all feature selection algorithms.   In this project, we formulated a new information theoretic problem which we call the "universal feature selection" problem: we choose features of high dimensional data to answer not one specific query but to solve a family of inference problems. We use the average of the performance over this family of potential inference problems as a measure of the quality of the features, and solve the optimization problem to find the best features. Through this process, we have the following findings:  We found that a number of learning algorithms, including deep neural networks, are in fact numerical methods to solve this problem. This gives interpretations of the behavior and results of neural networks; We found that several variations of neural networks, such as regulators in the loss functions, different network connectivity structures, dropouts, can be interpreted as solving the universal feature selection problem for different family of potential tasks; We found that the optimal solution to the universal feature selection problem is connected to a number of well-studied problems in information theory and statistics, which connects these fields of study to the current practice of deep learning; We practiced designs of new learning algorithms based on our theory, which i) connects modules of different learning algorithms to jointly solve the feature selection problem, ii) utilizes external knowledge such as the regularity and structure in the data or physical laws and prior knowledge, and iii) in transfer learning applications. We tested these algorithms with some real-world datasets and obtained good performance in general.     Overall, we believe the project has the intellectual merit of establishing a theoretic framework to unify the understanding and designs of a wide variety of learning algorithms. It helps to bring some well-understood results in information theory and statistic to the area of learning, which allows for more systematic design procedures and more versatile and more targeted algorithms.   The project has broader impacts in that it establishes a quantitative analysis method the describe how knowledge of different forms can be combined with the information extracted directly from the data. This is a critical building block to allow machine learning to be used in different engineering fields, where utilizing the domain knowledge in data processing is a major challenge. The project also has impacts on how we understand and teach machine learning in general by introducing methods to visualize and intuit the information flows in learning algorithms.   This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.          Last Modified: 11/30/2018       Submitted by: Lizhong Zheng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
