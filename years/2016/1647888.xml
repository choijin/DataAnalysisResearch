<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  GHz Time-of-Flight Depth Sensing for Robotic Bin Picking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/15/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>224844.00</AwardTotalIntnAmount>
<AwardAmount>224844</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial impact of this project will be to create a depth vision system that accelerates&lt;br/&gt;the transition to intelligent automation. The intelligent automation movement is driven by the opportunity&lt;br/&gt;to increase human productivity, and it can be seen in all domains: from Siemen?s self-organizing&lt;br/&gt;manufacturing plants to Amazon?s proposed delivery drones. This project aims to improve robotic vision,&lt;br/&gt;which is currently a limitation to greater autonomy. The research uses innovations in semiconductor&lt;br/&gt;physics to create time-of-flight depth cameras that reach new levels of spatial resolution and fidelity. These&lt;br/&gt;cameras, in conjunction with rapidly evolving software that uses depth data to model the physical world,&lt;br/&gt;give robotic systems the human-like abilities they need to perform more complicated tasks in dynamic&lt;br/&gt;environments. Robotics will become increasingly effective at the navigation, material handling, and object&lt;br/&gt;manipulation tasks that are the basis of so many manufacturing and logistics processes. The project&lt;br/&gt;outcomes will advance the state-of-the-art in depth vision technology, and provide a prototype camera that&lt;br/&gt;can be used to test a variety of challenging object recognition tasks.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project addresses an important limitation in depth&lt;br/&gt;cameras that use the phase delay time-of-flight (TOF) method. Existing TOF cameras use ?pixel-level&lt;br/&gt;demodulation?: specially designed photodetector arrays where each pixel performs both light detection and&lt;br/&gt;demodulation. This design limits the modulation frequency, and thus depth accuracy, because the high&lt;br/&gt;frequency signals create excessive noise and parasitic capacitances. The authors propose an alternative&lt;br/&gt;?optical demodulation? design wherein light is demodulated by a single fast optical shutter, and then&lt;br/&gt;collected by a separate conventional image sensor. By separating light modulation from light detection,&lt;br/&gt;parasitic capacitances are eliminated and it is possible to reach high frequencies with acceptable power&lt;br/&gt;consumption and noise. The research program will develop new optical shutters that demonstrate these&lt;br/&gt;characteristics, and integrate the shutters into a functioning prototype depth camera. Success will show that&lt;br/&gt;TOF cameras can be produced with substantially higher spatial resolution and design flexibility.</AbstractNarration>
<MinAmdLetterDate>12/07/2016</MinAmdLetterDate>
<MaxAmdLetterDate>03/07/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1647888</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Kohoutek</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John Kohoutek</PI_FULL_NAME>
<EmailAddress>jack@starsight.xyz</EmailAddress>
<PI_PHON>3124974971</PI_PHON>
<NSF_ID>000725528</NSF_ID>
<StartDate>12/07/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>StarSight Inc.</Name>
<CityName>Wilmette</CityName>
<ZipCode>600912437</ZipCode>
<PhoneNumber>3124974971</PhoneNumber>
<StreetAddress>1606 Spencer Avenue</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079820939</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>STARSIGHT, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[StarSight Inc.]]></Name>
<CityName>Evanston</CityName>
<StateCode>IL</StateCode>
<ZipCode>602014431</ZipCode>
<StreetAddress><![CDATA[820 Davis Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8034</Code>
<Text>Hardware Components</Text>
</ProgramReference>
<ProgramReference>
<Code>8035</Code>
<Text>Hardware Devices</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~224844</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many automated and robotic systems would benefit greatly from improved 3D vision. &nbsp;Broadly, these include self-driving cars, augmented reality headsets, and factory automation. &nbsp;In factory automation, an attractive application is random automated picking, where a robotic arm picks items out of a disordered collection and places it in a package, kit, or a parts-feeding line.&nbsp; Today the economic viability of this automated bin picking is challenged by the precision, capture time, and cost of existing 3D cameras.&nbsp; We proposed to design a high-performance time-of-flight 3D camera which enables automated bin picking in a wide range of applications.&nbsp;</p> <p>Phase delay time-of-flight imaging works by measuring the phase offset between amplitude modulated light at an illumination source, and the same light reflected back to a photodetector.&nbsp; The innovative hardware we proposed for this project was a monolithic quantum well structure that acts as a high-speed shutter to modulate light at a given wavelength in free space. These properties allow it to be used with off-the-shelf image sensors to demodulate incoming light from a scene that has been illuminated by a laser modulated at high frequency, thereby creating high resolution, low-cost time-of-flight cameras.&nbsp; During the project, we designed, fabricated, and tested the innovative modulator.</p><br> <p>            Last Modified: 11/20/2018<br>      Modified by: John&nbsp;Kohoutek</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many automated and robotic systems would benefit greatly from improved 3D vision.  Broadly, these include self-driving cars, augmented reality headsets, and factory automation.  In factory automation, an attractive application is random automated picking, where a robotic arm picks items out of a disordered collection and places it in a package, kit, or a parts-feeding line.  Today the economic viability of this automated bin picking is challenged by the precision, capture time, and cost of existing 3D cameras.  We proposed to design a high-performance time-of-flight 3D camera which enables automated bin picking in a wide range of applications.   Phase delay time-of-flight imaging works by measuring the phase offset between amplitude modulated light at an illumination source, and the same light reflected back to a photodetector.  The innovative hardware we proposed for this project was a monolithic quantum well structure that acts as a high-speed shutter to modulate light at a given wavelength in free space. These properties allow it to be used with off-the-shelf image sensors to demodulate incoming light from a scene that has been illuminated by a laser modulated at high frequency, thereby creating high resolution, low-cost time-of-flight cameras.  During the project, we designed, fabricated, and tested the innovative modulator.       Last Modified: 11/20/2018       Submitted by: John Kohoutek]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
