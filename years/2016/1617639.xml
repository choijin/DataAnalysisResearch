<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: High Confidence, Efficient Learning Under Rich Task Specifications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>470000.00</AwardTotalIntnAmount>
<AwardAmount>470000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Artificially intelligent machines such as autonomous vehicles and personal robots are poised to contribute in  many economic sectors, but cannot be deployed on a large scale without measurable confidence that they will operate correctly.  This is especially true for safety-critical systems in which humans could be injured or infrastructure could be damaged by incorrect behavior.  The proposed research will address this key issue by developing methods that allow intelligent agents to learn to perform challenging tasks and adapt to new situations, while simultaneously providing strong guarantees of correctness and safety.  Once deployed, these future robotic systems will have broad impacts on society ranging from automating small manufacturing to giving new freedom to disabled and elderly populations through safe and personalized in-home care.  The proposed robotics applications will additionally create opportunities for interactive educational K-12 programs to encourage interest in STEM areas, as well as undergraduate and graduate education.&lt;br/&gt;&lt;br/&gt;Towards these goals, the proposed work focuses on three primary research thrusts: 1) We will design safe learning algorithms that provide theoretical probabilistic satisfaction and data efficiency guarantees over both the expected reward of a policy and its correctness with respect to a high-level specification. 2) In order to account for the gap between theoretical and practical efficiency in learning, we will develop model-based and model-free off-policy evaluation methods that leverage active sampling strategies and bootstrapping to achieve practical efficiency. 3) We will develop hybrid techniques that combine and amplify the advantages of both strong theoretical and efficient practical guarantees.  The merit of the proposed algorithms will be systematically evaluated on complex, real-world problems in robotic reconfigurable manufacturing that require learning optimized, yet safe policies with a high degree of confidence in settings with low-to-medium quantities of available data.</AbstractNarration>
<MinAmdLetterDate>06/10/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617639</AwardID>
<Investigator>
<FirstName>Scott</FirstName>
<LastName>Niekum</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Scott D Niekum</PI_FULL_NAME>
<EmailAddress>sniekum@cs.utexas.edu</EmailAddress>
<PI_PHON>5122327471</PI_PHON>
<NSF_ID>000663218</NSF_ID>
<StartDate>06/10/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ufuk</FirstName>
<LastName>Topcu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ufuk Topcu</PI_FULL_NAME>
<EmailAddress>utopcu@utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000690245</NSF_ID>
<StartDate>06/10/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121757</ZipCode>
<StreetAddress><![CDATA[2317 Speedway, D9500]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~470000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Machine learning has enjoyed much success in recent years for  training autonomous systems, but the safety of such systems is not  typically guaranteed. Without such guarantees, whether they be  probabilistic or formal, it is unlikely that learning systems will be  delpoyable in many real-world problems. For example, society will not  accept the widespread usage of self-driving cars if their performance  cannot be guaranteed with high probability to be safe and correct.</p> <p>This project developed methods that provide both probabilistic and  formal guarantees of safety for reinforcement learning and imitation  learning systems, while also achieving state-of-the-art performance in  many domains. Notably, one particular piece of work that resulted from  this research is the first to be able to provide strong safety and  correctness guarantees when learning from human demonstrations without  direct knowledge of the human's objective. On the formal methods side,  one notable result is the first method for probably approximately  correct (PAC) learning in stochastic games with independent quantitative  and qualitative objectives. Furthermore, we examined how to expedite  learning speed and safety analysis by transfering knowledge from one  task to another, instead of the standard paradigm of learning every task  from scratch.</p> <p>Together, our results build toward a future in  which autonomous agents such as self-driving cars can learn from humans  and practice, while efficiently providing safety and performance  guarantees.</p><br> <p>            Last Modified: 11/06/2020<br>      Modified by: Scott&nbsp;D&nbsp;Niekum</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Machine learning has enjoyed much success in recent years for  training autonomous systems, but the safety of such systems is not  typically guaranteed. Without such guarantees, whether they be  probabilistic or formal, it is unlikely that learning systems will be  delpoyable in many real-world problems. For example, society will not  accept the widespread usage of self-driving cars if their performance  cannot be guaranteed with high probability to be safe and correct.  This project developed methods that provide both probabilistic and  formal guarantees of safety for reinforcement learning and imitation  learning systems, while also achieving state-of-the-art performance in  many domains. Notably, one particular piece of work that resulted from  this research is the first to be able to provide strong safety and  correctness guarantees when learning from human demonstrations without  direct knowledge of the human's objective. On the formal methods side,  one notable result is the first method for probably approximately  correct (PAC) learning in stochastic games with independent quantitative  and qualitative objectives. Furthermore, we examined how to expedite  learning speed and safety analysis by transfering knowledge from one  task to another, instead of the standard paradigm of learning every task  from scratch.  Together, our results build toward a future in  which autonomous agents such as self-driving cars can learn from humans  and practice, while efficiently providing safety and performance  guarantees.       Last Modified: 11/06/2020       Submitted by: Scott D Niekum]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
