<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>High-Performance, High-Level Tools for Statistical Inference and Unsupervised Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2016</AwardEffectiveDate>
<AwardExpirationDate>12/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>480000.00</AwardTotalIntnAmount>
<AwardAmount>480000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Using the "Julia" language for scientific computing developed at MIT, the UC Davis, MIT, and Julia Computing, Inc. teams funded by this project will extend the Julia language and runtime to utilize massively-parallel graphics processing units (GPUs) as first-class processors for scientific computing. Julia offers the twin advantages of straightforward, high-level programmability as well as excellent performance; adding GPU capability within Julia opens the door to even greater performance. The team will use Julia and its new GPU capabilities to address emerging important problems in statistical inference and unsupervised learning, an application area that aims to draw useful conclusions from massive amounts of data. Using a high-level, high-performance language such as Julia will allow non-computer-science experts to address these important problems.&lt;br/&gt;&lt;br/&gt;The project team brings together three threads of expertise to address the challenge of delivering best-of-breed performance from a high-level language in the context of the important application domain of statistical inference and unsupervised learning: (1) application experts in this domain; (2) the designers of the programming language Julia, which allows its users to express their ideas in high-level abstractions that are natural to statisticians and mathematicians; and (3) parallel computing experts, who will develop the new support within Julia to target high-performance GPUs as first-class processors. The major outcome of this project will be a significantly enhanced Julia language and runtime that will deliver both high-level programmability, targeted at scientists who are not parallel computing experts, and best-of-breed performance.</AbstractNarration>
<MinAmdLetterDate>09/13/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/16/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1622501</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Edelman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan Edelman</PI_FULL_NAME>
<EmailAddress>EDELMAN@MATH.MIT.EDU</EmailAddress>
<PI_PHON>6172536035</PI_PHON>
<NSF_ID>000142969</NSF_ID>
<StartDate>09/13/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Fisher</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John W Fisher</PI_FULL_NAME>
<EmailAddress>fisher@csail.mit.edu</EmailAddress>
<PI_PHON>6172530788</PI_PHON>
<NSF_ID>000483362</NSF_ID>
<StartDate>09/13/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Owens</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John D Owens</PI_FULL_NAME>
<EmailAddress>jowens@ece.ucdavis.edu</EmailAddress>
<PI_PHON>5307544289</PI_PHON>
<NSF_ID>000377403</NSF_ID>
<StartDate>09/13/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jeff</FirstName>
<LastName>Bezanson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeff Bezanson</PI_FULL_NAME>
<EmailAddress>alan@juliacomputing.com</EmailAddress>
<PI_PHON>6172017055</PI_PHON>
<NSF_ID>000687579</NSF_ID>
<StartDate>09/13/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Davis</Name>
<CityName>Davis</CityName>
<ZipCode>956186134</ZipCode>
<PhoneNumber>5307547700</PhoneNumber>
<StreetAddress>OR/Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1850 Research Park Dr., Ste 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047120084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, DAVIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Davis]]></Name>
<CityName>Davis</CityName>
<StateCode>CA</StateCode>
<ZipCode>956165270</ZipCode>
<StreetAddress><![CDATA[One Shields Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramElement>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~164612</FUND_OBLG>
<FUND_OBLG>2017~156667</FUND_OBLG>
<FUND_OBLG>2018~158721</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong id="docs-internal-guid-a1444684-7fff-bc8d-8d4d-441585bbe5b3" style="font-weight: normal;"> </strong></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><strong id="docs-internal-guid-a1444684-7fff-bc8d-8d4d-441585bbe5b3" style="font-weight: normal;"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Our work targeted the design, implementation, and use of building blocks for emerging machine learning applications, with the primary goal of making our primitives both high-performance and highly programmable. In this effort we used the Julia programming language, a relatively new language co-created by two of the principal investigators on this project. Julia has significant momentum in science and industry and is the ideal substrate for our twin goals of performance and portability.</span></strong></p> <p><strong id="docs-internal-guid-a1444684-7fff-bc8d-8d4d-441585bbe5b3" style="font-weight: normal;"> <br /> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">One major outcome is an open-source Julia library for unsupervised learning in unstructured data, specifically for Dirichlet and Hierarchical Direchlet process models. This library prioritizes scalability to large computers and large datasets, using Julia&rsquo;s support for parallel and distributed abstractions. The work involved both advances in techniques implemented inside this library as well as open-source development of the library for use by other researchers and professionals.</span></p> <br /> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The advanced algorithms included in this library include implementations of DP-means, collapsed Gibbs inference for Dirichlet Process Mixture Models (DPMM) and Hierarchical Dirichlet Process Mixture Models (HDPMM), quasi-collapsed Gibbs samplers, Direct Gibbs samplers, and finally parallel split/merge MCMC inference for DPMMs and HDPMMs.&nbsp;</span></p> <br /> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">A second major outcome is the development of a Julia compiler framework (&ldquo;Tulpar&rdquo;) for implementing embedded domain-specific languages within Julia. The primary use for this library is for developers to create their own high-performance domain-specific languages with minimal effort, languages that take advantage of Julia&rsquo;s powerful language and libraries. We expect it will be useful across a variety of application domains, have currently implemented image-processing and database-query languages in it, and are preparing an open-source release.</span></p> <br /> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Tulpar is based on the lightweight modular staging approach. Its features include a modular design for scheduling generic functions and fine-grained cooperative functional composition. The framework has three main layers: an LMS layer (major challenges: virtualizing Julia&rsquo;s non-function-call constructs, leveraging and enhancing Julia&rsquo;s type system for semi-automatic local binding-time analysis, and enabling polymorphic embedding with a trait system); a DSL layer (major challenges: designing the intermediate representation, DSL op-specific semantics, an effect tracking system, and an optimizer API); and an application layer (major challenge: debugging).</span></p> </strong><br class="Apple-interchange-newline" /></p> <p>&nbsp;</p><br> <p>            Last Modified: 04/07/2021<br>      Modified by: John&nbsp;D&nbsp;Owens</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Our work targeted the design, implementation, and use of building blocks for emerging machine learning applications, with the primary goal of making our primitives both high-performance and highly programmable. In this effort we used the Julia programming language, a relatively new language co-created by two of the principal investigators on this project. Julia has significant momentum in science and industry and is the ideal substrate for our twin goals of performance and portability.     One major outcome is an open-source Julia library for unsupervised learning in unstructured data, specifically for Dirichlet and Hierarchical Direchlet process models. This library prioritizes scalability to large computers and large datasets, using Julia’s support for parallel and distributed abstractions. The work involved both advances in techniques implemented inside this library as well as open-source development of the library for use by other researchers and professionals.   The advanced algorithms included in this library include implementations of DP-means, collapsed Gibbs inference for Dirichlet Process Mixture Models (DPMM) and Hierarchical Dirichlet Process Mixture Models (HDPMM), quasi-collapsed Gibbs samplers, Direct Gibbs samplers, and finally parallel split/merge MCMC inference for DPMMs and HDPMMs.    A second major outcome is the development of a Julia compiler framework ("Tulpar") for implementing embedded domain-specific languages within Julia. The primary use for this library is for developers to create their own high-performance domain-specific languages with minimal effort, languages that take advantage of Julia’s powerful language and libraries. We expect it will be useful across a variety of application domains, have currently implemented image-processing and database-query languages in it, and are preparing an open-source release.   Tulpar is based on the lightweight modular staging approach. Its features include a modular design for scheduling generic functions and fine-grained cooperative functional composition. The framework has three main layers: an LMS layer (major challenges: virtualizing Julia’s non-function-call constructs, leveraging and enhancing Julia’s type system for semi-automatic local binding-time analysis, and enabling polymorphic embedding with a trait system); a DSL layer (major challenges: designing the intermediate representation, DSL op-specific semantics, an effect tracking system, and an optimizer API); and an application layer (major challenge: debugging).           Last Modified: 04/07/2021       Submitted by: John D Owens]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
