<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Sketching Geometry and Physics Informed Inference for Mobile Robot Manipulation in Cluttered Scenes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>286434.00</AwardTotalIntnAmount>
<AwardAmount>286434</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to improve the ability of robots to manipulate and interact with objects, such as when assisting people to support their daily activities.  The key idea is that people can provide robots with important information about their environment and the objects within their environment. In particular, people can use their cognitive skills to name objects, provide an understanding of the geometrical structure of objects, and describe an object's behavior in relation to other objects. Specifically, the project will develop a natural user interface that enables people to provide such information by drawing and sketching on top of the robot's view of the world.  Physical simulation will then be used to fill in the missing gaps needed for a robot to complete autonomous manipulation tasks. Thus, the project aims to combine object sketching and physical simulation to better support mobile manipulation tasks as well as learn to perform new manipulation tasks when encountered.  The project will support a "Put That There" task, where a user can simply give high-level manipulation commands, with the robot filling in the details necessary to complete the task in a cluttered environment.&lt;br/&gt;&lt;br/&gt;This project aims to improve goal-directed dexterous robotic manipulation in cluttered and unstructured environments through sketching and physical simulation. Robots operating in human environments face considerable uncertainty in perception due to physical contact and occlusions between objects. This project will address such perceptual uncertainty by combining methods for probabilistic inference with natural sketch-based interfaces to extract, label, and automatically infer the geometry, pose, and behavior of objects in complicated scenes.  From a human usability perspective, the project addresses how to best create a sketching language and interfaces for intuitive human-in-the-loop extraction of object geometries and behavior from robot sensing.  The planned exploration into sketching methods will also explore what underlying representations, raw point clouds, RGB images and video, or RGBD images will be most conducive to supporting accurate geometry extraction and grasp location identification.  Given sketched objects, the project will develop probabilistic physically plausible methods for scene estimation that will enable perception for manipulation in cluttered environments.  These methods build upon advances in physical simulation to constrain scene estimates to only plausible configurations to both improve estimation accuracy and enable computational tractability.  The project will also develop a "Put That There" testbed using a tablet-based web application to support exploration of these concepts as well as act as user studies to evaluate geometry extraction accuracy and the robustness of physics-based scene estimation algorithms.</AbstractNarration>
<MinAmdLetterDate>08/10/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1638060</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>LaViola</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>Joseph J LaViola</PI_FULL_NAME>
<EmailAddress>jjl@cs.ucf.edu</EmailAddress>
<PI_PHON>4078822285</PI_PHON>
<NSF_ID>000483537</NSF_ID>
<StartDate>08/10/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>The University of Central Florida Board of Trustees</Name>
<CityName>Orlando</CityName>
<ZipCode>328168005</ZipCode>
<PhoneNumber>4078230387</PhoneNumber>
<StreetAddress>4000 CNTRL FLORIDA BLVD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>150805653</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Central Florida]]></Name>
<CityName>Orlando</CityName>
<StateCode>FL</StateCode>
<ZipCode>328263252</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~286434</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The focus of this project was the development of human-in-the-loop strategies to support mobile robot manipulation tasks. These human-in-the-loop strategies include the extracting of 3D geometries from point clouds and defining object behaviors which are then used as part of path planning algorithms that are the basis for robotic manipulation of objects in cluttered scenes. We focused on the development of sketch-based interfaces that let the user provide a minimal amount of input for 3D geometry extraction and defining of object behaviors.&nbsp;</p> <p>One of the major contributions of our work was the development of GemSketch, a sketch-based interface for extracting 3D geometries from point clouds that a robot provides using RGBD cameras.&nbsp; With GemSketch, users simply sketch the silhouettes&nbsp;of 3D objects from images and the system automatically extracts the 3D geometry from the corresponding point cloud.&nbsp;&nbsp;Leveraging the user's perceptual understanding of what an object looks like, GemSketch is capable of extracting accurate models, even in the presence of occlusion, clutter or incomplete point cloud data, while preserving the original object's details and scale.&nbsp; These extracted 3D geometries can then be sent to a robot's path planner to compute the required trajectories for mobile manipulation.&nbsp;&nbsp;</p> <p>A second contribution was the development of a sketch-based interface that let users define various object behaviors and their relationship to other objects.&nbsp; Using the GemSketch framework, users could define how objects should move in 3D space based on the images obtained from a robot's vision system.&nbsp; For example, users could sketch out an extruding line from a desk drawer indicating that the drawer geometry could move outward along that line or draw a line near the hinges of a door to indicate the door could be opened perpendicular to the sketched line.&nbsp; These object behavior definitions could then be sent to a robot's path planning routine to assist it in developing mobile manipulation strategies.&nbsp;</p> <p>A third contribution from this project was the development of new methods for sketch and gesture recognition that are fundamental to the sketch-based interfaces we want to use in the human-in-the-loop strategies for assisting a robot with its mobile manipulation tasks.&nbsp; We developed a novel sketch recognition system called Jackknife, which can perform accurate recognition using a variety of different inputs.&nbsp; Jackknife using dynamic time warping to robustly recognize different sketches and gestures with only a few examples of the gestures for training.&nbsp; In addition, we also developed Machete, a system for automatically segmenting a sketch or gesture sequence so Jackknife can accurately recognize the segmented gestures.&nbsp; These tools work hand in hand to complement a sketch-based interface that can be used to extract 3D geometries and establish object behaviors based on what the robot sees in the phyiscal environment.&nbsp;&nbsp;</p> <p>We believe that the sketch-based interfaces and tools we have developed in this project push the state of the art in providing human-in-the loop- methods for supporting a robot with the necessary information to perform mobile manipulations in cluttered scenes.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/16/2020<br>      Modified by: Joseph&nbsp;J&nbsp;Laviola</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The focus of this project was the development of human-in-the-loop strategies to support mobile robot manipulation tasks. These human-in-the-loop strategies include the extracting of 3D geometries from point clouds and defining object behaviors which are then used as part of path planning algorithms that are the basis for robotic manipulation of objects in cluttered scenes. We focused on the development of sketch-based interfaces that let the user provide a minimal amount of input for 3D geometry extraction and defining of object behaviors.   One of the major contributions of our work was the development of GemSketch, a sketch-based interface for extracting 3D geometries from point clouds that a robot provides using RGBD cameras.  With GemSketch, users simply sketch the silhouettes of 3D objects from images and the system automatically extracts the 3D geometry from the corresponding point cloud.  Leveraging the user's perceptual understanding of what an object looks like, GemSketch is capable of extracting accurate models, even in the presence of occlusion, clutter or incomplete point cloud data, while preserving the original object's details and scale.  These extracted 3D geometries can then be sent to a robot's path planner to compute the required trajectories for mobile manipulation.    A second contribution was the development of a sketch-based interface that let users define various object behaviors and their relationship to other objects.  Using the GemSketch framework, users could define how objects should move in 3D space based on the images obtained from a robot's vision system.  For example, users could sketch out an extruding line from a desk drawer indicating that the drawer geometry could move outward along that line or draw a line near the hinges of a door to indicate the door could be opened perpendicular to the sketched line.  These object behavior definitions could then be sent to a robot's path planning routine to assist it in developing mobile manipulation strategies.   A third contribution from this project was the development of new methods for sketch and gesture recognition that are fundamental to the sketch-based interfaces we want to use in the human-in-the-loop strategies for assisting a robot with its mobile manipulation tasks.  We developed a novel sketch recognition system called Jackknife, which can perform accurate recognition using a variety of different inputs.  Jackknife using dynamic time warping to robustly recognize different sketches and gestures with only a few examples of the gestures for training.  In addition, we also developed Machete, a system for automatically segmenting a sketch or gesture sequence so Jackknife can accurately recognize the segmented gestures.  These tools work hand in hand to complement a sketch-based interface that can be used to extract 3D geometries and establish object behaviors based on what the robot sees in the phyiscal environment.    We believe that the sketch-based interfaces and tools we have developed in this project push the state of the art in providing human-in-the loop- methods for supporting a robot with the necessary information to perform mobile manipulations in cluttered scenes.          Last Modified: 12/16/2020       Submitted by: Joseph J Laviola]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
