<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: SHF: Toward Sustainable Software for Science - Implementing and Assessing Systematic Testing Approaches for Scientific Software</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2017</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>152110.00</AwardTotalIntnAmount>
<AwardAmount>157006</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scientific software is widely used in science and engineering. In&lt;br/&gt;addition, results obtained from scientific software are used as evidence in&lt;br/&gt;research publications. Despite the critical usage of such software, many&lt;br/&gt;studies have pointed out a lack of systematic testing of scientific software.&lt;br/&gt;As a result, subtle program errors can remain undetected. There are numerous&lt;br/&gt;reports of subtle faults in scientific software causing losses of billions of&lt;br/&gt;dollars and the withdrawal of scientific publications. This research aims to develop&lt;br/&gt;automated techniques for test oracle creation, test case selection, and develop&lt;br/&gt;methods for test oracle prioritization targeting scientific software. The intellectual&lt;br/&gt;merits of this research are the following: (1) It advances the understanding of&lt;br/&gt;the scientific software development process and investigates methods to&lt;br/&gt;incorporate systematic software testing activities into scientific software&lt;br/&gt;development without interfering with the scientific inquiry, (2) It forges new&lt;br/&gt;approaches to develop automated test oracles for programs that produce complex&lt;br/&gt;outputs and for programs that produce outputs that are previously unknown, (3)&lt;br/&gt;It develops new metrics to measure the effectiveness of partial test oracles&lt;br/&gt;and uses them for test oracle prioritization, (4) It extends the boundaries of&lt;br/&gt;current test case selection to effectively work with partial or approximate&lt;br/&gt;test oracles. The project's broader significance and importance are (1)&lt;br/&gt;produces a publicly available, easy to use testing tool that can be incorporated&lt;br/&gt;into the scientific software development culture such that the testing&lt;br/&gt;activities will not interfere with ?doing science,? (2) recruits Native Americans&lt;br/&gt;and women into computer science research, (3) develops a new higher level&lt;br/&gt;undergraduate course titled ?Software development methods for Scientists?&lt;br/&gt;targeting senior level undergraduate students in non-CS disciplines.&lt;br/&gt;&lt;br/&gt;This project develops METtester: an automated testing framework that can effectively&lt;br/&gt;test scientific software. This testing framework analyzes the source code of&lt;br/&gt;the program under test and utilizes machine learning techniques in order to&lt;br/&gt;identify suitable test oracles called metamorphic relations (MRs). Then, it automatically&lt;br/&gt;generates effective test cases to conduct automated testing based on the&lt;br/&gt;identified MRs using a mutation based approach. After that, it creates a&lt;br/&gt;prioritized order of MRs to be used with testing in order to identify faults as&lt;br/&gt;early as possible during the testing process. Finally, METtester conducts&lt;br/&gt;testing on the program under test using the prioritized order of MRs with the generated&lt;br/&gt;test cases.</AbstractNarration>
<MinAmdLetterDate>02/21/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/20/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1656877</AwardID>
<Investigator>
<FirstName>Upulee</FirstName>
<LastName>Kanewala</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Upulee Kanewala</PI_FULL_NAME>
<EmailAddress>upulee.kanewala@unf.edu</EmailAddress>
<PI_PHON>9702864153</PI_PHON>
<NSF_ID>000706102</NSF_ID>
<StartDate>02/21/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Montana State University</Name>
<CityName>BOZEMAN</CityName>
<ZipCode>597172470</ZipCode>
<PhoneNumber>4069942381</PhoneNumber>
<StreetAddress>328 MONTANA HALL</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Montana</StateName>
<StateCode>MT</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MT00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>625447982</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MONTANA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079602596</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Montana State University]]></Name>
<CityName>Bozeman</CityName>
<StateCode>MT</StateCode>
<ZipCode>597172470</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Montana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MT00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~157006</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Scientific software is widely used in science and engineering. Such software plays a vital role in critical decision-making in fields such as the nuclear industry, medicine, and the military. In addition, results obtained from scientific software are used as evidence in research publications. Despite the critical usage of such software, many studies have pointed out a lack of systematic testing of scientific software. As a result, subtle program errors can remain undetected. These subtle errors can produce seemingly correct outputs without causing the program to crash. There are numerous reports of subtle faults in scientific software causing losses of billions of dollars and the withdrawal of scientific publications. One of the greatest challenges faced when testing scientific software is the oracle problem. Systematic automated testing requires a test oracle to check whether the outputs produced by test cases are correct according to the expected behavior of the program. But most scientific software is written to find answers that are previously unknown. Therefore, test oracles do not exist, or it is practically difficult to implement them for these programs.</p> <p>Consequently, testing techniques that depend on a perfect test oracle cannot be used for testing most scientific software. Up to now, software testing research has not provided effective solutions for the challenges presented by scientific software. This project aimed to address these challenges by developing automated techniques for test oracle creation, test case selection, and developing methods for test oracle prioritization. Further, these developed techniques are implemented in a publicly available, easy-to-use testing tool so that they can be incorporated into the scientific software development culture such that the testing activities will not interfere with &ldquo;doing science.&rdquo;</p> <p>Our results show that systematic software testing, specifically metamorphic testing combined with the novel approaches developed in this project, can be incorporated into scientific software development. Specifically, our work shows that</p> <ul> <li>Machine learning methods such as supervised learning and semi-supervised learning combined with text mining can be used to predict metamorphic relations. These predicted metamorphic relations allow determining test cases that have passed or failed when testing scientific software. Thus, these predicted metamorphic relations allow conducting automated testing on programs that produce complex outputs and programs that produce previously unknown outputs.</li> <li>The two systematic source test case generation approaches that we developed can increase the fault-detection effectiveness of metamorphic testing compared to the state-of-the-art approach, which is random source test case generation.</li> <li>Prioritizing metamorphic relations using fault detection information on previous versions of the software will save resources during regression testing of scientific software.</li> </ul> <p>&nbsp;</p> <p>In addition, the developed techniques mentioned above are implemented in METtester, a publicly available metamorphic testing tool, so that they can be utilized during scientific software development activities.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/21/2021<br>      Modified by: Upulee&nbsp;Kanewala</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Scientific software is widely used in science and engineering. Such software plays a vital role in critical decision-making in fields such as the nuclear industry, medicine, and the military. In addition, results obtained from scientific software are used as evidence in research publications. Despite the critical usage of such software, many studies have pointed out a lack of systematic testing of scientific software. As a result, subtle program errors can remain undetected. These subtle errors can produce seemingly correct outputs without causing the program to crash. There are numerous reports of subtle faults in scientific software causing losses of billions of dollars and the withdrawal of scientific publications. One of the greatest challenges faced when testing scientific software is the oracle problem. Systematic automated testing requires a test oracle to check whether the outputs produced by test cases are correct according to the expected behavior of the program. But most scientific software is written to find answers that are previously unknown. Therefore, test oracles do not exist, or it is practically difficult to implement them for these programs.  Consequently, testing techniques that depend on a perfect test oracle cannot be used for testing most scientific software. Up to now, software testing research has not provided effective solutions for the challenges presented by scientific software. This project aimed to address these challenges by developing automated techniques for test oracle creation, test case selection, and developing methods for test oracle prioritization. Further, these developed techniques are implemented in a publicly available, easy-to-use testing tool so that they can be incorporated into the scientific software development culture such that the testing activities will not interfere with "doing science."  Our results show that systematic software testing, specifically metamorphic testing combined with the novel approaches developed in this project, can be incorporated into scientific software development. Specifically, our work shows that  Machine learning methods such as supervised learning and semi-supervised learning combined with text mining can be used to predict metamorphic relations. These predicted metamorphic relations allow determining test cases that have passed or failed when testing scientific software. Thus, these predicted metamorphic relations allow conducting automated testing on programs that produce complex outputs and programs that produce previously unknown outputs. The two systematic source test case generation approaches that we developed can increase the fault-detection effectiveness of metamorphic testing compared to the state-of-the-art approach, which is random source test case generation. Prioritizing metamorphic relations using fault detection information on previous versions of the software will save resources during regression testing of scientific software.      In addition, the developed techniques mentioned above are implemented in METtester, a publicly available metamorphic testing tool, so that they can be utilized during scientific software development activities.                Last Modified: 06/21/2021       Submitted by: Upulee Kanewala]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
