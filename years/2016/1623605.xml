<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EXP: Modeling Perceptual Fluency with Visual Representations in an Intelligent Tutoring System for Undergraduate Chemistry</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>540396.00</AwardTotalIntnAmount>
<AwardAmount>540396</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hector Munoz-Avila</SignBlockName>
<PO_EMAI>hmunoz@nsf.gov</PO_EMAI>
<PO_PHON>7032924481</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects design and build new kinds of learning technologies in order to explore their viability, to understand the challenges to using them effectively, and to study their potential for fostering learning. This EXP project aims to help students become visually fluent with visual representations (similar to becoming fluent in a second language). Instructors often use visuals to help students learn (e.g., pie charts of fractions, or ball-and-stick models of chemical molecules) and assume that students can quickly discern relevant information (e.g., whether or not two visuals show the same chemical) once that visual representation has been introduced. But comprehension is not the same as fluency -- students still expend significant mental effort and time interpreting even visuals that they understand conceptually, and the resulting cognitive load can cause them to miss other important information that instructors are imparting. To help improve student fluency with visuals, a series of experiments with undergraduate students and chemistry professors will investigate which visual features they pay attention to and use sophisticated statistical methods to devise example sequences that will most efficiently help students learn to pay attention to relevant visual features. Based on this research, the project team will develop a visual fluency training that will be incorporated into an existing, successful online learning technology for chemistry. The potential educational impact will not be limited to chemistry instruction: given the pervasiveness of visual representations in STEM fields and the number of students who struggle with rapid processing of those visuals, the products of this research could be integrated into other educational technologies.&lt;br/&gt;&lt;br/&gt;The PIs will develop a methodology for cognitive modeling of perceptual learning processes that can create adaptive support for perceptual learning tasks. The research will combine machine learning with educational psychology experiments using an Intelligent Tutoring System (ITS) for undergraduate chemistry. In Phase 1, metric learning will assess which visual features of representations novice students and chemistry experts focus on. Applying metric learning to a novice-expert experiment will establish a skill model of student perceptions and perceptual learning goals for the ITS. In Phase 2, the team will use machine learning to develop a cognitive model of perceptual learning. The team will conduct a chemistry learning experiment and apply machine learning to test cognitive models. In Phase 3, the team will use the cognitive model to reverse-engineer optimal sequences of perceptual learning tasks. An experiment will evaluate the effectiveness of these sequences, and the team will build on this analysis to create an adaptive version of perceptual learning tasks. A final experiment will evaluate whether incorporating adaptive perceptual learning tasks with conceptually focused instruction enhances learning.  Because educational technologies have traditionally focused on explicit learning processes that lead to conceptual competencies, they cannot currently assess the implicit learning processes that lead to perceptual fluency.  Combining educational psychology, cognitive science, and machine learning will yield new cognitive models that could transform the adaptive capabilities of educational technologies to support such perceptual fluency as well as other implicit forms of learning. The project will also yield next-generation computational algorithms to model human similarity judgments and to use adaptive surveying to collect data on perceptual judgments more efficiently.</AbstractNarration>
<MinAmdLetterDate>08/30/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/30/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1623605</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Nowak</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert D Nowak</PI_FULL_NAME>
<EmailAddress>rdnowak@wisc.edu</EmailAddress>
<PI_PHON>6082653914</PI_PHON>
<NSF_ID>000338270</NSF_ID>
<StartDate>08/30/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Xiaojin</FirstName>
<LastName>Zhu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaojin Zhu</PI_FULL_NAME>
<EmailAddress>jerryzhu@cs.wisc.edu</EmailAddress>
<PI_PHON>6088900129</PI_PHON>
<NSF_ID>000211108</NSF_ID>
<StartDate>08/30/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Martina</FirstName>
<LastName>Rau</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martina A Rau</PI_FULL_NAME>
<EmailAddress>marau@wisc.edu</EmailAddress>
<PI_PHON>6082620833</PI_PHON>
<NSF_ID>000656294</NSF_ID>
<StartDate>08/30/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 6401]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>161202122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>Madison</CityName>
<StateCode>WI</StateCode>
<ZipCode>537151218</ZipCode>
<StreetAddress><![CDATA[21 N Park St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<ProgramReference>
<Code>8244</Code>
<Text>EHR CL Opportunities (NSF 14-302)</Text>
</ProgramReference>
<ProgramReference>
<Code>8841</Code>
<Text>Exploration Projects</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~540396</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Instructors often use visuals to help students learn (e.g., pie charts of fractions, ball-and-stick models of chemical molecules). Often, after having explained to students how to interpret the visual representation, the instructor assumes that students can quickly see relevant information in a visual representation (e.g., whether or not two visuals show the same chemical). But for students, it takes significant mental effort and time to interpret the visuals; and this effortful process may cause them to miss relevant information from the instructor?s explanation of other important topics. The goal of this project was to help students become <em>visually fluent </em>with visual representations (similar to becoming fluent in a second language). To achieve this goal, the project took four steps:</p> <ol> <li>Develop a <em>measure</em> of      implicit visual fluency by studying which visual features undergraduate      students and chemistry experts pay attention to.</li> <li>Develop a <em>model</em> that captures students? perception      of visual representations (i.e., visual fluency) while they solve      chemistry problems, similar to how Netflix captures what movies someone      likes to watch.</li> <li>Develop an algorithmic approach to automatically optimize a      sequence of instructional problems that maximizes students? visual      fluency.</li> <li>Evaluate whether the      sequence of instructional problems enhance students?      learning of chemistry knowledge.</li> </ol> <p>For the first step of the project, the project team used a type of machine learning method that identifies which features of a visual representation a student attends to, without requiring expensive methods such as eye tracking. The method was compared against other methods, such as students? self-reports, to establish that it really measures what it is intended to measure. The team used this method to determine which visual features students do not attend to, even though they show important information. The goal of the instructional activities is for students to learn to pay attention to those features.</p> <p>For the second step, the team created a machine algorithm that models how human students learn visual fluency. The algorithm is based on what we know about how humans learn visual fluency and how they process visual images. The algorithm mimics this human behavior. It takes two images as an input, internally processes them, and outputs an answer to the question whether the two images show the same chemical molecule (i.e., yes or no). This algorithm allowed the team to test whether particular sequences of instructional problems are effective, without having to test these problems on human students. In essence, the algorithm is a testbed that emulates human learning of visual fluency.</p> <p>For the third step, we used the algorithm to find effective sequences of instructional problems. To this end, the project team gave the algorithm a pretest of its visual fluency, asked the algorithm to produce a tentative instructional sequence, and then a posttest of visual fluency. This procedure was repeated many times while the algorithm adjusts the instructional sequence to increase the gain between the pretest-vs-posttest performance.&nbsp;&nbsp; Eventually the algorithm selects the instructional sequence which leads to the highest gains in visual fluency.&nbsp; Because the algorithm was developed to emulate human learning, the sequence that led to the highest learning gains for the algorithm was hypothesized to also lead to the highest learning gains for human students. To test this hypothesis, three experiments with human students compared the instructional sequence generated by the algorithm to other instructional sequences. The hypothesis was confirmed: the instructional sequence generated by the algorithm was most effective, especially for low-performing students.</p> <p>For the fourth step, we developed, based on the results from the previous steps, adaptive instruction for visual fluency. The instruction is adaptive similar to how Netflix adaptively selects the next movie to watch, based on a model of what movies a given person likes. The adaptive instruction for visual fluency highlights visual features that students should learn to pay attention to if they make a mistake. It also adaptively provides particular instructional sequences based on an assessment of what the student still needs to learn. Both parts of the adaptive instruction were evaluated separately. One study showed that highlighting was not necessary to improve students? learning of visual fluency. We think that highlighting may detract students from noticing by themselves which visual features are important. Another study showed that adaptive selection of instructional sequences improved students? learning over non-adaptive selection.&nbsp;</p> <p>The final, effective instructional sequence was integrated into an existing, successful online learning technology for chemistry, which is available for free at <a href="https://chem.tutorshop.web.cmu.edu/">https://chem.tutorshop.web.cmu.edu/</a>.</p><br> <p>            Last Modified: 10/14/2020<br>      Modified by: Martina&nbsp;A&nbsp;Rau</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Instructors often use visuals to help students learn (e.g., pie charts of fractions, ball-and-stick models of chemical molecules). Often, after having explained to students how to interpret the visual representation, the instructor assumes that students can quickly see relevant information in a visual representation (e.g., whether or not two visuals show the same chemical). But for students, it takes significant mental effort and time to interpret the visuals; and this effortful process may cause them to miss relevant information from the instructor?s explanation of other important topics. The goal of this project was to help students become visually fluent with visual representations (similar to becoming fluent in a second language). To achieve this goal, the project took four steps:  Develop a measure of      implicit visual fluency by studying which visual features undergraduate      students and chemistry experts pay attention to. Develop a model that captures students? perception      of visual representations (i.e., visual fluency) while they solve      chemistry problems, similar to how Netflix captures what movies someone      likes to watch. Develop an algorithmic approach to automatically optimize a      sequence of instructional problems that maximizes students? visual      fluency. Evaluate whether the      sequence of instructional problems enhance students?      learning of chemistry knowledge.   For the first step of the project, the project team used a type of machine learning method that identifies which features of a visual representation a student attends to, without requiring expensive methods such as eye tracking. The method was compared against other methods, such as students? self-reports, to establish that it really measures what it is intended to measure. The team used this method to determine which visual features students do not attend to, even though they show important information. The goal of the instructional activities is for students to learn to pay attention to those features.  For the second step, the team created a machine algorithm that models how human students learn visual fluency. The algorithm is based on what we know about how humans learn visual fluency and how they process visual images. The algorithm mimics this human behavior. It takes two images as an input, internally processes them, and outputs an answer to the question whether the two images show the same chemical molecule (i.e., yes or no). This algorithm allowed the team to test whether particular sequences of instructional problems are effective, without having to test these problems on human students. In essence, the algorithm is a testbed that emulates human learning of visual fluency.  For the third step, we used the algorithm to find effective sequences of instructional problems. To this end, the project team gave the algorithm a pretest of its visual fluency, asked the algorithm to produce a tentative instructional sequence, and then a posttest of visual fluency. This procedure was repeated many times while the algorithm adjusts the instructional sequence to increase the gain between the pretest-vs-posttest performance.   Eventually the algorithm selects the instructional sequence which leads to the highest gains in visual fluency.  Because the algorithm was developed to emulate human learning, the sequence that led to the highest learning gains for the algorithm was hypothesized to also lead to the highest learning gains for human students. To test this hypothesis, three experiments with human students compared the instructional sequence generated by the algorithm to other instructional sequences. The hypothesis was confirmed: the instructional sequence generated by the algorithm was most effective, especially for low-performing students.  For the fourth step, we developed, based on the results from the previous steps, adaptive instruction for visual fluency. The instruction is adaptive similar to how Netflix adaptively selects the next movie to watch, based on a model of what movies a given person likes. The adaptive instruction for visual fluency highlights visual features that students should learn to pay attention to if they make a mistake. It also adaptively provides particular instructional sequences based on an assessment of what the student still needs to learn. Both parts of the adaptive instruction were evaluated separately. One study showed that highlighting was not necessary to improve students? learning of visual fluency. We think that highlighting may detract students from noticing by themselves which visual features are important. Another study showed that adaptive selection of instructional sequences improved students? learning over non-adaptive selection.   The final, effective instructional sequence was integrated into an existing, successful online learning technology for chemistry, which is available for free at https://chem.tutorshop.web.cmu.edu/.       Last Modified: 10/14/2020       Submitted by: Martina A Rau]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
