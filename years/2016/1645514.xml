<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: EAGER: Developing General Techniques for Tightening Bounds of the Data-Movement Complexity of Large Scale Parallel Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Enabling faster and more energy-efficient numerical simulations is critical, for example, in basic science for enabling novel scientific discoveries, or in engineering for developing revolutionary new products.  In the last decades, technology trends in computer systems have resulted in widely differing rates of improvement in computational throughput as compared to data movement performance.  With future systems, the cost of data movement through the memory hierarchy is expected to become even more dominant relative to the cost of performing arithmetic operations, both in terms of time and energy. Consequently, the data movement and communication costs of a numerical simulation become determinant factors for the time to solution and the energy consumption.  &lt;br/&gt;This research is developing novel generic techniques for tightening bounds on the data movement complexity of numerical simulations. The&lt;br/&gt;outcomes of this research are methods to derive the communication needs of numerical simulations. The ability to assess the optimality of an algorithm enables understanding of the implications of various computing platform's parameters on the performance of a numerical simulation. The PIs are developing novel generic techniques for tightening bounds on the data movement complexity of a given algorithm on a given architecture.  This work engages in novel interdisciplinary perspectives and brings together applied mathematics, theoretical computer science, data analytics and high performance computing.</AbstractNarration>
<MinAmdLetterDate>08/03/2016</MinAmdLetterDate>
<MaxAmdLetterDate>03/31/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1645514</AwardID>
<Investigator>
<FirstName>Julien</FirstName>
<LastName>Langou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Julien Langou</PI_FULL_NAME>
<EmailAddress>julien.langou@ucdenver.edu</EmailAddress>
<PI_PHON>3033151730</PI_PHON>
<NSF_ID>000069563</NSF_ID>
<StartDate>08/03/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Denver-Downtown Campus</Name>
<CityName>Aurora</CityName>
<ZipCode>800452571</ZipCode>
<PhoneNumber>3037240090</PhoneNumber>
<StreetAddress>F428, AMC Bldg 500</StreetAddress>
<StreetAddress2><![CDATA[13001 E 17th Place, Rm W1126]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>015634884</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Denver-Downtown Campus]]></Name>
<CityName>Denver</CityName>
<StateCode>CO</StateCode>
<ZipCode>802173364</ZipCode>
<StreetAddress><![CDATA[1201 Larimer Street, Suite 4000]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-c4ea231c-7fff-3d16-e7a2-b59dfd23c5e3"> <p dir="ltr"><span>Enabling faster and more energy-efficient numerical simulations is critical, for example, in basic science for enabling novel scientific discoveries, or in engineering for developing revolutionary new products. In the last decades, technology trends in computer systems have resulted in widely differing rates of improvement in computational throughput as compared to data movement performance. With future systems, the cost of data movement through the memory hierarchy is expected to become even more dominant relative to the cost of performing arithmetic operations, both in terms of time and energy. Consequently, the data movement and communication costs of a numerical simulation become determinant factors for the time to solution and the energy consumption.&nbsp;</span></p> <br /> <p dir="ltr"><span>This project developed novel generic techniques for tightening bounds on the data movement complexity of numerical simulations. Here are some of the results obtained by this project. Firstly, we derived a tight lower bound for the data movement complexity of matrix-matrix multiplication. The first known nontrivial lower bound for the data movement complexity of matrix-matrix multiplication was given by Hong and Kung in 1981, and then this lower bound got progressively improved over time. In this project, after 40 years of research, we concluded the initial work of Hong and Kung by obtaining a lower bound and a schedule which have the same data movement complexity. Therefore we established the optimality of this new lower bound and this schedule. Secondly, we managed to automate our reasoning in a tool. Our tool uses polyhedral compiler techniques and takes as input a parametric affine code and returns as output a parametric lower bound. We used our tool on 30 different well-known and important algorithms. Our tool performs remarkably well. When we compared it to schedules, we managed to prove optimality for 13 of them. For 15 of them, we proved optimality up to a constant. We note that an important side contribution is in these optimal schedules, we designed many of these optimal schedules ourselves while studying our new lower bounds. These new innovative schedules represent a significant contribution of this project. Our tool is not performing as well as we would have liked in 2 cases. Further research needs to be done in that respect. These results are overall ground-breaking and impact research in various areas. The fact that we can automate the derivation of lower bounds is extremely powerful. It is not easy to explain our proof techniques. And, even for experts, this gets tedious pretty fast. It took us two years to derive a tight bound for matrix-matrix multiplication. Now, we can use our tool on 30 algorithms in a matter of minutes. Of course, our tool learned our proof techniques and most of our time during these two years was spent developing these proof techniques. Yet the gain in productivity from our tool is absolutely not negligible. In addition to obtaining a result (a lower bound), our tool also can output a &ldquo;proof&rdquo; that is the various branches of logic that enables it to conclude its result. The proofs are quite readable and have led to new insights in these algorithms and their optimal schedules. Thirdly, we also proved new improved parameterized lower bounds for the schedule of tiled Cholesky factorization in a parallel homogeneous multicore environment. Our result is a clear improvement with respect to previous knowledge. Trivial lower bounds were known, our new results enable to better understand the intrinsic performance limit of the Cholesky factorization algorithm. Fourthly, we studied various error tolerance (bit flip) mechanisms and we showed that, contrary to popular belief, &ldquo;residual checking&rdquo; could be used for error correction in a similar way as, but with many advantages over, the more popular &ldquo;algorithm-based fault tolerance&rdquo; . This research is related to the new area of &ldquo;coded computing&rdquo;. We demonstrated that &ldquo;residual checking&rdquo; does not need to make any assumption on the maximum number of errors and, yet, &ldquo;residual checking&rdquo; enables us to decode an arbitrary number of errors.&nbsp;</span></p> <br /> <p dir="ltr"><span>A hispanic minority male PhD student was supported by this award throughout the length of the award.&nbsp;</span></p> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/04/2020<br>      Modified by: Julien&nbsp;Langou</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Enabling faster and more energy-efficient numerical simulations is critical, for example, in basic science for enabling novel scientific discoveries, or in engineering for developing revolutionary new products. In the last decades, technology trends in computer systems have resulted in widely differing rates of improvement in computational throughput as compared to data movement performance. With future systems, the cost of data movement through the memory hierarchy is expected to become even more dominant relative to the cost of performing arithmetic operations, both in terms of time and energy. Consequently, the data movement and communication costs of a numerical simulation become determinant factors for the time to solution and the energy consumption.    This project developed novel generic techniques for tightening bounds on the data movement complexity of numerical simulations. Here are some of the results obtained by this project. Firstly, we derived a tight lower bound for the data movement complexity of matrix-matrix multiplication. The first known nontrivial lower bound for the data movement complexity of matrix-matrix multiplication was given by Hong and Kung in 1981, and then this lower bound got progressively improved over time. In this project, after 40 years of research, we concluded the initial work of Hong and Kung by obtaining a lower bound and a schedule which have the same data movement complexity. Therefore we established the optimality of this new lower bound and this schedule. Secondly, we managed to automate our reasoning in a tool. Our tool uses polyhedral compiler techniques and takes as input a parametric affine code and returns as output a parametric lower bound. We used our tool on 30 different well-known and important algorithms. Our tool performs remarkably well. When we compared it to schedules, we managed to prove optimality for 13 of them. For 15 of them, we proved optimality up to a constant. We note that an important side contribution is in these optimal schedules, we designed many of these optimal schedules ourselves while studying our new lower bounds. These new innovative schedules represent a significant contribution of this project. Our tool is not performing as well as we would have liked in 2 cases. Further research needs to be done in that respect. These results are overall ground-breaking and impact research in various areas. The fact that we can automate the derivation of lower bounds is extremely powerful. It is not easy to explain our proof techniques. And, even for experts, this gets tedious pretty fast. It took us two years to derive a tight bound for matrix-matrix multiplication. Now, we can use our tool on 30 algorithms in a matter of minutes. Of course, our tool learned our proof techniques and most of our time during these two years was spent developing these proof techniques. Yet the gain in productivity from our tool is absolutely not negligible. In addition to obtaining a result (a lower bound), our tool also can output a "proof" that is the various branches of logic that enables it to conclude its result. The proofs are quite readable and have led to new insights in these algorithms and their optimal schedules. Thirdly, we also proved new improved parameterized lower bounds for the schedule of tiled Cholesky factorization in a parallel homogeneous multicore environment. Our result is a clear improvement with respect to previous knowledge. Trivial lower bounds were known, our new results enable to better understand the intrinsic performance limit of the Cholesky factorization algorithm. Fourthly, we studied various error tolerance (bit flip) mechanisms and we showed that, contrary to popular belief, "residual checking" could be used for error correction in a similar way as, but with many advantages over, the more popular "algorithm-based fault tolerance" . This research is related to the new area of "coded computing". We demonstrated that "residual checking" does not need to make any assumption on the maximum number of errors and, yet, "residual checking" enables us to decode an arbitrary number of errors.    A hispanic minority male PhD student was supported by this award throughout the length of the award.            Last Modified: 10/04/2020       Submitted by: Julien Langou]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
