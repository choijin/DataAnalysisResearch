<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Workshop on a Systematic Approach to Robustness,  Reliability, and Reproducibility in Scientific Research February 24-26, 2017 in Atlanta, GA</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>10/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>82285.00</AwardTotalIntnAmount>
<AwardAmount>82285</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03010000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>PHY</Abbreviation>
<LongName>Division Of Physics</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this workshop is to gain insight from the scientific community on aspects related to the robustness, reliability and reproducibility of the scientific practice. The workshop will explore fundamental concepts that underpin the collective understanding of how to assess the quality of science, a challenge for all scientific disciplines. The format of the workshop is designed to elicit a broad range of input from leaders in their respective fields. &lt;br/&gt;&lt;br/&gt;This workshop concerns the development of effective practices with a manifestly cross-disciplinary character, designed to improve the transparency of the scientific process, to speed discovery and innovation, and to inform policy makers and the public. The output of the workshop will include a report that summarizes the conclusions reached by the attendees, together with a roadmap for future workshops that will address specific community needs identified at this workshop.&lt;br/&gt;&lt;br/&gt;This project is supported by the Physics Division in the Directorate of Mathematical and Physical Sciences, the Division of Molecular and Cellular Biosciences in the Directorate for Biological Sciences, and the Division of Advanced Cyberinfrastructure in the Directorate for Computer &amp; Information Science &amp; Engineering.</AbstractNarration>
<MinAmdLetterDate>08/10/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1650892</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Weitz</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David A Weitz</PI_FULL_NAME>
<EmailAddress>weitz@seas.harvard.edu</EmailAddress>
<PI_PHON>6174962842</PI_PHON>
<NSF_ID>000107754</NSF_ID>
<StartDate>08/10/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia International Convention Center]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303374200</ZipCode>
<StreetAddress><![CDATA[2000 Convention Center Concourse]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1114</Code>
<Text>Cellular Dynamics and Function</Text>
</ProgramElement>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7556</Code>
<Text>CONFERENCE AND WORKSHOPS</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~82285</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The purpose of this project was to organize a focused workshop that brought together leading experts concerned about robustness, reliability and reproducibility in scientific research. This workshop was held at the Beckman Center, University of California at Irvine on February 25-26, 2017 with twenty-one scientists and engineers representing a broad range of disciplines. The goal of the workshop was to establish a framework to initiate a discussion on these topics with the broader scientific community, and to develop a consensus on how to deal with this important issue in a systematic manner.&nbsp;</p> <p>A lack of reproducibility has been widely noted in many areas of scientific research, including the physical sciences, biomedicine, computer science, and psychology. Irreproducibility can lead to lack of confidence in research reports, can widen the gap between discovery and product development, and challenges public policy-making. The concern about this reproducibility issue has spawned activities by funding agencies, journals, and private organizations to address this issue. Their efforts have included imposing new requirements and checklists which are often discipline specific, and sponsoring laboratory efforts to try to reproduce studies performed in other laboratories.</p> <p>The workshop was designed to gain input from the participants on the issue of enhancing robustness and reliability of the scientific practice. The workshop explored fundamental concepts that underpin our collective understanding of how to assess the quality of science, a challenge for all scientific disciplines.&nbsp; The objectives of the workshops were to:</p> <p>1. Develop a lexicon around reproducibility,</p> <p>2. Discuss what aspects of (ir)reproducibility are common across the scientific enterprise and propose common solutions,</p> <p>3. Suggest a roadmap for engaging the community, and</p> <p>4. Write a report (see below).</p> <p>The <em>intellectual merit</em> of the project was in the gathering of leading experts concerned with rigor, robustness and quality in research to discuss the issue in a comprehensive manner, and to develop suggestions from the working groups for the wider scientific community in a subsequent series of workshops.&nbsp;</p> <p>The <em>broader impact</em> of the project will come from the suggestions to address the reproducibility issue developed from a consensus of workshop participants designed to be transdisciplinary, to take account of state-of-the-art data sharing tools, to speed discovery and innovation, and to inform policy makers and the public.</p> <p>The conclusions of the workshop were:</p> <p>1. Variability in research results is an essential component of the scientific process. Exploration of such variabilities can shed light on previously unrecognized variables and yield improved understanding of the natural world.</p> <p>2. There are significant differences in the amount of variability in scientific results from field to field. Fields with higher levels of variability tend to be less mature, so that there are more unrecognized variables that can affect results.</p> <p>3. Scientific fields that are more mature do not suffer from problems in reproducibility and replicability. Instead variability in results is part of the natural scientific process and often leads to new discoveries.</p> <p>4. Scientific fields that are not as mature can be subject to some problems. These are best addressed through full reporting of all data. Tools for reporting and retaining scientific results have substantially improved over the past ten years. Improvements in transparency of published data also increase reliability and reproducibility.</p> <p>5. There is considerable value in communicating with the public about the nature of the scientific process,</p> <p>A product of this workshop is a report with suggestions to further engaged the scientific community, including further workshops and training opportunities; the report is publicly available at: http://www.mrsec.harvard.edu/2017NSFReliability/cw/report.php</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/28/2017<br>      Modified by: David&nbsp;A&nbsp;Weitz</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The purpose of this project was to organize a focused workshop that brought together leading experts concerned about robustness, reliability and reproducibility in scientific research. This workshop was held at the Beckman Center, University of California at Irvine on February 25-26, 2017 with twenty-one scientists and engineers representing a broad range of disciplines. The goal of the workshop was to establish a framework to initiate a discussion on these topics with the broader scientific community, and to develop a consensus on how to deal with this important issue in a systematic manner.   A lack of reproducibility has been widely noted in many areas of scientific research, including the physical sciences, biomedicine, computer science, and psychology. Irreproducibility can lead to lack of confidence in research reports, can widen the gap between discovery and product development, and challenges public policy-making. The concern about this reproducibility issue has spawned activities by funding agencies, journals, and private organizations to address this issue. Their efforts have included imposing new requirements and checklists which are often discipline specific, and sponsoring laboratory efforts to try to reproduce studies performed in other laboratories.  The workshop was designed to gain input from the participants on the issue of enhancing robustness and reliability of the scientific practice. The workshop explored fundamental concepts that underpin our collective understanding of how to assess the quality of science, a challenge for all scientific disciplines.  The objectives of the workshops were to:  1. Develop a lexicon around reproducibility,  2. Discuss what aspects of (ir)reproducibility are common across the scientific enterprise and propose common solutions,  3. Suggest a roadmap for engaging the community, and  4. Write a report (see below).  The intellectual merit of the project was in the gathering of leading experts concerned with rigor, robustness and quality in research to discuss the issue in a comprehensive manner, and to develop suggestions from the working groups for the wider scientific community in a subsequent series of workshops.   The broader impact of the project will come from the suggestions to address the reproducibility issue developed from a consensus of workshop participants designed to be transdisciplinary, to take account of state-of-the-art data sharing tools, to speed discovery and innovation, and to inform policy makers and the public.  The conclusions of the workshop were:  1. Variability in research results is an essential component of the scientific process. Exploration of such variabilities can shed light on previously unrecognized variables and yield improved understanding of the natural world.  2. There are significant differences in the amount of variability in scientific results from field to field. Fields with higher levels of variability tend to be less mature, so that there are more unrecognized variables that can affect results.  3. Scientific fields that are more mature do not suffer from problems in reproducibility and replicability. Instead variability in results is part of the natural scientific process and often leads to new discoveries.  4. Scientific fields that are not as mature can be subject to some problems. These are best addressed through full reporting of all data. Tools for reporting and retaining scientific results have substantially improved over the past ten years. Improvements in transparency of published data also increase reliability and reproducibility.  5. There is considerable value in communicating with the public about the nature of the scientific process,  A product of this workshop is a report with suggestions to further engaged the scientific community, including further workshops and training opportunities; the report is publicly available at: http://www.mrsec.harvard.edu/2017NSFReliability/cw/report.php          Last Modified: 12/28/2017       Submitted by: David A Weitz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
