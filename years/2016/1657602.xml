<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CSR: System Support for Reactive Sensor Operation for Efficiency and Performance</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2017</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>174950.00</AwardTotalIntnAmount>
<AwardAmount>182950</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>On mobile and wearable devices, many sensors provide connections between the real world and the digital realm, enabling computing access to many forms of raw data, including audio, video, inertial motion sensing, and touchscreen control. Through continuous sensing, mobile device services play a large role in personal computing, assistive technology, and many other fields. However deriving expressive input from high data rate sensors currently suffers from fundamental limitations to the efficiency and performance of handling densely pipelined streams of raw data. This is largely because sensor configuration and control in modern mobile systems induces significant system latency, preventing immediate control of sensor operation and limiting algorithmic expressiveness. &lt;br/&gt;&lt;br/&gt;This project pursues the transformation of operating system and sensor architecture to provide system support for reactive sensor operation: using a low-level process to observe and react to context, immediately executing developer-defined sensor configuration, sampling, and processing before providing data access at the application layer. This paradigm provides developers with expressive access to sensor operation while maintaining energy-efficiency and high-performance in the mobile system. Thus, the project discerns systems principles to model and design reactive programming abstractions for sensor processing and control through operating system services, system libraries, daemon processes, and sensor system architecture modifications.</AbstractNarration>
<MinAmdLetterDate>02/16/2017</MinAmdLetterDate>
<MaxAmdLetterDate>12/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657602</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>LiKamWa</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert LiKamWa</PI_FULL_NAME>
<EmailAddress>likamwa@asu.edu</EmailAddress>
<PI_PHON>4809652686</PI_PHON>
<NSF_ID>000724741</NSF_ID>
<StartDate>02/16/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<StreetAddress2><![CDATA[660 South Mill Avenue, Suite 310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>943360412</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ARIZONA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>806345658</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Arizona State University]]></Name>
<CityName>Tempe</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852816011</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~174950</FUND_OBLG>
<FUND_OBLG>2018~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Default"><span>On mobile and wearable devices, many sensors provide connections between the real world and the digital realm, enabling computing access to many forms of raw data, including audio, video, inertial motion sensing, and touchscreen control. Through continuous sensing, mobile device services play a large role in personal computing, assistive technology, and many other fields. However, deriving expressive input from high data rate sensors currently suffers from fundamental limitations to the efficiency and performance of handling densely pipelined streams of raw data. This is largely because sensor configuration and control in modern mobile systems induces significant system latency, preventing immediate control of sensor operation and limiting algorithmic expressiveness.</span></p> <p class="Default"><span>This project pursued the transformation of operating system and sensor architecture to provide system support for reactive sensor operation: using a low-level process to observe and react to context, immediately executing developer-defined sensor configuration, sampling, and processing before providing data access at the application layer. This paradigm provides developers with expressive access to sensor operation while maintaining energy-efficiency and high-performance in the mobile system.&nbsp;</span><span>&nbsp;</span></p> <p>This project characteized the issues of sensor reconfiguration latency by measuring visual sensor performance during reconfiguration operations. The project found that current systems delay frame reconfiguration by several hundreds of milliseconds, degrading user experience. The project further identifiedd that the critical bottleneck was in the operating system's management of the capture stream during reconfiguration.</p> <p>The project solved these issues through identifying and evaluating principles to modify mobile operating systems. These principles included providing strategies to reconfigure sensors in parallel with sensor capture routines, as well as reducnig repeated memory allocation patterns. Ultimatelly, for visual computing, this enables power reduction of 49% by allowing systems to capture and use lower resolution image frames rather than capturing and scaling down larger frames. This&nbsp;<span>unlocks unprecedented capabilities for mobile vision applications to dynamically reconfigure sensor resolutions to balance the energy efficiency and task accuracy tradeoff.&nbsp;</span></p> <p>The project incorporated undergradduate student in the research, wheerin she designed augmented reality application frameworks to leverage the reconfigurable system. The PI also taught a mobile augmented reality course at a high school summer camp at ASU to encourage broader participation in mobile computer systems programming and engineering.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/03/2020<br>      Modified by: Robert&nbsp;Likamwa</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[On mobile and wearable devices, many sensors provide connections between the real world and the digital realm, enabling computing access to many forms of raw data, including audio, video, inertial motion sensing, and touchscreen control. Through continuous sensing, mobile device services play a large role in personal computing, assistive technology, and many other fields. However, deriving expressive input from high data rate sensors currently suffers from fundamental limitations to the efficiency and performance of handling densely pipelined streams of raw data. This is largely because sensor configuration and control in modern mobile systems induces significant system latency, preventing immediate control of sensor operation and limiting algorithmic expressiveness. This project pursued the transformation of operating system and sensor architecture to provide system support for reactive sensor operation: using a low-level process to observe and react to context, immediately executing developer-defined sensor configuration, sampling, and processing before providing data access at the application layer. This paradigm provides developers with expressive access to sensor operation while maintaining energy-efficiency and high-performance in the mobile system.    This project characteized the issues of sensor reconfiguration latency by measuring visual sensor performance during reconfiguration operations. The project found that current systems delay frame reconfiguration by several hundreds of milliseconds, degrading user experience. The project further identifiedd that the critical bottleneck was in the operating system's management of the capture stream during reconfiguration.  The project solved these issues through identifying and evaluating principles to modify mobile operating systems. These principles included providing strategies to reconfigure sensors in parallel with sensor capture routines, as well as reducnig repeated memory allocation patterns. Ultimatelly, for visual computing, this enables power reduction of 49% by allowing systems to capture and use lower resolution image frames rather than capturing and scaling down larger frames. This unlocks unprecedented capabilities for mobile vision applications to dynamically reconfigure sensor resolutions to balance the energy efficiency and task accuracy tradeoff.   The project incorporated undergradduate student in the research, wheerin she designed augmented reality application frameworks to leverage the reconfigurable system. The PI also taught a mobile augmented reality course at a high school summer camp at ASU to encourage broader participation in mobile computer systems programming and engineering.          Last Modified: 03/03/2020       Submitted by: Robert Likamwa]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
