<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:   Formal Models of Trainer Feedback for I-Learning Theoretical Guarantees</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>12/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>70043.00</AwardTotalIntnAmount>
<AwardAmount>70043</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As virtual agents and physical robots become more common, there is an increasing number of complex tasks they can usefully perform to assist humans. These tasks are typically formalized as sequential decision tasks, where robots and agents perceive states, take actions, and receive a reward feedback signal. In practice, there is a critical need to learn directly from human users---the majority of human users will not be able to directly program or fully specify a useful reward function. On the other hand, they can likely train an agent to perform tasks unanticipated by the original designer. Machine reinforcement learning (RL), a paradigm often used for solving sequential decision making tasks, was originally developed with inspiration from animal learning research from the applied behavior analysis (ABA) community. Existing RL approaches operationalize a limited set of ABA principles effectively; however, there are additional principles and properties from ABA research that are not well encapsulated in the existing RL formalisms, and that are likely sources of new inspiration for designing more effective RL techniques capable of learning from human teachers. The objective of this project is to leverage insights from animal training to reformulate the learning of sequential tasks from an agent learning alone in a fixed environment to an agent learning cooperatively with a competent, but not necessarily perfect, human teacher. Successful completion of this project will contribute a foundation of knowledge that will aide in the development new technologies to allow end users to customize the functions of their gadgets. &lt;br/&gt;&lt;br/&gt;This project is a part of a larger and collaborative effort between North Carolina State University (NCSU), Brown University, and Washington State University (WSU). The NCSU effort will include theoretical contributions along with empirical analyses and data collection. The emphasis of the NCSU portion of the project will be on the development of theoretical models of human feedback. When humans provide rewards to learning machines, describing the properties of the algorithms those machines use requires knowledge of how the humans provide feedback. For example, knowing when and how they make errors, the circumstances where they provide reinforcement or punishment, or use extinction, etc. Understanding the theoretical properties of I-Learning under different trainer paradigms will be the primary effort of NCSU project personnel. NCSU personnel will also work in concert with collaborators at Brown to use these models of feedback for describing the performance properties of I-Learning under different assumptions of trainer behavior. In addition, NCSU personnel will work with WSU collaborators to collect data from human trainers in virtual settings in order to validate and set the parameters of the theoretical models.</AbstractNarration>
<MinAmdLetterDate>07/15/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/15/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1643411</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Roberts</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David L Roberts</PI_FULL_NAME>
<EmailAddress>robertsd@csc.ncsu.edu</EmailAddress>
<PI_PHON>9195137182</PI_PHON>
<NSF_ID>000578917</NSF_ID>
<StartDate>07/15/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>North Carolina State University</Name>
<CityName>Raleigh</CityName>
<ZipCode>276957514</ZipCode>
<PhoneNumber>9195152444</PhoneNumber>
<StreetAddress>2601 Wolf Village Way</StreetAddress>
<StreetAddress2><![CDATA[Admin. III, STE 240]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042092122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTH CAROLINA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[North Carolina State University]]></Name>
<CityName/>
<StateCode>NC</StateCode>
<ZipCode>276958206</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~70043</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The core hypothesis of this work is that a new model of machine learning that more closely tracks the lawfulness studied in applied behavior analysis will unlock more efficient, natural, and effective interactions between learning machines and their human teachers. Whereas the historical focus of on machine reinforcement learning has been on feedback signals that are numeric and generated by algorithms or mathematical functions, this work focused on cases where humans decide when and how to provide feedback. The implications of designing algorithms in this setting are many, but this project has focused on two important ones: 1) human involvement is "expensive", and so the time and number of feedbacks required to learn should both be reduced as much as possible; and 2) humans have intentionality in their choices feedback choices, resulting in "pragmatic implicatures" that when accounted for can allow learning agents to extract additional information from each feedback. Progress in this work has emphasized the second implication to indirectly address the first, resulting in the theoretical design of two new algorithms: "Behavior Aware Modeling" (BAM) and "Human Abstraction Learning" (HAL). The BAM algorithm allows learning agents to simultaneously learn about the dynamics of the environment they are operating in along with the goal the human teacher has for the behavior in that environment, resulting in more efficient learning in single tasks as well as better generalization between tasks in similar environments. The HAL algorithm goes a step further to explicitly build a model of classes of tasks that humans may desire to teach, enabling even more efficient learning and transfer.&nbsp;</p> <p>The project emphasized the validation of the BAM algorithm through empirical testing in simulation and some preliminary human subjects experiments. The BAM algorithm was developed with practical constraints in mind: namely that the modes of communication human teachers, especially non-technical human teachers, tend to prefer are: demonstrations and discrete (i.e., non-numerical) feedback. Building on prior work where it was shown that human-delivered feedback is often "policy dependant" (meaning the choices of when and how to provide feedback aren't solely determined by the task, but also the prior performance of the learner), the underlying learning model for BAM is based on the Strategy-Aware Behavior Learning (SABL) class of learning algorithms. To incorporate demonstrations and these policy dependant feedbacks, BAM uses a new variant of SABL---Advantage SABL (A-SABL)---which treats human feedback as an indicator of "advantage", or the improvement over the prior action choices rather than an indicator of global optimality of choices. &nbsp;</p> <p>BAM, together with the underlying A-SABL implementation, were the focus of the experiments that demonstrated the advantages to using the BAM algorithm. BAM's performance was compared against two state-of-the-art algorithms from the literature: model-based inverse reinforcement learning and behavioral cloning. The simulation experiments used algorithmically-delivered feedback (referred to as "simulated teachers"). The simulated teachers were configured in several ways to illustrate performance under a variety of potential real-world conditions---fully validating with human teachers is the subject of near-term future work, beyond the scope of this project. Three learning domains were used: a grid-world navigation domain, a farming domain with three farm implements, and a gravity domain where changing gravity direction affects the available actions' effects. Across all domains with all simulated teachers, BAM performed at least as well as the other two algorithms, and in many cases significantly better. Further, similar performance results were found in preliminary user studies. Measuring whether or not a human teacher was able to teach the algorithms the correct policy as well as the number of feedbacks required to reach success, BAM performed as well as the other algorithms in the worst cases and significantly better in the remaining cases. In short, BAM was able to learn the correct policy from human teachers more frequently and required fewer interactions from the teachers in order to do so.&nbsp;</p> <p>Machine learning from human teachers is an increasing area of interest in the research community, and this work directly impacts progress in that subfield. The demonstration that BAM improves learning performance significantly is impactful on the field as it sets a new performance standard for other researchers to strive for. Further, the success of BAM indicates that the approach---explicitly modeling trainers---is one that can bear significant fruit in terms of improving the performance of machine learning algorithms that have human teachers. Further implications beyond the research community are also apparent when the recent proliferation of smart devices in the home are considered. End-users of smart devices aren't given a way to program the devices explicitly, and instead must implicitly train those devices with simple interactions that can be treated as feedbacks. The results of this project demonstrate how algorithms designed explicitly to learn from human teachers may some day help improve end users' interactions with smart devices.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/20/2018<br>      Modified by: David&nbsp;L&nbsp;Roberts</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The core hypothesis of this work is that a new model of machine learning that more closely tracks the lawfulness studied in applied behavior analysis will unlock more efficient, natural, and effective interactions between learning machines and their human teachers. Whereas the historical focus of on machine reinforcement learning has been on feedback signals that are numeric and generated by algorithms or mathematical functions, this work focused on cases where humans decide when and how to provide feedback. The implications of designing algorithms in this setting are many, but this project has focused on two important ones: 1) human involvement is "expensive", and so the time and number of feedbacks required to learn should both be reduced as much as possible; and 2) humans have intentionality in their choices feedback choices, resulting in "pragmatic implicatures" that when accounted for can allow learning agents to extract additional information from each feedback. Progress in this work has emphasized the second implication to indirectly address the first, resulting in the theoretical design of two new algorithms: "Behavior Aware Modeling" (BAM) and "Human Abstraction Learning" (HAL). The BAM algorithm allows learning agents to simultaneously learn about the dynamics of the environment they are operating in along with the goal the human teacher has for the behavior in that environment, resulting in more efficient learning in single tasks as well as better generalization between tasks in similar environments. The HAL algorithm goes a step further to explicitly build a model of classes of tasks that humans may desire to teach, enabling even more efficient learning and transfer.   The project emphasized the validation of the BAM algorithm through empirical testing in simulation and some preliminary human subjects experiments. The BAM algorithm was developed with practical constraints in mind: namely that the modes of communication human teachers, especially non-technical human teachers, tend to prefer are: demonstrations and discrete (i.e., non-numerical) feedback. Building on prior work where it was shown that human-delivered feedback is often "policy dependant" (meaning the choices of when and how to provide feedback aren't solely determined by the task, but also the prior performance of the learner), the underlying learning model for BAM is based on the Strategy-Aware Behavior Learning (SABL) class of learning algorithms. To incorporate demonstrations and these policy dependant feedbacks, BAM uses a new variant of SABL---Advantage SABL (A-SABL)---which treats human feedback as an indicator of "advantage", or the improvement over the prior action choices rather than an indicator of global optimality of choices.    BAM, together with the underlying A-SABL implementation, were the focus of the experiments that demonstrated the advantages to using the BAM algorithm. BAM's performance was compared against two state-of-the-art algorithms from the literature: model-based inverse reinforcement learning and behavioral cloning. The simulation experiments used algorithmically-delivered feedback (referred to as "simulated teachers"). The simulated teachers were configured in several ways to illustrate performance under a variety of potential real-world conditions---fully validating with human teachers is the subject of near-term future work, beyond the scope of this project. Three learning domains were used: a grid-world navigation domain, a farming domain with three farm implements, and a gravity domain where changing gravity direction affects the available actions' effects. Across all domains with all simulated teachers, BAM performed at least as well as the other two algorithms, and in many cases significantly better. Further, similar performance results were found in preliminary user studies. Measuring whether or not a human teacher was able to teach the algorithms the correct policy as well as the number of feedbacks required to reach success, BAM performed as well as the other algorithms in the worst cases and significantly better in the remaining cases. In short, BAM was able to learn the correct policy from human teachers more frequently and required fewer interactions from the teachers in order to do so.   Machine learning from human teachers is an increasing area of interest in the research community, and this work directly impacts progress in that subfield. The demonstration that BAM improves learning performance significantly is impactful on the field as it sets a new performance standard for other researchers to strive for. Further, the success of BAM indicates that the approach---explicitly modeling trainers---is one that can bear significant fruit in terms of improving the performance of machine learning algorithms that have human teachers. Further implications beyond the research community are also apparent when the recent proliferation of smart devices in the home are considered. End-users of smart devices aren't given a way to program the devices explicitly, and instead must implicitly train those devices with simple interactions that can be treated as feedbacks. The results of this project demonstrate how algorithms designed explicitly to learn from human teachers may some day help improve end users' interactions with smart devices.           Last Modified: 09/20/2018       Submitted by: David L Roberts]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
