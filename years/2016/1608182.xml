<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Statistical Theory and Methodology</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>700000.00</AwardTotalIntnAmount>
<AwardAmount>700000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern computational capabilities, modern theory, and the expanded data sets produced by modern scientific equipment have greatly increased the scope of statistical inference. This research project investigates a set of questions in probability and statistics raised by large-scale data collection. While of increasing use, empirical Bayes methods have proved difficult to justify. The approach under development in this project brings a novel application of exponential family theory to the job, with the goal of clarifying how empirical Bayes analyses converge to traditional Bayes methods as sample sizes increase. The project aims to develop empirical Bayes methods that use large-scale parallel data sets, such as those from microarray studies, to improve estimation in situations reporting many small sub-experiments, each of which by itself has low accuracy; and improved Monte Carlo methods for the computer solution of massive optimization problems.&lt;br/&gt;&lt;br/&gt;Specific topics under investigation in this research project include large-scale empirical Bayes strategies, importance sampling for computer-assisted inference in formerly intractable situations, and a theory of stability assessment for traditional methods of accuracy estimation.  Exponential families of probability distributions play a central role in both computation and statistical inference. A particularly stubborn impediment to their use in massive data analyses is the lack of a suitable norming constant in the exponential family density. This research project further develops computational methods based on solutions of appropriate variational problems; a promising application is in the area of graphical models. A second application of exponential families involves efficient deconvolution of datasets to obtain empirical Bayes estimates.</AbstractNarration>
<MinAmdLetterDate>05/23/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/01/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1608182</AwardID>
<Investigator>
<FirstName>Bradley</FirstName>
<LastName>Efron</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bradley Efron</PI_FULL_NAME>
<EmailAddress>brad@stat.stanford.edu</EmailAddress>
<PI_PHON>6507232206</PI_PHON>
<NSF_ID>000385119</NSF_ID>
<StartDate>05/23/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Persi</FirstName>
<LastName>Diaconis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Persi Diaconis</PI_FULL_NAME>
<EmailAddress>pwd@stanford.edu</EmailAddress>
<PI_PHON>6507251965</PI_PHON>
<NSF_ID>000125920</NSF_ID>
<StartDate>05/23/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>943054000</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1263</Code>
<Text>PROBABILITY</Text>
</ProgramElement>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~229043</FUND_OBLG>
<FUND_OBLG>2017~232926</FUND_OBLG>
<FUND_OBLG>2018~238031</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Interest in Statistics and Probability has increased in the past 20 years, both in the scientific world and the public community. The longtime purpose of this grant, <em>Statistical Theory and Methodology</em>, has been to translate cutting-edge research, which is often highly abstract and mathematical, into scientifically useful methodology. Over the long history of the grant, some of the new methods have included shrinkage estimation (the "James-Stein estimator"), the bootstrap, the empirical Bayes approach to false discovery rates, and practical group-theoretic probability techniques. Most recently, efforts have concentrated on deconvolution problems connected with large-scale research output: given a large set of results from parallel but not identical studies, how does one "deconvolve" the noisy outcomes to get at the underlying mechanism? An example is a genetics study involving thousands of genes, each of which provides a binomial estimate, where the researcher wants to know the underlying distribution of the individual true binomial probabilities.</p> <p>Another focus of the grant has been careful understanding and improvements of widely used algorithms for scientific computing. Almost every area of science, social science, and business now studies the real world through simulation. Complex mathematical models are simulated on the computer by a slew of Monte Carlo algorithms, the output is studied and used as an aid to decision-making:</p> <ul> <li>"Is this method of averaging more accurate than that method?"</li> <li>"Does this number of bank tellers result in long lines for customers?"</li> <li>"How do the odds of success in search-and-rescue operations depend on the number of helicopters assigned?"</li> </ul> <p>This raises the underlying question of how long a simulation should be run to obtain useful answers. Research under this grant has resulted in dozens of&nbsp; studies where things can be carefully proved. And this has yielded some surprises: sometimes a single new move can result in an exponential speed-up, the algorithm converging in 10 steps instead of 100,000. The main algorithms studied have been Markov chain Monte Carlo (MCMC). Work of the last few years has been able to translate very deep theorems from geometry and partial differential equations to MCMC's stochastic setting. This allowed basic problems, long, long open, to be tamed. One noteworthy example is gambler's ruin with several gamblers. A second is real progress in analyzing the widely used Metropolis algorithm.</p> <p>A second class of widely used algorithms that has been tackled (and triumphed over), as a result of research conducted during the period of this award, is sequential importance sampling. This has been actively studied by the theoretical computer science community who produced "polynomial time algorithms". Alas, these were useless in practice, with running times of N^7. We found algorithms which actually work for problem sizes of practical interest. Curiously, within the large N limit, these algorithms have exponential running times; but the exponent in front of N is 0.001 and for any N of scientific interest, the exponential algorithms dominate. This work was performed in the context of practical statistical problems such as testing for independence in contingency tables; testing for independence in censored data problems; and understanding models for random networks.</p> <p>All of this practical improvement benefits from beautiful mathematics and the fresh questions that statisticians ask, and itself has led to progress in esoteric areas of mathematics such as modular representation theory and geometric analysis.</p><br> <p>            Last Modified: 09/04/2020<br>      Modified by: Bradley&nbsp;Efron</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Interest in Statistics and Probability has increased in the past 20 years, both in the scientific world and the public community. The longtime purpose of this grant, Statistical Theory and Methodology, has been to translate cutting-edge research, which is often highly abstract and mathematical, into scientifically useful methodology. Over the long history of the grant, some of the new methods have included shrinkage estimation (the "James-Stein estimator"), the bootstrap, the empirical Bayes approach to false discovery rates, and practical group-theoretic probability techniques. Most recently, efforts have concentrated on deconvolution problems connected with large-scale research output: given a large set of results from parallel but not identical studies, how does one "deconvolve" the noisy outcomes to get at the underlying mechanism? An example is a genetics study involving thousands of genes, each of which provides a binomial estimate, where the researcher wants to know the underlying distribution of the individual true binomial probabilities.  Another focus of the grant has been careful understanding and improvements of widely used algorithms for scientific computing. Almost every area of science, social science, and business now studies the real world through simulation. Complex mathematical models are simulated on the computer by a slew of Monte Carlo algorithms, the output is studied and used as an aid to decision-making:  "Is this method of averaging more accurate than that method?" "Does this number of bank tellers result in long lines for customers?" "How do the odds of success in search-and-rescue operations depend on the number of helicopters assigned?"   This raises the underlying question of how long a simulation should be run to obtain useful answers. Research under this grant has resulted in dozens of  studies where things can be carefully proved. And this has yielded some surprises: sometimes a single new move can result in an exponential speed-up, the algorithm converging in 10 steps instead of 100,000. The main algorithms studied have been Markov chain Monte Carlo (MCMC). Work of the last few years has been able to translate very deep theorems from geometry and partial differential equations to MCMC's stochastic setting. This allowed basic problems, long, long open, to be tamed. One noteworthy example is gambler's ruin with several gamblers. A second is real progress in analyzing the widely used Metropolis algorithm.  A second class of widely used algorithms that has been tackled (and triumphed over), as a result of research conducted during the period of this award, is sequential importance sampling. This has been actively studied by the theoretical computer science community who produced "polynomial time algorithms". Alas, these were useless in practice, with running times of N^7. We found algorithms which actually work for problem sizes of practical interest. Curiously, within the large N limit, these algorithms have exponential running times; but the exponent in front of N is 0.001 and for any N of scientific interest, the exponential algorithms dominate. This work was performed in the context of practical statistical problems such as testing for independence in contingency tables; testing for independence in censored data problems; and understanding models for random networks.  All of this practical improvement benefits from beautiful mathematics and the fresh questions that statisticians ask, and itself has led to progress in esoteric areas of mathematics such as modular representation theory and geometric analysis.       Last Modified: 09/04/2020       Submitted by: Bradley Efron]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
