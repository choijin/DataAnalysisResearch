<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: AF: Characterization and Complexity of Information Elicitation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>182300</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The way one judges the accuracy of predictions can greatly impact what predictions people or computers make.  For example, Glenn Brier argued in 1950 that the way meteorologists were evaluated would actually give them an incentive to distort the true probability of rain.  Brier's study inspired a growing body of work in statistics, economics, and now computer science, which studies evaluation metrics that incentivize accurate reports from people or machines.  These evaluation metrics are also used in machine learning, a branch of artificial intelligence, where a designer implicitly tells the computer what statistic to predict by providing only the evaluation metric itself.  This project seeks to mathematically characterize this link between statistics and evaluation metrics, and moreover, to understand the computational and statistical difficulty of evaluating different statistics.  A precise understanding of this link would provide new evaluation metrics with the potential to increase predictive power across a vast array of applications such as climate simulations and smart cities.  In particular, metrics for statistics that quantify uncertainty or risk could improve decision making in many fields, including healthcare, engineering, and finance.&lt;br/&gt;&lt;br/&gt;A dominant algorithmic paradigm in machine learning, encompassing most regression techniques and classification algorithms, is that of empirical risk minimization (ERM): choosing a model from some class that best fits the data, according to some evaluation metric called a loss function.  A thread of research in theoretical machine learning called property elicitation gives a mathematical formalism to describe the link between loss functions and their corresponding statistics.  In these terms, this project seeks to characterize the statistics which have calibrated loss functions, and determine how many regression parameters or data points are required for the calibration to hold.  These questions are particularly relevant to machine learning when restricting attention to certain classes of loss functions which can be easily optimized or which have desirable statistical learning guarantees.  The class of statistics from mathematical finance known as risk measures, which are used to regulate banks, form an important focus of the project.</AbstractNarration>
<MinAmdLetterDate>02/21/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/29/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657598</AwardID>
<Investigator>
<FirstName>Rafael</FirstName>
<LastName>Frongillo</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rafael M Frongillo</PI_FULL_NAME>
<EmailAddress>raf@colorado.edu</EmailAddress>
<PI_PHON>3034927514</PI_PHON>
<NSF_ID>000707778</NSF_ID>
<StartDate>02/21/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090572</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, Room 481]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7796</Code>
<Text>ALGORITHMIC FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7932</Code>
<Text>COMPUT GAME THEORY &amp; ECON</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~175000</FUND_OBLG>
<FUND_OBLG>2018~7300</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Evaluation metrics, called loss functions, are a crucial component of modern statistics and machine learning, a branch of artificial intelligence. The goal of this project was to mathematically characterize the link between loss functions and the problems for which they are well-suited, or calibrated. The project set out to develop a framework to design calibrated loss functions for various statistical learning problems, and furthermore, to understand the computational and statistical difficulty of learning or evaluation for these problems. New loss functions have the potential to increase predictive power across a vast array of applications such as climate simulations and smart cities. Loss functions which are calibrated for measures of uncertainty or risk are particularly relevant to this project; such loss functions could improve decision making in many fields, including healthcare, engineering, and finance.</p> <p>The project made substantial progress on these directions, resulting in significant contributions to machine learning, statistics, and finance. Specific contributions include: (1) A new framework to design calibrated loss functions In machine learning; (2) Several testable geometric characterizations of calibrated loss functions in one dimension; (3) A new connection between online machine learning, when one learns while getting feedback, and the design of loss functions; (4) Characterizations of loss functions which yield practical market-based mechanisms to crowdsource information; (5) New loss functions which are calibrated for important measures of risk/uncertainty, together with impossibility results for some of these measures; (6) A general framework to study the difficulty of learning measures of risk/uncertainty; (7) A new theory of loss functions which measure error based on multiple data points simultaneously. Outcomes (5,6) have particular relevance to the ongoing discussion in the finance community about how best to regulate banks and other large financial institutions to control systemic financial risk. More generally, these results lay the foundation for many other important problems.</p> <p>Beyond these research outcomes, the project supported the education and training of three graduate students and two undergraduate students. These students participated in the research itself, as well as its dissemination through writing and oral presentations to the scientific community and public.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/18/2020<br>      Modified by: Rafael&nbsp;M&nbsp;Frongillo</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Evaluation metrics, called loss functions, are a crucial component of modern statistics and machine learning, a branch of artificial intelligence. The goal of this project was to mathematically characterize the link between loss functions and the problems for which they are well-suited, or calibrated. The project set out to develop a framework to design calibrated loss functions for various statistical learning problems, and furthermore, to understand the computational and statistical difficulty of learning or evaluation for these problems. New loss functions have the potential to increase predictive power across a vast array of applications such as climate simulations and smart cities. Loss functions which are calibrated for measures of uncertainty or risk are particularly relevant to this project; such loss functions could improve decision making in many fields, including healthcare, engineering, and finance.  The project made substantial progress on these directions, resulting in significant contributions to machine learning, statistics, and finance. Specific contributions include: (1) A new framework to design calibrated loss functions In machine learning; (2) Several testable geometric characterizations of calibrated loss functions in one dimension; (3) A new connection between online machine learning, when one learns while getting feedback, and the design of loss functions; (4) Characterizations of loss functions which yield practical market-based mechanisms to crowdsource information; (5) New loss functions which are calibrated for important measures of risk/uncertainty, together with impossibility results for some of these measures; (6) A general framework to study the difficulty of learning measures of risk/uncertainty; (7) A new theory of loss functions which measure error based on multiple data points simultaneously. Outcomes (5,6) have particular relevance to the ongoing discussion in the finance community about how best to regulate banks and other large financial institutions to control systemic financial risk. More generally, these results lay the foundation for many other important problems.  Beyond these research outcomes, the project supported the education and training of three graduate students and two undergraduate students. These students participated in the research itself, as well as its dissemination through writing and oral presentations to the scientific community and public.                 Last Modified: 09/18/2020       Submitted by: Rafael M Frongillo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
