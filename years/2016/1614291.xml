<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAPSI: Audio Attendant: A User Interface for Learning Peripheral Sounds</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>5400.00</AwardTotalIntnAmount>
<AwardAmount>5400</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Korean orthography makes clear the relationship between written and spoken language to facilitate phonological decoding of its written form. When a person learns a language, they learn to represent the sounds of the language in meaningful categories. Insofar as dyslexia is a struggle with mapping sounds to their written form, the expression of dyslexia in the Korean language is likely to differ significantly from other languages such as English or Spanish. This project proposes to design and create a user interface accessible in a mobile context that facilitates phonological awareness in the Korean language.  The research will be performed in collaboration with Dr. Joonhwan Lee of the Human Computer Interaction and Design Lab at Seoul National University in Korea. The project will inform development of a lightweight dyslexia screener that could be deployed by nonexperts (for free or minimal cost) from around the world to support access to needed resources for a person with dyslexia. The work will advance research areas as language-based assistive technology, human-computer interaction for individuals with cognitive disabilities, and crowdsourcing and will contribute to identifying individuals with dyslexia irrespective of their native language to help make the web accessible to persons with language impairments from around the world.&lt;br/&gt;&lt;br/&gt;Prior work highlights a role for a system that can help a person implicitly learn sound categories that are specific to a target language, though may not be at the center of a person?s attention or part of their lexicon. This project will investigate how auditory cues can be integrated in a contextually appropriate manner when balancing cues with attending to conversation. Building on Edge, et al., we will extend contextual vocabulary learning of a target language to contextual learning of sound categories. We will determine what sound categories should be acquired by adapting a scoring system of importance that has been found to work well for dynamic visual displays when driving, to an auditory attention service that reflects the learner?s goals and contextual importance.  Finally, we will design and create a user interface for learning contextual audio categories based off this prior work to be used in a mobile setting during conversation.&lt;br/&gt;&lt;br/&gt;This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the National Research Foundation of Korea.</AbstractNarration>
<MinAmdLetterDate>07/26/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/26/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1614291</AwardID>
<Investigator>
<FirstName>Kristin</FirstName>
<LastName>Williams</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristin Williams</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>7706951203</PI_PHON>
<NSF_ID>000710523</NSF_ID>
<StartDate>07/26/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Williams                Kristin</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152171442</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Williams                Kristin]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152171442</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5942</Code>
<Text>KOREA</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~5400</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Speaking and writing communicate in fundamentally different ways. Yet, current, speech-to-text transformation renders spoken language into a written code of the words irrespective of the verbal context or speaker intonation. For a person who relies solely on the text to interpret what is said, this process omits important cues to the speaker&rsquo;s meaning. A mobile chat service that actively makes use of audio processing of spoken language could address these limitations by using the visual features of chat to draw attention to features of spoken language. To design such a service, we modified a kinetic typography engine to incorporate features of spoken mood and energy into a typographic library.</p> <p>We examined whether acoustic information that is peripheral to spoken content&mdash;like vocal signatures of emotion&mdash;can be encoded in kinetic typography and recognized by second language learners. Recognition of these peripheral cues could facilitate language learners&rsquo; development of sociolinguistic competence. This competence consists in the ability to know when it is contextually appropriate to use a language&rsquo;s vocabulary and captures a sense of socio-political knowledge that is tacitly learned when immersed in a language&rsquo;s spoken environment. Sociolinguistic sensitivity characterizes the speaker&rsquo;s ability to actively listen and know both when and how to participate in conversation. The Korean orthography does not directly convey sociolinguistic aspects that are implicitly expressed by the speaker. Yet, these cues have important meaning in Korean culture: like when it is appropriate to use honorifics, internet speak, and gendered forms of communication. We modified a kinetic typography to support learning these peripheral cues embedded in spoken language.</p> <p><strong>INTELLECTUAL MERIT AND BROADER IMPACTS </strong></p> <p>Our work contributes a kinetic typography library that can interface with the web browser audio API and is compatible with speech recognition algorithms to facilitate peripheral attention to desired categories of spoken language. We created a dataset consisting of 18 sentences spoken by male and female voice actors with 9 different emotions. We also created a script which extracts audio signatures of arousal and valence based on the circumplex model of emotion and use these as parameters to be passed to a kinetic typography library. This dataset and script allows a kinetic typography library used in a web browser to render the animated type alongside the audio file and draw visual attention to the emotional cues in the spoken audio. This dataset and script contribute an approach for rendering kinetic typography alongside spoken language which could be used in both the Korean and English languages and could support individuals with reading disabilities, second language learners, or the deaf or hard of hearing. These techniques could be readily integrated in web chat services, captioning online video-streaming content, or other language based assistive technology.</p><br> <p>            Last Modified: 03/15/2017<br>      Modified by: Kristin&nbsp;Williams</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Speaking and writing communicate in fundamentally different ways. Yet, current, speech-to-text transformation renders spoken language into a written code of the words irrespective of the verbal context or speaker intonation. For a person who relies solely on the text to interpret what is said, this process omits important cues to the speaker?s meaning. A mobile chat service that actively makes use of audio processing of spoken language could address these limitations by using the visual features of chat to draw attention to features of spoken language. To design such a service, we modified a kinetic typography engine to incorporate features of spoken mood and energy into a typographic library.  We examined whether acoustic information that is peripheral to spoken content&mdash;like vocal signatures of emotion&mdash;can be encoded in kinetic typography and recognized by second language learners. Recognition of these peripheral cues could facilitate language learners? development of sociolinguistic competence. This competence consists in the ability to know when it is contextually appropriate to use a language?s vocabulary and captures a sense of socio-political knowledge that is tacitly learned when immersed in a language?s spoken environment. Sociolinguistic sensitivity characterizes the speaker?s ability to actively listen and know both when and how to participate in conversation. The Korean orthography does not directly convey sociolinguistic aspects that are implicitly expressed by the speaker. Yet, these cues have important meaning in Korean culture: like when it is appropriate to use honorifics, internet speak, and gendered forms of communication. We modified a kinetic typography to support learning these peripheral cues embedded in spoken language.  INTELLECTUAL MERIT AND BROADER IMPACTS   Our work contributes a kinetic typography library that can interface with the web browser audio API and is compatible with speech recognition algorithms to facilitate peripheral attention to desired categories of spoken language. We created a dataset consisting of 18 sentences spoken by male and female voice actors with 9 different emotions. We also created a script which extracts audio signatures of arousal and valence based on the circumplex model of emotion and use these as parameters to be passed to a kinetic typography library. This dataset and script allows a kinetic typography library used in a web browser to render the animated type alongside the audio file and draw visual attention to the emotional cues in the spoken audio. This dataset and script contribute an approach for rendering kinetic typography alongside spoken language which could be used in both the Korean and English languages and could support individuals with reading disabilities, second language learners, or the deaf or hard of hearing. These techniques could be readily integrated in web chat services, captioning online video-streaming content, or other language based assistive technology.       Last Modified: 03/15/2017       Submitted by: Kristin Williams]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
