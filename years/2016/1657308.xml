<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Facilitating Information Exchange in Crowd Problem Solving Discussions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>182776</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Online communities, from question and answer sites to general purpose discussion forums, are increasingly working to solve hard problems together, often through a process of open sharing and discussion of ideas and information.  The idea is that large groups can increase the diversity of information available and lead to better outcomes.  This may not be true; research shows that in small groups, people often fail to share relevant knowledge and tend to overlook minority points of view.  This proposal's goal is to examine how these research findings about small groups transfer to larger, online "crowds", which typically have more members and different contribution patterns than small groups working more closely together; further, participating in a crowd platform is typically voluntary and thus their membership can change quickly during the discussion.  To understand how these differences affect information sharing behavior, the research team will first modify the well-known hidden profile task that is used to study small groups' information sharing to fit crowd contexts.  The team will then run a series of studies that vary the size, participation requirements, task duration, and composition of the crowd, and measure how both the sharing of information and the quality of the crowd's responses change as the design of the task changes.  This will lead to both a deeper understanding of information sharing behaviors in groups, and design guidelines for building crowd platforms that are better for problem solving.  The work will also provide training opportunities for both PhD and undergraduate researchers, materials to use in teaching courses around social computing, and software that will be shared with other researchers who want to study online crowds. &lt;br/&gt;&lt;br/&gt;The work will be based on classic hidden profile tasks where information is unequally distributed among group members.  Because group sizes are generally small because of practicalities of recruitment, in typical hidden profile designs a piece of information is either known to a majority of the group, or to one unique member.  For crowd contexts, the team will introduce a third category of rare clues that are neither unique nor majority, since this is likely to be a common phenomenon in large groups; the relative proportion of shared, rare, and unique clues can be adjusted to set the difficulty of a particular case.  The team plans to conduct four sets of experiments using a real online crowd platform, allowing them to recruit large groups subject to the actual incentives of crowd participation.  Each set of experiments will manipulate one major difference identified between crowds and smaller groups that theory suggests might affect the crowd's information sharing behavior: group size, the ability to choose a preferred task, the length of the discussion (minutes versus days), and the amount of turnover in the crowd.  Doing this will lead to a richer theoretical picture of how large groups perform information sharing, discussion, and analysis tasks, as well as generate practical guidelines for the design of both specific crowd tasks and crowd platforms more generally.</AbstractNarration>
<MinAmdLetterDate>01/24/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/15/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657308</AwardID>
<Investigator>
<FirstName>Yla</FirstName>
<LastName>Tausczik</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yla Tausczik</PI_FULL_NAME>
<EmailAddress>ylatau@umd.edu</EmailAddress>
<PI_PHON>3014052058</PI_PHON>
<NSF_ID>000696272</NSF_ID>
<StartDate>01/24/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~175000</FUND_OBLG>
<FUND_OBLG>2018~7776</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The use of social media and other online communication is widespread, and the design of these platforms facilitates (or hinders) sharing of accurate and complete information. Accuracy and completeness are of particular importance for online communities used in STEM professions, like software development and data science. Online communities, such as question and answer sites (e.g. Stack Overflow), online forums (e.g. Hacker News), and special interest groups on social media platforms (e.g. www.reddit.com/r/Python), have been shown to be important sources of timely information. However, these communities face two major challenges: 1) the information provided is often incomplete and 2) it may be difficult to combine and make sense of the information provided. Four experiments were conducted on a simulated online discussion platform using a well-known method from social psychology, called a hidden profile task. Hidden profile tasks require a group of individuals to share their specialized knowledge in order to draw the right conclusion about a topic. Analysis of the experimental results led to three main conclusions about information sharing in online groups and their differences from offline groups:</p> <p>1) These experiments showed that online groups were biased in the same way as offline groups, that is individuals were more likely to share information known to many people in the group and less likely to share information known to only a few people. As a result, in most groups information shared was not complete and a majority of individuals drew the wrong conclusion about the topic.</p> <p>2) This research identified four key differences between online and offline groups: i) highly variable group size; ii) self-selected participation; iii) staggered arrival; and iv) asynchronous communication. We found that some of these differences affected information sharing, in particular group size and self-selected participation. Larger groups were shown to share more information; however, they were less likely to draw the right conclusion from that information. Therefore, medium-sized groups performed the best; they were large enough to share specialized information, but small enough that they were able to process that information as a group. Group formation was also shown to affect information sharing. When individuals were assigned to groups, they shared more information than when individuals self-selected which groups to join. We found that individuals chose which groups to join for many different reasons, this meant that on average they did not always select the most relevant group. In contrast, when individuals were assigned to groups, they made worse decisions than when they self-selected which groups to join. Therefore, self-selected participation seems to have a multifaceted effect on information sharing and group decisions.</p> <p>3) The research used a data mining technique, LASSO regression, to identify the factors that best explained which groups were more successful. Groups shared more complete information when more people knew the most specialized knowledge, groups communicated more, and individuals expressed the limits of their own expertise. Groups came to more accurate conclusions when they shared more specialized knowledge (and less general knowledge) and used short-cuts (heuristics) to make sense of information instead of complex strategies.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/08/2020<br>      Modified by: Yla&nbsp;Tausczik</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The use of social media and other online communication is widespread, and the design of these platforms facilitates (or hinders) sharing of accurate and complete information. Accuracy and completeness are of particular importance for online communities used in STEM professions, like software development and data science. Online communities, such as question and answer sites (e.g. Stack Overflow), online forums (e.g. Hacker News), and special interest groups on social media platforms (e.g. www.reddit.com/r/Python), have been shown to be important sources of timely information. However, these communities face two major challenges: 1) the information provided is often incomplete and 2) it may be difficult to combine and make sense of the information provided. Four experiments were conducted on a simulated online discussion platform using a well-known method from social psychology, called a hidden profile task. Hidden profile tasks require a group of individuals to share their specialized knowledge in order to draw the right conclusion about a topic. Analysis of the experimental results led to three main conclusions about information sharing in online groups and their differences from offline groups:  1) These experiments showed that online groups were biased in the same way as offline groups, that is individuals were more likely to share information known to many people in the group and less likely to share information known to only a few people. As a result, in most groups information shared was not complete and a majority of individuals drew the wrong conclusion about the topic.  2) This research identified four key differences between online and offline groups: i) highly variable group size; ii) self-selected participation; iii) staggered arrival; and iv) asynchronous communication. We found that some of these differences affected information sharing, in particular group size and self-selected participation. Larger groups were shown to share more information; however, they were less likely to draw the right conclusion from that information. Therefore, medium-sized groups performed the best; they were large enough to share specialized information, but small enough that they were able to process that information as a group. Group formation was also shown to affect information sharing. When individuals were assigned to groups, they shared more information than when individuals self-selected which groups to join. We found that individuals chose which groups to join for many different reasons, this meant that on average they did not always select the most relevant group. In contrast, when individuals were assigned to groups, they made worse decisions than when they self-selected which groups to join. Therefore, self-selected participation seems to have a multifaceted effect on information sharing and group decisions.  3) The research used a data mining technique, LASSO regression, to identify the factors that best explained which groups were more successful. Groups shared more complete information when more people knew the most specialized knowledge, groups communicated more, and individuals expressed the limits of their own expertise. Groups came to more accurate conclusions when they shared more specialized knowledge (and less general knowledge) and used short-cuts (heuristics) to make sense of information instead of complex strategies.          Last Modified: 12/08/2020       Submitted by: Yla Tausczik]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
