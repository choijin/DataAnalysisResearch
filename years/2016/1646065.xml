<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: Collaborative Research: Cyber-Physical Sensing, Modeling, and Control with Augmented Reality for Smart Manufacturing Workforce Training and Operations Management</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2017</AwardEffectiveDate>
<AwardExpirationDate>01/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>196000.00</AwardTotalIntnAmount>
<AwardAmount>196000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bruce Kramer</SignBlockName>
<PO_EMAI>bkramer@nsf.gov</PO_EMAI>
<PO_PHON>7032925348</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Smart manufacturing integrates information, technology, and human ingenuity to inspire the next revolution in the manufacturing industry. Manufacturing has been identified as a key strategic investment area by the U.S. government, private sector, and university leaders to spur innovation and keep America competitive. However, the lack of new methodologies and tools is challenging continuous innovation in the smart manufacturing industry. This award supports fundamental research to develop a cyber-physical sensing, modeling, and control infrastructure, coupled with augmented reality, to significantly improve the efficiency of future workforce training, performance of operations management, safety and comfort of workers for smart manufacturing. Results from this research are expected to transform the practice of worker-machine-task coordination and provide a powerful tool for operations management. This research involves several disciplines including sensing, data analytics, modeling, control, augmented reality, and workforce training and will provide unique interdisciplinary training opportunities for students and future manufacturing engineers. &lt;br/&gt;&lt;br/&gt;An effective way for manufacturers to tackle and outpace the increasing complexity of product designs and ever-shortening product lifecycles is to effectively develop and assist the workforce. Yet the current management of manufacturing workforce systems relies mostly on the traditional methods of data collection and modeling, such as subjective observations and after-the-fact statistics of workforce performance, which has reached a bottleneck in effectiveness. The goal of this project is to investigate an integrated set of cyber-physical system methods and tools to sense, understand, characterize, model, and optimize the learning and operation of manufacturing workers, so as to achieve significantly improved efficiency in worker training, effectiveness of behavioral operations management, and safety of front-line workers. The research team will instrument a suite of sensors to gather real-time data about individual workers, worker-machine interactions, and the working environment,develop advanced methods and tools to track and understand workers' actions and physiological status, and detect their knowledge and skill deficiencies or assistance needs in real time. The project will also establish mathematical models that encode the manufacturing process in the research sensing and analysis framework, characterize the efficiency of worker-machine-task coordination, model the learning curves of individual workers, investigate various multi-modal augmented reality-based visualization, guidance, control, and intervention schemes to improve task efficiency and worker safety, and deploy, test, and conduct comprehensive performance assessments of the Researched technologies.</AbstractNarration>
<MinAmdLetterDate>08/07/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1646065</AwardID>
<Investigator>
<FirstName>Zhihai</FirstName>
<LastName>He</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhihai He</PI_FULL_NAME>
<EmailAddress>hezhi@missouri.edu</EmailAddress>
<PI_PHON>5738823495</PI_PHON>
<NSF_ID>000388185</NSF_ID>
<StartDate>08/07/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Missouri-Columbia</Name>
<CityName>COLUMBIA</CityName>
<ZipCode>652110001</ZipCode>
<PhoneNumber>5738827560</PhoneNumber>
<StreetAddress>115 Business Loop 70 W</StreetAddress>
<StreetAddress2><![CDATA[Mizzou North, Room 501]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<StateCode>MO</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MO04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153890272</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MISSOURI SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006326904</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Missouri-Columbia]]></Name>
<CityName/>
<StateCode>MO</StateCode>
<ZipCode>652110001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MO04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>018Y</Code>
<Text>CM - Cybermanufacturing System</Text>
</ProgramElement>
<ProgramElement>
<Code>7918</Code>
<Text>CPS-Cyber-Physical Systems</Text>
</ProgramElement>
<ProgramReference>
<Code>016Z</Code>
<Text>Cybermanufacturing Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>152E</Code>
<Text>Cyber-Physical Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>1654</Code>
<Text>HUMAN COMPUTER INTERFACE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>MANU</Code>
<Text>MANUFACTURING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~196000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Smart manufacturing needs smart workforce training and operations management. Current practices rely on traditional methods, such as subjective observations, manual reports, and after-the-facts statistics to manage worker training and floor operations. In this project, we aim to develop an integrated set of cyber-physical methods and tools to sense, understand,&nbsp;and characterize the training performance and operation efficiency of manufacturing workers. Specifically, we have successfully developed accurate and robust methods to estimate the body pose of workers, track their actions, and automatically evaluate the efficiency of their task operations. We have also developed methods to learn their behavior patterns and predict their moving intentions on the manufacturing floors for effective human-machine collaborations. These technologies have the potential to improve the efficiency of worker training, effectiveness of behavioral operations management, and safety of front-line workers, for smart manufacturing. The project advances the state-of-the-art of workforce training and behavioral operations management. With sensor data collected in real-time, data-driven models can be built just-in-time to capture the ability, behavior, and manageability of individual workers in trainings and operations, which provides executives a new ability to manage the employee talents and skills in an optimized way. This project provides unique and exciting opportunities to train our graduate students in a highly interdisciplinary setting. Students from electrical engineering, mechanical and civil engineering, and education and learning science in both institutes worked together to address important problems of smart workforce training for smart manufacturing.</p> <p>&nbsp;This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/05/2021<br>      Modified by: Zhihai&nbsp;He</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1646065/1646065_10447819_1622903449522_moton--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1646065/1646065_10447819_1622903449522_moton--rgov-800width.jpg" title="Human motion prediction"><img src="/por/images/Reports/POR/2021/1646065/1646065_10447819_1622903449522_moton--rgov-66x44.jpg" alt="Human motion prediction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Using a tightly coupled forward and backward prediction networks, we have developed a reciprocal learning methods for modeling and predicting future motion trajectory of human based past observations.</div> <div class="imageCredit">Zhihai He</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Zhihai&nbsp;He</div> <div class="imageTitle">Human motion prediction</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Smart manufacturing needs smart workforce training and operations management. Current practices rely on traditional methods, such as subjective observations, manual reports, and after-the-facts statistics to manage worker training and floor operations. In this project, we aim to develop an integrated set of cyber-physical methods and tools to sense, understand, and characterize the training performance and operation efficiency of manufacturing workers. Specifically, we have successfully developed accurate and robust methods to estimate the body pose of workers, track their actions, and automatically evaluate the efficiency of their task operations. We have also developed methods to learn their behavior patterns and predict their moving intentions on the manufacturing floors for effective human-machine collaborations. These technologies have the potential to improve the efficiency of worker training, effectiveness of behavioral operations management, and safety of front-line workers, for smart manufacturing. The project advances the state-of-the-art of workforce training and behavioral operations management. With sensor data collected in real-time, data-driven models can be built just-in-time to capture the ability, behavior, and manageability of individual workers in trainings and operations, which provides executives a new ability to manage the employee talents and skills in an optimized way. This project provides unique and exciting opportunities to train our graduate students in a highly interdisciplinary setting. Students from electrical engineering, mechanical and civil engineering, and education and learning science in both institutes worked together to address important problems of smart workforce training for smart manufacturing.   This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.          Last Modified: 06/05/2021       Submitted by: Zhihai He]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
