<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Workshop on  Architecture and Software for Emerging Applications (WASEA)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>11/01/2016</AwardEffectiveDate>
<AwardExpirationDate>10/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>49900.00</AwardTotalIntnAmount>
<AwardAmount>49900</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Matt Mutka</SignBlockName>
<PO_EMAI>mmutka@nsf.gov</PO_EMAI>
<PO_PHON>7032927344</PO_PHON>
</ProgramOfficer>
<AbstractNarration>High-valued domain applications in areas such as medicine,&lt;br/&gt;biology, physics, engineering, and social phenomena demand&lt;br/&gt;both fast innovation and high execution speed and require&lt;br/&gt;productive development environments for domain experts who&lt;br/&gt;may not be computer science experts. This workshop brings together leading researchers in architecture,&lt;br/&gt;compilers and programming languages, and domain experts to discuss&lt;br/&gt;and debate potential approaches to accelerating progress in such&lt;br/&gt;high-valued domains with an emphasis on developing strategies for&lt;br/&gt;exploiting machine learning, including strategies for accelerating&lt;br/&gt;learning algorithms through parallelism.  The goal is to stimulate&lt;br/&gt;an in-depth discussion of the potential benefits of joint architecture&lt;br/&gt;and compiler approaches.  The workshop will promote broadening&lt;br/&gt;participation by including speakers from groups underrepresented&lt;br/&gt;in computing and early career researchers.&lt;br/&gt;&lt;br/&gt;The workshop will produce a report providing recommendations on:&lt;br/&gt;joint compiler/language and architecture approaches, compiler/language&lt;br/&gt;support enabling more aggressive hardware capabilities, architecture&lt;br/&gt;support enabling more effective compilers, and applications whose&lt;br/&gt;development process could benefit by these advances, The report&lt;br/&gt;will identify research opportunities in the interaction between&lt;br/&gt;developers, and architecture and language/compiler researchers to&lt;br/&gt;enable productive domain application development and highly efficient&lt;br/&gt;and scalable implementation on heterogeneous computing systems.  The&lt;br/&gt;report will outline promising approaches and the research required for&lt;br/&gt;these approaches to become usable by the domain application developers.</AbstractNarration>
<MinAmdLetterDate>10/25/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/16/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657976</AwardID>
<Investigator>
<FirstName>Nancy</FirstName>
<LastName>Amato</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nancy M Amato</PI_FULL_NAME>
<EmailAddress>namato@illinois.edu</EmailAddress>
<PI_PHON>2173333373</PI_PHON>
<NSF_ID>000430397</NSF_ID>
<StartDate>10/25/2016</StartDate>
<EndDate>05/16/2019</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lawrence</FirstName>
<LastName>Rauchwerger</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lawrence Rauchwerger</PI_FULL_NAME>
<EmailAddress>rwerger@illinois.edu</EmailAddress>
<PI_PHON>9792550424</PI_PHON>
<NSF_ID>000468621</NSF_ID>
<StartDate>10/25/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M Engineering Experiment Station</Name>
<CityName>College Station</CityName>
<ZipCode>778454645</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy S</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>847205572</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A&amp;M ENGINEERING EXPERIMENT STATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&amp;M Engineering Experiment Station]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778454645</ZipCode>
<StreetAddress><![CDATA[400 Harvey Mitchell Pkwy S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7556</Code>
<Text>CONFERENCE AND WORKSHOPS</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~49900</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-00965d71-7fff-8ce8-d93a-5eacd7f51853">&nbsp;</span></p> <div class="page" title="Page 4"> <div class="layoutArea"> <div class="column"> <p><span>Intellectual Merit </span></p> <p><span>Bringing together some of best researchers to discuss domain specific applications and their software and architecture environment may lead to new approaches to solving this difficult problem. The development of faster, more capable machine learning environments as well as better and faster graph processing capabilities may greatly increase our capabilities to study and optimize problems in a wide range: from physics and engineering to social phenomena. </span></p> <p><span>Broader Impact </span></p> <p><span>The development of better tools for using machine learning and graph algorithms can affect such areas as medicine, biology, social studies, etc. It can grant us access to a wealth of data that is being collected but it is not being used yet.</span></p> <p><span>Outcome</span></p> </div> </div> </div> <p dir="ltr"><span>High-valued domain applications such as image recognition demand both fast innovation and high execution speed. For example, the methods for image recognition using deep neural networks evolve very quickly and require productive development environments for domain experts who may not be computer science experts. Geoffrey Hinton&rsquo;s team won the 2012 ImageNet competition by training a deep neural network with 1.2 million images. Since then, many new algorithm innovations have been proposed for significant improvement over the version from Hinton&rsquo;s team. Furthermore, the success in applying deep neural networks to image recognition has ignited a lot of research on applying deep neural networks to other areas such as speech recognition and natural language process, where traditional approaches have not been successful for decades.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>At the same time, the research process for these methods require fast turnaround time of experiments that involve training deep neural networks using millions of images. Training of deep convolution neural networks involves fine-grained parallel computation within each layer but is constrained by data dependencies from one layer to the next. This is why traditional scale-out schemes based on clusters have not been successful. The best hardware for the training process has been tightly coupled multi-GPU systems that support extremely fast synchronization among a very large number of cooperating fine grained threads. Even with these systems, each such experiment can take weeks to complete. The creation of an effective algorithm can take many rounds of experiments. Dramatically more efficient implementation of candidate algorithms and thus much faster turnaround time of experiments can have a significant impact on the progress of the field.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The state of the art in developing and implementing high-valued domain applications is based on application frameworks. For example, in machine learning, developers typically build on library frameworks such as Caffe, Torch, and TensorFlow. The library functions of these frameworks are implemented for CPUs, GPUs, and other accelerators. However, in the current practice, it takes tremendous amount of effort and time to bring up the implementation of an existing training method for a new compute device. It takes even more effort and latency to introduce a new method that needs to be implemented for the existing devices.&nbsp; Individual kernels in each method needs to be hand-optimized for a new architecture. Efficient arrangement of the execution of kernels with respect to each other in a methods to take advantage of the memory hierarchy and interconnect capabilities requires even more effort.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Experience from the CUDA and Heterogeneous System Architecture (HSA) shows that architecture support can significantly reduce the cost of implementing application functions on heterogeneous parallel devices. Unified address space, user-level command queues, re-optimizable intermediate representations, and coherent shared memory are among the frequently cited system architecture features that reduce the barrier of implementation. However, little has been done in the architecture of the compute devices to explicitly lower the barrier of implementation.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>On the compiler side, much progress has been made in C++ and OpenCL compilation to provide efficient code optimization, scheduling, and generation for a given algorithm. However, there has been little work on the support of specifying alternative algorithms that can achieve the&nbsp; same application-level results but different levels of efficiency on different hardware types and hierarchies. Some recent work on DSL have shown promise in image processing. However, one needs to wrestle with the burden on the developers to learn multiple languages. A more generic extension to the existing C++ or Python language to allow more compiler support may be a more realistic long-term solution.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The idea of the workshop is to bring together architecture and compiler researchers to discuss and debate on the potential approaches to accelerating the innovation in high-valued domain applications. The goal is to stimulate an in-depth discussion of what each can potentially achieve and how much more joint architecture and compiler approaches could accomplish. The product of the workshop is a report on the recommendations on each approach based on in-depth discussions of leading researchers from compilers, architecture, and important application domains.</span></p> <p dir="ltr"><span>&nbsp;&nbsp;</span></p><br> <p>            Last Modified: 03/27/2020<br>      Modified by: Lawrence&nbsp;Rauchwerger</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[       Intellectual Merit   Bringing together some of best researchers to discuss domain specific applications and their software and architecture environment may lead to new approaches to solving this difficult problem. The development of faster, more capable machine learning environments as well as better and faster graph processing capabilities may greatly increase our capabilities to study and optimize problems in a wide range: from physics and engineering to social phenomena.   Broader Impact   The development of better tools for using machine learning and graph algorithms can affect such areas as medicine, biology, social studies, etc. It can grant us access to a wealth of data that is being collected but it is not being used yet.  Outcome    High-valued domain applications such as image recognition demand both fast innovation and high execution speed. For example, the methods for image recognition using deep neural networks evolve very quickly and require productive development environments for domain experts who may not be computer science experts. Geoffrey Hinton’s team won the 2012 ImageNet competition by training a deep neural network with 1.2 million images. Since then, many new algorithm innovations have been proposed for significant improvement over the version from Hinton’s team. Furthermore, the success in applying deep neural networks to image recognition has ignited a lot of research on applying deep neural networks to other areas such as speech recognition and natural language process, where traditional approaches have not been successful for decades.    At the same time, the research process for these methods require fast turnaround time of experiments that involve training deep neural networks using millions of images. Training of deep convolution neural networks involves fine-grained parallel computation within each layer but is constrained by data dependencies from one layer to the next. This is why traditional scale-out schemes based on clusters have not been successful. The best hardware for the training process has been tightly coupled multi-GPU systems that support extremely fast synchronization among a very large number of cooperating fine grained threads. Even with these systems, each such experiment can take weeks to complete. The creation of an effective algorithm can take many rounds of experiments. Dramatically more efficient implementation of candidate algorithms and thus much faster turnaround time of experiments can have a significant impact on the progress of the field.     The state of the art in developing and implementing high-valued domain applications is based on application frameworks. For example, in machine learning, developers typically build on library frameworks such as Caffe, Torch, and TensorFlow. The library functions of these frameworks are implemented for CPUs, GPUs, and other accelerators. However, in the current practice, it takes tremendous amount of effort and time to bring up the implementation of an existing training method for a new compute device. It takes even more effort and latency to introduce a new method that needs to be implemented for the existing devices.  Individual kernels in each method needs to be hand-optimized for a new architecture. Efficient arrangement of the execution of kernels with respect to each other in a methods to take advantage of the memory hierarchy and interconnect capabilities requires even more effort.    Experience from the CUDA and Heterogeneous System Architecture (HSA) shows that architecture support can significantly reduce the cost of implementing application functions on heterogeneous parallel devices. Unified address space, user-level command queues, re-optimizable intermediate representations, and coherent shared memory are among the frequently cited system architecture features that reduce the barrier of implementation. However, little has been done in the architecture of the compute devices to explicitly lower the barrier of implementation.    On the compiler side, much progress has been made in C++ and OpenCL compilation to provide efficient code optimization, scheduling, and generation for a given algorithm. However, there has been little work on the support of specifying alternative algorithms that can achieve the  same application-level results but different levels of efficiency on different hardware types and hierarchies. Some recent work on DSL have shown promise in image processing. However, one needs to wrestle with the burden on the developers to learn multiple languages. A more generic extension to the existing C++ or Python language to allow more compiler support may be a more realistic long-term solution.     The idea of the workshop is to bring together architecture and compiler researchers to discuss and debate on the potential approaches to accelerating the innovation in high-valued domain applications. The goal is to stimulate an in-depth discussion of what each can potentially achieve and how much more joint architecture and compiler approaches could accomplish. The product of the workshop is a report on the recommendations on each approach based on in-depth discussions of leading researchers from compilers, architecture, and important application domains.          Last Modified: 03/27/2020       Submitted by: Lawrence Rauchwerger]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
