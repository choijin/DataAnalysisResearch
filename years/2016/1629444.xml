<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: Bridging Parallel and Queueing-Theoretic Scheduling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>825000.00</AwardTotalIntnAmount>
<AwardAmount>991000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scheduling computational tasks on computational resources has played a fundamental role in computer science since the 1950s.  One branch of scheduling (queueing theoretic) has focused on analyzing multiple sequential jobs competing for a shared resource with the goal of minimizing the response time (latency) over all jobs.  Most operating system and server schedulers use ideas developed as part of this research.  Another branch of scheduling (parallel) has focused on analyzing a single parallel job running on a dedicated parallel machine, with the goal of maximizing efficiency (throughput) of the job.  Most schedulers for dynamically parallel programs use ideas developed as part of this research.  Until recently these branches were adequate on their own, and the communities studying them have had very little interaction.  However, the mainstream availability of parallel hardware, and the need to handle many parallel jobs sharing a single resource, has recently changed this.  The goal of this project is to bridge the two branches by developing new theory and practical scheduling algorithms, that can handle multiple dynamically parallel jobs competing for shared resources.  The project brings together PIs with expertise from each area, and will apply and combine techniques from each area.  The project has the potential to have significant broad impact on the theory and practice of widely used shared parallel systems.  The project will include an educational outreach component in which the PIs will include ideas from the project in courses they teach.&lt;br/&gt;&lt;br/&gt;This project is the first to tackle the union of these two domains. The PIs will develop scheduling algorithms for a stream of arriving jobs, where the jobs are complex multi-threaded fine-grained parallel jobs with dynamic parallelism and dependencies among tasks.  The research will address three specific challenges, as follows. Challenge 1, Statistical Characterization/Modeling: Scheduling multiple jobs requires knowing something about when each job will complete and what it will do in the future, such as the number of tasks it will create or the granularity of tasks.  A significant part of the project is devoted to measuring parallel jobs and statistically characterizing them to create simple models of their behavior.  Challenge 2, Algorithmic Development and Analysis: There are currently no scheduling algorithms for the problem that is being considered.  While queueing theory touts the optimality of always running the job with the "shortest expected remaining time," this has little meaning when jobs are parallel and there are many resources.  New theorems and analytical techniques will be developed. Challenge 3, Implementation and Benchmarking: An important component of the project will be the implementation and benchmarking of our algorithms on prototype systems.  The PIs will investigate multiple metrics including latency, throughput, fairness, and robustness.</AbstractNarration>
<MinAmdLetterDate>06/29/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1629444</AwardID>
<Investigator>
<FirstName>Guy</FirstName>
<LastName>Blelloch</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guy E Blelloch</PI_FULL_NAME>
<EmailAddress>guyb@cs.cmu.edu</EmailAddress>
<PI_PHON>4122686245</PI_PHON>
<NSF_ID>000196851</NSF_ID>
<StartDate>06/29/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mor</FirstName>
<LastName>Harchol-Balter</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mor Harchol-Balter</PI_FULL_NAME>
<EmailAddress>harchol@cs.cmu.edu</EmailAddress>
<PI_PHON>4122687893</PI_PHON>
<NSF_ID>000201705</NSF_ID>
<StartDate>06/29/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Umut</FirstName>
<LastName>Acar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Umut Acar</PI_FULL_NAME>
<EmailAddress>umut@cs.cmu.edu</EmailAddress>
<PI_PHON>4122689527</PI_PHON>
<NSF_ID>000636864</NSF_ID>
<StartDate>06/29/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>042Y</Code>
<Text>PPoSS-PP of Scalable Systems</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<ProgramReference>
<Code>026Z</Code>
<Text>NSCI: National Strategic Computing Initi</Text>
</ProgramReference>
<ProgramReference>
<Code>7934</Code>
<Text>PARAL/DISTRIBUTED ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~825000</FUND_OBLG>
<FUND_OBLG>2018~16000</FUND_OBLG>
<FUND_OBLG>2019~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The aim of this project is to develop new theories and approaches to scheduling by combining knowledge from two disparate communities:&nbsp; (i) the stochastic queueing-theoretic scheduling community, and (ii) the adversarial parallel job scheduling community.&nbsp;&nbsp;&nbsp;</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">On the technical end, the project has tackled several important questions in scheduling and has resulted in some major breakthroughs. &nbsp; One of the key questions involves how to optimally share a set of cores among multiple parallelizable jobs so as to minimize the mean response time across all the jobs (response time is the time from when a job arrives until it completes). &nbsp; Jobs have different sizes, where a job?s size is the inherent work associated with the job. &nbsp; Each job also has a speedup function that indicates how much benefit the job gets from each additional core. &nbsp; On the one hand, when allocating cores to jobs, it makes sense to favor short jobs, in accordance with the well-known Shortest-Remaining-Processing-Time (SRPT) algorithm. &nbsp; On the other hand, given that jobs have speedup functions, one has to be careful not to allocate cores to jobs that insufficiently benefit from these cores. &nbsp; The core algorithm that we develop is called heSRPT, or ?high-efficiency SRPT,? because it combines the ideas of SRPT scheduling, while maintaining high efficiency which is key to parallel workloads. &nbsp; In the case where all jobs are present at the start, job sizes are known a priori, and all jobs have the same concave speedup function, we have developed an exact explicit formula for the optimal allocation of cores to jobs, at each moment of time, so as to minimize mean response time.&nbsp; &nbsp; Under the same model, we also have derived a formula for minimizing mean slowdown (slowdown is a version of weighted response time where small jobs count more, as they are more sensitive to high response time).&nbsp;&nbsp;&nbsp;&nbsp;</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">We also consider the problem of optimal core allocation where jobs have different speedup functions, specifically one class of jobs is fully parallelizable (elastic) while another class is only parallelizable up to some point (inelastic). &nbsp; Here we assume that jobs arrive over time and their sizes are drawn from exponential distributions where elastic jobs are larger on average.&nbsp; In this setting, we prove that the optimal core allocation policy is the Inelastic First policy, which always favors inelastic jobs.&nbsp; There is a lot more work to be done in this area.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">We also tackled the problem of how to schedule jobs optimally when the job sizes are not known and the sizes are drawn from some arbitrary distribution.&nbsp; Here we derived and analyzed simple yet near-optimal scheduling policies (called M-Gittins, or "monotonic Gittins") for both a single server queue and a multi-server queue.&nbsp;&nbsp;</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In the context of work-stealing schedulers we considered the problem where a single job has classes of subtasks (e.g. readers and writers) which must each receive a fair share of the computational power. &nbsp; We developed a work-stealing scheduler that can create pools of tasks such that every dynamic subtask&nbsp; can be forked into any specified pool. &nbsp; On creation, each pool is given a fixed share of resources. &nbsp; We applied this to both a database application and a graph processing engine, where in both cases we have a mix of large parallel queries with much smaller dynamic updates.&nbsp; In the context of hardware scheduling of memory requests we studied how hardware schedules requests to cache lines. &nbsp; This included an extensive study and the development of a tool called Severus.</span></p> <p>&nbsp;</p> <p id="docs-internal-guid-3f5fdd07-7fff-1fb8-b798-06154b0e2590" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The project also produced several broader impacts. &nbsp; Seven PhD students were partially funded under this project as well as four undergraduates. &nbsp; One received their PhD during the time of the grant. &nbsp; Between them, they produced over a dozen papers, including 5 Best Paper or Best Student Paper Awards and 1 Best Video Presentation Award. &nbsp; The research in this grant also inspired scheduling work at Google, which led to a Google Faculty Research Award. &nbsp; One of the PIs completed a first draft of an undergraduate textbook on Probability Theory for Computer Scientists, which combines both stochastic analysis and adversarial worst-case analysis as is the goal of this grant. &nbsp; The work on the pool scheduler was released on github as part of the Parlay library (https://github.com/cmuparlay/parlaylib).&nbsp; &nbsp; It was also used in an in-memory multiversion database system, also available on github (</span><a style="text-decoration: none;" href="https://github.com/cmuparlay/PAM"><span style="font-size: 11pt; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">https://github.com/cmuparlay/PAM</span></a><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">).&nbsp; Our tool for analyzing the scheduling of memory requests, Severus, is also available on github (</span><a style="text-decoration: none;" href="https://github.com/cmuparlay/severus"><span style="font-size: 11pt; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">https://github.com/cmuparlay/severus</span></a><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">).</span></p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><br /></span></p> <p><br /><br /></p><br> <p>            Last Modified: 08/01/2020<br>      Modified by: Mor&nbsp;Harchol-Balter</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The aim of this project is to develop new theories and approaches to scheduling by combining knowledge from two disparate communities:  (i) the stochastic queueing-theoretic scheduling community, and (ii) the adversarial parallel job scheduling community.       On the technical end, the project has tackled several important questions in scheduling and has resulted in some major breakthroughs.   One of the key questions involves how to optimally share a set of cores among multiple parallelizable jobs so as to minimize the mean response time across all the jobs (response time is the time from when a job arrives until it completes).   Jobs have different sizes, where a job?s size is the inherent work associated with the job.   Each job also has a speedup function that indicates how much benefit the job gets from each additional core.   On the one hand, when allocating cores to jobs, it makes sense to favor short jobs, in accordance with the well-known Shortest-Remaining-Processing-Time (SRPT) algorithm.   On the other hand, given that jobs have speedup functions, one has to be careful not to allocate cores to jobs that insufficiently benefit from these cores.   The core algorithm that we develop is called heSRPT, or ?high-efficiency SRPT,? because it combines the ideas of SRPT scheduling, while maintaining high efficiency which is key to parallel workloads.   In the case where all jobs are present at the start, job sizes are known a priori, and all jobs have the same concave speedup function, we have developed an exact explicit formula for the optimal allocation of cores to jobs, at each moment of time, so as to minimize mean response time.    Under the same model, we also have derived a formula for minimizing mean slowdown (slowdown is a version of weighted response time where small jobs count more, as they are more sensitive to high response time).        We also consider the problem of optimal core allocation where jobs have different speedup functions, specifically one class of jobs is fully parallelizable (elastic) while another class is only parallelizable up to some point (inelastic).   Here we assume that jobs arrive over time and their sizes are drawn from exponential distributions where elastic jobs are larger on average.  In this setting, we prove that the optimal core allocation policy is the Inelastic First policy, which always favors inelastic jobs.  There is a lot more work to be done in this area.    We also tackled the problem of how to schedule jobs optimally when the job sizes are not known and the sizes are drawn from some arbitrary distribution.  Here we derived and analyzed simple yet near-optimal scheduling policies (called M-Gittins, or "monotonic Gittins") for both a single server queue and a multi-server queue.      In the context of work-stealing schedulers we considered the problem where a single job has classes of subtasks (e.g. readers and writers) which must each receive a fair share of the computational power.   We developed a work-stealing scheduler that can create pools of tasks such that every dynamic subtask  can be forked into any specified pool.   On creation, each pool is given a fixed share of resources.   We applied this to both a database application and a graph processing engine, where in both cases we have a mix of large parallel queries with much smaller dynamic updates.  In the context of hardware scheduling of memory requests we studied how hardware schedules requests to cache lines.   This included an extensive study and the development of a tool called Severus.    The project also produced several broader impacts.   Seven PhD students were partially funded under this project as well as four undergraduates.   One received their PhD during the time of the grant.   Between them, they produced over a dozen papers, including 5 Best Paper or Best Student Paper Awards and 1 Best Video Presentation Award.   The research in this grant also inspired scheduling work at Google, which led to a Google Faculty Research Award.   One of the PIs completed a first draft of an undergraduate textbook on Probability Theory for Computer Scientists, which combines both stochastic analysis and adversarial worst-case analysis as is the goal of this grant.   The work on the pool scheduler was released on github as part of the Parlay library (https://github.com/cmuparlay/parlaylib).    It was also used in an in-memory multiversion database system, also available on github (https://github.com/cmuparlay/PAM).  Our tool for analyzing the scheduling of memory requests, Severus, is also available on github (https://github.com/cmuparlay/severus).             Last Modified: 08/01/2020       Submitted by: Mor Harchol-Balter]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
