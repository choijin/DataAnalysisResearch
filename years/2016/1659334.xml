<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Nonlinear Factor and Latent Variable Models</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>229997.00</AwardTotalIntnAmount>
<AwardAmount>229997</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project will create new nonlinear dimension-reducing data analysis methods. The methods will be designed to extract information regarding a few specific unobserved variables of interest from large amounts of observed but potentially error-contaminated data. This type of nonlinear approach is inherently challenging due to the risk that the data alone may not uniquely identify the features of the variables of interest unless carefully motivated assumptions are made and a detailed formal analysis is performed. The new nonlinear methods will generalize current linear dimension-reducing statistical analysis methods that are widely used in statistical surveys, economics, medical imaging, data compression, machine learning, and internet search engines. Developing efficient nonlinear extensions of these methods will advance data analysis capabilities in these fields considerably by enabling researchers to uncover intricate nonlinear relationships that are currently masked through the lenses of linear approaches. A graduate student will obtain valuable research education and training by playing an important role in the development and implementation of the new methods. Software developed from this project will be made publicly available.&lt;br/&gt;&lt;br/&gt;The idea that the information contained in a large number of even imperfectly measured variables can be summarized by a small number of variables (the "factors" or "principal components") has been widely adopted in economics and statistics and is gathering even more attention with the increasing prevalence of "big data." Some of the methods to be developed can be seen as a unification and generalization of widely used classes of techniques, such as linear latent factor models, multiway array decomposition, and nonclassical measurement error models. Other methods to be developed generalize the widely used linear principal component analysis to nonlinear settings. The main questions addressed in this work are: What reasonable assumptions ensure that the distribution of the many observed variables uniquely determine the distribution of the specific unobserved variables of interest?  Can efficient numerical algorithms be devised to find this unique mapping between observed and unobserved distributions? Do the new methods reduce to existing methods in special cases? The approaches used to answer these questions draw from the fields of optimal transport, entropy maximization, operator theory, and measure theory.</AbstractNarration>
<MinAmdLetterDate>03/23/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/23/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1659334</AwardID>
<Investigator>
<FirstName>Susanne</FirstName>
<LastName>Schennach</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Susanne Schennach</PI_FULL_NAME>
<EmailAddress>smschenn@brown.edu</EmailAddress>
<PI_PHON>4018631234</PI_PHON>
<NSF_ID>000096073</NSF_ID>
<StartDate>03/23/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brown University</Name>
<CityName>Providence</CityName>
<ZipCode>029129002</ZipCode>
<PhoneNumber>4018632777</PhoneNumber>
<StreetAddress>BOX 1929</StreetAddress>
<StreetAddress2><![CDATA[350 Eddy Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<StateCode>RI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>RI01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001785542</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BROWN UNIVERSITY IN PROVIDENCE IN THE STATE OF RHODE ISLAND AND PROVIDENCE PLANTATIONS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001785542</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>Providence</CityName>
<StateCode>RI</StateCode>
<ZipCode>029129093</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Projects]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>RI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1320</Code>
<Text>Economics</Text>
</ProgramElement>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~229997</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has improved our abilities to perform accurate and efficient statistical inference in a number of settings.</p> <p>The first contribution focuses on nonparametric estimation (i.e., methods which do not a priori restrict the functional form of the functions to be determined). This project, started during the PI's prior grant and completed under this one, shows that it is possible to bypass long-standing difficulties in obtaining confidence intervals that are both statistically valid and optimal (i.e., as narrow as possible). As it is known that optimal nonparametric estimators are necessarily biased, the prevailing approach had been to instead use suboptimal estimators that have no bias. This project's key idea is to observe valid inference with an optimal but biased estimator can nevertheless be obtained when this bias can be bounded in terms of observable quantities. This apparently simple idea leads to a completely different approach to inference, which turns out to be both simple and very effective. This work shows that methods that seek to eliminate the bias exhibit slower convergence than those that simply bound it. These findings are presented in a forthcoming publication in the Review of Economic Studies, one of economics' top broad-audience journals.</p> <p>This project's second main contribution is a comprehensive review of analytical methods to handle the presence of mismeasured and unobserved variables in economic models (such as unobserved abilities, preferences, or the accurate value of a misreported income or expenditures). The main takeaway is that the most effective techniques involve gathering multiple, yet imperfect, indicators (or measurements) of the underlying unobserved variables and exploit the fact that the "signal" can be disentangled from the "noise" because the "signal" is common to all indicators, while the "noise" is not. This work will appear in a forthcoming volume of the Handbook of Econometrics, one of the main references in the field.</p> <p>This project also makes more specific contributions in the field of errors-in-variables models. While it is well-known that traditional regression analysis makes it possible to determine the relationship between two variables when one of them is error-contaminated, it is less well known that it is also possible to do so when both variables are error-contaminated. This project goes a step further and show that this result holds even if one of the variables exhibits errors that do not follow the "classical" assumptions (independence and zero mean). This considerably broadens the scope of errors-in-variables models that can be practically handled without resorting to expensive validation data.</p> <p>Finally, this project introduces a generalization of the widely used machine learning technique known as principal component analysis (PCA), to nonlinear settings. These techniques are used to reduce the dimensionality of the data while losing as little information as possible. The new method combines techniques from optimal transport theory with those of information theory (such as entropy maximization) to determine the most informative nonlinear features of the data. The method offers the advantages of automatically reducing to standard PCA in the special case of linear Gaussian models and of avoiding the need for user-specified criteria to constrain the complexity of the model's nonlinearity. We illustrate the method's effectiveness in an application to excess bond returns prediction from a large number of macro factors (see attached illustration).</p><br> <p>            Last Modified: 08/29/2020<br>      Modified by: Susanne&nbsp;Schennach</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1659334/1659334_10478428_1598740418347_rot--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1659334/1659334_10478428_1598740418347_rot--rgov-800width.jpg" title="Graphical representation of independent nonlinear components identified within a high-dimensional set of macroeconomic factors"><img src="/por/images/Reports/POR/2020/1659334/1659334_10478428_1598740418347_rot--rgov-66x44.jpg" alt="Graphical representation of independent nonlinear components identified within a high-dimensional set of macroeconomic factors"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Colored surfaces show contours of constant estimated density of the data [from Ludvigson & Ng, Rev. Fin. Stud. 22, 5027 (2009).] while the grids represent the 3 most important nonlinear factors as a curvilinear coordinate system (L-shaped cross-section shown, for clarity).</div> <div class="imageCredit">Susanne Schennach and Florian Gunsilius</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Susanne&nbsp;Schennach</div> <div class="imageTitle">Graphical representation of independent nonlinear components identified within a high-dimensional set of macroeconomic factors</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has improved our abilities to perform accurate and efficient statistical inference in a number of settings.  The first contribution focuses on nonparametric estimation (i.e., methods which do not a priori restrict the functional form of the functions to be determined). This project, started during the PI's prior grant and completed under this one, shows that it is possible to bypass long-standing difficulties in obtaining confidence intervals that are both statistically valid and optimal (i.e., as narrow as possible). As it is known that optimal nonparametric estimators are necessarily biased, the prevailing approach had been to instead use suboptimal estimators that have no bias. This project's key idea is to observe valid inference with an optimal but biased estimator can nevertheless be obtained when this bias can be bounded in terms of observable quantities. This apparently simple idea leads to a completely different approach to inference, which turns out to be both simple and very effective. This work shows that methods that seek to eliminate the bias exhibit slower convergence than those that simply bound it. These findings are presented in a forthcoming publication in the Review of Economic Studies, one of economics' top broad-audience journals.  This project's second main contribution is a comprehensive review of analytical methods to handle the presence of mismeasured and unobserved variables in economic models (such as unobserved abilities, preferences, or the accurate value of a misreported income or expenditures). The main takeaway is that the most effective techniques involve gathering multiple, yet imperfect, indicators (or measurements) of the underlying unobserved variables and exploit the fact that the "signal" can be disentangled from the "noise" because the "signal" is common to all indicators, while the "noise" is not. This work will appear in a forthcoming volume of the Handbook of Econometrics, one of the main references in the field.  This project also makes more specific contributions in the field of errors-in-variables models. While it is well-known that traditional regression analysis makes it possible to determine the relationship between two variables when one of them is error-contaminated, it is less well known that it is also possible to do so when both variables are error-contaminated. This project goes a step further and show that this result holds even if one of the variables exhibits errors that do not follow the "classical" assumptions (independence and zero mean). This considerably broadens the scope of errors-in-variables models that can be practically handled without resorting to expensive validation data.  Finally, this project introduces a generalization of the widely used machine learning technique known as principal component analysis (PCA), to nonlinear settings. These techniques are used to reduce the dimensionality of the data while losing as little information as possible. The new method combines techniques from optimal transport theory with those of information theory (such as entropy maximization) to determine the most informative nonlinear features of the data. The method offers the advantages of automatically reducing to standard PCA in the special case of linear Gaussian models and of avoiding the need for user-specified criteria to constrain the complexity of the model's nonlinearity. We illustrate the method's effectiveness in an application to excess bond returns prediction from a large number of macro factors (see attached illustration).       Last Modified: 08/29/2020       Submitted by: Susanne Schennach]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
