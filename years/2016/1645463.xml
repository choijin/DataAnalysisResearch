<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Wide Field of View Augmented Reality Display with Dynamic Focus</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>81657.00</AwardTotalIntnAmount>
<AwardAmount>81657</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Augmented reality (AR) has the potential to integrate computer graphics and the human visual system.  The technology has been demonstrated to improve task performance in areas such as communication, accessibility, worker efficiency and medicine.  However, use of AR technology is not yet widespread, and it is still viewed as a novelty. The PI believes this is partially due to the lack of adequate display technology to support compelling applications.  The most advanced head-mounted devices available today present a flat image located at a fixed distance from the user's eyes, and optical see-through devices are often quite limited in coverage of the visual field.  In this exploratory project, the PI and his team will design and implement a prototype of a novel see-through augmented reality display system that provides both wide field of view and variable focal depth. If successful, this device will lay the foundation for a new class of augmented reality display that enables virtual objects to have the same focal depth as real world objects at any location, creating better fusion of real and virtual, which will enable society to take advantage of the capabilities augmented reality can provide.  Such a new class of display will open exciting research avenues; for example, by incorporating gaze tracking such devices could have even wider research applications by adapting to the user's gaze direction and fixation distance.&lt;br/&gt;&lt;br/&gt;The PI's approach relies upon a deformable membrane optical combiner which can be shaped to change the optical depth of the virtual image. This single optical element solution also allows for the creation of virtual imagery that covers a large portion of the visual field.  By employing a single reflective optical element as the vari-focal relay optics, this project will simplify the design of see-through vari-focal optical systems for near-eye displays.  This technique also promises a large aperture size, leading to wide field of view near-eye display solutions for AR applications. In theory, this approach can provide a full field of view solution with a display and aperture in the proper configuration. Upon completion of a working prototype, many perceptual user studies which were previously difficult to perform will become easily available, enabling a deeper understanding of the human visual system.</AbstractNarration>
<MinAmdLetterDate>07/06/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1645463</AwardID>
<Investigator>
<FirstName>Henry</FirstName>
<LastName>Fuchs</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Henry Fuchs</PI_FULL_NAME>
<EmailAddress>fuchs@cs.unc.edu</EmailAddress>
<PI_PHON>9199714951</PI_PHON>
<NSF_ID>000451367</NSF_ID>
<StartDate>07/06/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S. Columbia St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~81657</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project sought to develop improved head-mounted displays for augmented reality, displays that allow the wearer to observe computer-generated objects while also viewing the wearer's surroundings. Often these computer generated object are three-dimensional and are placed at fixed locations in the user's surroundings, for example a virtual sofa in the user's actual living room (or a virtual "zombie" next to the user's friend). The apparent distance of real objects is determined by a combination of vergence of the user's two eyes (the angle that the two eyes need to rotate for the two images of the object to match) and accommodation, the focus adjustment of each of the user's eyes. These two factors, vergence and accommodation, are naturally neurally coupled, and when they don't match, the user experiences discomfort and sometimes double vision, and occasionally nausea.&nbsp; One of the problems with most current augmented reality displays is that their optics put the internal, virtual image at a fixed distance from the user, for example 2 meters, even though the objects displayed may be much closer or much farther away. Since the user is simultaneously observing both virtual imagery and also real objects, this mismatch between verge and accommodation is especially problematic.</p> <p>The research supported by this grant sought to ameliorate this problem by developing displays whose focus distance can be dynamically adjusted to match the distance of the virtual object at the user's current gaze.&nbsp;&nbsp;</p> <p>The prototype display was developed in collaboration with researchers from NVIDIA and MPI Informatics,&nbsp; proved to be quite effective. The research paper describing it and its tests received the best paper award at the 2017 IEEE Virtual Reality Conference.&nbsp; We also demonstrated this prototype at ACM Siggraph Emerging Technologies exhibition, where it won the Digital Content Expo Japan Prize.</p> <p>Encouraged by the effectiveness of the prototype display, we develop a still-more capable display, "FocusAR," that could not only dynamically adjust the focal depth of the internal (virtual) imagery but could also simultaneously and independently adjust the focus of the view from the user's "real world" surroundings. This dynamic adjustment is particularly important for user over 45 years old, who almost always need some adjustment either for near objects or far objects or both.</p> <p>Results from this second display, developed in collaboration with Dr. Kaan Aksit, of NVIDIA Research, were also well received. The paper introducing this display received the Best Paper Award at IEEE ISMAR 2018, the International Symposium on Mixed and Augmented Reality.</p> <p>In the future, these kinds of dynamic adjustments may find their way to commercial head-mounte displays, which will make them more comfortable to use and thus more effective for a wide variety of tasks.</p><br> <p>            Last Modified: 12/28/2018<br>      Modified by: Henry&nbsp;Fuchs</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1645463/1645463_10438251_1546022814846_Dunn2017fig.1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1645463/1645463_10438251_1546022814846_Dunn2017fig.1--rgov-800width.jpg" title="Images from Augmented Reality Display with Variable Focus Membrane Display"><img src="/por/images/Reports/POR/2018/1645463/1645463_10438251_1546022814846_Dunn2017fig.1--rgov-66x44.jpg" alt="Images from Augmented Reality Display with Variable Focus Membrane Display"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Wide field of view augmented reality display showing virtual teapot at far and near distance together with real objects, soda cans, at near and far. Photos through display system left and right eyes with focus at far (top row), focus near (bottom row), and overhead view (right) of the system.</div> <div class="imageCredit">David Dunn, Cary Tippets, Kent Torell, Petr Kellnhofer, Kaan Aksit, Piotr Didyk, Karol Myszkowski, David Luebke, and Henry Fuchs</div> <div class="imageSubmitted">Henry&nbsp;Fuchs</div> <div class="imageTitle">Images from Augmented Reality Display with Variable Focus Membrane Display</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1645463/1645463_10438251_1546024121017_2018.10FocusARimage--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1645463/1645463_10438251_1546024121017_2018.10FocusARimage--rgov-800width.jpg" title="FocusAR real objects and virtual imagery"><img src="/por/images/Reports/POR/2018/1645463/1645463_10438251_1546024121017_2018.10FocusARimage--rgov-66x44.jpg" alt="FocusAR real objects and virtual imagery"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Left: Older user (whose natural focus is at 7 Meters) using FocusAR display.  Middle: Imagery in FocusAR, everything out of focus. Right: Imagery with nearby virtual bunny and real book brought into focus for this user. (Yellow outline indicates region of gaze.)</div> <div class="imageCredit">Praneeth Chakravarthula, David Dunn, Kaan Aksit and Henry Fuchs</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Henry&nbsp;Fuchs</div> <div class="imageTitle">FocusAR real objects and virtual imagery</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project sought to develop improved head-mounted displays for augmented reality, displays that allow the wearer to observe computer-generated objects while also viewing the wearer's surroundings. Often these computer generated object are three-dimensional and are placed at fixed locations in the user's surroundings, for example a virtual sofa in the user's actual living room (or a virtual "zombie" next to the user's friend). The apparent distance of real objects is determined by a combination of vergence of the user's two eyes (the angle that the two eyes need to rotate for the two images of the object to match) and accommodation, the focus adjustment of each of the user's eyes. These two factors, vergence and accommodation, are naturally neurally coupled, and when they don't match, the user experiences discomfort and sometimes double vision, and occasionally nausea.  One of the problems with most current augmented reality displays is that their optics put the internal, virtual image at a fixed distance from the user, for example 2 meters, even though the objects displayed may be much closer or much farther away. Since the user is simultaneously observing both virtual imagery and also real objects, this mismatch between verge and accommodation is especially problematic.  The research supported by this grant sought to ameliorate this problem by developing displays whose focus distance can be dynamically adjusted to match the distance of the virtual object at the user's current gaze.    The prototype display was developed in collaboration with researchers from NVIDIA and MPI Informatics,  proved to be quite effective. The research paper describing it and its tests received the best paper award at the 2017 IEEE Virtual Reality Conference.  We also demonstrated this prototype at ACM Siggraph Emerging Technologies exhibition, where it won the Digital Content Expo Japan Prize.  Encouraged by the effectiveness of the prototype display, we develop a still-more capable display, "FocusAR," that could not only dynamically adjust the focal depth of the internal (virtual) imagery but could also simultaneously and independently adjust the focus of the view from the user's "real world" surroundings. This dynamic adjustment is particularly important for user over 45 years old, who almost always need some adjustment either for near objects or far objects or both.  Results from this second display, developed in collaboration with Dr. Kaan Aksit, of NVIDIA Research, were also well received. The paper introducing this display received the Best Paper Award at IEEE ISMAR 2018, the International Symposium on Mixed and Augmented Reality.  In the future, these kinds of dynamic adjustments may find their way to commercial head-mounte displays, which will make them more comfortable to use and thus more effective for a wide variety of tasks.       Last Modified: 12/28/2018       Submitted by: Henry Fuchs]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
