<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Collaborative Research: Detailed Shape and Reflectance Capture with Light Field Cameras</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A major transformation is occurring in the way we sense the visual world.  Traditional 2D photography is increasingly being replaced with light-field sensors that capture the full spatial and angular variation of the incoming light field, rather than simply pixels that integrate over incoming directions.  This development opens up the possibility of ubiquitous 3D imaging of our visual world.  Light-field sensors are particularly attractive as a depth acquisition device, since they are completely passive without needing to project light into the scene, and they do not experience a reduction in performance outdoors.  Moreover, the rich ray-space of light fields provides significant cues for recovering fine-scale depth.  However, current RGBD and light-field systems produce only coarse depth; while useful for tasks like refocusing images, the depth channel offers little benefit for photography beyond conventional 2D RGB images.  This research seeks to address these challenges, by developing practical algorithms for detailed 3D shape and reflectance capture with light-field cameras, coupled with a theoretical and experimental analysis of the achievable accuracy.  Project outcomes will have broad impact on diverse fields including computer graphics and virtual/augmented reality, enabling acquisition of high-quality detailed 3D shape and the subsequent use of the 3D geometry with computer-generated synthetic objects.  Methods to acquire 3D images, including on mobile sensors, will transform the photographic process from 2D to 3D, with immense industrial and societal impact.&lt;br/&gt;&lt;br/&gt;The PIs will address four important problems in light-field shape acquisition.  First, they will exploit the rich nature of light-field data, combining multiple cues (defocus, correspondence, shading, specularity) in a unified way to obtain the overall global 3D shape.  Moreover, they will seek to go beyond the common Lambertian reflectance assumption, developing a novel BRDF-invariant framework for surface reconstruction with general glossy materials like metals, plastics, or ceramics, while supporting textures and spatially-varying reflectance.  Another key objective will be to ground the practical results in a theoretical framework that can establish the limits of light-field camera shape resolution, and the signal-to-noise accuracy, and how this relates to novel designs for light-field cameras to obtain the best achievable resolution in 3D shape capture.  Finally, the PIs will move from overall shape to fine-scale surface detail, proposing new methods for shape/reflectance capture for fine-scale geometry like hair.  The ultimate goal is to enable a full 3D processing pipeline for photography, computer graphics and applications like virtual and augmented reality.</AbstractNarration>
<MinAmdLetterDate>07/18/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/18/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617234</AwardID>
<Investigator>
<FirstName>Ravi</FirstName>
<LastName>Ramamoorthi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ravi Ramamoorthi</PI_FULL_NAME>
<EmailAddress>ravir@cs.ucsd.edu</EmailAddress>
<PI_PHON>5102258896</PI_PHON>
<NSF_ID>000486826</NSF_ID>
<StartDate>07/18/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>920930404</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project dealt with using light field cameras for detailed shape and reflectance capture.&nbsp; Light Field cameras store the full distribution of light in the scene.&nbsp; A conventional camera captures a 2D image, with each point or each pixel considering the combined light over the entire aperture of the camera.&nbsp; On the other hand, the light field camera also records the direction of incident light, capturing a 4D function.&nbsp; In effect, it's like an array of tiny micro-cameras along the aperture of the main lens.&nbsp; This enables novel photographic effects like refocusing after capture, but this project sought to investigate other applications of light field cameras, including their ability to capture the detailed shape and reflectance of objects, to allow the objects to be included in new scenes and composed with computer-generated imagery.</p> <p>The key results of this project are with reference to the uploaded images:</p> <p>1. We have developed a method to use light field cameras to capture the shape of even objects that have sharp highlights and complex reflectance.&nbsp; This is a new robust method, leveraging a solution that is invariant to the reflectance properties.&nbsp; In this example, we show that the turtle shape is accurately recovered by our method from the image taken with a light field camera.</p> <p>2. Light Fields enable new types of applications.&nbsp; Here, we demonstrate that we can deblur the motion in an image captured with a light field camera significantly more effectively than when the image is captured by a conventional camera.&nbsp; Note that motion blur and motion deblurring is a key aspect of photography, especially for fast-moving scenes.</p> <p>3. While much of this project focuses on photographs taken with a light field camera, even greater opportunity is enabled by light field video.&nbsp; However, the amount of data generated is too large for light field cameras to capture 60fps video.&nbsp; We show that by coupling a conventional video camera with a light field camera, we can obtain 60fps light field video.&nbsp; This enables exciting capabilities like refocusing the video while it is playing, changing one's viewpoint slightly or focus tracking.</p> <p>4. Increasingly, the world is moving to imaging on mobile phones.&nbsp; As such, we seek methods to acquire light fields from simple mobile phone captures, and ultimately to leverage the multiple mobile phone cameras as a light field camera.&nbsp; We are able to show that with very few images casually captured in a semi-regular grid with a standard cell phone, we can create a light field and enable view synthesis with robust sampling guidelines for how far apart and where new images should be captured.</p> <p>5. The ability to capture light fields from sparse images opens up the door to capture detailed shape and reflectance from a set of sparse images on a mobile phone.&nbsp; This not just allows one to view the scene from new directions, but also to relight objects in the scene.&nbsp; We show that we can enable relightable scene reconstructions from a sparse set of input images with greatest realism obtained by using a volumetric representation of the scene.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/08/2020<br>      Modified by: Ravi&nbsp;Ramamoorthi</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599606207452_teaser--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599606207452_teaser--rgov-800width.jpg" title="Robust Energy Minimization for BRDF-Invariant Shape from Light Fields"><img src="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599606207452_teaser--rgov-66x44.jpg" alt="Robust Energy Minimization for BRDF-Invariant Shape from Light Fields"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our method accurately recovers the 3D shape of an object from an image taken with a light field camera, even if the object has complex textures and reflectance.  This provides a new capability to robustly enable 3D photography, and invariant to surface reflectance.</div> <div class="imageCredit">Robust Energy Minimization for BRDF-Invariant Shape from Light Fields by Li, Xu, Ramamoorthi and Chandraker.  CVPR 2017.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ravi&nbsp;Ramamoorthi</div> <div class="imageTitle">Robust Energy Minimization for BRDF-Invariant Shape from Light Fields</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599606482911_lfmb--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599606482911_lfmb--rgov-800width.jpg" title="Light Field Blind Motion Deblurring"><img src="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599606482911_lfmb--rgov-66x44.jpg" alt="Light Field Blind Motion Deblurring"></a> <div class="imageCaptionContainer"> <div class="imageCaption">In photographing fast moving scenes, or when the camera shakes in manual photography, motion blur is common and a key photographic challenge is to minimize or enable deblurring.  We show theoretically and practically that light field cameras enable more effective motion deblurring.</div> <div class="imageCredit">Light Field Motion Deblurring by Srinivasan, Ng and Ramamoorthi, CVPR 2017</div> <div class="imageSubmitted">Ravi&nbsp;Ramamoorthi</div> <div class="imageTitle">Light Field Blind Motion Deblurring</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599607056759_teaser_highres--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599607056759_teaser_highres--rgov-800width.jpg" title="Light Field Video Capture Using a Learning-Based Hybrid Imaging System"><img src="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599607056759_teaser_highres--rgov-66x44.jpg" alt="Light Field Video Capture Using a Learning-Based Hybrid Imaging System"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Light Field Video is important, but data rates limit it to 3fps.  We show that by combining this data with a conventional video camera, we can achieve light field video at 30fps.  This enables exciting applications such as interactively refocused video or focus tracking.</div> <div class="imageCredit">Wang, Zhu, Kalantari, Efros, Ramamoorthi.  Light Field Video Capture Using a Learning-Based Hybrid Imaging System, SIGGRAPH 2017</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ravi&nbsp;Ramamoorthi</div> <div class="imageTitle">Light Field Video Capture Using a Learning-Based Hybrid Imaging System</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599608013394_llff--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599608013394_llff--rgov-800width.jpg" title="Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines"><img src="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599608013394_llff--rgov-66x44.jpg" alt="Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We develop a new efficient and robust method to capture light fields from a sparse set of images taken with a standard mobile phone.  Our method enables a simple app that tells one to take casually captured images on a semi-regular grid, then view the scene in augmented reality on the mobile device.</div> <div class="imageCredit">Mildenhall, Srinivasan, Ortiz-Cayon, Kalantari, Ramamoorthi, Ng, Kar.  Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines.  SIGGRAPH 2019</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ravi&nbsp;Ramamoorthi</div> <div class="imageTitle">Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599608355883_saibi_eccv20--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599608355883_saibi_eccv20--rgov-800width.jpg" title="Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images"><img src="/por/images/Reports/POR/2020/1617234/1617234_10441592_1599608355883_saibi_eccv20--rgov-66x44.jpg" alt="Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images"></a> <div class="imageCaptionContainer"> <div class="imageCaption">From only a sparse sampling of the views of a light field, one can reconstruct relightable scene representations, which enable one to change both viewpoint and lighting.  The most realistic results are obtained using a volumetric representation of the scene.</div> <div class="imageCredit">Bi, Xu, Sunkavalli, Hasan, Hold-Geoffroy, Kriegman, Ramamoorthi.  Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images.  ECCV 2020</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ravi&nbsp;Ramamoorthi</div> <div class="imageTitle">Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project dealt with using light field cameras for detailed shape and reflectance capture.  Light Field cameras store the full distribution of light in the scene.  A conventional camera captures a 2D image, with each point or each pixel considering the combined light over the entire aperture of the camera.  On the other hand, the light field camera also records the direction of incident light, capturing a 4D function.  In effect, it's like an array of tiny micro-cameras along the aperture of the main lens.  This enables novel photographic effects like refocusing after capture, but this project sought to investigate other applications of light field cameras, including their ability to capture the detailed shape and reflectance of objects, to allow the objects to be included in new scenes and composed with computer-generated imagery.  The key results of this project are with reference to the uploaded images:  1. We have developed a method to use light field cameras to capture the shape of even objects that have sharp highlights and complex reflectance.  This is a new robust method, leveraging a solution that is invariant to the reflectance properties.  In this example, we show that the turtle shape is accurately recovered by our method from the image taken with a light field camera.  2. Light Fields enable new types of applications.  Here, we demonstrate that we can deblur the motion in an image captured with a light field camera significantly more effectively than when the image is captured by a conventional camera.  Note that motion blur and motion deblurring is a key aspect of photography, especially for fast-moving scenes.  3. While much of this project focuses on photographs taken with a light field camera, even greater opportunity is enabled by light field video.  However, the amount of data generated is too large for light field cameras to capture 60fps video.  We show that by coupling a conventional video camera with a light field camera, we can obtain 60fps light field video.  This enables exciting capabilities like refocusing the video while it is playing, changing one's viewpoint slightly or focus tracking.  4. Increasingly, the world is moving to imaging on mobile phones.  As such, we seek methods to acquire light fields from simple mobile phone captures, and ultimately to leverage the multiple mobile phone cameras as a light field camera.  We are able to show that with very few images casually captured in a semi-regular grid with a standard cell phone, we can create a light field and enable view synthesis with robust sampling guidelines for how far apart and where new images should be captured.  5. The ability to capture light fields from sparse images opens up the door to capture detailed shape and reflectance from a set of sparse images on a mobile phone.  This not just allows one to view the scene from new directions, but also to relight objects in the scene.  We show that we can enable relightable scene reconstructions from a sparse set of input images with greatest realism obtained by using a volumetric representation of the scene.                      Last Modified: 09/08/2020       Submitted by: Ravi Ramamoorthi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
