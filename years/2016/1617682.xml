<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Extending Verb Semantics with Causality towards Physical World</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>485350.00</AwardTotalIntnAmount>
<AwardAmount>493350</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With the emergence of a new generation of cognitive robots, the capability to communicate with these robots using natural language has become increasingly important.  Verbal communication often involves the use of verbs, for example, to ask a robot to perform some tasks or to monitor some physical activities. Concrete action verbs often denote some change of state as a result of an action; for example, "slice a pizza" implies the state of the object pizza will be changed from one piece to several smaller pieces. The change of state can be perceived from the physical world through different sensors. Given a human utterance, if the robot can anticipate the potential change of the state signaled by the verbs, it can then actively sense the environment and better connect language with the perceived physical world such as who performs the action and what objects and locations are involved.  This improved connection will benefit many applications relying on human-robot communication.  Through a cognitive robot, this project will bring new educational experiences to K-12 students and encourage broader participation in engineering. &lt;br/&gt; &lt;br/&gt;This research project develops novel causality models for concrete action verbs to capture intended change of state of the physical world. It augments meanings of concrete verbs based on how they might change the environment (i.e., causality) and meanings of concrete nouns based on how they might be changed by actions (i.e., affordance).  It incorporates causality models into learning and inference algorithms for grounding language to the physical world.  This work will provide a new dimension to connect verb semantics to perception and action.  Verb causality models will allow the robot to predict potential change of state from human linguistic utterances. This prediction will provide top-down information to guide visual processing and action modeling.</AbstractNarration>
<MinAmdLetterDate>06/03/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617682</AwardID>
<Investigator>
<FirstName>Joyce</FirstName>
<LastName>Chai</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joyce Y Chai</PI_FULL_NAME>
<EmailAddress>chaijy@umich.edu</EmailAddress>
<PI_PHON>7347648505</PI_PHON>
<NSF_ID>000477137</NSF_ID>
<StartDate>06/03/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488241226</ZipCode>
<StreetAddress><![CDATA[428 S. Shaw Lane, Room 2136]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~485350</FUND_OBLG>
<FUND_OBLG>2017~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>With the emergence of a new generation of cognitive robots, the capability to communicate with these robots using natural language has become increasingly important. Verbal communication often involves the use of verbs, for example, to ask a robot to perform some actions or to monitor some physical activities. Linguistic studies have shown that concrete action verbs often denote some change of state as a result of an action; for example, "slice a pizza" implies the state of the object pizza will be changed from one piece to several smaller pieces. If the robot has some knowledge about this state change, it can anticipate the potential change of the state implied by the verbs and help better perceive from the environment and make plans to act to the environment.</p> <p>Motivated by these ideas, this project has developed novel causality models for concrete action verbs to capture intended change of state of the physical world (i.e., physical causality). First, this project has identified a set of eighteen main categories to characterize physical causality and defined a set of change-of-state detectors focusing on visual perception. &nbsp;These detectors are incorporated into computer systems that ground language to perception, for example, to ground &ldquo;the man&rdquo;, &ldquo;a cup&rdquo;, and &ldquo;the shelf&rsquo;&rsquo; from the utterance &ldquo;the man is taking a cup from the shelf&rsquo;&rsquo; to the perceived objects in the environment. Empirical results have demonstrated a significant improvement in language grounding with an overall 46%-56% performance gain compared to previous approaches.&nbsp;</p> <p>This project has further explored approaches where humans can teach the robot new actions through language communication, incorporating the notion of physical causality of verbs. A verb in a language instruction is represented by a hypothesis space of fluents (i.e., desired goal states) of the physical world, which is incrementally acquired and updated through interaction with the human and the environment. To cope with uncertainties from the environment, this work has incorporated interactive question answering by allowing the robot to ask different kinds of questions during the learning process. Our empirical results have shown that this interactive learning process leads to more reliable representations of state-based verb semantics, which contribute to significantly better action performance in new situations. Through incremental learning and acquisition, results from this work have provided some building blocks towards life-long learning from humans.</p> <p>This project also initiated a new investigation on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. Based on a new dataset collected for this investigation, this project explored ways that harness web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can potentially be used to complement a small number of seed for model learning. This opens up new possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans.</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/04/2021<br>      Modified by: Joyce&nbsp;Y&nbsp;Chai</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ With the emergence of a new generation of cognitive robots, the capability to communicate with these robots using natural language has become increasingly important. Verbal communication often involves the use of verbs, for example, to ask a robot to perform some actions or to monitor some physical activities. Linguistic studies have shown that concrete action verbs often denote some change of state as a result of an action; for example, "slice a pizza" implies the state of the object pizza will be changed from one piece to several smaller pieces. If the robot has some knowledge about this state change, it can anticipate the potential change of the state implied by the verbs and help better perceive from the environment and make plans to act to the environment.  Motivated by these ideas, this project has developed novel causality models for concrete action verbs to capture intended change of state of the physical world (i.e., physical causality). First, this project has identified a set of eighteen main categories to characterize physical causality and defined a set of change-of-state detectors focusing on visual perception.  These detectors are incorporated into computer systems that ground language to perception, for example, to ground "the man", "a cup", and "the shelf’’ from the utterance "the man is taking a cup from the shelf’’ to the perceived objects in the environment. Empirical results have demonstrated a significant improvement in language grounding with an overall 46%-56% performance gain compared to previous approaches.   This project has further explored approaches where humans can teach the robot new actions through language communication, incorporating the notion of physical causality of verbs. A verb in a language instruction is represented by a hypothesis space of fluents (i.e., desired goal states) of the physical world, which is incrementally acquired and updated through interaction with the human and the environment. To cope with uncertainties from the environment, this work has incorporated interactive question answering by allowing the robot to ask different kinds of questions during the learning process. Our empirical results have shown that this interactive learning process leads to more reliable representations of state-based verb semantics, which contribute to significantly better action performance in new situations. Through incremental learning and acquisition, results from this work have provided some building blocks towards life-long learning from humans.  This project also initiated a new investigation on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. Based on a new dataset collected for this investigation, this project explored ways that harness web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can potentially be used to complement a small number of seed for model learning. This opens up new possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans.          Last Modified: 02/04/2021       Submitted by: Joyce Y Chai]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
