<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Distributed-Computing: What Does it Compute?</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rahul Shah</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>In this project, the PI, Prof. Eli Gafni, proposes to develop guidelines, adapted from the PI's research in area of Theoretical Distributed-Computing, and apply them as guidelines to the area of Distributed-Systems Design. If successful, building distributed systems can become less of an ad-hoc process. It will push the designer to answer questions like ``what-if'' and ``why-not'' akin, but not as rudimentary, to the success of check-lists in the practice of medicine. The PI will collaborate with practitioners to test and tweak the development of these guidelines, as well as collaborate with colleagues of teach software/system-design to test such guidelines in class. This project will also advance workforce development through the training of graduate students, as well as through education activities for undergraduates and graduate students.&lt;br/&gt;&lt;br/&gt;The project is based on the realization within the Theoretical Distributed-Computing community that ``one-shot tasks are to distributed-computing what functions are to centralized-computing.'' This has contributed to the PI's research immensely by pushing the PI to ask the right questions. The PI feels, as already demonstrated on small scale, that pushing a designer to identify the core tasks the system solved, will not be unlike pushing a centralized system designer to realize that the system designed solves an NP-complete problem. Such realization pushes a designer to expose hidden undocumented assumptions, and expose more avenues the design might have taken, that need to be justified if they are to be abandoned.</AbstractNarration>
<MinAmdLetterDate>09/06/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1655166</AwardID>
<Investigator>
<FirstName>Eliezer</FirstName>
<LastName>Gafni</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eliezer M Gafni</PI_FULL_NAME>
<EmailAddress>eli@cs.ucla.edu</EmailAddress>
<PI_PHON>3107953211</PI_PHON>
<NSF_ID>000316763</NSF_ID>
<StartDate>09/06/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[UCLA Computer Science Dept.]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951596</ZipCode>
<StreetAddress><![CDATA[Box 951596, 3731F Boelter Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7934</Code>
<Text>PARAL/DISTRIBUTED ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Distributed Algorithms:</p> <p>Unlike Centralized Algorithms which are executed on&nbsp; single computer and the main concern is efficiency in solving a Mathematical Problem, Distributed Algorithms are about few computers coordinating their actions. The most famous coordination action is that of reaching consensus among computers with competing agendas. But coordination may come in many ways most of them not equivalent to the strongest coordination, that of consensus.</p> <p>To coordinate computers need to communicate. Communication may happen in two main mechanisms. One is Message Passing, and the other is through ``writing on the board'' that is, applying actions to shared objects. In 1985 with a celebrated result called FLP it became clear that under only the most stringent conditions can consensus be achieved. This gave rise to the question what can be achieved under less that the most stringent condition?&nbsp;</p> <p>This gave rise to the area in distributed computing called computability. What conditions are required minimally to solve a given problem. The next question than is if we identify conditions with models are there infinitely many models, and do these collections of models differ if communication is through message passing or shared objects.</p> <p>The proposal was to show that the models can be characterized by their set-consensus power immaterial if communication is accomplished through Message passing or Shared-Objects. We knew that in Shared Objects the only ones that make sense are those called deterministic. But classifyingShared Objects models through set consensus power faced major hurdle which has not been resolved since it was realized in 1994. The hurdle is to show that the power classification makes sense in that it is invariant to the number of shared object used. This was called the Robustness problem.</p> <p>We were delayed by this question and we resolved it just for consensus rather than set consensus. We fell short of the grand objective. But we accomplished some side results.&nbsp;</p> <p>We showed the existence of pure set consensus power in the context of shared objects as we know this power exists in the message passing realm.</p> <p>And, we applied our knowledge of distributed algorithms a proof methodology to improve and prove algorithms in the Stellar Block-Chain system - a work of cooperation between the project and Prof David Mazieres&nbsp; from Stanford who is the CTO of Stellar.</p> <p>As for the Verification Aspect:&nbsp;</p> <pre class="aLF-aPX-K0-aPE">Computer-assisted design (CAD) software allows civil engineers to predict the behavior of a bridge under the constraints of traffic and weather to ensure that it will be safe. Similarly, formal verification tools allow software engineers to predict the behavior of software and make sure that the software will operate safely. However, unlike CAD software for bridges, formal verification tools are still not widely available to practitioners, which translates into undetected bugs affecting the end user.  Most existing verification tools can only be used effectively by academic experts in the field, which makes their use in the industry prohibitively expensive or just impossible, as there are only a few experts available to serve the market. One reason which makes formal verification tools so hard to use in practice is that, although those tools try to help the user by performing automatic mathematical reasoning to predict software behavior, they succeed or fail for reasons that are obscure to the non-expert. When such verification tools fail to produce a verdict about the conforming or non-conforming behavior of software, the user has no information to correct the problem and make the tool work. Ideally, formal verification tools would be transparent and explain the reason for their failure to the user.  Another problem is that software verification tools are unstable: seemingly trivial changes to the software being verified can render the tools unusable when it worked well before those changes, again for reasons that are obscure to the user. The opaque failures and the instability of state-of-the-art formal verification are often cited as the number one obstacle to their productive use and adoption in software engineering practice.  To curb the problems of opaque failures and instability of formal verification tools, we have pursued an approach based on decidable logics. Crucially, decidable logics have the property that there is an algorithm, called a decision procedure, that can decide the satisfiability of every proposition. Moreover, whether a proposition belongs to a decidable logic is easy to check and can be explained to the user. This guarantees that a proposition about software correctness can either be decided (true or not true) or that it falls outside the decidable logics of interest, with an explanation as to why this is the case.  We have shown that in many interesting cases pertaining to the verification of distributed systems, decidable logics form the basis for effective verification tools that do not suffer from the opaque failure and instability problems. </pre> <p>&nbsp;</p><br> <p>            Last Modified: 12/27/2018<br>      Modified by: Eliezer&nbsp;M&nbsp;Gafni</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Distributed Algorithms:  Unlike Centralized Algorithms which are executed on  single computer and the main concern is efficiency in solving a Mathematical Problem, Distributed Algorithms are about few computers coordinating their actions. The most famous coordination action is that of reaching consensus among computers with competing agendas. But coordination may come in many ways most of them not equivalent to the strongest coordination, that of consensus.  To coordinate computers need to communicate. Communication may happen in two main mechanisms. One is Message Passing, and the other is through ``writing on the board'' that is, applying actions to shared objects. In 1985 with a celebrated result called FLP it became clear that under only the most stringent conditions can consensus be achieved. This gave rise to the question what can be achieved under less that the most stringent condition?   This gave rise to the area in distributed computing called computability. What conditions are required minimally to solve a given problem. The next question than is if we identify conditions with models are there infinitely many models, and do these collections of models differ if communication is through message passing or shared objects.  The proposal was to show that the models can be characterized by their set-consensus power immaterial if communication is accomplished through Message passing or Shared-Objects. We knew that in Shared Objects the only ones that make sense are those called deterministic. But classifyingShared Objects models through set consensus power faced major hurdle which has not been resolved since it was realized in 1994. The hurdle is to show that the power classification makes sense in that it is invariant to the number of shared object used. This was called the Robustness problem.  We were delayed by this question and we resolved it just for consensus rather than set consensus. We fell short of the grand objective. But we accomplished some side results.   We showed the existence of pure set consensus power in the context of shared objects as we know this power exists in the message passing realm.  And, we applied our knowledge of distributed algorithms a proof methodology to improve and prove algorithms in the Stellar Block-Chain system - a work of cooperation between the project and Prof David Mazieres  from Stanford who is the CTO of Stellar.  As for the Verification Aspect:  Computer-assisted design (CAD) software allows civil engineers to predict the behavior of a bridge under the constraints of traffic and weather to ensure that it will be safe. Similarly, formal verification tools allow software engineers to predict the behavior of software and make sure that the software will operate safely. However, unlike CAD software for bridges, formal verification tools are still not widely available to practitioners, which translates into undetected bugs affecting the end user.  Most existing verification tools can only be used effectively by academic experts in the field, which makes their use in the industry prohibitively expensive or just impossible, as there are only a few experts available to serve the market. One reason which makes formal verification tools so hard to use in practice is that, although those tools try to help the user by performing automatic mathematical reasoning to predict software behavior, they succeed or fail for reasons that are obscure to the non-expert. When such verification tools fail to produce a verdict about the conforming or non-conforming behavior of software, the user has no information to correct the problem and make the tool work. Ideally, formal verification tools would be transparent and explain the reason for their failure to the user.  Another problem is that software verification tools are unstable: seemingly trivial changes to the software being verified can render the tools unusable when it worked well before those changes, again for reasons that are obscure to the user. The opaque failures and the instability of state-of-the-art formal verification are often cited as the number one obstacle to their productive use and adoption in software engineering practice.  To curb the problems of opaque failures and instability of formal verification tools, we have pursued an approach based on decidable logics. Crucially, decidable logics have the property that there is an algorithm, called a decision procedure, that can decide the satisfiability of every proposition. Moreover, whether a proposition belongs to a decidable logic is easy to check and can be explained to the user. This guarantees that a proposition about software correctness can either be decided (true or not true) or that it falls outside the decidable logics of interest, with an explanation as to why this is the case.  We have shown that in many interesting cases pertaining to the verification of distributed systems, decidable logics form the basis for effective verification tools that do not suffer from the opaque failure and instability problems.           Last Modified: 12/27/2018       Submitted by: Eliezer M Gafni]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
