<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Collaborative Research: F: Foundations of Nonconvex Problems in BigData Science and Engineering: Models, Algorithms, and Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>349999.00</AwardTotalIntnAmount>
<AwardAmount>349999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Victor Roytburd</SignBlockName>
<PO_EMAI>vroytbur@nsf.gov</PO_EMAI>
<PO_PHON>7032928584</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In today's digital world, huge amounts of data, i.e., big data, can be found in almost every aspect of scientific research and human activity.  These data need to be managed effectively for reliable prediction and inference to improve decision making.  Statistical learning is an emergent scientific discipline wherein mathematical modeling, computational algorithms, and statistical analysis are jointly employed to address these challenging data management problems.  Invariably, quantitative criteria need to be introduced for the overall learning process in order to gauge the quality of the solutions obtained. This research focuses on two important criteria: data fitness and sparsity representation of the underlying learning model.  Potential applications of the results can be found in computational statistics, compressed sensing, imaging, machine learning, bio-informatics, portfolio selection, and decision making under uncertainty, among many areas involving big data.&lt;br/&gt;&lt;br/&gt;Till now, convex optimization has been the dominant methodology for statistical learning in which the two criteria employed are expressed by convex functions either to be optimized and/or set as constraints of the variables being sought.  Recently, non-convex functions of the difference-of-convex (DC) type and the difference-of-convex algorithm (DCA) have been shown to yield superior results in many contexts and serve as the motivation for this project.  The goal is to develop a solid foundation and a unified framework to address many fundamental issues in big data problems in which non-convexity and non-differentiability are present in the optimization problems to be solved. These two non-standard features in computational statistical learning are challenging and their rigorous treatment requires the fusion of expertise from different domains of mathematical sciences.  Technical issues to be investigated will cover the optimality, sparsity, and statistical properties of computable solutions to the non-convex, non-smooth optimization problems arising from statistical learning and its many applications.  Novel algorithms will be developed and tested first on synthetic data sets for preliminary experimentation and then on publicly available data sets for realism; comparisons will be made among different formulations of the learning problems.</AbstractNarration>
<MinAmdLetterDate>08/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/17/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1632935</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Xin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack Xin</PI_FULL_NAME>
<EmailAddress>jxin@math.uci.edu</EmailAddress>
<PI_PHON>9498245309</PI_PHON>
<NSF_ID>000181369</NSF_ID>
<StartDate>08/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926973875</ZipCode>
<StreetAddress><![CDATA[Rowland Hall room 540E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~349999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><pre class="western"><br /></pre> <p><span><span style="font-size: medium;"><span style="color: #000000;">The project studied nonconvex and nonsmooth&nbsp;optimization problems arising in statistical learning&nbsp;and deep learning.&nbsp; A central issue is to find&nbsp;solutions with discrete characteristics&nbsp; such as&nbsp;solutions in sparse and quantized forms&nbsp;so that direct application of gradient descent&nbsp;is not feasible. We developed a notion of&nbsp;coarse gradient and showed that it guided&nbsp;the optimization to local or global minima,&nbsp;rigorously in training simplified neural networks&nbsp;&nbsp;and computationally in training the state of the art&nbsp;deep networks on large image data&nbsp;sets.&nbsp; We devised and analyzed convergence&nbsp;of efficient variable splitting algorithms to&nbsp;enforce sparsity and group sparsity on&nbsp;network weights during training to enable structured pruning&nbsp;and speed-up of deep network inference.&nbsp;We discovered an exact differentiable penalty&nbsp;to recover permutation matrices and&nbsp;demonstrated its effectiveness in automating shuffle-net,&nbsp;a cutting-edge light weight deep convolutional network.&nbsp;We successfully coupled and trained&nbsp;differential equation epidemic models&nbsp;with recurrent neural networks on&nbsp;graph structured spatio-temporal data and&nbsp;applied our framework to Covid-19 infection data.&nbsp; &nbsp;<br /><br />The computational tools studied in the project advanced our understanding and capability of solving high dimensional non-smooth optimization problems,&nbsp; contributing to a broad range of applications such as mobile artificial intelligence, threat detection, data guided decision making, disease forecasting, and data driven low dimensional approximations of complex physical models in turbulent diffusion and combustion. The project provides systematic and hands-on training of graduate students towards advanced degrees in computational mathematics and careers in industry and academia. The computational and mathematical methodologies developed in the project help advance information technology, physical and data sciences, as well as benefit the country and the well-being of general public in the digital age. </span></span></span></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/28/2020<br>      Modified by: Jack&nbsp;Xin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The project studied nonconvex and nonsmooth optimization problems arising in statistical learning and deep learning.  A central issue is to find solutions with discrete characteristics  such as solutions in sparse and quantized forms so that direct application of gradient descent is not feasible. We developed a notion of coarse gradient and showed that it guided the optimization to local or global minima, rigorously in training simplified neural networks  and computationally in training the state of the art deep networks on large image data sets.  We devised and analyzed convergence of efficient variable splitting algorithms to enforce sparsity and group sparsity on network weights during training to enable structured pruning and speed-up of deep network inference. We discovered an exact differentiable penalty to recover permutation matrices and demonstrated its effectiveness in automating shuffle-net, a cutting-edge light weight deep convolutional network. We successfully coupled and trained differential equation epidemic models with recurrent neural networks on graph structured spatio-temporal data and applied our framework to Covid-19 infection data.     The computational tools studied in the project advanced our understanding and capability of solving high dimensional non-smooth optimization problems,  contributing to a broad range of applications such as mobile artificial intelligence, threat detection, data guided decision making, disease forecasting, and data driven low dimensional approximations of complex physical models in turbulent diffusion and combustion. The project provides systematic and hands-on training of graduate students towards advanced degrees in computational mathematics and careers in industry and academia. The computational and mathematical methodologies developed in the project help advance information technology, physical and data sciences, as well as benefit the country and the well-being of general public in the digital age.              Last Modified: 12/28/2020       Submitted by: Jack Xin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
