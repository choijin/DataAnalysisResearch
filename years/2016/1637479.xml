<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Collaborative Research: Experiential Learning for Robots: From Physics to Actions to Tasks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2016</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>752000.00</AwardTotalIntnAmount>
<AwardAmount>760000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.&lt;br/&gt;&lt;br/&gt;The proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception.</AbstractNarration>
<MinAmdLetterDate>08/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/15/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1637479</AwardID>
<Investigator>
<FirstName>Dieter</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dieter Fox</PI_FULL_NAME>
<EmailAddress>fox@cs.washington.edu</EmailAddress>
<PI_PHON>2066852517</PI_PHON>
<NSF_ID>000210667</NSF_ID>
<StartDate>08/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ali</FirstName>
<LastName>Farhadi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ali Farhadi</PI_FULL_NAME>
<EmailAddress>afarhad2@gmail.com</EmailAddress>
<PI_PHON>2065431695</PI_PHON>
<NSF_ID>000611236</NSF_ID>
<StartDate>08/17/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952500</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave N.E.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~752000</FUND_OBLG>
<FUND_OBLG>2018~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Developing robots that are able to reliably act and interact with humans in an unstructured world requires substantial advances in perception-based control, and in the ability to learn new tasks from a small number of examples. Unfortunately, it is extremely difficult, if not impossible, to program all necessary robot capabilities by hand. The goal of this project was to overcome the limitations of manually designing perception, control, and reasoning abilities by developing an experiential learning framework that enables robots to learn general models of their environments from raw sensor information. Specifically, our framework aimed at learning such models about how rigid objects move when manipulated by a robot and how liquids behave when being poured from a container.&nbsp;</p> <p>We developed deep learning techniques that enabled a robot manipulator to reason about objects and their physical interactions purely from its own experience, without requiring human data annotation. Regarding liquids, our model enabled a robot to pour certain amounts of liquid by observing the flow of liquid into a target container. Here, the robot learned to detect even transparent liquids such as water using its color camera.&nbsp;</p> <p>Learning robust representations from weakly labeled data is an important question for robots. In follow up work we showed how visual representations can be learned from raw video data, enabling us to extract features from images that are highly robust and readily applicable to a variety of semantic image understanding tasks. This work has the potential to enable robots to interpret their visual data in a more robust way, without requiring large amounts of labeled data.&nbsp;</p> <p>Learning to understand human language to reason about their environments and execute complex tasks is another important ability for robots. Toward this goal, we developed ALFRED, a very large scale dataset and training environment that enables the research community to develop and benchmark different techniques. We also provided strong baseline methods that show how language input can be interpreted automatically to guide a robot to perform complex manipulation tasks.&nbsp;</p> <p>Overall, this project demonstrates how a robot can learn predictive models of its environment from raw observation traces by incorporating physics-based structure into deep learning frameworks. Such an approach can be used in various other tasks, ultimately leading to robots that learn to understand and control their environments in a fully data driven, robust way. The techniques also demonstrate how high level understanding can be learned from simple, low-level supervision. This can shed light on how robust Artificial Intelligence systems could be developed, also making strong connections to the discipline of cognitive science.&nbsp;</p> <p>Learning models from raw data without explicit supervision has the potential to provide far more robust and capable robots operating in complex, open-ended environments. This can have significant impact on robots being able to help people with disabilities, elderly citizens, or in smart manufacturing.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/05/2021<br>      Modified by: Dieter&nbsp;Fox</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1637479/1637479_10451339_1614791682025_Fox_3--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1637479/1637479_10451339_1614791682025_Fox_3--rgov-800width.jpg" title="Robot pouring liquids"><img src="/por/images/Reports/POR/2021/1637479/1637479_10451339_1614791682025_Fox_3--rgov-66x44.jpg" alt="Robot pouring liquids"></a> <div class="imageCaptionContainer"> <div class="imageCaption">UW Baxter robot performing a liquid pouring experiment. The robot pours desired amounts into target containers using its ability to detect and keep track of the liquid.</div> <div class="imageCredit">Connor Schenck</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Dieter&nbsp;Fox</div> <div class="imageTitle">Robot pouring liquids</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1637479/1637479_10451339_1614791818731_Fox_1--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1637479/1637479_10451339_1614791818731_Fox_1--rgov-800width.jpg" title="Language-based task execution"><img src="/por/images/Reports/POR/2021/1637479/1637479_10451339_1614791818731_Fox_1--rgov-66x44.jpg" alt="Language-based task execution"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The training data enables agents to learn to understand complex language commands and execute  them in 3D simulated environments.</div> <div class="imageCredit">Mohit Shridhar</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Dieter&nbsp;Fox</div> <div class="imageTitle">Language-based task execution</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Developing robots that are able to reliably act and interact with humans in an unstructured world requires substantial advances in perception-based control, and in the ability to learn new tasks from a small number of examples. Unfortunately, it is extremely difficult, if not impossible, to program all necessary robot capabilities by hand. The goal of this project was to overcome the limitations of manually designing perception, control, and reasoning abilities by developing an experiential learning framework that enables robots to learn general models of their environments from raw sensor information. Specifically, our framework aimed at learning such models about how rigid objects move when manipulated by a robot and how liquids behave when being poured from a container.   We developed deep learning techniques that enabled a robot manipulator to reason about objects and their physical interactions purely from its own experience, without requiring human data annotation. Regarding liquids, our model enabled a robot to pour certain amounts of liquid by observing the flow of liquid into a target container. Here, the robot learned to detect even transparent liquids such as water using its color camera.   Learning robust representations from weakly labeled data is an important question for robots. In follow up work we showed how visual representations can be learned from raw video data, enabling us to extract features from images that are highly robust and readily applicable to a variety of semantic image understanding tasks. This work has the potential to enable robots to interpret their visual data in a more robust way, without requiring large amounts of labeled data.   Learning to understand human language to reason about their environments and execute complex tasks is another important ability for robots. Toward this goal, we developed ALFRED, a very large scale dataset and training environment that enables the research community to develop and benchmark different techniques. We also provided strong baseline methods that show how language input can be interpreted automatically to guide a robot to perform complex manipulation tasks.   Overall, this project demonstrates how a robot can learn predictive models of its environment from raw observation traces by incorporating physics-based structure into deep learning frameworks. Such an approach can be used in various other tasks, ultimately leading to robots that learn to understand and control their environments in a fully data driven, robust way. The techniques also demonstrate how high level understanding can be learned from simple, low-level supervision. This can shed light on how robust Artificial Intelligence systems could be developed, also making strong connections to the discipline of cognitive science.   Learning models from raw data without explicit supervision has the potential to provide far more robust and capable robots operating in complex, open-ended environments. This can have significant impact on robots being able to help people with disabilities, elderly citizens, or in smart manufacturing.          Last Modified: 03/05/2021       Submitted by: Dieter Fox]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
