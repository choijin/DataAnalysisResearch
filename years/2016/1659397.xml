<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC* Network Design: The Bucknell Science DMZ Network Design and Implementation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>399200.00</AwardTotalIntnAmount>
<AwardAmount>399200</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Bucknell University with the support of the Pennsylvania-wide education and research network (KINBER) is building a next generation campus research network to enable secure transfer of large datasets across campus and between off-campus research partners.  Campus network infrastructure limitations emerge when such data transfers compete with other campus network traffic and further limitations prevent broad sharing of large data sets outside of the network to other potential collaborators.  Building on Bucknell's strong teacher-scholar model, new, quality, data-intensive, in-lab and in-classroom data transfer capabilities are accelerating student and faculty research, including off-campus data sharing collaborations.  &lt;br/&gt; &lt;br/&gt;This project creates a local area network optimized for high-performance scientific applications, a Science DMZ.  Key to the project's initial design is the addition of a data transfer node, combined with increased local network capacity, to improve Bucknell's ability to work with and exchange GIS, bioinformatics, and clinical data with partners within and outside of Bucknell, while preparing students for postgraduate work and study in data-intensive fields.  With special focus on improving network function to improve the throughput for larger data sets, the project will include improvements in border router technology and distribution/access nodes that support lab spaces on campus as well as the construction of a Science DMZ and incorporation of the data transfer nodes alongside the campus High Performance Computing Cluster (HPCC).  In addition, this project establishes network performance monitoring metrics, improves inter and intra-institutional data exchange capabilities, and strengthens current and future research support for faculty, students and their collaborators.</AbstractNarration>
<MinAmdLetterDate>06/27/2017</MinAmdLetterDate>
<MaxAmdLetterDate>02/14/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1659397</AwardID>
<Investigator>
<FirstName>Param</FirstName>
<LastName>Bedi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Param Bedi</PI_FULL_NAME>
<EmailAddress>param.bedi@bucknell.edu</EmailAddress>
<PI_PHON>5705773855</PI_PHON>
<NSF_ID>000728809</NSF_ID>
<StartDate>06/27/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Yerger</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark E Yerger</PI_FULL_NAME>
<EmailAddress>mark.yerger@bucknell.edu</EmailAddress>
<PI_PHON>5705773855</PI_PHON>
<NSF_ID>000729834</NSF_ID>
<StartDate>06/27/2017</StartDate>
<EndDate>02/14/2019</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Bernard</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher J Bernard</PI_FULL_NAME>
<EmailAddress>cjb034@bucknell.edu</EmailAddress>
<PI_PHON>5705773011</PI_PHON>
<NSF_ID>000797817</NSF_ID>
<StartDate>02/14/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Bucknell University</Name>
<CityName>LEWISBURG</CityName>
<ZipCode>178372111</ZipCode>
<PhoneNumber>5705773510</PhoneNumber>
<StreetAddress>One Dent Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003030335</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BUCKNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003030335</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Bucknell University]]></Name>
<CityName>Lewisburg</CityName>
<StateCode>PA</StateCode>
<ZipCode>178372005</ZipCode>
<StreetAddress><![CDATA[One Dent Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~399200</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Bucknell Science DMZ Network Design and Implementation project funded through NSF grant 1659397 sought to implement a dedicated network to support the research initiatives at a private liberal arts university. Bucknell prides itself not only on their curriculum and preparation of students but also opportunities for undergraduate students to participate in research combining the ideas learned in the classroom with real-life understanding.</p> <p>The installation and testing of core technologies in support of the Science DMZ were completed including the installation of 40 Gbps network backbone in support of the Science DMZ infrastructure and the Globus data management software enabling collaboration opportunities both within and external to the organization for research datasets. The implementation of Globus also served as the progenitor for the management of research data as it serves as the central nexus for file storage used for High-Performance Computing Storage (Elastifile), cloud storage (AWS) and long-term object storage (Swiftstack) as a tool to improve data management plans.</p> <p>Improved job processing and management of the Science DMZ equipment was accomplished through the deployment of Bright Computing&rsquo;s Bright Cluster Manager enabling better management for iterative jobs and job submissions as part of both faculty and student-based research. Bright Cluster Manager has also laid the foundations for the ability to burst to the cloud when workloads demand additional resources outside of those available in the current environment, or very specialized resources available in the cloud.</p> <p>Documentation and support for the improved new services and capabilities associated with the Science DMZ were created including BisonNet public website (<a href="https://www.bucknell.edu/bisonnet">https://www.bucknell.edu/bisonnet</a>), faculty outreach to key faculty research partners for testing and documentation.&nbsp;</p> <p>While documentation efforts will continue into the foreseeable future, sufficient testing has been conducted to move past the pilot phase of all Science DMZ service to full production this summer.</p> <p>Through the pilot phase of the Science DMZ implementation, we have executed 5451 jobs on the new High-Performance Computing Cluster that is a component of the Science DMZ and currently have stored 19.4 Tb of data on the High-Performance Computing Storage Platform and 35.6 Tb of storage on long-term object storage.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/10/2019<br>      Modified by: Christopher&nbsp;J&nbsp;Bernard</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Bucknell Science DMZ Network Design and Implementation project funded through NSF grant 1659397 sought to implement a dedicated network to support the research initiatives at a private liberal arts university. Bucknell prides itself not only on their curriculum and preparation of students but also opportunities for undergraduate students to participate in research combining the ideas learned in the classroom with real-life understanding.  The installation and testing of core technologies in support of the Science DMZ were completed including the installation of 40 Gbps network backbone in support of the Science DMZ infrastructure and the Globus data management software enabling collaboration opportunities both within and external to the organization for research datasets. The implementation of Globus also served as the progenitor for the management of research data as it serves as the central nexus for file storage used for High-Performance Computing Storage (Elastifile), cloud storage (AWS) and long-term object storage (Swiftstack) as a tool to improve data management plans.  Improved job processing and management of the Science DMZ equipment was accomplished through the deployment of Bright Computing?s Bright Cluster Manager enabling better management for iterative jobs and job submissions as part of both faculty and student-based research. Bright Cluster Manager has also laid the foundations for the ability to burst to the cloud when workloads demand additional resources outside of those available in the current environment, or very specialized resources available in the cloud.  Documentation and support for the improved new services and capabilities associated with the Science DMZ were created including BisonNet public website (https://www.bucknell.edu/bisonnet), faculty outreach to key faculty research partners for testing and documentation.   While documentation efforts will continue into the foreseeable future, sufficient testing has been conducted to move past the pilot phase of all Science DMZ service to full production this summer.  Through the pilot phase of the Science DMZ implementation, we have executed 5451 jobs on the new High-Performance Computing Cluster that is a component of the Science DMZ and currently have stored 19.4 Tb of data on the High-Performance Computing Storage Platform and 35.6 Tb of storage on long-term object storage.          Last Modified: 07/10/2019       Submitted by: Christopher J Bernard]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
