<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRCNS: Bayesian Modeling of Interacting Time Series to Discover Cortical Networks Associated with Auditory Processing Dysfunction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>800000.00</AwardTotalIntnAmount>
<AwardAmount>800000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Despite receiving normal audiological assessments, some listeners still complain to clinicians that they struggle to hear, particularly in noisy or crowded environments. However, systematic investigations into how the brain processes sound (and how it can go wrong) are lacking. The goal of this funded project is to apply novel statistical approaches to study patterns of activity in the brain while it processes sound in complex situations like when listening in a multi-talker environment.&lt;br/&gt;&lt;br/&gt;A wide variety of behavioral and electrophysiological responses will be collected under several different types of auditory stimulation. Behavioral data and physiological measures of brainstem response will be used to characterize individuals' hearing health in both monaural and binaural pathways, providing complementary information to their cortical magneto- and electroencephalography (M-EEG) responses during similar auditory tasks. Auditory attentional network connectivity will also be analyzed, to account for the neural underpinnings of aspects of auditory dysfunction such as the inability to maintain or switch attention between speakers. Using computationally-driven statistical approaches, flexible graphical model-based representations of high-dimensional time series will be learned, in order to characterize the auditory attentional network based on collected M-EEG data. Specifically, two computational aims will be tackled: 1) Construct Bayesian models to characterize dynamical cortical interactions at different spatial resolutions and 2) Develop models that infer connectivity structure at different canonical cortical rhythmic bands. This research program leverages the complementary expertise of the two investigators, bringing together auditory behavioral and systems neuroscience, with flexible and scalable statistical time series modeling approaches. Temporal structure is often ignored in big data analyses as well as in systems neuroscience, and this funded research will directly address this shortcoming by using a high-dimensional set of temporally continuous neural data. The cortical network discovered by this approach will enable neuroscientists to better understand the variability inherent in the auditory attentional network across both task types and individual differences in listening abilities.</AbstractNarration>
<MinAmdLetterDate>08/25/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1607468</AwardID>
<Investigator>
<FirstName>Emily</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emily B Fox</PI_FULL_NAME>
<EmailAddress>ebfox@uw.edu</EmailAddress>
<PI_PHON>2062219341</PI_PHON>
<NSF_ID>000514440</NSF_ID>
<StartDate>08/25/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Adrian</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adrian Lee</PI_FULL_NAME>
<EmailAddress>akclee@uw.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000631152</NSF_ID>
<StartDate>08/25/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington - Office of Sponsored Programs]]></Name>
<CityName>SEATTLE</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Avenue NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>7327</Code>
<Text>CRCNS</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~800000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-2c9690ac-7fff-5ed3-5169-990456ec19f0" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span id="docs-internal-guid-dfd87166-7fff-51ba-21d1-d0f881970dae" style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Despite passing standard hearing tests used by clinicians, some people still struggle to hear people talking in crowded restaurants or in other complex acoustic environments. We hypothesized that how different regions of the brains are connected functionally -- that is, how they interact with each other over time -- could explain some aspects of the inability to maintain or switch attention between speakers. In general, connectivity of the brain on short time scales (hundredths of seconds) has been underexplored in neuroscience. This is likely due to the challenge of modeling structured relationships between time series in a big data setting. In this project, we collected behavioral, physiological, and neuroimaging data from a sizable number of subjects with a wide range of listening abilities. We also invented scalable statistical models to infer connectivity within the auditory attentional network. The brain networks discovered by this approach will allow neuroscientists to better assess the auditory attentional network while accounting for individual differences frequently observed. We are also in the process of integrating the developed models and algorithms into an open-source, well-known, and widely used scientific software package (MNE-Python) so that this tool can be available for others interested in studying brain connectivity. We hope that our proposed mathematical framework can be translated to different clinical domains beyond audition. Findings of the project have also been made publicly available in numerous conference talks and research proceedings. </span></span></p><br> <p>            Last Modified: 09/10/2020<br>      Modified by: Adrian&nbsp;Lee</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Despite passing standard hearing tests used by clinicians, some people still struggle to hear people talking in crowded restaurants or in other complex acoustic environments. We hypothesized that how different regions of the brains are connected functionally -- that is, how they interact with each other over time -- could explain some aspects of the inability to maintain or switch attention between speakers. In general, connectivity of the brain on short time scales (hundredths of seconds) has been underexplored in neuroscience. This is likely due to the challenge of modeling structured relationships between time series in a big data setting. In this project, we collected behavioral, physiological, and neuroimaging data from a sizable number of subjects with a wide range of listening abilities. We also invented scalable statistical models to infer connectivity within the auditory attentional network. The brain networks discovered by this approach will allow neuroscientists to better assess the auditory attentional network while accounting for individual differences frequently observed. We are also in the process of integrating the developed models and algorithms into an open-source, well-known, and widely used scientific software package (MNE-Python) so that this tool can be available for others interested in studying brain connectivity. We hope that our proposed mathematical framework can be translated to different clinical domains beyond audition. Findings of the project have also been made publicly available in numerous conference talks and research proceedings.        Last Modified: 09/10/2020       Submitted by: Adrian Lee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
