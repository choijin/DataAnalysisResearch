<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Collaborative Research: Coupling Computation and Communication in FPGA-Enhanced Clouds and Clusters</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>224992.00</AwardTotalIntnAmount>
<AwardAmount>240992</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The introduction of Field Programmable Gate Arrays (FPGAs) to accelerate clusters of servers in datacenters and clouds provides a great, immediate opportunity to leverage a new technology in high-end computing. With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore?s Law world. Since the hardware adapts to the application higher efficiency can be achieved, and since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Large-scale communication can consequently proceed with both higher bandwidth, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. The proposed design allows for useful processing while data is in flight in the network resulting in reduced software overhead in parallel middleware and reduced network congestion. The key tenets of the research are to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially software overhead. &lt;br/&gt;The research project, FC5 (an FPGA framework for coupling communication and computation in clouds and clusters) has several thrusts. First, hardware support for FC5 and investigation of methods of configurability in FC5 to reduce communication latency and support computing in the network are studied. A second outcome is a prototype version of the Open MPI open source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations. Third, proof-of-concept versions of multiple FC5 software models, including direct hardware access, a transparent MPI-in-OpenCL, and an API-based mechanism that exposes essential functionality. Finally, because FC5 is evolving rapidly with major new announcements expected imminently, continued refinement is essential. At least two model applications, Molecular Dynamics and Map-Reduce, will be used as test cases. &lt;br/&gt;With the continued consolidation of computing services into the cloud, the potential broader impact is to increase both the scale and availability of parallel applications. The broad range of uses of cloud and cluster computing for commercial, government, and academic applications means that acceleration offered will have a widespread impact applicable across many sectors. The growing acceptance of high performance computing in industry (e.g., fast machine learning) is one particular potential commercial sector that will be enhanced by this project.</AbstractNarration>
<MinAmdLetterDate>06/03/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/16/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618303</AwardID>
<Investigator>
<FirstName>Martin</FirstName>
<LastName>Herbordt</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martin C Herbordt</PI_FULL_NAME>
<EmailAddress>herbordt@bu.edu</EmailAddress>
<PI_PHON>6173539850</PI_PHON>
<NSF_ID>000112548</NSF_ID>
<StartDate>06/03/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress><![CDATA[881 Commonwealth Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~224992</FUND_OBLG>
<FUND_OBLG>2017~8000</FUND_OBLG>
<FUND_OBLG>2018~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>High-performance computing as is done in large datacenters provides essential findings across all areas of science and engineering. It is critical to basic research, such as understanding disease processes, and development of new products, such as emerging classes of drugs. While these advances require ever more computing capability, this is become more difficult to obtain as the decades-long advances in process technology--following Moore&rsquo;s Law--taper off, and as high-end systems come to rely almost entirely on using commodity, rather than bespoke, components. As a result, high-end computing is running into computing limits, particularly in the amount of computing that can be brought to bear on any single problem.</p> <p>In this project the overall goal was to address the scalability problem by breaking the barrier between execution of communication and computation that is a consequence of current computer architectures. This was done by exploring the use of Field Programmable Gate Arrays (FPGAs). With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore&rsquo;s Law world. Two reasons are, first, since the hardware adapts to the application rather than the reverse, one generally obtains high efficiency. And second, since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Consequently, large-scale communication can proceed with higher flow rate, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. Reduced software overhead in parallel middleware and reduced network congestion result; the design also allows for useful processing while data is in flight in the network. The key tenets of the research were to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially the software overhead.&nbsp; We refer to this project as an FPGA framework for coupling communication and computation in clouds and clusters, or FC5.</p> <p>The research had five thrusts, all of which resulted in both fundamental findings of intellectual merit and practical extensions with broader impact. The latter include follow-on collaborations with various industrial and national lab partners.</p> <p>The first thrust, hardware support for FC5, resulted in new methods to improve communication performance and to support computing in the network (rather than just the processors at the endpoints). This thrust had several projects of which we highlight three. The first determined that complex operations could be performed in network switches to combine data in-flight, the methods for doing so, and that this can result in substantial speed-ups and scalability for critical applications. The second invented new ways to offload communication processing from software into hardware resulting in latency improvement of data transfers. The third created novel mappings of data compression onto FPGAs and demonstrated that a small number of components could substantially reduce the total amount of communication hardware required in the data center. Follow-on collaborations are with Intel, Xilinx, and Sandia National Lab.</p> <p>&nbsp;The second thrust resulted in a prototype version of the Open MPI open-source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations.&nbsp; Work continues with multiple partners.</p> <p>The third thrust explored methods of programming FC5 hardware that require no specialized knowledge. The problem addressed is the performance gap between ordinary programs and those hand-coded for FPGAs. It was found that much of this gap was the result of hidden policies within the compiler, and that most of this gap could be closed by applying simple transformations to the input code. This work continues in collaboration with Red Hat.</p> <p>The fourth thrust is the creation of model applications for use in evaluating the rapidly evolving FC5 design space. Both of these, Molecular Dynamics and Machine Learning, grew to become independent research projects, the first in collaboration with the drug discovery company Silicon Therapeutics, and the second with Pacific Northwest National Lab.</p> <p>The fifth thrust crosscut the other four: the creation of an FC5 testbed. This work has been extended and is a major part of the new Open Cloud Testbed, a new national research infrastructure being built, in part, to explore FC5 architectures, systems, and applications.</p> <p>The broader impacts, besides the technical broader impacts already described, included training and outreach. More than ten students were supported directly by this project, including five undergraduates and several members of under-represented groups. Two PhDs were granted to women.</p><br> <p>            Last Modified: 09/28/2020<br>      Modified by: Martin&nbsp;C&nbsp;Herbordt</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ High-performance computing as is done in large datacenters provides essential findings across all areas of science and engineering. It is critical to basic research, such as understanding disease processes, and development of new products, such as emerging classes of drugs. While these advances require ever more computing capability, this is become more difficult to obtain as the decades-long advances in process technology--following Moore’s Law--taper off, and as high-end systems come to rely almost entirely on using commodity, rather than bespoke, components. As a result, high-end computing is running into computing limits, particularly in the amount of computing that can be brought to bear on any single problem.  In this project the overall goal was to address the scalability problem by breaking the barrier between execution of communication and computation that is a consequence of current computer architectures. This was done by exploring the use of Field Programmable Gate Arrays (FPGAs). With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore’s Law world. Two reasons are, first, since the hardware adapts to the application rather than the reverse, one generally obtains high efficiency. And second, since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Consequently, large-scale communication can proceed with higher flow rate, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. Reduced software overhead in parallel middleware and reduced network congestion result; the design also allows for useful processing while data is in flight in the network. The key tenets of the research were to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially the software overhead.  We refer to this project as an FPGA framework for coupling communication and computation in clouds and clusters, or FC5.  The research had five thrusts, all of which resulted in both fundamental findings of intellectual merit and practical extensions with broader impact. The latter include follow-on collaborations with various industrial and national lab partners.  The first thrust, hardware support for FC5, resulted in new methods to improve communication performance and to support computing in the network (rather than just the processors at the endpoints). This thrust had several projects of which we highlight three. The first determined that complex operations could be performed in network switches to combine data in-flight, the methods for doing so, and that this can result in substantial speed-ups and scalability for critical applications. The second invented new ways to offload communication processing from software into hardware resulting in latency improvement of data transfers. The third created novel mappings of data compression onto FPGAs and demonstrated that a small number of components could substantially reduce the total amount of communication hardware required in the data center. Follow-on collaborations are with Intel, Xilinx, and Sandia National Lab.   The second thrust resulted in a prototype version of the Open MPI open-source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations.  Work continues with multiple partners.  The third thrust explored methods of programming FC5 hardware that require no specialized knowledge. The problem addressed is the performance gap between ordinary programs and those hand-coded for FPGAs. It was found that much of this gap was the result of hidden policies within the compiler, and that most of this gap could be closed by applying simple transformations to the input code. This work continues in collaboration with Red Hat.  The fourth thrust is the creation of model applications for use in evaluating the rapidly evolving FC5 design space. Both of these, Molecular Dynamics and Machine Learning, grew to become independent research projects, the first in collaboration with the drug discovery company Silicon Therapeutics, and the second with Pacific Northwest National Lab.  The fifth thrust crosscut the other four: the creation of an FC5 testbed. This work has been extended and is a major part of the new Open Cloud Testbed, a new national research infrastructure being built, in part, to explore FC5 architectures, systems, and applications.  The broader impacts, besides the technical broader impacts already described, included training and outreach. More than ten students were supported directly by this project, including five undergraduates and several members of under-represented groups. Two PhDs were granted to women.       Last Modified: 09/28/2020       Submitted by: Martin C Herbordt]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
