<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC* Networking Infrastructure: Improving Network Infrastructure to Enable Large Scale Scientific Data Flows and Collaboration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2017</AwardEffectiveDate>
<AwardExpirationDate>02/29/2020</AwardExpirationDate>
<AwardTotalIntnAmount>499646.00</AwardTotalIntnAmount>
<AwardAmount>499646</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Campus networks are required to protect information and inspect data flows to safeguard security and privacy. However, researchers need open and unfettered access to large data flows and instruments across the globe to reduce time to discovery.  The Rice University network is a shared resource that not only needs to support the administrative and teaching functions but also enable scientists to use that network in new and innovative ways for research.  Five key data-intensive application teams act as drivers of the new extension of network functionality and are providing feedback to the technical design staff.  These application areas include earth and atmospheric sciences; urban data science; computational biosciences and neuroengineering; particle physics and distributed cluster computing.  These applications build on long-term science investments aimed, amongst others, at understanding seismic events, the weather patterns in the Gulf regions and beyond, as well as urban trends in large, diverse cities such as Houston, TX. &lt;br/&gt;&lt;br/&gt;The basic model adopted by the project is "the science DMZ." A Science DMZ is "a portion of the network, built at or near the campus local network perimeter designed so that equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose business systems." This approach allows Rice to aggressively upgrade its network capacity for greatly enhanced science data access. This project supports 100 GB/s flows between the data transfer facilities at our off-campus data center and national and international R&amp;E data repositories and takes advantage of SDN (Openflow) mechanisms.</AbstractNarration>
<MinAmdLetterDate>12/08/2016</MinAmdLetterDate>
<MaxAmdLetterDate>12/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1659348</AwardID>
<Investigator>
<FirstName>Moshe</FirstName>
<LastName>Vardi</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Moshe Y Vardi</PI_FULL_NAME>
<EmailAddress>vardi@rice.edu</EmailAddress>
<PI_PHON>7133485977</PI_PHON>
<NSF_ID>000462277</NSF_ID>
<StartDate>12/08/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Keith</FirstName>
<LastName>Cooper</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Keith D Cooper</PI_FULL_NAME>
<EmailAddress>keith@rice.edu</EmailAddress>
<PI_PHON>7133486013</PI_PHON>
<NSF_ID>000114845</NSF_ID>
<StartDate>12/08/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jan</FirstName>
<LastName>Odegard</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jan E Odegard</PI_FULL_NAME>
<EmailAddress>odegard@rice.edu</EmailAddress>
<PI_PHON>7133483128</PI_PHON>
<NSF_ID>000374767</NSF_ID>
<StartDate>12/08/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Padley</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Padley</PI_FULL_NAME>
<EmailAddress>padley@rice.edu</EmailAddress>
<PI_PHON>7133484820</PI_PHON>
<NSF_ID>000538968</NSF_ID>
<StartDate>12/08/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Klara</FirstName>
<LastName>Jelinkova</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Klara Jelinkova</PI_FULL_NAME>
<EmailAddress>klaraj@rice.edu</EmailAddress>
<PI_PHON>7133482211</PI_PHON>
<NSF_ID>000621175</NSF_ID>
<StartDate>12/08/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 Main Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~499646</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As part of the project titled Improving Network Infrastructure to Enable Large Scale Scientific Data Flows and Collaboration (award #1659348), Rice University built advanced 100 Gigabit per second (Gbps) computer network infrastructure to support data-intensive science and engineering research. The advanced network supports 100 Gbps external connectivity to South East Texas Gig pop (SetG), Lone Star Research and Education Network (LEARN) and Internet2. Additionally, the project deployed data transfer nodes and utilized Globus services to aid with large data transfers. PerfSONAR nodes and a BRO cluster were deployed to monitor performance and address the security needs of the new Science DMZ research infrastructure.</p> <p>As a direct result of the project, Rice University researchers in many science and engineering fields can more efficiently utilize national high-performance computing and commercial cloud resources. For example, a research group working on a three-dimensional structure and dynamics of proteins was able to use multi-hop data movement from the DOE Advanced Photon Source to the campus lab and also utilize and commercial cloud resources (Google Drive). A research group working on high temperature and low-density plasma was able to use the Science DMZ and Globus API to allow for multi-hop and multi-destination 100 Gbps transfers to facilitate modular data workflows.&nbsp; A research group working on environmental fluid dynamics was able to use the Science DMZ to support data movement between computing resources at the Texas Advanced Computing Center (TACC), Pittsburgh Supercomputing Center (PSC), National Center for Atmospheric Research (NCAR) and Rice University. The group was also able to efficiently collect datasets from the German Climate Computing Center. A research group working on the on mobile health and affective computing was able to collaborate more effectively with the University of Memphis. The deployed infrastructure facilitated a large-scale data transfer through the ScienceDMZ, utilizing flash storage on the flash-enabled Globus DTN gateway, assuring a secure, encrypted data transfer. The Science DMZ also supported a research group working on environmental and energy systems with petabyte transfers of distributed acoustic sensing data between Lawrence Berkeley National Laboratory (LBL) and Rice University. Other science areas that were part of the project were heavy-ion physics and bioinformatics.</p> <p>A significant side effect of the project has been a much closer working relationship between the Center for Research Computing, OIT Infrastructure and Operations group, and the Information Security Office. The project helped the three teams develop new workflows in support of researchers on and off campus. The project and gave them the impetus to work with peers at other institutions to ensure faculty research needs were being met most effectively. These benefits, though hard to measure, are impactful for all researchers across campus.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/28/2020<br>      Modified by: Klara&nbsp;Jelinkova</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As part of the project titled Improving Network Infrastructure to Enable Large Scale Scientific Data Flows and Collaboration (award #1659348), Rice University built advanced 100 Gigabit per second (Gbps) computer network infrastructure to support data-intensive science and engineering research. The advanced network supports 100 Gbps external connectivity to South East Texas Gig pop (SetG), Lone Star Research and Education Network (LEARN) and Internet2. Additionally, the project deployed data transfer nodes and utilized Globus services to aid with large data transfers. PerfSONAR nodes and a BRO cluster were deployed to monitor performance and address the security needs of the new Science DMZ research infrastructure.  As a direct result of the project, Rice University researchers in many science and engineering fields can more efficiently utilize national high-performance computing and commercial cloud resources. For example, a research group working on a three-dimensional structure and dynamics of proteins was able to use multi-hop data movement from the DOE Advanced Photon Source to the campus lab and also utilize and commercial cloud resources (Google Drive). A research group working on high temperature and low-density plasma was able to use the Science DMZ and Globus API to allow for multi-hop and multi-destination 100 Gbps transfers to facilitate modular data workflows.  A research group working on environmental fluid dynamics was able to use the Science DMZ to support data movement between computing resources at the Texas Advanced Computing Center (TACC), Pittsburgh Supercomputing Center (PSC), National Center for Atmospheric Research (NCAR) and Rice University. The group was also able to efficiently collect datasets from the German Climate Computing Center. A research group working on the on mobile health and affective computing was able to collaborate more effectively with the University of Memphis. The deployed infrastructure facilitated a large-scale data transfer through the ScienceDMZ, utilizing flash storage on the flash-enabled Globus DTN gateway, assuring a secure, encrypted data transfer. The Science DMZ also supported a research group working on environmental and energy systems with petabyte transfers of distributed acoustic sensing data between Lawrence Berkeley National Laboratory (LBL) and Rice University. Other science areas that were part of the project were heavy-ion physics and bioinformatics.  A significant side effect of the project has been a much closer working relationship between the Center for Research Computing, OIT Infrastructure and Operations group, and the Information Security Office. The project helped the three teams develop new workflows in support of researchers on and off campus. The project and gave them the impetus to work with peers at other institutions to ensure faculty research needs were being met most effectively. These benefits, though hard to measure, are impactful for all researchers across campus.          Last Modified: 04/28/2020       Submitted by: Klara Jelinkova]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
