<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF:Small: Nearest Neighbor Search in High Dimensional Spaces</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>449960.00</AwardTotalIntnAmount>
<AwardAmount>449960</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to advance the state of the art of algorithms for the nearest neighbor search (NNS) problem. The NNS problem is one of the central computational problems arising when dealing with modern massive datasets.  For example, it underlies a classical classification rule in machine learning: to label a new object (such as an image), one can simply find the most similar objects (the nearest neighbors) in a preprocessed database, and use the label of the objects found. More generally, NNS is a key algorithmic tool in many areas including databases, data mining, information retrieval, computer vision, computational geometry, signal processing, bioinformatics, and others.  In such applications, the objects are usually represented in a high-dimensional space: e.g., a 20x20 image is naturally represented by a 400-dimensional vector, with one coordinate per pixel.&lt;br/&gt;&lt;br/&gt;The PI intends to study the high-dimensional NNS problem, by addressing both foundational algorithmic questions, as well as applied aspects. The project aims for both scientific and educational impact. First, the study of the NNS problem is instrumental in developing fundamental concepts in areas such as high-dimensional computational geometry as well as sublinear space algorithms (including concepts such as dimension reduction, sketching, metric embeddings, etc). Second, due to the numerous applications of NNS, its efficient implementations are used widely in industry. Overall, the PI aims to foster a stronger connection between the theory and practice of NNS by code dissemination, public lectures, and student training. The PI's affiliation with Columbia's Data Science Institute puts the PI in a particularly good position to accomplish these goals.&lt;br/&gt;&lt;br/&gt;To accomplish the project's algorithmic goals, the PI will leverage the recently developed approach of data-dependent hashing, where the hash function itself adapts to a given dataset. As a proof-of-concept, the PI and co-authors recently demonstrated that this new approach to NNS leads to algorithms outperforming the classical NNS algorithms (such as those based on the Locality-Sensitive Hashing). The project aims to develop this methodology further to its maturity, extend it to other relevant metrics (similarity measures) which have traditionally resisted efficient solutions, develop practical versions of the algorithms, and to understand the limits of these techniques.</AbstractNarration>
<MinAmdLetterDate>05/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/17/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617955</AwardID>
<Investigator>
<FirstName>Alexandr</FirstName>
<LastName>Andoni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexandr Andoni</PI_FULL_NAME>
<EmailAddress>andoni@cs.columbia.edu</EmailAddress>
<PI_PHON>6503806954</PI_PHON>
<NSF_ID>000711680</NSF_ID>
<StartDate>05/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>100276623</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7929</Code>
<Text>COMPUTATIONAL GEOMETRY</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~449960</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has advanced the state of the art for the problem of Nearest Neighbor Search (NNS) in high dimensions, a central computational problems arising when dealing with the modern massive datasets, for example in machine learning (ML). As such, NNS (also known as similarity search) is a key algorithmic tool in many areas including databases, data mining, information retrieval, computer vision,computational geometry, signal processing, bioinformatics, and others. Despite decades of study, our understanding of the best algorithms (sometimes even any non-trivial) has remained wide open in most settings of interest depending on (dis)similarity measure, accuracy, query time, and space. This project has made substantial progress on and even settled some of these questions on a number of fronts: new algorithmic approaches, impossibility barriers, and stronger connections to applied aspects.</p> <p>One of the key new insights is that dramatically better NNS algorithms are possible if one uses <em>data-dependent</em> hashing. To wit, a classic ingredient is to map high-dimensional objects into hash codes -- think assign a "color". Then, for a given query object, to find a similar object in the database, one determines the "color" of the query object and then looks up only objects of the same "color" in the dataset. Data-dependent hashing is the idea that we can design dramatically better-quality maps (<em>how</em> we color), if we allow it to depend on the dataset itself. This project has demonstrated the wide-reaching effectiveness of this idea.</p> <p>A <strong>major highlight</strong> of the project is a new generic approach for designing algorithms for NNS, which provably yields better algorithms than all the previously known approaches. We gave a generic reduction from the notion of "nonlinear spectral gaps" of metric spaces to spacepartitions, in the form of data-dependent hashing.&nbsp; Using this reduction, we obtain a generic NNS algorithm under an arbitrary high-dimensional normed space. Most importantly, the new data structure achieves accuracy that is exponentially better than all the previous approaches. Additionally, our work provided improved algorithms for specific spaces, such as L_p and Schatten-p matrix norms.</p> <p>This result has been covered in Communications of the ACM (flagship journal in Computer Science), and a popular-science article in the Quanta magazine. It has also been featured in two talks at the International Congress of Mathematicians (ICM) in 2018.</p> <p><br />Some <strong>other highlights</strong> of the project include:</p> <ul> <li>We gave complete characterization of the trade-offs between accuracy, query time, and space for the NNS algorithms under the Euclidean space. The latter is the most common setting for the NNS problem and has been studied for decades. Our work gave both vastly improved algorithms, and also provided (conditional and unconditional) impossibility results suggesting our algorithms are close to the best possible.</li> <li>We studied a version of a practical data &shy;dependent hashing scheme for NNS. In particular, we showed that the 10+ year-&shy;old LSH Forest algorithm, with a simple modification, outperforms vanilla LSH algorithms (which are the classic theoretically&shy;-sound NNS algorithms).</li> <li>Our work has also led to or inspired practical algorithms not only for NNS but also for problems in numerical linear algebra and partition function/kernel density estimation (a core primitive in ML).</li> </ul> <p>The project supported a number of students collaborating on the project. A few of the undergraduate/MS students collaborating on this project have continued on to PhD programs (at Columbia, Stanford, UC Berkeley, UC San Diego).</p> <p>The PI has also contributed to public dissemination by giving invited talks (e.g., a public lecture at the Simons Foundation), and co-authored survey accompaning an ICM'18 talk.</p><br> <p>            Last Modified: 07/28/2021<br>      Modified by: Alexandr&nbsp;Andoni</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has advanced the state of the art for the problem of Nearest Neighbor Search (NNS) in high dimensions, a central computational problems arising when dealing with the modern massive datasets, for example in machine learning (ML). As such, NNS (also known as similarity search) is a key algorithmic tool in many areas including databases, data mining, information retrieval, computer vision,computational geometry, signal processing, bioinformatics, and others. Despite decades of study, our understanding of the best algorithms (sometimes even any non-trivial) has remained wide open in most settings of interest depending on (dis)similarity measure, accuracy, query time, and space. This project has made substantial progress on and even settled some of these questions on a number of fronts: new algorithmic approaches, impossibility barriers, and stronger connections to applied aspects.  One of the key new insights is that dramatically better NNS algorithms are possible if one uses data-dependent hashing. To wit, a classic ingredient is to map high-dimensional objects into hash codes -- think assign a "color". Then, for a given query object, to find a similar object in the database, one determines the "color" of the query object and then looks up only objects of the same "color" in the dataset. Data-dependent hashing is the idea that we can design dramatically better-quality maps (how we color), if we allow it to depend on the dataset itself. This project has demonstrated the wide-reaching effectiveness of this idea.  A major highlight of the project is a new generic approach for designing algorithms for NNS, which provably yields better algorithms than all the previously known approaches. We gave a generic reduction from the notion of "nonlinear spectral gaps" of metric spaces to spacepartitions, in the form of data-dependent hashing.  Using this reduction, we obtain a generic NNS algorithm under an arbitrary high-dimensional normed space. Most importantly, the new data structure achieves accuracy that is exponentially better than all the previous approaches. Additionally, our work provided improved algorithms for specific spaces, such as L_p and Schatten-p matrix norms.  This result has been covered in Communications of the ACM (flagship journal in Computer Science), and a popular-science article in the Quanta magazine. It has also been featured in two talks at the International Congress of Mathematicians (ICM) in 2018.   Some other highlights of the project include:  We gave complete characterization of the trade-offs between accuracy, query time, and space for the NNS algorithms under the Euclidean space. The latter is the most common setting for the NNS problem and has been studied for decades. Our work gave both vastly improved algorithms, and also provided (conditional and unconditional) impossibility results suggesting our algorithms are close to the best possible. We studied a version of a practical data &shy;dependent hashing scheme for NNS. In particular, we showed that the 10+ year-&shy;old LSH Forest algorithm, with a simple modification, outperforms vanilla LSH algorithms (which are the classic theoretically&shy;-sound NNS algorithms). Our work has also led to or inspired practical algorithms not only for NNS but also for problems in numerical linear algebra and partition function/kernel density estimation (a core primitive in ML).   The project supported a number of students collaborating on the project. A few of the undergraduate/MS students collaborating on this project have continued on to PhD programs (at Columbia, Stanford, UC Berkeley, UC San Diego).  The PI has also contributed to public dissemination by giving invited talks (e.g., a public lecture at the Simons Foundation), and co-authored survey accompaning an ICM'18 talk.       Last Modified: 07/28/2021       Submitted by: Alexandr Andoni]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
