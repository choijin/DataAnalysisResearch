<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Ultra-High Speed In-Memory Searchable Dynamic Random Access Memory</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rick Schwerdtfeger</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is provided by a novel data processing architecture, utilizing high-parallel in-memory computing for certain recurring and data intensive functions. Traditional computer architecture funnels all data through the CPU. Multiple CPU cores and very high clock frequencies are often used to address the issue of ever increasing demands on data processing capability. This is not sustainable from both an economical and an environmental standpoint, due to the power consumption involved and the lack of performance resulting from the limited parallelism of computation. Datacenters and cloud computing have long lost the ?green label? originally associated with internet technology and software. Their operators need to find more economical ways to process large amounts of data. Linear searches and data indexing can be performed economically within the memory itself, and at a several orders of magnitude increase in performance, due to the inherent parallelism of memory architecture. At the same time, this comes with significant power savings due to the elimination of data transport and CPU clock cycles involved. Datacenters will be able to serve more clients, with fever servers and less power consumption.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project investigates the feasibility of in-memory data search and compare algorithms which utilize the inherent repetitive structures of high density memory chips. Initial calculations suggest several orders of magnitude improvement in data throughput by in-place computation and simultaneous elimination of data transport. This addresses the industry's significant need for lower power solutions for so called "Big Data" applications, which are expected to continue their exponential rise through the Internet of Things (IoT), where cloud-stored sensor data are expected to eclipse data uploaded by humans in the near future. This Phase I project will perform a detailed architecture study along with a commercially viable and backward compatible communication protocol as well as behavioral models aimed at establishing the commercial viability and at attracting potential clients and licensing partners. Detailed simulations of the novel digital to analog sensing circuitry will thoroughly investigate the key intellectual property, laying the groundwork for the commercial realization of this innovation.</AbstractNarration>
<MinAmdLetterDate>06/20/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1621443</AwardID>
<Investigator>
<FirstName>Wolfgang</FirstName>
<LastName>Hokenmaier</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wolfgang Hokenmaier</PI_FULL_NAME>
<EmailAddress>whokenmaier@greenmountainsemi.com</EmailAddress>
<PI_PHON>8023438175</PI_PHON>
<NSF_ID>000710820</NSF_ID>
<StartDate>06/20/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Green Mountain Semiconductor, Inc.</Name>
<CityName>Burlington</CityName>
<ZipCode>054018349</ZipCode>
<PhoneNumber>8023438175</PhoneNumber>
<StreetAddress>182 Main St., Suite 304</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Vermont</StateName>
<StateCode>VT</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VT00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079501886</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GREEN MOUNTAIN SEMICONDUCTOR, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Green Mountain Semiconductor, Inc.]]></Name>
<CityName>Burlington</CityName>
<StateCode>VT</StateCode>
<ZipCode>054018349</ZipCode>
<StreetAddress><![CDATA[182 Main St., Suite 304]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Vermont</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VT00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>097E</Code>
<Text>High Freq Devices &amp; Circuits</Text>
</ProgramReference>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8035</Code>
<Text>Hardware Devices</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This Phase I SBIR project explored the idea to decentralize the computational architecture in search of lower power consumption for large scale data operations. In a departure from the Von-Neuman architecture, which is at the center of almost all computers, we designed structures to be added onto traditional memory devices, such that certain basic computational procedures can be performed within these memory devices, without the need for data transportation to a central CPU. Not only does this reduce power due to the elimination of memory data transportation, it also leverages the very high parallelsim typically found with memory devices architectures, thereby allowing for significant increases in processing throughput.</p> <p>During this NSF-funded project we developed the circuitry needed for the added functionality, which was designed such that the core memory architecture will remain intact. This is very important because the physical properties of the core memory arrays is expensive and time-consuming to develop, to the effect that memory device foundries will not typically permit any alterations to the physical representation of the memory array.</p> <p>After successful simulation of the circuitry, we designed a test chip which we had manufactured by a third party foundry. Testing of our hardware received from the foundry confirms out initial estimates and simulations.&nbsp;</p> <p>We target this solution for multiple fields in data processing, from large data centers needing a quick characterization of incoming data to high performance computing for applications like genome sequencing and cancer screening. Modifications of the initial circuitry might be able to penetrate into applications for artificial neural networks such as used by autonomous vehicles, where power concumption of current GPU or CPU based solutions can be prohibitive.&nbsp;</p> <p>Not all computation is equally suited for this approach. Traditional CPU or GPU computation is very flexible and fast, due to the use of multiple cores, very high clock frequencies and latest manufacturing technologies. This comes at the price of high power consumption. Repetitive operations such as data word searches, data comparisons or basic algorithmic operations can benefit significantly from our decentralized in-memory approach, where a slower clock rate is more than offset by a significant increase in parallelism and the possibility to scale the parallelism with increased memory installation.&nbsp;</p><br> <p>            Last Modified: 09/08/2017<br>      Modified by: Wolfgang&nbsp;Hokenmaier</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This Phase I SBIR project explored the idea to decentralize the computational architecture in search of lower power consumption for large scale data operations. In a departure from the Von-Neuman architecture, which is at the center of almost all computers, we designed structures to be added onto traditional memory devices, such that certain basic computational procedures can be performed within these memory devices, without the need for data transportation to a central CPU. Not only does this reduce power due to the elimination of memory data transportation, it also leverages the very high parallelsim typically found with memory devices architectures, thereby allowing for significant increases in processing throughput.  During this NSF-funded project we developed the circuitry needed for the added functionality, which was designed such that the core memory architecture will remain intact. This is very important because the physical properties of the core memory arrays is expensive and time-consuming to develop, to the effect that memory device foundries will not typically permit any alterations to the physical representation of the memory array.  After successful simulation of the circuitry, we designed a test chip which we had manufactured by a third party foundry. Testing of our hardware received from the foundry confirms out initial estimates and simulations.   We target this solution for multiple fields in data processing, from large data centers needing a quick characterization of incoming data to high performance computing for applications like genome sequencing and cancer screening. Modifications of the initial circuitry might be able to penetrate into applications for artificial neural networks such as used by autonomous vehicles, where power concumption of current GPU or CPU based solutions can be prohibitive.   Not all computation is equally suited for this approach. Traditional CPU or GPU computation is very flexible and fast, due to the use of multiple cores, very high clock frequencies and latest manufacturing technologies. This comes at the price of high power consumption. Repetitive operations such as data word searches, data comparisons or basic algorithmic operations can benefit significantly from our decentralized in-memory approach, where a slower clock rate is more than offset by a significant increase in parallelism and the possibility to scale the parallelism with increased memory installation.        Last Modified: 09/08/2017       Submitted by: Wolfgang Hokenmaier]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
