<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:   Perceptual-Quality-Aware Video Communication in Wireless Camera Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>178070.00</AwardTotalIntnAmount>
<AwardAmount>178070</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Monisha Ghosh</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Recent advances in imaging hardware and wireless communications have fostered the deployment of embedded camera sensors in various wireless imaging applications, such as surveillance, smart building operations, intelligent transportation, remote health care, and consumer electronics and entertainment. It is essential to guarantee the delivery of videos generated by camera sensors to an end user with good quality. In particular, the need to maintain good perceptual quality is driven by many human-centered imaging applications, where human users rely on the received videos to make critical decisions. However, many subjective tests have shown that the perceptual quality of networked videos as evaluated by viewers cannot be merely determined by common network QoS (quality-of-service) metrics such as bit rate and packet loss ratio. Factors such as video content characteristics and compression parameters also have significant impacts on perceptual quality. The objective of this project is to achieve efficient perceptual-quality-aware video communication in wireless camera networks by jointly considering the various factors contributing to perceptual video quality. Success of the proposed research will boost user experience and enhance user's capabilities to explore and interact with the physical world through applications based on wireless camera networks.&lt;br/&gt;&lt;br/&gt;This project proposes a systematic solution for predicting and controlling perceptual video quality in wireless camera networks. The proposed research is based on: i) light-weight and accurate prediction of perceptual video quality in the network; ii) comprehensive analysis of the relationships between perceptual video quality and network parameters; iii) exploring possible ways to achieve energy-efficiency and bandwidth-efficiency by utilizing the properties of perceptual quality; and iv) designing novel rate control and scheduling protocols that can effectively control perceptual quality in dynamic network conditions. The research solutions of the project are expected to provide satisfactory perceptual video quality to users using the minimum energy and bandwidth resources in wireless camera networks. The research solutions will demonstrate how to improve user experience in wireless camera networks by utilizing the properties of perceptual video quality. The proposed research will bridge the gap between the studies in perceptual video quality and the design of wireless video communication protocols, and it will inspire new research in perceptual quality provisioning for many other wireless sensing applications.</AbstractNarration>
<MinAmdLetterDate>09/08/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1644946</AwardID>
<Investigator>
<FirstName>Rui</FirstName>
<LastName>Dai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rui Dai</PI_FULL_NAME>
<EmailAddress>rui.dai@uc.edu</EmailAddress>
<PI_PHON>5135560134</PI_PHON>
<NSF_ID>000631305</NSF_ID>
<StartDate>09/08/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Cincinnati Main Campus</Name>
<CityName>Cincinnati</CityName>
<ZipCode>452210222</ZipCode>
<PhoneNumber>5135564358</PhoneNumber>
<StreetAddress>University Hall, Suite 530</StreetAddress>
<StreetAddress2><![CDATA[51 Goodman Dr.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041064767</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CINCINNATI</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041064767</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Cincinnati Main Campus]]></Name>
<CityName>Cincinnati</CityName>
<StateCode>OH</StateCode>
<ZipCode>452210030</ZipCode>
<StreetAddress><![CDATA[812 Rhodes Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~178070</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Recent advances in imaging hardware and wireless communications have fostered the deployment of embedded camera sensors in various wireless networked imaging applications, such as surveillance, smart building operations, intelligent transportation, and remote health care. It is essential to guarantee the delivery of videos generated by camera sensors to users with good quality. In this project, the properties of perceptual video quality were investigated in depth, and several new quality prediction, adjustment, and control solutions were designed for camera systems. Specifically, this project have resulted in four major accomplishments. First, a forward error correction solution was designed for wireless camera networks that could efficiently provide satisfactory perceptual video quality to human users. Second, a new video compression scheme was designed for improving human users' experience on understanding video content. Third, an image quality adjustment framework was proposed for wireless embedded cameras with the objective to improve users' experience on understanding the content of images. Lastly, an adaptive video streaming algorithm was designed that could enhance the fluidity of video streaming over content-centric networks.</p> <p>&nbsp;</p> <p>Intellectual merit: This project has conducted systematic study in perceptual quality aware video communication solutions. It has provided comprehensive understanding of perceptual quality and different quality-contributing factors during the sensing, compression, and communication processes for image and video. It has demonstrated how to measure and control perceptual video quality in dynamic network environments, and it has proposed new video compression and communication solutions to achieve bandwidth- and energy- efficiency by utilizing the properties of perceptual video quality. The results from this project could boost user experience and enhance user's capabilities to explore and interact with the physical world through camera-based network applications.</p> <p>&nbsp;</p> <p>Broader impact: The proposed research could impact the design of various cyber-physical systems (CPS) that incorporate visual sensing components and human operators, such as smart surveillance systems, medical CPSs with augmented reality, and smart and connected vehicles. Results from the proposed research will advance the capabilities of camera-based applications by enabling efficient processing and dissemination of visual information to meet with user requirements. Topics on perceptual quality aware video communication have enhanced the materials of a graduate course the PI is teaching. This project supported the training of two PhD students and provided research experience for one female undergraduate student at University of Cincinnati. Results from this research were published in several major journals and conferences in multimedia and networking, such as IEEE Transactions on Multimedia and IEEE Internal Symposium on Multimedia. Results from this project were also disseminated to audience with broad background through several research seminars and project meetings.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/25/2019<br>      Modified by: Rui&nbsp;Dai</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577329844631_MOS-distribution--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577329844631_MOS-distribution--rgov-800width.jpg" title="Mean Opinion Scores (MOS) for 6 videos with different content characteristics"><img src="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577329844631_MOS-distribution--rgov-66x44.jpg" alt="Mean Opinion Scores (MOS) for 6 videos with different content characteristics"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Perceptual video quality is evaluated by the MOS obtained from human subjective tests.</div> <div class="imageCredit">Rui Dai</div> <div class="imageSubmitted">Rui&nbsp;Dai</div> <div class="imageTitle">Mean Opinion Scores (MOS) for 6 videos with different content characteristics</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577330186862_Decision-tree--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577330186862_Decision-tree--rgov-800width.jpg" title="Decision-tree-based perceptual video quality prediction model"><img src="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577330186862_Decision-tree--rgov-66x44.jpg" alt="Decision-tree-based perceptual video quality prediction model"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The decision-tree based quality prediction model considers features related to video content characteristics, encoding parameters, and network conditions, and it reveals explicit relationships between the input features and the predictions of perceptual quality.</div> <div class="imageCredit">Rui Dai</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Rui&nbsp;Dai</div> <div class="imageTitle">Decision-tree-based perceptual video quality prediction model</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577330343007_Quality-adjustment--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577330343007_Quality-adjustment--rgov-800width.jpg" title="Image quality adjustment framework for wireless embedded cameras"><img src="/por/images/Reports/POR/2019/1644946/1644946_10459688_1577330343007_Quality-adjustment--rgov-66x44.jpg" alt="Image quality adjustment framework for wireless embedded cameras"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The image quality adjustment framework is designed to guarantee good image quality during the sensing and local processing of images captured by embedded cameras.</div> <div class="imageCredit">Rui Dai</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Rui&nbsp;Dai</div> <div class="imageTitle">Image quality adjustment framework for wireless embedded cameras</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Recent advances in imaging hardware and wireless communications have fostered the deployment of embedded camera sensors in various wireless networked imaging applications, such as surveillance, smart building operations, intelligent transportation, and remote health care. It is essential to guarantee the delivery of videos generated by camera sensors to users with good quality. In this project, the properties of perceptual video quality were investigated in depth, and several new quality prediction, adjustment, and control solutions were designed for camera systems. Specifically, this project have resulted in four major accomplishments. First, a forward error correction solution was designed for wireless camera networks that could efficiently provide satisfactory perceptual video quality to human users. Second, a new video compression scheme was designed for improving human users' experience on understanding video content. Third, an image quality adjustment framework was proposed for wireless embedded cameras with the objective to improve users' experience on understanding the content of images. Lastly, an adaptive video streaming algorithm was designed that could enhance the fluidity of video streaming over content-centric networks.     Intellectual merit: This project has conducted systematic study in perceptual quality aware video communication solutions. It has provided comprehensive understanding of perceptual quality and different quality-contributing factors during the sensing, compression, and communication processes for image and video. It has demonstrated how to measure and control perceptual video quality in dynamic network environments, and it has proposed new video compression and communication solutions to achieve bandwidth- and energy- efficiency by utilizing the properties of perceptual video quality. The results from this project could boost user experience and enhance user's capabilities to explore and interact with the physical world through camera-based network applications.     Broader impact: The proposed research could impact the design of various cyber-physical systems (CPS) that incorporate visual sensing components and human operators, such as smart surveillance systems, medical CPSs with augmented reality, and smart and connected vehicles. Results from the proposed research will advance the capabilities of camera-based applications by enabling efficient processing and dissemination of visual information to meet with user requirements. Topics on perceptual quality aware video communication have enhanced the materials of a graduate course the PI is teaching. This project supported the training of two PhD students and provided research experience for one female undergraduate student at University of Cincinnati. Results from this research were published in several major journals and conferences in multimedia and networking, such as IEEE Transactions on Multimedia and IEEE Internal Symposium on Multimedia. Results from this project were also disseminated to audience with broad background through several research seminars and project meetings.          Last Modified: 12/25/2019       Submitted by: Rui Dai]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
