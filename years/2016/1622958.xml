<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STTR Phase I:  A platform for reactive and adaptive motion generation for real-world manipulation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this project revolves around the democratization of extremely complex motion generation, perception, and control systems. Most roboticists today program without understanding heat transfer, motor controllers, and lowlevel&lt;br/&gt;message passing. Abstractions improve robot accessibility and enable organizations to leverage a broader range of expertise. This project will further the modern robotics abstraction to obviate specialized knowledge of low level task processes such as movement and vision across a variety of robots. Product driven companies are best suited to develop business logic and a thoughtful customer&lt;br/&gt;experience, and most struggle to acquire suitable in-house talent to remain competitive with foundational robotics techniques. This system addresses fundamental behavioral prerequisites for applications ranging from collaborative robotics in manufacturing and fulfillment to assistive robotics, caretaking, and personal home robotics as well as Unmanned Aerial Vehicles (UAVs),&lt;br/&gt;autonomous vehicles, and entertainment robots. With higher level interfaces, programmers can more easily leverage their creativity and intuition enabling a new generation of applications unhindered by the present day difficulties of low level development. Beyond the commercial potential, these advantages are pervasive: researchers, students, and children alike, of all genders and ethnicities, will be empowered to interact with, program, and study robots at a substantially higher level.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project will address the increasing need for a higher level interface to robots that abstracts away perception, motion generation, and control. Many competing techniques for motion generation and perception populate the literature, but recently optimization has become a common unifying theme playing a prominent role in continuously running and adapting behavior with the promise of speed and generality. This project will utilize state of the art technology in continuous motion optimization and modern real time vision and tracking techniques to build an integrated system appropriate for use across multiple robots. These techniques involve optimization, Riemannian geometry, low level control, online learning and more&amp;#894; many components are well suited to generalizing across robotic platforms, but questions remain regarding processing speed, robustness, and the utility of&lt;br/&gt;existing modeling tools. This project will assess the viability of such a higher level system by studying the latencies of reactions, precision of manipulation, and its long term robustness especially in the context of collaborative robotics for manufacturing using evaluations across multiple physical robotic platforms. The technical results will illuminate the role of optimization in&lt;br/&gt;motion generation and perception for real world productionizable systems and uncover any remaining scientific question that must be addressed.</AbstractNarration>
<MinAmdLetterDate>06/21/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/21/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1622958</AwardID>
<Investigator>
<FirstName>Dieter</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dieter Fox</PI_FULL_NAME>
<EmailAddress>fox@cs.washington.edu</EmailAddress>
<PI_PHON>2066852517</PI_PHON>
<NSF_ID>000210667</NSF_ID>
<StartDate>06/21/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nathan</FirstName>
<LastName>Ratliff</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nathan D Ratliff</PI_FULL_NAME>
<EmailAddress>nathan.ratliff@lularobotics.com</EmailAddress>
<PI_PHON>2062096733</PI_PHON>
<NSF_ID>000712909</NSF_ID>
<StartDate>06/21/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Lula Robotics Inc.</Name>
<CityName>Seattle</CityName>
<ZipCode>981025170</ZipCode>
<PhoneNumber>2062096733</PhoneNumber>
<StreetAddress>535 13th Ave E Apt. 307</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080080868</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LULA ROBOTICS INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1505</Code>
<Text>STTR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>1505</Code>
<Text>STTR PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8034</Code>
<Text>Hardware Components</Text>
</ProgramReference>
<ProgramReference>
<Code>8035</Code>
<Text>Hardware Devices</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-48822ff2-5fab-f942-d378-26ba49ff6d41"> </span></p> <p dir="ltr"><span>Our NSF STTR was joint between Lula Robotics and the University of Washington (UW) to combine the robot motion and control expertise of Lula with the vision and tracking expertise of UW into a single robotic platform. The aim was to develop a software platform that would abstract away the details of perception and motion generation for collaborative robots, that would work uniformly across multiple robot manufacturers to offer users a simple and intuitive API for programming robot manipulation.</span></p> <p dir="ltr"><span>The project was highly successful. Lula developed a reactive and adaptive motion generation platform built for consuming continuous sensory feedback such as visual processing. The motion components were uniquely designed for the types of continual adaptation needed for working in unstructured and changing human environments. Building a motion system from that perspective offered intriguing insight into the motion planning problem, which has been a core field of study in robotics for many years. We were able to develop a novel long-range motion generation technique for addressing the more difficult motion planning problem for common human environments, building on these reactive controllers as a fundamental building block, that substantially outperforms most modern motion planning algorithms in terms of speed and real-time adaptability. This result speaks to the importance of these underlying reactive controllers and local motion generators, and even suggests new ways in which modern machine learning techniques, such as Deep Learning, can be leveraged in such a system to create a more expressive trainable pipeline from sensory input to motion generation. Through this project, we have been able to productionize this portion of the system, the motion components, with early interest from top industrial robotics OEM clients.</span></p> <p dir="ltr"><span>The University of Washington provided a unique toolset for continuous robot tracking called Dense Articulated Real-time Tracking, or DART for short. Through this project we were able to interface to Lula&rsquo;s motion generation system with that perception system to close the loop on hand-eye coordination. This integration demonstrated an unprecedented level of precision and the ability to control points on external objects held by the robot. For instance, just as humans do, if the robot picks up a cup, the visual feedback will maintain the precise relative configuration of the cup to the robot&rsquo;s hand and to other objects in the world, so the robot can control a point on the bottom of the cup just as it would it&rsquo;s own built-in end-effector. So if the cup is bumped, or any other disturbance is encountered, the visual feedback will instantaneously report that change and the system will correct for it, just by the nature of its objective to control the point on the cup, with no added programming. This integration showed that continuous adaptive and reactive motion, coupled with real-time visual feedback from a tracker such as DART, was a powerful combination, resulting in a system that could robustly execute simple high-level programs despite the uncertainty of physics and the unstructured nature of human environments. Through our API, the user need only tell the robot what the do, and the system figures out on its own the details of how to do it, and how to adapt its behavior on the fly as needed to get it done.</span></p> <p dir="ltr"><span>Our system, funded through this NSF STTR grant, demonstrates the potential of continuous feedback and reactions in collaborative robotic systems, and resulted in a commercially viable productionized motion system with a well-defined sensory-feedback API, and showed exciting results when interfaced with a state-of-the-art tracking system such as DART.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/27/2017<br>      Modified by: Nathan&nbsp;D&nbsp;Ratliff</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Our NSF STTR was joint between Lula Robotics and the University of Washington (UW) to combine the robot motion and control expertise of Lula with the vision and tracking expertise of UW into a single robotic platform. The aim was to develop a software platform that would abstract away the details of perception and motion generation for collaborative robots, that would work uniformly across multiple robot manufacturers to offer users a simple and intuitive API for programming robot manipulation. The project was highly successful. Lula developed a reactive and adaptive motion generation platform built for consuming continuous sensory feedback such as visual processing. The motion components were uniquely designed for the types of continual adaptation needed for working in unstructured and changing human environments. Building a motion system from that perspective offered intriguing insight into the motion planning problem, which has been a core field of study in robotics for many years. We were able to develop a novel long-range motion generation technique for addressing the more difficult motion planning problem for common human environments, building on these reactive controllers as a fundamental building block, that substantially outperforms most modern motion planning algorithms in terms of speed and real-time adaptability. This result speaks to the importance of these underlying reactive controllers and local motion generators, and even suggests new ways in which modern machine learning techniques, such as Deep Learning, can be leveraged in such a system to create a more expressive trainable pipeline from sensory input to motion generation. Through this project, we have been able to productionize this portion of the system, the motion components, with early interest from top industrial robotics OEM clients. The University of Washington provided a unique toolset for continuous robot tracking called Dense Articulated Real-time Tracking, or DART for short. Through this project we were able to interface to Lula?s motion generation system with that perception system to close the loop on hand-eye coordination. This integration demonstrated an unprecedented level of precision and the ability to control points on external objects held by the robot. For instance, just as humans do, if the robot picks up a cup, the visual feedback will maintain the precise relative configuration of the cup to the robot?s hand and to other objects in the world, so the robot can control a point on the bottom of the cup just as it would it?s own built-in end-effector. So if the cup is bumped, or any other disturbance is encountered, the visual feedback will instantaneously report that change and the system will correct for it, just by the nature of its objective to control the point on the cup, with no added programming. This integration showed that continuous adaptive and reactive motion, coupled with real-time visual feedback from a tracker such as DART, was a powerful combination, resulting in a system that could robustly execute simple high-level programs despite the uncertainty of physics and the unstructured nature of human environments. Through our API, the user need only tell the robot what the do, and the system figures out on its own the details of how to do it, and how to adapt its behavior on the fly as needed to get it done. Our system, funded through this NSF STTR grant, demonstrates the potential of continuous feedback and reactions in collaborative robotic systems, and resulted in a commercially viable productionized motion system with a well-defined sensory-feedback API, and showed exciting results when interfaced with a state-of-the-art tracking system such as DART.          Last Modified: 10/27/2017       Submitted by: Nathan D Ratliff]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
