<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: SMALL: Parallelization and Memory System Techniques for Heterogeneous Microprocessors</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Traditional discrete GPUs have demonstrated dramatic performance gains over CPUs, but the improvements have been limited to simpler codes with massive parallelism.  Recently, computer manufacturers have been producing heterogeneous microprocessors in which a CPU and GPU are integrated on the same die.  Such processors provide shared memory between the CPU and GPU, and fast CPU-GPU communication.  These features potentially enable acceleration of new types of computations, allowing the GPU to gainfully execute smaller loops and to support more complex codes.  This project investigates techniques for exploiting heterogeneous microprocessors and to realize their benefits.  If successful, the project will enable most programs, not just those with massive parallelism, to utilize GPUs.  The project will also provide valuable training to both graduate and undergraduate students, and improve graduate-level coursework.&lt;br/&gt;&lt;br/&gt;On the research side, the project will pursue the following research directions.  First, it will create a suite of benchmarks exhibiting complex loop nests that demonstrate the new capabilities of heterogeneous microprocessors.  Second, the project will also develop novel parallelization schemes for the new benchmark suite.  The parallelization schemes will map multiple levels of parallelism to CPU and GPU cores simultaneously to fully utilize the compute resources in a heterogeneous microprocessor.  Third, new cache coherence protocols will be developed that efficiently support the bulk producer-consumer communication that occurs as finer-grained computations migrate from CPU to GPU and back.  And finally, adaptive memory address mapping schemes will be investigated that exploit DRAM page locality for CPU accesses and channel-level parallelism for GPU accesses.</AbstractNarration>
<MinAmdLetterDate>07/27/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618963</AwardID>
<Investigator>
<FirstName>Donald</FirstName>
<LastName>Yeung</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Donald Yeung</PI_FULL_NAME>
<EmailAddress>yeung@umd.edu</EmailAddress>
<PI_PHON>3014053649</PI_PHON>
<NSF_ID>000460886</NSF_ID>
<StartDate>07/27/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Heterogeneous microprocessors, in which a CPU and GPU are both<br />integrated on the same die, are commonplace today.&nbsp; Such processors<br />support shared memory (often coherently) between the CPU and GPU, and<br />provide fast CPU-GPU communication.&nbsp; These features permit a wide<br />range of loops to be gainfully off-loaded onto the GPU, including<br />those with smaller amounts of work, thus enabling acceleration of a<br />greater variety of computations.<br /><br />This research project developed a new parallelization scheme, called<br />"nested MIMD-SIMD parallelization," that parallelizes codes with<br />complex loop nests onto the CPU and GPU of a heterogeneous<br />microprocessor.&nbsp; In particular, our technique targets large parallel<br />code regions that also contain smaller data-parallel loops.&nbsp; In such<br />nested parallel code structures, our technique parallelizes the<br />outer-most code region and executes it on the CPU cores.&nbsp; At the same<br />time, our technique also parallelizes the inner-most parallel loops<br />and executes them on the integrated GPU.&nbsp; Because the resulting GPU<br />kernels are launched from multiple CPU threads, simultaneous kernel<br />launches can occur, boosting GPU utilization especially when<br />inidividual kernels contain smaller amounts of work.&nbsp; Several example<br />benchmarks illustrating our nested MIMD-SIMD parallization technique<br />are available for download at github.com/dgerzhoy/Nested_MIMD_SIMD.<br /><br />Fast CPU-GPU communication not only permits heterogeneous<br />microprocessors to accelerate new types of codes, it can also improve<br />the performance and efficiency of traditional massively parallel<br />codes.&nbsp; Kernel launches are a form of computation migration in which<br />producer-consumer communication occurs from the CPU to the GPU and<br />back.&nbsp; Compared to discrete systems, heterogeneous microprocessors can<br />support such producer-consumer communication through shared main<br />memory which is much more efficient than having to go through the<br />system I/O bus, as is needed for discrete GPUs.&nbsp; However,<br />heterogeneous microprocessors have the potential to provide even<br />higher performance if the CPU-GPU communication can be performed<br />entirely on-chip.&nbsp; Unfortunately, this normally does not happen due to<br />poor temporal reuse on the communicated data structures.<br /><br />This research project introduced a new code transformation, called<br />"pipelined CPU-GPU scheduling for caches," that improves the temporal<br />reuse of producer-consumer communication between the CPU and GPU so<br />that the communication can happen via the on-chip shared cache of a<br />heterogeneous microprocessor.&nbsp; Specifically, we divide CPU and GPU<br />execution into blocks.&nbsp; Rather than execute all of the CPU (GPU)<br />blocks prior to a kernel launch (completion), we overlap the execution<br />of CPU and GPU blocks in a software pipelined fashion.&nbsp; By properly<br />selecting the block size, one can tune the combined working sets of<br />the CPU and GPU blocks such that they fit in the last-level cache.<br />When this happens, communication from the CPU to the GPU across kernel<br />launches (and similarly, from the GPU back to the CPU across kernel<br />completions) can occur entirely through the on-chip cache.&nbsp; This<br />results in both performance gains and energy savings.</p><br> <p>            Last Modified: 07/14/2021<br>      Modified by: Donald&nbsp;Yeung</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Heterogeneous microprocessors, in which a CPU and GPU are both integrated on the same die, are commonplace today.  Such processors support shared memory (often coherently) between the CPU and GPU, and provide fast CPU-GPU communication.  These features permit a wide range of loops to be gainfully off-loaded onto the GPU, including those with smaller amounts of work, thus enabling acceleration of a greater variety of computations.  This research project developed a new parallelization scheme, called "nested MIMD-SIMD parallelization," that parallelizes codes with complex loop nests onto the CPU and GPU of a heterogeneous microprocessor.  In particular, our technique targets large parallel code regions that also contain smaller data-parallel loops.  In such nested parallel code structures, our technique parallelizes the outer-most code region and executes it on the CPU cores.  At the same time, our technique also parallelizes the inner-most parallel loops and executes them on the integrated GPU.  Because the resulting GPU kernels are launched from multiple CPU threads, simultaneous kernel launches can occur, boosting GPU utilization especially when inidividual kernels contain smaller amounts of work.  Several example benchmarks illustrating our nested MIMD-SIMD parallization technique are available for download at github.com/dgerzhoy/Nested_MIMD_SIMD.  Fast CPU-GPU communication not only permits heterogeneous microprocessors to accelerate new types of codes, it can also improve the performance and efficiency of traditional massively parallel codes.  Kernel launches are a form of computation migration in which producer-consumer communication occurs from the CPU to the GPU and back.  Compared to discrete systems, heterogeneous microprocessors can support such producer-consumer communication through shared main memory which is much more efficient than having to go through the system I/O bus, as is needed for discrete GPUs.  However, heterogeneous microprocessors have the potential to provide even higher performance if the CPU-GPU communication can be performed entirely on-chip.  Unfortunately, this normally does not happen due to poor temporal reuse on the communicated data structures.  This research project introduced a new code transformation, called "pipelined CPU-GPU scheduling for caches," that improves the temporal reuse of producer-consumer communication between the CPU and GPU so that the communication can happen via the on-chip shared cache of a heterogeneous microprocessor.  Specifically, we divide CPU and GPU execution into blocks.  Rather than execute all of the CPU (GPU) blocks prior to a kernel launch (completion), we overlap the execution of CPU and GPU blocks in a software pipelined fashion.  By properly selecting the block size, one can tune the combined working sets of the CPU and GPU blocks such that they fit in the last-level cache. When this happens, communication from the CPU to the GPU across kernel launches (and similarly, from the GPU back to the CPU across kernel completions) can occur entirely through the on-chip cache.  This results in both performance gains and energy savings.       Last Modified: 07/14/2021       Submitted by: Donald Yeung]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
