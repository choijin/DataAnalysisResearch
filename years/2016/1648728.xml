<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps: Artificial Intelligence and Deep Learning System for Product Search</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>01/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Steven Konsek</SignBlockName>
<PO_EMAI>skonsek@nsf.gov</PO_EMAI>
<PO_PHON>7032927021</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this I-Corps project are in various applications in ecommerce, security, robotics, etc. This project's technology enables computer systems and their users to better contextually understand products and locate them. For example, in e-commerce, this technology enables people to make informed decisions about product selection. This may enhance the e-tail experience and help to eliminate wasted time as people can easily and precisely find things they desire. The aggregated data on product searches can be used by suppliers to better understand and plan for demand. Also, in robotics, as another example, computer vision is an essential element of the technology and the multi-object detection system built into software will enable robots to be more effective as they will "understand" products or objects in the context of our world. In security, this technology will enable better detection of theft and threats. Overall, the technology has many applications - many that can make lives more convenient and safer.&lt;br/&gt;&lt;br/&gt;This I-Corps project is based on machine learning technology.  Billions of data points that include images and textual information about products are trained into artificial neural networks. These neural networks understand products with context with both images and natural language. Further, the neural networks are able to self-learn with new information from the internet. This technology was developed after researching various machine learning methods for large scale objection detection and search. The result is that neural networks provide the most scalable and efficient technology in terms of object detection. The uniqueness of this system is that parts of the learning in the neural network can be changed without affecting the rest of the network and the network can self-learn from the internet about products and other choice-centric decisions.</AbstractNarration>
<MinAmdLetterDate>08/08/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1648728</AwardID>
<Investigator>
<FirstName>Carol</FirstName>
<LastName>Mimura</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carol Mimura</PI_FULL_NAME>
<EmailAddress>carolm@berkeley.edu</EmailAddress>
<PI_PHON>5106428109</PI_PHON>
<NSF_ID>000617590</NSF_ID>
<StartDate>08/08/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Regents of the University of California]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947045940</ZipCode>
<StreetAddress><![CDATA[2150 Shattuck Ave, Ste 300]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><em>The search experience for most products is limited to keywords and menus; this is clunky and frustrating for many users especially on mobile phones where screen real estate is limited. Our technology could help to solve this problem by enabling consumers to upload a picture and use natural language to instantly find exact or similar products. This technology can be used in the $500B e-commerce industry as well as various other industries such as security, robotics, etc.<br /><br />If used in e-commerce, a typical customer will be a consumer who sees a compelling image while browsing on social media sites like Pinterest or Instagram and wants to find a similar item for sale online. Describing products precisely by words is very difficult and so we are able to solve that problem by enabling the customer to search by using a picture. Secondly customers want to compare prices and options across multiple sites. Our search engine will search across 500+ stores and 25 million items to show all relevant products from multiple retailers on a single screen.<br /><br />The objective of this proposal was to secure funding to use in testing our market and product hypothesis that there is an unmet need for search for products in various application including e-commerce. This funding was to test our value proposition by conducting customer interviews of consumers across demographics as well as enterprises to explore product-market fit. However, we were unable to complete the I-Corps program.</em></p><br> <p>            Last Modified: 05/09/2017<br>      Modified by: Carol&nbsp;Mimura</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The search experience for most products is limited to keywords and menus; this is clunky and frustrating for many users especially on mobile phones where screen real estate is limited. Our technology could help to solve this problem by enabling consumers to upload a picture and use natural language to instantly find exact or similar products. This technology can be used in the $500B e-commerce industry as well as various other industries such as security, robotics, etc.  If used in e-commerce, a typical customer will be a consumer who sees a compelling image while browsing on social media sites like Pinterest or Instagram and wants to find a similar item for sale online. Describing products precisely by words is very difficult and so we are able to solve that problem by enabling the customer to search by using a picture. Secondly customers want to compare prices and options across multiple sites. Our search engine will search across 500+ stores and 25 million items to show all relevant products from multiple retailers on a single screen.  The objective of this proposal was to secure funding to use in testing our market and product hypothesis that there is an unmet need for search for products in various application including e-commerce. This funding was to test our value proposition by conducting customer interviews of consumers across demographics as well as enterprises to explore product-market fit. However, we were unable to complete the I-Corps program.       Last Modified: 05/09/2017       Submitted by: Carol Mimura]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
