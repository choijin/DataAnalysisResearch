<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>II-New: Collaborative: A Mixed Reality Environment for Enabling Everywhere Data-Centric Work</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2016</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>350703.00</AwardTotalIntnAmount>
<AwardAmount>350703</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balakrishnan Prabhakaran</SignBlockName>
<PO_EMAI>bprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032924847</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This infrastructure project will develop an open source software toolkit, called OpenMR, to support building "mixed reality" data analysis systems that project data into the physical world using a new class of display devices such as Microsoft Hololens and Oculus Rift. Through OpenMR, these lightweight, wearable, mobile devices will tap into data-intensive infrastructures hosted in the cloud, with the goal of developing systems that allow users to perform data-intensive tasks from anywhere, without requiring heavy dedicated large-format displays supported by dedicated local computers.  To pursue this research, the investigators will acquire both dedicated cloud-computing servers (to support data analysis) and mixed reality hardware devices (to create the interfaces).  They will develop OpenMR to connect this hardware, to support common analysis tasks such as selecting, filtering, and classifying data, and to create data displays in the physical world. To both demonstrate the toolkit and advance data analysis research, they will build a number of prototype mixed reality interfaces for researchers whose work requires analyzing a large amount of data in domains including weather, biology, and medical imaging.  In addition to advancing those specific research areas, studying these prototypes with real users will support research around the underlying data analysis techniques, the cognitive science of how people interact with data in the physical world, and the design principles needed to build mixed reality systems.  This, in turn, will make these emerging technologies more likely to succeed and spread, and increase the chance of finding potential 'killer apps' for these systems.  The infrastructure will also directly support education and research at the partner universities around data visualization, computer graphics, computer vision, and machine learning, while the release of the toolkit will benefit the wider community.  This research is timely and important because as smart devices, in particular virtual and mixed reality devices such as Google Glass, Microsoft Hololens, Oculus Rift and Google Cardboard, become commonplace, these devices will play an increasingly important role relative to traditional laptop and digital computers when interacting with digital information. &lt;br/&gt;&lt;br/&gt;The long-term vision of the project is to develop a mixed reality research infrastructure to support everywhere data-centric innovations, providing immersive, intuitive, location-free, advanced machine learning, data analysis, reduction, summary and storage tools.  This includes advanced support for the full pipeline of data-centric work in mixed reality spaces through the OpenMR open source toolkit, including front end visualization and interaction that leverages awareness of available rendering spaces and hardware along with effective visualization patterns in 2D and 3D spaces to optimize interaction; key components of data analysis and machine learning on the middle layers including automatic, generic feature engineering and joint optimization of classification performance and effective identification of discriminating features; and high-performance computing and cost-sensitive job management on the server.  The team will evaluate OpenMR's efficiency, stability, scalability, functionality, flexibility, and ease of adoption through a number of mechanisms, including self-evaluations and documentation of the design process, review from domain experts, and evaluation with both expert and novice users on data analysis tasks that cur across the specific application domains described above.  The toolkit itself will be released on the GitHub open source platform during the third year of the project after it has reached an initial level of maturity and usefulness.  The investigators will publicize OpenMR through a Youtube channel with a set of demonstration videos; outreach to relevant researchers interested in immersive visualization, visual analytics, multi-sensory human-computer interaction, machine learning with human-in-the-loop, and high-performance computing; and collaboration with undergraduates in the Students, Technology, Academia, Research, and Service Computing Corps consortium.</AbstractNarration>
<MinAmdLetterDate>07/25/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/25/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1629890</AwardID>
<Investigator>
<FirstName>Jian</FirstName>
<LastName>Huang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jian Huang</PI_FULL_NAME>
<EmailAddress>huangj@eecs.utk.edu</EmailAddress>
<PI_PHON>8659744398</PI_PHON>
<NSF_ID>000285716</NSF_ID>
<StartDate>07/25/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName/>
<StateCode>TN</StateCode>
<ZipCode>379960001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~350703</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we studied how to carry out data-intensive work using portable personal devices. Albeit some of such devices are still not ready for widespread practical use, especially mixed reality devices like Hololens, the fundamental scientific question we can ask is exciting. That is, what kind of infrastructure can deliver the same kind of user experience to remote settings, where users have only their personal devices, such as laptop, tablet, smart phone, and VR/AR goggles.</p> <div></div> <div>Hence, we coined the term "Everywhere Data-Centric Work". Our technological vision revolved around making innovative use of cloud computing. While the concept of our project predated 2020, the ongoing pandemic has made remote work and remote learning prevalent throughout the world. This disruption gives our research even stronger relevance than we had originally envisioned.</div> <div></div> <div>Through our work, we have developed new methods to bridge two traditionally separated architectural domains: (1) large systems with high-scalability parallelism that can provide throughput and elasticity, (2) high-portability interactive objects that can be embedded into small applications.</div> <div></div> <div>A key example outcome is that authentic interactive user experiences with large scientific datasets can be embedded into a Wikipedia webpage flexibly. The extra in-application memory overheads are on the order of a couple of megabytes. The extra loading time of an augmented Wikipedia page is about 150 milliseconds. At such low overheads, an augmented Wikipedia page can have live 3D visualization that becomes live instantaneously. Such augmented Wikipedia pages can be loaded and used on laptop, tablets, smart phones alike. In addition, the same technology supports high-end user devices such as a PowerWall display as well as Hololens too. The same technology supports interactive use cases as well as moviemaking and movie editing.</div> <div></div> <div>Ultimately, our project has expanded the application space that can make use of high-end, always parallel rendering and only needs to pay a minimum amount of overheads. Each live scientific visualization to embed requires adding only 2-3 lines of code in Javascript. In all cases, standard Amazon AWS cloud instances can be used. In order to support up to 20 concurrent users at typical desktop resolutions, the AWS cost is only $4.59/hour.</div> <div></div> <div>The demonstration of this feasibility, and the entire research infrastructure being made open source, has broad implications on how online tools can support Everywhere Data-Centric Work and enable interactivity with large datasets at superior cost efficiency and system scalability.</div><br> <p>            Last Modified: 11/24/2020<br>      Modified by: Jian&nbsp;Huang</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1629890/1629890_10443304_1606234549849_tapestry-presentation--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1629890/1629890_10443304_1606234549849_tapestry-presentation--rgov-800width.jpg" title="Tapestry Presentation"><img src="/por/images/Reports/POR/2020/1629890/1629890_10443304_1606234549849_tapestry-presentation--rgov-66x44.jpg" alt="Tapestry Presentation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A key outcome of this project is an open source system called Tapestry. Tapestry enables a broad new application space: Everywhere Data-Centric Work.</div> <div class="imageCredit">Jian Huang, University of Tennessee</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Jian&nbsp;Huang</div> <div class="imageTitle">Tapestry Presentation</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we studied how to carry out data-intensive work using portable personal devices. Albeit some of such devices are still not ready for widespread practical use, especially mixed reality devices like Hololens, the fundamental scientific question we can ask is exciting. That is, what kind of infrastructure can deliver the same kind of user experience to remote settings, where users have only their personal devices, such as laptop, tablet, smart phone, and VR/AR goggles.  Hence, we coined the term "Everywhere Data-Centric Work". Our technological vision revolved around making innovative use of cloud computing. While the concept of our project predated 2020, the ongoing pandemic has made remote work and remote learning prevalent throughout the world. This disruption gives our research even stronger relevance than we had originally envisioned.  Through our work, we have developed new methods to bridge two traditionally separated architectural domains: (1) large systems with high-scalability parallelism that can provide throughput and elasticity, (2) high-portability interactive objects that can be embedded into small applications.  A key example outcome is that authentic interactive user experiences with large scientific datasets can be embedded into a Wikipedia webpage flexibly. The extra in-application memory overheads are on the order of a couple of megabytes. The extra loading time of an augmented Wikipedia page is about 150 milliseconds. At such low overheads, an augmented Wikipedia page can have live 3D visualization that becomes live instantaneously. Such augmented Wikipedia pages can be loaded and used on laptop, tablets, smart phones alike. In addition, the same technology supports high-end user devices such as a PowerWall display as well as Hololens too. The same technology supports interactive use cases as well as moviemaking and movie editing.  Ultimately, our project has expanded the application space that can make use of high-end, always parallel rendering and only needs to pay a minimum amount of overheads. Each live scientific visualization to embed requires adding only 2-3 lines of code in Javascript. In all cases, standard Amazon AWS cloud instances can be used. In order to support up to 20 concurrent users at typical desktop resolutions, the AWS cost is only $4.59/hour.  The demonstration of this feasibility, and the entire research infrastructure being made open source, has broad implications on how online tools can support Everywhere Data-Centric Work and enable interactivity with large datasets at superior cost efficiency and system scalability.       Last Modified: 11/24/2020       Submitted by: Jian Huang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
