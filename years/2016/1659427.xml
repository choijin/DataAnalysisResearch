<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC* Networking Infrastructure: A Science DMZ to Enable Friction-Free Scientific Data Workflows</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2017</AwardEffectiveDate>
<AwardExpirationDate>03/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>498961.00</AwardTotalIntnAmount>
<AwardAmount>498961</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The University of Kansas (KU) is implementing a 100 Gigabit per second (Gbps) Science DMZ network for research scientists which provides a separate network for dedicated scientific research traffic workflows.  This serves many research projects at the Center for Research Computing (CRC), the Center for Remote Sensing of Ice Sheets (CReSIS), the Genome Sequencing Core (GSC), the Biodiversity Institute (BI), and the Microscopy and Analytical Imaging laboratory (MAI).  The project team is composed of domain scientists, computer scientists, and KU Information Technology administration.  The common themes uniting these diverse scientific areas are enhanced external institutional collaboration, data sharing, remote data transfer, and educational outreach.&lt;br/&gt;&lt;br/&gt;The infrastructure upgrades are the foundation for a network architecture that is anchored by a dedicated 100 Gbps research network backbone devoted to prioritizing research data flows and is easily extensible for adding new connections.  This design is modeled after the successful Science DMZ network architecture from the Energy Sciences Network (ESnet), which has been replicated at more than 100 universities.  The project addresses the ?last mile? network bottleneck and the campus enterprise designed network architecture.  The current network design places barriers on research data traffic on campus enterprise networks that compete for limited network bandwidth.  This project elevates scientific research data at KU to transfer at higher speeds, which enhances interdisciplinary and external collaborations, and enables current and future scientific access to friction-free networks.</AbstractNarration>
<MinAmdLetterDate>12/21/2016</MinAmdLetterDate>
<MaxAmdLetterDate>11/15/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1659427</AwardID>
<Investigator>
<FirstName>Eduardo</FirstName>
<LastName>Rosa-Molinar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eduardo Rosa-Molinar</PI_FULL_NAME>
<EmailAddress>erm@ku.edu</EmailAddress>
<PI_PHON>7858643091</PI_PHON>
<NSF_ID>000513697</NSF_ID>
<StartDate>01/23/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Eduardo</FirstName>
<LastName>Rosa-Molinar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eduardo Rosa-Molinar</PI_FULL_NAME>
<EmailAddress>erm@ku.edu</EmailAddress>
<PI_PHON>7858643091</PI_PHON>
<NSF_ID>000513697</NSF_ID>
<StartDate>12/21/2016</StartDate>
<EndDate>01/23/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Paden</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John Paden</PI_FULL_NAME>
<EmailAddress>paden@ku.edu</EmailAddress>
<PI_PHON>7858643441</PI_PHON>
<NSF_ID>000585428</NSF_ID>
<StartDate>12/21/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dan</FirstName>
<LastName>Voss</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dan Voss</PI_FULL_NAME>
<EmailAddress>dan.voss@ku.edu</EmailAddress>
<PI_PHON>7858643441</PI_PHON>
<NSF_ID>000691417</NSF_ID>
<StartDate>12/21/2016</StartDate>
<EndDate>01/23/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bob</FirstName>
<LastName>Lim</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bob Lim</PI_FULL_NAME>
<EmailAddress>blim@ku.edu</EmailAddress>
<PI_PHON>7858643441</PI_PHON>
<NSF_ID>000729822</NSF_ID>
<StartDate>12/21/2016</StartDate>
<EndDate>11/15/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Kansas Center for Research Inc</Name>
<CityName>Lawrence</CityName>
<ZipCode>660457552</ZipCode>
<PhoneNumber>7858643441</PhoneNumber>
<StreetAddress>2385 IRVING HILL RD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Kansas</StateName>
<StateCode>KS</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>KS02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>076248616</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF KANSAS CENTER FOR RESEARCH, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007180078</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Kansas Center for Research Inc]]></Name>
<CityName>Lawrence</CityName>
<StateCode>KS</StateCode>
<ZipCode>660457568</ZipCode>
<StreetAddress><![CDATA[2385 Irving Hill Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Kansas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>KS02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~498961</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Center for Research Computing</strong></p> <p>The Arista 7508 reached End of Life and was replaced with a Dell Z9100 network switch. The proposed purchase of a Mellanox SN2700&nbsp; was changed to purchase an Arista 7280CR2A-30 for the increased buffer sizes. The Z9100 and 7280CR2A-30 were connected with a single 100 Gbps cable. The new perfSONAR computer is connected at 100 Gbps to the 7280CR2A-30 and was tuned using iperf to the perfSONAR computer housed at Price Computing Center. Using multiple streams of iperf, a throughput of 77 Gbps was achieved between the two perfSONAR computers. The network layout for the DTN was redesigned and was placed at Price Computer Center attached to the Juniper MX960 at 100 Gbps to have it closer to the border router. The CRC Community Cluster storage is mounted on the DTN through the DMZ network. Transfer speeds from the CRC Community Cluster storage to the Research File Storage maxed out at 5 Gbps. Transferring 1TB of data using DTN from the CRC Community Cluster storage to the PSC XSEDE site through Globus was completed in 1 hour and 33 minutes.</p> <p>&nbsp;</p> <p><strong>Genome Sequencing Core</strong></p> <p>The Arista 7508 reached End of Life and was replaced with a Dell Z9100 network switch. The proposed purchase of a Mellanox SN2700 was changed to an Arista 7280CR2A-30. The IlluminaCompute is connected at 10 Gbps to the Z9100 and the Z9100 is connected at 100 Gbps to the 7280CR2A-30. This allows transfers from the IlluminaCompute to have minimal hops directly to the DTN. Transfers of 50GB of sequence data done from the IlluminaCompute to a 1 Gbps connected laptop took 10 minutes. Transfers done to the Research File Storage were completed in 6 minutes. The IlluminaCompute storage is the bottleneck in the throughput. Due to using non-standard permissions on the IlluminaCompute, this aspect of the project is not complete, but progress is being made on exporting shared data.</p> <p>&nbsp;</p> <p><strong>&nbsp;</strong></p> <p><strong>Center for Remote Sensing of Ice Sheets</strong></p> <p>Due to the Dell S4048 switches&rsquo; becoming End of Life, CReSIS purchased the Dell S4148F-ON switch, and it was delivered in February 2019. The switch was installed in the cold room for operation in May. However, due to unforeseen difficulties with Dell&rsquo;s new operating system, Dell has not yet been able to configure the switch. Dell continues to debug the switch configuration and is currently expecting the switch to be online by mid-November 2019. Thus, at this point, CReSIS is unable to move forward with network tests. Once the switch configuration is completed, CReSIS will conduct the throughput tests to verify a minimum throughput of &gt;1 Gbps and verify the ability to send network traffic to our external collaborator (e.g. Indiana University Bloomington) over the Science DMZ network. These tests will include data transfers using Rsync and FTP.</p> <p>Existing infrastructure was replaced with Cat6A ethernet home runs to the 10Gbps switch, new Cat6A patch cables were cut and terminated for instrumentation computers, and necessary adjustments to locations of network drops were completed to ensure compliance with networking standards for the additional throughput. Appropriate 10Gbps capable cards were identified that would give the widest compatibility with diverse microscope instrument computer systems and were then installed to instrument computers identified as generating large data sets.</p><br> <p>            Last Modified: 11/14/2019<br>      Modified by: Eduardo&nbsp;Rosa-Molinar</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1659427/1659427_10468096_1573759030157_Figure1NSFScienceDMZReoirt--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1659427/1659427_10468096_1573759030157_Figure1NSFScienceDMZReoirt--rgov-800width.jpg" title="Figure 1."><img src="/por/images/Reports/POR/2019/1659427/1659427_10468096_1573759030157_Figure1NSFScienceDMZReoirt--rgov-66x44.jpg" alt="Figure 1."></a> <div class="imageCaptionContainer"> <div class="imageCaption">A notional diagram of KU network upgrade and the essential components along with data paths of the Science DMZ.</div> <div class="imageCredit">Riley J. Epperson,</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Eduardo&nbsp;Rosa-Molinar</div> <div class="imageTitle">Figure 1.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Center for Research Computing  The Arista 7508 reached End of Life and was replaced with a Dell Z9100 network switch. The proposed purchase of a Mellanox SN2700  was changed to purchase an Arista 7280CR2A-30 for the increased buffer sizes. The Z9100 and 7280CR2A-30 were connected with a single 100 Gbps cable. The new perfSONAR computer is connected at 100 Gbps to the 7280CR2A-30 and was tuned using iperf to the perfSONAR computer housed at Price Computing Center. Using multiple streams of iperf, a throughput of 77 Gbps was achieved between the two perfSONAR computers. The network layout for the DTN was redesigned and was placed at Price Computer Center attached to the Juniper MX960 at 100 Gbps to have it closer to the border router. The CRC Community Cluster storage is mounted on the DTN through the DMZ network. Transfer speeds from the CRC Community Cluster storage to the Research File Storage maxed out at 5 Gbps. Transferring 1TB of data using DTN from the CRC Community Cluster storage to the PSC XSEDE site through Globus was completed in 1 hour and 33 minutes.     Genome Sequencing Core  The Arista 7508 reached End of Life and was replaced with a Dell Z9100 network switch. The proposed purchase of a Mellanox SN2700 was changed to an Arista 7280CR2A-30. The IlluminaCompute is connected at 10 Gbps to the Z9100 and the Z9100 is connected at 100 Gbps to the 7280CR2A-30. This allows transfers from the IlluminaCompute to have minimal hops directly to the DTN. Transfers of 50GB of sequence data done from the IlluminaCompute to a 1 Gbps connected laptop took 10 minutes. Transfers done to the Research File Storage were completed in 6 minutes. The IlluminaCompute storage is the bottleneck in the throughput. Due to using non-standard permissions on the IlluminaCompute, this aspect of the project is not complete, but progress is being made on exporting shared data.        Center for Remote Sensing of Ice Sheets  Due to the Dell S4048 switches’ becoming End of Life, CReSIS purchased the Dell S4148F-ON switch, and it was delivered in February 2019. The switch was installed in the cold room for operation in May. However, due to unforeseen difficulties with Dell’s new operating system, Dell has not yet been able to configure the switch. Dell continues to debug the switch configuration and is currently expecting the switch to be online by mid-November 2019. Thus, at this point, CReSIS is unable to move forward with network tests. Once the switch configuration is completed, CReSIS will conduct the throughput tests to verify a minimum throughput of &gt;1 Gbps and verify the ability to send network traffic to our external collaborator (e.g. Indiana University Bloomington) over the Science DMZ network. These tests will include data transfers using Rsync and FTP.  Existing infrastructure was replaced with Cat6A ethernet home runs to the 10Gbps switch, new Cat6A patch cables were cut and terminated for instrumentation computers, and necessary adjustments to locations of network drops were completed to ensure compliance with networking standards for the additional throughput. Appropriate 10Gbps capable cards were identified that would give the widest compatibility with diverse microscope instrument computer systems and were then installed to instrument computers identified as generating large data sets.       Last Modified: 11/14/2019       Submitted by: Eduardo Rosa-Molinar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
