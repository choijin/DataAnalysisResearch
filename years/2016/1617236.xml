<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Collaborative Research: Detailed Shape and Reflectance Capture with Light Field Cameras</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A major transformation is occurring in the way we sense the visual world.  Traditional 2D photography is increasingly being replaced with light-field sensors that capture the full spatial and angular variation of the incoming light field, rather than simply pixels that integrate over incoming directions.  This development opens up the possibility of ubiquitous 3D imaging of our visual world.  Light-field sensors are particularly attractive as a depth acquisition device, since they are completely passive without needing to project light into the scene, and they do not experience a reduction in performance outdoors.  Moreover, the rich ray-space of light fields provides significant cues for recovering fine-scale depth.  However, current RGBD and light-field systems produce only coarse depth; while useful for tasks like refocusing images, the depth channel offers little benefit for photography beyond conventional 2D RGB images.  This research seeks to address these challenges, by developing practical algorithms for detailed 3D shape and reflectance capture with light-field cameras, coupled with a theoretical and experimental analysis of the achievable accuracy.  Project outcomes will have broad impact on diverse fields including computer graphics and virtual/augmented reality, enabling acquisition of high-quality detailed 3D shape and the subsequent use of the 3D geometry with computer-generated synthetic objects.  Methods to acquire 3D images, including on mobile sensors, will transform the photographic process from 2D to 3D, with immense industrial and societal impact.&lt;br/&gt;&lt;br/&gt;The PIs will address four important problems in light-field shape acquisition.  First, they will exploit the rich nature of light-field data, combining multiple cues (defocus, correspondence, shading, specularity) in a unified way to obtain the overall global 3D shape.  Moreover, they will seek to go beyond the common Lambertian reflectance assumption, developing a novel BRDF-invariant framework for surface reconstruction with general glossy materials like metals, plastics, or ceramics, while supporting textures and spatially-varying reflectance.  Another key objective will be to ground the practical results in a theoretical framework that can establish the limits of light-field camera shape resolution, and the signal-to-noise accuracy, and how this relates to novel designs for light-field cameras to obtain the best achievable resolution in 3D shape capture.  Finally, the PIs will move from overall shape to fine-scale surface detail, proposing new methods for shape/reflectance capture for fine-scale geometry like hair.  The ultimate goal is to enable a full 3D processing pipeline for photography, computer graphics and applications like virtual and augmented reality, which will bring ubiquitous 3D to the casual photographer.</AbstractNarration>
<MinAmdLetterDate>07/18/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/18/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617236</AwardID>
<Investigator>
<FirstName>Szymon</FirstName>
<LastName>Rusinkiewicz</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Szymon M Rusinkiewicz</PI_FULL_NAME>
<EmailAddress>smr@princeton.edu</EmailAddress>
<PI_PHON>6092587479</PI_PHON>
<NSF_ID>000379377</NSF_ID>
<StartDate>07/18/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[87 Prospect Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A major transformation is occurring in the way one senses the visual world.<br />2D photography is increasingly being replaced with light-field sensors that<br />capture the full spatial and angular variation of the incoming light field,<br />rather than simply pixels that integrate over incoming directions.&nbsp; These<br />developments open up the possibility of ubiquitous three-dimensional<br />imaging of our visual world, pushing everyday photography from 2D to 3D,<br />with the 3D equivalents of sites like Flickr or Youtube, the use of 3D<br />printers, and mobile 3D displays such as Google Cardboard.<br />&nbsp;<br />This project investigated the ultimate precision limits for 3D<br />reconstruction with light-field sensors.&nbsp; We observed an interesting<br />tradeoff:&nbsp; those reconstructions that utilized higher-resolution images<br />were able to capture more detail in the 3D model, but the smaller number of<br />views also meant that they were more susceptible to noise, especially<br />around silhouettes.&nbsp; Conversely, the reconstructions that started with more<br />(but lower-resolution) views were "cleaner," but contained less surface<br />detail.<br />&nbsp;<br />The project also investigated the problem of texture-based localization.<br />This was motivated by the fact that location-awsare applications play an<br />increasingly critical role in everyday life.&nbsp; However, satellite-based<br />localization (e.g., GPS) has limited accuracy and can be unusable in dense<br />urban areas and indoors.&nbsp; We introduce an image-based global localization<br />system that is accurate to a few millimeters and performs reliable<br />localization both indoors and outside.&nbsp; The key idea is to capture and<br />index distinctive local keypoints in ground textures.&nbsp; This is based on the<br />observation that ground textures including wood, carpet, tile, concrete,<br />and asphalt may look random and homogeneous, but all contain cracks,<br />scratches, or unique arrangements of fibers.&nbsp; These imperfections are<br />persistent, and can serve as local features.&nbsp; Our system incorporates a<br />downward-facing camera to capture the fine texture of the ground, together<br />with an image processing pipeline that locates the captured texture patch<br />in a compact database constructed offline.&nbsp; We demonstrate the capability<br />of our system to robustly, accurately, and quickly locate test images on<br />various types of outdoor and indoor ground surfaces.</p><br> <p>            Last Modified: 03/14/2020<br>      Modified by: Szymon&nbsp;M&nbsp;Rusinkiewicz</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A major transformation is occurring in the way one senses the visual world. 2D photography is increasingly being replaced with light-field sensors that capture the full spatial and angular variation of the incoming light field, rather than simply pixels that integrate over incoming directions.  These developments open up the possibility of ubiquitous three-dimensional imaging of our visual world, pushing everyday photography from 2D to 3D, with the 3D equivalents of sites like Flickr or Youtube, the use of 3D printers, and mobile 3D displays such as Google Cardboard.   This project investigated the ultimate precision limits for 3D reconstruction with light-field sensors.  We observed an interesting tradeoff:  those reconstructions that utilized higher-resolution images were able to capture more detail in the 3D model, but the smaller number of views also meant that they were more susceptible to noise, especially around silhouettes.  Conversely, the reconstructions that started with more (but lower-resolution) views were "cleaner," but contained less surface detail.   The project also investigated the problem of texture-based localization. This was motivated by the fact that location-awsare applications play an increasingly critical role in everyday life.  However, satellite-based localization (e.g., GPS) has limited accuracy and can be unusable in dense urban areas and indoors.  We introduce an image-based global localization system that is accurate to a few millimeters and performs reliable localization both indoors and outside.  The key idea is to capture and index distinctive local keypoints in ground textures.  This is based on the observation that ground textures including wood, carpet, tile, concrete, and asphalt may look random and homogeneous, but all contain cracks, scratches, or unique arrangements of fibers.  These imperfections are persistent, and can serve as local features.  Our system incorporates a downward-facing camera to capture the fine texture of the ground, together with an image processing pipeline that locates the captured texture patch in a compact database constructed offline.  We demonstrate the capability of our system to robustly, accurately, and quickly locate test images on various types of outdoor and indoor ground surfaces.       Last Modified: 03/14/2020       Submitted by: Szymon M Rusinkiewicz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
