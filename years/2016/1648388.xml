<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Fingertip Ranging with Micro Light-Field Cameras</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/01/2016</AwardEffectiveDate>
<AwardExpirationDate>11/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>224900.00</AwardTotalIntnAmount>
<AwardAmount>224900</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this project lies in its provision of a novel ranging capability whose quality, ease of placement, and scale in deployment will introduce new opportunities in robot operation in situations where there is uncertainty in the relative positions of parts and robots. These include small-batch production (where fixturing cannot be made cost effective), inspection and repair in confined spaces (where 3D sensing must be carried aboard the actuator), hazardous situations (where human presence incurs risk to life), and collaborative interaction with people (where inadvertent contact must be avoided). In the proposed development, robot grasp will be empowered with dynamic 3D range mapping at each fingertip, enabling direct computation of trajectories and velocities tailored to the geometry and structure about to be manipulated.  Extension of the technology to the larger challenge of 3D vision and object modeling offers economic impact in diverse applications.  These include autonomous and semi-autonomous vehicle navigation (drones, cars), virtual reality and augmented reality interfaces, 3D teleconferencing and communication, cultural site modeling, and immersive cinema.  Each is an area where increases in reliability and precision with decreases in power and computational cost can bring an application over the threshold in price/performance, into viability.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project will establish a new level of real-time passive visual perception in near-range for robot operation, providing 3D data for accurate, precise and rapid grasp. Binocular imaging systems have not demonstrated success in near range robotics due to the inability of their match-based methods to deliver reliable depth measures in complex settings where disparity range is large. Current methods using a few cameras are based on matching so make mistakes, use search that is exponential in covered range so are expensive, and deliver parsimonious descriptions of the world (point clouds) so are weak in descriptive power.  All of these diminish the reliability of their processing and the utility of their analysis in real-world applications.  The technology of this project combined with recent wafer-level integration module packages overcomes these limitations through use of dense sampling, extended baselines, and maintaining and exploiting image spatial continuity. The technical challenge involves mechanical and electrical design to enable micro light-field ranging with analysis on an embedded processor, near-field calibration of imagers/optics/system, and coordination of these with robot control for assessing measurement accuracy and precision. The project will result in high-quality frame-rate light-field ranging on a robot fingertip.</AbstractNarration>
<MinAmdLetterDate>12/02/2016</MinAmdLetterDate>
<MaxAmdLetterDate>12/02/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1648388</AwardID>
<Investigator>
<FirstName>Henry</FirstName>
<LastName>Baker</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME>Dr</PI_SUFX_NAME>
<PI_FULL_NAME>Henry H Baker</PI_FULL_NAME>
<EmailAddress>harlyn@epiimaging.com</EmailAddress>
<PI_PHON>6509491052</PI_PHON>
<NSF_ID>000711295</NSF_ID>
<StartDate>12/02/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>EPIImaging, LLC</Name>
<CityName>Los Altos</CityName>
<ZipCode>940243827</ZipCode>
<PhoneNumber>6509491052</PhoneNumber>
<StreetAddress>414 Paco Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080057464</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EPIIMAGING, LLC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[EPIImaging, LLC]]></Name>
<CityName>Los Altos</CityName>
<StateCode>CA</StateCode>
<ZipCode>940243827</ZipCode>
<StreetAddress><![CDATA[414 Paco Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8034</Code>
<Text>Hardware Components</Text>
</ProgramReference>
<ProgramReference>
<Code>8035</Code>
<Text>Hardware Devices</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~224900</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><em>Fingertip Ranging with Micro Light-Field Cameras</em> has been a business exploration aimed at evaluating 3D ranging at the fingertip of a robot during grasp operations.&nbsp; This task has earlier been performed with visual guidance at a high level (indication of target location) followed by a <em>move-sense</em> loop driven by tactile feedback.&nbsp; This can be slow, imprecise, and destructive.&nbsp; Our intention was to provide a robot hand with grasp final-stage servoing using 3D dynamic vision.</p> <p>The advance in image sensors and computational power brought by the past decade&rsquo;s revolution in mobile computing (i.e., smartphones) has given us the smaller-cheaper-better cameras and computational resources that now make such dynamic imaging possible.&nbsp; &nbsp;Further, an earlier analysis development in which the principal investigator was a major contributor, Epipolar Plane Image (EPI) analysis, brings significant advantage to such 3D imaging. EPI uses a special alignment of many imagers to bypass the usual expense and uncertainty of standard passive 3D ranging, and it is now facilitated by these recent advances in cameras and computers. &nbsp;EPI processing helped spawn the development of light-field analysis.</p> <p>Our challenge here has been to build an EPI multi-camera system to provide dynamic 3D data to a robot controller. &nbsp;Achieving this required advances in several areas:</p> <ul> <li>mechanical, optical, and electronic design of the imaging and analysis systems;  <ul> <li>PCB development to host the imagers (connectors being the bane of complex systems, we sought an integrated array with fabricated connectivity)</li> <li>A compact miniaturized package with careful matching of lenses and sensors for quality and coverage</li> <li>Computing covers a wide selection of tasks &ndash; needed were: synchronized multi-camera capture; DMA delivery; rapid computation over a large data set (in our case 17 imagers with about 5-8Gbs bandwidth); mastery of an embedded Operating System controlling a multi-and mixed-processor environment; and transmission of results to a remote action host for timely response.</li> </ul> </li> <li>demonstration of analysis algorithms that attain metric 3D information about the scene before the camera system;  <ul> </ul> </li> <li>calibration of the system to enable such analysis and its metric results <ul> </ul> </li> </ul> <p>During the project it became evident that the size of sensors we intended to use could not be properly synchronized and the mating electronics would be too large for sufficiently nearby placement.&nbsp; We chose to develop with larger components and do demonstration at that scale.&nbsp; Our company had other funding where this scale was relevant and had identified other markets were the larger format device could attract customers, so felt this adjustment to plan made good sense, even within the robotics context. Once feasibility was demonstrated and time passed to permit advance on the small sensors, we would return to fingertip placement.</p> <p>We achieved our goal of designing and building the imaging system and its infrastructure, and implementing an EPI 3D ranging computation on the device. The attached figures show the EPIModule from front, back, and in operation displaying its different viewpoints; and a sampling of EPI range results.</p> <p>During the project a VCSEL-based LIDAR commercial product was released that posed a competitive challenge to our developments.&nbsp; We obtained several of these and performed experiments in a robot setting to determine the device&rsquo;s utility.&nbsp; Its near-range operation was restricted &ndash; about a foot from the sensor, making it not ideal for close grasp work, but this provided data for assessing the utility of real-time range for more effective grasp operations (we are continuing this evaluation).</p> <p>It has also been clear that there are other application areas with requirements well served by our EPI imaging system. We identified several, including advanced 3D cinema, augmented and virtual reality, and &ndash; perhaps most importantly &ndash; autonomous vehicles (AV). Mid project we began receiving encouragement from Advisory Board members that we turn our attention to image-based AV ranging tasks.&nbsp; The data behind this &ndash; the rapid advances in self-driving car technology, the visibility of its public demonstrations, and the accelerated investment that has accompanied this growth &ndash; persuaded us to consider a pivot to this application.&nbsp; A market review (similar to that used in the NSF Bootcamp) led us to decide on this pivot.&nbsp; We continue to apply our technology to robotics and to advanced 3D cinema applications since these are areas where there is great interest in our technology and they are putting money on our table, but our focus is now to be on positioning our technology in the AV market.</p> <p>In summary, we met our objective of establishing the technical merit, feasibility, and commercial potential of the originally proposed research and development effort centered on an EPI processor. &nbsp;Successful development of the EPIModule positions us now for capitalizing on its utility as we pivot to the AV domain while continuing sales into the related areas of robotics and cinematic 3D capture.</p><br> <p>            Last Modified: 02/07/2018<br>      Modified by: Henry&nbsp;H&nbsp;Baker</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1648388/1648388_10466576_1518037534182_PublicReportfigure--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1648388/1648388_10466576_1518037534182_PublicReportfigure--rgov-800width.jpg" title="EPIModule and EPI range results"><img src="/por/images/Reports/POR/2018/1648388/1648388_10466576_1518037534182_PublicReportfigure--rgov-66x44.jpg" alt="EPIModule and EPI range results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">EPIImaging's EPIModule and typical EPI ranging results</div> <div class="imageCredit">Harlyn Baker, EPIImaging, LLC</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Henry&nbsp;H&nbsp;Baker</div> <div class="imageTitle">EPIModule and EPI range results</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Fingertip Ranging with Micro Light-Field Cameras has been a business exploration aimed at evaluating 3D ranging at the fingertip of a robot during grasp operations.  This task has earlier been performed with visual guidance at a high level (indication of target location) followed by a move-sense loop driven by tactile feedback.  This can be slow, imprecise, and destructive.  Our intention was to provide a robot hand with grasp final-stage servoing using 3D dynamic vision.  The advance in image sensors and computational power brought by the past decade?s revolution in mobile computing (i.e., smartphones) has given us the smaller-cheaper-better cameras and computational resources that now make such dynamic imaging possible.   Further, an earlier analysis development in which the principal investigator was a major contributor, Epipolar Plane Image (EPI) analysis, brings significant advantage to such 3D imaging. EPI uses a special alignment of many imagers to bypass the usual expense and uncertainty of standard passive 3D ranging, and it is now facilitated by these recent advances in cameras and computers.  EPI processing helped spawn the development of light-field analysis.  Our challenge here has been to build an EPI multi-camera system to provide dynamic 3D data to a robot controller.  Achieving this required advances in several areas:  mechanical, optical, and electronic design of the imaging and analysis systems;   PCB development to host the imagers (connectors being the bane of complex systems, we sought an integrated array with fabricated connectivity) A compact miniaturized package with careful matching of lenses and sensors for quality and coverage Computing covers a wide selection of tasks &ndash; needed were: synchronized multi-camera capture; DMA delivery; rapid computation over a large data set (in our case 17 imagers with about 5-8Gbs bandwidth); mastery of an embedded Operating System controlling a multi-and mixed-processor environment; and transmission of results to a remote action host for timely response.   demonstration of analysis algorithms that attain metric 3D information about the scene before the camera system;     calibration of the system to enable such analysis and its metric results      During the project it became evident that the size of sensors we intended to use could not be properly synchronized and the mating electronics would be too large for sufficiently nearby placement.  We chose to develop with larger components and do demonstration at that scale.  Our company had other funding where this scale was relevant and had identified other markets were the larger format device could attract customers, so felt this adjustment to plan made good sense, even within the robotics context. Once feasibility was demonstrated and time passed to permit advance on the small sensors, we would return to fingertip placement.  We achieved our goal of designing and building the imaging system and its infrastructure, and implementing an EPI 3D ranging computation on the device. The attached figures show the EPIModule from front, back, and in operation displaying its different viewpoints; and a sampling of EPI range results.  During the project a VCSEL-based LIDAR commercial product was released that posed a competitive challenge to our developments.  We obtained several of these and performed experiments in a robot setting to determine the device?s utility.  Its near-range operation was restricted &ndash; about a foot from the sensor, making it not ideal for close grasp work, but this provided data for assessing the utility of real-time range for more effective grasp operations (we are continuing this evaluation).  It has also been clear that there are other application areas with requirements well served by our EPI imaging system. We identified several, including advanced 3D cinema, augmented and virtual reality, and &ndash; perhaps most importantly &ndash; autonomous vehicles (AV). Mid project we began receiving encouragement from Advisory Board members that we turn our attention to image-based AV ranging tasks.  The data behind this &ndash; the rapid advances in self-driving car technology, the visibility of its public demonstrations, and the accelerated investment that has accompanied this growth &ndash; persuaded us to consider a pivot to this application.  A market review (similar to that used in the NSF Bootcamp) led us to decide on this pivot.  We continue to apply our technology to robotics and to advanced 3D cinema applications since these are areas where there is great interest in our technology and they are putting money on our table, but our focus is now to be on positioning our technology in the AV market.  In summary, we met our objective of establishing the technical merit, feasibility, and commercial potential of the originally proposed research and development effort centered on an EPI processor.  Successful development of the EPIModule positions us now for capitalizing on its utility as we pivot to the AV domain while continuing sales into the related areas of robotics and cinematic 3D capture.       Last Modified: 02/07/2018       Submitted by: Henry H Baker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
