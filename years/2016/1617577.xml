<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: New Perspectives on Mathematical Programming Relaxations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project will study methods for designing efficient heuristics with provable guarantees for hard optimization problems. Optimization problems arise in many different contexts (e.g. finding the best routes for delivery trucks, finding clusters in a social network, etc). We now know that for many such problems, we do not expect to design algorithms that are efficient (i.e. run in polynomial time) that produce optimal solutions. The field of approximation algorithms develops principled methods to design algorithms for such hard optimization problems. This project will advance our knowledge of mathematical programming relaxations ? a widely used tool in the design of such approximation algorithms. The idea here is as follows: First, express the problem in terms of integer variables with constraints on them. Next relax the problem by allowing variables to take on fractional values. The relaxed version can be solved exactly in polynomial time. Finally, map solutions to the fractional version back to integers, while attempting to maintain the quality of the solution as far as possible. The project will attempt to advance our understanding of the power of this technique and its limitations for fundamental problems in the field. In particular, the PI will investigate structure in problem instances where exact solutions to the original problem can be obtained from solving the relaxation, study new relaxations for basic problems such as k-means clustering. The PI will also study relaxations for problems where the goal is find a dense structure inside a graph and problems involving laying out the vertices of a graph on a line.&lt;br/&gt;&lt;br/&gt;Course materials for graduate and undergraduate courses will be developed distilling research results from this project, as well as new developments in the field. The exact recovery questions investigated in this project for mathematical programming relaxations are of great interest in communities other than theoretical computer science; new insights here have the potential to influence and impact these other communities. Undergraduates will be involved in appropriately chosen pieces of the project so as to expose them to research at an early stage. The PI will encourage the participation of women and minorities in this research. He will organize outreach and community building activities at Stanford, leveraging his experience with organizing such activities in the past.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will study and shed new light on mathematical programming relaxations for hard combinatorial optimization problems in various settings. The broad goals of this project are:&lt;br/&gt;&lt;br/&gt;1. Exact recovery via mathematical programming: Investigate and understand the phenomenon of mathematical programming relaxations returning integer solutions. The PI will apply this new lens to linear programming (LP) and semidefinite programming (SDP) relaxations for several problems. Specifically, the PI will study SDP relaxations for computing transformation invariant metrics on sets of points and solving approximate graph isomorphism for correlated random graph models (introduced recently in the study of de-anonymization and privacy in social networks).&lt;br/&gt;&lt;br/&gt;2. Clustering problems: Investigate a recently introduced SDP relaxation for the k-means objective to obtain a better worst case approximation factor. The PI will also investigate the performance of this relaxation on random models of clustered points that better represent instances in practice.&lt;br/&gt;&lt;br/&gt;3. Densest subgraph: Investigate hypergraph generalizations of the k-densest-subgraph problem and a close variant, the smallest-m-edge-subgraph, to understand the best approximation factor achievable and whether natural planted instances of random hypergraphs suggest a natural limit for efficient approximation algorithms. The PI will also study lower bounds for densest subgraph, to understand the Lasserre hierarchy for planted instances that are predicted to be the hardest for the problem.&lt;br/&gt;&lt;br/&gt;4. Layout problems: The PI will study two different classical optimization problems involving mapping the vertices of a (un)directed graph to a line: minimizing (a) Cutwidth, and (b) Feedback Arc Set, to improve the worst case approximation factors. For cutwidth, the PI will investigate a new mathematical programming relaxation motivated by lift-and-project hierarchies. For Feedback Arc Set, the PI will study a new semidefinite programming relaxation.&lt;br/&gt;&lt;br/&gt;5. Constructive Discrepancy: Recent advances in designing efficient algorithms for minimizing discrepancy suggest that new breakthroughs are possible. A recent result builds on a breakthrough on the Kadison-Singer problem to show that the integrality gap of a natural relaxation for Asymmetric Traveling Salesman is small. The PI will investigate whether this existential result can be made algorithmic. The PI will also study the Beck-Fiala and Komlos conjectures, where efficient algorithms can lead to new progress.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>06/03/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617577</AwardID>
<Investigator>
<FirstName>Moses</FirstName>
<LastName>Charikar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Moses Charikar</PI_FULL_NAME>
<EmailAddress>moses@cs.stanford.edu</EmailAddress>
<PI_PHON>6507254404</PI_PHON>
<NSF_ID>000488746</NSF_ID>
<StartDate>06/03/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943059025</ZipCode>
<StreetAddress><![CDATA[353 Serra Mall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project made advances in several different areas, initiating significant new lines of work in the areas of hierarchical clustering, learning with untrusted data and high dimensional statistics, as well as making progress on several other problems.</p> <p>A hierarchical clustering of a data set is a sequence of partitions of the data set with increasing granularity, where each partition refines previous partitions in the sequence. Algorithms to produce such hierarchical clusterings have been proposed since the inception of cluster analysis of data in the 60's. This project made several leaps in our understanding of a new objective function for hierarchical clustering introduced by Dasgupta. The project resulted in the current best known approximation algorithms for this objective, an understanding of the power of mathematical programming relaxations in designing approximations, and fundamental barriers to achieving tighter approximations. The project studied hierarchical clustering with structural constraints representing prior information, variants of the objective introduced to understand the performance of average-linkage clustering, as well hierarchical clustering on Euclidean data.</p> <p>The project studied machine learning problems in the setting where some of the data comes from untrusted sources that might be malicious -- a significant challenge for machine learning systems. In the setting where only a small fraction of the data comes from trusted sources, techniques were developed to produce a small set of hypotheses&nbsp; such that one of them is guaranteed to be close to the correct one. Convex mathematical programming relaxations played a significant role in the design of these algorithms. The project also proposed a criterion called resilience which allows properties of a dataset to be robustly computed even in the presence of a large fraction of adversarially corrupted data.</p> <p>The project also developed algorithmic techniques for high dimensional kernel density estimation. This is a basic primitive in non-parametric statistics, where we take a discrete set of points and smear each point to produce a heat map over the entire space. The challenge is to preprocess the dataset so as to efficiently compute the value of this heat map at any point in the space, without making a linear pass over the data each time. The key observation was that locality sensitive hashing, a technique previously developed for high dimensional nearest neighbor search is a useful building block in developing algorithms for kernel density estimation. This idea has been developed over a series of papers during the project, first developing the theoretical ideas and eventually, evaluating the performance of these ideas in practice. The empirical evaluation introduced efficient diagnostic tests that help determine when these sophisticated ideas give an improvement over other approaches, allowing practitioners to choose the best algorithm for their own datasets.</p> <p>A significant number of results were obtained outside the three thrusts highlighted above. For example, the project developed the first efficient non-trivial approximation algorithm for profile maximum likelihood, a variant of maximum likelihood for random samples of discrete distributions, where we are interested in the profile of the sample, i.e. the number of elements that appear once in the sample, the number of elements that appear twice and so on. The goal is to find a distribution that maximizes the likelihood of producing random samples with this particular profile. The efficient approximation algorithm obtained directly leads to a generic method for estimating symmetric properties of distributions, i.e. properties such as entropy that are invariant to permuting labels.</p> <p>Other problems studied during the project include: graph partitioning problems with local objectives, online min-cost bipartite matching with delays, dynamic matching, edit distance, dynamic time warping, quadratic tensor completion, counting motifs in temporal graphs and Stochastic Gradient Langevin Dynamics.</p> <p>Several students were trained through their involvement in this project, including one undergraduate, one Masters student and several PhD students.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/29/2020<br>      Modified by: Moses&nbsp;Charikar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project made advances in several different areas, initiating significant new lines of work in the areas of hierarchical clustering, learning with untrusted data and high dimensional statistics, as well as making progress on several other problems.  A hierarchical clustering of a data set is a sequence of partitions of the data set with increasing granularity, where each partition refines previous partitions in the sequence. Algorithms to produce such hierarchical clusterings have been proposed since the inception of cluster analysis of data in the 60's. This project made several leaps in our understanding of a new objective function for hierarchical clustering introduced by Dasgupta. The project resulted in the current best known approximation algorithms for this objective, an understanding of the power of mathematical programming relaxations in designing approximations, and fundamental barriers to achieving tighter approximations. The project studied hierarchical clustering with structural constraints representing prior information, variants of the objective introduced to understand the performance of average-linkage clustering, as well hierarchical clustering on Euclidean data.  The project studied machine learning problems in the setting where some of the data comes from untrusted sources that might be malicious -- a significant challenge for machine learning systems. In the setting where only a small fraction of the data comes from trusted sources, techniques were developed to produce a small set of hypotheses  such that one of them is guaranteed to be close to the correct one. Convex mathematical programming relaxations played a significant role in the design of these algorithms. The project also proposed a criterion called resilience which allows properties of a dataset to be robustly computed even in the presence of a large fraction of adversarially corrupted data.  The project also developed algorithmic techniques for high dimensional kernel density estimation. This is a basic primitive in non-parametric statistics, where we take a discrete set of points and smear each point to produce a heat map over the entire space. The challenge is to preprocess the dataset so as to efficiently compute the value of this heat map at any point in the space, without making a linear pass over the data each time. The key observation was that locality sensitive hashing, a technique previously developed for high dimensional nearest neighbor search is a useful building block in developing algorithms for kernel density estimation. This idea has been developed over a series of papers during the project, first developing the theoretical ideas and eventually, evaluating the performance of these ideas in practice. The empirical evaluation introduced efficient diagnostic tests that help determine when these sophisticated ideas give an improvement over other approaches, allowing practitioners to choose the best algorithm for their own datasets.  A significant number of results were obtained outside the three thrusts highlighted above. For example, the project developed the first efficient non-trivial approximation algorithm for profile maximum likelihood, a variant of maximum likelihood for random samples of discrete distributions, where we are interested in the profile of the sample, i.e. the number of elements that appear once in the sample, the number of elements that appear twice and so on. The goal is to find a distribution that maximizes the likelihood of producing random samples with this particular profile. The efficient approximation algorithm obtained directly leads to a generic method for estimating symmetric properties of distributions, i.e. properties such as entropy that are invariant to permuting labels.  Other problems studied during the project include: graph partitioning problems with local objectives, online min-cost bipartite matching with delays, dynamic matching, edit distance, dynamic time warping, quadratic tensor completion, counting motifs in temporal graphs and Stochastic Gradient Langevin Dynamics.  Several students were trained through their involvement in this project, including one undergraduate, one Masters student and several PhD students.          Last Modified: 05/29/2020       Submitted by: Moses Charikar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
