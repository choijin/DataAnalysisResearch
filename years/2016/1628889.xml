<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Cross-Validation for High-Dimensional and Nonparametric Models in Econometrics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2016</AwardEffectiveDate>
<AwardExpirationDate>12/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>238700.00</AwardTotalIntnAmount>
<AwardAmount>238700</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nancy Lutz</SignBlockName>
<PO_EMAI>nlutz@nsf.gov</PO_EMAI>
<PO_PHON>7032927280</PO_PHON>
</ProgramOfficer>
<AbstractNarration>It is now well-known in empirical economics that nonparametric/high-dimensional methods are important because these methods substantially reduce misspecification of the economic models, thereby minimizing the possibility of inconsistent estimation of model parameters. The implementation of nonparametric/high-dimensional methods, however, typically requires the practitioners to select a smoothing parameter. Nevertheless, adequate data-driven procedures for selecting smoothing parameters are not readily available, which might explain the slow adoption of nonparametric methods in applied work. In addition, estimators based on an inappropriate choice of the smoothing parameter may have large estimation or approximation error, and hence lead to detrimental mistakes in policy recommendations. This project therefore focuses on how to select a smoothing parameter for Lasso estimators and sieve estimators, because Lasso estimators are commonly used to estimate high-dimensional models using "big data" and machine learning techniques, and sieve estimators are one of important approaches to estimate non-parametric models. The investigators will derive theoretical results for these estimators and provide theoretically justified procedures to select the smoothing parameter. &lt;br/&gt;&lt;br/&gt;This project investigates two lines of research: first, convergence rates of cross-validated Lasso estimator, and second, convergence rates of cross-validated sieve estimator. In the first line of research, the investigators will provide novel and practical cross-validation-based procedure to choose the regularization parameter for the Lasso estimator and show that the cross-validated Lasso estimator achieves the fastest possible rate of convergence up-to the logarithmic factor under certain conditions. In the second line of research, the investigators demonstrate that cross-validation produces sieve estimators with optimal rates of convergence for a large class of sieve estimators. This project will thus derive theoretical results for a large class of non-linear nonparametric/high-dimensional estimators when the smoothing/regularization parameter for the estimators is selected using cross-validation. Therefore, this project will provide a data-driven procedure for selecting the smoothing parameter, which should be of interest to researchers in diverse academic fields as well as in industry.</AbstractNarration>
<MinAmdLetterDate>08/31/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1628889</AwardID>
<Investigator>
<FirstName>Zhipeng</FirstName>
<LastName>Liao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhipeng Liao</PI_FULL_NAME>
<EmailAddress>zhipeng.liao@econ.ucla.edu</EmailAddress>
<PI_PHON>3107945427</PI_PHON>
<NSF_ID>000662942</NSF_ID>
<StartDate>08/31/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Denis</FirstName>
<LastName>Chetverikov</LastName>
<PI_MID_INIT>N</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Denis N Chetverikov</PI_FULL_NAME>
<EmailAddress>Chetverikov@econ.ucla.edu</EmailAddress>
<PI_PHON>3107940102</PI_PHON>
<NSF_ID>000716026</NSF_ID>
<StartDate>08/31/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951477</ZipCode>
<StreetAddress><![CDATA[8283 Bunche Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1320</Code>
<Text>Economics</Text>
</ProgramElement>
<ProgramReference>
<Code>1320</Code>
<Text>ECONOMICS</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~238700</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main purpose of the project is to provide cross-validation-based smoothing parameter selection procedures for: (i) nonparametric series estimation of conditional quantile functions; and (ii) Lasso estimation of high-dimensional models with the number of regressors is possibly larger than the sample size. We establish the properties of the series quantile estimator and the Lasso estimator when the smoothing parameters are determined by the <em>V</em>-fold cross-validation where <em>V</em> denotes the number of subsamples in cross-validation.</p> <p>For the nonparametric quantile estimation, the <em>V</em>-fold cross-validated series quantile estimator is evaluated under a criterion-based distance and the L<sub>2</sub>-norm. We show that with probability approaching 1, the criterion-based distance of the cross-validated estimator to the true unknown function is bounded from above by that of the oracle estimator multiplied by some finite constant C(<em>V</em>) which depends on <em>V</em>. When <em>V</em> grows with sample size, the upper bound C(<em>V</em>) approaches 1 from above which implies that the cross-validated estimator is asymptotically optimal under the criterion-based distance. Among different cross-validated estimators based on different sample splitting schemes, we find that the cross-validated estimator based on the equal sample splitting scheme has the smallest upper bound C(<em>V</em>). Moreover, we show that the cross-validated quantile series estimator has the optimal convergence rate under the L<sub>2</sub>-norm.</p> <p>For the Lasso estimation of high dimensional models, the <em>V</em>-fold cross-validated Lasso estimator is investigated under the prediction norm, L<sub>2</sub>-norm and L<sub>1</sub>-norm. We show that in the model with the Gaussian noise and under fairly general assumptions on the candidate set of values of the penalty parameter, the estimation error of the cross-validated Lasso estimator converges to zero in the prediction norm with the (<em>s*</em>log(<em>p</em>)/<em>n</em>)<span style="font-size: 10px;"><sup>1/2</sup></span>(log(<em>pn</em>))<span style="font-size: 10px;"><sup>1/2</sup></span>&nbsp;rate, where <em>n</em> is the sample size of available data, <em>p</em> is the number of covariates, and <em>s</em> is the number of non-zero coefficients in the model. Thus, the cross-validated Lasso estimator achieves the fastest possible rate of convergence in the prediction norm up to a small logarithmic factor (log(<em>pn</em>))<sup>1/2</sup>, and similar conclusions apply for the convergence rate both in L<sub>2</sub>-norm and L<sub>1</sub>-norm. Importantly, our results cover the case when <em>p</em> is (potentially much) larger than <em>n</em> and also allow for the case of non-Gaussian noise.</p> <p>Although our primary focus in this project is on economics and econometrics audience, our results also benefit statistics and machine learning literature since cross-validation techniques have been for a long time of interest in these disciplines as well. Providing theoretical foundation for the use of cross-validation to select smoothing/regularization parameters for the Lasso estimator and for the nonparametric quantile estimator also has a broad impact on empirical researchers by explaining conditions when the cross-validation can be used, and perhaps more importantly, when it cannot be used. The project also has direct educational impact by involving help of graduate students. Results in this project has been incorporated in a second-year graduate econometrics class.</p><br> <p>            Last Modified: 04/29/2019<br>      Modified by: Zhipeng&nbsp;Liao</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main purpose of the project is to provide cross-validation-based smoothing parameter selection procedures for: (i) nonparametric series estimation of conditional quantile functions; and (ii) Lasso estimation of high-dimensional models with the number of regressors is possibly larger than the sample size. We establish the properties of the series quantile estimator and the Lasso estimator when the smoothing parameters are determined by the V-fold cross-validation where V denotes the number of subsamples in cross-validation.  For the nonparametric quantile estimation, the V-fold cross-validated series quantile estimator is evaluated under a criterion-based distance and the L2-norm. We show that with probability approaching 1, the criterion-based distance of the cross-validated estimator to the true unknown function is bounded from above by that of the oracle estimator multiplied by some finite constant C(V) which depends on V. When V grows with sample size, the upper bound C(V) approaches 1 from above which implies that the cross-validated estimator is asymptotically optimal under the criterion-based distance. Among different cross-validated estimators based on different sample splitting schemes, we find that the cross-validated estimator based on the equal sample splitting scheme has the smallest upper bound C(V). Moreover, we show that the cross-validated quantile series estimator has the optimal convergence rate under the L2-norm.  For the Lasso estimation of high dimensional models, the V-fold cross-validated Lasso estimator is investigated under the prediction norm, L2-norm and L1-norm. We show that in the model with the Gaussian noise and under fairly general assumptions on the candidate set of values of the penalty parameter, the estimation error of the cross-validated Lasso estimator converges to zero in the prediction norm with the (s*log(p)/n)1/2(log(pn))1/2 rate, where n is the sample size of available data, p is the number of covariates, and s is the number of non-zero coefficients in the model. Thus, the cross-validated Lasso estimator achieves the fastest possible rate of convergence in the prediction norm up to a small logarithmic factor (log(pn))1/2, and similar conclusions apply for the convergence rate both in L2-norm and L1-norm. Importantly, our results cover the case when p is (potentially much) larger than n and also allow for the case of non-Gaussian noise.  Although our primary focus in this project is on economics and econometrics audience, our results also benefit statistics and machine learning literature since cross-validation techniques have been for a long time of interest in these disciplines as well. Providing theoretical foundation for the use of cross-validation to select smoothing/regularization parameters for the Lasso estimator and for the nonparametric quantile estimator also has a broad impact on empirical researchers by explaining conditions when the cross-validation can be used, and perhaps more importantly, when it cannot be used. The project also has direct educational impact by involving help of graduate students. Results in this project has been incorporated in a second-year graduate econometrics class.       Last Modified: 04/29/2019       Submitted by: Zhipeng Liao]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
