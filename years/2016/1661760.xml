<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: F: DKA: Collaborative Research: Randomized Numerical Linear Algebra (RandNLA) for multi-linear and non-linear data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>246664.00</AwardTotalIntnAmount>
<AwardAmount>246664</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Data are often modeled as matrices; and, as a result, linear algebraic algorithms such as matrix decompositions have proven extremely successful in the analysis of many data sets.  Randomized Numerical Linear Algebra (RandNLA) integrates the complementary perspectives that Theoretical Computer Science and Numerical Linear Algebra bring to matrix computations, and it is a new paradigm for the design and analysis of such algorithms and for using the resulting insight to solve important scientific and societal problems.  Current RandNLA algorithms extract linear structure from data matrices.  The proposed work will extend RandNLA methods to multi-linear and other non-linear structure in data matrices.&lt;br/&gt;&lt;br/&gt;In more detail, the proposed work will investigate two important, non-linear, structural settings in order to start making progress towards using RandNLA approaches in situations where the underlying data exhibit non-linear structure: it will investigate how to design the next generation of RandNLA algorithms that can handle data that exhibit multi-linear structures captured by tensors; and it will investigate the applicability of RandNLA approaches to data that exhibit non-linear structure, as captured by non-linear dimensionality reduction techniques, local spectral methods, and related semi-supervised eigenvector tools. In addition, it will evaluate the proposed approaches on data applications where the PIs have significant expertise, such as the statistical analysis of population genetics data and astronomical data.  Broader impacts of the project include graduate and undergraduate training, workshops and code development for RandNLA.  For further information see the project web site at:http://www.stat.berkeley.edu/~mmahoney/projects/nsf-multilinear/</AbstractNarration>
<MinAmdLetterDate>09/21/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/21/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1661760</AwardID>
<Investigator>
<FirstName>Petros</FirstName>
<LastName>Drineas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Petros Drineas</PI_FULL_NAME>
<EmailAddress>pdrineas@purdue.edu</EmailAddress>
<PI_PHON>2039013682</PI_PHON>
<NSF_ID>000117416</NSF_ID>
<StartDate>09/21/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072114</ZipCode>
<StreetAddress><![CDATA[155 S. Grant Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~246664</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The research supported multiple extensions of Randomized Linear Algebra techniques beyond core Randomized Linear Algebra to include non-linear, multi-linear, first- and second-order optimization, and other areas.&nbsp; The theoretical methods used are central to the theoretical foundations of large-scale data analysis, and the broader impact lies in the usefulness of these methods in domains such as genetics, mass spec imaging, astronomy.&nbsp; Major accomplishments include demonstrating that Randomized Linear Algebra methods can improve optimization, that they can be used with local methods such as local spectral methods to identify non-linear structure in large-scale data, and that they can be used with multi-linear and non-negative constraints. We also presented multiple novel analyses of Krylov subspace methods, as well as tensor-based element-wise sampling algorithms.&nbsp; &nbsp;Implementations and applications of these methods in several scientific big data domains, including tera-byte scale implementations of low-rank approximation, and applications in populations genetics, were also supported.&nbsp; In terms of broader impact, major summer schools associated with SIAM (G2S3) and IAS (PCMI) were organized to disseminate these and related techniques to a broad range of students; and workshops and meetings such as MMDS 2016 and the Simons Institute program on Foundations of Data were organized.&nbsp; Related to these were articles such as a review article in CACM and a summary in SIAM News highlighting these events.&nbsp;</p> <p>Publications supported by this award include:</p> <p><strong>1.</strong> A. Chowdhuri, J. Yang, and P. Drineas, Randomized Iterative Algorithms for Fisher Discriminant Analysis, Conference on Uncertainty in Artificial Intelligence (UAI), 64, 2019. Selected for oral presentation.&nbsp;</p> <p><strong>2.</strong> P. Drineas and I. Ipsen, Low-Rank Matrix Approximations Do Not Need a Singular Value Gap, SIAM Journal on Matrix Analysis and Applications, 40(1),pp. 299-319, 2019.</p> <p><strong>3.</strong> A. Chowdhuri, J. Yang, and P. Drineas, An Iterative, Sketching-based Framework for Ridge Regression, Proceedings of the International Conference on Machine Learning (ICML), 2018.</p> <p><strong>4.</strong> P. Drineas, I. Ipsen, E. Kontopoulou, and M. Magdon-Ismail, Structural Convergence Results for Approximations of Dominant Subspaces from Block Krylov Spaces, SIAM Journal on Matrix Analysis and Applications, 39(2), pp. 567-586, 2018.</p> <p><strong>5.</strong> P. Drineas and M. W. Mahoney, Lectures on Randomized Numerical Linear Algebra, The Mathematics of Data, IAS/Park City Math. Ser., vol. 25, Amer. Math. Soc., Providence, RI, 2018.&nbsp;</p> <p><strong>6.</strong> K. Fountoulakis, A. Kundu, E. Kontopoulou, and P. Drineas, A Randomized Rounding Algorithm for Sparse PCA, ACM Transactions on Knowledge Discovery from Data (TKDD), 11(3), pp. 1-26, 2017.</p> <p><strong>7.</strong> K. Clarkson, P. Drineas, M. Magdon-Ismail, M. W. Mahoney, X. Meng, and D. P. Woodruff, Faster Robust Linear Regression, SIAM Journal on Computing, 45(3), pp. 763-810, 2016.</p> <p><strong>8.</strong> S. Paul, M. Magdon-Ismail, and P. Drineas, Feature Selection for Linear SVMs with Provable Guarantees, Pattern Recognition, 60, pp. 205-214, 2016.</p> <p><strong>9.</strong> M. W. Mahoney and P. Drineas, Structural Properties Underlying high-quality Randomized Numerical Linear Algebra algorithms, CRC Handbook on Big Data, pp. 137-154, 2016.</p> <p><strong>10.</strong> E. Gallopoulos, P. Drineas, I. Ipsen, and M. W. Mahoney, RandNLA, Pythons, and the CUR for your Data problems, SIAM News, p. 7, February 2016.</p> <p><strong>11.</strong> W. Mahoney and P. Drineas, RandNLA: Randomized Numerical Linear Algebra, Communications of the ACM (CACM), 59 (6), pp. 80-90, 2016.</p> <p><strong>12.</strong> A. Kundu, P. Drineas, and M. Magdon-Ismail, Approximating Sparse PCA from Incomplete Data, Proc. of Neural Information Processing Systems (NIPS), 2015.</p> <p><strong>13.</strong> S. Paul, M. Magdon-Ismail, and P. Drineas, Column Selection via Adaptive Sampling, Proc. of Neural Information Processing Systems (NIPS), 2015.</p> <p><strong>14.</strong> N. Nguyen, P. Drineas, and T. Tran, Tensor Sparsification via a Bound on the Spectral Norm of Random Tensors, Information and Inference: A Journal of the IMA, 4(3), pp. 195-229, 2015.</p><br> <p>            Last Modified: 09/24/2019<br>      Modified by: Petros&nbsp;Drineas</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The research supported multiple extensions of Randomized Linear Algebra techniques beyond core Randomized Linear Algebra to include non-linear, multi-linear, first- and second-order optimization, and other areas.  The theoretical methods used are central to the theoretical foundations of large-scale data analysis, and the broader impact lies in the usefulness of these methods in domains such as genetics, mass spec imaging, astronomy.  Major accomplishments include demonstrating that Randomized Linear Algebra methods can improve optimization, that they can be used with local methods such as local spectral methods to identify non-linear structure in large-scale data, and that they can be used with multi-linear and non-negative constraints. We also presented multiple novel analyses of Krylov subspace methods, as well as tensor-based element-wise sampling algorithms.   Implementations and applications of these methods in several scientific big data domains, including tera-byte scale implementations of low-rank approximation, and applications in populations genetics, were also supported.  In terms of broader impact, major summer schools associated with SIAM (G2S3) and IAS (PCMI) were organized to disseminate these and related techniques to a broad range of students; and workshops and meetings such as MMDS 2016 and the Simons Institute program on Foundations of Data were organized.  Related to these were articles such as a review article in CACM and a summary in SIAM News highlighting these events.   Publications supported by this award include:  1. A. Chowdhuri, J. Yang, and P. Drineas, Randomized Iterative Algorithms for Fisher Discriminant Analysis, Conference on Uncertainty in Artificial Intelligence (UAI), 64, 2019. Selected for oral presentation.   2. P. Drineas and I. Ipsen, Low-Rank Matrix Approximations Do Not Need a Singular Value Gap, SIAM Journal on Matrix Analysis and Applications, 40(1),pp. 299-319, 2019.  3. A. Chowdhuri, J. Yang, and P. Drineas, An Iterative, Sketching-based Framework for Ridge Regression, Proceedings of the International Conference on Machine Learning (ICML), 2018.  4. P. Drineas, I. Ipsen, E. Kontopoulou, and M. Magdon-Ismail, Structural Convergence Results for Approximations of Dominant Subspaces from Block Krylov Spaces, SIAM Journal on Matrix Analysis and Applications, 39(2), pp. 567-586, 2018.  5. P. Drineas and M. W. Mahoney, Lectures on Randomized Numerical Linear Algebra, The Mathematics of Data, IAS/Park City Math. Ser., vol. 25, Amer. Math. Soc., Providence, RI, 2018.   6. K. Fountoulakis, A. Kundu, E. Kontopoulou, and P. Drineas, A Randomized Rounding Algorithm for Sparse PCA, ACM Transactions on Knowledge Discovery from Data (TKDD), 11(3), pp. 1-26, 2017.  7. K. Clarkson, P. Drineas, M. Magdon-Ismail, M. W. Mahoney, X. Meng, and D. P. Woodruff, Faster Robust Linear Regression, SIAM Journal on Computing, 45(3), pp. 763-810, 2016.  8. S. Paul, M. Magdon-Ismail, and P. Drineas, Feature Selection for Linear SVMs with Provable Guarantees, Pattern Recognition, 60, pp. 205-214, 2016.  9. M. W. Mahoney and P. Drineas, Structural Properties Underlying high-quality Randomized Numerical Linear Algebra algorithms, CRC Handbook on Big Data, pp. 137-154, 2016.  10. E. Gallopoulos, P. Drineas, I. Ipsen, and M. W. Mahoney, RandNLA, Pythons, and the CUR for your Data problems, SIAM News, p. 7, February 2016.  11. W. Mahoney and P. Drineas, RandNLA: Randomized Numerical Linear Algebra, Communications of the ACM (CACM), 59 (6), pp. 80-90, 2016.  12. A. Kundu, P. Drineas, and M. Magdon-Ismail, Approximating Sparse PCA from Incomplete Data, Proc. of Neural Information Processing Systems (NIPS), 2015.  13. S. Paul, M. Magdon-Ismail, and P. Drineas, Column Selection via Adaptive Sampling, Proc. of Neural Information Processing Systems (NIPS), 2015.  14. N. Nguyen, P. Drineas, and T. Tran, Tensor Sparsification via a Bound on the Spectral Norm of Random Tensors, Information and Inference: A Journal of the IMA, 4(3), pp. 195-229, 2015.       Last Modified: 09/24/2019       Submitted by: Petros Drineas]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
