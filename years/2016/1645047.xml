<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Active Citizen Engagement to Enable Lifecycle Management of Infrastructure Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robin Dillon-Merrill</SignBlockName>
<PO_EMAI>rdillonm@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Flaws and defects in structures evolve gradually over a structure's lifetime, and potential degradation must be evaluated periodically to make sound management decisions and set repair priorities. Human observation, still the predominant mechanism for such evaluation, is time-consuming and costly. Citizen science and crowdsourcing provide opportunities to collect large numbers of photos of certain structures, from many perspectives, at frequent intervals, and under many conditions. Such visual data reach across both time and space, enabling a detailed record of deterioration over time. This EArly-concept Grant for Exploratory Research (EAGER) project will exploit the latest knowledge in computer vision to automate many tasks related to lifecycle structural evaluation and management, reducing both lifecycle cost and risk. The methodology developed within this project will also launch new opportunities for structural engineers seeking to exploit data science to address a broad range of structural engineering problems. A streaming video demonstrating the methodology on our target structure will be developed for broad dissemination and student engagement. &lt;br/&gt;&lt;br/&gt;Everyday images from citizens, not trained as engineers, are very different than the types of images than engineers capture. Large portions of these images have information irrelevant for engineering purposes and automated processing of these images would generate faulty conclusions. Furthermore, these images are collected from random locations and perspectives, and lack scale and orientation information. However, these barriers can be overcome. This project will incorporate essential knowledge about the structural evaluation process to enable the use of these images for engineering purposes. Geometric relationships between each of the query images and the model of the target structure will be computed by matching their local features. Automatic localization of each of these images with respect to the target structure will be performed, and relevant portion of the images, called the region of interest, will be extracted for structural evaluation by human or machine. Experimental validation will be performed using images collected from active engaged citizens through social media. A quantitative evaluation of the capabilities of the methodology will be performed, targeting especially vulnerable regions of a structure. This project will overcome the inherent challenges in using visual data from citizen scientists, facilitating a transformation in how we perform lifecycle structural evaluation.</AbstractNarration>
<MinAmdLetterDate>07/28/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1645047</AwardID>
<Investigator>
<FirstName>Shirley</FirstName>
<LastName>Dyke</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shirley J Dyke</PI_FULL_NAME>
<EmailAddress>sdyke@purdue.edu</EmailAddress>
<PI_PHON>7655887877</PI_PHON>
<NSF_ID>000312078</NSF_ID>
<StartDate>07/28/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072088</ZipCode>
<StreetAddress><![CDATA[585 Purdue Mall Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1642</Code>
<Text>Special Initiatives</Text>
</ProgramElement>
<ProgramReference>
<Code>043Z</Code>
<Text>PPSR- Public Participation in Scientific</Text>
</ProgramReference>
<ProgramReference>
<Code>1638</Code>
<Text>INFRAST MGMT &amp; EXTREME EVENTS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="BodyParCxSpFirst"><strong>Citizen science&nbsp;</strong>and&nbsp;<strong>crowdsourcing&nbsp;</strong>provide opportunities to acquire large quantities of photographs of structures, from many perspectives, at frequent intervals, and under many conditions providing a plethora of photos to use for inspections. Many such photos are taken of structures with historical significance or that have large numbers of visitors, and thus they contain valuable information that can inform periodic condition assessment, and can be useful to document the actual condition of a given structure over many years.&nbsp;</p> <p class="BodyParCxSpMiddle">In this project, we devised a realistic approach to visual lifecycle assessment of structural systems by enabling the active use of citizen science and crowdsourced images for such visual inspection. We developed a novel region-of-interest (ROI) localization technique that enables us to extract highly relevant and useful portions of citizen science images to conduct frequent and robust visual assessments. The technique was demonstrated and validated using a large volume of citizen science images (CIMs). Purdue University&rsquo;s Bell Tower, as shown in Fig 1, is used for our demonstration and validation. Vulnerable regions were identified and damage-like markings were temporarily placed on the structure, and images of those regions were extracted automatically to facilitate high quality inspection by trained engineers.&nbsp;</p> <p class="BodyParCxSpLast">Certain challenges needed to be overcome to make use of these images. When engineers collect images, the relevant regions of the structure (that need inspection) are contained in the image. The quality of the image is also high and the target of the image is the component being inspected. However, when these images are not collected by field engineers for the purpose of visual assessment, there are significant challenges in using these photos. The important regions of the images for inspection may be blurred (out of focus), the components that require inspection may not be in the central area of the image or may only occupy a small portion of the image, and the image may have occlusions covering the regions that may be most relevant for inspections. We automatically overcome these challenges to extract relevant and useful ROIs for inspection.&nbsp;</p> <p>There are several steps in the technique:&nbsp;</p> <ul> <li>Step 1 is to construct a robust baseline model of the structure (see Figure 2). 5,254 images were collected over several months under different conditions.</li> <li>Step 2 involves collecting CIMS of the Purdue Bell Tower at times when there are large numbers of people on campus, such as graduation (see Figure 1).&nbsp;</li> <li>Step 3 in the technique is to register each image to the baseline model. We find matches between the 3D points in the baseline model and the 2D features on each of the CIMs (see Figure 3).&nbsp;</li> <li>The next step is to localize and extract the relevant ROIs from each photo for three preselected target areas.&nbsp;</li> <li>The final step is to&nbsp;detect (crack) damage on the ROIs. Our method increases their reliability for accurate damage detection.</li> </ul> <p><strong>Education/Training</strong>: All of the students have gained experience with the following: high-quality data collection under crowdsourcing platform and documentation of experiments; traditional and novel image-based visual inspection techniques, along with the theory of computer vision methods; cutting-edge point cloud baseline model generation methods and tools; &nbsp;and technical writing and the research process.&nbsp;</p> <p><strong>Dissemination</strong>: We deployed a demonstration of these capabilities on a public web server to allow the public-at-large to try out the technique using their own images. The file package is uploaded in designsafe-ci.org for all to download and use.&nbsp;</p> <p><strong>Beyond</strong>: We explored two novel applications :&nbsp;</p> <ul> <li><span style="text-decoration: underline;">Automated Region-of-Interest Localization and Filtering for Vision-based Damage Detection</span>. Low-cost aerial vehicles with integrated high-resolution visual sensors can readily be used to collect visual data from structures. The large volumes of complex visual data, collected under uncontrolled circumstances (e.g. varied lighting, cluttered regions, occlusions, and variations in environmental conditions) impose a major challenge to past automated damage detection techniques. Large volumes of images would result in a high number of false positives. We developed an approach to automatically remove images with blur, low resolution and occlusions from the image set.&nbsp;<em>&nbsp;The capability of the technique is successfully demonstrated using a full-scale highway sign truss with welded connections.</em></li> <li><span style="text-decoration: underline;">Rapid assessment of building fa&ccedil;ades</span>. We developed an approach to perform rapid and accurate visual inspection of building fa&ccedil;ades using images collected from UAVs. An orthophoto of a flat region on the building (e.g., a fa&ccedil;ade or building side) is automatically constructed using structure-from-motion (SfM), followed by image stitching and blending. Based on the geometric relationship between the collected images and the constructed orthophoto, high-resolution region-of-interest are automatically extracted from the collected images, enabling efficient visual inspection.&nbsp;<em>The technique can be used to inspect any target components (e.g., broken window panes, spalling concrete, etc.) on any relatively flat surface of a structure.&nbsp;</em></li> </ul><br> <p>            Last Modified: 09/11/2018<br>      Modified by: Shirley&nbsp;J&nbsp;Dyke</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536697194384_Outcome_fig1-3--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536697194384_Outcome_fig1-3--rgov-800width.jpg" title="Citizen Scientists Public Upload"><img src="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536697194384_Outcome_fig1-3--rgov-66x44.jpg" alt="Citizen Scientists Public Upload"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Any citizen scientist can simply drag-and-drop their graduation photos to our drive</div> <div class="imageCredit">Jongseong Choi</div> <div class="imageSubmitted">Shirley&nbsp;J&nbsp;Dyke</div> <div class="imageTitle">Citizen Scientists Public Upload</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536678548441_3.DamageLocalization--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536678548441_3.DamageLocalization--rgov-800width.jpg" title="Damage Localization"><img src="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536678548441_3.DamageLocalization--rgov-66x44.jpg" alt="Damage Localization"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 3. Everyday photos and their background scenes can provide valuable information toward lifecycle monitoring of infrastructures.</div> <div class="imageCredit">Jongseong Choi</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Shirley&nbsp;J&nbsp;Dyke</div> <div class="imageTitle">Damage Localization</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536678400301_2.Pointcloudsidebyside--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536678400301_2.Pointcloudsidebyside--rgov-800width.jpg" title="Photo and Point Cloud"><img src="/por/images/Reports/POR/2018/1645047/1645047_10444914_1536678400301_2.Pointcloudsidebyside--rgov-66x44.jpg" alt="Photo and Point Cloud"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 2. Point cloud generated from the more than 5,000 photos taken of the Purdue Bell Tower, along with the photograph of the same.</div> <div class="imageCredit">Jongseong Choi</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Shirley&nbsp;J&nbsp;Dyke</div> <div class="imageTitle">Photo and Point Cloud</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Citizen science and crowdsourcing provide opportunities to acquire large quantities of photographs of structures, from many perspectives, at frequent intervals, and under many conditions providing a plethora of photos to use for inspections. Many such photos are taken of structures with historical significance or that have large numbers of visitors, and thus they contain valuable information that can inform periodic condition assessment, and can be useful to document the actual condition of a given structure over many years.  In this project, we devised a realistic approach to visual lifecycle assessment of structural systems by enabling the active use of citizen science and crowdsourced images for such visual inspection. We developed a novel region-of-interest (ROI) localization technique that enables us to extract highly relevant and useful portions of citizen science images to conduct frequent and robust visual assessments. The technique was demonstrated and validated using a large volume of citizen science images (CIMs). Purdue University?s Bell Tower, as shown in Fig 1, is used for our demonstration and validation. Vulnerable regions were identified and damage-like markings were temporarily placed on the structure, and images of those regions were extracted automatically to facilitate high quality inspection by trained engineers.  Certain challenges needed to be overcome to make use of these images. When engineers collect images, the relevant regions of the structure (that need inspection) are contained in the image. The quality of the image is also high and the target of the image is the component being inspected. However, when these images are not collected by field engineers for the purpose of visual assessment, there are significant challenges in using these photos. The important regions of the images for inspection may be blurred (out of focus), the components that require inspection may not be in the central area of the image or may only occupy a small portion of the image, and the image may have occlusions covering the regions that may be most relevant for inspections. We automatically overcome these challenges to extract relevant and useful ROIs for inspection.   There are several steps in the technique:   Step 1 is to construct a robust baseline model of the structure (see Figure 2). 5,254 images were collected over several months under different conditions. Step 2 involves collecting CIMS of the Purdue Bell Tower at times when there are large numbers of people on campus, such as graduation (see Figure 1).  Step 3 in the technique is to register each image to the baseline model. We find matches between the 3D points in the baseline model and the 2D features on each of the CIMs (see Figure 3).  The next step is to localize and extract the relevant ROIs from each photo for three preselected target areas.  The final step is to detect (crack) damage on the ROIs. Our method increases their reliability for accurate damage detection.   Education/Training: All of the students have gained experience with the following: high-quality data collection under crowdsourcing platform and documentation of experiments; traditional and novel image-based visual inspection techniques, along with the theory of computer vision methods; cutting-edge point cloud baseline model generation methods and tools;  and technical writing and the research process.   Dissemination: We deployed a demonstration of these capabilities on a public web server to allow the public-at-large to try out the technique using their own images. The file package is uploaded in designsafe-ci.org for all to download and use.   Beyond: We explored two novel applications :   Automated Region-of-Interest Localization and Filtering for Vision-based Damage Detection. Low-cost aerial vehicles with integrated high-resolution visual sensors can readily be used to collect visual data from structures. The large volumes of complex visual data, collected under uncontrolled circumstances (e.g. varied lighting, cluttered regions, occlusions, and variations in environmental conditions) impose a major challenge to past automated damage detection techniques. Large volumes of images would result in a high number of false positives. We developed an approach to automatically remove images with blur, low resolution and occlusions from the image set.  The capability of the technique is successfully demonstrated using a full-scale highway sign truss with welded connections. Rapid assessment of building fa&ccedil;ades. We developed an approach to perform rapid and accurate visual inspection of building fa&ccedil;ades using images collected from UAVs. An orthophoto of a flat region on the building (e.g., a fa&ccedil;ade or building side) is automatically constructed using structure-from-motion (SfM), followed by image stitching and blending. Based on the geometric relationship between the collected images and the constructed orthophoto, high-resolution region-of-interest are automatically extracted from the collected images, enabling efficient visual inspection. The technique can be used to inspect any target components (e.g., broken window panes, spalling concrete, etc.) on any relatively flat surface of a structure.         Last Modified: 09/11/2018       Submitted by: Shirley J Dyke]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
