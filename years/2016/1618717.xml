<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: New classes of optimization methods for nonconvex large scale machine learning models.</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>499143.00</AwardTotalIntnAmount>
<AwardAmount>499143</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Brass</SignBlockName>
<PO_EMAI>pbrass@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Intelligent systems that say, recommend music or movies based on past interests, or recognize faces or handwriting based on labeled samples, often learn from examples using "supervised learning."  The system tries to find a prediction function: a combination of feature values of the song, movie, image, or pen movements, that, on known inputs, produces score values that agree with known preferences. Some combinations may add with simple positive or negative weight parameters (The more guitar the better, or I really don't want accordion), while others can be more complex (nether too loud nor too soft).  If parameters for such a function can be found, then it can be hoped that, on a new input, the function will be a good approximation for the preference. &lt;br/&gt;&lt;br/&gt;In scientific computing, there are many optimization techniques used to find the best parameters.  The type called "gradient methods" is like a group hike that gets caught in the hills after dark; the members want to go downhill to return to the valley quickly, but take small steps so as not to trip. With a little light, the group can discover more about its vicinity to 1) suggest the best direction, 2) take longer steps without tripping, or 3) send different members in different directions so that someone finds the best way. When there are many parameters (not just latitude and longitude) there are many more directions to step.  Simple combinations define simple (aka convex) valleys, and many optimization-based learning methods (including support vector machines (SVM),  least squares, and logistic regression) have been effectively applied to find the best parameters.  More complex combinations that sometime lead to better learning, may define non-convex valleys, so the known methods may get stuck in dips or have to take very small steps -- they often lack theoretical convergence guarantees and do not always work well in practice. &lt;br/&gt;&lt;br/&gt;This project will explore non-convex optimization for machine learning with three techniques that are analogous to the hikers? use of the light: &lt;br/&gt;First, new techniques will be explored for exploiting approximate second-order derivatives within stochastic methods, which is expected to improve performance over stochastic gradient methods, avoid convergence to saddle points, and improve complexity guarantees over first-order approaches. Compared to other such techniques that have been proposed, these approaches will be unique as they will be set within trust-region frameworks, the exploration of which represents the second component of the project. Known for decades to offer improved performance for nonconvex optimization, trust region algorithms have not fully been explored for machine learning, and we believe that, when combined with second-order information, dramatic improvements (both theoretically and practically) can be achieved. Finally, for such methods to be efficient in large-scale settings, one needs to offer techniques for solving trust region subproblems in situations when all data might not be stored on a single computer. To address this,  parallel and distributed optimization techniques will be developed for solving trust region subproblems and related problems.  The three PIs work together with about a dozen students at Lehigh; their website is one way they disseminate research papers, software, and news of weekly activities. &lt;br/&gt;&lt;br/&gt;This project is funded jointly by NSF CISE CCF Algorithmic Foundations, and NSF MPS DMS Computational Mathematics.</AbstractNarration>
<MinAmdLetterDate>06/10/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/12/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618717</AwardID>
<Investigator>
<FirstName>Katya</FirstName>
<LastName>Scheinberg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katya Scheinberg</PI_FULL_NAME>
<EmailAddress>katyas@cornell.edu</EmailAddress>
<PI_PHON>9178737981</PI_PHON>
<NSF_ID>000544723</NSF_ID>
<StartDate>06/12/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Katya</FirstName>
<LastName>Scheinberg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katya Scheinberg</PI_FULL_NAME>
<EmailAddress>katyas@cornell.edu</EmailAddress>
<PI_PHON>9178737981</PI_PHON>
<NSF_ID>000544723</NSF_ID>
<StartDate>06/10/2016</StartDate>
<EndDate>06/12/2019</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Frank</FirstName>
<LastName>Curtis</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Frank E Curtis</PI_FULL_NAME>
<EmailAddress>fec309@lehigh.edu</EmailAddress>
<PI_PHON>6107584879</PI_PHON>
<NSF_ID>000549599</NSF_ID>
<StartDate>06/12/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Frank</FirstName>
<LastName>Curtis</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Frank E Curtis</PI_FULL_NAME>
<EmailAddress>fec309@lehigh.edu</EmailAddress>
<PI_PHON>6107584879</PI_PHON>
<NSF_ID>000549599</NSF_ID>
<StartDate>06/10/2016</StartDate>
<EndDate>06/12/2019</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Martin</FirstName>
<LastName>Takac</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martin Takac</PI_FULL_NAME>
<EmailAddress>mat614@lehigh.edu</EmailAddress>
<PI_PHON>6105709720</PI_PHON>
<NSF_ID>000691472</NSF_ID>
<StartDate>06/10/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Lehigh University</Name>
<CityName>Bethlehem</CityName>
<ZipCode>180153005</ZipCode>
<PhoneNumber>6107583021</PhoneNumber>
<StreetAddress>Alumni Building 27</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>808264444</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LEHIGH UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068570936</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Lehigh University]]></Name>
<CityName>Bethlehem</CityName>
<StateCode>PA</StateCode>
<ZipCode>180151582</ZipCode>
<StreetAddress><![CDATA[200 West Packer ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7929</Code>
<Text>COMPUTATIONAL GEOMETRY</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~499143</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Machine learning, one of the main branches of artificial intelligence, is now ubiquitous throughout modern science, with numerous important consequences in society.&nbsp; This project has aimed to produce new optimization algorithms for solving problems arising in machine learning applications, specifically in the context of supervised learning when one aims to learn from copious amounts of available data.&nbsp; This goal has been accomplished with tremendous success.&nbsp; The investigators and their collaborators have published numerous articles on new optimization algorithms for machine learning in multiple top journal and conference proceedings venues.&nbsp; They have also produced open source software that has been made available to the public so that researchers and practitioners alike can experiment with and employ the newly proposed techniques.</p> <p>The investigators have trained two postdoctoral researchers in their lab over the course of the project duration, both of whom have acquired academic positions at top universities.&nbsp; They have also trained multiple graduate students who have entered (or will soon be entering) the workforce to be next-generation leaders in the intersection between applied mathematics and machine learning.</p> <p>The articles and software that have been produced from the project fall into a few categories.&nbsp; In one category, new algorithms have been proposed, analyzed, implemented, and tested that help contemporary techniques from producing poor solutions.&nbsp; In another, new techniques have been developed that make use of next-generation parallel and distributed computing platforms.&nbsp; The outcomes from this project are broadly applicable for users of machine learning methods.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/28/2020<br>      Modified by: Frank&nbsp;E&nbsp;Curtis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Machine learning, one of the main branches of artificial intelligence, is now ubiquitous throughout modern science, with numerous important consequences in society.  This project has aimed to produce new optimization algorithms for solving problems arising in machine learning applications, specifically in the context of supervised learning when one aims to learn from copious amounts of available data.  This goal has been accomplished with tremendous success.  The investigators and their collaborators have published numerous articles on new optimization algorithms for machine learning in multiple top journal and conference proceedings venues.  They have also produced open source software that has been made available to the public so that researchers and practitioners alike can experiment with and employ the newly proposed techniques.  The investigators have trained two postdoctoral researchers in their lab over the course of the project duration, both of whom have acquired academic positions at top universities.  They have also trained multiple graduate students who have entered (or will soon be entering) the workforce to be next-generation leaders in the intersection between applied mathematics and machine learning.  The articles and software that have been produced from the project fall into a few categories.  In one category, new algorithms have been proposed, analyzed, implemented, and tested that help contemporary techniques from producing poor solutions.  In another, new techniques have been developed that make use of next-generation parallel and distributed computing platforms.  The outcomes from this project are broadly applicable for users of machine learning methods.          Last Modified: 12/28/2020       Submitted by: Frank E Curtis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
