<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Development of an Observatory for Quantitative Analysis of Collective Behavior in Animals</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2016</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>339174.00</AwardTotalIntnAmount>
<AwardAmount>339174</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project, developing a new instrument to enable an accurate quantitative analysis of the movement of animals and vocal expressions in real world scenes, aims to facilitate innovative research in the study of animal behavior and neuroscience in complex realistic environments. While much progress has been made investigating brain mechanisms of behavior, these have been limited primarily to studying individual subjects in relatively simple settings. For many social species, including humans, understanding neurobiological processes within the confines of these more complex environments is critical because their brains have evolved to perceive and evaluate signals within a social context. Indeed, today's advances in video capture hardware and storage and in algorithms in computer vision and network science make this facilitation with animals possible. Past work has relied on subjective and time-consuming observations from video streams, which suffer from imprecision, low dimensionality, and the limitations of the expert analyst's sensory discriminability. This instrument will not only automate the process of detecting behaviors but also provide an exact numeric characterization in time and space for each individual in the social group. While not explicitly part of the instrument, the quantitative description provided by our system will allow the ability to correlate social context with neural measurements, a task that may only be accomplished when sufficient spatiotemporal precision has been achieved.&lt;br/&gt;&lt;br/&gt;The instrument enables research in the behavioral and neural sciences and development of novel algorithms in computer vision and network theory. In the behavioral sciences, the instrumentation allows the generation of network models of social behavior in small groups of animals or humans that can be used to ask questions that can range from how the dynamics of the networks influence sexual selection, reproductive success, and even health messaging to how vocal decision making in individuals gives rise to social dominance hierarchies. In the neural sciences, the precise spatio-temporal information the system would provide can be used to evaluate the neural bases of sensory processing and behavioral decision under precisely defined social contexts. Sensory responses to a given vocal stimulus, for example, can be evaluated by the context in which the animal heard the stimulus and both his and the sender's prior behavioral history in the group. In computer vision, we propose novel approaches for the calibration of multiple cameras "in the wild", the combination of appearance and geometry for the extraction of exact 3D pose and body parts from video, the learning of attentional focus among animals in a group, and the estimation of sound source and the classification of vocalizations. New approaches will be used on hierarchical discovery of behaviors in graphs, the incorporation of interactions beyond the pairwise level with simplicial complices, and a novel theory of graph dynamics for the temporal evolution of social behavior. The instrumentation benefits behavioral and neural scientists. Therefore, the code and algorithms developed will be open-source so that the scientific community can extend them based on the application. The proposed work also impacts computer vision and network science because the fundamental algorithms designed should advance the state of the art. For performance evaluation of other computer vision algorithms, established datasets will be employed.</AbstractNarration>
<MinAmdLetterDate>09/16/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/16/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1626008</AwardID>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>09/16/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jianbo</FirstName>
<LastName>Shi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jianbo Shi</PI_FULL_NAME>
<EmailAddress>jshi@cis.upenn.edu</EmailAddress>
<PI_PHON>2157462857</PI_PHON>
<NSF_ID>000193799</NSF_ID>
<StartDate>09/16/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Marc</FirstName>
<LastName>Schmidt</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marc F Schmidt</PI_FULL_NAME>
<EmailAddress>marcshm@sas.upenn.edu</EmailAddress>
<PI_PHON>2158987133</PI_PHON>
<NSF_ID>000218252</NSF_ID>
<StartDate>09/16/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel D Lee</PI_FULL_NAME>
<EmailAddress>ddl46@cornell.edu</EmailAddress>
<PI_PHON>6463709862</PI_PHON>
<NSF_ID>000322679</NSF_ID>
<StartDate>09/16/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Danielle</FirstName>
<LastName>Bassett</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Danielle S Bassett</PI_FULL_NAME>
<EmailAddress>dsb@seas.upenn.edu</EmailAddress>
<PI_PHON>2157461754</PI_PHON>
<NSF_ID>000624204</NSF_ID>
<StartDate>09/16/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191043409</ZipCode>
<StreetAddress><![CDATA[3330 Walnut STreet]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~339174</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-f57d4ffa-7fff-ffa0-f4b6-e5a1cf4a1a42"> </span></p> <p dir="ltr"><span>Neural and behavioral sciences are entering a phase where new advances in artificial intelligence now make it a possibility to quantify behaviors within complex naturalistic environments and relate them to underlying neural mechanisms. Because social animals, including humans, have evolved to perceive and evaluate signals within a social context, the ability to link neural function with the precise social context that surrounds the individual promises to be transformative.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>This MRI project produced the world?s first computational observatory for birds, a smart aviary.&nbsp;</span>Birds have widespread impacts on our everyday lives, ranging from agriculture to science and the arts. Although studying birds' motion is a critical step in many of these areas, current methods were not able to robustly estimate pose and shape of birds maneuvering in three dimensions and anytime during the day and any season.&nbsp;&nbsp;</p> <p>&nbsp;</p> <p dir="ltr"><span>To facilitate continuous monitoring of behavior&nbsp; the aviary was equipped with eight cameras and 24 microphones that can record life in the aviary 24/7. Inspired by the unparalleled performance of articulated mesh-based models for pose and shape estimation in humans, a multi-view dataset was collected that includes over 1000 images of 15 cowbirds housed together in an outdoor aviary. A novel articulated 3D bird model was introduced as well as a deep learning pipeline that estimates the parameters of the 3D bird model from segmented bird silhouettes and characteristic keypoints. The dataset can be used by biologists and neuroscientists in studying bird posture in social settings as well as the social graph of birds based on their position, orientation, and singing.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/13/2020<br>      Modified by: Kostas&nbsp;Daniilidis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1626008/1626008_10462921_1581655690922_birdMasks--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1626008/1626008_10462921_1581655690922_birdMasks--rgov-800width.jpg" title="Smart Aviary"><img src="/por/images/Reports/POR/2020/1626008/1626008_10462921_1581655690922_birdMasks--rgov-66x44.jpg" alt="Smart Aviary"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Images are gathered 24/7 from eight different viewpoints and under drastically changing illuminations (shown in three left rows). 3D models (right) are fitted to the detected silhouettes (left) and keypoints (middle).</div> <div class="imageCredit">Kostas Daniilidis and Marc Schmidt</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Kostas&nbsp;Daniilidis</div> <div class="imageTitle">Smart Aviary</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Neural and behavioral sciences are entering a phase where new advances in artificial intelligence now make it a possibility to quantify behaviors within complex naturalistic environments and relate them to underlying neural mechanisms. Because social animals, including humans, have evolved to perceive and evaluate signals within a social context, the ability to link neural function with the precise social context that surrounds the individual promises to be transformative.     This MRI project produced the world?s first computational observatory for birds, a smart aviary. Birds have widespread impacts on our everyday lives, ranging from agriculture to science and the arts. Although studying birds' motion is a critical step in many of these areas, current methods were not able to robustly estimate pose and shape of birds maneuvering in three dimensions and anytime during the day and any season.      To facilitate continuous monitoring of behavior  the aviary was equipped with eight cameras and 24 microphones that can record life in the aviary 24/7. Inspired by the unparalleled performance of articulated mesh-based models for pose and shape estimation in humans, a multi-view dataset was collected that includes over 1000 images of 15 cowbirds housed together in an outdoor aviary. A novel articulated 3D bird model was introduced as well as a deep learning pipeline that estimates the parameters of the 3D bird model from segmented bird silhouettes and characteristic keypoints. The dataset can be used by biologists and neuroscientists in studying bird posture in social settings as well as the social graph of birds based on their position, orientation, and singing.               Last Modified: 02/13/2020       Submitted by: Kostas Daniilidis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
