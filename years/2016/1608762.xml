<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDS&amp;E: Enabling Time-critical Decision-support for Disaster Response and Structural Engineering through Automated Visual Data Analytics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>299999.00</AwardTotalIntnAmount>
<AwardAmount>299999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Joanne Culbertson</SignBlockName>
<PO_EMAI>jculbert@nsf.gov</PO_EMAI>
<PO_PHON>7032924602</PO_PHON>
</ProgramOfficer>
<AbstractNarration>After a disaster, teams of trained engineers are charged with the task of collecting perishable data. These building reconnaissance teams collect data and information from the buildings that experienced the disaster, including photographs and measurements in the region. This information is collected to better understand the consequences of these events, and to improve the design of future structures. An enormous amount of images and videos is generated in just a few days, and to gather the most critical information in the time allowed, the engineers on these teams must quickly make daily decisions on where and what data to collect to achieve their mission. This research, which harnesses powerful computer vision methods to address real world civil engineering problems, aims to develop efficient methods to analyze and organize the collected images in the field, thereby enabling teams to collect the most useful data for building resilient communities worldwide. The project will leverage decades of experience in field missions from project researchers and domestic and international collaborators. A diverse set of students will be engaged in interdisciplinary research with international opportunities.&lt;br/&gt;&lt;br/&gt;The application of computer vision methods to address disaster response and structural engineering problems is not simple or straightforward. This project will systematically build the knowledge needed for their successful implementation in time-critical situations. Engineers with significant field-mission experience will annotate images. These records will provide the basis for determining the visual contents needed to make decisions in the field and how the contents are spatially interconnected in the images. This forms the foundation for determining the prior knowledge that can and must be included in the deep neural network structures to facilitate rapid decision-making in the field. To quantitatively evaluate the approach, a reconnaissance testbed will be established using a diverse set of images from past data collection missions. The computational time and accuracy will be measured and documented to establish a detailed profile of the classification results. This knowledge will enable the team collecting data during a reconnaissance mission to maximize the value of the data they collect by ensuring that they can successfully perform a given task, in a certain amount of time, applied to a suite of images. This capability will provide the evidence on which to base recommendations for further investigations and/or changes to design guidelines.</AbstractNarration>
<MinAmdLetterDate>07/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>12/21/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1608762</AwardID>
<Investigator>
<FirstName>Shirley</FirstName>
<LastName>Dyke</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shirley J Dyke</PI_FULL_NAME>
<EmailAddress>sdyke@purdue.edu</EmailAddress>
<PI_PHON>7655887877</PI_PHON>
<NSF_ID>000312078</NSF_ID>
<StartDate>07/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bedrich</FirstName>
<LastName>Benes</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bedrich Benes</PI_FULL_NAME>
<EmailAddress>bbenes@purdue.edu</EmailAddress>
<PI_PHON>7654944600</PI_PHON>
<NSF_ID>000291472</NSF_ID>
<StartDate>07/17/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072101</ZipCode>
<StreetAddress><![CDATA[1040 South River Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~299999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="BodyParCxSpFirst"><strong>Every natural disaster presents a unique opportunity to assess the performance of our infrastructure</strong>under circumstances that cannot entirely be reproduced in the laboratory or through numerical simulation. When performing reconnaissance in the field, engineers record much of the information being gathered in the form of images. This process is intended to record the visual appearance of structures and their components so that engineers can learn from disasters, before that perishable data is destroyed. However, when large quantities of images are collected within a short time period and have such a wide variety of content, timely organization and documentation is important. Clearly, manual sorting and analysis of all these images would be prohibitively tedious and expensive.&nbsp;</p> <p class="BodyParCxSpMiddle">To efficiently and thoroughly use these images, engineers need to&nbsp;<strong><em>exploit recent advances in computer vision and machine learning</em></strong>. In this project, we devised a series of automated capabilities, exploiting recent computer vision and machine learning algorithms, to classify, organize, and document the large volumes of visual data collected during a mission. We also established a database of reconnaissance images with photos gathered from many field engineers and researchers. We used this to&nbsp;<strong>generate a suite of novel capabilities that are transforming the current data collection and organization procedures</strong>, directly supporting field missions, including:&nbsp;</p> <p><strong>+ Scene classifiers&nbsp;</strong>and&nbsp;<strong>Object detection&nbsp;</strong>methods: Using our database of images, we established the ability to use deep learning methods to train robust classifiers to successfully classify scenes from such images while overcoming the potential for biases of these processes.&nbsp;<br /><strong><strong>+&nbsp;</strong>Automated Reconnaissance Image Organizer&nbsp;</strong>(ARIO): We integrated these new capabilities for automated image classification into this tool to directly support field engineers as they collect and organize post-earthquake images. It includes a suite of completely automated visual data classifiers and associated metadata based on the needs of researchers in organizing and finding visual data based on the visual contents, region and event. The tool can ingest several hundred images from a building, and automatically organize them into a report on that building.&nbsp;<br /><strong><strong>+&nbsp;</strong>Post-event rapid assessment for windstorms</strong>: We created an automated technique to support reconnaissance teams to rapidly perform detailed post-windstorm survey planning. A post-windstorm reconnaissance mission includes a preliminary survey, followed by a detailed survey. The objective of the preliminary survey is to develop an understanding of the overall situation in the field, e.g., buildings types and amount of damage, to plan the detailed survey. We first extract information about the target buildings from pre- and post-event images. We classify buildings based on their key physical/structural attributes and rapidly assess their post-event overall structural condition.&nbsp;<br /><strong><strong>+&nbsp;</strong>Google street-view extraction tool</strong>: We developed an automated technique to collect pre-event building images using 360 degree Google Street View photos. Images of severely damaged buildings often do not have sufficient spatial context and are not interpretable unless information regarding their pristine condition is provided. Thus, a set of pre-disaster images plays a crucial role in understanding what causes the building to be damaged or to survive an event. The technique automatically extracts high-quality external views of entire buildings from Google Street Views, which are publicly available online.<br /><strong><strong>+&nbsp;</strong>Structural drawing reconstruction tool</strong>: We developed a technique to reconstruct high-resolution full structural drawing images from raw reconnaissance images. Structural drawings are essential as a record of the structural information needed to understand the consequence of events and extract valuable lessons to improve future performance. In the field teams often collect drawings in the form of partial drawing images (PDI). As PDIs, these cannot be used easily and field engineers spend a lot of time trying to use them. Our technique will automatically assess the PDIs that capture a portion of the drawing, and stitch and blend them to&nbsp;<em>construct&nbsp;</em><em>high-quality full drawing images</em>.<br /><strong>+&nbsp;Associated schema and visual categories</strong> to enable efficient and systematic analysis of reconnaissance images.&nbsp;</p> <p><strong>Education and Training</strong>: All students/postdocs have gained experience with: high-quality data collection; novel computer vision methods; deep learning (machine learning) methods and avoiding biases; reconnaissance mission procedures and purposes; deployment of community tools and demonstrations online; and, technical writing and research processes.&nbsp;</p> <p><strong>Dissemination</strong>: In addition to the papers generated from this research, and the dataset published with ground truth labels, we deployed a demonstration of the ARIO tool.</p> <p><strong><span style="white-space: pre;">Published papers include: </span></strong></p> <p><span style="white-space: pre;">&nbsp;</span>+ Chul Min Yeum, Alana Lund, Shirley J. Dyke, Julio A. Ramirez, (2019). Automated Recovery of Structural Drawing Images Collected from Post-Disaster Reconnaissance,&nbsp;<em>J. of Computing in Civil Engineering</em>,&nbsp;<a href="https://doi.org/10.1061/(ASCE)CP.1943-5487.0000798">https://doi.org/10.1061/(ASCE)CP.1943-5487.0000798</a></p> <p><span style="white-space: pre;">+ </span>Chul Min Yeum, Shirley J. Dyke, Bedrich Benes, Thomas Hacker, Julio A. Ramirez, Alana Lund, Santiago Pujol (2019), Post-Event Reconnaissance Image Documentation using Automated Classification,&nbsp;<em>J. of Performance of Constructed Facilities,</em>&nbsp;<a href="https://doi.org/10.1061/(ASCE)CF.1943-5509.0001253">https://doi.org/10.1061/(ASCE)CF.1943-5509.0001253</a></p> <p><span style="white-space: pre;">+ </span>Chul Min Yeum, Shirley J. Dyke, Julio A. Ramirez, Visual Data Classification in Post-Event Building Reconnaissance,&nbsp;<em>Engineering Structures</em>,&nbsp;<a href="https://doi.org/10.1016/j.engstruct.2017.10.057">https://doi.org/10.1016/j.engstruct.2017.10.057</a></p> <p>+ Chul Min Yeum, et al., (2019-07-06). DesignSafe-CI.&nbsp;<a href="https://doi.org/10.17603/ds2-r0tv-sv29">https://doi.org/10.17603/ds2-r0tv-sv29</a>.</p><br> <p>            Last Modified: 07/06/2019<br>      Modified by: Shirley&nbsp;J&nbsp;Dyke</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1608762/1608762_10441322_1562439157465_ARIO_userinterface--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1608762/1608762_10441322_1562439157465_ARIO_userinterface--rgov-800width.jpg" title="UserInterface"><img src="/por/images/Reports/POR/2019/1608762/1608762_10441322_1562439157465_ARIO_userinterface--rgov-66x44.jpg" alt="UserInterface"></a> <div class="imageCaptionContainer"> <div class="imageCaption">User Interface for ARIO Tool</div> <div class="imageCredit">Mathieu Gaillard</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Shirley&nbsp;J&nbsp;Dyke</div> <div class="imageTitle">UserInterface</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1608762/1608762_10441322_1561507696704_Image1Outcomes--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1608762/1608762_10441322_1561507696704_Image1Outcomes--rgov-800width.jpg" title="Splash"><img src="/por/images/Reports/POR/2019/1608762/1608762_10441322_1561507696704_Image1Outcomes--rgov-66x44.jpg" alt="Splash"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Splash page for ARIO Tool</div> <div class="imageCredit">Mathieu Gaillard</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Shirley&nbsp;J&nbsp;Dyke</div> <div class="imageTitle">Splash</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Every natural disaster presents a unique opportunity to assess the performance of our infrastructureunder circumstances that cannot entirely be reproduced in the laboratory or through numerical simulation. When performing reconnaissance in the field, engineers record much of the information being gathered in the form of images. This process is intended to record the visual appearance of structures and their components so that engineers can learn from disasters, before that perishable data is destroyed. However, when large quantities of images are collected within a short time period and have such a wide variety of content, timely organization and documentation is important. Clearly, manual sorting and analysis of all these images would be prohibitively tedious and expensive.  To efficiently and thoroughly use these images, engineers need to exploit recent advances in computer vision and machine learning. In this project, we devised a series of automated capabilities, exploiting recent computer vision and machine learning algorithms, to classify, organize, and document the large volumes of visual data collected during a mission. We also established a database of reconnaissance images with photos gathered from many field engineers and researchers. We used this to generate a suite of novel capabilities that are transforming the current data collection and organization procedures, directly supporting field missions, including:   + Scene classifiers and Object detection methods: Using our database of images, we established the ability to use deep learning methods to train robust classifiers to successfully classify scenes from such images while overcoming the potential for biases of these processes.  + Automated Reconnaissance Image Organizer (ARIO): We integrated these new capabilities for automated image classification into this tool to directly support field engineers as they collect and organize post-earthquake images. It includes a suite of completely automated visual data classifiers and associated metadata based on the needs of researchers in organizing and finding visual data based on the visual contents, region and event. The tool can ingest several hundred images from a building, and automatically organize them into a report on that building.  + Post-event rapid assessment for windstorms: We created an automated technique to support reconnaissance teams to rapidly perform detailed post-windstorm survey planning. A post-windstorm reconnaissance mission includes a preliminary survey, followed by a detailed survey. The objective of the preliminary survey is to develop an understanding of the overall situation in the field, e.g., buildings types and amount of damage, to plan the detailed survey. We first extract information about the target buildings from pre- and post-event images. We classify buildings based on their key physical/structural attributes and rapidly assess their post-event overall structural condition.  + Google street-view extraction tool: We developed an automated technique to collect pre-event building images using 360 degree Google Street View photos. Images of severely damaged buildings often do not have sufficient spatial context and are not interpretable unless information regarding their pristine condition is provided. Thus, a set of pre-disaster images plays a crucial role in understanding what causes the building to be damaged or to survive an event. The technique automatically extracts high-quality external views of entire buildings from Google Street Views, which are publicly available online. + Structural drawing reconstruction tool: We developed a technique to reconstruct high-resolution full structural drawing images from raw reconnaissance images. Structural drawings are essential as a record of the structural information needed to understand the consequence of events and extract valuable lessons to improve future performance. In the field teams often collect drawings in the form of partial drawing images (PDI). As PDIs, these cannot be used easily and field engineers spend a lot of time trying to use them. Our technique will automatically assess the PDIs that capture a portion of the drawing, and stitch and blend them to construct high-quality full drawing images. + Associated schema and visual categories to enable efficient and systematic analysis of reconnaissance images.   Education and Training: All students/postdocs have gained experience with: high-quality data collection; novel computer vision methods; deep learning (machine learning) methods and avoiding biases; reconnaissance mission procedures and purposes; deployment of community tools and demonstrations online; and, technical writing and research processes.   Dissemination: In addition to the papers generated from this research, and the dataset published with ground truth labels, we deployed a demonstration of the ARIO tool.  Published papers include:    + Chul Min Yeum, Alana Lund, Shirley J. Dyke, Julio A. Ramirez, (2019). Automated Recovery of Structural Drawing Images Collected from Post-Disaster Reconnaissance, J. of Computing in Civil Engineering, https://doi.org/10.1061/(ASCE)CP.1943-5487.0000798  + Chul Min Yeum, Shirley J. Dyke, Bedrich Benes, Thomas Hacker, Julio A. Ramirez, Alana Lund, Santiago Pujol (2019), Post-Event Reconnaissance Image Documentation using Automated Classification, J. of Performance of Constructed Facilities, https://doi.org/10.1061/(ASCE)CF.1943-5509.0001253  + Chul Min Yeum, Shirley J. Dyke, Julio A. Ramirez, Visual Data Classification in Post-Event Building Reconnaissance, Engineering Structures, https://doi.org/10.1016/j.engstruct.2017.10.057  + Chul Min Yeum, et al., (2019-07-06). DesignSafe-CI. https://doi.org/10.17603/ds2-r0tv-sv29.       Last Modified: 07/06/2019       Submitted by: Shirley J Dyke]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
