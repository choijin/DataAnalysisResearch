<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EXP: Linking Eye Movements with Vvisual Attention to Enhance Cyberlearning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>549936.00</AwardTotalIntnAmount>
<AwardAmount>549936</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Soo-Siang Lim</SignBlockName>
<PO_EMAI>slim@nsf.gov</PO_EMAI>
<PO_PHON>7032927878</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by designing and building new kinds of learning technologies and studying their possibilities for fostering learning and challenges to using them effectively. This project will lay the groundwork necessary for incorporating eye movements into cyberlearning. Although hardware and software solutions are rapidly advancing the ability to detect and track cyberlearners' eye movements, the scientific understanding of the link between these eye movements and actual learning remains tentative. This issue is particularly important because research demonstrates surprising limits to the visual information that people take in: Even when it can be demonstrated that they have looked at something, this is no guarantee that learners gain knowledge of what they have seen. This project will address this problem in two ways. First, the researchers will develop a cognitive theory that can help specify how eye movements reveal what cyberlearners have absorbed when they view and interact with technology-based learning systems. Second, the researchers will develop a novel software application that helps cyberlearning content creators to incorporate assessment of eye movements into their practice. These projects will converge not only to develop cognitive theory that can help cyberlearners achieve more effective interactions, but also to enrich cognitive theory with input from real-world cyberlearning practitioners who struggle every day with the need to understand the sometimes confounding link between showing a learner something and learners' actual ability to understand and remember what they have seen. &lt;br/&gt;&lt;br/&gt;In particular, the investigators hypothesize that the link between fixation patterns and learning is mediated by visual modes that vary the relationship between concrete coding of visual properties and abstract focus on causal relationships and the goals of actions. The project will include experiments in which learners have their eyes tracked while they view a screen-captured information technology lesson. Some learners will be induced to deploy an "encoding" mode in which they focus on the specific sequence of steps needed to complete the task, while other learners will view the same materials using a "causal" mode in which they focus on the concepts underlying the lesson. Initial research has demonstrated significant differences in fixation patterns in these tasks (the strongest of these is that learners follow the instructor's mouse movements more closely in the encoding mode), and the current project will test whether these modes are associated with different patterns of visual and conceptual learning. The project will leverage these results by incorporating mode-revealing analytics into a novel software application that allows content creators to record screen-capture videos of their lessons while recording their own eye movements. In addition, a panel of viewers will be equipped with their own eye trackers and will view the content creators' lessons. Viewer eye movements will be returned to content creators who will be able view fixation patterns in the application, along with analytics based on findings from the visual mode experiments. The prototype system will be integrated with an existing learning technology, courseware for computer science education titled "Betty's Brain," and deployed in both formal and informal learning environments, including the Nashville Adventure Science Center.</AbstractNarration>
<MinAmdLetterDate>08/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/17/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1623625</AwardID>
<Investigator>
<FirstName>Gautam</FirstName>
<LastName>Biswas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gautam Biswas</PI_FULL_NAME>
<EmailAddress>gautam.biswas@vanderbilt.edu</EmailAddress>
<PI_PHON>6153436204</PI_PHON>
<NSF_ID>000110341</NSF_ID>
<StartDate>08/17/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Levin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Levin</PI_FULL_NAME>
<EmailAddress>daniel.t.levin@vanderbilt.edu</EmailAddress>
<PI_PHON>6153221518</PI_PHON>
<NSF_ID>000238319</NSF_ID>
<StartDate>08/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Adriane</FirstName>
<LastName>Seiffert</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adriane Seiffert</PI_FULL_NAME>
<EmailAddress>a.seiffert@vanderbilt.edu</EmailAddress>
<PI_PHON>6153222631</PI_PHON>
<NSF_ID>000508083</NSF_ID>
<StartDate>08/17/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Vanderbilt University</Name>
<CityName>Nashville</CityName>
<ZipCode>372350002</ZipCode>
<PhoneNumber>6153222631</PhoneNumber>
<StreetAddress>Sponsored Programs Administratio</StreetAddress>
<StreetAddress2><![CDATA[PMB 407749 2301 Vanderbilt Place]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>965717143</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VANDERBILT UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004413456</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Vanderbilt University]]></Name>
<CityName>Nashville</CityName>
<StateCode>TN</StateCode>
<ZipCode>372122809</ZipCode>
<StreetAddress><![CDATA[OSP; 1400 18th Ave. South]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8841</Code>
<Text>Exploration Projects</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~549936</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our project has explored how to eye movements to assess attention and learning from multimedia such as screen-captured instructional videos and interactive teachable agent cyberlearning systems. We have also asked more basic questions about how these media are currently being structured, how they guide visual attention, and how they might be improved. In several projects we implement eye tracking as an assessment of cognition during learning. In one of our initial projects, we systematically documented how current content creators use visual cues to guide attention during instructional videos, and we related these cues to viewership. In other work, we explored how eye movements might reveal the learning-relevant cognitions while viewing videos, an analysis that is often not successful. We were able to reveal these cognitive inputs to eye movements, but computing between-learner similarities in eye movements and using these to predict between-learner similarities in content question responses. We also demonstrated that some basic measures of eye movements such as mean fixation duration, and proportion of successful tracking time can predict learning. Findings such as these will help develop eye tracking as a technology that can be used to track student attention and learning in realistic settings that go beyond the lab. In particular, we tested whether eye tracking data could be successfully collected in a middle-school classroom and we successful in obtaining data, and relating it to on-screen visual attention. We also performed careful analyses of the causes of data loss in this setting and specified how student movements can lessen data quality.&nbsp;</p> <p>Other work has explored how adding instructor eye movements to instructional videos may impact viewer attention and learning. We found clear signs that viewers do follow instructor gaze, and we observed that this gaze following in some cases fails to improve learning. We also explored how other modifications to instructional videos can improve the efficiency of learning. We found that speeding videos by up to 200% can be very efficient because it has little impact on learning. However, this speeding comes at the cost of viewer disdain, so we created a new form of selective speeding that has less impact on speech rate. This form of speeding concentrates time-efficiencies on speech pauses and unimportant speech and it can provide comparable levels of time efficiency while finding more favor with viewers.&nbsp;</p> <p>Overall, our work can help improve new forms of media that have been the topic of relatively little research despite the fact that they are very common, and are increasingly important. Not only is it likely that the pandemic will increase the importance of mediated instruction, but once the pandemic is over, these forms of instruction may be increasingly important both in formal educational settings, but also in informal settings, and in work training settings where a flexible and adaptive workforce can help meet challenges and provide employment opportunities.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/27/2020<br>      Modified by: Daniel&nbsp;Levin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our project has explored how to eye movements to assess attention and learning from multimedia such as screen-captured instructional videos and interactive teachable agent cyberlearning systems. We have also asked more basic questions about how these media are currently being structured, how they guide visual attention, and how they might be improved. In several projects we implement eye tracking as an assessment of cognition during learning. In one of our initial projects, we systematically documented how current content creators use visual cues to guide attention during instructional videos, and we related these cues to viewership. In other work, we explored how eye movements might reveal the learning-relevant cognitions while viewing videos, an analysis that is often not successful. We were able to reveal these cognitive inputs to eye movements, but computing between-learner similarities in eye movements and using these to predict between-learner similarities in content question responses. We also demonstrated that some basic measures of eye movements such as mean fixation duration, and proportion of successful tracking time can predict learning. Findings such as these will help develop eye tracking as a technology that can be used to track student attention and learning in realistic settings that go beyond the lab. In particular, we tested whether eye tracking data could be successfully collected in a middle-school classroom and we successful in obtaining data, and relating it to on-screen visual attention. We also performed careful analyses of the causes of data loss in this setting and specified how student movements can lessen data quality.   Other work has explored how adding instructor eye movements to instructional videos may impact viewer attention and learning. We found clear signs that viewers do follow instructor gaze, and we observed that this gaze following in some cases fails to improve learning. We also explored how other modifications to instructional videos can improve the efficiency of learning. We found that speeding videos by up to 200% can be very efficient because it has little impact on learning. However, this speeding comes at the cost of viewer disdain, so we created a new form of selective speeding that has less impact on speech rate. This form of speeding concentrates time-efficiencies on speech pauses and unimportant speech and it can provide comparable levels of time efficiency while finding more favor with viewers.   Overall, our work can help improve new forms of media that have been the topic of relatively little research despite the fact that they are very common, and are increasingly important. Not only is it likely that the pandemic will increase the importance of mediated instruction, but once the pandemic is over, these forms of instruction may be increasingly important both in formal educational settings, but also in informal settings, and in work training settings where a flexible and adaptive workforce can help meet challenges and provide employment opportunities.           Last Modified: 11/27/2020       Submitted by: Daniel Levin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
