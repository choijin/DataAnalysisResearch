<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Exploring Motivated Misreporting of Crowd Workers for Developing Data Quality Assurance Best Practices for Using Crowdsourcing Platforms in Engineering Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>99936.00</AwardTotalIntnAmount>
<AwardAmount>99936</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robin Dillon-Merrill</SignBlockName>
<PO_EMAI>rdillonm@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Researchers increasingly use online platforms to tap into the knowledge of crowds. They divide complex scientific and engineering work into small, manageable tasks that crowd workers can complete. This method is quicker and cheaper than using trained scientific researchers, and has been used to disseminate emergency information, monitor hazardous events, and improve performance of information systems or other engineering practices. However, the data from such online platforms may be subject to various factors which can reduce overall quality. Moreover, the internal quality control mechanisms, if any, offered by common online crowdsourcing platforms may not let researchers assess data quality appropriately. This EArly-concept Grant for Exploratory Research (EAGER) project will draw on social science theory to investigate the tendency of crowd workers to cut corners to reduce their workload while still collecting incentives for the task completion. The immediate impact of this cross-disciplinary research is to inform quality assurance practices for managing crowdsourcing tasks of various types and at different scales and paces. The results will improve researchers' use of crowdsourcing to engage the general public in rigorous scientific and engineering research. The research outcome may further enhance crowdsourcing applications in emergency response, post-disaster damage assessment, and other social contexts where reliable data quality is required for successful engineering practices.&lt;br/&gt;&lt;br/&gt;Guided by the research on motivated misreporting in social science methodology, this study will conduct experiments to investigate whether online research participants tend to shirk their assigned tasks to reduce their workload while still collecting incentives for completing the task. The PIs will study three common crowdsourcing tasks: coding of satellite images after a natural disaster, responding to surveys, and classifying sentiments of online social media content. The experiment will recruit workers from popular crowdsourcing platforms, panelists from commercial online survey panels, and citizen scientists from online volunteering sites. The experiment will explore the following questions: (1) are crowd workers likely to engage in motivated misreporting when they complete tasks online If so, do the patterns of misreporting vary by the different incentive schemes provided by the platforms? And (2) do the patterns of misreporting vary by different task types The results will inform the development of best practices and quality assurance procedures for researchers interested in incorporating collective intelligence to improve the system for massive online information analysis in engineering, computer, and social sciences.</AbstractNarration>
<MinAmdLetterDate>09/09/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1645307</AwardID>
<Investigator>
<FirstName>Yuli</FirstName>
<LastName>Hsieh</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yuli P Hsieh</PI_FULL_NAME>
<EmailAddress>yph@rti.org</EmailAddress>
<PI_PHON>3127775234</PI_PHON>
<NSF_ID>000612779</NSF_ID>
<StartDate>09/09/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Research Triangle Institute</Name>
<CityName>Research Triangle Park</CityName>
<ZipCode>277092194</ZipCode>
<PhoneNumber>9195416000</PhoneNumber>
<StreetAddress>3040 Cornwallis Road</StreetAddress>
<StreetAddress2><![CDATA[P. O. Box 12194]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004868105</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH TRIANGLE INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004868105</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Research Triangle Institute]]></Name>
<CityName/>
<StateCode>NC</StateCode>
<ZipCode>277092194</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1642</Code>
<Text>Special Initiatives</Text>
</ProgramElement>
<ProgramReference>
<Code>043Z</Code>
<Text>PPSR- Public Participation in Scientific</Text>
</ProgramReference>
<ProgramReference>
<Code>1638</Code>
<Text>INFRAST MGMT &amp; EXTREME EVENTS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~99936</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>1. We find evidence that crowdworkers from Amazon Mechanical Turk tend to work harder than commercial online panelists to participate in scientific research and complete tasks. They tend to spend longer time with training materials and instructions and produced less missing data in&nbsp;tweet coding and damaged building image classification tasks.&nbsp;</span></p> <p><span>2. However, we also find that crowdworkers are not immune from undesirable respondent behaviors. They also are likely to be motivated to misreport their responses when they realize the structure of the tasks. In particular, crowdworkers also tend to take shortcuts to avoid getting additional questions in surveys.&nbsp;</span></p> <p><span>3. Interestingly, we find mixed evidence regarding the hypothesized effect of question format on motivated misreporting in the two scientific research and engineering tasks that we experimented. Our results suggest that the question format prone to misreporting in surveys (i.e. interleafed format) did not encourage such an undesirable behavior in the tweet coding task. At the same time, participants in the image classification tasks are less likely to cut corners while classifying images under the interleafed format.&nbsp;&nbsp;</span></p> <p><span>4.&nbsp;The necessary next step is to compare the tweet coding and classification results from our experiment against the correct outcome found in the literature that we replicated. Once the comparison data are tabulated, then we can further analyze whether question format may motivate participants to misreport and produce inaccurate classification when they should not. The analyze will also reveal if crowdworkers can reliably conduct complex scientific research tasks.&nbsp;</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2018<br>      Modified by: Yuli&nbsp;P&nbsp;Hsieh</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ 1. We find evidence that crowdworkers from Amazon Mechanical Turk tend to work harder than commercial online panelists to participate in scientific research and complete tasks. They tend to spend longer time with training materials and instructions and produced less missing data in tweet coding and damaged building image classification tasks.   2. However, we also find that crowdworkers are not immune from undesirable respondent behaviors. They also are likely to be motivated to misreport their responses when they realize the structure of the tasks. In particular, crowdworkers also tend to take shortcuts to avoid getting additional questions in surveys.   3. Interestingly, we find mixed evidence regarding the hypothesized effect of question format on motivated misreporting in the two scientific research and engineering tasks that we experimented. Our results suggest that the question format prone to misreporting in surveys (i.e. interleafed format) did not encourage such an undesirable behavior in the tweet coding task. At the same time, participants in the image classification tasks are less likely to cut corners while classifying images under the interleafed format.    4. The necessary next step is to compare the tweet coding and classification results from our experiment against the correct outcome found in the literature that we replicated. Once the comparison data are tabulated, then we can further analyze whether question format may motivate participants to misreport and produce inaccurate classification when they should not. The analyze will also reveal if crowdworkers can reliably conduct complex scientific research tasks.           Last Modified: 11/30/2018       Submitted by: Yuli P Hsieh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
