<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: F: Open-World Foundations for Big Uncertain Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>432202.00</AwardTotalIntnAmount>
<AwardAmount>432202</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Driven by the need to learn from vast amounts of text data, efforts throughout natural language processing, information extraction, databases, and AI are coming together to build large-scale knowledge bases. These systems continuously crawl the web to extract relational data from text, and have already populated their databases with millions of entities and billions of tuples. Large-scale probabilistic knowledge bases are revolutionizing the way we access data. They are now routinely used by scientists to build knowledge bases of publications, by law enforcement to extract information from the dark web, and by regular search engine users who find their results augmented with structured information. Such knowledge bases are inherently probabilistic: to go from raw text to structured data, a sequence of statistical machine learning techniques associate probabilities with database tuples. This project revisits the semantics underlying such systems, and provide a more adequate foundational framework. In particular, the closed-world assumption of probabilistic databases, that facts not in the database have probability zero, clearly conflicts with their everyday use, and obstructs the progress in this area.&lt;br/&gt;&lt;br/&gt;More specifically, this project develops a new semantic foundation based on the open-world assumption, that facts not in the database are possible, but have unknown probability. It designs the basic algorithms for query answering in this setting, both exact and approximate. Moreover, in a deep theoretical component, this project studies fundamental questions of data and domain complexity that are unique to open-world reasoning about big uncertain data. Finally it develops proof-of-concept applications in machine learning and data mining, and additional knowledge-representation layers that strengthen open-world reasoning. The developed semantics provide meaningful answers when some tuple probabilities are not precisely known. The developed algorithms allow for efficient query answering, even when reasoning about the open world, in time linear in the database size for tractable queries. This project provides a scientific leap at the fundamental, semantic level. It also provides a context for training undergraduate and graduate students in subjects spanning databases, artificial intelligence, theory, and machine learning, and will target the integration of probabilistic knowledge bases into computer science curricula.</AbstractNarration>
<MinAmdLetterDate>09/06/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1633857</AwardID>
<Investigator>
<FirstName>Guy</FirstName>
<LastName>Van den Broeck</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guy Van den Broeck</PI_FULL_NAME>
<EmailAddress>guyvdb@cs.ucla.edu</EmailAddress>
<PI_PHON>3107940102</PI_PHON>
<NSF_ID>000711612</NSF_ID>
<StartDate>09/06/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[UCLA - Computer Science]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951596</ZipCode>
<StreetAddress><![CDATA[420 Westwood Plaza, BH 4531E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~432202</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Large-scale knowledge bases, knowledge graphs, and databases already have a broad societal impact. The knowledge they provide assists scientists in various fields, and helps companies understand their data. However, for these systems to reach their full potential, it is important that they can capture and are able to quantify uncertainty about the data (which arises due to incomplete information, measurement error, speculative prediction, etc.). Such data is best represented in the form of a probabilistic knowledge base, where probabilities are associated with each piece of data. For these probabilistic knowledge bases to become a tool that everybody uses on a daily basis, we need to make a scientific leap at the fundamental, semantic level. When these knowledge bases democratize the access to statistical information, it is clear that this will have wide-ranging benefits.</p> <p>This project has produced several novel algorithms for querying probabilistic knowledge bases. First, on the theoretical level, we extended the probabilistic database formalism to support open-world reasoning. This means that we acknowledge that our data is incomplete, and that we may not have knowledge in our knowledge base pertaining to certain individuals or events, and that we must account for this absence of information when doing probabilistic reasoning about our data. Second, we developed query evaluation algorithms, where a user can ask questions of the probabilistic knowledge base in a formal language, and get probabilistic answers back. We focused on several such languages, including support for linear inequalities, objects and relations, and functions learned by machine learning algorithms. Third, we focused on several key practical considerations that required improvements: how to make such algorithms energy efficient, how to learn such probabilistic knowledge bases from data, etc.</p> <p>Development of the algorithms and systems described above have provided hands-on training for many of the PhD students involved, as well as several undergraduate and Master students who have built on this code for their individual research projects. Discoveries made in this project have been shared with a broad scientific community, including graduate students at UCLA, members of the database and AI communities, though various initiatives of the PI. The research results have been disseminated as full research papers at top-tier conferences, and as open-source software. It has also been achieved through the development of a new graduate course, the organization of high-profile workshops, and invited talks and tutorials, some of which specifically targeted underrepresented groups.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/18/2021<br>      Modified by: Guy&nbsp;Van Den Broeck</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Large-scale knowledge bases, knowledge graphs, and databases already have a broad societal impact. The knowledge they provide assists scientists in various fields, and helps companies understand their data. However, for these systems to reach their full potential, it is important that they can capture and are able to quantify uncertainty about the data (which arises due to incomplete information, measurement error, speculative prediction, etc.). Such data is best represented in the form of a probabilistic knowledge base, where probabilities are associated with each piece of data. For these probabilistic knowledge bases to become a tool that everybody uses on a daily basis, we need to make a scientific leap at the fundamental, semantic level. When these knowledge bases democratize the access to statistical information, it is clear that this will have wide-ranging benefits.  This project has produced several novel algorithms for querying probabilistic knowledge bases. First, on the theoretical level, we extended the probabilistic database formalism to support open-world reasoning. This means that we acknowledge that our data is incomplete, and that we may not have knowledge in our knowledge base pertaining to certain individuals or events, and that we must account for this absence of information when doing probabilistic reasoning about our data. Second, we developed query evaluation algorithms, where a user can ask questions of the probabilistic knowledge base in a formal language, and get probabilistic answers back. We focused on several such languages, including support for linear inequalities, objects and relations, and functions learned by machine learning algorithms. Third, we focused on several key practical considerations that required improvements: how to make such algorithms energy efficient, how to learn such probabilistic knowledge bases from data, etc.  Development of the algorithms and systems described above have provided hands-on training for many of the PhD students involved, as well as several undergraduate and Master students who have built on this code for their individual research projects. Discoveries made in this project have been shared with a broad scientific community, including graduate students at UCLA, members of the database and AI communities, though various initiatives of the PI. The research results have been disseminated as full research papers at top-tier conferences, and as open-source software. It has also been achieved through the development of a new graduate course, the organization of high-profile workshops, and invited talks and tutorials, some of which specifically targeted underrepresented groups.          Last Modified: 01/18/2021       Submitted by: Guy Van Den Broeck]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
