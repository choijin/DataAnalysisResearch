<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Texture2Text: Rich Language-Based Understanding of Textures for Recognition and Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops techniques at the interface of vision and natural language to understand and synthesize textures. For example, given a texture the project develops techniques that provide a description of the pattern (e.g., "the surface is slippery", "red polka-dots on a white background"). Techniques for semantic understanding of textures benefit a large number of applications ranging from robotics where understanding material properties of surfaces is key to interaction, to analysis of various forms of imagery for meteorology, oceanography, conservation, geology, and forestry. In addition, the project develops techniques that allow modification and synthesis of textures based on natural language descriptions (e.g., "make the wallpaper more zig-zagged", "create a honeycombed pattern"), enabling new human-centric tools for creating textures. In addition to the numerous applications enabled by this project, the broader impacts of the work include: the development of new benchmarks and software for computer vision and language communities, undergraduate research and outreach, and collaboration with researchers and citizen scientists in areas of conservation.&lt;br/&gt;&lt;br/&gt;This research maps visual textures to natural language descriptions and vice versa. The research advances computer vision by providing texture representations that are robust to realistic imaging conditions, clutter, and occlusions in natural scenes; content retrieval by providing new ways to search and retrieve textures using descriptions; and image manipulation by providing new ways to create and modify textures using descriptions. The main technical contributions of the project are: (1) principled architectures that combine aspects of texture models with deep learning to enable end-to-end learning of texture representations; (2) techniques for understanding the properties of these representations through visualizations; (3) a large-scale benchmark to evaluate techniques for language-based texture understanding; (4) new models for texture captioning; (5) applications of texture representations for fine-grained recognition and semantic segmentation; and (6) techniques for retrieving and creating textures using natural language descriptions.</AbstractNarration>
<MinAmdLetterDate>06/10/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/22/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617917</AwardID>
<Investigator>
<FirstName>Subhransu</FirstName>
<LastName>Maji</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Subhransu Maji</PI_FULL_NAME>
<EmailAddress>smaji@cs.umass.edu</EmailAddress>
<PI_PHON>4135772570</PI_PHON>
<NSF_ID>000699819</NSF_ID>
<StartDate>06/10/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Amherst</CityName>
<StateCode>MA</StateCode>
<ZipCode>010039242</ZipCode>
<StreetAddress><![CDATA[Research Admin Bldg., 70 Butterf]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>9199</Code>
<Text>Unallocated Program Costs</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project has contributed to techniques for representing texture in natural images and describing them using natural language. This allows us to 1) infer properties of objects in images such as their materials and identity; 2) retrieve texture from image collections using natural language for applications in graphics and commerce; and 3) explain the behavior of deep networks for image classification as texture cues are often used to learn the classification rule.</p> <div><span><span><span>&nbsp;</span></span></span></div> <div><span><span>On the technical side we have contributed novel architectures for detailed image understanding. These models unify decades of work on classical texture representations with recent advances in visual recognition based on deep learning. These models have been found to be more robust to changes in the distribution of images at testing time, and better transferable to fine-grained natural domains. For example, our early work on bilinear convolutional networks improved the accuracy of models at recognizing animal and plant species and reduced the reliance on part annotations which allowed them to be deployed at many more domains. Variants of these techniques have been used to analyze citizen-science data to inform projects in ecology and biology. </span></span></div> <div><span><span><span>&nbsp;</span></span></span></div> <div><span><span>We have also contributed datasets and benchmarks that allow systematic evaluation and advancement of methods for connecting language and vision for texture understanding. The source code,&nbsp;models for automatic analysis, and datasets have been publicly released to the scientific community, along with the supporting publications. The proposal supported the PI and who mentored several PhD students, MS students, and undergraduates on research projects.</span></span></div><br> <p>            Last Modified: 12/28/2020<br>      Modified by: Subhransu&nbsp;Maji</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1617917/1617917_10432616_1609162225448_teaser-bcnn--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1617917/1617917_10432616_1609162225448_teaser-bcnn--rgov-800width.jpg" title="Bilinear CNNs for texture recognition"><img src="/por/images/Reports/POR/2020/1617917/1617917_10432616_1609162225448_teaser-bcnn--rgov-66x44.jpg" alt="Bilinear CNNs for texture recognition"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Bilinear CNNs for recognizing bird species (left) and visualization of the texture context</div> <div class="imageCredit">Tsung-Yu Lin and Subhransu Maji</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Subhransu&nbsp;Maji</div> <div class="imageTitle">Bilinear CNNs for texture recognition</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project has contributed to techniques for representing texture in natural images and describing them using natural language. This allows us to 1) infer properties of objects in images such as their materials and identity; 2) retrieve texture from image collections using natural language for applications in graphics and commerce; and 3) explain the behavior of deep networks for image classification as texture cues are often used to learn the classification rule.   On the technical side we have contributed novel architectures for detailed image understanding. These models unify decades of work on classical texture representations with recent advances in visual recognition based on deep learning. These models have been found to be more robust to changes in the distribution of images at testing time, and better transferable to fine-grained natural domains. For example, our early work on bilinear convolutional networks improved the accuracy of models at recognizing animal and plant species and reduced the reliance on part annotations which allowed them to be deployed at many more domains. Variants of these techniques have been used to analyze citizen-science data to inform projects in ecology and biology.    We have also contributed datasets and benchmarks that allow systematic evaluation and advancement of methods for connecting language and vision for texture understanding. The source code, models for automatic analysis, and datasets have been publicly released to the scientific community, along with the supporting publications. The proposal supported the PI and who mentored several PhD students, MS students, and undergraduates on research projects.       Last Modified: 12/28/2020       Submitted by: Subhransu Maji]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
