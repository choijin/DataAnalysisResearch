<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Building Strong Geometric Priors for Total Scene Understanding</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>377407.00</AwardTotalIntnAmount>
<AwardAmount>377407</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is exploring how capabilities for geometric image understanding can change the way people approach the problem of automatically interpreting the semantic content of individual photos or videos. By developing algorithms for accurately localizing cameras from images that integrate other sources of geo-spatial data, such as 3D models of buildings and maps of urban areas, the project aims to significantly improve the ability of computer vision systems to understand image content. Utilizing strong prior information for scene understanding has a wide range of important practical applications.  An assistive robot providing elderly care in a home should leverage knowledge of the appearance and location of objects in its immediate environment while adapting to changes on multiple time scales (a coffee cup sitting on the table moves much more frequently than the table itself).  A network of self-driving cars could benefit significantly from dynamically updated urban maps built from the stream of data collected by the cars and other cameras (e.g., adapting behavior to a temporary lane closure that changes typical car and pedestrian traffic patterns).  The project involves students in research spanning a range of traditional disciplines and is engaging a wider audience across the UC Irvine campus in understanding and applying these technologies to novel social and scientific applications.&lt;br/&gt;&lt;br/&gt;This research investigates an alternate approach in which scene priors (including affordances and semantic attributes) are represented in 3D geo-spatial model coordinates rather than in 2D image space. Incorporating geometric context into scene understanding has largely been pursued under very weak prior assumptions on scene geometry and camera pose. Importantly, the research allows for direct integration of non-visual data such as GIS maps. The project is developing the appropriate algorithms and datasets to integrating such data along with a continual stream of images to produce a strong, temporally-evolving (4D) scene prior that can improve accuracy of camera pose estimation, monocular geometry, object detection and semantic segmentation.</AbstractNarration>
<MinAmdLetterDate>07/13/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618806</AwardID>
<Investigator>
<FirstName>Charless</FirstName>
<LastName>Fowlkes</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Charless Fowlkes</PI_FULL_NAME>
<EmailAddress>fowlkes@ics.uci.edu</EmailAddress>
<PI_PHON>9498246945</PI_PHON>
<NSF_ID>000505333</NSF_ID>
<StartDate>07/13/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926973425</ZipCode>
<StreetAddress><![CDATA[Dept. of Computer Science]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~377407</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-7d6f5b93-7fff-e6af-2ec6-5b45085c07a4"> </span></p> <p dir="ltr"><span>This project explored how machine capabilities for geometric image understanding can change the way we approach the problem of automatically interpreting the semantic content of individual photos or videos. By developing algorithms for accurately localizing cameras from images and integrating other sources of geo-spatial data, such as 3D models of buildings and maps of urban areas, the project aims to significantly improve the ability of computer vision systems to understand image content. Utilizing strong prior information for scene understanding has a wide range of important practical applications. An assistive robot providing elderly care in a home should leverage knowledge of the appearance and location of objects in its immediate environment while adapting to changes on multiple time scales (a coffee cup sitting on the table moves much more frequently than the table itself). A network of self-driving cars could benefit significantly from dynamically updated urban maps built from the stream of data collected by the cars and other cameras (e.g., adapting behavior to a temporary lane closure that changes typical car and pedestrian traffic patterns).</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Over the course of this project we explored a number of alternate approaches to image understanding in which scene priors (including affordances and semantic attributes) are represented in 3D geo-spatial model coordinates rather than in 2D image space.&nbsp;</span>To highlight a few key technical results:</p> <p>&nbsp;</p> <p dir="ltr"><span>(1) We developed a computational approach to fusing together video data of indoor environments collected over long periods of time in order to create a 4D spatio-temporal maps. If an environment is perfectly static, then this involves aligning scans of the 3D shape of different parts of a scene and stitching them into a single coherent map. However, since some parts of the environment change from one day to the next (e.g., furniture is moved) there is an additional challenge of detecting these changes and estimating when in time they occurred. Our approach explicitly represents these changes in time as well as taking into account what parts of the scene were visible in a probabilistic framework that integrates these uncertain measurements into a final space-time map.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>(2) We developed learning-based techniques for estimating the complete geometry of a novel scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces, something which humans manage quite easily. A key aspect of our approach was to represent the geometric layout of the scene by tracing rays outward from each camera pixel and recording when they enter and exit surfaces in the scene. This provides a high-resolution representation of the scene geometry that maintains a direct mapping between the scene shape and where it projects in the image. This novel representation allowed us to train much more accurate models for predicting scene layout.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>(3) It is often the case that we have substantial prior knowledge about a scene from where an image was collected (e.g., a 3D map or model of a building). However, most existing computer vision techniques offer no way to readily incorporate such information.&nbsp; We developed several approaches for training learning-based models that make explicit use of such information in order to make more accurate estimates. For example, when estimating the 3D pose of a person from an image, we incorporate information about the height of the floor they are standing on in order to produce more accurate predictions. We evaluated these techniques in a variety of scenarios including an extensive dataset we collected using a commercial motion-capture system to record ground-truth poses of humans interacting with a rich environment.</span></p> <p dir="ltr"><span><br /></span></p> <p dir="ltr"><span>The results of this research were disseminated in more than 12 open-access papers published in top conferences in computer vision. In addition, the project supported in part the research and mentoring of 5 graduate students, 3 who have completed PhD theses and are now in postdoctoral and industry research positions. Finally, the award enriched the development of several undergraduate computer vision courses and independent projects taught by the PI at UC Irvine and helped engage 5+ undergraduate students in independent research projects.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/03/2020<br>      Modified by: Charless&nbsp;Fowlkes</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   This project explored how machine capabilities for geometric image understanding can change the way we approach the problem of automatically interpreting the semantic content of individual photos or videos. By developing algorithms for accurately localizing cameras from images and integrating other sources of geo-spatial data, such as 3D models of buildings and maps of urban areas, the project aims to significantly improve the ability of computer vision systems to understand image content. Utilizing strong prior information for scene understanding has a wide range of important practical applications. An assistive robot providing elderly care in a home should leverage knowledge of the appearance and location of objects in its immediate environment while adapting to changes on multiple time scales (a coffee cup sitting on the table moves much more frequently than the table itself). A network of self-driving cars could benefit significantly from dynamically updated urban maps built from the stream of data collected by the cars and other cameras (e.g., adapting behavior to a temporary lane closure that changes typical car and pedestrian traffic patterns).    Over the course of this project we explored a number of alternate approaches to image understanding in which scene priors (including affordances and semantic attributes) are represented in 3D geo-spatial model coordinates rather than in 2D image space. To highlight a few key technical results:    (1) We developed a computational approach to fusing together video data of indoor environments collected over long periods of time in order to create a 4D spatio-temporal maps. If an environment is perfectly static, then this involves aligning scans of the 3D shape of different parts of a scene and stitching them into a single coherent map. However, since some parts of the environment change from one day to the next (e.g., furniture is moved) there is an additional challenge of detecting these changes and estimating when in time they occurred. Our approach explicitly represents these changes in time as well as taking into account what parts of the scene were visible in a probabilistic framework that integrates these uncertain measurements into a final space-time map.    (2) We developed learning-based techniques for estimating the complete geometry of a novel scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces, something which humans manage quite easily. A key aspect of our approach was to represent the geometric layout of the scene by tracing rays outward from each camera pixel and recording when they enter and exit surfaces in the scene. This provides a high-resolution representation of the scene geometry that maintains a direct mapping between the scene shape and where it projects in the image. This novel representation allowed us to train much more accurate models for predicting scene layout.    (3) It is often the case that we have substantial prior knowledge about a scene from where an image was collected (e.g., a 3D map or model of a building). However, most existing computer vision techniques offer no way to readily incorporate such information.  We developed several approaches for training learning-based models that make explicit use of such information in order to make more accurate estimates. For example, when estimating the 3D pose of a person from an image, we incorporate information about the height of the floor they are standing on in order to produce more accurate predictions. We evaluated these techniques in a variety of scenarios including an extensive dataset we collected using a commercial motion-capture system to record ground-truth poses of humans interacting with a rich environment.   The results of this research were disseminated in more than 12 open-access papers published in top conferences in computer vision. In addition, the project supported in part the research and mentoring of 5 graduate students, 3 who have completed PhD theses and are now in postdoctoral and industry research positions. Finally, the award enriched the development of several undergraduate computer vision courses and independent projects taught by the PI at UC Irvine and helped engage 5+ undergraduate students in independent research projects.               Last Modified: 11/03/2020       Submitted by: Charless Fowlkes]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
