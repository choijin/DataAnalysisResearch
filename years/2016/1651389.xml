<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Construction of Social Interactions in 3D Space from First-Person Videos</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Precision modeling tools for realistic and complex human social interaction are not available today. First-person videos provide a unique opportunity to capture social interaction at unprecedented precision. In contrast, current third person surveillance video only records the few distance views of the interaction passively at a much reduced spatial resolution. This exploratory research project proposes to harness multiple first-person cameras as one collective instrument to capture, model, and predict social behaviors. The proposed research transforms the way we construct realistic social interaction models, while also advancing first-person video recognition. If successful, the envisioned computational model can act as a coach who learns what constitutes successful interactions and failures, thus being able to find solutions to mediate and prevent potential conflicts. &lt;br/&gt;&lt;br/&gt;The proposed research will model dynamic social interactions in 3D space from multiple personal perspectives. Recognition and prediction of complex social group interactions are challenging because people in the group can carry out unexpected actions intentionally or by mistake. In addition, due to variances in individuals' preferences and abilities, the same activities could be carried out in different ways. First-person videos can be highly jittery, resulting in fast and unpredictable object motions in the field of view. Building on PI's recent work establishing computational foundations for modeling social (people-people) and personal (people-scene) interactions using first-person cameras, this research will explore the novel concept the duality between social attention and roles: social attention provides a cue for recognizing social roles, and social roles facilitate the predictions of dynamic social formation change and its associated social attention. The formal foundation of the 3D model is based on constructing a visual memory that stores first-person social experiences in three forms: (a) geometric social formation, (b) visual image of first-person view, and (c) first-person seen by nearby third person views. As a proof-of-concept, the 3D space model capturing social interactions will be tested on collaborative social tasks such as assembling (Ikea) furniture, or building a block house with a group of friends. This research will construct a labeled dataset capturing the interactions, and perform analysis on both accuracy in recognizing social roles and precision in predicting spatial movements of the members in that social interaction. The results of this project, including papers and dataset, will be disseminated to the public through our project website (http://www.cis.upenn.edu/~jshi/NSF_SocialMemory/nsf_social_visual_memory.html). The software created under this project will be made available to the public through GitHub, a web-based Git repository hosting service</AbstractNarration>
<MinAmdLetterDate>08/24/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/24/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1651389</AwardID>
<Investigator>
<FirstName>Jianbo</FirstName>
<LastName>Shi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jianbo Shi</PI_FULL_NAME>
<EmailAddress>jshi@cis.upenn.edu</EmailAddress>
<PI_PHON>2157462857</PI_PHON>
<NSF_ID>000193799</NSF_ID>
<StartDate>08/24/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress><![CDATA[3451 Walnut Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;Construction of Social Interactions in 3D Space from First-Person Videos<br /> <br /> We study how people signal intention to interact with each other using first-person video.&nbsp;&nbsp;In the past few year, there have been major progress in recognizing human action and gesture. However, the overall body language for communicating intention can be very subtle. For example in adversarial game players are both trying to signal their intention and trying to conceal their intentions from the opposite team. Furthermore, body language for communication can be complex. Even for human observers recognizing human intention requires paying attention to details of facial expression, head posture, and overall body pose, and our performance is far from perfect. This makes generating human labelling for training a machine learning algorithm very costly and unreliable.<br /> <br /> We generated a dataset contains 48 distinct college-level players in unscripted basketball games with 10.3 hours of videos. We measured the human performance on this dataset, and measured human performance is between 80% to 93% accuracy across 5 subjects.&nbsp;&nbsp;We develop an EgoSupervision learning scheme, demonstrate that despite not using human-generated supervised labels our method achieves (77.% accuracy) similar or even better results than fully supervised method (77.8% accuracy).<br /> <br /> Looking further into how team player collaborate with each other, we study how generate plans in team sports by learning from first-person video. Our hypothesis is that knowing the external environment of a player (including the position and speed of players) is not sufficient for predicting his/her actions. Knowing what the player is looking at, and what (s)he sees provides strong cues of action prediction.&nbsp;&nbsp;We developed a method to predict the future movements (location and gaze direction) of basketball players as a whole from their first-person videos.&nbsp;&nbsp;We compare our predicted joint attention with 7 baseline algorithms which it shows 2 m error after 2 seconds vs. 15 m error after 2 seconds for algorithm without using first person video (position and orientation information only).<br /> <br /> Finally, we study how to learn to form complex and diverse stratege in an adverserial one-on-one basketball game using first-person video.&nbsp;&nbsp;&nbsp;We developed a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player?s 3D location and 3D head orientation throughout the sequence.&nbsp;&nbsp;We constructed a first-person basketball dataset consisting of 988 sequences from one-on-one basketball games between nine college-level players. To the best of our knowledge, we are the first to generate egocentric basketball motion sequences in the 12D camera configuration space from a single first-person image.</p><br> <p>            Last Modified: 08/13/2019<br>      Modified by: Jianbo&nbsp;Shi</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1651389/1651389_10454594_1565699448349_basketball-prediction-comparison--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1651389/1651389_10454594_1565699448349_basketball-prediction-comparison--rgov-800width.jpg" title="Social action prediction"><img src="/por/images/Reports/POR/2019/1651389/1651389_10454594_1565699448349_basketball-prediction-comparison--rgov-66x44.jpg" alt="Social action prediction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Visual semantics from first person view lead to better future prediction: left, prediction with location and orientation prior; right, prediction with first person view to retrieve trajectories, which shows strong selectivity. GT: ground truth trajectory, Ret: retrieved trajectory.</div> <div class="imageCredit">Shan Su, Jung Pyo Hong, Jianbo Shi, and Hyun Soo Park "Predicting  Behaviors  of  Basketball Players from First Person Videos" Conference on Computer Vision and Pattern Recognition (CVPR), 2017</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jianbo&nbsp;Shi</div> <div class="imageTitle">Social action prediction</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Construction of Social Interactions in 3D Space from First-Person Videos    We study how people signal intention to interact with each other using first-person video.  In the past few year, there have been major progress in recognizing human action and gesture. However, the overall body language for communicating intention can be very subtle. For example in adversarial game players are both trying to signal their intention and trying to conceal their intentions from the opposite team. Furthermore, body language for communication can be complex. Even for human observers recognizing human intention requires paying attention to details of facial expression, head posture, and overall body pose, and our performance is far from perfect. This makes generating human labelling for training a machine learning algorithm very costly and unreliable.    We generated a dataset contains 48 distinct college-level players in unscripted basketball games with 10.3 hours of videos. We measured the human performance on this dataset, and measured human performance is between 80% to 93% accuracy across 5 subjects.  We develop an EgoSupervision learning scheme, demonstrate that despite not using human-generated supervised labels our method achieves (77.% accuracy) similar or even better results than fully supervised method (77.8% accuracy).    Looking further into how team player collaborate with each other, we study how generate plans in team sports by learning from first-person video. Our hypothesis is that knowing the external environment of a player (including the position and speed of players) is not sufficient for predicting his/her actions. Knowing what the player is looking at, and what (s)he sees provides strong cues of action prediction.  We developed a method to predict the future movements (location and gaze direction) of basketball players as a whole from their first-person videos.  We compare our predicted joint attention with 7 baseline algorithms which it shows 2 m error after 2 seconds vs. 15 m error after 2 seconds for algorithm without using first person video (position and orientation information only).    Finally, we study how to learn to form complex and diverse stratege in an adverserial one-on-one basketball game using first-person video.   We developed a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player?s 3D location and 3D head orientation throughout the sequence.  We constructed a first-person basketball dataset consisting of 988 sequences from one-on-one basketball games between nine college-level players. To the best of our knowledge, we are the first to generate egocentric basketball motion sequences in the 12D camera configuration space from a single first-person image.       Last Modified: 08/13/2019       Submitted by: Jianbo Shi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
