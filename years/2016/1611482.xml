<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Creating a new assessment tool for quantitative critical thinking in introductory lab courses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>299176.00</AwardTotalIntnAmount>
<AwardAmount>299176</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>R. Corby Hovis</SignBlockName>
<PO_EMAI>chovis@nsf.gov</PO_EMAI>
<PO_PHON>7032924625</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A great deal of time and money is spent on science lab classes, but there is little evidence they are providing good educational value, and some indication that they are not. Currently, there are no accepted ways to measure whether or not students are learning the desired skills from such classes. This significant project will create a new way to measure students' learning of a particularly important set of skills that such labs can teach: how students reason with experimental data and test the validity of scientific models. These skills are an essential part of all science and are also important in public policy decisions and throughout the technical workforce. This test will be easy to administer to students and will guide instructional improvement. It will provide an essential first step for enabling the widespread improvement of the educational effectiveness of lab courses.  &lt;br/&gt;&lt;br/&gt;Investigators on this project will develop and validate an easy-to-use assessment of quantitative critical thinking skills in the context of introductory physics lab courses. The concise and validated assessment tool will be critical to building evidence about the effectiveness of laboratory instruction and to generate new knowledge about student competencies and difficulties in critical thinking and scientific decision making. The specific set of skills to be assessed include interpreting and drawing conclusions from data with uncertainties, comparing and evaluating models and data, and making decisions about improving the experimental design or data acquisition. These skills were identified as key goals for an undergraduate science curriculum and for which labs have unique affordances for teaching. Development of the instrument will involve cycles of revisions through: evaluating think-aloud interviews with students and experts; evaluating written responses from a wide range of students enrolled in physics lab courses; and through statistical tests of reliability and validity.</AbstractNarration>
<MinAmdLetterDate>06/17/2016</MinAmdLetterDate>
<MaxAmdLetterDate>11/30/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1611482</AwardID>
<Investigator>
<FirstName>Carl</FirstName>
<LastName>Wieman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carl Wieman</PI_FULL_NAME>
<EmailAddress>cwieman@stanford.edu</EmailAddress>
<PI_PHON>6507232300</PI_PHON>
<NSF_ID>000703119</NSF_ID>
<StartDate>06/17/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>943057464</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT/>
<CONGRESS_DISTRICT_PERF>CA"</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1998</Code>
<Text>IUSE</Text>
</ProgramElement>
<ProgramReference>
<Code>8209</Code>
<Text>Improv Undergrad STEM Ed(IUSE)</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0416</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~299176</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="paragraph">Increasing focus is being placed on developing critical thinking and other scientific skills in science curriculum; skills that can be uniquely taught through laboratory instruction. This project developed a widely usable assessment instrument to measure student critical thinking in the context of undergraduate physics labs. The instrument, called the Physics Lab Inventory of Critical thinking or PLIC, was used with over 12,000 students at over 40 different institutions. The instrument has provided a rich evidence-base, which will provide new knowledge about student competencies in critical thinking and scientific decision making and provide data and tools for how to best teach them.</p> <p class="paragraph">The PLIC is now freely available to instructors through the website PhysPort.org. When using the PLIC, instructors receive a report summarizing their students' responses, as well as a link to an interactive dashboard where they can download their students' scores, compare their students' scores by demographic variables (such as course level, gender, etc.), and compare their students' scores with those from comparable institutions.&nbsp;</p> <p>The project found interesting results about how to measure critical thinking and about how critical thinking skills are developed. For example, the project compared how students respond to an open-response version of the assessment with the closed-response version to determine which was a better measure of students' critical thinking skills. Results showed that students propose fewer critical evaluations of experimental case studies in the open-response version than when they can select from a list of options in the closed-response version and interviews with students indicated that the list of options cause students to critically evaluate each available choice. Thus, the closed-response version encourages students to perform a more critical analysis, thus providing better evidence of their critical thinking abilities.</p> <p>The analysis of the PLIC data across instructional styles and demographic characteristics also revealed several significant results. For example, students enrolled in labs designed to develop experimentation skills developed stronger critical thinking skills than when enrolled in labs designed to demonstrate physical concepts. Importantly, this pattern was consistent across demographic groups, meaning students of all genders and races/ethnicities score higher on the PLIC when enrolled in skills-based labs than in concepts-based labs. These skills-based labs typically included instructional activities that supported students in making decisions about the experiment (such as designing methods or posing questions to investigate) or that focused on developing students' communication skills, which explained most of the effect of the skills-based labs on students' critical thinking. These results provide important implications for instruction, such as that offering opportunities for students to make decisions or communicate their work may develop their critical thinking skills. &nbsp;</p> <p>The project also performed a larger analysis of the instructional activities in the more than 150 courses using the PLIC. The results showed that lab courses remain incredibly variable in their logistical structures (such as the number of students per section or amount of time students spend in lab) and their pedagogical strategies and goals. These analyses will be useful for instructors to situate their own courses in the national landscape and for researchers to evaluate changes in lab instruction in the future.</p> <p>Finally, the project also led to the development of an analogous instrument for ecology and evolutionary biology, the Biology Lab Inventory of Critical thinking for Ecology, or Eco-BLIC. Together, the two assessments will be used to compare students' critical thinking between and across disciplines and to evaluate how instruction in different disciplines impacts students' critical thinking skills.</p> <p class="paragraph">This project has made a large contribution to improving education in the critical area of quantitative critical thinking. It created and widely disseminated a tool for evaluating and improving education in this area, which plays a vital role in both technical work force performance and good public policy decision-making.</p><br> <p>            Last Modified: 07/20/2021<br>      Modified by: Carl&nbsp;Wieman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Increasing focus is being placed on developing critical thinking and other scientific skills in science curriculum; skills that can be uniquely taught through laboratory instruction. This project developed a widely usable assessment instrument to measure student critical thinking in the context of undergraduate physics labs. The instrument, called the Physics Lab Inventory of Critical thinking or PLIC, was used with over 12,000 students at over 40 different institutions. The instrument has provided a rich evidence-base, which will provide new knowledge about student competencies in critical thinking and scientific decision making and provide data and tools for how to best teach them. The PLIC is now freely available to instructors through the website PhysPort.org. When using the PLIC, instructors receive a report summarizing their students' responses, as well as a link to an interactive dashboard where they can download their students' scores, compare their students' scores by demographic variables (such as course level, gender, etc.), and compare their students' scores with those from comparable institutions.   The project found interesting results about how to measure critical thinking and about how critical thinking skills are developed. For example, the project compared how students respond to an open-response version of the assessment with the closed-response version to determine which was a better measure of students' critical thinking skills. Results showed that students propose fewer critical evaluations of experimental case studies in the open-response version than when they can select from a list of options in the closed-response version and interviews with students indicated that the list of options cause students to critically evaluate each available choice. Thus, the closed-response version encourages students to perform a more critical analysis, thus providing better evidence of their critical thinking abilities.  The analysis of the PLIC data across instructional styles and demographic characteristics also revealed several significant results. For example, students enrolled in labs designed to develop experimentation skills developed stronger critical thinking skills than when enrolled in labs designed to demonstrate physical concepts. Importantly, this pattern was consistent across demographic groups, meaning students of all genders and races/ethnicities score higher on the PLIC when enrolled in skills-based labs than in concepts-based labs. These skills-based labs typically included instructional activities that supported students in making decisions about the experiment (such as designing methods or posing questions to investigate) or that focused on developing students' communication skills, which explained most of the effect of the skills-based labs on students' critical thinking. These results provide important implications for instruction, such as that offering opportunities for students to make decisions or communicate their work may develop their critical thinking skills.    The project also performed a larger analysis of the instructional activities in the more than 150 courses using the PLIC. The results showed that lab courses remain incredibly variable in their logistical structures (such as the number of students per section or amount of time students spend in lab) and their pedagogical strategies and goals. These analyses will be useful for instructors to situate their own courses in the national landscape and for researchers to evaluate changes in lab instruction in the future.  Finally, the project also led to the development of an analogous instrument for ecology and evolutionary biology, the Biology Lab Inventory of Critical thinking for Ecology, or Eco-BLIC. Together, the two assessments will be used to compare students' critical thinking between and across disciplines and to evaluate how instruction in different disciplines impacts students' critical thinking skills. This project has made a large contribution to improving education in the critical area of quantitative critical thinking. It created and widely disseminated a tool for evaluating and improving education in this area, which plays a vital role in both technical work force performance and good public policy decision-making.       Last Modified: 07/20/2021       Submitted by: Carl Wieman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
