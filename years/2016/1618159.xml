<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Matching and Ranking via Proximity Graphs: Applications to Question Answering and Beyond</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>498491.00</AwardTotalIntnAmount>
<AwardAmount>498491</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will explore novel alternatives to a classic term-based full-text search, which is one of the most widely used computer algorithms. The current full-text search approaches heavily rely on memorizing which words and phrases appear in which text documents. The proposed research, in contrast, will examine methods that deviate from this well-studied path by using more generic similarity search methods. In doing so, the proposed research will pursue the following two objectives: (1) mitigating limitations of the existing approaches such as the mismatch between words that appear in queries and documents; and (2) developing approaches that permit an efficient separation of labor between data scientists and designers of retrieval algorithms. The latter would allow data scientists to focus on development of effective similarity models without worrying too much about low-level performance issues, while designers of retrieval algorithms and software engineers will be able to focus on development of more efficient and/or scalable approaches having fewer concerns about quality of results.&lt;br/&gt;&lt;br/&gt;The proposed research will investigate at least two scenarios where a term-based full-text search is replaced with a more generic high-accuracy k-nearest (k-NN) neighbor search. In the first scenario, it will develop a similarity function that goes beyond pure lexical matching and takes into account distributional similarity, similarity learned from a parallel (monolingual) corpus, and so on. In this scenario, the similarity function will be used as a black-box function coupled with a generic similarity search engine, implemented as a part of the Non-Metric Space Library (NMSLIB). Several search algorithms will be explored. One of the search approaches will rely on building a proximity graph (also known as a neighborhood graph), where nodes are objects and similar nodes are connected by edges. In the second scenario, the proposed research will build a pseudo inverted file over super terms. Super terms are (dense or sparse) vectorial representations of words appearing within a sliding window of small size. The super terms form a pseudo-vocabulary that can be indexed using a proximity graph (or any other efficient k-NN search method). At query time, the super terms will be extracted from the query and matched against the pseudo-vocabulary to obtain k nearest super terms (as well as documents where they occur). This approach will incorporate term proximity and term similarity (the latter will make the approach less affected by the vocabulary mismatch). Because preliminary experiments demonstrated that proximity graphs are not sufficiently accurate and efficient for the task in hand, the proposed research will also attempt to develop better variants of the proximity graphs methods. Should such an improvement fail, alternative search methods will also be explored. Experimental insights, algorithmic improvements, and new challenging datasets (resulting from the proposed work) will advance the state of the art in k-NN search, which is another widely used method. This, in turn, will benefit a variety of other NLP tasks such as classification, dictionary-based entity detection, and first story detection, which all heavily relying on the k-NN search. Additional project information will be made available at the project website: http://www.lti.cs.cmu.edu/PGraph</AbstractNarration>
<MinAmdLetterDate>09/06/2016</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618159</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Nyberg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric Nyberg</PI_FULL_NAME>
<EmailAddress>ehn@cs.cmu.edu</EmailAddress>
<PI_PHON>4122687281</PI_PHON>
<NSF_ID>000095073</NSF_ID>
<StartDate>09/06/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~498491</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-cac8ccca-7fff-dfc4-27fe-fa26fb08190c"> <p dir="ltr"><span>The goal of the project is to develop next-generation retrieval methods and tools for question-answering and information retrieval, which apply k-NN search to hybrid document representations. These devices can replace or complement existing full-text retrieval techniques based on inverted files. We implement a proof-of-concept system that demonstrates the feasibility of this approach, which is based on the classic IBM Model-1 translation approach. We observe that the loss of accuracy due to the approximate nature of k-NN search is potentially a limiting factor in applying such techniques. Furthermore, we experimented with several neural representations, but have not yet obtained conclusive evidence with respect to their effectiveness. Despite demonstrating practicality of k-NN search in a text retrieval domain, we note that it is not yet clear if k-NN search can find many substantially different results compared to a classic term-based retrieval pipeline (with re-ranking).&nbsp;</span></p> <br /> <p dir="ltr"><span>A broader impact of our work consists in helping develop and evaluate efficient methods for k-NN search, which can work for a variety of hard dissimilarities and distances including the Euclidean distance, the inner product, and even substantially non-symmetric dissimilarities such as Renyi- or KL-divergence.&nbsp;</span></p> <br /><span>The grant helped maintain and develop two open-source libraries: NMSLIB (</span><a href="https://github.com/nmslib/nmslib"><span>https://github.com/nmslib/nmslib</span></a><span>) and knn4qa (</span><a href="https://github.com/oaqa/knn4qa"><span>https://github.com/oaqa/knn4qa</span></a><span>). NMSLIB is an efficient and generic k-NN search library. knn4qa is a flexible and modular retrieval toolkit capable of searching for mixed dense-sparse representations with the help of NMSLIB.</span></span></p> <p>&nbsp;</p><br> <p>            Last Modified: 06/16/2020<br>      Modified by: Eric&nbsp;Nyberg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The goal of the project is to develop next-generation retrieval methods and tools for question-answering and information retrieval, which apply k-NN search to hybrid document representations. These devices can replace or complement existing full-text retrieval techniques based on inverted files. We implement a proof-of-concept system that demonstrates the feasibility of this approach, which is based on the classic IBM Model-1 translation approach. We observe that the loss of accuracy due to the approximate nature of k-NN search is potentially a limiting factor in applying such techniques. Furthermore, we experimented with several neural representations, but have not yet obtained conclusive evidence with respect to their effectiveness. Despite demonstrating practicality of k-NN search in a text retrieval domain, we note that it is not yet clear if k-NN search can find many substantially different results compared to a classic term-based retrieval pipeline (with re-ranking).    A broader impact of our work consists in helping develop and evaluate efficient methods for k-NN search, which can work for a variety of hard dissimilarities and distances including the Euclidean distance, the inner product, and even substantially non-symmetric dissimilarities such as Renyi- or KL-divergence.   The grant helped maintain and develop two open-source libraries: NMSLIB (https://github.com/nmslib/nmslib) and knn4qa (https://github.com/oaqa/knn4qa). NMSLIB is an efficient and generic k-NN search library. knn4qa is a flexible and modular retrieval toolkit capable of searching for mixed dense-sparse representations with the help of NMSLIB.          Last Modified: 06/16/2020       Submitted by: Eric Nyberg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
