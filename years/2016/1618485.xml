<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: An Optimization Framework for Understanding Deep Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>458000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The past few years have seen a dramatic increase in the performance of pattern recognition systems due to the introduction of deep neural networks. However, the mathematical reasons for this success remain elusive. A key challenge is that the problem of learning the parameters of a neural network is a non-convex optimization problem, which makes finding the globally optimal parameters extremely difficult. Another challenge is that there is currently very limited theory about how the network architecture should be constructed (i.e., number of layers, number of neurons per layer, connectivity patterns, etc.). The goal of this project is to develop an optimization framework that provides theoretical insights for the success of current network architectures and guides the design of novel architectures with guarantees of global optimality.&lt;br/&gt; &lt;br/&gt;This project will develop a mathematical framework for the analysis of a broad class of non-convex optimization problems, including matrix factorization, tensor factorization, and deep learning. In particular, this project will study the problem of minimizing the sum of a loss function and a regularization function, both of which can be non-convex, but should satisfy a certain "positive homogeneity" property. By properly designing positively homogeneous regularizers that constrain the "network size," this project aims to show that, under certain conditions, all local minima are globally optimal, and one can find a global minimum from any initialization using a local descent strategy. A deeper understanding of the mathematical properties of deep networks will impact not only machine learning and optimization, where our understanding of non-convex problems continues to be very limited, but also application areas such as computer vision, speech and natural language processing, where deep networks currently give state-of-the-art results.</AbstractNarration>
<MinAmdLetterDate>06/07/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/08/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1618485</AwardID>
<Investigator>
<FirstName>Rene</FirstName>
<LastName>Vidal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rene Vidal</PI_FULL_NAME>
<EmailAddress>rvidal@cis.jhu.edu</EmailAddress>
<PI_PHON>4105167306</PI_PHON>
<NSF_ID>000486258</NSF_ID>
<StartDate>06/07/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212182608</ZipCode>
<StreetAddress><![CDATA[3400 N Charles Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~450000</FUND_OBLG>
<FUND_OBLG>2018~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Overview:&nbsp;</strong>&nbsp;The past few years have seen a dramatic increase in the performance of pattern recognition systems due to the introduction of deep networks for representation learning. However, the mathematical reasons for this success remain elusive. A key challenge is that the training problem is non-convex, hence optimization algorithms can get stuck in non-global minima. Currently, there is no theory guaranteeing that all local minima are global, nor is there an algorithm guaranteed to give a global minima. Moreover, there is no theory regarding how the network architecture should be constructed (i.e., number of layers, number of neurons per layer, connectivity patterns, etc.) nor how the network weights should be regularized. This project developed an optimization framework for the analysis of the optimization landscape of several classes of deep networks.&nbsp;</p> <p><strong>Intellectual Merit :&nbsp;</strong>This project developed a mathematical framework for the analysis of a broad class of non-convex optimization problems, including matrix factorization, tensor factorization, and deep learning. In particular, we studied the problem of minimizing the sum of a loss function and a regularization function, both of which can be non-convex, but should satisfy a certain &rdquo;positive homogeneity&rdquo; property. By properly designing positively homogeneous regularizers that measure the &rdquo;network size,&rdquo; we showed that, under certain conditions, all local minima are global, and one can find a global minima from any initialization using a local descent strategy. We showed that this is the case for matrix factorization problems, where size is measured by the matrix rank; neural networks with one hidden layer, where size is measured by the number of neurons in the hidden layer; and multiple deep networks of a fixed architecture connected in parallel, where size is measured by the number of parallel networks. We validated our theoretical findings in applications in computer vision and medical imaging.</p> <p><strong>Broader Impacts :&nbsp;</strong>This project significantly broadened the applicability of optimization algorithms to more complex non-convex optimization problems arising in machine learning, including matrix factorization, tensor factorization and neural network training. This project impacted many diversity outreach activities, including undergraduate students joining ongoing REU programs, students from the Society of Hispanic Professional Engineers (SHPE), and students from the STEM Achievement in Baltimore Elementary Schools (SABES) program. This project trained three PhD students and two undergraduate students at the intersection of machine learning, computer vision and optimization. This project produced one book chapter, 2 journal papers, and 7 conference papers. This project also impacted the research community through the organization of workshops in geometric learning, deep learning, and semantic information. Datasets and code will be made publicly accessible for research and educational purposes.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/29/2020<br>      Modified by: Rene&nbsp;Vidal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Overview:  The past few years have seen a dramatic increase in the performance of pattern recognition systems due to the introduction of deep networks for representation learning. However, the mathematical reasons for this success remain elusive. A key challenge is that the training problem is non-convex, hence optimization algorithms can get stuck in non-global minima. Currently, there is no theory guaranteeing that all local minima are global, nor is there an algorithm guaranteed to give a global minima. Moreover, there is no theory regarding how the network architecture should be constructed (i.e., number of layers, number of neurons per layer, connectivity patterns, etc.) nor how the network weights should be regularized. This project developed an optimization framework for the analysis of the optimization landscape of several classes of deep networks.   Intellectual Merit : This project developed a mathematical framework for the analysis of a broad class of non-convex optimization problems, including matrix factorization, tensor factorization, and deep learning. In particular, we studied the problem of minimizing the sum of a loss function and a regularization function, both of which can be non-convex, but should satisfy a certain "positive homogeneity" property. By properly designing positively homogeneous regularizers that measure the "network size," we showed that, under certain conditions, all local minima are global, and one can find a global minima from any initialization using a local descent strategy. We showed that this is the case for matrix factorization problems, where size is measured by the matrix rank; neural networks with one hidden layer, where size is measured by the number of neurons in the hidden layer; and multiple deep networks of a fixed architecture connected in parallel, where size is measured by the number of parallel networks. We validated our theoretical findings in applications in computer vision and medical imaging.  Broader Impacts : This project significantly broadened the applicability of optimization algorithms to more complex non-convex optimization problems arising in machine learning, including matrix factorization, tensor factorization and neural network training. This project impacted many diversity outreach activities, including undergraduate students joining ongoing REU programs, students from the Society of Hispanic Professional Engineers (SHPE), and students from the STEM Achievement in Baltimore Elementary Schools (SABES) program. This project trained three PhD students and two undergraduate students at the intersection of machine learning, computer vision and optimization. This project produced one book chapter, 2 journal papers, and 7 conference papers. This project also impacted the research community through the organization of workshops in geometric learning, deep learning, and semantic information. Datasets and code will be made publicly accessible for research and educational purposes.             Last Modified: 10/29/2020       Submitted by: Rene Vidal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
