<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SI2-SSI: FAMII: High Performance and Scalable Fabric Analysis, Monitoring and Introspection Infrastructure for HPC and Big Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>800000.00</AwardTotalIntnAmount>
<AwardAmount>800000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert Beverly</SignBlockName>
<PO_EMAI>rbeverly@nsf.gov</PO_EMAI>
<PO_PHON>7032927068</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As the computing, networking, heterogeneous hardware, and storage&lt;br/&gt;technologies continue to evolve in High-End Computing (HEC) platforms,&lt;br/&gt;it becomes increasingly essential and challenging to understand the&lt;br/&gt;interactions between time-critical High-Performance Computing (HPC)&lt;br/&gt;and Big Data applications, the software infrastructures upon which&lt;br/&gt;they rely for achieving high-performing portable solutions, the&lt;br/&gt;underlying communication fabric these high-performance middlewares&lt;br/&gt;depend on and the schedulers that manage HPC clusters.  Such&lt;br/&gt;understanding will enable all involved parties (application&lt;br/&gt;developers/users, system administrators, and middleware developers) to&lt;br/&gt;maximize the efficiency and performance of the individual components&lt;br/&gt;that comprise a modern HPC system and solve different grand challenge&lt;br/&gt;problems. There is a clear need and unfortunate lack of a high-performance and&lt;br/&gt;scalable tool that is capable of analyzing and correlating the&lt;br/&gt;communication on the fabric with the behavior of HPC/Big Data&lt;br/&gt;applications, underlying middleware and the job scheduler on existing&lt;br/&gt;large HPC systems.  The proposed synergistic and collaborative effort,&lt;br/&gt;undertaken by a team of computer and computational scientists from OSU&lt;br/&gt;and OSC, aims to create an integrated software infrastructure &lt;br/&gt;for high-performance and scalable Fabric Analysis, Monitoring and&lt;br/&gt;Introspection for HPC and Big Data. This tool will achieve the&lt;br/&gt;following objectives: 1) be portable, easy to use and easy to&lt;br/&gt;understand, 2) have high performance and scalable rendering and&lt;br/&gt;storage techniques and, 3) be applicable to the different&lt;br/&gt;communication fabrics and programming models that are likely to be&lt;br/&gt;used on existing large HPC systems and emerging exascale systems.  The&lt;br/&gt;transformative impact of the proposed research and development effort&lt;br/&gt;is to design a comprehensive analysis and performance monitoring tool&lt;br/&gt;for applications of current and next generation multi&lt;br/&gt;petascale/exascale systems to harness the maximum performance and&lt;br/&gt;scalability.&lt;br/&gt;&lt;br/&gt;The proposed research and the associated infrastructure will have a&lt;br/&gt;significant impact on enabling optimizations of HPC and Big Data&lt;br/&gt;applications that have previously been difficult to provide. These&lt;br/&gt;potential outcomes will be demonstrated by using the proposed&lt;br/&gt;framework to validate a variety of HPC and Big Data benchmarks and&lt;br/&gt;applications under multiple scenarios.  The integrated middleware and&lt;br/&gt;tools will be made publicly available to the community through public&lt;br/&gt;repositories and publications in the top forums, enabling other MPI&lt;br/&gt;and Big Data stacks to adopt the designs.  Research results will also&lt;br/&gt;be disseminated to the collaborating organizations of the&lt;br/&gt;investigators to impact their HPC software products and&lt;br/&gt;applications. The proposed research directions and their solutions&lt;br/&gt;will be used in the curriculum of the PIs to train undergraduate and&lt;br/&gt;graduate students, including under-represented minorities and female&lt;br/&gt;students. The technical challenges addressed by the proposal include: 1)&lt;br/&gt;Scalable visualization of large and complex HEC networks so as to&lt;br/&gt;provide a near instant rendering to end users, 2) A generalized data&lt;br/&gt;gathering scheme which is easily portable to multiple communication&lt;br/&gt;fabrics, novel compute architectures and high-performance middleware,&lt;br/&gt;3) Enhanced data storage performance through optimized database&lt;br/&gt;schemas and the use of memory-backed key value stores/databases, 4)&lt;br/&gt;Support in MPI, PGAS, and Big Data libraries to enable the proposed&lt;br/&gt;monitoring, analysis, and introspection framework, and 5) Enabling&lt;br/&gt;deeper introspection of particular regions of application.  The&lt;br/&gt;research will also be driven by a set of HPC and Big Data&lt;br/&gt;applications. The transformative impact of the proposed research and&lt;br/&gt;development effort is to design a comprehensive analysis and&lt;br/&gt;performance monitoring tool for applications of current and next&lt;br/&gt;generation multi petascale/exascale systems to harness the maximum&lt;br/&gt;performance and scalability.</AbstractNarration>
<MinAmdLetterDate>05/04/2017</MinAmdLetterDate>
<MaxAmdLetterDate>11/21/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1664137</AwardID>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dhabaleswar K Panda</PI_FULL_NAME>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142925199</PI_PHON>
<NSF_ID>000487085</NSF_ID>
<StartDate>05/04/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Karen</FirstName>
<LastName>Tomko</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karen A Tomko</PI_FULL_NAME>
<EmailAddress>ktomko@osc.edu</EmailAddress>
<PI_PHON>6142921091</PI_PHON>
<NSF_ID>000330142</NSF_ID>
<StartDate>05/04/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Xiaoyi</FirstName>
<LastName>Lu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaoyi Lu</PI_FULL_NAME>
<EmailAddress>xiaoyi.lu@ucmerced.edu</EmailAddress>
<PI_PHON>6148860927</PI_PHON>
<NSF_ID>000696718</NSF_ID>
<StartDate>05/04/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hari</FirstName>
<LastName>Subramoni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hari Subramoni</PI_FULL_NAME>
<EmailAddress>subramoni.1@osu.edu</EmailAddress>
<PI_PHON>6146888735</PI_PHON>
<NSF_ID>000704577</NSF_ID>
<StartDate>05/04/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kevin</FirstName>
<LastName>Manalo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>PhD</PI_SUFX_NAME>
<PI_FULL_NAME>Kevin Manalo</PI_FULL_NAME>
<EmailAddress>kmanalo@osc.edu</EmailAddress>
<PI_PHON>6146888735</PI_PHON>
<NSF_ID>000714469</NSF_ID>
<StartDate>05/04/2017</StartDate>
<EndDate>11/21/2017</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName/>
<StateCode>OH</StateCode>
<ZipCode>432101206</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>026Z</Code>
<Text>NSCI: National Strategic Computing Initi</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<ProgramReference>
<Code>8009</Code>
<Text>Scientifc Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~800000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As the computing, networking, heterogeneous hardware, and storage<br />technologies continue to evolve in high-end computing platforms, it<br />becomes increasingly essential and challenging to understand the<br />interactions between time-critical High-Performance Computing (HPC),<br />Big Data, and AI applications, the software infrastructures upon which<br />they rely for achieving high-performing portable solutions, and the<br />underlying communication fabric (network between computer in an HPC<br />system). We refer to these high-performance software layers between<br />the applications and the hardware as middleware.&nbsp; It is critical for<br />the users and administrators of HPC installations as well as<br />developers of high-performance middleware that run on these HPC<br />installations to clearly understand this interaction. Such<br />understanding will enable all the involved parties (application<br />developers/users, system administrators, and MPI/PGAS/Big Data/AI<br />programming language and middleware developers) to maximize the<br />efficiency and performance of the various individual components that<br />comprise a modern HPC system and solve the various "grand challenge"<br />problems.<br /><br />One of the most common questions HPC scientists tend to have is: "Why<br />is my application running slower than usual now?" Interaction with a<br />concurrent job in the network, a fragmented allocation of nodes, or<br />issues with the network-based parallel file system are the most common<br />causes for such unexplained behavior.&nbsp; Several tools exist in the<br />literature and as the products that allow the system administrators to<br />analyze and inspect the communication.&nbsp; However, due to the lack of<br />interaction with, and knowledge about the communication software<br />stack, no existing HPC fabric monitoring tool can correlate network<br />level and middleware level behavior to classify traffic as belonging<br />to or being generated by particular communication operations (e.g.:<br />pairwise between two computers, remote memory access of a computer by<br />another computer, global sum between all computers solving an AI<br />problem).</p> <p>To address the above outlined challenges, in this project, we have<br />adopted a multi-year and multi-tiered approach to utilize the<br />underlying system architecture to improve MPI, Big Data and AI<br />communication and application performance and scalability.&nbsp; Challenges<br />have been addressed along the following directions:<br /><br />1. Designed and developed Optimized network graph display for<br />InfiniBand and Omni-Path fabric and MPI/AI application analysis<br /><br />2. Designed and developed interfaces between software components to<br />support monitoring communication between CPU within a computer and<br />between different computers in the HPC system.<br /><br />3. Designed FAMII data collection interface for fabric health and load<br />monitoring, collecting detailed information from MPI, Big Data and AI<br />application runs.<br /><br />4. Designed visualization interface to profile, monitor and understand<br />CPU- and GPU-based communication traffic.<br /><br />5. Designs have been made available to the HPC community through multiple releases.<br /><br />6. Deployment of the framework on various HPC systems at the Ohio<br />Supercomputer Center and partner institutions and continuous<br />engagement with the users to improve the design.<br /><br />Some highlights of the FAMII tool are as follows:<br /><br />* Running on multiple HPC systems with more than 2,000 computers in a stable manner for more than two years.<br /><br />* Discovering fabric topology within 5 minutes for ~1,500 computer system.<br /><br />* Remote read and store of all fabric metrics at sub-second granularity.<br /><br />* End-to-end overhead of less than 2 percent on application performance.<br /><br />* Display network traffic on HPC systems up to 2,000 computers in sub-second granularity<br /><br />* Capability to visualize application-based communication with 5 second granularity<br /><br />The results of this research (new designs, performance results,<br />benchmarks, etc.) have been made available to the community through<br />the FAMII/INAM software tool together with the enhanced MVAPICH2-x<br />library software. Multiple releases of these two software packages<br />have been made during the project period. More than 6,500 copies of<br />the INAM tool and the enhanced MVAPICH2-X library have been downloaded<br />from the project's web site. In each of these releases, features,<br />performance numbers and scalability information have been shared with<br />the MVAPICH user community through mailing lists and the project's web<br />site.&nbsp; In addition to the software distribution, the results have been<br />presented at various conferences and events through Keynote talks,<br />invited talks, tutorials, and hands-on sessions.&nbsp; The research has<br />also led to thesis for several M.S. and Ph.D. students.</p><br> <p>            Last Modified: 10/13/2020<br>      Modified by: Dhabaleswar&nbsp;K&nbsp;Panda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As the computing, networking, heterogeneous hardware, and storage technologies continue to evolve in high-end computing platforms, it becomes increasingly essential and challenging to understand the interactions between time-critical High-Performance Computing (HPC), Big Data, and AI applications, the software infrastructures upon which they rely for achieving high-performing portable solutions, and the underlying communication fabric (network between computer in an HPC system). We refer to these high-performance software layers between the applications and the hardware as middleware.  It is critical for the users and administrators of HPC installations as well as developers of high-performance middleware that run on these HPC installations to clearly understand this interaction. Such understanding will enable all the involved parties (application developers/users, system administrators, and MPI/PGAS/Big Data/AI programming language and middleware developers) to maximize the efficiency and performance of the various individual components that comprise a modern HPC system and solve the various "grand challenge" problems.  One of the most common questions HPC scientists tend to have is: "Why is my application running slower than usual now?" Interaction with a concurrent job in the network, a fragmented allocation of nodes, or issues with the network-based parallel file system are the most common causes for such unexplained behavior.  Several tools exist in the literature and as the products that allow the system administrators to analyze and inspect the communication.  However, due to the lack of interaction with, and knowledge about the communication software stack, no existing HPC fabric monitoring tool can correlate network level and middleware level behavior to classify traffic as belonging to or being generated by particular communication operations (e.g.: pairwise between two computers, remote memory access of a computer by another computer, global sum between all computers solving an AI problem).  To address the above outlined challenges, in this project, we have adopted a multi-year and multi-tiered approach to utilize the underlying system architecture to improve MPI, Big Data and AI communication and application performance and scalability.  Challenges have been addressed along the following directions:  1. Designed and developed Optimized network graph display for InfiniBand and Omni-Path fabric and MPI/AI application analysis  2. Designed and developed interfaces between software components to support monitoring communication between CPU within a computer and between different computers in the HPC system.  3. Designed FAMII data collection interface for fabric health and load monitoring, collecting detailed information from MPI, Big Data and AI application runs.  4. Designed visualization interface to profile, monitor and understand CPU- and GPU-based communication traffic.  5. Designs have been made available to the HPC community through multiple releases.  6. Deployment of the framework on various HPC systems at the Ohio Supercomputer Center and partner institutions and continuous engagement with the users to improve the design.  Some highlights of the FAMII tool are as follows:  * Running on multiple HPC systems with more than 2,000 computers in a stable manner for more than two years.  * Discovering fabric topology within 5 minutes for ~1,500 computer system.  * Remote read and store of all fabric metrics at sub-second granularity.  * End-to-end overhead of less than 2 percent on application performance.  * Display network traffic on HPC systems up to 2,000 computers in sub-second granularity  * Capability to visualize application-based communication with 5 second granularity  The results of this research (new designs, performance results, benchmarks, etc.) have been made available to the community through the FAMII/INAM software tool together with the enhanced MVAPICH2-x library software. Multiple releases of these two software packages have been made during the project period. More than 6,500 copies of the INAM tool and the enhanced MVAPICH2-X library have been downloaded from the project's web site. In each of these releases, features, performance numbers and scalability information have been shared with the MVAPICH user community through mailing lists and the project's web site.  In addition to the software distribution, the results have been presented at various conferences and events through Keynote talks, invited talks, tutorials, and hands-on sessions.  The research has also led to thesis for several M.S. and Ph.D. students.       Last Modified: 10/13/2020       Submitted by: Dhabaleswar K Panda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
