<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Integrated Computer Vision System Based on Human Eye Motion</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>332659.00</AwardTotalIntnAmount>
<AwardAmount>399020</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert Landers</SignBlockName>
<PO_EMAI>rlanders@nsf.gov</PO_EMAI>
<PO_PHON>7032922652</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to build a robotic vision system that mimics human eye motions. In particular, this research will lead to an enhanced vision system that can achieve quick scanning capability (saccadic eye motion) and smooth object following capability (smooth-pursuit) with minimal computational overhead and using an inexpensive generic hardware.  Current state-of-the-art systems require significant computer post-processing and expensive hardware.  This project will lead to a novel camera positioning system (hardware) and real-time vision (software) system that does not require any post-processing and produces ready-to-use images in real-time. To achieve this goal, the project draws upon the biomechanical and cognitive principles of human vision. The innovative methods and algorithms that can produce motions with speed, accuracy, and smoothness to mimic human eye can greatly enhance capabilities of autonomous vehicles used in a wide variety of applications such as agriculture, military, security, and traffic monitoring. The project will integrate research and outreach activities for K-12 and undergraduate students. &lt;br/&gt;&lt;br/&gt;This project studies a bio-inspired dynamics-based method for coordinating motion control and image processing where the system can mechanically displace the field of view by a large angle between frames. The concept behind this research is to merge the system dynamics area and image processing area while previous studies focused solely on either mechanical design or image processing.  The method is motivated by the principles of human vision for effective image de-blurring and panoramic image stitching. In coordination with inherently discrete and rapid ocular movements, the developed image processing methods inspired by ocular physiology will mimic saccades and smooth-pursuit in a fast-moving robotic eye.  A piezoelectrically driven robotic camera positioning mechanism will be employed to demonstrate the effectiveness of this ocular physiology-inspired approach. Hardware architecture that can synchronize image processing and real-time motion control will be configured and tested. A panoramic image of a scene will be generated from multiple images acquired during saccades by removing motion blur and stitching images within a single frame rate. The system architecture will be optimized to best coordinate hardware and software for real-time motion control.</AbstractNarration>
<MinAmdLetterDate>05/16/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/24/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1662029</AwardID>
<Investigator>
<FirstName>Jun</FirstName>
<LastName>Ueda</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jun Ueda</PI_FULL_NAME>
<EmailAddress>jun.ueda@me.gatech.edu</EmailAddress>
<PI_PHON>4043853900</PI_PHON>
<NSF_ID>000508370</NSF_ID>
<StartDate>05/16/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1642</Code>
<Text>Special Initiatives</Text>
</ProgramElement>
<ProgramElement>
<Code>7569</Code>
<Text>Dynamics, Control and System D</Text>
</ProgramElement>
<ProgramReference>
<Code>030E</Code>
<Text>CONTROL SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>034E</Code>
<Text>Dynamical systems</Text>
</ProgramReference>
<ProgramReference>
<Code>091Z</Code>
<Text>Data Initiatives</Text>
</ProgramReference>
<ProgramReference>
<Code>8024</Code>
<Text>Complex Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~332659</FUND_OBLG>
<FUND_OBLG>2020~66361</FUND_OBLG>
</Award>
</rootTag>
