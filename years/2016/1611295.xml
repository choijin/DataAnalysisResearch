<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CCSS: Programmable Mixed-Signal Vision Sensor for Continuous Mobile Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>350000.00</AwardTotalIntnAmount>
<AwardAmount>350000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Lawrence Goldberg</SignBlockName>
<PO_EMAI>lgoldber@nsf.gov</PO_EMAI>
<PO_PHON>7032928339</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The emergence of wearable devices has made it possible for computers to continuously interpret the user environment, or continuous mobile vision. It can extend a user's memory and attention, not only enabling previously impossible, personalized services but also assisting people with vision or attention impairment in an unprecedented way.  While modern devices are capable of capturing and interpreting what their users see, they face a daunting physical barrier: energy efficiency. For example, performing continuous vision workloads drains the battery of Google Glass in about 40 minutes. While process technology and system-level optimization techniques may continue to improve the energy efficiency of digital circuits, a recent measurement study has pointed to a fundamental bottleneck to energy efficiency of computer vision: the image sensor, especially its analog readout circuitry.  The goal of this project is to tackle this bottleneck by designing, prototyping and evaluating a novel vision sensor architecture along with its optimization framework. By targeting computer vision, this vision sensor architecture radically departs from existing image sensor designs that are optimized for photography. Instead of producing high-quality images, it outputs application-specific features by judiciously shifting processing into the analog domain. In doing so, it promises better efficiency by orders of magnitudes for computer vision workloads and relieves the privacy concern with continuous mobile vision.&lt;br/&gt;&lt;br/&gt;This project will pursue two complementary, interrelated directions toward the above goal: First, the mixed-signal vision sensor design must provide sufficient programmability under constrained complexity in the analog domain. The project will exploit a novel hardware architecture that cyclically reuses analog modules for a programmable dataflow. This architecture will employ a novel column-based topology that exploits data locality to reduce interconnect complexity for data access. The project will also investigate hardware mechanisms that allow programmable tradeoffs between efficiency and accuracy of processing in the analog domain. Second, vision workloads must be carefully partitioned into analog and digital stages given the sensor architecture. The project shall provide an optimization framework that leverages accurate energy models and noise tolerance of vision workload. The project will further contribute novel use cases of the proposed mixed-signal vision sensor with data privacy, energy consumption, and task performance being the possible optimization constraints.</AbstractNarration>
<MinAmdLetterDate>06/02/2016</MinAmdLetterDate>
<MaxAmdLetterDate>06/02/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1611295</AwardID>
<Investigator>
<FirstName>Lin</FirstName>
<LastName>Zhong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lin Zhong</PI_FULL_NAME>
<EmailAddress>lin.zhong@yale.edu</EmailAddress>
<PI_PHON>2034369450</PI_PHON>
<NSF_ID>000189041</NSF_ID>
<StartDate>06/02/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 Main St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7564</Code>
<Text>CCSS-Comms Circuits &amp; Sens Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>153E</Code>
<Text>Wireless comm &amp; sig processing</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~350000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-052eeda4-7fff-58e1-d050-ada9d398e678"> <p dir="ltr"><span>This project was inspired by wearable devices like Google Glass to empower a future in which computers can see what their users see, all the time, or continuous mobile vision. It addresses two technical challenges toward this future, i.e. energy efficiency and privacy, which have plagued existing commercial attempts. For example, performing continuous vision workloads drains the battery of Google Glass in about 40 minutes; and many restaurants banned it for privacy concerns. This project tackles both challenges with a simple insight: image sensors used in today&rsquo;s mobile devices are optimized for photography and intend their output for human consumption. In contrast, this project builds an energy-efficient, privacy-preserving image sensor for computer vision, or vision sensor. Instead of high-quality images, the vision sensor outputs privacy-preserving features learned via machine learning for machine consumption.</span></p> <p dir="ltr"><span>In doing so, this project makes the following technical contributions. First, it quantifies the energy impact of analog-digital conversion (ADC) in image sensing and contributes novel mixed-signal designs that exploit analog processing to reduce the workload and therefore, energy cost of ADC, and improve computation efficiency by making fidelity vs. energy tradeoffs that are more profitable than those possible in the digital domain. Second, its results highlight that energy-fidelity tradeoff is highly profitable in neural networks-based machine learning: neural networks are very resilient against noise introduced by analog processing in both training and inference. It suggests novel ways to compress neural networks for efficient inference on mobile and embedded systems. Finally, this project quantifies the privacy-utility tradeoff in neural network-based inference. It shows by a careful combination of discriminative, generative, and adversarial training, improved privacy can be realized without sacrificing utility. It provides both a concrete realization and an extensive set of empirical results, which suggest that the privacy constraint may function as a way of regularization and help the learned model generalize better.&nbsp;</span></p> <p dir="ltr"><span>By addressing the two largest technical challenges to continuous mobile vision, i.e. energy and privacy, the project is likely to revive commercial interests in consumer-oriented wearable devices that can see and analyze all the time. In the short term, the results will empower users of special needs, such as first responders and vision/cognitive impaired. In the long term, the project provides the key enabling technologies for mobile computing beyond smartphones. Results from this project have been shared with the community in peer-reviewed publications; tools developed have been made open-source.</span></p> <br /><br /><br /><br /><br /></span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/29/2019<br>      Modified by: Lin&nbsp;Zhong</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  This project was inspired by wearable devices like Google Glass to empower a future in which computers can see what their users see, all the time, or continuous mobile vision. It addresses two technical challenges toward this future, i.e. energy efficiency and privacy, which have plagued existing commercial attempts. For example, performing continuous vision workloads drains the battery of Google Glass in about 40 minutes; and many restaurants banned it for privacy concerns. This project tackles both challenges with a simple insight: image sensors used in today?s mobile devices are optimized for photography and intend their output for human consumption. In contrast, this project builds an energy-efficient, privacy-preserving image sensor for computer vision, or vision sensor. Instead of high-quality images, the vision sensor outputs privacy-preserving features learned via machine learning for machine consumption. In doing so, this project makes the following technical contributions. First, it quantifies the energy impact of analog-digital conversion (ADC) in image sensing and contributes novel mixed-signal designs that exploit analog processing to reduce the workload and therefore, energy cost of ADC, and improve computation efficiency by making fidelity vs. energy tradeoffs that are more profitable than those possible in the digital domain. Second, its results highlight that energy-fidelity tradeoff is highly profitable in neural networks-based machine learning: neural networks are very resilient against noise introduced by analog processing in both training and inference. It suggests novel ways to compress neural networks for efficient inference on mobile and embedded systems. Finally, this project quantifies the privacy-utility tradeoff in neural network-based inference. It shows by a careful combination of discriminative, generative, and adversarial training, improved privacy can be realized without sacrificing utility. It provides both a concrete realization and an extensive set of empirical results, which suggest that the privacy constraint may function as a way of regularization and help the learned model generalize better.  By addressing the two largest technical challenges to continuous mobile vision, i.e. energy and privacy, the project is likely to revive commercial interests in consumer-oriented wearable devices that can see and analyze all the time. In the short term, the results will empower users of special needs, such as first responders and vision/cognitive impaired. In the long term, the project provides the key enabling technologies for mobile computing beyond smartphones. Results from this project have been shared with the community in peer-reviewed publications; tools developed have been made open-source.                Last Modified: 10/29/2019       Submitted by: Lin Zhong]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
