<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>PFI:BIC: Multimodal-Sensor-Enabled Environments with Advanced Cognitive Computing Enabling Smart Group Meeting Facilitation Services.</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>1000000.00</AwardTotalIntnAmount>
<AwardAmount>1000000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jesus Soriano Molla</SignBlockName>
<PO_EMAI>jsoriano@nsf.gov</PO_EMAI>
<PO_PHON>7032927795</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Millions of meetings take place every day in the US, incurring a tremendous cost in terms of managers' and employees' precious time and salary.  Unfortunately, group meetings suffer from serious problems that undermine productivity and collegiality, including overt or unconscious bias, "groupthink", fear of speaking, and unfocused discussion.  Few automatic tools exist for keeping meetings on track, accurately recording who said what, and making group discussions more productive.  The goal of this research is to design intelligent rooms that provide facilitation services by identifying meeting participants, understand their conversations, summarize discussions, and help the group efficiently get through an agenda, all without requiring the participants to wear microphones or other sensors.   &lt;br/&gt;&lt;br/&gt;The research will have broader impacts in several aspects.  Any steps to make group meetings for complex, long-term projects more productive and easier to control would result in immediate economic impact.  The success of a service system that facilitates long-term group interactions will result in a major opportunity for technology transfer and a highly marketable hardware/software platform for collaboration in domains including business, education, and finance.  The project will result in new group meeting data to be used by researchers in different fields such as organizational psychologists, and computer scientists.  Finally, the project will produce highly visible infrastructure for research and education that has the potential for greater public engagement with science and technology.&lt;br/&gt;&lt;br/&gt;The research will be realized at different scales in two existing physical testbeds: the Smart Conference Room (SCR) at the Engineering Research Center for Lighting Enabled Systems and Applications (LESA) and the Collaborative Research Augmented Immersive Virtual Environment Lab (CRAIVE-Lab), a much larger space with a tall, panoramic screen.  Both testbeds will be expanded as part of this project to integrate sensing and computing technologies developed by the partners.  The major technology modules include:  (1) advanced time-of-flight sensors for robust occupant tracking, resulting in centimeter-accurate, real-time locations of all participants in the meeting without requiring video cameras or wearable sensors; (2) custom beamforming microphone arrays for acoustical tracking and sound source separation, allowing highly-directional beams to be directed at each participant's instantaneous location, clearly isolating their speech without requiring lapel or headset microphones; (3) natural language understanding algorithms for extracting knowledge from the speech, enabling the system to learn meeting-specific terminology, link concepts, assess speaker roles, and summarize discussion; and (4) cognitive computing tools for meeting facilitation, including assessments of participation and productivity, ideation interventions for brainstorming, and active decision support.  The resulting service systems will be cognitive physical environments that understand their occupants' locations, movement, speech, vocabulary, and intentions. A key aspect of the research is a multi-year study that tracks technical research groups that hold regular, unscripted meetings in the testbeds, assessing the effectiveness of the service system.  &lt;br/&gt;&lt;br/&gt;The lead institution for the project is Rensselaer Polytechnic Institute, with investigators from the departments of Electrical, Computer, and Systems Engineering, Computer Science, and Architecture.  The industrial partners in the effort are IBM Research (large business, Yorktown Heights, NY) and &lt;br/&gt;Heptagon Advanced MicroOptics (large business, Zurich, Switzerland).&lt;br/&gt;&lt;br/&gt;This award is partially supported by funds from the Directorate for Computer and Information Science and Engineering (CISE), Divisions of Information and Intelligent Systems (IIS) and Computer and Network Systems (CNS).</AbstractNarration>
<MinAmdLetterDate>08/29/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1631674</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Radke</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard Radke</PI_FULL_NAME>
<EmailAddress>rjradke@ecse.rpi.edu</EmailAddress>
<PI_PHON>5182766483</PI_PHON>
<NSF_ID>000185530</NSF_ID>
<StartDate>08/29/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jonas</FirstName>
<LastName>Braasch</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonas Braasch</PI_FULL_NAME>
<EmailAddress>braasj@rpi.edu</EmailAddress>
<PI_PHON>5182763864</PI_PHON>
<NSF_ID>000186127</NSF_ID>
<StartDate>08/29/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Heng</FirstName>
<LastName>Ji</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Heng Ji</PI_FULL_NAME>
<EmailAddress>hengji@illinois.edu</EmailAddress>
<PI_PHON>6466625355</PI_PHON>
<NSF_ID>000514379</NSF_ID>
<StartDate>08/29/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Karlicek</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert F Karlicek</PI_FULL_NAME>
<EmailAddress>karlir@rpi.edu</EmailAddress>
<PI_PHON>5182763310</PI_PHON>
<NSF_ID>000558685</NSF_ID>
<StartDate>08/29/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rensselaer Polytechnic Institute</Name>
<CityName>Troy</CityName>
<ZipCode>121803522</ZipCode>
<PhoneNumber>5182766000</PhoneNumber>
<StreetAddress>110 8TH ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002430742</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RENSSELAER POLYTECHNIC INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002430742</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rensselaer Polytechnic Institute]]></Name>
<CityName>Troy</CityName>
<StateCode>NY</StateCode>
<ZipCode>121803522</ZipCode>
<StreetAddress><![CDATA[110 8th St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1662</Code>
<Text>PFI-Partnrships for Innovation</Text>
</ProgramElement>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramReference>
<Code>1662</Code>
<Text>PARTNRSHIPS FOR INNOVATION-PFI</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~1000000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed smart service systems for sensing and understanding natural group meetings, realized in two Rensselaer campus testbeds previously funded by NSF (the Smart Conference Room and the CRAIVE-Lab). These testbeds were instrumented with advanced time-of-flight sensors, custom-made beamforming microphones, and new cognitive computing algorithms to process the resulting sensor data.&nbsp; The resulting service systems created cognitive physical environments that can understand their occupants' locations, movement, speech, vocabulary, and intentions.</p> <p>Intellectual Merit: &nbsp;The key aspects of the project that set it apart from existing systems were: (1) a focus on group discussion and interaction, rather than single-user-centered technologies, and (2) a tight integration of multimodal-sensor-enabled environments with advanced cognitive computing systems that allowed the testbeds to understand the behavior of their occupants. &nbsp;Over the course of the project, participants developed novel sensor platforms and algorithms for estimating human body orientation in a privacy-preserving manner; accurately estimating the visual focus of attention of all participants in a group; correlating automatically extracted visual, non-verbal, and speech features with participants' leadership and contribution scores; automatically understanding the progress and summarizing the results of a group discussion task from recorded speech; and predicting personality traits and social roles from sensed group meeting data.&nbsp;</p> <p>Broader Impacts: Millions of meetings take place every day in the US, incurring a tremendous cost in terms of managers' and employees' precious time and salary. Any steps to make group meetings for complex, long-term projects more productive and easier to control would result in immediate economic impact. &nbsp;The developed technologies for understanding group interactions could impact collaborations in domains including business, education, and finance. The testbeds that host the developed service system already reach a larger community than typical insular research environments, and are regularly toured by visiting speakers, industry visitors, and even high-school students in fields beyond science and engineering including architecture and the arts. The project expanded these technologically advanced testbeds with both cutting-edge sensors and underlying cognitive systems, making them even more powerful and versatile, and appealing to a broader range of external collaborators. &nbsp;The project also produced new publicly-available group meeting understanding datasets, including time and location-stamped sensor data and utterances from each participant, which are important to drive global research in group dynamics and computational linguistics.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/22/2020<br>      Modified by: Richard&nbsp;Radke</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603385114809_summary-image--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603385114809_summary-image--rgov-800width.jpg" title="Multimodal Analysis of Group Interactions"><img src="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603385114809_summary-image--rgov-66x44.jpg" alt="Multimodal Analysis of Group Interactions"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Multimodal-sensor-enabled environments were instrumented with advanced cognitive computing systems that allowed them to understand the behavior and activities of their occupants during a group discussion using several different modalities (visual, non-verbal audio, transcribed speech).</div> <div class="imageCredit">Richard J. Radke, Rensselaer Polytechnic Institute</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Richard&nbsp;Radke</div> <div class="imageTitle">Multimodal Analysis of Group Interactions</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383339110_comb_all--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383339110_comb_all--rgov-800width.jpg" title="Visual focus of attention estimation"><img src="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383339110_comb_all--rgov-66x44.jpg" alt="Visual focus of attention estimation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The 3D head pose and visual focus of attention for each participant in a meeting are estimated from time-of-flight sensors embedded in the ceiling of a Smart Conference Room.</div> <div class="imageCredit">Richard J. Radke, Rensselaer Polytechnic Institute</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Richard&nbsp;Radke</div> <div class="imageTitle">Visual focus of attention estimation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383452873_nlp_updated--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383452873_nlp_updated--rgov-800width.jpg" title="Automatic summarization of a structured group meeting"><img src="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383452873_nlp_updated--rgov-66x44.jpg" alt="Automatic summarization of a structured group meeting"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A snapshot of the graphical summarization of a group meeting. The areas of the red circles are proportional to thecumulative speaking time of each participant. Straight linesconnect the participants with items and item rankings they have proposed in the context of a group task.</div> <div class="imageCredit">Richard J. Radke, Rensselaer Polytechnic Institute</div> <div class="imageSubmitted">Richard&nbsp;Radke</div> <div class="imageTitle">Automatic summarization of a structured group meeting</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603384571850_pipeline3--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603384571850_pipeline3--rgov-800width.jpg" title="Correlating visual features with personality and performance"><img src="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603384571850_pipeline3--rgov-66x44.jpg" alt="Correlating visual features with personality and performance"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The co-occurrence of automatically extracted visual events (visual focus of attention, level of body movement, relative hand-face position) are used to accurately estimate and predict individual personality traits and perceived emergent leadership/contribution scores in a group discussion.</div> <div class="imageCredit">Richard J. Radke, Rensselaer Polytechnic Institute</div> <div class="imageSubmitted">Richard&nbsp;Radke</div> <div class="imageTitle">Correlating visual features with personality and performance</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383230466_body_pose2--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383230466_body_pose2--rgov-800width.jpg" title="Estimating body pose from overhead time-of-flight sensors"><img src="/por/images/Reports/POR/2020/1631674/1631674_10456327_1603383230466_body_pose2--rgov-66x44.jpg" alt="Estimating body pose from overhead time-of-flight sensors"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Data from time-of-flight sensors in a Smart Conference Room are processed to produce accurate estimates of participants' body orientation (green arrows).  The red circle indicates the active speaker, detected from microphone recordings.</div> <div class="imageCredit">Richard J. Radke, Rensselaer Polytechnic Institute</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Richard&nbsp;Radke</div> <div class="imageTitle">Estimating body pose from overhead time-of-flight sensors</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed smart service systems for sensing and understanding natural group meetings, realized in two Rensselaer campus testbeds previously funded by NSF (the Smart Conference Room and the CRAIVE-Lab). These testbeds were instrumented with advanced time-of-flight sensors, custom-made beamforming microphones, and new cognitive computing algorithms to process the resulting sensor data.  The resulting service systems created cognitive physical environments that can understand their occupants' locations, movement, speech, vocabulary, and intentions.  Intellectual Merit:  The key aspects of the project that set it apart from existing systems were: (1) a focus on group discussion and interaction, rather than single-user-centered technologies, and (2) a tight integration of multimodal-sensor-enabled environments with advanced cognitive computing systems that allowed the testbeds to understand the behavior of their occupants.  Over the course of the project, participants developed novel sensor platforms and algorithms for estimating human body orientation in a privacy-preserving manner; accurately estimating the visual focus of attention of all participants in a group; correlating automatically extracted visual, non-verbal, and speech features with participants' leadership and contribution scores; automatically understanding the progress and summarizing the results of a group discussion task from recorded speech; and predicting personality traits and social roles from sensed group meeting data.   Broader Impacts: Millions of meetings take place every day in the US, incurring a tremendous cost in terms of managers' and employees' precious time and salary. Any steps to make group meetings for complex, long-term projects more productive and easier to control would result in immediate economic impact.  The developed technologies for understanding group interactions could impact collaborations in domains including business, education, and finance. The testbeds that host the developed service system already reach a larger community than typical insular research environments, and are regularly toured by visiting speakers, industry visitors, and even high-school students in fields beyond science and engineering including architecture and the arts. The project expanded these technologically advanced testbeds with both cutting-edge sensors and underlying cognitive systems, making them even more powerful and versatile, and appealing to a broader range of external collaborators.  The project also produced new publicly-available group meeting understanding datasets, including time and location-stamped sensor data and utterances from each participant, which are important to drive global research in group dynamics and computational linguistics.          Last Modified: 10/22/2020       Submitted by: Richard Radke]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
