<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAPSI: Investigating Neural Coding of Spatiotemporal Events Using Visual Adaptation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2016</AwardEffectiveDate>
<AwardExpirationDate>05/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>5400.00</AwardTotalIntnAmount>
<AwardAmount>5400</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration>How we perceive the world depends strongly on our recent experience. For example, after briefly looking at a red square or tilted lines, our perception of color or orientation is biased. These adaptation effects have been widely studied and are thought to play an important role in calibrating and optimizing visual coding. This award research aims to test for analogous adaptation and renormalization to the temporal structure of the world.  In particular, the goal is to test for normalization in the temporal domain, testing for adaptation to events, to the unfolding of visual images over time, and measuring measure this for both artificial stimuli and for videos of naturalistic events. The research will be conducted in collaboration with Dr. Gerrit Maus of Nanyang Technological University in Singapore, is a leading authority on temporal and visual coding. The research is expected to contribute to the cognitive science of vision and visual adaptation.&lt;br/&gt;&lt;br/&gt;This study tests whether the patterns of adaptation found for color and form also extend in similar ways to events and time. The project will test how adaptation affects the norms for temporal information, and whether this points to common coding principles in the visual system. The research will test the following hypothesis: if temporal integration of spatial features is perceptually coded based on deviations from a norm (as is the case in other perceptual categories), then visual adaptation will reveal the perceptual norm for naturalistic events, giving important insight in the perception of spatiotemporal integration.&lt;br/&gt;&lt;br/&gt;This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the National Research Foundation of Singapore.</AbstractNarration>
<MinAmdLetterDate>07/20/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1614427</AwardID>
<Investigator>
<FirstName>Katherine</FirstName>
<LastName>Mussell</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katherine E Mussell</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>9707685386</PI_PHON>
<NSF_ID>000711340</NSF_ID>
<StartDate>07/20/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Mussell                 Katherine      E</Name>
<CityName>Sparks</CityName>
<ZipCode>894340783</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Nevada</StateName>
<StateCode>NV</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NV02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Mussell                 Katherine      E]]></Name>
<CityName>Sparks</CityName>
<StateCode>NV</StateCode>
<ZipCode>894340783</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nevada</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NV02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5927</Code>
<Text>EAST ASIA, OTHER</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~5400</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>How we perceive the world depends strongly on our recent experience. For example, after briefly looking at a red square, a gray square will appear greenish. This bias results from visual adaptation, and studies of these effects have provided an important tool for understanding how information is represented and calibrated within the visual system. The current project applied visual adaptation to understand the perception of time and events. Participants viewed scenes that change at different rates over time and then judged the apparent rate or duration of events. Our stimuli consisted of a scene of flickering squares, the mean rate of flicker was normally distributed to simulate a distribution of event changes that would occur in a more naturalistic scene. Our results revealed that, following adaptation to the scene, the perception of the rate of flicker drastically decreased (Figure 1). We also conducted a secondary experiment to explore whether or not participants could consistently and accurately report the average rate of flicker of the stimuli. This experiment is ongoing, but preliminary results revealed that participants tended to overestimate the average flicker rate of the scene (Figure 2). The research was conducted in collaboration with Dr. Gerrit Maus, a leading expert in interactions between temporal and spatial vision, at the Nanyang Technological University in Singapore. The project is currently ongoing.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/27/2017<br>      Modified by: Katherine&nbsp;E&nbsp;Mussell</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1614427/1614427_10442180_1490657001205_Figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1614427/1614427_10442180_1490657001205_Figure1--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2017/1614427/1614427_10442180_1490657001205_Figure1--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The average estimate of flicker rate before (red) and after (blue) adaptation. Results average across 6 participants, error bars represent standard error of the mean. Solid black line represents actual mean flicker rates.</div> <div class="imageCredit">Katherine Mussell</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Katherine&nbsp;E&nbsp;Mussell</div> <div class="imageTitle">Figure 1</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1614427/1614427_10442180_1490657057370_Figure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1614427/1614427_10442180_1490657057370_Figure2--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2017/1614427/1614427_10442180_1490657057370_Figure2--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">1 participant?s results from judging either the mean flicker rate of the entire field (blue), or judging the flicker rate of a single square within the field (red). The solid black line represents the actual mean flicker rate of the field.</div> <div class="imageCredit">Katherine Mussell</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Katherine&nbsp;E&nbsp;Mussell</div> <div class="imageTitle">Figure 2</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    How we perceive the world depends strongly on our recent experience. For example, after briefly looking at a red square, a gray square will appear greenish. This bias results from visual adaptation, and studies of these effects have provided an important tool for understanding how information is represented and calibrated within the visual system. The current project applied visual adaptation to understand the perception of time and events. Participants viewed scenes that change at different rates over time and then judged the apparent rate or duration of events. Our stimuli consisted of a scene of flickering squares, the mean rate of flicker was normally distributed to simulate a distribution of event changes that would occur in a more naturalistic scene. Our results revealed that, following adaptation to the scene, the perception of the rate of flicker drastically decreased (Figure 1). We also conducted a secondary experiment to explore whether or not participants could consistently and accurately report the average rate of flicker of the stimuli. This experiment is ongoing, but preliminary results revealed that participants tended to overestimate the average flicker rate of the scene (Figure 2). The research was conducted in collaboration with Dr. Gerrit Maus, a leading expert in interactions between temporal and spatial vision, at the Nanyang Technological University in Singapore. The project is currently ongoing.           Last Modified: 03/27/2017       Submitted by: Katherine E Mussell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
