<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Scalable Probabilistic Inference for Large Knowledge Bases</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>516000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Large Knowledge Bases are constructed today automatically from large corpora of text, like Web pages, journal articles, news stories.  The construction proceeds in two major stages.  First, several database queries are computed on the corpora of text, to extract candidate data items; the resulting data, called a factor graph, can be thought of as a very large, noisy, uncertain, redundant, and inconsistent database. Second, a complex probabilistic inference is performed on the factor graph to produce a large, probabilistic knowledge base.  Both stages are computationally expensive, but only the first stage has benefited so far from advances in database query processing techniques.  This project develops new database processing techniques for the probabilistic inference task.  These new techniques have theoretical guarantees, either in the form of absolute guarantees on the runtime of the probabilistic inference, or in the form of a trade-off between the run time and the precision of the probabilistic inference.&lt;br/&gt;&lt;br/&gt;The main technique pursued by the project is called lifted probabilistic inference, and consists of algorithms that compute the probability of a SQL query inductively on the structure of the query, without having to first ground the query to compute the large factor graph.  Lifted inference is very efficient, but possible only for some queries.  The project has four thrusts.  First, it combines sampling with lifted inference for efficient approximate probabilistic inference for any query; this algorithms can pushed in the database engine, and can therefore benefit immediately from all optimizations available today in modern, parallel query processors.  Second, the project studies the complexity of query evaluation on symmetric databases, a special case of high practical importance, since it scales easily to arbitrarily large domains.  In the third thrust, the project extends lifted inference techniques to queries with negations by combining probabilistic inference with resolution; this is necessary because soft constraints in knowledge bases almost always have negations.  Finally, the project develops a system prototype and benchmarks.</AbstractNarration>
<MinAmdLetterDate>07/14/2016</MinAmdLetterDate>
<MaxAmdLetterDate>07/23/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1614738</AwardID>
<Investigator>
<FirstName>Dan</FirstName>
<LastName>Suciu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dan Suciu</PI_FULL_NAME>
<EmailAddress>suciu@cs.washington.edu</EmailAddress>
<PI_PHON>2066851934</PI_PHON>
<NSF_ID>000218785</NSF_ID>
<StartDate>07/14/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~500000</FUND_OBLG>
<FUND_OBLG>2019~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project has studied fundamental applications of probability theory to data management.<br />Probabilistic databases are used for managing uncertain and dirty data.&nbsp; The records in the database are annotated with a probability score, and the system is responsible for computing the output probability for every query result.&nbsp; Computing these probabilities is, in general, computationally hard.&nbsp; However, a specialized technique called "lifted inference" has been developed, which can compute some (but not all!)&nbsp; queries very efficiently, using almost the same query plans as regular SQL.&nbsp; Many applications require only simple values of the probabilities, for example 1/2 for "maybe", 1 for "certain", 0 for "impossible".&nbsp; The question is whether queries could be evaluated more efficiently over databases where the probability values are restricted to 0, 1/2, 1.&nbsp; This project answer the question definitively, by showing that non-liftable queries remain difficult to compute, even if all probabilities are restricted to 0, 1/2, 1.&nbsp; The project has also studied a new application of lifted inference, to causal inference over large causal graphs.<br />A suite of surprising outcomes concern the use of information theory for query processing.&nbsp; "Information theory" refers to the study of "entropy", a measure of uncertainty of a probability space.&nbsp; The project has identified three very important applications of information theory to data management.&nbsp; First, it developed a new query processing algorithm that is provably optimal.&nbsp; Traditional SQL engines optimize query plans aiming to reduce their cost, but in general cannot find the absolute optimal algorithm to compute a query.&nbsp; Our new algorithm uses a novel approach where the query "plan" consists of an information-inequality, and is provably optimal.&nbsp; Second, the project has developed a framework for reasoning about soft constraints by using information theory.&nbsp; Constraints are vital in databases, for example they may specify that a column in a table represents a key.&nbsp; Today, however, it is common to find datasets that violate natural constraints, for example a person may have two addresses, or a company may be listed with two CEO's.&nbsp; This project has developed a new framework by which one can measure the degree of satisfaction of constraints by using information theory.&nbsp; Finally, the third outcome consists of a surprising connection between query containment under bag semantics and information theory.&nbsp; The query containment problem asks whether the number of records returned by one query is always less than or equal to the number of tuples returned by a second query, on any input databases; if the answer is yes, then an optimizer could consider replacing the second query by the first.&nbsp; We have proven that this problem is equivalent to an information inequality related to the two queries.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/18/2020<br>      Modified by: Dan&nbsp;Suciu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project has studied fundamental applications of probability theory to data management. Probabilistic databases are used for managing uncertain and dirty data.  The records in the database are annotated with a probability score, and the system is responsible for computing the output probability for every query result.  Computing these probabilities is, in general, computationally hard.  However, a specialized technique called "lifted inference" has been developed, which can compute some (but not all!)  queries very efficiently, using almost the same query plans as regular SQL.  Many applications require only simple values of the probabilities, for example 1/2 for "maybe", 1 for "certain", 0 for "impossible".  The question is whether queries could be evaluated more efficiently over databases where the probability values are restricted to 0, 1/2, 1.  This project answer the question definitively, by showing that non-liftable queries remain difficult to compute, even if all probabilities are restricted to 0, 1/2, 1.  The project has also studied a new application of lifted inference, to causal inference over large causal graphs. A suite of surprising outcomes concern the use of information theory for query processing.  "Information theory" refers to the study of "entropy", a measure of uncertainty of a probability space.  The project has identified three very important applications of information theory to data management.  First, it developed a new query processing algorithm that is provably optimal.  Traditional SQL engines optimize query plans aiming to reduce their cost, but in general cannot find the absolute optimal algorithm to compute a query.  Our new algorithm uses a novel approach where the query "plan" consists of an information-inequality, and is provably optimal.  Second, the project has developed a framework for reasoning about soft constraints by using information theory.  Constraints are vital in databases, for example they may specify that a column in a table represents a key.  Today, however, it is common to find datasets that violate natural constraints, for example a person may have two addresses, or a company may be listed with two CEO's.  This project has developed a new framework by which one can measure the degree of satisfaction of constraints by using information theory.  Finally, the third outcome consists of a surprising connection between query containment under bag semantics and information theory.  The query containment problem asks whether the number of records returned by one query is always less than or equal to the number of tuples returned by a second query, on any input databases; if the answer is yes, then an optimizer could consider replacing the second query by the first.  We have proven that this problem is equivalent to an information inequality related to the two queries.          Last Modified: 09/18/2020       Submitted by: Dan Suciu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
