<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Inference for Probabilistic Programs: A Symbolic Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2017</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>174639.00</AwardTotalIntnAmount>
<AwardAmount>174639</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Probabilistic machine learning and artificial intelligence have revolutionized the world and are present in most aspects of our life. However, the tools used to develop probabilistic machine learning solutions are limited in what they can express. Moreover, they require significant expert knowledge, and are not accessible to scientists in each discipline, let alone everybody else. Probabilistic programming aims to make probabilistic machine learning accessible to all, and as easy to program as a phone application. To make this dream a reality, probabilistic program execution, making probabilistic predictions from observations, has to become as highly efficient and robust as our current non-probabilistic software tools. This project develops general-purpose algorithms to execute probabilistic programs efficiently, using advanced symbolic reasoning techniques from artificial intelligence. Moreover, it does so for probabilistic programs that are significantly more complex than the ones in use today, involving a wide range of programming language features that are both discrete and continuous. This increase in scalability and expressive power will foster novel, increasingly advanced machine learning applications. &lt;br/&gt;&lt;br/&gt;More specifically, probabilistic programs subsume classical probabilistic graphical models and are additionally able to capture complex probabilistic dependencies that include arbitrary pieces of executable code. While many expressive probabilistic programming languages have been proposed in recent years, the current bottleneck and barrier to success is the lack of general-purpose reasoning algorithms to perform inference with probabilistic programs efficiently. This research tackles two key problems in probabilistic program inference. First, current sampling-based algorithms have problems reasoning about dependencies between large numbers of discrete random variables and explaining low-probability observations. In one thrust, this project develops new inference algorithms based on knowledge compilation. This technique compiles the program into a symbolic structure that is efficient for probability computation. The algorithm does not compile the entire program, which is generally intractable, but uses importance sampling on partially compiled programs to sample efficient subprograms. This combines the best of approximate program evaluation by sampling with highly efficient compilation techniques for exact inference. Second, symbolic approaches to inference are fundamentally discrete and have problems dealing with continuous and integer variables, which frequently appear in real code. Conversely, algorithms for continuous distributions cannot efficiently handle discrete program structure. In another thrust, this project studies symbolic approaches to probabilistic reasoning in programs with both types of structure, using recent breakthroughs based on satisfiability modulo theories and hashing-based sampling. This project provides a scientific leap at a fundamental level. It also provides a context for training undergraduate and graduate students in subjects spanning machine learning, artificial intelligence, statistics, and programming languages, and targets the integration of probabilistic programming into computer science curricula.</AbstractNarration>
<MinAmdLetterDate>02/21/2017</MinAmdLetterDate>
<MaxAmdLetterDate>02/21/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657613</AwardID>
<Investigator>
<FirstName>Guy</FirstName>
<LastName>Van den Broeck</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guy Van den Broeck</PI_FULL_NAME>
<EmailAddress>guyvdb@cs.ucla.edu</EmailAddress>
<PI_PHON>3107940102</PI_PHON>
<NSF_ID>000711612</NSF_ID>
<StartDate>02/21/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[UCLA Computer Science Dept.]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951596</ZipCode>
<StreetAddress><![CDATA[420 Westwood Plaza, BH 4531E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~174639</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!--   @page { size: 8.5in 11in; margin: 0.79in }   p { margin-bottom: 0.1in; line-height: 115%; background: transparent }   --> <p>Probabilistic machine learning and artificial intelligence have revolutionized the world and are present in most aspects of our life. Yet they are limited in what they can express, and are not accessible to domain experts, let alone everybody else. Probabilistic programming aims to make probabilistic machine learning accessible to all, and as easy to program as a phone application. To make this dream a reality, probabilistic program inference has to become as highly efficient and robust as our current non-probabilistic software compilers.</p> <p>This project has produced several novel algorithms for probabilistic programming. We have focussed our activities on solving probabilistic reasoning tasks that are strictly harder than the typical ones. We sought to do symbolic probabilistic inference for making decisions on top of a complex probability distribution. For example, our algorithm selects medical tests to run in order so make the same diagnosis as if the results of all possible tests were observed. We have also included constraints and rewards in these decision-making objectives, and support for computing maximum expected utilities.</p> <p>This project has produced new techniques to reason about probabilistic programs, specifically through the use of abstractions. In this framework, the concrete program is captured by a small number of properties, called predicates. Using these predicates, we construct an abstract probabilistic program over only these predicates, yielding a simpler probabilistic program to reason about. We prove theoretical relationships between such probabilistic predicate abstractions and concrete probabilistic programs, and illustrate how they are useful for speeding up the execution of such programs. In addition, this project has produced novel approximation algorithms that combine exact symbolic reasoning with approximate reasoning about uncertainty.</p> <p>Another major outcome has been to expand the applications of discrete probabilistic inference by looking a machine learning problems. The project has shown how probabilistic world models, such as probabilistic programs, can be used to improve the performance of pure function-based classifiers that are trained for a specific prediction task. Probabilistic models capture all dependencies between a large number of features of the world, regardless of the final prediction task. We have developed algorithms that show how that large joint distribution can be useful when our goal is a specific task, such as classifying images from the pixels. Specifically, our novel algorithms were shown to be more data efficient, and able to handle missing data at prediction time in ways that outperforms the state of the art.</p> <p>Development of the algorithms and systems described above have provided hands-on training for many of the PhD students involved, as well as several undergraduate and Master students who have built on this code for their individual research projects. The project was also crucial to the curriculum development at UCLA, where several graduate courses were introduced on the topic of this project. The research results have been disseminated as full research papers at top-tier conferences, and as open-source software.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/16/2019<br>      Modified by: Guy&nbsp;Van Den Broeck</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Probabilistic machine learning and artificial intelligence have revolutionized the world and are present in most aspects of our life. Yet they are limited in what they can express, and are not accessible to domain experts, let alone everybody else. Probabilistic programming aims to make probabilistic machine learning accessible to all, and as easy to program as a phone application. To make this dream a reality, probabilistic program inference has to become as highly efficient and robust as our current non-probabilistic software compilers.  This project has produced several novel algorithms for probabilistic programming. We have focussed our activities on solving probabilistic reasoning tasks that are strictly harder than the typical ones. We sought to do symbolic probabilistic inference for making decisions on top of a complex probability distribution. For example, our algorithm selects medical tests to run in order so make the same diagnosis as if the results of all possible tests were observed. We have also included constraints and rewards in these decision-making objectives, and support for computing maximum expected utilities.  This project has produced new techniques to reason about probabilistic programs, specifically through the use of abstractions. In this framework, the concrete program is captured by a small number of properties, called predicates. Using these predicates, we construct an abstract probabilistic program over only these predicates, yielding a simpler probabilistic program to reason about. We prove theoretical relationships between such probabilistic predicate abstractions and concrete probabilistic programs, and illustrate how they are useful for speeding up the execution of such programs. In addition, this project has produced novel approximation algorithms that combine exact symbolic reasoning with approximate reasoning about uncertainty.  Another major outcome has been to expand the applications of discrete probabilistic inference by looking a machine learning problems. The project has shown how probabilistic world models, such as probabilistic programs, can be used to improve the performance of pure function-based classifiers that are trained for a specific prediction task. Probabilistic models capture all dependencies between a large number of features of the world, regardless of the final prediction task. We have developed algorithms that show how that large joint distribution can be useful when our goal is a specific task, such as classifying images from the pixels. Specifically, our novel algorithms were shown to be more data efficient, and able to handle missing data at prediction time in ways that outperforms the state of the art.  Development of the algorithms and systems described above have provided hands-on training for many of the PhD students involved, as well as several undergraduate and Master students who have built on this code for their individual research projects. The project was also crucial to the curriculum development at UCLA, where several graduate courses were introduced on the topic of this project. The research results have been disseminated as full research papers at top-tier conferences, and as open-source software.             Last Modified: 07/16/2019       Submitted by: Guy Van Den Broeck]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
