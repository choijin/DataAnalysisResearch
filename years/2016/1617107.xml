<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Collaborative Research: Algorithms for Query by Example of Audio Databases</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>299775.00</AwardTotalIntnAmount>
<AwardAmount>311895</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Finding ways to automatically index, label, and access multimedia content (such as audio documents) is increasing in importance as multimedia repositories proliferate and grow. The community-generated SoundCloud repository is one example. It contains recordings of bands, sound effects, podcasts, etc., and contributors upload 12 hours of audio every minute. Repositories like SoundCloud typically tag audio at the file level with short text labels. Text-based search for a desired recording using these labels can be problematic. Text-based search within a track is not possible, since they are not indexed with tags in the body of the file. In this project, investigators at the University of Rochester and Northwestern University aim to develop methods and a system for audio search via query-by-example, where the example is similar, in some key way, to the desired audio in the database, but is not an exact match. This will allow search within files, bypassing the need for text-based tagging. This project will be focusing on using vocal imitations as search keys because they are natural for humans and are widely used in interaction. It will develop a novel search engine for sounds that takes vocal imitations as queries (e.g., imitation of a bird call to find recordings of the bird call). The technology developed for this novel way to search through audio/video collections will also benefit society in numerous other ways, such as crime surveillance (e.g., automated gunshot or scream detection for policy monitoring stations), biodiversity measurement (e.g., automatic ID of bird species that sound "like this" in field recordings), environmental awareness for the hearing impaired (e.g., alert me when my dog is the one barking), a production aid for a movie sound designer (e.g., finding door slam sounds in a database of thousands of sound effects), and sound-based diagnosis (e.g., "your car needs a new starter motor"). The project will benefit science technology engineering and mathematics (STEM) education as audio-based research has been shown to be a successful way to attract diverse college students into STEM disciplines.&lt;br/&gt;&lt;br/&gt;Vocal imitation conveys rich information covering many acoustic aspects: pitch, loudness, timbre, their temporal evolutions, and rhythmic patterns, etc. This lets a user query for precise sounds that are difficult to search for with text tags. For the same reason, however, vocal imitations may vary from the desired target on many dimensions. The query sound can also lies in a very constrained sound space compared to the sounds to be retrieved, due to the physical constraints of the human vocal system. Building a successful query-by-vocal-imitation system will require research into methods for representing audio and retrieving audio based on queries that are similar to target sounds only on a subset of their measurable dimensions. It will also require interfaces that facilitate providing queries and refining search results in a non-text-based context. For the former, the investigators will research on methods for learning of aspect-specific audio representations using deep neural networks. The investigators will also develop matching algorithms suitable for these representations. The investigators will design novel search interfaces that let users iteratively refine their search results. The system will learn from the interactions and adjust the weightings of different acoustic aspects to search for the wanted sound. Expected outcomes of this research are: (1) audio representations that highlight perceptually relevant features of vocal queries for matching to general audio target sounds; (2) algorithms for matching and aligning vocal queries to general audio; (3) interaction methods for iteratively refining search results using vocal imitations and sound examples; (4) a large vocal imitation and sound dataset; and (5) an open-source sound retrieval system that embodies these outcomes. More information about this project can be found at the project web site (http://www.ece.rochester.edu/projects/air/projects/audiosearch).</AbstractNarration>
<MinAmdLetterDate>08/26/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/28/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617107</AwardID>
<Investigator>
<FirstName>Zhiyao</FirstName>
<LastName>Duan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhiyao Duan</PI_FULL_NAME>
<EmailAddress>zhiyao.duan@rochester.edu</EmailAddress>
<PI_PHON>5852755302</PI_PHON>
<NSF_ID>000662423</NSF_ID>
<StartDate>08/26/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041294109</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ROCHESTER</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041294109</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Rochester]]></Name>
<CityName>Rochester</CityName>
<StateCode>NY</StateCode>
<ZipCode>146270162</ZipCode>
<StreetAddress><![CDATA[308 Hopeman]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~299775</FUND_OBLG>
<FUND_OBLG>2017~12120</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-cd92d4a8-7fff-48d0-2c57-93719b961a74"> </span></p> <p dir="ltr"><span>Traditional text-based search engines have profoundly improved how people access information online, letting people directly find relevant documents, based on their content, and find the appropriate place in lengthy documents the same way. Today, the audio in online repositories cannot be searched the same way. Often, audio files have a text description (e.g., "30 minutes of street scene, Chicago Aug 24, 2012") that does not describe the details of the audio content. This makes desired content (?is there a dog barking in this file??) undiscoverable through text-based search. Many files labeled with content-relevant tags often have non-specific tags that result in searches that return hundreds or thousands of examples (e.g., "Barking dogs" returns 1134 files on </span><a href="http://www.freesound.org/"><span>www.freesound.org</span></a><span>). Even when a specific file is known to contain the desired content, it is typically not indexed with tags throughout the body of the file, forcing the user to listen to the full file to find the desired portion of the audio ("This file is an hour long! I don?t have time to listen through all of it.").</span></p> <p dir="ltr"><span>The work funded by this grant makes general audio repositories content-searchable using vocal imitation of the desired sound as the query key:</span><span> A user vocalizes the audio concept in mind and the system retrieves audio recordings that are similar, in some way, to the vocalization. This can complement text-based search to reduce the search space, making search through large audio collections easier and quicker when text tags are available, and making search possible even in cases where text tags are not available or there are no broadly-agreed-upon labels for a sound.&nbsp;</span></p> <p dir="ltr"><span>We focus on vocal imitation because it is natural for humans and widely used in interaction. Humans use it to convey sounds that are difficult to describe in language and to improve the vividness of storytelling.&nbsp; Researchers have shown humans can often more easily identify a target sound from a vocal imitation than from a verbal description. This is because vocal imitation conveys rich information covering many acoustic aspects: pitch, loudness, timbre, their temporal evolutions, and rhythmic patterns, etc. This lets a user query for precise sounds that are difficult to search for with text tags (e.g., an imitation of the exact sound of a sputtering car vs. ?car engine noise?).</span></p> <p dir="ltr"><span>In the course of this research we have made a number of new advances that enable query and search through vocal imitation:</span></p> <p dir="ltr">1. New audio representations that highlight perceptually relevant features of audio</p> <p dir="ltr">2. New algorithms for matching and aligning vocal queries to general audio</p> <p dir="ltr">3. New user interaction methods to allow search using vocal imitations and query-by-example</p> <p dir="ltr">4. New approaches to training machine learning models to do sound object identification</p> <p dir="ltr">5. A large vocal imitation and sound dataset, available on Zenodo.org</p> <p dir="ltr">6. A database of example car engine sounds that allows query-by-imitation car problem diagnosis</p> <p dir="ltr">7. A database of general sounds with a hierarchical organization and text labels for building search engines</p> <p dir="ltr">8. Two open-source sound retrieval systems (Vroom and Voogle) that embody these outcomes. Source code and demonstration versions of these systems are available on-line</p> <p dir="ltr">9. Large-scale subjective evaluation results comparing vocal-imitation-based search with traditional text-based search.&nbsp;</p> <p dir="ltr"><span>The intellectual merit of this work is clear. Algorithms for audio search and retrieval are of great interest to the multimedia information retrieval community. This research is also of intellectual merit to a variety of fields including signal processing (new audio representations), human-computer interaction. (new interfaces), and artificial intelligence (systems that can parse and understand the perceptual world). Research in speech recognition, audio perception and computational auditory scene analysis have been advanced through this work.&nbsp;</span></p> <p dir="ltr"><span>The broader impacts of this work are numerous. Example-based search through audio is a key enabling technology for search through audio/video collections where it is impractical to hand-label the content of the data with text tags. This is useful in many contexts: surveillance (automated gunshot or scream detection for policy monitoring stations), biodiversity measurement (e.g., automatic ID of bird species that sound ?like this? in field recordings), environmental awareness for the hearing impaired (alert me when my dog is the one barking), a production aid for a movie sound designer (finding door slam sounds in a database of thousands of sound effects).&nbsp; Another application area is diagnosis from audio examples. We have developed a database of typical auto sounds and a matching search engine that one can search by recording the sound of a failing car (e.g., a bad starter motor), which is automatically matched to a sound with a known diagnosis (e.g., bad starter motor) in our database. All technologies that facilitate search and retrieval through sound examples are useful empowering technology for the visually impaired, as they allow for interfaces that focus on sound as the interaction modality.&nbsp;</span></p><br> <p>            Last Modified: 11/07/2020<br>      Modified by: Zhiyao&nbsp;Duan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Traditional text-based search engines have profoundly improved how people access information online, letting people directly find relevant documents, based on their content, and find the appropriate place in lengthy documents the same way. Today, the audio in online repositories cannot be searched the same way. Often, audio files have a text description (e.g., "30 minutes of street scene, Chicago Aug 24, 2012") that does not describe the details of the audio content. This makes desired content (?is there a dog barking in this file??) undiscoverable through text-based search. Many files labeled with content-relevant tags often have non-specific tags that result in searches that return hundreds or thousands of examples (e.g., "Barking dogs" returns 1134 files on www.freesound.org). Even when a specific file is known to contain the desired content, it is typically not indexed with tags throughout the body of the file, forcing the user to listen to the full file to find the desired portion of the audio ("This file is an hour long! I don?t have time to listen through all of it."). The work funded by this grant makes general audio repositories content-searchable using vocal imitation of the desired sound as the query key: A user vocalizes the audio concept in mind and the system retrieves audio recordings that are similar, in some way, to the vocalization. This can complement text-based search to reduce the search space, making search through large audio collections easier and quicker when text tags are available, and making search possible even in cases where text tags are not available or there are no broadly-agreed-upon labels for a sound.  We focus on vocal imitation because it is natural for humans and widely used in interaction. Humans use it to convey sounds that are difficult to describe in language and to improve the vividness of storytelling.  Researchers have shown humans can often more easily identify a target sound from a vocal imitation than from a verbal description. This is because vocal imitation conveys rich information covering many acoustic aspects: pitch, loudness, timbre, their temporal evolutions, and rhythmic patterns, etc. This lets a user query for precise sounds that are difficult to search for with text tags (e.g., an imitation of the exact sound of a sputtering car vs. ?car engine noise?). In the course of this research we have made a number of new advances that enable query and search through vocal imitation: 1. New audio representations that highlight perceptually relevant features of audio 2. New algorithms for matching and aligning vocal queries to general audio 3. New user interaction methods to allow search using vocal imitations and query-by-example 4. New approaches to training machine learning models to do sound object identification 5. A large vocal imitation and sound dataset, available on Zenodo.org 6. A database of example car engine sounds that allows query-by-imitation car problem diagnosis 7. A database of general sounds with a hierarchical organization and text labels for building search engines 8. Two open-source sound retrieval systems (Vroom and Voogle) that embody these outcomes. Source code and demonstration versions of these systems are available on-line 9. Large-scale subjective evaluation results comparing vocal-imitation-based search with traditional text-based search.  The intellectual merit of this work is clear. Algorithms for audio search and retrieval are of great interest to the multimedia information retrieval community. This research is also of intellectual merit to a variety of fields including signal processing (new audio representations), human-computer interaction. (new interfaces), and artificial intelligence (systems that can parse and understand the perceptual world). Research in speech recognition, audio perception and computational auditory scene analysis have been advanced through this work.  The broader impacts of this work are numerous. Example-based search through audio is a key enabling technology for search through audio/video collections where it is impractical to hand-label the content of the data with text tags. This is useful in many contexts: surveillance (automated gunshot or scream detection for policy monitoring stations), biodiversity measurement (e.g., automatic ID of bird species that sound ?like this? in field recordings), environmental awareness for the hearing impaired (alert me when my dog is the one barking), a production aid for a movie sound designer (finding door slam sounds in a database of thousands of sound effects).  Another application area is diagnosis from audio examples. We have developed a database of typical auto sounds and a matching search engine that one can search by recording the sound of a failing car (e.g., a bad starter motor), which is automatically matched to a sound with a known diagnosis (e.g., bad starter motor) in our database. All technologies that facilitate search and retrieval through sound examples are useful empowering technology for the visually impaired, as they allow for interfaces that focus on sound as the interaction modality.        Last Modified: 11/07/2020       Submitted by: Zhiyao Duan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
