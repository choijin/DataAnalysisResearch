<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: Incremental and Asynchronous Projective Splitting Methods for Mathematical Programming</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>457072.00</AwardTotalIntnAmount>
<AwardAmount>457072</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
<PO_EMAI>fergun@nsf.gov</PO_EMAI>
<PO_PHON>7032922216</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A key to solving large computational and mathematical problems, such as analyzing large datasets or planning for the operation of an electrical power grid or any other complicated systems with an uncertain future demands and supplies, is decomposing into smaller solvable subproblems or subsystems, then coordinating and integrating their results, and decomposing again into adjusted subproblems.  Properly designed decomposition methods repeat a decomposition - coordination cycle that converge to the solution of the entire original, non-decomposed problem.  The PI is working with Sandia National Laboratories and has particular interest in problems that arise in operating electrical power grids with high penetration of renewable generation sources, like solar and wind, where weather has unplanned affects the supply.   &lt;br/&gt;&lt;br/&gt;This project studies a new way to perform decomposition, called "incremental projective operator splitting" (IPOS) or "block-iterative splitting."  It is related to a popular decomposition method called the alternating direction method of multipliers (ADMM) but is far more flexible.  While essentially all prior decomposition methods follow a rigid cycle of decomposition and coordination steps, with every decomposition step encompassing all the subsystems, the new method has much greater flexibility:  only a subset of subsystems need to be considered between coordination steps, and decomposition and coordination calculations can overlap asynchronously.  This flexibility should allow more efficient use of parallel computers by eliminating rigid synchronization points.  This property is important because most future growth in computer performance is anticipated to result from larger numbers of parallel processing units, and only parallel computers will be able to manipulate the large datasets and decision problems we hope to analyze. &lt;br/&gt;&lt;br/&gt;Because the new IPOS methods are so flexible, there are numerous ways in which they could be used on the same class of problems.  The main goal of this project is to develop and experimentally evaluate strategies for applying IPOS on parallel computers.  It will focus on two common problem classes, large-scale data analysis and planning under uncertainty, using real-world input data to the maximum practical extent.  Other research topics include sharpening the mathematical theory of IPOS, and extending this theory to cover a broader range of problems, and development and release of software based on this new theory.</AbstractNarration>
<MinAmdLetterDate>08/01/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617617</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Eckstein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr</PI_SUFX_NAME>
<PI_FULL_NAME>Jonathan Eckstein</PI_FULL_NAME>
<EmailAddress>jeckstei@rci.rutgers.edu</EmailAddress>
<PI_PHON>8484450510</PI_PHON>
<NSF_ID>000195522</NSF_ID>
<StartDate>08/01/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University Newark</Name>
<CityName>Newark</CityName>
<ZipCode>071021896</ZipCode>
<PhoneNumber>9739720283</PhoneNumber>
<StreetAddress>Blumenthal Hall, Suite 206</StreetAddress>
<StreetAddress2><![CDATA[249 University Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>130029205</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers Business School, New Brunswick Building]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088548081</ZipCode>
<StreetAddress><![CDATA[100 Rockafeller Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramReference>
<ProgramReference>
<Code>7934</Code>
<Text>PARAL/DISTRIBUTED ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~457072</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed multiple new versions of the project splitting method for large-scale convex optimization and related problems.&nbsp; Such methods have application in machine learning and data analysis, among other areas.&nbsp; &ldquo;Splitting&rdquo; algorithms for large-scale optimization are constructed out of building blocks that process relatively small subsystems of the entire problem to be solved; the two fundamental ways of processing a subsystem are &ldquo;forward&rdquo; (gradient) and &ldquo;backward&rdquo; (implicit or proximal) steps.&nbsp; At the time of the project proposal, projective splitting methods were only known to work with backward steps, although these step could be computed approximately.&nbsp; Backward steps are generally more time consuming to compute than forward ones.&nbsp; Our first main result was that projective splitting methods, in essentially their full generality, can be made to work with forward steps.&nbsp; Various different configurations of the forward steps are possible, so we published a several papers covering different variations.&nbsp; We also proved that the theoretical convergence rate of our approach is comparable to other splitting methods.&nbsp; Most of our papers contain computational tests on difficult large-scale data fitting problems using real datasets, showing that our methods can be superior to or competitive in practice with other splitting methods.&nbsp; We also released open-source Python software that makes it possible to apply our methods to new kinds of problems without having to re-implement their basic logic.</p><br> <p>            Last Modified: 09/30/2020<br>      Modified by: Jonathan&nbsp;Eckstein</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed multiple new versions of the project splitting method for large-scale convex optimization and related problems.  Such methods have application in machine learning and data analysis, among other areas.  "Splitting" algorithms for large-scale optimization are constructed out of building blocks that process relatively small subsystems of the entire problem to be solved; the two fundamental ways of processing a subsystem are "forward" (gradient) and "backward" (implicit or proximal) steps.  At the time of the project proposal, projective splitting methods were only known to work with backward steps, although these step could be computed approximately.  Backward steps are generally more time consuming to compute than forward ones.  Our first main result was that projective splitting methods, in essentially their full generality, can be made to work with forward steps.  Various different configurations of the forward steps are possible, so we published a several papers covering different variations.  We also proved that the theoretical convergence rate of our approach is comparable to other splitting methods.  Most of our papers contain computational tests on difficult large-scale data fitting problems using real datasets, showing that our methods can be superior to or competitive in practice with other splitting methods.  We also released open-source Python software that makes it possible to apply our methods to new kinds of problems without having to re-implement their basic logic.       Last Modified: 09/30/2020       Submitted by: Jonathan Eckstein]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
