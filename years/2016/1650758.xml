<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:   Workload Analysis of Blue Waters</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
<AwardExpirationDate>01/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Edward Walker</SignBlockName>
<PO_EMAI>edwalker@nsf.gov</PO_EMAI>
<PO_PHON>7032924863</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Blue Waters is the largest NSF-supported leadership-class supercomputer.  It was acquired and is operated by the University of Illinois.   Its purpose is to greatly advance fundamental understanding represented in "grand challenge" problems within and across a wide range of science and engineering.  It does this by offering a computing system and environment suitable for and capable of solving the most challenging computational and data analysis problems.  Given the important and unique role that Blue Waters plays in the nation's research portfolio, it is important to have a detailed technical understanding of its scientific workload. This award to State University of New York (SUNY) at Buffalo undertakes such a study.  This workload characterization will guide performance optimization at the software and system configuration level to maximize job performance and workflow, as well as to help inform future computer architecture research and development.  Additionally, the analysis could concretely inform the system balance trade-offs of future leadership-class system deployments.  This study will leverage  prior Blue Waters studies and data, but will incorporate a comprehensive approach by extending the NSF-funded and publicly available XDMoD (XD Metrics on Demand) service to include data specific to Blue Waters architecture.&lt;br/&gt;&lt;br/&gt;The results of this study will not only provide detailed operational and performance analytics for Blue Waters, but will also be used as a template for similar studies carried out on other advanced high performance computing systems that are revolutionizing computational science. This "transfer of knowledge" will be facilitated through the use of Open XDMoD for the proposed analysis, which is already in wide use by HPC centers worldwide. In addition, one of the outcomes of this  project will be the modification of Open XDMoD to ingest job level performance data from the OVIS/LDMS monitoring framework. OVIS/LDMS is widely deployed on Cray HPC systems and this outcome will enable those centers with Cray systems to fully leverage Open XDMoD to provide comprehensive resource management.</AbstractNarration>
<MinAmdLetterDate>08/05/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1650758</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>DeLeon</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert L DeLeon</PI_FULL_NAME>
<EmailAddress>rldeleon@buffalo.edu</EmailAddress>
<PI_PHON>7166454167</PI_PHON>
<NSF_ID>000163680</NSF_ID>
<StartDate>08/05/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Furlani</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas R Furlani</PI_FULL_NAME>
<EmailAddress>furlani@buffalo.edu</EmailAddress>
<PI_PHON>7168818939</PI_PHON>
<NSF_ID>000146269</NSF_ID>
<StartDate>08/05/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>William</FirstName>
<LastName>Kramer</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William T Kramer</PI_FULL_NAME>
<EmailAddress>wtkramer@illinois.edu</EmailAddress>
<PI_PHON>2173336260</PI_PHON>
<NSF_ID>000179975</NSF_ID>
<StartDate>08/05/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Jones</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew D Jones</PI_FULL_NAME>
<EmailAddress>jonesm@ccr.buffalo.edu</EmailAddress>
<PI_PHON>7168818958</PI_PHON>
<NSF_ID>000170376</NSF_ID>
<StartDate>08/05/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>White</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph P White</PI_FULL_NAME>
<EmailAddress>jpwhite4@buffalo.edu</EmailAddress>
<PI_PHON>7166452634</PI_PHON>
<NSF_ID>000727254</NSF_ID>
<StartDate>08/05/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Buffalo</Name>
<CityName>Buffalo</CityName>
<ZipCode>142282567</ZipCode>
<PhoneNumber>7166452634</PhoneNumber>
<StreetAddress>520 Lee Entrance</StreetAddress>
<StreetAddress2><![CDATA[Suite 211]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY26</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>038633251</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Buffalo]]></Name>
<CityName>Buffalo</CityName>
<StateCode>NY</StateCode>
<ZipCode>142031101</ZipCode>
<StreetAddress><![CDATA[701 Ellicott St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY26</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7781</Code>
<Text>Leadership-Class Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Bkue Waters workload analysis sought to understand a wide variety of use patterns and performance requirements for the jobs running on Blue Waters, including differences that may exist between the computational needs of the various fields of science such as average job size, use of accelerator technology, I/O, communication, and memory use.&nbsp; Historical trends for almost the entire operational life of Blue Waters to date were also studied to determine if these requirements are changing over time since this may impact future architecture designs.&nbsp; File system performance, the use of numerical libraries, and the types of parallelism employed by users (MPI, threads, etc.) were also studied.&nbsp;&nbsp;&nbsp; Highlights of the analysis follow.&nbsp;</p> <p><strong>Science and Engineering fields and Application Codes</strong></p> <ul> <li>The overall historical trend for all NSF directorates is toward increasing use of Blue Waters, albeit at much different overall scales of utilization.</li> <li>The MPS and Biological Sciences directorates are the leading consumers of node hours, typically accounting for more than 2/3 of all node hours used.&nbsp;</li> <li>The number of fields of science represented in the Blue Waters portfolio has increased in each year of its operation &ndash; more than doubling since its first year of operation.</li> <li>The applications run on Blue Waters represent an increasingly diverse mix of disciplines, ranging from broad use of community codes to more specific scientific sub-disciplines.</li> <li>The top 10 applications consume about 2/3 of all node hours.</li> <li>Common algorithms, as characterized by Colella&rsquo;s original seven dwarfs, are roughly equally represented within the applications run on Blue Waters aside from unstructured grids and Monte Carlo methods, which exhibit a much smaller fraction.</li> <li>Optimized numerical library usage, essential for high performance, has been used throughout in areas in which such libraries are applicable.&nbsp;</li> </ul> <p><strong>Concurrency and Parallelism</strong></p> <ul> <li>Blue Waters supports a diverse mix of job sizes from single node jobs to jobs that use in excess of 20,000 nodes in a single application. The patterns of usage differ between the XE and XK (GPU) nodes.&nbsp;</li> <li>Single node jobs, some of which may be attributable to high throughput computing, represent a small fraction (less than 2%) of the total node hours consumed on Blue Waters.</li> <li>For XE node jobs, all of the major science areas (&gt; 1 million node hours) run a mix of job sizes and all have very large jobs (&gt; 4096 nodes). The relative proportions of job size vary between different parent science areas. &nbsp;The job size distribution weighted by node hours consumed peaks at 1025 &ndash; 2048 for XE jobs.&nbsp;&nbsp; The largest 3% of the jobs (by node hours) account for 90% of the total node-hours consumed.</li> <li>The majority of XE node hours on the machine are spent running parallel jobs that use some form of message passing for inter-process communication. At least 25% of the workload uses some form of threading, however the larger jobs (&gt; 4096 nodes) mostly use message passing with no threading.&nbsp; There is no obvious trend in the variation of thread usage over time, however, thread usage information is only available for a short time period.</li> <li>For the XK (GPU) nodes, the parent sciences Molecular Biosciences, Chemistry and Physics are the largest users with NAMD and AMBER the two most prevalent applications. The job size distribution weighted by node hours consumed peaks at 65 &ndash; 128 nodes for the XK jobs. &nbsp;&nbsp;Similarly to the XE nodes, the largest 7% of the jobs (by node-hour) account for 90% of the node-hours consumed on the XK nodes.</li> <li>The aggregate GPU utilization (efficiency) varies significantly by application.&nbsp;</li> </ul> <p><strong>Memory Usage</strong></p> <ul> <li>Most jobs that run on the XE nodes use less than 50% of the memory available on the node.&nbsp; However, the distribution of memory use has a substantial tail to higher memory usage. &nbsp;</li> <li>Most jobs that run on the XK nodes use less than 25% of the available memory in the node with a short tail to higher usage.&nbsp;</li> <li>GPU memory usage is very small with few jobs using more than 1GB per GPU.&nbsp;</li> <li>The XE and XK nodes show no historical differences in memory use from year to year.&nbsp;</li> <li>For almost all applications and parent fields of science, memory usage has not changed over time.</li> </ul> <p><strong>Storage and I/O</strong></p> <ul> <li>On average Blue Waters&rsquo; three filesystems have a balanced reads/writes ratio with large fluctuations. The volume of traffic on the largest filesystem peaks at 10PB per month.</li> <li>Users&rsquo; jobs exhibit a wide range of I/O patterns.&nbsp;</li> <li>Overall there is a tendency to use a very large number of small files, which is a challenge for many parallel file systems.</li> <li>Read and write rates stay significantly below possible peak filesystem performance.&nbsp;</li> <li>User jobs spend a very small fraction of time in filesystem I/O operations</li> <li>Many jobs utilize specialized libraries for their I/O operations&nbsp;</li> </ul><br> <p>            Last Modified: 03/03/2017<br>      Modified by: Thomas&nbsp;R&nbsp;Furlani</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Bkue Waters workload analysis sought to understand a wide variety of use patterns and performance requirements for the jobs running on Blue Waters, including differences that may exist between the computational needs of the various fields of science such as average job size, use of accelerator technology, I/O, communication, and memory use.  Historical trends for almost the entire operational life of Blue Waters to date were also studied to determine if these requirements are changing over time since this may impact future architecture designs.  File system performance, the use of numerical libraries, and the types of parallelism employed by users (MPI, threads, etc.) were also studied.    Highlights of the analysis follow.   Science and Engineering fields and Application Codes  The overall historical trend for all NSF directorates is toward increasing use of Blue Waters, albeit at much different overall scales of utilization. The MPS and Biological Sciences directorates are the leading consumers of node hours, typically accounting for more than 2/3 of all node hours used.  The number of fields of science represented in the Blue Waters portfolio has increased in each year of its operation &ndash; more than doubling since its first year of operation. The applications run on Blue Waters represent an increasingly diverse mix of disciplines, ranging from broad use of community codes to more specific scientific sub-disciplines. The top 10 applications consume about 2/3 of all node hours. Common algorithms, as characterized by Colella?s original seven dwarfs, are roughly equally represented within the applications run on Blue Waters aside from unstructured grids and Monte Carlo methods, which exhibit a much smaller fraction. Optimized numerical library usage, essential for high performance, has been used throughout in areas in which such libraries are applicable.    Concurrency and Parallelism  Blue Waters supports a diverse mix of job sizes from single node jobs to jobs that use in excess of 20,000 nodes in a single application. The patterns of usage differ between the XE and XK (GPU) nodes.  Single node jobs, some of which may be attributable to high throughput computing, represent a small fraction (less than 2%) of the total node hours consumed on Blue Waters. For XE node jobs, all of the major science areas (&gt; 1 million node hours) run a mix of job sizes and all have very large jobs (&gt; 4096 nodes). The relative proportions of job size vary between different parent science areas.  The job size distribution weighted by node hours consumed peaks at 1025 &ndash; 2048 for XE jobs.   The largest 3% of the jobs (by node hours) account for 90% of the total node-hours consumed. The majority of XE node hours on the machine are spent running parallel jobs that use some form of message passing for inter-process communication. At least 25% of the workload uses some form of threading, however the larger jobs (&gt; 4096 nodes) mostly use message passing with no threading.  There is no obvious trend in the variation of thread usage over time, however, thread usage information is only available for a short time period. For the XK (GPU) nodes, the parent sciences Molecular Biosciences, Chemistry and Physics are the largest users with NAMD and AMBER the two most prevalent applications. The job size distribution weighted by node hours consumed peaks at 65 &ndash; 128 nodes for the XK jobs.   Similarly to the XE nodes, the largest 7% of the jobs (by node-hour) account for 90% of the node-hours consumed on the XK nodes. The aggregate GPU utilization (efficiency) varies significantly by application.    Memory Usage  Most jobs that run on the XE nodes use less than 50% of the memory available on the node.  However, the distribution of memory use has a substantial tail to higher memory usage.   Most jobs that run on the XK nodes use less than 25% of the available memory in the node with a short tail to higher usage.  GPU memory usage is very small with few jobs using more than 1GB per GPU.  The XE and XK nodes show no historical differences in memory use from year to year.  For almost all applications and parent fields of science, memory usage has not changed over time.   Storage and I/O  On average Blue Waters? three filesystems have a balanced reads/writes ratio with large fluctuations. The volume of traffic on the largest filesystem peaks at 10PB per month. Users? jobs exhibit a wide range of I/O patterns.  Overall there is a tendency to use a very large number of small files, which is a challenge for many parallel file systems. Read and write rates stay significantly below possible peak filesystem performance.  User jobs spend a very small fraction of time in filesystem I/O operations Many jobs utilize specialized libraries for their I/O operations         Last Modified: 03/03/2017       Submitted by: Thomas R Furlani]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
