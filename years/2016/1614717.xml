<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Enabling Deep Neural Networks for Mobile-Cloud Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>427037.00</AwardTotalIntnAmount>
<AwardAmount>427037</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Over the past three years, Deep Neural Networks (DNNs) have become the dominant approach to solving a variety of important problems in computing. This includes problems in speech recognition, machine translation, handwriting recognition and many computer vision problems like face, object, and scene recognition. Although they are renowned for their excellent recognition performance, DNNs are also known to be computationally intensive: networks commonly used for speech, visual and language understanding tasks routinely consume hundreds of MB of memory and Gflops of computing power, typically the province of server-class computers. However, the relevance of the above applications to the mobile setting and the potential for developing new applications provides a strong case for executing DNNs on mobile devices.&lt;br/&gt;&lt;br/&gt;This project is to build an execution framework for deep-neural networks on mobile-cloud platforms so as to enable a broad class of emerging applications such as continuous mobile vision.  In particular, this work will look at enabling a large suite of DNN-based face, scene and object processing algorithms based on applying DNNs to video streams from wearable devices.  This framework, given an arbitrary DNN, will compile it down to a resource-efficient variant at modest loss in accuracy.  The project plans include developing novel techniques to specialize DNNs to contexts and to share resources across multiple simultaneously executing DNNs. Finally, it will create a run-time system for managing the optimized models generated. Using the challenging continuous mobile vision domain as a case-study, the plan is to demonstrate that these techniques yield very significant reductions in DNN resource usage, including orders of magnitude reduction in memory use and instructions executed, in common mobile settings.</AbstractNarration>
<MinAmdLetterDate>08/11/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/11/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1614717</AwardID>
<Investigator>
<FirstName>Arvind</FirstName>
<LastName>Krishnamurthy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arvind Krishnamurthy</PI_FULL_NAME>
<EmailAddress>arvind@cs.washington.edu</EmailAddress>
<PI_PHON>2066160957</PI_PHON>
<NSF_ID>000488256</NSF_ID>
<StartDate>08/11/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~427037</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">&nbsp;</p> <p class="p5">We anticipate a world where mobile devices continuously and routinely observe the state of the world and act on their owner'sbehalf. Computer vision is one of the richest sources of such observations, but the amount of data and computation it requiresis challenging given the limited resources of mobile systems. One solution is to split the load between mobile device and cloud. Recent work has examined this solution making few assumptions on the nature of the computations involved and has shown great promise in offloading the core vision computations to the cloud. There are however inherent limits to the performance obtained by these frameworks, much of which could be traced to the fact that these are general-purpose frameworks designed to address a generic class of mobile-cloud applications.</p> <p class="p3">In this project, we re-examine this problem space in light of advances in continuous sensing algorithms. In particular, we embrace the convergence of the vision community on convolutional neural networks or deep neural networks as the standard achievements prototype targets a rack-scale architecture, where a single switch centrally aggregates parameter updates from serviced workers. Though the single switch limits scalability, we note that commercially-available programmable switches can service up to 64 nodes at 100 Gbps or 256 at 25 Gbps. As each worker is typically equipped with multiple GPUs, this scale is sufficiently large to push the statistical limits of SGD.&nbsp; We show that SwitchML's in-network aggregation yields end-to-end improvements in training performance of up to 5.5x for popular DNN models. Focusing on a communication microbenchmark, compared to the best-in-class collective library NCCL, SwitchML is up to 2.9x faster than NCCL with RDMA and 9.1x than NCCL with TCP.&nbsp; While the magnitude of the performance improvements is dependent on the neural network architecture and the underlying physical network speed, it is greater for models with smaller compute-to-communication ratios -- good news for future, faster DNN training accelerators. Our approach is not tied to any particular ML framework; we have integrated SwitchML with Horovod, which supports several popular toolkits like TensorFlow and PyTorch.</p> <p class="p5">The students involved in the project are primarily graduate students who are getting experience in all aspects of research ? ranging from problem definition, devising solutions, implementing systems, and writing technical papers. Many of them have also started giving public talks at conferences to help disseminate their work. They are also closely working with our industry partners to better understand the hardware innovations that are happening and to perform tech transfer back to the industry.&nbsp; One of them has transitioned to industry and is doing DNN-based research at Amazon. Another recently graduated and is working with the machine learning team at Facebook.</p> <p class="p3">One of the projects that came out of this work is the TVM compiler. TVM, is an end-to-end compilation framework for deep learning systems that provides automated techniques for optimizing code generation on a variety of hardware platforms. TVM is now driven by an open source community involving multiple industry and academic institutions. It has been adopted by companies such as Amazon, Facebook, and Intel.</p> <p class="p5">&nbsp;</p><br> <p>            Last Modified: 09/25/2020<br>      Modified by: Arvind&nbsp;Krishnamurthy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  We anticipate a world where mobile devices continuously and routinely observe the state of the world and act on their owner'sbehalf. Computer vision is one of the richest sources of such observations, but the amount of data and computation it requiresis challenging given the limited resources of mobile systems. One solution is to split the load between mobile device and cloud. Recent work has examined this solution making few assumptions on the nature of the computations involved and has shown great promise in offloading the core vision computations to the cloud. There are however inherent limits to the performance obtained by these frameworks, much of which could be traced to the fact that these are general-purpose frameworks designed to address a generic class of mobile-cloud applications. In this project, we re-examine this problem space in light of advances in continuous sensing algorithms. In particular, we embrace the convergence of the vision community on convolutional neural networks or deep neural networks as the standard achievements prototype targets a rack-scale architecture, where a single switch centrally aggregates parameter updates from serviced workers. Though the single switch limits scalability, we note that commercially-available programmable switches can service up to 64 nodes at 100 Gbps or 256 at 25 Gbps. As each worker is typically equipped with multiple GPUs, this scale is sufficiently large to push the statistical limits of SGD.  We show that SwitchML's in-network aggregation yields end-to-end improvements in training performance of up to 5.5x for popular DNN models. Focusing on a communication microbenchmark, compared to the best-in-class collective library NCCL, SwitchML is up to 2.9x faster than NCCL with RDMA and 9.1x than NCCL with TCP.  While the magnitude of the performance improvements is dependent on the neural network architecture and the underlying physical network speed, it is greater for models with smaller compute-to-communication ratios -- good news for future, faster DNN training accelerators. Our approach is not tied to any particular ML framework; we have integrated SwitchML with Horovod, which supports several popular toolkits like TensorFlow and PyTorch. The students involved in the project are primarily graduate students who are getting experience in all aspects of research ? ranging from problem definition, devising solutions, implementing systems, and writing technical papers. Many of them have also started giving public talks at conferences to help disseminate their work. They are also closely working with our industry partners to better understand the hardware innovations that are happening and to perform tech transfer back to the industry.  One of them has transitioned to industry and is doing DNN-based research at Amazon. Another recently graduated and is working with the machine learning team at Facebook. One of the projects that came out of this work is the TVM compiler. TVM, is an end-to-end compilation framework for deep learning systems that provides automated techniques for optimizing code generation on a variety of hardware platforms. TVM is now driven by an open source community involving multiple industry and academic institutions. It has been adopted by companies such as Amazon, Facebook, and Intel.         Last Modified: 09/25/2020       Submitted by: Arvind Krishnamurthy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
