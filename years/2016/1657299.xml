<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: SHF: Understanding The Role of Software Test Adequacy Criteria in Search-Based Test Generation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2017</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>173528.00</AwardTotalIntnAmount>
<AwardAmount>173528</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Software testing ensures that software is robust and reliable. As testers cannot know what faults exist apriori, dozens of metrics---ranging from the measurement of structural coverage to the detection of synthetic faults---have been proposed to judge test case adequacy. In theory, if such metrics are fulfilled, tests should be adequate at detecting faults. To alleviate the high cost of testing, optimization algorithms can be used to automatically generate test suites. These adequacy metrics are well-suited for guiding automated test creation. However, there is no adequacy metric known to universally correspond to "effective fault detection.'' Testers are left with a bewildering number of testing options, and there is little guidance on when to use one criterion over another. These metrics are a solid starting point for test case generation. Many faults cannot be detected until the code has been executed. However, merely executing code does not ensure adequate testing. How code is executed is important. It is clear that testers do not yet understand which adequacy metrics actually correspond to a high probability of fault detection, or under what situations these metrics can be applied.&lt;br/&gt;&lt;br/&gt;Therefore, it is clear that improving automated test generation requires gaining a better understanding of the circumstances where particular metrics are effective, isolating the features of such metrics that correlate to fault detection in such circumstances, and establishing and evaluating guidelines for the use and combination of metrics - perhaps tied to particular system types or domains - that will result in real-world fault detection. Large-scale empirical investigations will be performed into the nature of the relationship between adequacy criteria and the probability of fault detection in order to understand the efficacy and applicability of the criteria that are used to guide test creation. This work will have broader impacts on industrial practices, software engineering education, and - through dissemination to and collaborations with industrial partners and regulatory agencies - public safety and security.</AbstractNarration>
<MinAmdLetterDate>01/25/2017</MinAmdLetterDate>
<MaxAmdLetterDate>01/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657299</AwardID>
<Investigator>
<FirstName>Gregory</FirstName>
<LastName>Gay</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregory J Gay</PI_FULL_NAME>
<EmailAddress>ggay@cse.sc.edu</EmailAddress>
<PI_PHON>8037777093</PI_PHON>
<NSF_ID>000702266</NSF_ID>
<StartDate>01/25/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of South Carolina at Columbia</Name>
<CityName>COLUMBIA</CityName>
<ZipCode>292080001</ZipCode>
<PhoneNumber>8037777093</PhoneNumber>
<StreetAddress>Sponsored Awards Management</StreetAddress>
<StreetAddress2><![CDATA[1600 Hampton Street, Suite 414]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041387846</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTH CAROLINA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041387846</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of South Carolina at Columbia]]></Name>
<CityName>Room 3A01L</CityName>
<StateCode>SC</StateCode>
<ZipCode>292080001</ZipCode>
<StreetAddress><![CDATA[315 Main St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~173528</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Automated unit test generation, if effective at fault detection, can lower the cost of testing. Automated techniques, particularly search-based test generation techniques, target measurable optimization targets thought to improve the likelihood of fault detection. Satisfaction of these goals is measured using fitness functions - domain-based scoring functions. Ultimately, such functions represent test goals that approximate - but do not ensure - fault detection. The need to rely on approximations led to two questions - can fitness functions produce effective tests and, if so, which should be used to generate tests?</p> <p>To answer these questions, we have assessed the fault-detection capabilities of unit test suites generated to satisfy individual fitness functions and combinations of fitness functions. We found that the strongest indicators of effectiveness are a high level of code coverage over the targeted class and high satisfaction of a criterion&rsquo;s obligations. Consequently, the branch coverage fitness function was the most effective single function. Our findings indicate that multi-function approaches are the most effective. Fitness functions that thoroughly explore system structure should be used as primary generation objectives&mdash;supported by secondary fitness functions that explore orthogonal, supporting scenarios. Our results also provided further evidence that future approaches to test generation should focus on attaining higher coverage of private code and better initialization and manipulation of class dependencies.</p> <p>Choosing informative fitness functions is crucial to meeting the goals of a tester. Unfortunately, we found that many goals - such as forcing the class-under-test to throw exceptions - do not have universally effective fitness function formulations. Further, we found that universal approaces to test generation fail to take into account the unique factors of each tested system. To address both issues, we proposed that fitness function selection should be treated as a secondary optimization step. An adaptive algorithm that can vary the selection of fitness functions can adjust its selection throughout the generation process to maximize goal attainment, based on the current population of test suites. To test this hypothesis, we implemented two reinforcement learning algorithms in the EvoSuite test generation framework, and used these algorithms to dynamically set the fitness functions used during generation. We have evaluated our framework, EvoSuiteFIT, on a set of real faults. EvoSuiteFIT discovered and retained more exception-triggering input and produceed suites that detected a variety of faults missed by the other techniques. The ability to adjust fitness functions allows EvoSuiteFIT to make strategic choices that efficiently produce more effective test suites.</p><br> <p>            Last Modified: 04/03/2020<br>      Modified by: Gregory&nbsp;J&nbsp;Gay</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Automated unit test generation, if effective at fault detection, can lower the cost of testing. Automated techniques, particularly search-based test generation techniques, target measurable optimization targets thought to improve the likelihood of fault detection. Satisfaction of these goals is measured using fitness functions - domain-based scoring functions. Ultimately, such functions represent test goals that approximate - but do not ensure - fault detection. The need to rely on approximations led to two questions - can fitness functions produce effective tests and, if so, which should be used to generate tests?  To answer these questions, we have assessed the fault-detection capabilities of unit test suites generated to satisfy individual fitness functions and combinations of fitness functions. We found that the strongest indicators of effectiveness are a high level of code coverage over the targeted class and high satisfaction of a criterionâ€™s obligations. Consequently, the branch coverage fitness function was the most effective single function. Our findings indicate that multi-function approaches are the most effective. Fitness functions that thoroughly explore system structure should be used as primary generation objectives&mdash;supported by secondary fitness functions that explore orthogonal, supporting scenarios. Our results also provided further evidence that future approaches to test generation should focus on attaining higher coverage of private code and better initialization and manipulation of class dependencies.  Choosing informative fitness functions is crucial to meeting the goals of a tester. Unfortunately, we found that many goals - such as forcing the class-under-test to throw exceptions - do not have universally effective fitness function formulations. Further, we found that universal approaces to test generation fail to take into account the unique factors of each tested system. To address both issues, we proposed that fitness function selection should be treated as a secondary optimization step. An adaptive algorithm that can vary the selection of fitness functions can adjust its selection throughout the generation process to maximize goal attainment, based on the current population of test suites. To test this hypothesis, we implemented two reinforcement learning algorithms in the EvoSuite test generation framework, and used these algorithms to dynamically set the fitness functions used during generation. We have evaluated our framework, EvoSuiteFIT, on a set of real faults. EvoSuiteFIT discovered and retained more exception-triggering input and produceed suites that detected a variety of faults missed by the other techniques. The ability to adjust fitness functions allows EvoSuiteFIT to make strategic choices that efficiently produce more effective test suites.       Last Modified: 04/03/2020       Submitted by: Gregory J Gay]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
