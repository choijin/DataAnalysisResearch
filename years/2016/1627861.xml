<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research:   Designing Voice Analysis Technologies for Mental Health Applications in the United States</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>01/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>28796.00</AwardTotalIntnAmount>
<AwardAmount>28796</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jeffrey Mantz</SignBlockName>
<PO_EMAI>jmantz@nsf.gov</PO_EMAI>
<PO_PHON>7032927783</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project, which trains a graduate student in methods of conducting empirically-grounded scientific research, asks how new, artificial intelligence (AI)-enabled technologies reshape not only the future of mental health care in the United States, but also basic assumptions about the relationship between language, mind, and brain. This research explores these questions through an ethnographic study of interdisciplinary research teams at three U.S. universities that are seeking to develop computer-assisted speech analysis technologies for mental health applications. In the research teams that this study focuses on, neuroscientists, psychiatrists, psychologists and engineers are working together to develop technology that can be used to diagnose and track mental illness by analyzing the formal, acoustic properties of speech (such as pitch, timbre, intonation, and speed), bypassing its semantic content (what the words mean) altogether. The project will have implications for the mental health researchers themselves as they move into uncharted ethical domains in regards to privacy, surveillance, and the increased diagnostic reliance on experimental technologies.&lt;br/&gt;&lt;br/&gt;Beth Semel, under the supervision of Dr. Graham Jones at the Massachusetts Institute of Technology, explores how experts across disciplinary boundaries collaborate to design, develop and test artificial intelligence-enabled technologies when they appear to hold very different assumptions about the relationship between language and inner, psychological states. The researcher hypothesizes that collaborations surrounding the development of these technologies not only enact fundamental tensions within dominant views about how language works, but also reflect a re-working of claims of authority and expertise within U.S. mental health care. Increasingly, mental health researchers are eschewing traditional techniques of psychiatric diagnosis, which depend upon patients' subjective, verbal accounts of their psychological states and clinicians' observational and interpretive skills. Instead, they are enlisting the expertise of computer engineers who use AI techniques of pattern recognition to decipher the biomedical significance of behavioral symptoms. Using ethnographic participant observation, the researcher will collect data about how psychiatrists, psychologists, neuroscientists and engineers work together to design, test, and develop voice analysis technologies. Focusing on teams situated at the confluence of academic, commercial, and military arenas, this study explores the variety of ways in which mental illness is conceptualized in terms of scientific, public health, and national security concerns. By exploring how listening practices can shape assumptions about speech, and how the production of new listening techniques and technologies can reshape such assumptions, this research contributes to ongoing debates in linguistic anthropology about how culture affects understandings of the way language works.</AbstractNarration>
<MinAmdLetterDate>08/01/2016</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1627861</AwardID>
<Investigator>
<FirstName>Graham</FirstName>
<LastName>Jones</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Graham M Jones</PI_FULL_NAME>
<EmailAddress>gmj@mit.edu</EmailAddress>
<PI_PHON>6177154969</PI_PHON>
<NSF_ID>000624691</NSF_ID>
<StartDate>08/01/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Beth</FirstName>
<LastName>Semel</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Beth M Semel</PI_FULL_NAME>
<EmailAddress>bsemel@mit.edu</EmailAddress>
<PI_PHON>6172531000</PI_PHON>
<NSF_ID>000702615</NSF_ID>
<StartDate>08/01/2016</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[MIT]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7605</Code>
<Text>Cult Anthro DDRI</Text>
</ProgramElement>
<ProgramReference>
<Code>1390</Code>
<Text>CULTURAL ANTHROPOLOGY</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~28796</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The award of the doctoral dissertation research grant was used to conduct comparative, multi-sited, ethnographic fieldwork in support of the co-PI&rsquo;s doctoral dissertation, &ldquo;Designing Voice Analysis Technologies for Mental Health Applications in the United States.&rdquo; This project explored the work of U.S. university-based research teams of psychiatric and engineering professionals collaborating to develop artificial intelligence (AI)-enabled speech analysis technologies. Emphasizing how the value of AI is connected to its &ldquo;objective&rdquo; capacities, the co-PI investigated how researchers insist that their technologies can identify signs of mental illness that are otherwise inaudible to humans, and will be agnostic to some differences &ndash; like race, gender, and class &ndash; but not to the difference between a psychologically well and unwell person. The co-PI found that researchers&rsquo; attempts to use AI to cut through the sociocultural aspects of language in order to capture supposedly universal, biological aspects of mental illness reflect ambivalent attitudes about how best to listen to patients&rsquo; speech, and are part of a broader reconfiguration of expertise within U.S. mental health professions.</p> <p>&nbsp;</p> <p>Crucially, the co-PI&rsquo;s ethnographic fieldwork revealed that researchers did not necessarily locate pre-existing connections between ways of speaking and mental illness. Instead, they actively constituted and produced these connections. In other words, the supposedly immediate connection between acoustic features of speech and inner states that the technologies could make was in fact culturally mediated through researchers&rsquo; explicit decisions about what counts as objective and subjective features of speech, and was materially and sensorially mediated by invisible and under-valued human listening labor. The analysis of these data, which will be reported in the co-PI&rsquo;s dissertation, focused on questions such as: why do researchers insist that AI helps to insure that their technologies will be free of bias and agnostic to some differences but not others? What is the value of the &ldquo;caring&rdquo; components of mental health care in the age of big data? In addressing these questions, the dissertation will enhance scholarship on (1) the cultural and historical dimensions of listening practices and technologies, (2) anthropological critiques of language universals, and science studies literature on (3) biomedicine, computation, and surveillance and (4) expertise, knowledge production, and labor in biomedical research.</p> <p>&nbsp;</p> <p>Beyond anthropology, this dissertation has implications for disciplines engaged in conducting mental health research (neuroscience, psychiatry, psychology), especially as cross-disciplinary partnerships with engineers and computer scientists become increasingly common. For instance, because the dissertation provides three case studies of cross-disciplinary collaboration, it may help give future collaborators across mental health-related fields and engineering a clearer picture of the potential differences in ways of thinking, research design conventions, and ethical conventions that they might face. Additionally, this project will have an impact on machine learning experts and computer/data science writ large. Because it illustrates that automated systems of pattern recognition are not necessarily neutral or objective but can have assumptions, biases, and cultural conventions built into their frameworks, this project underscores that computer scientists and engineers should be critically self-reflexive about their day-to-day practices.</p> <p>&nbsp;</p> <p>The partnering of mental health care researchers with computer engineers is a relatively new phenomenon in the U.S. Mental health care researchers have yet to partake in any kind of official, collective conversation about how the involvement of these experts in their research and clinical trials might redraw or modify the standards for conducting ethical studies involving human subjects. This has implications for the intended end-user of these technologies, which include not only mental health care practitioners but also individuals seeking care or diagnosis in a psychiatric clinical context. Studying diagnostic listening technologies in formation, from a ground-up perspective, paints a picture of the unanticipated or under-examined safety, privacy, and general ethical concerns that need to be addressed before these technologies are put to use at the clinical or even commercial level.</p> <p>&nbsp;</p> <p>Finally, with the rise of &ldquo;smart listening&rdquo; devices and AI-enabled conversational agents like Siri, Alexa, and Google Voice, anxieties about the capacity of personal computing devises to gather, store, and potentially transmit personal information are running high in the U.S. Similarly, the extent to which AI might displace or even replace human labor is an ongoing topic of public debate. These are policy-level issues, touching upon economic concerns and concerns about privacy and surveillance, that have implications for society at large. This project has the potential to contribute to and inform these public conversations, providing non-technical descriptions of highly technical fields. Moreover, the project highlights that human labor plays and will continue to play an invisible but nevertheless crucial role in building and maintaining automated technology.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/30/2018<br>      Modified by: Beth&nbsp;M&nbsp;Semel</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The award of the doctoral dissertation research grant was used to conduct comparative, multi-sited, ethnographic fieldwork in support of the co-PI?s doctoral dissertation, "Designing Voice Analysis Technologies for Mental Health Applications in the United States." This project explored the work of U.S. university-based research teams of psychiatric and engineering professionals collaborating to develop artificial intelligence (AI)-enabled speech analysis technologies. Emphasizing how the value of AI is connected to its "objective" capacities, the co-PI investigated how researchers insist that their technologies can identify signs of mental illness that are otherwise inaudible to humans, and will be agnostic to some differences &ndash; like race, gender, and class &ndash; but not to the difference between a psychologically well and unwell person. The co-PI found that researchers? attempts to use AI to cut through the sociocultural aspects of language in order to capture supposedly universal, biological aspects of mental illness reflect ambivalent attitudes about how best to listen to patients? speech, and are part of a broader reconfiguration of expertise within U.S. mental health professions.     Crucially, the co-PI?s ethnographic fieldwork revealed that researchers did not necessarily locate pre-existing connections between ways of speaking and mental illness. Instead, they actively constituted and produced these connections. In other words, the supposedly immediate connection between acoustic features of speech and inner states that the technologies could make was in fact culturally mediated through researchers? explicit decisions about what counts as objective and subjective features of speech, and was materially and sensorially mediated by invisible and under-valued human listening labor. The analysis of these data, which will be reported in the co-PI?s dissertation, focused on questions such as: why do researchers insist that AI helps to insure that their technologies will be free of bias and agnostic to some differences but not others? What is the value of the "caring" components of mental health care in the age of big data? In addressing these questions, the dissertation will enhance scholarship on (1) the cultural and historical dimensions of listening practices and technologies, (2) anthropological critiques of language universals, and science studies literature on (3) biomedicine, computation, and surveillance and (4) expertise, knowledge production, and labor in biomedical research.     Beyond anthropology, this dissertation has implications for disciplines engaged in conducting mental health research (neuroscience, psychiatry, psychology), especially as cross-disciplinary partnerships with engineers and computer scientists become increasingly common. For instance, because the dissertation provides three case studies of cross-disciplinary collaboration, it may help give future collaborators across mental health-related fields and engineering a clearer picture of the potential differences in ways of thinking, research design conventions, and ethical conventions that they might face. Additionally, this project will have an impact on machine learning experts and computer/data science writ large. Because it illustrates that automated systems of pattern recognition are not necessarily neutral or objective but can have assumptions, biases, and cultural conventions built into their frameworks, this project underscores that computer scientists and engineers should be critically self-reflexive about their day-to-day practices.     The partnering of mental health care researchers with computer engineers is a relatively new phenomenon in the U.S. Mental health care researchers have yet to partake in any kind of official, collective conversation about how the involvement of these experts in their research and clinical trials might redraw or modify the standards for conducting ethical studies involving human subjects. This has implications for the intended end-user of these technologies, which include not only mental health care practitioners but also individuals seeking care or diagnosis in a psychiatric clinical context. Studying diagnostic listening technologies in formation, from a ground-up perspective, paints a picture of the unanticipated or under-examined safety, privacy, and general ethical concerns that need to be addressed before these technologies are put to use at the clinical or even commercial level.     Finally, with the rise of "smart listening" devices and AI-enabled conversational agents like Siri, Alexa, and Google Voice, anxieties about the capacity of personal computing devises to gather, store, and potentially transmit personal information are running high in the U.S. Similarly, the extent to which AI might displace or even replace human labor is an ongoing topic of public debate. These are policy-level issues, touching upon economic concerns and concerns about privacy and surveillance, that have implications for society at large. This project has the potential to contribute to and inform these public conversations, providing non-technical descriptions of highly technical fields. Moreover, the project highlights that human labor plays and will continue to play an invisible but nevertheless crucial role in building and maintaining automated technology.          Last Modified: 05/30/2018       Submitted by: Beth M Semel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
