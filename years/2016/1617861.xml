<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Data-Driven Material Understanding and Decomposition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>494212.00</AwardTotalIntnAmount>
<AwardAmount>510212</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>We are in daily contact with a rich range of materials (metals, woods, fabrics, granites, etc.) that contribute to how we understand the world.  Recognizing and modeling real-world materials have long been core challenges in computer vision and graphics.  Recently, scene understanding has experienced an explosion of research activity driven by deep learning models trained on large-scale datasets.  But the focus has mainly been on objects; materials have received less attention, and it has predominantly focused on careful measurements in laboratory settings.  Of course, there is a large gap between materials in the real world and these laboratory settings.  The PI's goal is to bridge that gap, to enable material understanding "in the wild."  Toward this end, her group recently released large-scale crowdsourced datasets (OpenSurfaces, Intrinsic Images in the Wild, MINC) that are already being used extensively in the research community.  Using this data to develop new material segmentations and recognition algorithms, the PI's team has produced state-of-the-art methods which open up new possibilities for data-driven material understanding that will impact a wide range of applications such as interior design, material editing, visual search, and robotics.  Project outcomes (including new datasets, annotations, and code) will be made fully open and public.  The PI actively mentors underrepresented minorities at Cornell, and is working with Women in Computing at Cornell (WICC) and Girls Who Code (GWC) to reach middle and high school students.  This research will build prototypes of the annotation tools, and integrate them into summer workshops at Cornell aimed at high school minority students.  The PI's group will also organize a Material Understanding Competition (MUC) to drive innovation in material recognition and segmentation, and intrinsic image decomposition.&lt;br/&gt;&lt;br/&gt;This project includes two major technical thrusts in material understanding:&lt;br/&gt;&lt;br/&gt;1.  Intrinsic images for material understanding.  Intrinsic image decomposition aims to decompose images into intrinsic properties such as material and illumination.  This decomposition is ill-posed and challenging for images in the wild.  This work will collect new pairwise shading and depth annotations for intrinsic image decomposition; introduce a new perceptual metric to evaluate algorithms; solve for joint material recognition and intrinsic decomposition; and develop proof-of-concept applications for image-based editing using intrinsic image decomposition.&lt;br/&gt;&lt;br/&gt;2.  Material recognition for semantic understanding.  Recognizing materials in the wild is extremely challenging.  This work will collect large-scale material annotations with "click" data and train weakly supervised recognition algorithms; collect fine-grained material data for subcategories like wood and metal; develop new algorithms for coarse and fine-grain recognition; and develop proof-of-concept applications for intelligent material search, and material assignment to shapes.</AbstractNarration>
<MinAmdLetterDate>07/11/2016</MinAmdLetterDate>
<MaxAmdLetterDate>04/18/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1617861</AwardID>
<Investigator>
<FirstName>Kavita</FirstName>
<LastName>Bala</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kavita Bala</PI_FULL_NAME>
<EmailAddress>kb@cs.cornell.edu</EmailAddress>
<PI_PHON>6072551383</PI_PHON>
<NSF_ID>000179613</NSF_ID>
<StartDate>07/11/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148537501</ZipCode>
<StreetAddress><![CDATA[107 Hoy Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~159747</FUND_OBLG>
<FUND_OBLG>2017~334465</FUND_OBLG>
<FUND_OBLG>2019~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-c1974e45-7fff-0349-da23-2c988ef526da"> </span></p> <p dir="ltr"><span>We interact with a rich range of materials in our everyday lives: fabrics, metals, glass, etc. A human observer is able to recognize these materials reliably in their day-to-day life to make important judgments on how to interact with and use these objects. For example, we can easily answer questions like: is this table made of wood or metal, is this spoon made of plastic or metal, is this fabric silk or velvet? While the ability to recognize materials is straightforward for humans, computers have lagged in their ability to recognize materials. But driven by the rise of mixed reality and augmented reality applications where users are interacting with materials and objects, and with the rise of robots that interact with the world, material recognition is becoming increasingly important. For example, an automated navigation robot would like to assess if a road is slick or rough for safety reasons, or a cleaning robot would like to distinguish between materials like paper, cloth, food, etc. to adapt to real-world situations.&nbsp;</span></p> <p dir="ltr"><span>Material recognition has been challenging for a computer for several reasons. Shapes, material and lighting from the world are all merged together in the formation of an image; an automated approach must separate out these components in an image, but without extra information this separation is ill-posed. To address this issue, machine learning algorithms could be trained to recognize materials, but datasets of training data are lacking, significantly preventing progress in material recognition and understanding.&nbsp;</span></p> <p dir="ltr"><span>In this project we advanced the state-of-the-art in material recognition and understanding by introducing:&nbsp; large-scale datasets of materials to train material recognition algorithms, pipelines to collect this data efficiently and accurately from human observers, state-of-the-art automated algorithms to recognize materials, and perceptual evaluation metrics to evaluate progress in the field.</span></p> <p dir="ltr"><span>We have collected new large-scale datasets of materials including material taxonomies and fine-grained labels, detailed boundaries of materials in photographs and paintings, and shape and lighting information. We have made these open source datasets publicly available and they are serving as a resource beyond researchers in graphics and vision to a broader community of&nbsp; perception psychologists and neuroscientists working on understanding human perception.&nbsp;</span>To collect these large scale datasets in a cost-effective and efficient manner, we have introduced new publicly available data annotation pipelines.</p> <p dir="ltr"><span>With these large-scale datasets that include the most detailed material information to date, we train algorithms to recognize materials in images, and have demonstrated their application in robot navigation applications. Further, we show how to evaluate such algorithms using perceptual metrics so that materials are reliably recognized.</span></p> <p dir="ltr"><span>Given commercial interest and products of augmented reality where users are interacting with materials and objects, and with the surge in automation through commercial and personal robots, material recognition is becoming increasingly important. This project significantly advances the state-of-the-art through the contribution of open source public datasets for large-scale training data, annotation pipelines,</span>&nbsp;algorithms to recognize materials in images, and perceptual metrics that are fueling innovation in this problem domain.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/11/2021<br>      Modified by: Kavita&nbsp;Bala</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   We interact with a rich range of materials in our everyday lives: fabrics, metals, glass, etc. A human observer is able to recognize these materials reliably in their day-to-day life to make important judgments on how to interact with and use these objects. For example, we can easily answer questions like: is this table made of wood or metal, is this spoon made of plastic or metal, is this fabric silk or velvet? While the ability to recognize materials is straightforward for humans, computers have lagged in their ability to recognize materials. But driven by the rise of mixed reality and augmented reality applications where users are interacting with materials and objects, and with the rise of robots that interact with the world, material recognition is becoming increasingly important. For example, an automated navigation robot would like to assess if a road is slick or rough for safety reasons, or a cleaning robot would like to distinguish between materials like paper, cloth, food, etc. to adapt to real-world situations.  Material recognition has been challenging for a computer for several reasons. Shapes, material and lighting from the world are all merged together in the formation of an image; an automated approach must separate out these components in an image, but without extra information this separation is ill-posed. To address this issue, machine learning algorithms could be trained to recognize materials, but datasets of training data are lacking, significantly preventing progress in material recognition and understanding.  In this project we advanced the state-of-the-art in material recognition and understanding by introducing:  large-scale datasets of materials to train material recognition algorithms, pipelines to collect this data efficiently and accurately from human observers, state-of-the-art automated algorithms to recognize materials, and perceptual evaluation metrics to evaluate progress in the field. We have collected new large-scale datasets of materials including material taxonomies and fine-grained labels, detailed boundaries of materials in photographs and paintings, and shape and lighting information. We have made these open source datasets publicly available and they are serving as a resource beyond researchers in graphics and vision to a broader community of  perception psychologists and neuroscientists working on understanding human perception. To collect these large scale datasets in a cost-effective and efficient manner, we have introduced new publicly available data annotation pipelines. With these large-scale datasets that include the most detailed material information to date, we train algorithms to recognize materials in images, and have demonstrated their application in robot navigation applications. Further, we show how to evaluate such algorithms using perceptual metrics so that materials are reliably recognized. Given commercial interest and products of augmented reality where users are interacting with materials and objects, and with the surge in automation through commercial and personal robots, material recognition is becoming increasingly important. This project significantly advances the state-of-the-art through the contribution of open source public datasets for large-scale training data, annotation pipelines, algorithms to recognize materials in images, and perceptual metrics that are fueling innovation in this problem domain.                Last Modified: 08/11/2021       Submitted by: Kavita Bala]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
