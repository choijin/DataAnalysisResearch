<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Automatically Generating Domain Specific Structured Ontologies for Video</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/15/2016</AwardEffectiveDate>
<AwardExpirationDate>03/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase 1 project is to make video searchable and discoverable in a cost-effective manner by automating video annotation tasks that are currently done manually. Video is being created at an increasingly high rate, and media companies are becoming overwhelmed by the sheer amount of video in their libraries.  Companies have resorted to paying people to manually watch their videos and "tag" them with relevant descriptions so that the content becomes searchable and therefore useable.  In this project the company will build an inference engine that can leverage the data and structure within video to discover specific multimodal concepts significant to the particular domain and automatically train and refine classifiers to apply unique and meaningful data describing the video.  Leveraging this metadata, companies can index and search their videos more efficiently, enabling them to generate tailored video clips to meet specific goals, and ultimately publish video at a much larger scale.  The company believes the unique domain-specific video metadata generated by its system will have an overwhelming effect on the ability of companies to disseminate informative videos, and improve their use of video online.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase 1 project proposes to develop an inference engine that combines the modalities of information in video to automatically discover and train multimodal classifiers for important concepts within specific domains.  Current approaches to video understanding seek to develop general visual classification models; these approaches focus on leveraging labeled data to train supervised learning algorithms in order to describe the video. These approaches fail in specific verticals, because the information output is not granular enough to provide value within the context of the specific domain, and the cost for manual annotation is high. The approach proposed in this project takes into account the multimodal information associated with a group of related videos to automatically learn classifiers for concepts that are important in this video domain, without expensive manual annotation. The approach leverages the correlations between the different modalities in video and existing domain-specific structure to intelligently accomplish this task.  The company expects this research to lead to the discovery of many new domain-specific video concept classifiers that do not exist in current visual ontologies, and a reusable approach for training visual classifiers that can be applied across many domains.</AbstractNarration>
<MinAmdLetterDate>12/11/2016</MinAmdLetterDate>
<MaxAmdLetterDate>12/11/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1647799</AwardID>
<Investigator>
<FirstName>Joe</FirstName>
<LastName>Ellis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joe Ellis</PI_FULL_NAME>
<EmailAddress>joe.ellis@vidrovr.com</EmailAddress>
<PI_PHON>5756393273</PI_PHON>
<NSF_ID>000725404</NSF_ID>
<StartDate>12/11/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Vidrovr Inc.</Name>
<CityName>New York</CityName>
<ZipCode>100149453</ZipCode>
<PhoneNumber>4156522388</PhoneNumber>
<StreetAddress>175 Varick St.</StreetAddress>
<StreetAddress2><![CDATA[1st Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080267386</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VIDROVR INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Vidrovr Inc.]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100144604</ZipCode>
<StreetAddress><![CDATA[175 Varick St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-4101961d-4964-5a6d-498b-5a9c29801966"> </span></p> <p dir="ltr"><span>During the NSF SBIR project, Vidrovr Inc. launched the Vidrovr Platform, a video processing pipeline and search system that leverages NSF-funded research and developments in video classification and information extraction. &nbsp;The Vidrovr Platform is used today to provide smart search and video clipping functionality over both video archives and real time video streams for some of the largest broadcast/media companies in the United States.  The Vidrovr platform includes algorithms for person recognition and discovery, visual concept discovery, video content categorization, and a system that automatically associates hashtags found from social media websites to clips of video. &nbsp;These algorithms are powered by an always growing knowledge base of entities that are both domain and customer specific.  By associating Vidrovr&rsquo;s knowledge engine to specific portions of video, our algorithms enable companies to discover, clip and publish video content to the public faster and more efficiently. </span></p> <p dir="ltr"><span>Between June 2017 to April 2018, The Vidrovr Platform has processed over 5,722 hours of video content and our knowledge graph has grown to encompass over 10,000+ unique social media hashtags, 20,079 unique people learned and extracted from videos sent to our platform, and over 12,000+ unique visual concepts that have been discovered within the video streams sent to the Vidrovr Platform. &nbsp;Using this information, we have developed a proprietary and patented multimodal search capability that allows users to perform detailed queries, such as finding clips of video that contain a &ldquo;baseball stadium&rdquo;, where &ldquo;Cardinals&rdquo; appears on screen and &ldquo;Stan Musial&rdquo; is speaking.  This functionality, allows videos to be made searchable, allowing for a greater access to information that was previously unavailable to end consumers of this content, whether they are internal producers, editors, or video consumers such as students and general viewers. &nbsp;To enhance the Vidrovr consumer facing offering,  Vidrovr offers a graphical user search interface that can be easily deployed on their websites. This search interface which based on a search query and a set of returned videos, presents to the user entities from the Vidrovr Knowledge Graph that are related to that query, and can be used to quickly filter through the video content, enabling new video search experiences.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/28/2018<br>      Modified by: Joe&nbsp;Ellis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   During the NSF SBIR project, Vidrovr Inc. launched the Vidrovr Platform, a video processing pipeline and search system that leverages NSF-funded research and developments in video classification and information extraction.  The Vidrovr Platform is used today to provide smart search and video clipping functionality over both video archives and real time video streams for some of the largest broadcast/media companies in the United States.  The Vidrovr platform includes algorithms for person recognition and discovery, visual concept discovery, video content categorization, and a system that automatically associates hashtags found from social media websites to clips of video.  These algorithms are powered by an always growing knowledge base of entities that are both domain and customer specific.  By associating Vidrovr?s knowledge engine to specific portions of video, our algorithms enable companies to discover, clip and publish video content to the public faster and more efficiently.  Between June 2017 to April 2018, The Vidrovr Platform has processed over 5,722 hours of video content and our knowledge graph has grown to encompass over 10,000+ unique social media hashtags, 20,079 unique people learned and extracted from videos sent to our platform, and over 12,000+ unique visual concepts that have been discovered within the video streams sent to the Vidrovr Platform.  Using this information, we have developed a proprietary and patented multimodal search capability that allows users to perform detailed queries, such as finding clips of video that contain a "baseball stadium", where "Cardinals" appears on screen and "Stan Musial" is speaking.  This functionality, allows videos to be made searchable, allowing for a greater access to information that was previously unavailable to end consumers of this content, whether they are internal producers, editors, or video consumers such as students and general viewers.  To enhance the Vidrovr consumer facing offering,  Vidrovr offers a graphical user search interface that can be easily deployed on their websites. This search interface which based on a search query and a set of returned videos, presents to the user entities from the Vidrovr Knowledge Graph that are related to that query, and can be used to quickly filter through the video content, enabling new video search experiences.               Last Modified: 06/28/2018       Submitted by: Joe Ellis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
