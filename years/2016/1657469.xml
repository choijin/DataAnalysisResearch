<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Joint Models of Language and Context for Robotic Language Acquisition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>163057.00</AwardTotalIntnAmount>
<AwardAmount>163057</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As robots become smaller, less expensive, and more capable, they are able to perform an increasing variety of tasks, leading to revolutionary improvements in domains such as automobile safety and manufacturing. However, their inflexibility makes them hard to deploy in human-centric environments such as homes and schools, where their tasks and environments are constantly changing. Meanwhile, learning to understand language about the physical world is a growing research area in both robotics and natural language processing. The core problem is how the meanings of words are grounded in the noisy, perceptual world in which a robot operates. This project explores how robots can learn about the world from natural language in order to take instructions and learn about their environment naturally and intuitively from people. The ability to follow directions reduces the adoption barrier for robots in domains such as assistive technology, education, and caretaking, where interactions with non-specialists are crucial. Such robots have the potential to ultimately improve autonomy and independence for populations such as aging-in-place elders; for example, a manipulator arm that can learn from a user?s explanation how to handle food or open novel containers would directly affect the independence of persons with dexterity concerns such as advanced arthritis. &lt;br/&gt;&lt;br/&gt;This is an exploratory investigation of how linguistic and perceptual models can be expanded during interaction, allowing robots to understand novel language about unanticipated domains. In particular, the focus is on developing new learning approaches that correctly induce joint models of language and perception, building data-driven language models that add new semantic representations over time. The work combines semantic parser learning, which provides a distribution over possible interpretations of language, with perceptual representations of the underlying world. New concepts are added on the fly as new words and new perceptual data are encountered, and a semantically meaningful model can be trained by maximizing the expected likelihood of language and visual components. This integrated approach allows for effective model updates with no explicit labeling of words or percepts. This approach will be combined with experiments on improving learning efficiency by incorporating active learning, leveraging a robot's ability to ask questions about objects in the world.</AbstractNarration>
<MinAmdLetterDate>07/20/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1657469</AwardID>
<Investigator>
<FirstName>Cynthia</FirstName>
<LastName>Matuszek</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cynthia Matuszek</PI_FULL_NAME>
<EmailAddress>cmat@umbc.edu</EmailAddress>
<PI_PHON>5125774025</PI_PHON>
<NSF_ID>000690099</NSF_ID>
<StartDate>07/20/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland Baltimore County</Name>
<CityName>Baltimore</CityName>
<ZipCode>212500002</ZipCode>
<PhoneNumber>4104553140</PhoneNumber>
<StreetAddress>1000 Hilltop Circle</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>061364808</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND BALTIMORE COUNTY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland Baltimore County]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212500002</ZipCode>
<StreetAddress><![CDATA[1000 Hilltop Circle]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~163057</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To be deployed in human environments such as homes, schools, and care facilities, robots will need to be able to learn about their world and perform tasks based on what their users are telling them, that is, from language. Learning this grounded language (language that refers to the physical environment and its tasks) is an important problem in robotics and human-robot interaction. In this project, we have developed new tools intended to make grounded language learning more interactive, able to learn more different kinds of language, and accessible to a wider population.</p> <p>When a robot asks questions in order to learn more about the world or follow up on something it is told, it is using active learning. To avoid asking too many questions, it needs to decide what questions will give it the most information. This area has been explored before; in this project, we tested a variety of approaches to choosing questions for grounded language learning. This let us develop and formalize metrics that are intended to help robotics practitioners choose the right active learning approach for these problems.</p> <p>Existing work on learning grounded language about objects has largely, although not entirely, focused on learning specific kinds of traits such as color, weight, etc. In this project, we used deep learning to find features of sensor input that let us learn about whatever terms users decide to use when describing things, even if they are talking about traits we didn&rsquo;t consider when building the robot. For example, our robot learned to identify objects by material after people unexpectedly described objects as being made of ceramic or aluminum.</p> <p>Work on understanding natural languages is often language-specific, which can affect whether the technology is usable by different groups and demographics. In order to understand how to make broadly accessible robots, we tested our existing learning system in Spanish and Hindi. While it worked to some extent, performance did degrade compared to learning from English. We explored why and proposed general design constraints for language learning systems that will be effective in multiple languages with fewer changes.</p> <p>Broadly, work performed under this project explored how we can make robots that learn from human language more flexible, better at actively gathering information, and available to a wider range of possible end users. These results take active steps towards knowing how to build robots that can be broadly deployed in human-centric environments, where they can learn about the world and its contents from their end users, in exactly the way those users care about. Ultimately, these results are part of the path towards deploying robots that are useful and customizable, making robotic technology more available to more people.</p><br> <p>            Last Modified: 03/04/2021<br>      Modified by: Cynthia&nbsp;Matuszek</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To be deployed in human environments such as homes, schools, and care facilities, robots will need to be able to learn about their world and perform tasks based on what their users are telling them, that is, from language. Learning this grounded language (language that refers to the physical environment and its tasks) is an important problem in robotics and human-robot interaction. In this project, we have developed new tools intended to make grounded language learning more interactive, able to learn more different kinds of language, and accessible to a wider population.  When a robot asks questions in order to learn more about the world or follow up on something it is told, it is using active learning. To avoid asking too many questions, it needs to decide what questions will give it the most information. This area has been explored before; in this project, we tested a variety of approaches to choosing questions for grounded language learning. This let us develop and formalize metrics that are intended to help robotics practitioners choose the right active learning approach for these problems.  Existing work on learning grounded language about objects has largely, although not entirely, focused on learning specific kinds of traits such as color, weight, etc. In this project, we used deep learning to find features of sensor input that let us learn about whatever terms users decide to use when describing things, even if they are talking about traits we didn’t consider when building the robot. For example, our robot learned to identify objects by material after people unexpectedly described objects as being made of ceramic or aluminum.  Work on understanding natural languages is often language-specific, which can affect whether the technology is usable by different groups and demographics. In order to understand how to make broadly accessible robots, we tested our existing learning system in Spanish and Hindi. While it worked to some extent, performance did degrade compared to learning from English. We explored why and proposed general design constraints for language learning systems that will be effective in multiple languages with fewer changes.  Broadly, work performed under this project explored how we can make robots that learn from human language more flexible, better at actively gathering information, and available to a wider range of possible end users. These results take active steps towards knowing how to build robots that can be broadly deployed in human-centric environments, where they can learn about the world and its contents from their end users, in exactly the way those users care about. Ultimately, these results are part of the path towards deploying robots that are useful and customizable, making robotic technology more available to more people.       Last Modified: 03/04/2021       Submitted by: Cynthia Matuszek]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
