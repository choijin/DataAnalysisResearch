<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CIF: Universal Analysis of Optimization Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2017</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Iterative optimization algorithms lie at the heart of modern data-intensive applications such as machine learning, computer vision, and data science. The choice of algorithm, or even the choice of tuning parameters for a particular algorithm, has a dramatic effect on performance and viability. Even simple variants of well-known algorithms can be troublesome to analyze and must be considered on a case-by-case basis with the help of either deep insights by experts or extensive numerical simulations. Then, theoretical analyses of algorithms may fail to provide accurate or faithful performance guarantees because they do not account for sensitivity to parameter choice, robustness to noise either inherent to the algorithm or built in to how the iterations are computed, or other sources of uncertainty.&lt;br/&gt;&lt;br/&gt;The starting point for this research is to view iterative algorithms as dynamical systems with feedback. In gradient-based descent methods, for example, gradients are evaluated at each step and used to compute subsequent iterates. This research leverages tools from robust control (specifically, integral quadratic constraints and semidefinite programming) to develop a versatile, scalable, and modular framework capable of analyzing a variety of algorithms under different assumptions in an efficient and systematic manner. Of particular interest will be large-scale algorithms such as stochastic gradient descent and its variants, as well as distributed or asynchronous implementations.</AbstractNarration>
<MinAmdLetterDate>02/10/2017</MinAmdLetterDate>
<MaxAmdLetterDate>02/10/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1656951</AwardID>
<Investigator>
<FirstName>Laurent</FirstName>
<LastName>Lessard</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laurent Lessard</PI_FULL_NAME>
<EmailAddress>l.lessard@northeastern.edu</EmailAddress>
<PI_PHON>6503531566</PI_PHON>
<NSF_ID>000718272</NSF_ID>
<StartDate>02/10/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 6401]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>161202122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName/>
<StateCode>WI</StateCode>
<ZipCode>537061691</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7797</Code>
<Text>COMM &amp; INFORMATION FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Iterative optimization methods lie at the heart of most data-driven computational tasks, such as training machine learning models to perform classification, or solving large-scale logistics problems. The performance of such optimization methods is often quantified empirically or in an ad hoc fashion by using extensive numerical simulation. The aim of this project was to develop efficient methodology to enable the automated analysis of algorithms, thereby predicting how well they would perform on applications of practical interest without the need for time-consuming simulations.</p> <p>This project develops an efficient and systematic approach inspired by dynamical systems and control theory to analyze algorithms and provide robust performance guarantees. Roughly, the user identifies a class of problems of interest and an algorithm they would like to use, and the approach developed in this project quantifies the worst-case behavior of the algorithm over any problem in the class of interest, thereby ensuring that it will always perform in a predictably safe way when used in practice.</p> <p>We demonstrate that this approach for understanding algorithm performance extends to many popular classes of algorithms, such as algorithms for unconstrained optimization, distributed optimization, and settings containing stochastic disturbances. In each case, the algorithms can be modeled and analyzed in a similar way. This effort paves the way for developing a new science of algorithm design. Rather than algorithms being tested and tuned in an ad hoc fashion, algorithms could be engineered in the same way that cars, airplanes, and other products are engineered: through principled design rather than trial-and-error.</p> <p>Finally, this work bridges several traditionally separate disciplines of research, which is sure to spur future innovation. This project introduces tools from control theory to a new application of algorithm analysis, which is largely studied by optimizers, statisticians, and machine learning specialists.</p><br> <p>            Last Modified: 04/27/2020<br>      Modified by: Laurent&nbsp;Lessard</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Iterative optimization methods lie at the heart of most data-driven computational tasks, such as training machine learning models to perform classification, or solving large-scale logistics problems. The performance of such optimization methods is often quantified empirically or in an ad hoc fashion by using extensive numerical simulation. The aim of this project was to develop efficient methodology to enable the automated analysis of algorithms, thereby predicting how well they would perform on applications of practical interest without the need for time-consuming simulations.  This project develops an efficient and systematic approach inspired by dynamical systems and control theory to analyze algorithms and provide robust performance guarantees. Roughly, the user identifies a class of problems of interest and an algorithm they would like to use, and the approach developed in this project quantifies the worst-case behavior of the algorithm over any problem in the class of interest, thereby ensuring that it will always perform in a predictably safe way when used in practice.  We demonstrate that this approach for understanding algorithm performance extends to many popular classes of algorithms, such as algorithms for unconstrained optimization, distributed optimization, and settings containing stochastic disturbances. In each case, the algorithms can be modeled and analyzed in a similar way. This effort paves the way for developing a new science of algorithm design. Rather than algorithms being tested and tuned in an ad hoc fashion, algorithms could be engineered in the same way that cars, airplanes, and other products are engineered: through principled design rather than trial-and-error.  Finally, this work bridges several traditionally separate disciplines of research, which is sure to spur future innovation. This project introduces tools from control theory to a new application of algorithm analysis, which is largely studied by optimizers, statisticians, and machine learning specialists.       Last Modified: 04/27/2020       Submitted by: Laurent Lessard]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
