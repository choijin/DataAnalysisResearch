<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Advancing Interaction Paradigms in Mobile Augmented Reality using Eye Tracking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2013</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>529862.00</AwardTotalIntnAmount>
<AwardAmount>537862</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In this project the PI will develop the science needed to enhance mobile augmented reality applications with (a) eye tracking and (b) gaze direction.  Mobile augmented reality uses a hand-held device to provide a see-through view of the physical world in which an image of the physical world is superimposed with information about the things in that view.  It is as if you held a piece of glass up to the world, and text appeared on that piece of glass labeling the things you see through the glass.  This project will investigate how mobile eye tracking, which monitors where a person is looking while on the go, can be used to determine what objects in a visual scene a person is interested in, and thus might like to have annotated in their augmented reality view.  This project will investigate how to make these scene annotations appear and disappear in a manner that is neither distracting nor obtrusive by conducting experiments that measure a person's ability to accomplish visual tasks while presented with text annotations in different sizes, transparency levels, and distances from the point-of-gaze (the point where a person is looking).  The project will develop algorithms to automatically manage the density and placement of these labels to best support human tasks while avoiding the creation of distracting "visual clutter".&lt;br/&gt;&lt;br/&gt;The project will also develop the science that is needed to direct a person's gaze in the physical world by means of new visualization techniques for use in mobile augmented reality systems.  In this case, it is as if the piece of glass through which you are viewing the world periodically changed slightly to unobtrusively motivate you to look more closely at different, specific, task-relevant parts of the scene.  This aspect of the project will be conducted in collaboration with the Houston Museum of Fine Arts.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The project will advance discovery of visualization techniques to permit mobile applications to enhance the viewing of the physical world, while promoting the teaching and learning of science in multiple contexts.  Collaborating with the Houston Museum of Fine Arts, the project will develop a freely-downloadable iPhone application that will enhance museum-goers' learning in the arts, and provide a proof-of-concept of how the techniques developed in this project could be used in other contexts.&lt;br/&gt;&lt;br/&gt;The project pursues a number of specific new opportunities in science education, including the development of (a) project-related curriculum for a science-based summer camp for junior high school students at Texas A&amp;M University, (b) new University-level courses on the programming for augmented reality and the human performance aspects of lighting and cinematography, and (c) conference tutorials on experimental design, eye tracking, and perception in computer graphics.</AbstractNarration>
<MinAmdLetterDate>02/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>01/30/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1253432</AwardID>
<Investigator>
<FirstName>Ann</FirstName>
<LastName>McNamara</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ann McNamara</PI_FULL_NAME>
<EmailAddress>ann@viz.tamu.edu</EmailAddress>
<PI_PHON>9798454715</PI_PHON>
<NSF_ID>000521199</NSF_ID>
<StartDate>02/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M University</Name>
<CityName>College Station</CityName>
<ZipCode>778454375</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy South</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>020271826</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A &amp; M UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&M University Main Campus]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778433137</ZipCode>
<StreetAddress><![CDATA[TAMU 3137]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~98854</FUND_OBLG>
<FUND_OBLG>2014~110275</FUND_OBLG>
<FUND_OBLG>2015~105832</FUND_OBLG>
<FUND_OBLG>2016~109526</FUND_OBLG>
<FUND_OBLG>2017~113375</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Augmented Reality (AR) systems provide an enhanced vision of the physical world by integrating virtual elements, such as text and graphics, with real-world environments. The advent of affordable mobile technology has sparked a resurgence of interest in mobile AR applications. The goal of this work was to make AR systems more responsive to individual user interest, and to use the dynamic nature of AR elements to provide a heightened perception of the real world.&nbsp;</p> <p>The novelty of this research lies in the use of eye-tracking as a strategy to improve mobile AR applications.</p> <p><em>Two specific research problems were addressed:</em></p> <p>1. Augmented elements, or labels, are contextually linked to real world objects or locations.To ensure the correct associations between virtual elements and real objects the augmentedelements must be placed in the vicinity of the object it describes. Enforcing this spatialassociations can lead to problems: labels can overlap each other rendering them unreadable and, labels can obscure real world objects that are relevant to the user. Optimal placement oflabels is an active area of research. This research used eye-tracking to identifywhere the user is looking, and then evaluated the most appropriate information placement strategies based on that gaze location.</p> <p>2. Inherent in mobile AR applications is the powerful ability to visually highlight information inthe real world. We harnessed this this ability to direct gaze to Points of Interest (POIs).Combining mobile AR and image manipulation will enabling us to give visual distinctionto POIs and directly influence or direct gaze in real world scenes.&nbsp; Through this work we have developed a general model of interest and attention based on eye-tracking, and combinethis with novel layout strategies to maximize use of limited screen space for mobile AR.</p> <p><br /><strong>Intellectual Merit: </strong>This is research that would benefit both developers and educators in advancing collective understanding of information presentation in AR.&nbsp; This researchopens doors for future research on eye tracking and AR, and fills a current void in the existing research on the use of eye-tracking for the effectiveness of mobile AR systems. This novel research has the potential to transform the way in which informationis presented and viewed in mobile AR imagery.</p> <p><br /><strong>Broader Impact: </strong>New eye-tracked mobile AR systems will enhance the infrastructure for research and education in many domains, including art, geoscience, physics, mathematics or engineering. We integrated education into all aspects of these research projects.&nbsp; We note that the integration of Art and Science in this research is attractive to a diverse demographic, and thereby entices students that may not typically consider STEM disciplines as an academic path. Finally, all results have been widely disseminated in the form of conference publications.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/29/2020<br>      Modified by: Ann&nbsp;Mcnamara</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Augmented Reality (AR) systems provide an enhanced vision of the physical world by integrating virtual elements, such as text and graphics, with real-world environments. The advent of affordable mobile technology has sparked a resurgence of interest in mobile AR applications. The goal of this work was to make AR systems more responsive to individual user interest, and to use the dynamic nature of AR elements to provide a heightened perception of the real world.   The novelty of this research lies in the use of eye-tracking as a strategy to improve mobile AR applications.  Two specific research problems were addressed:  1. Augmented elements, or labels, are contextually linked to real world objects or locations.To ensure the correct associations between virtual elements and real objects the augmentedelements must be placed in the vicinity of the object it describes. Enforcing this spatialassociations can lead to problems: labels can overlap each other rendering them unreadable and, labels can obscure real world objects that are relevant to the user. Optimal placement oflabels is an active area of research. This research used eye-tracking to identifywhere the user is looking, and then evaluated the most appropriate information placement strategies based on that gaze location.  2. Inherent in mobile AR applications is the powerful ability to visually highlight information inthe real world. We harnessed this this ability to direct gaze to Points of Interest (POIs).Combining mobile AR and image manipulation will enabling us to give visual distinctionto POIs and directly influence or direct gaze in real world scenes.  Through this work we have developed a general model of interest and attention based on eye-tracking, and combinethis with novel layout strategies to maximize use of limited screen space for mobile AR.   Intellectual Merit: This is research that would benefit both developers and educators in advancing collective understanding of information presentation in AR.  This researchopens doors for future research on eye tracking and AR, and fills a current void in the existing research on the use of eye-tracking for the effectiveness of mobile AR systems. This novel research has the potential to transform the way in which informationis presented and viewed in mobile AR imagery.   Broader Impact: New eye-tracked mobile AR systems will enhance the infrastructure for research and education in many domains, including art, geoscience, physics, mathematics or engineering. We integrated education into all aspects of these research projects.  We note that the integration of Art and Science in this research is attractive to a diverse demographic, and thereby entices students that may not typically consider STEM disciplines as an academic path. Finally, all results have been widely disseminated in the form of conference publications.          Last Modified: 05/29/2020       Submitted by: Ann Mcnamara]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
