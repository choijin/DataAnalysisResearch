<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI-Small: The Intelligent Workcell - Enabling Robots and People to Work Together Safely in Manufacturing Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>690000.00</AwardTotalIntnAmount>
<AwardAmount>690000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Irina Dolinskaya</SignBlockName>
<PO_EMAI>idolinsk@nsf.gov</PO_EMAI>
<PO_PHON>7032927078</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The research objective of this award is to investigate methods to enable people and industrial robots to work safely within the same workspace.  Current robotic manufacturing practice requires the physical separation of people and robots, which ensures safety, but is inefficient in terms of time and resources, and limits the tasks suitable for robotic manufacturing.  This research will develop an "Intelligent Workcell," which augments the traditional robotic workcell with perception systems that observe workers within the workspace.  Methods to explicitly track workers and estimate their body pose will enable dynamically adaptive safety zones surrounding the robot, thereby preventing the robot from injuring workers.  Algorithms will be developed to recognize the activities that workers are performing.  These algorithms will learn a task-independent vocabulary of fundamental action components, which will form the building blocks for a hierarchical activity recognition framework.  Finally, mechanisms for providing feedback to workers about the robot's intended actions will be studied.&lt;br/&gt;&lt;br/&gt;This research is expected to provide new capabilities in robotic workcell safety and monitoring, allowing people and industrial robots to work safely and effectively in the same environment.  Such capabilities would improve the efficiency of existing robotic workcells, since the robot would not be required to stop whenever a person enters the workspace (as is current practice).  Furthermore, new manufacturing processes that involve robots and people working together on a single task would be enabled.  Students at the graduate and undergraduate level will benefit from using the prototype Intelligent Workcell in project courses, and grade-school students will participate in short courses and workshops designed to ignite interest in STEM activities related to industrial robotics and computer vision.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/18/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208598</AwardID>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Rybski</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Rybski</PI_FULL_NAME>
<EmailAddress>prybski@cs.cmu.edu</EmailAddress>
<PI_PHON>4122688746</PI_PHON>
<NSF_ID>000333955</NSF_ID>
<StartDate>05/18/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Rybski</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Rybski</PI_FULL_NAME>
<EmailAddress>prybski@cs.cmu.edu</EmailAddress>
<PI_PHON>4122688746</PI_PHON>
<NSF_ID>000333955</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate>03/27/2013</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Huber</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Huber</PI_FULL_NAME>
<EmailAddress>dhuber@cs.cmu.edu</EmailAddress>
<PI_PHON>4122682991</PI_PHON>
<NSF_ID>000357502</NSF_ID>
<StartDate>05/18/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Huber</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Huber</PI_FULL_NAME>
<EmailAddress>dhuber@cs.cmu.edu</EmailAddress>
<PI_PHON>4122682991</PI_PHON>
<NSF_ID>000357502</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate>03/27/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Deva</FirstName>
<LastName>Ramanan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Deva Ramanan</PI_FULL_NAME>
<EmailAddress>deva@cs.cmu.edu</EmailAddress>
<PI_PHON>4122686966</PI_PHON>
<NSF_ID>000083629</NSF_ID>
<StartDate>05/18/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kris</FirstName>
<LastName>Kitani</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kris Kitani</PI_FULL_NAME>
<EmailAddress>kkitani@cs.cmu.edu</EmailAddress>
<PI_PHON>4122685186</PI_PHON>
<NSF_ID>000663208</NSF_ID>
<StartDate>05/18/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~690000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-ff2c2d68-e914-e39b-8d2e-67cbee984b1a" style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">Our research objective is to revolutionize the manner in which industrial robots and people interact by allowing them to work safely together within the same workspace simultaneously. Current robotic manufacturing practice requires the physical separation of people and robots, which ensures safety, but is inefficient in terms of time and resources, and limits the tasks suitable for robotic manufacturing. </span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">The</span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;"> </span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">Intelligent Workcell</span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;"> </span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">is our vision of the robotic workcell of the future. It uses automated perception systems to observe workers within the robot&rsquo;s workspace, allowing for dynamically adaptive safety zones; it uses intelligence to understand what the workers are doing, allowing it to monitor the workers&rsquo; tasks and plan accordingly; and it provides feedback to the worker to indicate what the robot is intending to do. In the Intelligent Workcell, robots and people will work together seamlessly and safely. The Intelligent Workcell builds upon our experience developing the Intelligent Monitoring of Assembly Operations (IMAO) system, which uses multiple 3D sensors to detect moving objects in a workcell and establish real-time dynamic safety zones surrounding the robot and the workers. If the safety zones intersect, the robot stops until the problem is resolved. The IMAO system is limited in that it does not explicitly recognize or track people, it does not understand what the people are doing, and it does not provide feedback about the robot&rsquo;s intentions.</span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">We have made advances in the following areas:</span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">Intelligent Workcell Prototype.</span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;"> The first prototype of an Intelligent Workcell was designed and implemented as a foundational component of this project. &nbsp;We obtained an industrial robot and integrated it with the sensor suite and hybrid safety system algorithms developed for the IMAO system. The workcell was further augmented with embedded feedback mechanisms, including a custom-designed illuminated floor mosaic, programmable lighting on the robot manipulator, and synthetic robot motion visualization via an external display.</span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">3D Sensor Network Calibration.</span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;"> We developed a novel method to calibrate a network of 3D sensors that is easy to use by an untrained operator. The method is suitable for deploying in a factory environment where the Intelligent Workcell operates. The operator simply waves the calibration sphere around the workcell, and the algorithm automatically estimates the positions of all cameras relative to the workcell.</span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">3D Pose Estimation. </span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">A core component of the perceptual model is the ability to detect one or more humans and track their 3D body postures over time. We have made several improvements to the state-of-the-art in this space. We devised a 3D pose pipeline that reasoned about 2D poses at an intermediate stage, and that tracks multiple people across occlusions through efficient and online combinatorial algorithms.</span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">Models of Human Activity.</span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;"> We have developed various structured policies which can be used to model both the partonomic and cyclic nature of agent activities. In addition we have develop novel methods for understanding nonverbal behavior signals, in particular head gestures, to enable co-robot systems to work more seamlessly with people.</span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">Robot Intent Feedback.</span><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;"> &nbsp;We have shown empirically that feedback (floor-based and arm-mounted lights) improves collaboration for human-robot assembly tasks. Results show that participants completing tasks with the benefit of feedback exhibited a smaller average number of wait periods, shorter average wait periods, and fewer robot blockages.</span></p> <p>&nbsp;</p> <p style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;" dir="ltr"><span style="font-size: 12pt; font-family: Calibri; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;">Together, these advances serve as a proof of concept that the Intelligent Workcell provides valuable advantages over today&rsquo;s conventional industrial workcells. &nbsp;The foundational algorithms in sensor calibration, pose estimation, tracking, and activity monitoring will extend this project&rsquo;s impact beyond industrial robotics into wide-ranging domains involving the interaction of people and robots.</span></p><br> <p>            Last Modified: 01/15/2018<br>      Modified by: Daniel&nbsp;Huber</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1208598/1208598_10211498_1515740112506_iw--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208598/1208598_10211498_1515740112506_iw--rgov-800width.jpg" title="Floor light mosaic"><img src="/por/images/Reports/POR/2018/1208598/1208598_10211498_1515740112506_iw--rgov-66x44.jpg" alt="Floor light mosaic"></a> <div class="imageCaptionContainer"> <div class="imageCaption">We evaluated different methods of  feedback and found that embedded feedback improved collaboration and efficiency for tasks involving robots and people.</div> <div class="imageCredit">Daniel Huber</div> <div class="imageSubmitted">Daniel&nbsp;Huber</div> <div class="imageTitle">Floor light mosaic</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1208598/1208598_10211498_1515739965373_iw2--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208598/1208598_10211498_1515739965373_iw2--rgov-800width.jpg" title="The Intelligent Workcell"><img src="/por/images/Reports/POR/2018/1208598/1208598_10211498_1515739965373_iw2--rgov-66x44.jpg" alt="The Intelligent Workcell"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Intelligent Workcell consists of an industrial robot, a safety monitoring system that detects people and objects in the workspace, and a feedback mechanism for determining what the robot is trying to do.</div> <div class="imageCredit">Daniel Huber</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;Huber</div> <div class="imageTitle">The Intelligent Workcell</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Our research objective is to revolutionize the manner in which industrial robots and people interact by allowing them to work safely together within the same workspace simultaneously. Current robotic manufacturing practice requires the physical separation of people and robots, which ensures safety, but is inefficient in terms of time and resources, and limits the tasks suitable for robotic manufacturing.     The Intelligent Workcell is our vision of the robotic workcell of the future. It uses automated perception systems to observe workers within the robot?s workspace, allowing for dynamically adaptive safety zones; it uses intelligence to understand what the workers are doing, allowing it to monitor the workers? tasks and plan accordingly; and it provides feedback to the worker to indicate what the robot is intending to do. In the Intelligent Workcell, robots and people will work together seamlessly and safely. The Intelligent Workcell builds upon our experience developing the Intelligent Monitoring of Assembly Operations (IMAO) system, which uses multiple 3D sensors to detect moving objects in a workcell and establish real-time dynamic safety zones surrounding the robot and the workers. If the safety zones intersect, the robot stops until the problem is resolved. The IMAO system is limited in that it does not explicitly recognize or track people, it does not understand what the people are doing, and it does not provide feedback about the robot?s intentions.    We have made advances in the following areas:    Intelligent Workcell Prototype. The first prototype of an Intelligent Workcell was designed and implemented as a foundational component of this project.  We obtained an industrial robot and integrated it with the sensor suite and hybrid safety system algorithms developed for the IMAO system. The workcell was further augmented with embedded feedback mechanisms, including a custom-designed illuminated floor mosaic, programmable lighting on the robot manipulator, and synthetic robot motion visualization via an external display.    3D Sensor Network Calibration. We developed a novel method to calibrate a network of 3D sensors that is easy to use by an untrained operator. The method is suitable for deploying in a factory environment where the Intelligent Workcell operates. The operator simply waves the calibration sphere around the workcell, and the algorithm automatically estimates the positions of all cameras relative to the workcell.    3D Pose Estimation. A core component of the perceptual model is the ability to detect one or more humans and track their 3D body postures over time. We have made several improvements to the state-of-the-art in this space. We devised a 3D pose pipeline that reasoned about 2D poses at an intermediate stage, and that tracks multiple people across occlusions through efficient and online combinatorial algorithms.    Models of Human Activity. We have developed various structured policies which can be used to model both the partonomic and cyclic nature of agent activities. In addition we have develop novel methods for understanding nonverbal behavior signals, in particular head gestures, to enable co-robot systems to work more seamlessly with people.    Robot Intent Feedback.  We have shown empirically that feedback (floor-based and arm-mounted lights) improves collaboration for human-robot assembly tasks. Results show that participants completing tasks with the benefit of feedback exhibited a smaller average number of wait periods, shorter average wait periods, and fewer robot blockages.    Together, these advances serve as a proof of concept that the Intelligent Workcell provides valuable advantages over today?s conventional industrial workcells.  The foundational algorithms in sensor calibration, pose estimation, tracking, and activity monitoring will extend this project?s impact beyond industrial robotics into wide-ranging domains involving the interaction of people and robots.       Last Modified: 01/15/2018       Submitted by: Daniel Huber]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
