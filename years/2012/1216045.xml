<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Efficient Bayesian Learning from Stochastic Gradients</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The total volume of data was estimated to be 0.8 Zettabytes in 2009 (1 Zettabyte = 1 trillion gigabytes) and predicted to grow to a staggering 35 Zettabytes in 2020, doubling every two years. Therefore, one of the primary challenges for machine learning is to develop statistically principled methods that will scale up to very large datasets. Moreover, we would like to (efficiently) learn highly complex models without the worry of overfitting and with confidence levels on our predictions. While Bayesian methods satisfy these latter desiderata, the current state-of-the-art inference procedures based on Markov Chain Monte Carlo (MCMC) posterior sampling do not meet the "big-data" challenge.&lt;br/&gt;&lt;br/&gt;We propose a new family of MCMC procedures that typically requires only a few hundred data-cases per update. These "stochastic gradient MCMC samplers" inherit the efficiencies of stochastic approximation methods, but will asymptotically sample from the correct posterior distribution. This endows this family of methods with an "anytime" property, namely that one can sample cheaply from a rough approximation of the posterior but can obtain more accurate samples in exchange for more computation.&lt;br/&gt;&lt;br/&gt;We believe this new class of methods will for the first time unlock the full strength of Bayesian methods for very large datasets. Due to their highly practical nature, the techniques developed under this grant are likely to gain widespread acceptance across a broad spectrum of academic disciplines as well as in industry. To expedite the transfer process we will publish open source software on our webpages and collaborate with a company (ID Analytics) to work on realistic, large scale inference problems. Two students at the University of California, Irvine (UCI) will be employed on this grant who will collaborate with a number of students and postdocs in the UK (University of Oxford and University of Bristol). UCI and UK students will also be exchanged for a few weeks a year to cross-fertilize research and to gain international experience. Research results from this grant will be integrated into artificial intelligence and machine learning courses at UCI through class projects.</AbstractNarration>
<MinAmdLetterDate>08/30/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/30/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1216045</AwardID>
<Investigator>
<FirstName>Max</FirstName>
<LastName>Welling</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Max Welling</PI_FULL_NAME>
<EmailAddress>welling@ics.uci.edu</EmailAddress>
<PI_PHON>9498244768</PI_PHON>
<NSF_ID>000487522</NSF_ID>
<StartDate>08/30/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Babak</FirstName>
<LastName>Shahbaba</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Babak Shahbaba</PI_FULL_NAME>
<EmailAddress>babaks@uci.edu</EmailAddress>
<PI_PHON>9492028624</PI_PHON>
<NSF_ID>000537349</NSF_ID>
<StartDate>08/30/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926973425</ZipCode>
<StreetAddress><![CDATA[5171 California Avenue, Ste 150]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Massive datasets have created exciting new opportunities yet have imposed new challenges for the scientific community. These new data-intensive problems are especially challenging for Bayesian methods, which are robust but typically involve intractable models that rely on computationally intensive simulations for their implementation. While existing algorithms might be effective for simple problems, they can be very inefficient for complex, high-dimensional problems. To address this issue, we have explored various subsampling strategies, i.e., restricting computations involved in Bayesian inference to random subsets of the observed data. This is based on the idea that big datasets contain a large amount of redundancy, so the overall information can be retrieved from a small subset. Using these strategies, we were able to reduce the computational cost of Bayesian methods so they can be applied to big data problems. Our methods are currently considered as the state of the art in machine learning and have created a new active research community around the topic of "stochastic gradient MCMC algorithms", with many other research groups also contributing.&nbsp;Throughout our research, we created an environment for training and mentoring the next generation of computer scientists and statisticians by supervising graduated students (who are now active members of academia and industry) and organizing workshops.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/21/2016<br>      Modified by: Max&nbsp;Welling</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Massive datasets have created exciting new opportunities yet have imposed new challenges for the scientific community. These new data-intensive problems are especially challenging for Bayesian methods, which are robust but typically involve intractable models that rely on computationally intensive simulations for their implementation. While existing algorithms might be effective for simple problems, they can be very inefficient for complex, high-dimensional problems. To address this issue, we have explored various subsampling strategies, i.e., restricting computations involved in Bayesian inference to random subsets of the observed data. This is based on the idea that big datasets contain a large amount of redundancy, so the overall information can be retrieved from a small subset. Using these strategies, we were able to reduce the computational cost of Bayesian methods so they can be applied to big data problems. Our methods are currently considered as the state of the art in machine learning and have created a new active research community around the topic of "stochastic gradient MCMC algorithms", with many other research groups also contributing. Throughout our research, we created an environment for training and mentoring the next generation of computer scientists and statisticians by supervising graduated students (who are now active members of academia and industry) and organizing workshops.           Last Modified: 12/21/2016       Submitted by: Max Welling]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
