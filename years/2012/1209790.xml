<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NSF East Asia and Pacific Summer Institute for FY 2012 in China</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2012</AwardEffectiveDate>
<AwardExpirationDate>05/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>5836.00</AwardTotalIntnAmount>
<AwardAmount>5836</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This action funds Sean C. Reber of Kent State University to conduct a research project, entitled "Computational feature-based texture synthesis on flow fields," during the summer of 2012 at the Shenzhen Institute of Advanced Integration Technology in Shenzhen, Guangdong Province, China.  The host scientist is George Chen.&lt;br/&gt;&lt;br/&gt;The Intellectual Merit of the research project is producing realistic fluid representation and animation in movies, games and other digital media and assisting in visualizing and analyzing flow data or vector fields for scientific applications.  The goal of the research is to improve upon the existing methods of Lagrangian based texture advection, using a Poisson-disk distribution of particles, and optimization based texture synthesis by adapting them with structural features extracted from flow fields using finite-time Lyapunov exponents (FTLE).  &lt;br/&gt;&lt;br/&gt;The Broader Impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce.</AbstractNarration>
<MinAmdLetterDate>05/23/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/23/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1209790</AwardID>
<Investigator>
<FirstName>Sean</FirstName>
<LastName>Reber</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sean C Reber</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>3306035542</PI_PHON>
<NSF_ID>000606036</NSF_ID>
<StartDate>05/23/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Reber                   Sean           C</Name>
<CityName>North Canton</CityName>
<ZipCode>447208132</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH16</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Reber                   Sean           C]]></Name>
<CityName>North Canton</CityName>
<StateCode>OH</StateCode>
<ZipCode>447208132</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5936</Code>
<Text>GERMANY (F.R.G.)</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<ProgramReference>
<Code>9200</Code>
<Text>US CHINA COOP IN BASIC SCIENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~5836</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="margin-bottom: 0in;"><strong>Sean C Reber</strong></p> <p style="margin-bottom: 0in;"><strong>Project Outcomes Report (POR)</strong></p> <p style="margin-bottom: 0in;"><strong>EASPI China 2012</strong></p> <p style="margin-bottom: 0in; line-height: 100%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 100%;"><span style="font-family: Times New Roman,serif;"><span style="font-size: small;"> The goal of my research was to implement a distributed and parallel version of the Lattice Boltzmann method of fluid simulation over the Hadoop open-source framework for distributed computing provided by Apache. The result would be improved speed of the LBM algorithm operating over an adaptable and distributed cloud computing environment.</span></span></p> <p style="margin-bottom: 0in; line-height: 100%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 100%;"><span style="font-family: Times New Roman,serif;"><span style="font-size: small;"> To give a little background, the Lattice Boltzmann method (LBM) is a computational fluid simulation method which doesn't solve the Navier-Stokes equations, instead taking advantage of the fact that the collective behavior of microscopic particles results in the macroscopic dynamics of a fluid. LBM achieves this using a regular grid with individual particle packets moving on a discrete lattice within each cell on the grid, at discrete time steps. The interaction rules between cells on the grid are defined such that the conservation of mass and momentum at each grid cell is satisfied, thus globally satisfying the macroscopic Navier-Stokes equations. LBM is known to help with handling boundary conditions between the simulated fluid and objects that may be present in the simulation, and is also known to be favorable to parallel computing implementations due to the linear and regular calculations at each local cell of the grid. The process of LBM can be thought of generally as having two steps for each cell on the grid, a collision step where the collision operations and equilibrium packet distributions are calculated for that cell and an update step where these new values are propagated to all neighboring cells.</span></span></p> <p style="margin-bottom: 0in; line-height: 100%;">&nbsp;</p> <p style="margin-bottom: 0in; line-height: 100%;"><span style="font-family: Times New Roman,serif;"><span style="font-size: small;"> The Hadoop framework involves a two-step programming model known as MapReduce.Essentially this model is based on functional programming concepts of turning lists of input data into lists of output data. This is done by 'mapping' the input list across various nodes throughout a cluster of machines from a master node, performing operations on them, and then collecting the answers to these sub-problems and 'reducing' them into the final output. This concept can be applied to the LBM algorithm by considering each cell of the grid, for maximum parallelism, as an input element of a list.  The necessary calculations could then be carried out at each node in the cloud and the reducing function would aggregate the updated values of the particle packet distributions and then propagate them to all neighboring cells, completing one time step. While this process is highly parallel in the collision step, it suffers from high communication overhead which may outweigh the benefit of the speed increase from the parallel processing.  Due to Hadoop's forced process isolation on each cluster node, particle packet distributions can only be propagated to neighboring cells after the results are aggregated in the reduce phase. This means that at most one time step can be calculated for each map and reduce cycle.  This large communication overhead is very costly, as at each time step each cell of the grid needs to be transmitted to the individual nodes of the cluster from the master node in the map phase and retrieved back to the master...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Sean C Reber Project Outcomes Report (POR) EASPI China 2012    The goal of my research was to implement a distributed and parallel version of the Lattice Boltzmann method of fluid simulation over the Hadoop open-source framework for distributed computing provided by Apache. The result would be improved speed of the LBM algorithm operating over an adaptable and distributed cloud computing environment.    To give a little background, the Lattice Boltzmann method (LBM) is a computational fluid simulation method which doesn't solve the Navier-Stokes equations, instead taking advantage of the fact that the collective behavior of microscopic particles results in the macroscopic dynamics of a fluid. LBM achieves this using a regular grid with individual particle packets moving on a discrete lattice within each cell on the grid, at discrete time steps. The interaction rules between cells on the grid are defined such that the conservation of mass and momentum at each grid cell is satisfied, thus globally satisfying the macroscopic Navier-Stokes equations. LBM is known to help with handling boundary conditions between the simulated fluid and objects that may be present in the simulation, and is also known to be favorable to parallel computing implementations due to the linear and regular calculations at each local cell of the grid. The process of LBM can be thought of generally as having two steps for each cell on the grid, a collision step where the collision operations and equilibrium packet distributions are calculated for that cell and an update step where these new values are propagated to all neighboring cells.    The Hadoop framework involves a two-step programming model known as MapReduce.Essentially this model is based on functional programming concepts of turning lists of input data into lists of output data. This is done by 'mapping' the input list across various nodes throughout a cluster of machines from a master node, performing operations on them, and then collecting the answers to these sub-problems and 'reducing' them into the final output. This concept can be applied to the LBM algorithm by considering each cell of the grid, for maximum parallelism, as an input element of a list.  The necessary calculations could then be carried out at each node in the cloud and the reducing function would aggregate the updated values of the particle packet distributions and then propagate them to all neighboring cells, completing one time step. While this process is highly parallel in the collision step, it suffers from high communication overhead which may outweigh the benefit of the speed increase from the parallel processing.  Due to Hadoop's forced process isolation on each cluster node, particle packet distributions can only be propagated to neighboring cells after the results are aggregated in the reduce phase. This means that at most one time step can be calculated for each map and reduce cycle.  This large communication overhead is very costly, as at each time step each cell of the grid needs to be transmitted to the individual nodes of the cluster from the master node in the map phase and retrieved back to the master node in the reduce phase. The amount of data transferred at each time step would be near the size of the data structure storing the grid. In the end it becomes a question of how does the speed increase from doing the calculations in parallel compare to the overhead of the network data transfer between nodes.    It was later decided to try to implement another popular graphics algorithm over Hadoop, the mesh subdivision algorithm. This algorithm was chosen due to it having less potential network overhead and being inherently simpler than the LBM algorithm, whose complexity was making determining performance metrics over a cluster difficult. Early tests show that at least for the mesh subdivision algorithm the speed increase from the parallel computational outweighs the network overhead, however these are very prel...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
