<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Software Institute for Abstractions and Methodologies for HPC Simulation Codes on Future Architectures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2012</AwardEffectiveDate>
<AwardExpirationDate>02/28/2015</AwardExpirationDate>
<AwardTotalIntnAmount>146719.00</AwardTotalIntnAmount>
<AwardAmount>146719</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Large, complex, multi-scale, multi-physics simulation codes, running on high performance computing (HPC) platforms, are essential to advancing science and engineering research in disciplines such as lattice field theory, astrophysics and cosmology, computational fluid dynamics/fluid structure interaction,and high energy density physics. Progress in computational science together with the adoption of high-level frameworks and modular development have produced widely used community simulation software specific to individual communities. These state-of-the-art codes have been under development and optimization for several years and currently simulate multi-scale, multi-physics phenomena with unprecedented fidelity on petascale platforms. Currently each of these codes have solvers with varied performance characteristics, but all face challenges because of changing hardware architecture. Efforts underway to cope with these challenges, are largely fragmented. While it is true that the scientific codes used in various domains differ significantly from one another, many solutions are likely to be conceptually similar, even if they differ in details. The goal of the proposed conceptualization project, Software Institute for Methodologies and Abstractions for Codes (SIMAC) is to find common abstractions and frameworks applicable across a broad range of applications through cooperation, coordination and interdisciplinary interactions among the participants. The core group of participating codes includes FLASH (astrophysics, cosmology, CFD, HEDP), Cactus (CFD, numerical relativity, and quantum relativity), the code suite used by the Lattice QCD community, and Enzo (cosmology). &lt;br/&gt;&lt;br/&gt;The proposed collaborative research will produce benefit beyond the four simulation codes and collaborating institutions by exploring: a common software infrastructure applicable to a broad range of science and engineering application domains; an engagement model between computer science research and application development; a multidisciplinary immersion program for research, education and training of students, postdoctoral fellows and visitors on future platform architectures.</AbstractNarration>
<MinAmdLetterDate>09/10/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1228696</AwardID>
<Investigator>
<FirstName>Rajeev</FirstName>
<LastName>Thakur</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rajeev S Thakur</PI_FULL_NAME>
<EmailAddress>thakur@anl.gov</EmailAddress>
<PI_PHON>6302527847</PI_PHON>
<NSF_ID>000120835</NSF_ID>
<StartDate>09/10/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Hovland</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Hovland</PI_FULL_NAME>
<EmailAddress>hovland@cs.uchicago.edu</EmailAddress>
<PI_PHON>6302526384</PI_PHON>
<NSF_ID>000340999</NSF_ID>
<StartDate>09/10/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Donald</FirstName>
<LastName>Lamb</LastName>
<PI_MID_INIT>Q</PI_MID_INIT>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>Donald Q Lamb</PI_FULL_NAME>
<EmailAddress>lamb@oddjob.uchicago.edu</EmailAddress>
<PI_PHON>7737027194</PI_PHON>
<NSF_ID>000499379</NSF_ID>
<StartDate>09/10/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anshu</FirstName>
<LastName>Dubey</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anshu Dubey</PI_FULL_NAME>
<EmailAddress>dubey@flash.uchicago.edu</EmailAddress>
<PI_PHON>7738342999</PI_PHON>
<NSF_ID>000349899</NSF_ID>
<StartDate>09/10/2012</StartDate>
<EndDate>03/07/2013</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Petros</FirstName>
<LastName>Tzeferacos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Petros Tzeferacos</PI_FULL_NAME>
<EmailAddress>p.tzeferacos@rochester.edu</EmailAddress>
<PI_PHON>5852756751</PI_PHON>
<NSF_ID>000656470</NSF_ID>
<StartDate>01/31/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Boyana</FirstName>
<LastName>Norris</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Boyana Norris</PI_FULL_NAME>
<EmailAddress>norris@cs.uoregon.edu</EmailAddress>
<PI_PHON>5413464413</PI_PHON>
<NSF_ID>000385007</NSF_ID>
<StartDate>01/08/2014</StartDate>
<EndDate>01/31/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Boyana</FirstName>
<LastName>Norris</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Boyana Norris</PI_FULL_NAME>
<EmailAddress>norris@cs.uoregon.edu</EmailAddress>
<PI_PHON>5413464413</PI_PHON>
<NSF_ID>000385007</NSF_ID>
<StartDate>09/10/2012</StartDate>
<EndDate>03/07/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606371433</ZipCode>
<StreetAddress><![CDATA[5735 S Ellis Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<ProgramReference>
<Code>8211</Code>
<Text>S2I2 - Scient Sftwre Innovat Insts</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~146719</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Software Institute for Abstractions and Methodologies for HPC Simulations Codes on Future Architectures is a project to conceptualize a crosscutting Software Institute, which can serve a large collection of communities that rely upon simulations for advancing their science. Large, complex, multi-scale, multi-physics simulation codes, running on high performance computing (HPC) platforms, have become essential to advancing science and engineering in many fields. Codes used by these communities simulate multi-scale, multi-physics phenomena with unprecedented fidelity on petascale platforms. Increasing the capabilities of these codes and maintaining their ability to run on future platforms are as crucial to these communities as continued improvements in instruments and facilities are to experimental scientists. However, computing as it has been practiced over the last couple of decades is nearing its end. Cluster computing with uniform node architecture and distributed memory programming model can no longer deliver the kind of advancement in computing capabilities that are increasingly demanded by the scientific simulations.</p> <p>Even though most research communities have become aware of the upcoming crisis in computing, they are all but paralyzed from taking any action for two reasons: (1) the technologies that could address some of the challenges faced are still in research stages and it is very difficult to tell which ones of these will deliver, and (2) there is very little knowledge about how to go about transforming the codes to make use of these possibly helpful technologies. The challenges facing us are daunting, but finding a way forward is not only crucial, it is also urgent. The findings of our workshops and the general community input tells us that an Institute that is prepared to lay the foundations for transitioning and sustaining the important software artifacts of research communities will be welcome by many research communities. This is especially true of the communities that rely upon multi-component multi-physics codes operating upon HPC resources for computing to tackle the Grand Challenge problems in their respective fields, such as Astrophysics, Cosmology, Relativistic Astrophysics, High Energy Density Physics, lattice QCD, Computational Fluid Dynamics, Chemical Combustion and Plasma Physics. The re-factorization and code transformation that are needed are at the fundamental implementation design level in codes and libraries alike. The data layout, the wrapper layers, and the communication channels between different code components have to be designed with an awareness of the semantics of the programming abstractions using asynchronous task management and code transformations, enhancing also reliability and resiliency of the code. It is imperative that support is provided for refactoring of the mature codes that are already serving their communities well, as the changes to the architecture of most codes will be highly disruptive and therefore labor intensive.</p> <p><strong>Intellectual Merit</strong>: The conceptualized Software Institute will provide a cross-disciplinary hub around which the computational communities can rally. The Institute must first take the pioneering role of tackling the challenges of code transformation for a select set of codes. The experimentation, prototype building, and outcomes must be meticulously documented for future use. The tools, developed internally and externally and utilized in the process, should become a part of the repository along with the knowledge base for dissemination by the Institute. The Institute needs to affect a wide variety of disciplines and engage domain experts. This can be accomplished by embedding said experts in the Institute, initially with a longer-term prospect and be active participants in developing the transition process. Later the participation from domain experts can become m...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Software Institute for Abstractions and Methodologies for HPC Simulations Codes on Future Architectures is a project to conceptualize a crosscutting Software Institute, which can serve a large collection of communities that rely upon simulations for advancing their science. Large, complex, multi-scale, multi-physics simulation codes, running on high performance computing (HPC) platforms, have become essential to advancing science and engineering in many fields. Codes used by these communities simulate multi-scale, multi-physics phenomena with unprecedented fidelity on petascale platforms. Increasing the capabilities of these codes and maintaining their ability to run on future platforms are as crucial to these communities as continued improvements in instruments and facilities are to experimental scientists. However, computing as it has been practiced over the last couple of decades is nearing its end. Cluster computing with uniform node architecture and distributed memory programming model can no longer deliver the kind of advancement in computing capabilities that are increasingly demanded by the scientific simulations.  Even though most research communities have become aware of the upcoming crisis in computing, they are all but paralyzed from taking any action for two reasons: (1) the technologies that could address some of the challenges faced are still in research stages and it is very difficult to tell which ones of these will deliver, and (2) there is very little knowledge about how to go about transforming the codes to make use of these possibly helpful technologies. The challenges facing us are daunting, but finding a way forward is not only crucial, it is also urgent. The findings of our workshops and the general community input tells us that an Institute that is prepared to lay the foundations for transitioning and sustaining the important software artifacts of research communities will be welcome by many research communities. This is especially true of the communities that rely upon multi-component multi-physics codes operating upon HPC resources for computing to tackle the Grand Challenge problems in their respective fields, such as Astrophysics, Cosmology, Relativistic Astrophysics, High Energy Density Physics, lattice QCD, Computational Fluid Dynamics, Chemical Combustion and Plasma Physics. The re-factorization and code transformation that are needed are at the fundamental implementation design level in codes and libraries alike. The data layout, the wrapper layers, and the communication channels between different code components have to be designed with an awareness of the semantics of the programming abstractions using asynchronous task management and code transformations, enhancing also reliability and resiliency of the code. It is imperative that support is provided for refactoring of the mature codes that are already serving their communities well, as the changes to the architecture of most codes will be highly disruptive and therefore labor intensive.  Intellectual Merit: The conceptualized Software Institute will provide a cross-disciplinary hub around which the computational communities can rally. The Institute must first take the pioneering role of tackling the challenges of code transformation for a select set of codes. The experimentation, prototype building, and outcomes must be meticulously documented for future use. The tools, developed internally and externally and utilized in the process, should become a part of the repository along with the knowledge base for dissemination by the Institute. The Institute needs to affect a wide variety of disciplines and engage domain experts. This can be accomplished by embedding said experts in the Institute, initially with a longer-term prospect and be active participants in developing the transition process. Later the participation from domain experts can become more transitory, on an "as needed" basis. Broader Impact:  The workshops were invaluable in ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
