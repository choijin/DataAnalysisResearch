<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI-Large: Collaborative Research: Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>533332.00</AwardTotalIntnAmount>
<AwardAmount>533332</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In order for robots to collaborate with humans, they need to be able to accurately forecast human intent and action.  People act with purpose: that is, they make sequences of decisions to achieve long-term objectives. For instance, in driving from home to a store, people carefully plan a sequence of roads that will get them there efficiently. In predicting a person's next decision, algorithms must be developed that reflect these purposeful actions. &lt;br/&gt;&lt;br/&gt;Currently, robots are unable to anticipate human needs and goals, and this represents a fundamental barrier to their large-scale deployment in the home and workplace. The aim of this project is to develop a new science of purposeful prediction that can be applied to human-robot interaction across a wide variety of domains.  The work draws on recent techniques based on Inverse Optimal Control and Inverse Equilibria Theory that enable statistically sound reasoning about observed deliberate behavior. These new methods provide the foundations of a theoretical framework that integrates traditional decision making techniques like optimal control, search and planning with probabilistic methods that reason about uncertainty and hidden information, particularly about goals, utility and intent. &lt;br/&gt;&lt;br/&gt;Intellectual merit:  The project will provide a general framework that allows robots to anticipate and adapt to the activities of their human co-workers based on perceptual cues. The investigators will develop the theory, a computational toolbox, and, in collaboration with industrial partners, prototype deployments of these new methods for the prediction of peoples' behavior in a diverse set of robotics domains from computer vision to motor control. The project is transformative in that it combines a novel theoretical/algorithmic framework with extensive support in terms of volume of data and validation infrastructure in the context of many applications.        &lt;br/&gt;&lt;br/&gt;Broader impacts: A revolution in personal robotics in both the home and workplace depends on the ability to forecast human activities and intents; small- and medium- scale manufacturing will make a leap forward through agile robotic systems intelligent enough to understand and assist their co-workers in flexible assembly tasks; and robust models of pedestrian and vehicular traffic flow will enable more effective driver warning systems and safer autonomous mobile robots. Purposeful prediction technology is an important step towards enabling such understanding of actions and intents in these arenas. The research work will involve the training and mentoring of undergraduate, masters and doctoral students as well as post-doctoral fellows in this emerging multi-disciplinary research area at the intersection of computer and cognitive sciences and robotics.</AbstractNarration>
<MinAmdLetterDate>09/10/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/08/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1227234</AwardID>
<Investigator>
<FirstName>Dieter</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dieter Fox</PI_FULL_NAME>
<EmailAddress>fox@cs.washington.edu</EmailAddress>
<PI_PHON>2066852517</PI_PHON>
<NSF_ID>000210667</NSF_ID>
<StartDate>09/10/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~133333</FUND_OBLG>
<FUND_OBLG>2013~133333</FUND_OBLG>
<FUND_OBLG>2014~266666</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span><span style="font-size: small;">The main goal of this project was to develop techniques that enable robots to naturally and robustly interact with people and their environment. Over the course of the project, we investigated how robot manipulators can learn specific behaviors from human demonstrations and how they can learn to predict their environment from raw visual experience.</span></span></p> <p><span style="font-family: Times New Roman; font-size: small;">&nbsp;</span><span><span style="font-size: small;">The idea behind the first part was to leverage the fact that humans are great at demonstrating a desired behavior, but not so good at precisely specifying how such behavior can be programmed. Here, we developed variants of inverse optimal control, where a human shows how a robot manipulator should move specific objects, and the robot learns cost functions that let it replicate the demonstrated behavior. For instance, just by showing it some example tasks, our robot was able to learn that cups filled with liquid should be moved in an upright position while avoiding to move them above electronic equipment. Ultimately, this line of research will greatly enhance robot capabilities to learn from humans in a natural way, with applications in smart manufacturing, service robotics, and home care.</span></span></p> <p><span style="font-family: Times New Roman; font-size: small;">&nbsp;</span><span><span style="font-size: small;">Motivated by this first project, graduate student Arunkumar Byravan started to investigate how robots can learn to predict what happens in their environments solely based on their own experiences. This is a hard problem since robot experiences, such as videos of images observing a scene, are very high dimensional. Building on recent advances in deep learning, we developed a framework that can learn to predict the motion of objects based on physical interaction, such as a robot manipulator pushing an object on a table. Rather than using a vanilla deep learning approach, we developed SE3-Nets, which leverage our knowledge about the physical motion of objects. By incorporating physics-based constraints into the network structure, SE3-Nets were able to learn more accurate prediction models from less data.&nbsp; We also demonstrated that the models learned by SE3-Nets can be used to control robot manipulators to desired target configurations.&nbsp; Ultimately, this work will help robots to operate more robustly in their environments since the learned models are based on the robots' own experiences, rather than pre-designed by a person. Furthermore, this research has intriguing connections to behavioral developmental science, since it indicates that it is possible to learn complex motion skills from experiences of raw perception and control signals.</span></span></p> <p><strong>&nbsp;</strong><em>&nbsp;</em></p> <p>&nbsp;</p><br> <p>            Last Modified: 01/08/2018<br>      Modified by: Dieter&nbsp;Fox</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main goal of this project was to develop techniques that enable robots to naturally and robustly interact with people and their environment. Over the course of the project, we investigated how robot manipulators can learn specific behaviors from human demonstrations and how they can learn to predict their environment from raw visual experience.   The idea behind the first part was to leverage the fact that humans are great at demonstrating a desired behavior, but not so good at precisely specifying how such behavior can be programmed. Here, we developed variants of inverse optimal control, where a human shows how a robot manipulator should move specific objects, and the robot learns cost functions that let it replicate the demonstrated behavior. For instance, just by showing it some example tasks, our robot was able to learn that cups filled with liquid should be moved in an upright position while avoiding to move them above electronic equipment. Ultimately, this line of research will greatly enhance robot capabilities to learn from humans in a natural way, with applications in smart manufacturing, service robotics, and home care.   Motivated by this first project, graduate student Arunkumar Byravan started to investigate how robots can learn to predict what happens in their environments solely based on their own experiences. This is a hard problem since robot experiences, such as videos of images observing a scene, are very high dimensional. Building on recent advances in deep learning, we developed a framework that can learn to predict the motion of objects based on physical interaction, such as a robot manipulator pushing an object on a table. Rather than using a vanilla deep learning approach, we developed SE3-Nets, which leverage our knowledge about the physical motion of objects. By incorporating physics-based constraints into the network structure, SE3-Nets were able to learn more accurate prediction models from less data.  We also demonstrated that the models learned by SE3-Nets can be used to control robot manipulators to desired target configurations.  Ultimately, this work will help robots to operate more robustly in their environments since the learned models are based on the robots' own experiences, rather than pre-designed by a person. Furthermore, this research has intriguing connections to behavioral developmental science, since it indicates that it is possible to learn complex motion skills from experiences of raw perception and control signals.              Last Modified: 01/08/2018       Submitted by: Dieter Fox]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
