<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Preliminary Study of Hashing Algorithms for Large-Scale Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many emerging applications of data mining call for techniques that can deal with data instances with millions, if not billions of dimensions. Hence, there is a need for effective approaches to dealing with extremely high dimensional data sets. &lt;br/&gt;&lt;br/&gt;This project focuses on a class of novel theoretically well-founded hashing algorithms that allow high dimensional data to be encoded in a form that can be efficiently processed by standard machine learning algorithms. Specifically, it explores: One-permutation hashing, to dramatically reduce the computational and energy cost of hashing; Sparsity-preserving hashing, to take advantage of  data sparsity for efficient data storage and improved generalization; Application of the new hashing techniques with standard algorithms for learning  "linear"  separators in high dimensional spaces. The  success of this EAGER project could lay the foundations of a longer-term research agenda by the PI and other investigators focused on developing effective methods for building predictive models from extremely high dimensional data using "standard" machine learning algorithms. &lt;br/&gt;&lt;br/&gt;Broader Impacts: Effective approaches to building predictive models from  extremely high dimensional data can impact many areas of science that rely on machine learning as the primary methodology for knowledge acquisition from data. The PI's education and outreach efforts aim to  broaden the participation of women and underrepresented groups. The publications, software, and datasets resulting from the project will be freely disseminated to the larger scientific community.</AbstractNarration>
<MinAmdLetterDate>08/18/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1249316</AwardID>
<Investigator>
<FirstName>Ping</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ping Li</PI_FULL_NAME>
<EmailAddress>pingli@stat.rutgers.edu</EmailAddress>
<PI_PHON>8484457667</PI_PHON>
<NSF_ID>000083097</NSF_ID>
<StartDate>08/18/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148537501</ZipCode>
<StreetAddress><![CDATA[4130 Upson Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has focused on developing novel, practical, and mathematically rigorous hashing algorithms&nbsp; for processing&nbsp; massive and extremely high-dimensional data, which are common in morden applications, for example, search engines. The core of the project is the development of "one permutation hashing", which is a safe replacement of "minwise hashing", the standard data processing algorithm used in search.</p> <p>Minwise hashing has been a popular tool for efficiently computing data similarities using small storage space. It is also a standard indexing tool to organize data in a way that allows searching for nearest neighbors in sublinear time (i.e., faster than linear scans).The drawback of minwise hsahing is that it needs to permute the data many (hundreds or thousands of) times. This preprocessing cost typically dominates many subsequent operations such as search and learning. Even though the preprocessing can be trivially paralelized, it is not an engergy-efficient solution.</p> <p>One permutation hashing solves the problem by doing only ONE permutation on the data and exploiting statistical properities which were not studied in the past. This dramatically reduces the processing cost by a factor of hundreds or thousands, depending on applications. One permutation can be used for both learning (e.g., building SVMs or logistic regression models) and sublinear time near neighbor search (i.e., indexing). However, in order to use it for indexing sparse data, we must address a crucial issue, that is, the existence of "empty bins".</p> <p>Basically, one permutation hashing works by breaking the space (after permutation) into bins and keeping the smallest nonzero entry in each bin. When the data are sparse, empty bins are not avoidable, in fact they can&nbsp; dominate when the data are extremely sparse. Our solution to this&nbsp; problem is "denisified one permutation hashing", by borrowing the neigbhoring non-empty bins. We have shown that this approach is mathematically rigorous and we have successfully used the method for indexing &amp; sublinear time near neighbor search.</p> <p>In summary, this project is a successful in that we have developed "one permutation hashing" and "denisified one permutation" which are simple, practical, mathemetically rigorous, and&nbsp; can safely replace the standard data processing methods used in industry.</p><br> <p>            Last Modified: 11/27/2014<br>      Modified by: Ping&nbsp;Li</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has focused on developing novel, practical, and mathematically rigorous hashing algorithms  for processing  massive and extremely high-dimensional data, which are common in morden applications, for example, search engines. The core of the project is the development of "one permutation hashing", which is a safe replacement of "minwise hashing", the standard data processing algorithm used in search.  Minwise hashing has been a popular tool for efficiently computing data similarities using small storage space. It is also a standard indexing tool to organize data in a way that allows searching for nearest neighbors in sublinear time (i.e., faster than linear scans).The drawback of minwise hsahing is that it needs to permute the data many (hundreds or thousands of) times. This preprocessing cost typically dominates many subsequent operations such as search and learning. Even though the preprocessing can be trivially paralelized, it is not an engergy-efficient solution.  One permutation hashing solves the problem by doing only ONE permutation on the data and exploiting statistical properities which were not studied in the past. This dramatically reduces the processing cost by a factor of hundreds or thousands, depending on applications. One permutation can be used for both learning (e.g., building SVMs or logistic regression models) and sublinear time near neighbor search (i.e., indexing). However, in order to use it for indexing sparse data, we must address a crucial issue, that is, the existence of "empty bins".  Basically, one permutation hashing works by breaking the space (after permutation) into bins and keeping the smallest nonzero entry in each bin. When the data are sparse, empty bins are not avoidable, in fact they can  dominate when the data are extremely sparse. Our solution to this  problem is "denisified one permutation hashing", by borrowing the neigbhoring non-empty bins. We have shown that this approach is mathematically rigorous and we have successfully used the method for indexing &amp; sublinear time near neighbor search.  In summary, this project is a successful in that we have developed "one permutation hashing" and "denisified one permutation" which are simple, practical, mathemetically rigorous, and  can safely replace the standard data processing methods used in industry.       Last Modified: 11/27/2014       Submitted by: Ping Li]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
