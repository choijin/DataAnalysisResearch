<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Video Analytics in Large Heterogeneous Repositories</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>214195.00</AwardTotalIntnAmount>
<AwardAmount>214195</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The planned research will take video analysis indexing and retrieval in a new and promising direction. The research is driven by the need for intelligence analysts to be able to express video queries more efficiently than traditional relevance feedback and to be able to "provide more expressive queries that include "nouns" and "verbs" as they would with human language. While still constrained, the approach goes a long way toward bridging the gap between traditional relevance feedback based only on assumed relationships in the image, and full human language queries. The graduate students involved in the project will be required to publish in international conferences and journals and will likely use this research as a basis for their dissertations.  Other impacts of this work include the mentoring of graduate students and the inclusion of junior personnel in the management of the project. Research will be disseminated through local, national, and international meetings and journals. The team will also install a server and public interface for demonstration on existing datasets. The system will be accessible through the web on limited datasets and on the full dataset by request.</AbstractNarration>
<MinAmdLetterDate>09/13/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/13/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1262121</AwardID>
<Investigator>
<FirstName>Larry</FirstName>
<LastName>Davis</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Larry S Davis</PI_FULL_NAME>
<EmailAddress>lsd@umiacs.umd.edu</EmailAddress>
<PI_PHON>3014056718</PI_PHON>
<NSF_ID>000194186</NSF_ID>
<StartDate>09/13/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Doermann</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David S Doermann</PI_FULL_NAME>
<EmailAddress>doermann@buffalo.edu</EmailAddress>
<PI_PHON>7166451557</PI_PHON>
<NSF_ID>000230523</NSF_ID>
<StartDate>09/13/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland University College]]></Name>
<CityName>ADELPHI</CityName>
<StateCode>MD</StateCode>
<ZipCode>207838040</ZipCode>
<StreetAddress><![CDATA[3501 UNIVERSITY BLVD EAST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>L583</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>170E</Code>
<Text>Interagency Agreements</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~214195</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our research is motivated by the need to search very large databases of images using queries that contain images, text or a combination of images and text.&nbsp; We have focused on two general approaches to the problem:</p> <ol> <li>Image databases are massive.&nbsp; A single image from a camera is typically more than 5 MB in size.&nbsp; Computer vision researchers have been investigating methods to represent the visual information in images for decades.&nbsp; The most successful representations are actually many times larger in size than the original images!&nbsp; Image retrieval from such a database involves comparing the visual representation of the query image to the representations of the images in the database.&nbsp; However, such enormous visual representations make this impractical for a variety of reasons.&nbsp; So, our research investigated ways to create VERY compact representations that are as effective for retrieval as the original very large visual representations.&nbsp; The work is an extension of the famous locality sensitive hashing method.&nbsp; The idea is to reduce the large visual representation to only small number of bits.&nbsp; Each bit reflects whether or not the image has a particular characteristic or attribute &ndash; the attributes might be semantic (outdoor image or not, contains people or not), or could be purely statistical based on the distribution of visual representations.&nbsp; Often, the database images also come with some kind of textual description.&nbsp; This could be as simple as tags, or as complex as stories about the pictures (think newspaper articles).&nbsp; The queries might be just text, just images or some combination.&nbsp; The question we addressed is how to find compact representations for both the text and the images that are in a particular mathematical sense, consistent.&nbsp; We call this general problem &ndash; where data is represented in more than one way &ndash; predictable multi-view hashing, and our research involved developing and evaluating algorithms for solving such problems. </li> <li>There is generally no single visual representation that captures all intuitive notions of image similarity.&nbsp; So, one user might come to an image database system with a query image and is interested in images with similar color distributions.&nbsp; Another user might be interested in images with similar looking buildings.&nbsp; So, we developed a number of methods that integrate retrieval results based on more than one visual representation of the database images.&nbsp; In the image database field such methods are referred to as reranking methods.&nbsp; The first method involved first performing retrieval using each representation, and then forming a graph of all retrieved imaged based on their visual similarity.&nbsp; A diffusion process on the graph then effectively merged the retrievals into a single ranked list for the user.&nbsp; This was very effective, but expensive in computation.&nbsp; So, a second method using an efficient search algorithm by constraining the ranking score function to be sub-modular.&nbsp; Intuitively, this means that the algorithm can successively just find the next best image to show the user rather than having to consider possible combinations of images (and the number of combinations is enormous), yet the set returned to the user is almost as good as if the expensive combinatorial search had been run instead.&nbsp; </li> </ol><br> <p>            Last Modified: 02/11/2015<br>      Modified by: Larry&nbsp;S&nbsp;Davis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our research is motivated by the need to search very large databases of images using queries that contain images, text or a combination of images and text.  We have focused on two general approaches to the problem:  Image databases are massive.  A single image from a camera is typically more than 5 MB in size.  Computer vision researchers have been investigating methods to represent the visual information in images for decades.  The most successful representations are actually many times larger in size than the original images!  Image retrieval from such a database involves comparing the visual representation of the query image to the representations of the images in the database.  However, such enormous visual representations make this impractical for a variety of reasons.  So, our research investigated ways to create VERY compact representations that are as effective for retrieval as the original very large visual representations.  The work is an extension of the famous locality sensitive hashing method.  The idea is to reduce the large visual representation to only small number of bits.  Each bit reflects whether or not the image has a particular characteristic or attribute &ndash; the attributes might be semantic (outdoor image or not, contains people or not), or could be purely statistical based on the distribution of visual representations.  Often, the database images also come with some kind of textual description.  This could be as simple as tags, or as complex as stories about the pictures (think newspaper articles).  The queries might be just text, just images or some combination.  The question we addressed is how to find compact representations for both the text and the images that are in a particular mathematical sense, consistent.  We call this general problem &ndash; where data is represented in more than one way &ndash; predictable multi-view hashing, and our research involved developing and evaluating algorithms for solving such problems.  There is generally no single visual representation that captures all intuitive notions of image similarity.  So, one user might come to an image database system with a query image and is interested in images with similar color distributions.  Another user might be interested in images with similar looking buildings.  So, we developed a number of methods that integrate retrieval results based on more than one visual representation of the database images.  In the image database field such methods are referred to as reranking methods.  The first method involved first performing retrieval using each representation, and then forming a graph of all retrieved imaged based on their visual similarity.  A diffusion process on the graph then effectively merged the retrievals into a single ranked list for the user.  This was very effective, but expensive in computation.  So, a second method using an efficient search algorithm by constraining the ranking score function to be sub-modular.  Intuitively, this means that the algorithm can successively just find the next best image to show the user rather than having to consider possible combinations of images (and the number of combinations is enormous), yet the set returned to the user is almost as good as if the expensive combinatorial search had been run instead.          Last Modified: 02/11/2015       Submitted by: Larry S Davis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
