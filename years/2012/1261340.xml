<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Mobile Devices for Survey Data Collection</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>519492.00</AwardTotalIntnAmount>
<AwardAmount>601922</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will examine mobile devices, specifically smartphones and tablet computers, as vehicles for survey data collection.  The appeal of these devices for survey researchers is clear.  Because they are lightweight and relatively inexpensive, they make it easier to collect data using such existing survey modes as computer-assisted personal interviewing.  The research will examine three issues raised by use of such devices.  First, the input methods that these devices permit (such as touchscreen interfaces) are relatively unfamiliar to many users and may create response problems.  Although these interfaces are sometimes used on laptops, tablets and smartphones require them, making usability concerns more central.  Second, the screens on tablets and smartphones are considerably smaller than those on laptop or desktop computers.  Experiments on web surveys demonstrate the importance of "visual prominence."  Any information that respondents need to use should be immediately visible to them without their having to perform any action (such as a mouse click) to make the information visible.  Even the need for an eye movement may effectively render information invisible.  Because of the small screens on mobile devices, it may be much harder to make all of the potentially useful information visible to respondents than it is with a laptop or desktop computer.  The final issue is the perceived privacy of data collected on these devices.  Respondents are willing to reveal sensitive information about themselves when a computer administers the questions, and web surveys seem to retain the advantages of earlier methods of computerized self-administration.  But it is unclear whether respondents will display the same level of candor when the survey is administered over the Internet on a tablet computer or a smartphone.  Two realistic field experiments and a usability study will examine these issues.  Both experiments will be conducted in a single, face-to-face survey.  The first experiment will compare laptop computers with tablets and smartphones and will examine the effects of both screen size and input method on breakoffs, missing data, completion times, and indicators of the quality of the responses.  The second experiment will compare the same three data collection platforms as vehicles for collecting sensitive information.  The experiment will ask respondents to assess the sensitivity of the questions, because item sensitivity may vary as a function of the device used to collect the data.&lt;br/&gt;&lt;br/&gt;Surveys are a central tool for social scientists and policymakers in the United States, and survey research is a multi-billion dollar industry in the United States alone.  Any set of technological advances, such as the widespread adoption of smartphones and tablet computers, is likely to have a major impact on how surveys are done.  Although mobile devices will be widely used for surveys regardless of whether this research is done, the work will produce practical guidelines for using such devices to collect survey data and will alert survey researchers to some of the potential pitfalls of these devices.</AbstractNarration>
<MinAmdLetterDate>09/10/2013</MinAmdLetterDate>
<MaxAmdLetterDate>02/26/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1261340</AwardID>
<Investigator>
<FirstName>Roger</FirstName>
<LastName>Tourangeau</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Roger Tourangeau</PI_FULL_NAME>
<EmailAddress>RogerTourangeau@Westat.com</EmailAddress>
<PI_PHON>3012942828</PI_PHON>
<NSF_ID>000329370</NSF_ID>
<StartDate>09/10/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Westat Inc</Name>
<CityName>Rockville</CityName>
<ZipCode>208500319</ZipCode>
<PhoneNumber>3012511500</PhoneNumber>
<StreetAddress>1600 RESEARCH BLVD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049508120</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WESTAT, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049508120</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Westat Inc]]></Name>
<CityName>Rockville</CityName>
<StateCode>MD</StateCode>
<ZipCode>208503129</ZipCode>
<StreetAddress><![CDATA[1600 Research Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<ProgramElement>
<Code>8800</Code>
<Text>SCIENCE RESOURCES STATISTICS</Text>
</ProgramElement>
<ProgramElement>
<Code>MX17</Code>
<Text/>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~519492</FUND_OBLG>
<FUND_OBLG>2015~82430</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Web surveys have become a dominant, if not <em>the</em> dominant, mode of data collection in the United States and Europe.&nbsp; According to a recent book on web surveys (Tourangeau, Conrad, and Couper, 2013), web surveys generally have good measurement properties. However, virtually all of the work supporting their conclusion was based on web surveys done on laptop or desktop computers.&nbsp; However, more and more web surveys are being done by respondents on tablet computers and smartphones.&nbsp; Several prior studies have examined the potential effects of this switch from PCs to mobile devices for the completion of web surveys.&nbsp; The studies have looked at a range of outcomes, including completion rates (do people actually finish surveys they start on a smartphone or tablets or do they break off?) and item nonresponse (do they leave more items unanswered?).&nbsp;</p> <p>We carried out a field experiment that compared responses obtained by smartphones, tablets, and laptop computers, focusing on the potential effects of the different devices on <em>measurement</em> errors.&nbsp; We examined how the differences across devices in screen size (and the related need to scroll to see the entire question or the full set of response options) might increase the effects of response order, affect the strategy respondents used to decide which of two options was preferable, change the effect of question context, or influence respondents&rsquo; use of definitions.&nbsp; These experiments were based on the principle of visual prominence &mdash; the idea that respondents are more likely to notice and consider information that is easy to see.&nbsp; The experiments were deliberately designed to maximize the impact of screen size on the results, since the screen size would affect the visual prominence of important information.&nbsp; For example, it was harder for smartphone respondents to see the definitional information accompanying some of the questions because they had to scroll to see it.&nbsp; However, like many of the prior studies examining mobile devices, although we did find the expected effects of the order of the answer categories, question context, and evaluation strategy on the answers respondents gave, we find few differences across the three types of device.&nbsp; This is good news &mdash; the small screens of the smartphones did not make known measurement problems any worse.</p> <p>We also examined several other indicators of data quality, such as rates of missing data, the length of time it took for respondents to complete the survey, the correlations among related items, and the willingness of respondents to make embarrassing admissions about themselves (for example, that they used illicit drugs or drank heavily).&nbsp; Again, the effects of device type were minimal.&nbsp; Responses obtained on a smartphone seem to be just as good as those obtained on a tablet or laptop computer. &nbsp;And that is very good news for survey researchers, since many respondents will complete only complete web surveys if they can do it on their smartphones.</p> <p>&nbsp;</p> <p>Tourangeau, R., Conrad, F.G., &amp; Couper, M.P. (2013).&nbsp; <em>The Science of Web Surveys</em>.&nbsp; Oxford:&nbsp; Oxford University Press.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/12/2016<br>      Modified by: Roger&nbsp;Tourangeau</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Web surveys have become a dominant, if not the dominant, mode of data collection in the United States and Europe.  According to a recent book on web surveys (Tourangeau, Conrad, and Couper, 2013), web surveys generally have good measurement properties. However, virtually all of the work supporting their conclusion was based on web surveys done on laptop or desktop computers.  However, more and more web surveys are being done by respondents on tablet computers and smartphones.  Several prior studies have examined the potential effects of this switch from PCs to mobile devices for the completion of web surveys.  The studies have looked at a range of outcomes, including completion rates (do people actually finish surveys they start on a smartphone or tablets or do they break off?) and item nonresponse (do they leave more items unanswered?).   We carried out a field experiment that compared responses obtained by smartphones, tablets, and laptop computers, focusing on the potential effects of the different devices on measurement errors.  We examined how the differences across devices in screen size (and the related need to scroll to see the entire question or the full set of response options) might increase the effects of response order, affect the strategy respondents used to decide which of two options was preferable, change the effect of question context, or influence respondents? use of definitions.  These experiments were based on the principle of visual prominence &mdash; the idea that respondents are more likely to notice and consider information that is easy to see.  The experiments were deliberately designed to maximize the impact of screen size on the results, since the screen size would affect the visual prominence of important information.  For example, it was harder for smartphone respondents to see the definitional information accompanying some of the questions because they had to scroll to see it.  However, like many of the prior studies examining mobile devices, although we did find the expected effects of the order of the answer categories, question context, and evaluation strategy on the answers respondents gave, we find few differences across the three types of device.  This is good news &mdash; the small screens of the smartphones did not make known measurement problems any worse.  We also examined several other indicators of data quality, such as rates of missing data, the length of time it took for respondents to complete the survey, the correlations among related items, and the willingness of respondents to make embarrassing admissions about themselves (for example, that they used illicit drugs or drank heavily).  Again, the effects of device type were minimal.  Responses obtained on a smartphone seem to be just as good as those obtained on a tablet or laptop computer.  And that is very good news for survey researchers, since many respondents will complete only complete web surveys if they can do it on their smartphones.     Tourangeau, R., Conrad, F.G., &amp; Couper, M.P. (2013).  The Science of Web Surveys.  Oxford:  Oxford University Press.          Last Modified: 09/12/2016       Submitted by: Roger Tourangeau]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
