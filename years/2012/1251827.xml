<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Declarative Crowdsourcing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>199931.00</AwardTotalIntnAmount>
<AwardAmount>199931</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A variety of applications are increasingly relying on crowdsourcing services, such as Amazon Mechanical Turk or CrowdFlower, in order to access human computation at a large scale and solve problems that cannot be tackled using only machine computation. Despite the surge in crowdsourcing platforms, it remains very difficult and error-prone to employ crowdsourcing within an application: existing services mostly expose a procedural interface to post individual human-computation tasks, and provide little support (if any) for task coordination, reward management, or clean-up of the obtained answers. As a result, a large fraction of the application logistics is devoted to orchestrating and optimizing the interaction with the crowdsourcing service. This project explores the novel paradigm of declarative crowdsourcing through the development of the Deco database system. Deco models and offers support for accessing the collective knowledge of the crowd by posing declarative queries over a relational-like database. The project explores methods to mitigate the effect of "noisy" human workers who provide data of low quality and to model the resulting uncertainty in the answers returned by Deco. The two problems are tightly coupled with a tradeoff among the latency to contact human workers, the expense to recruit them and the quality of the data they provide. Handling this tradeoff in the context of query optimization is one of the key technical challenges addressed by the project. &lt;br/&gt;&lt;br/&gt;The project represents a high-risk research effort, as it targets non-trivial problems that are inherent in the usage of crowdsourcing in practice. The corresponding high payoff is that the results of this research provide a robust and principled foundation for declarative crowdsourcing, thus enabling a wide variety of applications to incorporate crowdsourcing as a core component of their software stack. Moreover, this project identifies desirable features of crowdsourcing services in order to support this novel declarative interface, thereby providing valuable guidance for the design of next-generation crowdsourcing platforms. Finally, the project provides training to students and the opportunity to engage in the emerging research area that lies in the intersection of databases and crowdsourcing. Details for the project can be found at the project web site (http://db.cs.ucsc.edu/deco).</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1251827</AwardID>
<Investigator>
<FirstName>Neoklis</FirstName>
<LastName>Polyzotis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Neoklis Polyzotis</PI_FULL_NAME>
<EmailAddress>alkis.polyzotis@gmail.com</EmailAddress>
<PI_PHON>4084290111</PI_PHON>
<NSF_ID>000340492</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Cruz</Name>
<CityName>Santa Cruz</CityName>
<ZipCode>950641077</ZipCode>
<PhoneNumber>8314595278</PhoneNumber>
<StreetAddress>1156 High Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>125084723</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA CRUZ</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Cruz]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>950641077</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~199931</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project has the following major goals:</p> <p>- To develop techniques that can reduce/mitigate the inherent uncertainty of information obtained through crowdsourcing. This uncertainty has several sources: ambiguity of the task used to obtain information; honest mistakes made by workers accomplishing the task; random mistakes made by workers who do not care to complete the task as described (spammers); intentional mistakes, made by workers who intend to provide erroneous information (vandals).</p> <p>- Using these techniques, to develop operators that can be used by applications in order to obtain reliable information from the crowd. Example operators are filtering, top-k selection, or sorting. These operators have to reason about information uncertainty in order to aggregate the weak signals of individual workers into one single output of higher quality.&nbsp;</p> <p>To this end, the project has generated two concrete outcomes:</p> <p>- A new operator&nbsp;for filtering a large set of items using humans. Filtering is one of the most commonly used building blocks in crowdsourcing applications and systems. While solutions for crowd-powered ?filtering exist, they make a range of implicit assumptions and restrictions, ultimately rendering them not powerful enough for real-world applications. We describe two approaches to discard these implicit assumptions and restrictions: one, that carefully generalizes prior work, leading to an optimal, but o?entimes intractable solution, and another, that provides a novel way of reasoning about ?filtering strategies, leading to a sometimes suboptimal, but e?fficiently computable solution (that is asymptotically close to optimal).&nbsp;</p> <p>- A operator to obtain top-k lists of items out of larger itemsets, using human workers to perform comparisons among items. The operator has several variants which correspond to different points in the trade-off among tolerance to adversarial behavior and common measures of performance (latency, expense and quality of results). In particular, a randomized variant achieves significant budget saves, especially for very large itemsets and large top-k lists, with negligible risk of lowering the quality of the output.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/27/2017<br>      Modified by: Neoklis&nbsp;Polyzotis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project has the following major goals:  - To develop techniques that can reduce/mitigate the inherent uncertainty of information obtained through crowdsourcing. This uncertainty has several sources: ambiguity of the task used to obtain information; honest mistakes made by workers accomplishing the task; random mistakes made by workers who do not care to complete the task as described (spammers); intentional mistakes, made by workers who intend to provide erroneous information (vandals).  - Using these techniques, to develop operators that can be used by applications in order to obtain reliable information from the crowd. Example operators are filtering, top-k selection, or sorting. These operators have to reason about information uncertainty in order to aggregate the weak signals of individual workers into one single output of higher quality.   To this end, the project has generated two concrete outcomes:  - A new operator for filtering a large set of items using humans. Filtering is one of the most commonly used building blocks in crowdsourcing applications and systems. While solutions for crowd-powered ?filtering exist, they make a range of implicit assumptions and restrictions, ultimately rendering them not powerful enough for real-world applications. We describe two approaches to discard these implicit assumptions and restrictions: one, that carefully generalizes prior work, leading to an optimal, but o?entimes intractable solution, and another, that provides a novel way of reasoning about ?filtering strategies, leading to a sometimes suboptimal, but e?fficiently computable solution (that is asymptotically close to optimal).   - A operator to obtain top-k lists of items out of larger itemsets, using human workers to perform comparisons among items. The operator has several variants which correspond to different points in the trade-off among tolerance to adversarial behavior and common measures of performance (latency, expense and quality of results). In particular, a randomized variant achieves significant budget saves, especially for very large itemsets and large top-k lists, with negligible risk of lowering the quality of the output.          Last Modified: 09/27/2017       Submitted by: Neoklis Polyzotis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
