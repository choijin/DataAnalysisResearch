<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Telecollaboration in Physical Spaces</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>499970.00</AwardTotalIntnAmount>
<AwardAmount>499970</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to develop and evaluate novel methods for telecollaboration - remote collaboration that effectively integrates video and voice communication, computer vision based tracking, and augmented reality display in order to enable participants to more fully leverage the local physical environment in their remote interactions. In this telecollaboration paradigm, remote and local users will interact with the physical environment using models derived from live imagery from a camera that the local user holds or wears, rather than merely passively viewing the video stream. A key aspect of the approach is that it does not require preparation of the environment or model information, so it can be viewed as an "anywhere, anytime" mobile communication technology.&lt;br/&gt;    &lt;br/&gt;Intellectual merit: The proposed research builds on promising preliminary work on telecollaboration showing the effectiveness of world-stabilized "telepointers" - markers controlled by the remote user that stick to the real-world referents in a dynamic environment. The project will advance the state of the art in remote collaboration by providing new capabilities to integrate real and virtual, verbal and spatial, local and remote. The proposed developments are needed in order to make the physical space a more fundamental part of telecollaboration, and significant user studies will be conducted to acquire a better understanding of the opportunities and limitations of physically-grounded remote interaction. &lt;br/&gt;    &lt;br/&gt;Broader impacts: Better, more compelling tools for telecollaboration can have a tremendous impact in terms of productivity and environmental consequences, as improved remote interaction reduces the need for physical collocation and thus travel. The educational impacts of the project include the training and mentoring of graduate students and a new seminar course. The research team will disseminate research results and collected data widely and use the developed technologies to provide outreach opportunities for select groups to participate in lab open houses.</AbstractNarration>
<MinAmdLetterDate>08/30/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/30/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1219261</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Turk</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Turk</PI_FULL_NAME>
<EmailAddress>mturk@cs.ucsb.edu</EmailAddress>
<PI_PHON>8058934236</PI_PHON>
<NSF_ID>000245931</NSF_ID>
<StartDate>08/30/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tobias</FirstName>
<LastName>Hollerer</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tobias H Hollerer</PI_FULL_NAME>
<EmailAddress>holl@cs.ucsb.edu</EmailAddress>
<PI_PHON>8058938759</PI_PHON>
<NSF_ID>000166428</NSF_ID>
<StartDate>08/30/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Barbara</Name>
<CityName>Santa Barbara</CityName>
<ZipCode>931062050</ZipCode>
<PhoneNumber>8058934188</PhoneNumber>
<StreetAddress>Office of Research</StreetAddress>
<StreetAddress2><![CDATA[Rm 3227 Cheadle Hall]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA24</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>094878394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA BARBARA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Barbara]]></Name>
<CityName>Santa Barbara</CityName>
<StateCode>CA</StateCode>
<ZipCode>931065110</ZipCode>
<StreetAddress><![CDATA[Harold Frank Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA24</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~499970</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project developed new methods and systems for remote video-based collaboration, integrating 3D computer vision techniques with augmented reality (AR) and virtual navigation techniques to create new tools and interfaces for supporting remote participants working together on real-world physical problems. The project resulted in new algorithms and prototypes for AR-based remote collaboration, several publications (and paper awards) based on the research in a range of top research venues, the training of several undergraduate and graduate students, and a startup company that was acquired to add these new capabilities into an AR platform for eventual wide-scale use.</p> <p>Initial work in the project focused on advances in real-time tracking and modeling and integrating computer vision and AR methods into a complete system, implemented in working prototypes using both mobile and desktop devices. The system allowed remote collaborators to virtually explore the local environment of a user and add annotations, which were rendered to the local user in a world-stabilized fashion (made possible by the real-time spatial modeling). Later work focused on better understanding and supporting the remote user&rsquo;s abilities to navigate the virtual scene and for annotations to be properly interpreted and rendered.</p> <p>Two PhD students focused on these challenges for their dissertation work, and many others were involved in various capacities over the course of the project. The work has been widely disseminated via peer-reviewed papers, talks, the lab&rsquo;s website, and in a variety of outreach events at UC Santa Barbara and in the local community. The startup company that spun out from the lab&rsquo;s research on this topic provided new opportunities and experiences for some of the personnel and allowed for direct transfer of the research to a widely-used commercial system.</p><br> <p>            Last Modified: 11/23/2016<br>      Modified by: Matthew&nbsp;Turk</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089362820_Computerrepair--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089362820_Computerrepair--rgov-800width.jpg" title="Computer repair example"><img src="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089362820_Computerrepair--rgov-66x44.jpg" alt="Computer repair example"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The remote collaboration prototype in use on a computer repair problem.</div> <div class="imageCredit">UCSB Four Eyes Lab</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Matthew&nbsp;Turk</div> <div class="imageTitle">Computer repair example</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089289851_Carengineexample--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089289851_Carengineexample--rgov-800width.jpg" title="Remote collaboration example - car problems"><img src="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089289851_Carengineexample--rgov-66x44.jpg" alt="Remote collaboration example - car problems"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Car repair scenario. Top left: the local user needing help. Bottom left: the local user's device view. Top right: the remote expert helping out. Bottom right: The remote user's screen view.</div> <div class="imageCredit">UCSB Four Eyes Lab</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Matthew&nbsp;Turk</div> <div class="imageTitle">Remote collaboration example - car problems</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089458593_Annotationrenderings--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089458593_Annotationrenderings--rgov-800width.jpg" title="Annotation rendering experiment"><img src="/por/images/Reports/POR/2016/1219261/1219261_10208349_1477089458593_Annotationrenderings--rgov-66x44.jpg" alt="Annotation rendering experiment"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An experiment comparing four rendering models for interpreting 2D annotations in 3D.</div> <div class="imageCredit">UCSB Four Eyes Lab</div> <div class="imageSubmitted">Matthew&nbsp;Turk</div> <div class="imageTitle">Annotation rendering experiment</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project developed new methods and systems for remote video-based collaboration, integrating 3D computer vision techniques with augmented reality (AR) and virtual navigation techniques to create new tools and interfaces for supporting remote participants working together on real-world physical problems. The project resulted in new algorithms and prototypes for AR-based remote collaboration, several publications (and paper awards) based on the research in a range of top research venues, the training of several undergraduate and graduate students, and a startup company that was acquired to add these new capabilities into an AR platform for eventual wide-scale use.  Initial work in the project focused on advances in real-time tracking and modeling and integrating computer vision and AR methods into a complete system, implemented in working prototypes using both mobile and desktop devices. The system allowed remote collaborators to virtually explore the local environment of a user and add annotations, which were rendered to the local user in a world-stabilized fashion (made possible by the real-time spatial modeling). Later work focused on better understanding and supporting the remote user?s abilities to navigate the virtual scene and for annotations to be properly interpreted and rendered.  Two PhD students focused on these challenges for their dissertation work, and many others were involved in various capacities over the course of the project. The work has been widely disseminated via peer-reviewed papers, talks, the lab?s website, and in a variety of outreach events at UC Santa Barbara and in the local community. The startup company that spun out from the lab?s research on this topic provided new opportunities and experiences for some of the personnel and allowed for direct transfer of the research to a widely-used commercial system.       Last Modified: 11/23/2016       Submitted by: Matthew Turk]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
