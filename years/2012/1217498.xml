<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:Small:Fine-grain Tasking and Virtual memory for Massively Parallel Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2012</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In order to address the performance and programmability challenges of massively parallel systems  new innovative approaches are needed.  This research continues earlier work on the Fresh Breeze model of computation for which basic simulation results have demonstrated applicability to linear algebra kernels as well as the Graph500 challenge application with competitive performance. The merits of the programming model developed in the proposed research are demonstrated, and evaluated for usability and performance, using a new distributed simulation tool based on packet communication architecture.&lt;br/&gt;&lt;br/&gt;This project proposes innovative combination of a general tasking model, a proposal for global virtual memory, and the matching system architecture. The innovations are expected to serve as the basis for systems that are more energy-efficient and resilient. The form and structure of computer programs for  massively  parallel computation are studied and implementation challenges analyzed and resolved. The three components of the project concern: general support for the major forms of parallelism found in sound and well-structured concurrent programs; structured means for expressing transactions on shared data such as data bases; and demonstration of a global virtual memory based on use of tree structures to represent all data objects. This is expected to reduce development effort and increase performance of software needed to address current and future scientific, environmental and social problems.</AbstractNarration>
<MinAmdLetterDate>05/16/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217498</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Dennis</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack B Dennis</PI_FULL_NAME>
<EmailAddress>dennis@csail.mit.edu</EmailAddress>
<PI_PHON>6172536856</PI_PHON>
<NSF_ID>000113542</NSF_ID>
<StartDate>05/16/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project is in the field of computer architecture where the goal is to improve the efficiency and useability of the computer systems that serve universities, industry and society. In recent years a revolution has occurred with recognition that exploiting parallelism is the only realistic path toward greater efficiency of computation in the future. In the past the architecture of computers has been guided by the need for an individual processing unit to run as fast as possible. In the new era of computation a new guideline is operative: Construct a system with several or many processing units that performs computations with the least energy consumption. Two important outcomes of the subject research are:<br /><br />1. The scheduling of computation tasks over multiple processing units can be done by means built into the system architecture, yielding a substantial performance improvement.<br /><br />2. The ability to complete large software projects expeditiously depends on the ability of software engineers to use principles of modular software construction. Although limited means for practicing modular programming are known and applied in sigle processor computer systems, the new multiprocessor computer systems have been engineered with no attempt to extend the ability to practice modular software construction. This research project has shown a path forward such that future parallel computer systems can have the property that any parallel program can be used, unchanged, as a component of larger parallel programs.</p><br> <p>            Last Modified: 09/25/2015<br>      Modified by: Jack&nbsp;B&nbsp;Dennis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project is in the field of computer architecture where the goal is to improve the efficiency and useability of the computer systems that serve universities, industry and society. In recent years a revolution has occurred with recognition that exploiting parallelism is the only realistic path toward greater efficiency of computation in the future. In the past the architecture of computers has been guided by the need for an individual processing unit to run as fast as possible. In the new era of computation a new guideline is operative: Construct a system with several or many processing units that performs computations with the least energy consumption. Two important outcomes of the subject research are:  1. The scheduling of computation tasks over multiple processing units can be done by means built into the system architecture, yielding a substantial performance improvement.  2. The ability to complete large software projects expeditiously depends on the ability of software engineers to use principles of modular software construction. Although limited means for practicing modular programming are known and applied in sigle processor computer systems, the new multiprocessor computer systems have been engineered with no attempt to extend the ability to practice modular software construction. This research project has shown a path forward such that future parallel computer systems can have the property that any parallel program can be used, unchanged, as a component of larger parallel programs.       Last Modified: 09/25/2015       Submitted by: Jack B Dennis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
