<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Achieving Quality Crowdsourcing across Tasks, Data Scales, and Operational Settings</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2013</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>550000.00</AwardTotalIntnAmount>
<AwardAmount>582000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>While crowdsourcing and human computation methods are rapidly transforming the practice of data collection in research and industry, ensuring quality of the collected data remains difficult in practice and exposes projects relaying on quality of crowdsourced data to significant risk. This reduces the benefits of crowdsourcing for both current adopters and a wider community of potential beneficiaries. Although diverse communities have developed statistical algorithms for quality assurance, the splintered nature of these communities has led to relatively little comparative benchmarking and/or integration of alternative techniques. Dearth of reference implementations and shared datasets has further abated progress, as have evaluations based on tightly-coupled systems, domain specific tasks, and synthetic data. This project investigates, integrates, and rigorously benchmarks diverse quality assurance algorithms across a range of tasks, data scales, and operational settings. Overall, technical findings are expected to transform current understanding of quality assurance methods for crowdsourcing, including identifying key limitations of the current state-of-the-art in order to focus ongoing research and innovation where it can have the greatest impact. Reference implementations of key algorithms are designed to support reuse, reproducible findings, continual benchmarking, and ongoing progress. Project will yield new, sanitized public datasets to support ongoing community benchmarking and shared-task evaluations.&lt;br/&gt;&lt;br/&gt;Technical contributions from the project are expected to offset risk of growing social inequity as online, distributed work becomes increasingly prevalent. Assumptions that crowdsourcing workers are unreliable or interchangeable limit the complexity and scope of work which can be successfully accomplished with online crowdsourcing. By limiting the amount of work available online and opportunities for skilled work, this further restricts the range of upward economic mobility achievable via crowd work. By developing effective methods to measure work quality over time and identify trusted workers, it will be possible to differentiate, recognize, and reward quality work to promote merit-based economic mobility. Educational activities include a new crowdsourcing course designed for college freshman from diverse backgrounds, a graduate seminar integrating the project's research software, presentations to the student chapter of a professional society, and tutorials and short courses benefiting industry practitioners and researchers. This project will inform the principal investigator's community advisory and organizational activities, including Advisory Board service for an annual industrial conference and organizing workshops bringing the industry and research communities together. Information and results will be disseminated via the project web site (http://ir.ischool.utexas.edu/career/).</AbstractNarration>
<MinAmdLetterDate>03/15/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/19/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1253413</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Lease</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Lease</PI_FULL_NAME>
<EmailAddress>ml@ischool.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000544588</NSF_ID>
<StartDate>03/15/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName/>
<StateCode>TX</StateCode>
<ZipCode>787011213</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>21</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX21</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~109370</FUND_OBLG>
<FUND_OBLG>2014~127849</FUND_OBLG>
<FUND_OBLG>2015~114180</FUND_OBLG>
<FUND_OBLG>2016~116889</FUND_OBLG>
<FUND_OBLG>2017~113712</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-a55e82ad-7fff-6f8b-2c46-2543ed1d3972"> <p dir="ltr"><span>Intellectual Merit includes developing rigorous evaluation methodology to investigate, integrate, and benchmark diverse quality assurance algorithms for crowdsourcing data. Open source reference implementations, new public datasets, and community shared tasks have supported reproducible findings, benchmarking, re-use, and continuing field advancement. A 2016 AAAI HCOMP paper on search evaluation via crowdsourcing received the conference&rsquo;s Best Paper award. In the five years of the project, we have benchmarked aggregation techniques across a range of tasks, scales (e.g., the Crowdsourcing at Scale workshop at HCOMP), varying supervision settings, online and offline settings, and other platforms. We have released a variety of source code and datasets, and run shared tasks (at TREC and the aforementioned HCOMP workshop). We also investigated behavioral data collection and analysis, both in isolation and in combination with other quality assurance methods in order to reduce need for gold data or repeated labeling, as well as further increase labeling accuracy.</span></p> <br /> <p dir="ltr"><span>Broader Impacts highlights include reducing growing social inequity in our new digital economy by identifying high quality crowd work, thereby promoting merit-based economic mobility for workers. Relatively low wages, depersonalized work, and asymmetric power relationships have led some conscientious researchers to raise ethical concerns that we may be building a future of crowd-powered computing on the backs of exploited workers in digital sweatshops. Just as consumers can choose to buy fair trade goods or invest in social choice funds, we wanted to create a similar choice for Requesters to provide fair pay to workers, making it easy to provide fair pay. Two NSF REU supplements have led to published undergraduate research as well as engaging high-school students in research. </span></p> <div><span><br /></span></div> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/07/2019<br>      Modified by: Matthew&nbsp;Lease</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Intellectual Merit includes developing rigorous evaluation methodology to investigate, integrate, and benchmark diverse quality assurance algorithms for crowdsourcing data. Open source reference implementations, new public datasets, and community shared tasks have supported reproducible findings, benchmarking, re-use, and continuing field advancement. A 2016 AAAI HCOMP paper on search evaluation via crowdsourcing received the conference?s Best Paper award. In the five years of the project, we have benchmarked aggregation techniques across a range of tasks, scales (e.g., the Crowdsourcing at Scale workshop at HCOMP), varying supervision settings, online and offline settings, and other platforms. We have released a variety of source code and datasets, and run shared tasks (at TREC and the aforementioned HCOMP workshop). We also investigated behavioral data collection and analysis, both in isolation and in combination with other quality assurance methods in order to reduce need for gold data or repeated labeling, as well as further increase labeling accuracy.   Broader Impacts highlights include reducing growing social inequity in our new digital economy by identifying high quality crowd work, thereby promoting merit-based economic mobility for workers. Relatively low wages, depersonalized work, and asymmetric power relationships have led some conscientious researchers to raise ethical concerns that we may be building a future of crowd-powered computing on the backs of exploited workers in digital sweatshops. Just as consumers can choose to buy fair trade goods or invest in social choice funds, we wanted to create a similar choice for Requesters to provide fair pay to workers, making it easy to provide fair pay. Two NSF REU supplements have led to published undergraduate research as well as engaging high-school students in research.              Last Modified: 10/07/2019       Submitted by: Matthew Lease]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
