<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Hard Clustering via Bayesian Nonparametrics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
<AwardExpirationDate>06/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>439689.00</AwardTotalIntnAmount>
<AwardAmount>439689</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Modern machine learning algorithms often encounter a trade off between scalability and modeling power.  For the problem of data clustering, Bayesian approaches enjoy numerous modeling advantages over classical methods, but hard clustering methods such as k-means are often preferred in practice due to their simplicity and scalability.&lt;br/&gt;&lt;br/&gt;This project explores bridging the gap between classical hard clustering methods and clustering models based on Bayesian nonparametrics.  The first step is an asymptotic result connecting the Dirichlet process Gaussian mixture model with a k-means-like algorithm that does not fix the number of clusters in advance.  Using this key result, the PI and his team will explore four related research directions which collectively demonstrate the utility of this asymptotic approach: (1) extensions of the analysis to hierarchical Bayesian models, leading to scalable hard clustering methods over multiple data sets; (2) connections to spectral methods and graph clustering, leading to novel and flexible graph clustering methods; (3) extensions beyond the Gaussian setting, leading to new approaches to topic modeling and other discrete-data clustering problems; and (4) extensive experiments in both the computer vision and text domains.&lt;br/&gt;&lt;br/&gt;Given that k-means is truly a workhorse of machine learning, these four directions have the potential to impact a wide array of large-scale applications including computer vision, bioinformatics, social network analysis, and many other domains.  Furthermore, the research will benefit the broader community through released software and integration into coursework at Ohio State University.</AbstractNarration>
<MinAmdLetterDate>06/27/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/25/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217433</AwardID>
<Investigator>
<FirstName>Mikhail</FirstName>
<LastName>Belkin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mikhail Belkin</PI_FULL_NAME>
<EmailAddress>mbelkin@ucsd.edu</EmailAddress>
<PI_PHON>6148053884</PI_PHON>
<NSF_ID>000107334</NSF_ID>
<StartDate>06/25/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Kulis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian Kulis</PI_FULL_NAME>
<EmailAddress>bkulis@bu.edu</EmailAddress>
<PI_PHON>6173535909</PI_PHON>
<NSF_ID>000601190</NSF_ID>
<StartDate>06/27/2012</StartDate>
<EndDate>06/25/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName/>
<StateCode>OH</StateCode>
<ZipCode>432101063</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~144248</FUND_OBLG>
<FUND_OBLG>2013~295441</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Clustering is one of the fundamental problems of unsupervised machine learning. &nbsp;Classical methods such as k-means are widely used in practice due to their simplicity and scalability but lack flexibility to be applied in several settings, such as dynamic data, clustering of multiple data sources simultaneously, or handling an unknown or changing number of clusters. &nbsp;On the other hand, rich Bayesian nonparametric models can address limitations of the classical methods, but are difficult to apply in real-world, large-scale settings due to the complexity of their underlying inference algorithms.</p> <p>This project explored how to achieve the best of both worlds by developing hard clustering algorithms (e.g., the k-means family) from a Bayesian nonparametric viewpoint. &nbsp;To that end, hard clustering methods were derived for a number of challenging clustering problems, including cluster evolution models, hierarchical models, hidden Markov models, and power-law clustering problems.</p> <p>As a result of this project, the PI and his team made significant progress in developing the underpinnings of small-variance asymptotics, the technique used to develop the algorithms studied in this project. &nbsp;In particular, they showed general results on how to derive both algorithms and underlying objective functions for a wide class of problems, even outside of the clustering setting. &nbsp;This work has led to development by other researchers on further developing this growing field of study.</p> <p>Empirical results and software released by the PI and his group have demonstrated that the clustering algorithms studied in this project are typically orders of magnitude faster than standard Bayesian nonparametric inference algorithms, while retaining the rich and flexible structure of the original nonparametric models. &nbsp;Applications of the work include segmentation problems in images in video and topic modeling for text analysis, and further applications continue to be explored.</p><br> <p>            Last Modified: 08/17/2016<br>      Modified by: Mikhail&nbsp;Belkin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Clustering is one of the fundamental problems of unsupervised machine learning.  Classical methods such as k-means are widely used in practice due to their simplicity and scalability but lack flexibility to be applied in several settings, such as dynamic data, clustering of multiple data sources simultaneously, or handling an unknown or changing number of clusters.  On the other hand, rich Bayesian nonparametric models can address limitations of the classical methods, but are difficult to apply in real-world, large-scale settings due to the complexity of their underlying inference algorithms.  This project explored how to achieve the best of both worlds by developing hard clustering algorithms (e.g., the k-means family) from a Bayesian nonparametric viewpoint.  To that end, hard clustering methods were derived for a number of challenging clustering problems, including cluster evolution models, hierarchical models, hidden Markov models, and power-law clustering problems.  As a result of this project, the PI and his team made significant progress in developing the underpinnings of small-variance asymptotics, the technique used to develop the algorithms studied in this project.  In particular, they showed general results on how to derive both algorithms and underlying objective functions for a wide class of problems, even outside of the clustering setting.  This work has led to development by other researchers on further developing this growing field of study.  Empirical results and software released by the PI and his group have demonstrated that the clustering algorithms studied in this project are typically orders of magnitude faster than standard Bayesian nonparametric inference algorithms, while retaining the rich and flexible structure of the original nonparametric models.  Applications of the work include segmentation problems in images in video and topic modeling for text analysis, and further applications continue to be explored.       Last Modified: 08/17/2016       Submitted by: Mikhail Belkin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
