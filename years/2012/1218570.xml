<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Non-Visual Skimming: Improving the Usability of Web Access for Blind People</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In our information driven Web-based society, we are all gradually falling victims to information overload.  But while sighted people can develop ways to quickly skim Web content in order to get the gist of the information and find what they need, blind users are stymied because they must rely on screen reader software with limited functionality to narrate content using computer-generated speech through a serial audio interface, which does not allow these users to find out what content is important before they listen to it.  So, they either listen to all content or listen to the first part of each sentence or paragraph before skipping to the next.  The PI's goal in this project is to address this problem by developing novel interfaces and algorithmic techniques for non-visual skimming that will empower people with visual impairments to access information on the Web significantly faster than is currently possible with state-of-the-art screen readers.&lt;br/&gt;&lt;br/&gt;When skimming, sighted people quickly look through content and pick out keywords and relevant phrases.  The PI's approach is to emulate this process and enable a computer-assisted skimming experience for screen-reader users.  To this end, he will iteratively and concurrently pursue two research directions: designing interfaces for non-visual skimming, and developing algorithms to enable these interfaces.  Through a process of participatory design interfaces will be derived for skimming with standard shortcut-driven screen-readers, touch-based devices, and simulated haptic surfaces, while algorithms for generating summaries will be created to support skimming of various types of Web content at different levels of granularity and speed.  Controlled and in-situ real-world experiments will be conducted to evaluate the utility of the resulting interfaces and algorithms.  Project outcomes will include an open-source skimming tool and reusable datasets of Web pages with annotations and summaries.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  Technology to facilitate non-visual skimming will transform information access for visually impaired users while also contributing to the fields of natural language processing, machine learning, and Web information retrieval.  Additionally, this research will help us better understand how a combination of touch and haptic interfaces can improve website navigation and skimming.  Although not explored in this research, project outcomes will likely prove useful to people with other disabilities such as cognitive and motor impairments, and they may ultimately also be helpful to sighted people.</AbstractNarration>
<MinAmdLetterDate>09/11/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/11/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1218570</AwardID>
<Investigator>
<FirstName>I.</FirstName>
<LastName>Ramakrishnan</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>I. V Ramakrishnan</PI_FULL_NAME>
<EmailAddress>ram@cs.stonybrook.edu</EmailAddress>
<PI_PHON>6316328451</PI_PHON>
<NSF_ID>000365929</NSF_ID>
<StartDate>09/11/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yevgen</FirstName>
<LastName>Borodin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yevgen Borodin</PI_FULL_NAME>
<EmailAddress>borodin@charmtechlabs.com</EmailAddress>
<PI_PHON>5163137356</PI_PHON>
<NSF_ID>000532638</NSF_ID>
<StartDate>09/11/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Stony Brook]]></Name>
<CityName>Stony Brook</CityName>
<StateCode>NY</StateCode>
<ZipCode>117944400</ZipCode>
<StreetAddress><![CDATA[Stony Brook University]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In our information-driven web-based society, we are all gradually falling &ldquo;victims&rdquo; to <em>information overload</em>. However, while sighted people are finding ways to sift through information faster, computer users who are blind are experiencing an even greater information overload. These people access computers and the Web using screen-reader software, which reads the information on a computer screen sequentially using computer-generated speech and allows them to navigate using keyboard shortcuts or gestures.</p> <p>While sighted people can learn how to quickly skim web content to get the gist of information and find information they need, people who are blind have to process volumes of content narrated through a serial audio interface, which does not allow them to find out what content is important before they listen to it. So, they either listen to all content or listen to the first part of each sentence or paragraph before they skip to the next one.</p> <p>The goal of this Project was to develop computer methods for <strong>non-visual skimming</strong> to empower people with vision impairments to access digitized information significantly faster and thereby reduce the cognitive load associated with non-visual browsing.</p> <p>In the process of skimming, sighted people quickly look through content while picking out words and phrases that are emphasized visually and/or carry the most meaning. This Project explored the processes employed by sighted people to skim web content, and designed algorithmic methods to enable a computer-assisted skimming experience for screen-reader users. The project pursued two parallel directions of research:</p> <p>1) the design of interfaces for non-visual skimming and <br /> 2) the design of algorithms to enable these interfaces.</p> <p>In the first direction, interfaces were designed for skimming with standard shortcut-driven screen-readers, touch-based devices, and simulated haptic surfaces. In the second direction, algorithms were designed for generating summaries to support skimming of various types of web content at different levels of granularity and speed.</p> <p>Participatory design approaches were used to devise all user interfaces. Controlled and in-situ real-world experiments were conducted with over 50 blind participants to evaluate the proposed approach and interfaces. The outcomes of the project included:</p> <p>1) novel interfaces for non-visual skimming on regular computers and touch-screen devices;</p> <p>2) novel computer algorithms to support skimming interfaces;</p> <p>3) an open-source skimming tool that implemented these approaches and enabled skimming with existing screen readers; and</p> <p>4) reusable datasets of web pages with annotations, summaries, and other data.</p> <p>The skimming methods that resulted from this Project have gone a long way towards bridging the accessibility gap between how sighted and blind people consume digital information on the Web.</p><br> <p>            Last Modified: 12/30/2016<br>      Modified by: Yevgen&nbsp;Borodin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In our information-driven web-based society, we are all gradually falling "victims" to information overload. However, while sighted people are finding ways to sift through information faster, computer users who are blind are experiencing an even greater information overload. These people access computers and the Web using screen-reader software, which reads the information on a computer screen sequentially using computer-generated speech and allows them to navigate using keyboard shortcuts or gestures.  While sighted people can learn how to quickly skim web content to get the gist of information and find information they need, people who are blind have to process volumes of content narrated through a serial audio interface, which does not allow them to find out what content is important before they listen to it. So, they either listen to all content or listen to the first part of each sentence or paragraph before they skip to the next one.  The goal of this Project was to develop computer methods for non-visual skimming to empower people with vision impairments to access digitized information significantly faster and thereby reduce the cognitive load associated with non-visual browsing.  In the process of skimming, sighted people quickly look through content while picking out words and phrases that are emphasized visually and/or carry the most meaning. This Project explored the processes employed by sighted people to skim web content, and designed algorithmic methods to enable a computer-assisted skimming experience for screen-reader users. The project pursued two parallel directions of research:  1) the design of interfaces for non-visual skimming and   2) the design of algorithms to enable these interfaces.  In the first direction, interfaces were designed for skimming with standard shortcut-driven screen-readers, touch-based devices, and simulated haptic surfaces. In the second direction, algorithms were designed for generating summaries to support skimming of various types of web content at different levels of granularity and speed.  Participatory design approaches were used to devise all user interfaces. Controlled and in-situ real-world experiments were conducted with over 50 blind participants to evaluate the proposed approach and interfaces. The outcomes of the project included:  1) novel interfaces for non-visual skimming on regular computers and touch-screen devices;  2) novel computer algorithms to support skimming interfaces;  3) an open-source skimming tool that implemented these approaches and enabled skimming with existing screen readers; and  4) reusable datasets of web pages with annotations, summaries, and other data.  The skimming methods that resulted from this Project have gone a long way towards bridging the accessibility gap between how sighted and blind people consume digital information on the Web.       Last Modified: 12/30/2016       Submitted by: Yevgen Borodin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
