<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Understanding the perception and recognition of spoken words: Effects of phonetics, phonological variation, and speech mode</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2012</AwardEffectiveDate>
<AwardExpirationDate>02/28/2018</AwardExpirationDate>
<AwardTotalIntnAmount>399814.00</AwardTotalIntnAmount>
<AwardAmount>399814</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tyler Kendall</SignBlockName>
<PO_EMAI>tkendall@nsf.gov</PO_EMAI>
<PO_PHON>7032922434</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Everyday, people face variation in language.  Readers see words printed in different fonts and typefaces, typically static on a page.  Listeners hear a speech signal that is riddled with variation.  Words are pronounced at different speeds, with different emotions, by different speakers with various accents in different situations, streaming by listeners at a rate of about 5 to 7 syllables per second.  No two utterances of a single word are identical, yet this variation typically goes unnoticed.  Despite this variation, listeners quickly and adeptly understand these different pronunciations as one word and not another; this is remarkable, considering that many words sound the same.  An issue central to linguistic theory is how listeners are able to take this variable speech string and understand it.  &lt;br/&gt;&lt;br/&gt;Explanations of how listeners accomplish this task incorporate purely linguistic solutions to the problem.  Dr. Sumner's research program takes a new approach, grounded by the real-world information conveyed by acoustic-phonetic values.  From any window of speech, listeners encounter cues about sound patterns, but they also encounter cues about the social characteristics of a particular speech situation.  The impetus behind this work is that advancement in understanding the perception and recognition of spoken words will come from examining the ways in which listeners use these ever-present cues together. Dr. Sumner will examine this question by examining the ways in which listeners respond to words that vary along different linguistic and social dimensions.  She will use experimental methods that inform us about how a word pronunciation improves recognition to related words in the mental lexicon, and about how listeners categorize different sounds depending on the linguistic and social context in which they are uttered.&lt;br/&gt;&lt;br/&gt;This work investigates the claims that listeners rely on socially-informative acoustic values of nearby sounds and words when understanding speech, and that linguistic information is stored with social representations that influence speech perception at a level much lower than once thought. This work adds a new link between linguistic and social experience to theories of speech perception and should prompt the field to reconsider the role of acoustic patterns and their place in theory more broadly.  Integrating linguistic and social information, this research may provide a path to improve interactive spoken language technologies.</AbstractNarration>
<MinAmdLetterDate>09/21/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1226963</AwardID>
<Investigator>
<FirstName>Meghan</FirstName>
<LastName>Sumner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Meghan Sumner</PI_FULL_NAME>
<EmailAddress>sumner@stanford.edu</EmailAddress>
<PI_PHON>6506449965</PI_PHON>
<NSF_ID>000182516</NSF_ID>
<StartDate>09/21/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943012150</ZipCode>
<StreetAddress><![CDATA[Margaret Jacks Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~131133</FUND_OBLG>
<FUND_OBLG>2013~268681</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>What do we understand and remember, when we hear spoken language?&nbsp; The short answer, is that it depends on who is talking.&nbsp; The longer answer is more complicated than one might think and has changed substantially as our knowledge as a field has grown.&nbsp; Forty years ago, the answer to this question would have been that we understand words, and mapping sounds onto highly abstract mental categories.&nbsp; How else would we be able to understand words, given that one utterance of a single word is physically distinct from any other?&nbsp; Twenty years ago, the answer to this question would have been that we understand words, storing specific acoustic attributes in memory, which contribute to dynamic higher level categories.&nbsp; Today, the answer is that we understand words <em>and</em> people.&nbsp; For speech is the great confound in our field &ndash; we cannot separate the talker from the utterance.&nbsp; And as humans, we are pattern recognizers, making sense out of acoustic packages that map to simultaneously to linguistic features categories and social ones.&nbsp; The research produced through this grant set out to clarify this aspect of spoken word recognition and memory.&nbsp; In doing so, we found that:</p> <p>(1)&nbsp;&nbsp; Listeners actively use and depend on word-, sentence, and speaker-specific phonetic patterns to facilitate the perception and recognition of word forms with different phonological variants,</p> <p>(2)&nbsp;&nbsp; Non-lexical acoustic patterns reliably activate social cues and features during spoken language processing,</p> <p>(3)&nbsp;&nbsp; A direct link between linguistic and social experience facilitates speech perception and recognition, and</p> <p>(4)&nbsp;&nbsp; This link heavily influences the encoding and subsequent recall of spoken words, leading to biases in the automatic processing of spoken words, and to differences in weighting of linguistic experiences based on real and stereotyped social experience.&nbsp;</p> <p>The implications of this work are methodological, theoretical, and societal.&nbsp; First, from a methodology standpoint, we conducted a number of studies using two different speakers that share macrosocial categories (same age, race, gender, socioeconomic status, education level, dialect region, etc.).&nbsp; We found that using these two &ldquo;controlled&rdquo; voices, the data are voice-independent in some ways (e.g., repeated words are always recognized faster than new words). But, they are highly voice dependent in other ways.&nbsp; For example, some words with some phonological variants ([t] vs. tap in words like <em>city</em>) are remembered better when spoken by one speaker, but other words with different variants (t] vs. [n_] in words like <em>winter; </em>full vs. reduced vowels in words like <em>police</em>) are remembered better when produced by the other speaker.&nbsp; The reason this is a crucial finding for methodology is that the vast majority of studies in the realm of speech perception and spoken word recognition, and sentence processing of speech (vs. text) &ndash; even those not focused on phonetics and phonetic variation &ndash; use a single talker for their studies.&nbsp; The logic then goes that this single talker is representative of the entire talker population, and how listeners respond to this one talker is indicative of how they would respond to all talkers of this &ldquo;group&rdquo;.&nbsp; We have shown clearly, this is not true.</p> <p>From a theoretical perspective, the various studies in this research program have investigated the recognition of and memory for spoken words across speech styles and across talkers who are diverse in their social characteristics.&nbsp; In doing so, we found that infrequent, unfamiliar words and word forms are responded to on par with frequent, familiar words and word forms.&nbsp; Using a variety of tasks, we were able to pinpoint this effect both to phonetic co-occurrence and to the allocation of cognitive resources during stimulus presentation (in other words &ndash; the differentia allocation of attention).&nbsp; Together, these provide evidence against an oversimplified theory that suggests how often we hear something predicts how well we understand it.&nbsp; In other words, we are unable to separate linguistic processing from social processing.&nbsp; In addition, and perhaps more important to our theories, is the finding across tasks that information about talkers is available before lexical access &ndash; meaning these effects are automatic, early, and independent of the lexicon.&nbsp; If we think for a moment of listening to foreign languages or some pseudoword babble, anecdotally, this makes sense &ndash; we don&rsquo;t need linguistic units to parse social features; though they certainly interact.</p> <p>Finally, from the view of society, we find the automatic nature of these effects, intertwined with the effects of attention and memory, to be troubling.&nbsp; In a sense, it outlines a domino effect whereby our social experiences and biases are reinforced through spoken language communication, and that our memory for speech across talkers, groups, and speech styles may either be boosted or demoted, depending on the social expectations that are established in our lifetimes, with voice information influencing listener behavior as early as age 5.&nbsp;</p><br> <p>            Last Modified: 10/21/2019<br>      Modified by: Meghan&nbsp;Sumner</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ What do we understand and remember, when we hear spoken language?  The short answer, is that it depends on who is talking.  The longer answer is more complicated than one might think and has changed substantially as our knowledge as a field has grown.  Forty years ago, the answer to this question would have been that we understand words, and mapping sounds onto highly abstract mental categories.  How else would we be able to understand words, given that one utterance of a single word is physically distinct from any other?  Twenty years ago, the answer to this question would have been that we understand words, storing specific acoustic attributes in memory, which contribute to dynamic higher level categories.  Today, the answer is that we understand words and people.  For speech is the great confound in our field &ndash; we cannot separate the talker from the utterance.  And as humans, we are pattern recognizers, making sense out of acoustic packages that map to simultaneously to linguistic features categories and social ones.  The research produced through this grant set out to clarify this aspect of spoken word recognition and memory.  In doing so, we found that:  (1)   Listeners actively use and depend on word-, sentence, and speaker-specific phonetic patterns to facilitate the perception and recognition of word forms with different phonological variants,  (2)   Non-lexical acoustic patterns reliably activate social cues and features during spoken language processing,  (3)   A direct link between linguistic and social experience facilitates speech perception and recognition, and  (4)   This link heavily influences the encoding and subsequent recall of spoken words, leading to biases in the automatic processing of spoken words, and to differences in weighting of linguistic experiences based on real and stereotyped social experience.   The implications of this work are methodological, theoretical, and societal.  First, from a methodology standpoint, we conducted a number of studies using two different speakers that share macrosocial categories (same age, race, gender, socioeconomic status, education level, dialect region, etc.).  We found that using these two "controlled" voices, the data are voice-independent in some ways (e.g., repeated words are always recognized faster than new words). But, they are highly voice dependent in other ways.  For example, some words with some phonological variants ([t] vs. tap in words like city) are remembered better when spoken by one speaker, but other words with different variants (t] vs. [n_] in words like winter; full vs. reduced vowels in words like police) are remembered better when produced by the other speaker.  The reason this is a crucial finding for methodology is that the vast majority of studies in the realm of speech perception and spoken word recognition, and sentence processing of speech (vs. text) &ndash; even those not focused on phonetics and phonetic variation &ndash; use a single talker for their studies.  The logic then goes that this single talker is representative of the entire talker population, and how listeners respond to this one talker is indicative of how they would respond to all talkers of this "group".  We have shown clearly, this is not true.  From a theoretical perspective, the various studies in this research program have investigated the recognition of and memory for spoken words across speech styles and across talkers who are diverse in their social characteristics.  In doing so, we found that infrequent, unfamiliar words and word forms are responded to on par with frequent, familiar words and word forms.  Using a variety of tasks, we were able to pinpoint this effect both to phonetic co-occurrence and to the allocation of cognitive resources during stimulus presentation (in other words &ndash; the differentia allocation of attention).  Together, these provide evidence against an oversimplified theory that suggests how often we hear something predicts how well we understand it.  In other words, we are unable to separate linguistic processing from social processing.  In addition, and perhaps more important to our theories, is the finding across tasks that information about talkers is available before lexical access &ndash; meaning these effects are automatic, early, and independent of the lexicon.  If we think for a moment of listening to foreign languages or some pseudoword babble, anecdotally, this makes sense &ndash; we don?t need linguistic units to parse social features; though they certainly interact.  Finally, from the view of society, we find the automatic nature of these effects, intertwined with the effects of attention and memory, to be troubling.  In a sense, it outlines a domino effect whereby our social experiences and biases are reinforced through spoken language communication, and that our memory for speech across talkers, groups, and speech styles may either be boosted or demoted, depending on the social expectations that are established in our lifetimes, with voice information influencing listener behavior as early as age 5.        Last Modified: 10/21/2019       Submitted by: Meghan Sumner]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
