<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Preparing Lattice QCD for Accelerated Computing and Future Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>228000.00</AwardTotalIntnAmount>
<AwardAmount>228000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03010000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>PHY</Abbreviation>
<LongName>Division Of Physics</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The nature of high performance computing is undergoing rapid change with the advent of general purpose GPU-accelerated architectures.  These changes open new opportunities for scientific discovery at a much reduced capital and energy cost, but to exploit them, new algorithms and methods are required.  &lt;br/&gt;&lt;br/&gt;This project supports the collaborative development and implementation of GPU (Graphical Processing Unit) codes and innovative algorithms for the numerical simulation of the strong interactions of quarks and gluons, namely quantum chromodynamics (QCD), on a lattice.  Specifically, this project supports the development of CUDA (parallel coding model from NVIDIA)-based code modules and OpenMP threading that can be used with the MILC [Multiple Instruction (and Multiple Data) Lattice Calculation] code.  This 200,000-line code suite is used by several research groups around the world and supports some 100 million core-hours of computing at NSF and DOE national centers and laboratories mostly on conventional supercomputers.  Our code development will allow improved performance on GPU clusters and on computers such as Blue Waters at NCSA that contain both GPUs and conventional multicore processors.  Performance models developed will guide the performance tuning and help to plan future calculations and should also be of value in studing the effectiveness of exascale architectures. &lt;br/&gt;&lt;br/&gt;Interpretation of many of the experimental results in particle physics is currently limited by lack of theoretical understanding.  This reduces our ability to determine parameters of the Standard Model and to find evidence for physics beyond the Standard Model through precision experiments.  The calculations that are enabled are thus important for the interpretation of many experiments in elementary particle physics.  Some of these experiments at Fermilab, Cornell, and at SLAC have been completed.  Others such as in Beijing, Geneva, and Japan are still running.  &lt;br/&gt;&lt;br/&gt;We have publicized previous work on code development and performance modeling by presentations at appropriate conference and publication.  We will continue to do so for the current work in order that others can benefit from our methodology and innovations. The MILC code is widely used in benchmarking by several high-performance computing centers.  This in turn has led to several vendors contacting us regarding performance of our code on their current and future chips.  Thus, having GPU-accelerated code available in MILC could help in the evaluation of or co-design of future high-performance computing architectures.  &lt;br/&gt;&lt;br/&gt;Finally this project will contribute to the professional education and training of future scientists via the participation of postdoctoral reseachers.</AbstractNarration>
<MinAmdLetterDate>07/09/2012</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1212270</AwardID>
<Investigator>
<FirstName>Volodymyr</FirstName>
<LastName>Kindratenko</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Volodymyr V Kindratenko</PI_FULL_NAME>
<EmailAddress>kindrtnk@illinois.edu</EmailAddress>
<PI_PHON>2172650209</PI_PHON>
<NSF_ID>000316832</NSF_ID>
<StartDate>07/09/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Craig</FirstName>
<LastName>Steffen</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Craig P Steffen</PI_FULL_NAME>
<EmailAddress>csteffen@ncsa.uiuc.edu</EmailAddress>
<PI_PHON>2173332186</PI_PHON>
<NSF_ID>000402482</NSF_ID>
<StartDate>07/09/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Urbana</CityName>
<StateCode>IL</StateCode>
<ZipCode>618013620</ZipCode>
<StreetAddress><![CDATA[506 S Wright St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramReference>
<Code>7569</Code>
<Text>CYBERINFRASTRUCTURE/SCIENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~76000</FUND_OBLG>
<FUND_OBLG>2013~76000</FUND_OBLG>
<FUND_OBLG>2014~76000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This was a collaborative project conducted by Indiana University and theNational Center for Supercomputing Applications (NCSA) at theUniversity of Illinois. &nbsp;The goal of the project was to enhance thesoftware capable of running on graphical processing units (GPUs)&nbsp;for carrying out research in lattice Quantum Chromodynamics (QCD).There are now several possible methods for general purpose programmingon GPUs. &nbsp;These include OpenACC and OpenMP pragmas. &nbsp;However, we employa particular framework called QUDA which was first developed atBoston University almost a decade ago for our software development. &nbsp;QUDA is based on CUDA an extension to the C programming language developed&nbsp;by the video hardware company NVIDIA to ease massively parallel program&nbsp;development for non-video applications on their GPUs.<br />Two of the initial QUDA developers Kate Clark and Ron Babich are now working&nbsp;at NVIDIA. &nbsp;During the course of this project, four postdoctoral researchers&nbsp;were supported at one time or another. &nbsp;Of the two who were employed the longest,one is doing postdoctoral research in his native Portugal and the otheris employed by NVIDIA in his native Germany where he remains a leadingdeveloper of QUDA. &nbsp;Two postdocs were also employed for shorter periods oftime. &nbsp;One went on to a postdoc at Fermilab and then transitioned to anindustrial position in Silicon Valley in data science. &nbsp;The other is stillemployed at Indiana University and will be working on exascale computingapplications with Department of Energy support.<br />The software developed under this project includes gauge fixing and enhancementsto heavy-quark solvers important for running Blue Waters. &nbsp;In addition,software was developed (and development continues) for generalizingMILC QCD software to support study of both quantum chromodynamics andelectromagnetism. &nbsp;The inclusion of electromagnetism is important becauselattice QCD calculations are now reaching sub-percent level precision ona number of quantities and electromagnetism can be significant on thatlevel. &nbsp;The MILC collaboration has used quenched quantum electrodynamicsto study the ratio of the up and down quark masses and would like to beable to do a fully dynamical calculation that avoids the quenched approximation.<br />The software developed to run on the GPUs is part of the QUDA library and canbe found at https://github.com/lattice/quda. &nbsp;The software that is part ofthe MILC code can be found at https://github.com/milc-qcd/milc_qcd.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/26/2017<br>      Modified by: Craig&nbsp;P&nbsp;Steffen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This was a collaborative project conducted by Indiana University and theNational Center for Supercomputing Applications (NCSA) at theUniversity of Illinois.  The goal of the project was to enhance thesoftware capable of running on graphical processing units (GPUs) for carrying out research in lattice Quantum Chromodynamics (QCD).There are now several possible methods for general purpose programmingon GPUs.  These include OpenACC and OpenMP pragmas.  However, we employa particular framework called QUDA which was first developed atBoston University almost a decade ago for our software development.  QUDA is based on CUDA an extension to the C programming language developed by the video hardware company NVIDIA to ease massively parallel program development for non-video applications on their GPUs. Two of the initial QUDA developers Kate Clark and Ron Babich are now working at NVIDIA.  During the course of this project, four postdoctoral researchers were supported at one time or another.  Of the two who were employed the longest,one is doing postdoctoral research in his native Portugal and the otheris employed by NVIDIA in his native Germany where he remains a leadingdeveloper of QUDA.  Two postdocs were also employed for shorter periods oftime.  One went on to a postdoc at Fermilab and then transitioned to anindustrial position in Silicon Valley in data science.  The other is stillemployed at Indiana University and will be working on exascale computingapplications with Department of Energy support. The software developed under this project includes gauge fixing and enhancementsto heavy-quark solvers important for running Blue Waters.  In addition,software was developed (and development continues) for generalizingMILC QCD software to support study of both quantum chromodynamics andelectromagnetism.  The inclusion of electromagnetism is important becauselattice QCD calculations are now reaching sub-percent level precision ona number of quantities and electromagnetism can be significant on thatlevel.  The MILC collaboration has used quenched quantum electrodynamicsto study the ratio of the up and down quark masses and would like to beable to do a fully dynamical calculation that avoids the quenched approximation. The software developed to run on the GPUs is part of the QUDA library and canbe found at https://github.com/lattice/quda.  The software that is part ofthe MILC code can be found at https://github.com/milc-qcd/milc_qcd.             Last Modified: 10/26/2017       Submitted by: Craig P Steffen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
