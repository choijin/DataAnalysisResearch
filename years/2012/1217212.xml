<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: HCC: Small: Effects of Automated Information Selection and Presentation in Online Information Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>486093.00</AwardTotalIntnAmount>
<AwardAmount>502093</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Socio-technical systems provide access to ever-increasing quantities of information online. To help people cope with information overload, these systems implement "algorithmic curation": automated selection of what content should be displayed to users, what should be hidden, and how it should be presented. Virtually every Internet user who reads online news, visits social media sites, or uses a search engine has encountered algorithmic curation at some point, probably without even realizing it. In a socio-technical system, user contributions, social relationships and behavior, and features of the technology are interdependent, and determine what the system is used for, how it is used, and how it evolves over time. The goal of this research project is to investigate the relationship between social behavior and algorithmic curation, in order to better predict the effects of this pervasive practice on what we read, contribute, and communicate about online.&lt;br/&gt;&lt;br/&gt;This project uses a multi-method approach to identify ways in which social and technical mechanisms influence individual users' information production and consumption, and thereby shape system-level properties of the user population and the corpus of contributions. Lab experiments investigate how social processes, such as obeying social norms and altering communications for an intended audience, are affected by different types of algorithmic curation. Field studies augment the lab experiments, using technology interventions to demonstrate how these changes play out for people in the real world over time, and as algorithms change. At the system level, agent-based models connect individual-level processes with system-level effects of algorithmic curation, and large-scale data collection looks for signs of those effects on real systems.&lt;br/&gt;&lt;br/&gt;This project advances the current understanding of forces that shape information access and use in an increasingly connected and automated environment. Results will be used to provide guidance to system designers who create and manipulate algorithms, in the form of design patterns that will support a systematic, generalizable way of planning for effects of algorithmic curation at different scales. The project Web site (http://bitlab.cas.msu.edu/curation) provides information. Undergraduate and graduate students involved in the project will become better problem solvers, and work effectively on collaborative interdisciplinary projects.</AbstractNarration>
<MinAmdLetterDate>08/31/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/14/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217212</AwardID>
<Investigator>
<FirstName>Emilee</FirstName>
<LastName>Rader</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emilee J Rader</PI_FULL_NAME>
<EmailAddress>emilee@msu.edu</EmailAddress>
<PI_PHON>5173558372</PI_PHON>
<NSF_ID>000580969</NSF_ID>
<StartDate>08/31/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488241212</ZipCode>
<StreetAddress><![CDATA[430 Communication Arts & Science]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~486093</FUND_OBLG>
<FUND_OBLG>2013~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To help people cope with information overload, online systems use "algorithmic curation", or automated selection and prioritization of the content that is displayed to users. Internet users who read online news, visit social media sites, or use a search engine encounter algorithmic curation, probably without even realizing it. This project investigated the relationship between social behavior and algorithmic curation, to identify its effects on what people read, contribute, and communicate about online.</p> <p>We discovered that users of the Facebook News Feed, a system that uses algorithmic curation, form their own theories about how the Facebook News Feed works just by interacting with it. However, their theories do not always involve a curation algorithm. User beliefs about why they see the posts they do ranged widely, from thinking that the News Feed shows all possible posts from their friends, to automated selection by an algorithm. Some people believed that the system could make inferences about their preferences based on which posts they read and whose pages they visited. Others noticed the News Feed showing them posts "out of order" or showing more posts from certain friends and few posts from others. People disliked missing posts from their friends, and believed that when they did it was evidence of system intervention.</p> <p>When we made people aware that they had missed posts, by asking them to visit the pages of specific friends and report whether they saw posts that they had not seen before, most reported that they had missed posts from at least one friend. But, how close they felt to specific people had no bearing on whether they were likely to notice missed posts from someone, indicating that the system may not base it's content prioritization on an accurate model of relationship closeness. Because Facebook posts present opportunities for feedback important for social support and maintaining social ties, any bias or inaccuracy in the way the algorithm promotes content could affect users&rsquo; ability to maintain relationships on Facebook. Also, missed posts from close friends were more surprising, even when participants believed that the actions of the system caused the missed posts. Stronger beliefs that missed posts were because of system intervention were associated with more surprise, showing that users expect the algorithm to do a good job showing them posts that they expect to see.</p> <p>We learned that the rank at which content is displayed in the News Feed interacts with how far down users scroll to determine which stories they see, making it very difficult to measure these influences separately. Measuring an algorithm's effects as if it were a filter or gatekeeper that acts before users make choices about what content to consume may lead to mis-estimating its impact on the overall diversity of the information users see. Our results also showed that the algorithm's control over what users see is greatest at the top of the News Feed (high ranks) because those are the posts users are most likely to view and capture their attention. Our model, which allows us to quantify interdependence between ranks and rank-related user behavior, identifies a source of biased exposure to diverse information that previous literature has not addressed.</p> <p>As more systems begin using algorithms to sort and rank information for users, it is increasingly important that users are able to understand how their access to information has been affected. We discovered that blog posts published by Facebook describing how the News Feed works focus mostly on why the algorithm works the way it does, and on the types of "signals", or the data that the algorithm considers, when it calculates the rank for a particular News Feed post. We learned that this information, when shown to Facebook users, caused them to become more aware that there is an algorithm that affects what they see, and helped them to make judgments about whether the system is biased. However, it didn't work as well at helping people understand how the system prioritizes content. People did not gain new understanding that would help them to take action to change their behavior with respect to the system, which calls into question whether transparency in systems that use algorithmic curation would be an effective strategy.</p> <p>Our interdisciplinary work emphasizes the sociotechnical nature of these systems -- the people, information, and algorithms all interact -- and that this combination is both difficult to measure and can have unexpected effects. Our work shows that these systems are different from recommender systems in that the feedback loop is intentionally hidden from users, yet it shapes their experience in measureable ways. This project also trained 6 graduate students and 6 undergraduate students, including both computer science students and social science students. Their training included helping the students learn how to communicate across disciplines and work together on a interdisciplinary team.</p><br> <p>            Last Modified: 11/30/2017<br>      Modified by: Emilee&nbsp;J&nbsp;Rader</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To help people cope with information overload, online systems use "algorithmic curation", or automated selection and prioritization of the content that is displayed to users. Internet users who read online news, visit social media sites, or use a search engine encounter algorithmic curation, probably without even realizing it. This project investigated the relationship between social behavior and algorithmic curation, to identify its effects on what people read, contribute, and communicate about online.  We discovered that users of the Facebook News Feed, a system that uses algorithmic curation, form their own theories about how the Facebook News Feed works just by interacting with it. However, their theories do not always involve a curation algorithm. User beliefs about why they see the posts they do ranged widely, from thinking that the News Feed shows all possible posts from their friends, to automated selection by an algorithm. Some people believed that the system could make inferences about their preferences based on which posts they read and whose pages they visited. Others noticed the News Feed showing them posts "out of order" or showing more posts from certain friends and few posts from others. People disliked missing posts from their friends, and believed that when they did it was evidence of system intervention.  When we made people aware that they had missed posts, by asking them to visit the pages of specific friends and report whether they saw posts that they had not seen before, most reported that they had missed posts from at least one friend. But, how close they felt to specific people had no bearing on whether they were likely to notice missed posts from someone, indicating that the system may not base it's content prioritization on an accurate model of relationship closeness. Because Facebook posts present opportunities for feedback important for social support and maintaining social ties, any bias or inaccuracy in the way the algorithm promotes content could affect users? ability to maintain relationships on Facebook. Also, missed posts from close friends were more surprising, even when participants believed that the actions of the system caused the missed posts. Stronger beliefs that missed posts were because of system intervention were associated with more surprise, showing that users expect the algorithm to do a good job showing them posts that they expect to see.  We learned that the rank at which content is displayed in the News Feed interacts with how far down users scroll to determine which stories they see, making it very difficult to measure these influences separately. Measuring an algorithm's effects as if it were a filter or gatekeeper that acts before users make choices about what content to consume may lead to mis-estimating its impact on the overall diversity of the information users see. Our results also showed that the algorithm's control over what users see is greatest at the top of the News Feed (high ranks) because those are the posts users are most likely to view and capture their attention. Our model, which allows us to quantify interdependence between ranks and rank-related user behavior, identifies a source of biased exposure to diverse information that previous literature has not addressed.  As more systems begin using algorithms to sort and rank information for users, it is increasingly important that users are able to understand how their access to information has been affected. We discovered that blog posts published by Facebook describing how the News Feed works focus mostly on why the algorithm works the way it does, and on the types of "signals", or the data that the algorithm considers, when it calculates the rank for a particular News Feed post. We learned that this information, when shown to Facebook users, caused them to become more aware that there is an algorithm that affects what they see, and helped them to make judgments about whether the system is biased. However, it didn't work as well at helping people understand how the system prioritizes content. People did not gain new understanding that would help them to take action to change their behavior with respect to the system, which calls into question whether transparency in systems that use algorithmic curation would be an effective strategy.  Our interdisciplinary work emphasizes the sociotechnical nature of these systems -- the people, information, and algorithms all interact -- and that this combination is both difficult to measure and can have unexpected effects. Our work shows that these systems are different from recommender systems in that the feedback loop is intentionally hidden from users, yet it shapes their experience in measureable ways. This project also trained 6 graduate students and 6 undergraduate students, including both computer science students and social science students. Their training included helping the students learn how to communicate across disciplines and work together on a interdisciplinary team.       Last Modified: 11/30/2017       Submitted by: Emilee J Rader]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
