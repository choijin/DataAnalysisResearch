<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Aquisition of Computer Cluster for Heliophysics, Plasma, and Turbulence Modeling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>534977.00</AwardTotalIntnAmount>
<AwardAmount>534977</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03010000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>PHY</Abbreviation>
<LongName>Division Of Physics</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Vyacheslav (Slava) Lukin</SignBlockName>
<PO_EMAI>vlukin@nsf.gov</PO_EMAI>
<PO_PHON>7032927382</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project involves the acquisition of a 4,300 core computer cluster to enable leading-edge research in solar physics, the solar wind and its properties, both fluid and plasma turbulence, magnetic reconnection and plasma instabilities, and magnetospheric physics. The cluster will be located in the University of New Hampshire Research Computation and Instrumentation (RCI) core facility and operated by its staff. The installation of the cluster will take advantage of the recent modernization and expansion of the RCI machine room, which provides a stable infrastructure and efficient power supply and cooling to minimize the carbon footprint. The cluster replaces an older one, Zaphod, which is now in its eighth year of operation. While Zaphod has enabled numerous studies and some breakthroughs in computational physics, the new cluster will be at least 15 times more powerful and thus enable more leading-edge science.&lt;br/&gt;&lt;br/&gt;The cluster will significantly enhance our ability to simulate plasma and fluid systems, benefiting research projects with more than $3.5M in combined yearly expenditures. Progress will not be just incremental, but we have good reason to believe that in many cases our progress will open up new physical regimes. Most of the simulation codes that will be used are already mature, thus the impact of the cluster on scientific productivity will be immediate. Over its lifetime, the computer cluster will enable leading-edge research for at least a dozen faculty, and many more researchers, post-docs, graduate, and undergraduate students.&lt;br/&gt;&lt;br/&gt;Much of the research to be performed on the cluster is directly related to space weather. Our progress in modeling the solar corona, solar wind, and geospace has an immediate effect on the Nation's ability to forecast and nowcast space weather, and to understand and to mitigate its effects. The National Space Weather Program explicitly calls for the development of such capabilities. One of the projects supported addresses fundamental issues related to climate change, which is also a topic of great societal concern. The cluster will also make an impact on the education of students in all physical disciplines at UNH by giving them access to a state-of-the art computing facility. It will thus help us to attract the most qualified faculty, post-docs, and students, and it will be an extremely valuable resource for educating the next generation of computational scientists.</AbstractNarration>
<MinAmdLetterDate>08/31/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1229408</AwardID>
<Investigator>
<FirstName>Joachim</FirstName>
<LastName>Raeder</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joachim Raeder</PI_FULL_NAME>
<EmailAddress>J.Raeder@unh.edu</EmailAddress>
<PI_PHON>6038623412</PI_PHON>
<NSF_ID>000466501</NSF_ID>
<StartDate>08/31/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of New Hampshire</Name>
<CityName>Durham</CityName>
<ZipCode>038243585</ZipCode>
<PhoneNumber>6038622172</PhoneNumber>
<StreetAddress>51 COLLEGE RD SERVICE BLDG 107</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<StateCode>NH</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NH01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>111089470</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY SYSTEM OF NEW HAMPSHIRE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001765866</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of New Hampshire]]></Name>
<CityName>Durham</CityName>
<StateCode>NH</StateCode>
<ZipCode>038242600</ZipCode>
<StreetAddress><![CDATA[8 College Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NH01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<ProgramReference>
<Code>1303</Code>
<Text>CLIMATE MODELING &amp; PREDICTION</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~534977</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We acquired and installed <span>Trillian, a CRAY XE6m-200 supercomputer with 4,096 compute cores, 4 TB of memory, GEMINI interconnect fabric, and 160 TB Lustre disk space. Trillian has a top speed of about 40 TeraFlops. Trillian runs the Cray Linux Environment (CLE) and its software stack consists of the GNU, Portland Group, and CRAY compilers along with the typical libraries such as MPI and HDF. Trillian batch jobs are managed using PBS. </span></p> <p>Trillian has been in operation since 2013 and has supported numerous research projects at UNH that require high-performance computing (HPC). &nbsp;So far, Trillian has delivered approximately 100 million core-hours computing power. &nbsp;It would be difficult to obtain that much computing power from shared resources such as the NSF funded XSEDE centers. &nbsp;In addition, since we have full control over the machine, we could perform some long duration (longer than one month) contiguous "hero" runs, which would have been impossible to perform otherwise.</p> <p>To date, 38 peer-reviewed publications and 6 Ph.D. theses have resulted from work performed on Trillian. &nbsp;In addition, Trillian is routinely used as a resource in classroom teaching for classes that involve HPC, and thus contributes substantially to educate the high tech workforce. &nbsp;Although NSF funding to operate Trillian has ended, UNH is committed to continue to support Trillian for the remaining useful lifetime of some 3-4 years.</p><br> <p>            Last Modified: 12/20/2016<br>      Modified by: Joachim&nbsp;Raeder</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We acquired and installed Trillian, a CRAY XE6m-200 supercomputer with 4,096 compute cores, 4 TB of memory, GEMINI interconnect fabric, and 160 TB Lustre disk space. Trillian has a top speed of about 40 TeraFlops. Trillian runs the Cray Linux Environment (CLE) and its software stack consists of the GNU, Portland Group, and CRAY compilers along with the typical libraries such as MPI and HDF. Trillian batch jobs are managed using PBS.   Trillian has been in operation since 2013 and has supported numerous research projects at UNH that require high-performance computing (HPC).  So far, Trillian has delivered approximately 100 million core-hours computing power.  It would be difficult to obtain that much computing power from shared resources such as the NSF funded XSEDE centers.  In addition, since we have full control over the machine, we could perform some long duration (longer than one month) contiguous "hero" runs, which would have been impossible to perform otherwise.  To date, 38 peer-reviewed publications and 6 Ph.D. theses have resulted from work performed on Trillian.  In addition, Trillian is routinely used as a resource in classroom teaching for classes that involve HPC, and thus contributes substantially to educate the high tech workforce.  Although NSF funding to operate Trillian has ended, UNH is committed to continue to support Trillian for the remaining useful lifetime of some 3-4 years.       Last Modified: 12/20/2016       Submitted by: Joachim Raeder]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
