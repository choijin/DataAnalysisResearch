<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Prediction-Based Data Placement for New Memory and Storage Hierarchies</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many high-end computing and data-intensive applications, such as transaction processing systems, large-scale scientific simulations, and on-demand services, are limited by the performance of the underlying memory and storage systems. The goal of this project is to improve the memory/storage hierarchy by integrating new and emerging memory/storage technologies with the existing memory/storage hierarchy.  Flash memory based Solid State Drives (SSDs) and Phase Change Memory (PCM) are two emerging memory/storage technologies that show tremendous promise to improve the performance of current computer systems. Shingled Write Disks (SWDs) have been recently proposed to further improve the magnetic disk space capacity to another level. This project is to investigate effective data placement in the new memory/storage hierarchies to better support high-end computing and data-intensive applications. Considering the special characteristics of these new technologies, this project provides innovative solutions for prediction-based data placement in the new memory/storage hierarchies to effectively improve the performance of both memory and storage data accesses. It speeds up the integration of PCM with DRAM as a new memory hierarchy and flash memory-based SSDs and SWDs with the existing storage hierarchy. Providing efficient solutions to the fundamental performance issues greatly improves the performance of many critical applications including weather forecasting, large-scale scientific simulations, e-commerce, etc. Therefore, the project makes huge impact on our daily life. Quickly integrating SWDs into the existing storage hierarchy will also enhance our capability to handle huge amounts of available data. This project creates positive impact on every layer of the memory and storage hierarchies.</AbstractNarration>
<MinAmdLetterDate>08/28/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/28/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217569</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Du</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David H Du</PI_FULL_NAME>
<EmailAddress>du@cs.umn.edu</EmailAddress>
<PI_PHON>6126252560</PI_PHON>
<NSF_ID>000429678</NSF_ID>
<StartDate>08/28/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName/>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many high-end computing and data-intensive applications, such as transaction processing systems, large-scale scientific simulations, and on-demand services are limited by the performance of the underlying memory and storage systems. The limited capacity and bandwidth of widely used DRAM and magnetic disks in the traditional memory/storage hierarchy impose a significant hurdle in scaling these applications to satisfy the increasing growth of data. One promising way to improve the traditional memory/storage hierarchy is to bring in new memory/storage technologies and to fully integrate them with the existing memory/storage hierarchy. Flash memory based Solid State Drives (SSDs) and Non-Volatile Memory (NVRAM) are emerging memory/storage technologies that show tremendous promise to improve the performance of current computer systems. Shingled Magnetic Recording Drives (SMR) have been recently proposed to further improve the magnetic disk space capacity to another level. PI and his team have investigated the possible places for the new memory/storage technologies in memory/storage hierarchies. They investigate effective data placement in the new memory/storage hierarchies to better support high-end computing and data-intensive applications.</p> <p>&nbsp;</p> <p>Since these new devices have their own special characteristics, the data placement and migration in the newly formed memory/storage hierarchies will be different from the past. In the project, the team emphasizes more on prediction capability of data accesses. This project &nbsp;provides innovative solutions for prediction-based data placement in the new memory/storage hierarchies. The research outcomes speed up the integration of NVRAM with DRAM as a new memory hierarchy and flash memory-based SSDs and SMRs with the existing storage hierarchy. Providing efficient solutions to the fundamental performance issues has greatly improved the performance of many critical applications including weather forecasting, large-scale scientific simulations, e-commerce, etc. Therefore, the project has successful made huge impact on our daily life. Quickly integrating SMRs into the existing storage hierarchy has also enhanced our capability to handle huge amounts of available data. As storage performance is always critical, solving the issues targeted by the project has positively impacted every layer of the memory and storage hierarchies.</p> <p>In this big data era, industries have provided multi-tier storage through dynamically placing hot data in the fast tiers and cold data in the slow tiers to reduce the total cost of ownership while keep the performance high. As discussed, recently more new storage devices/technologies have emerged. Each of them has its own unique price-performance tradeoffs and idiosyncrasies with respect to the workloads they prefer to support. Moving data tier by tier may not be efficient and even worse it may lead to unnecessary data movements. Therefore, the team studies the storage architecture with fully connected (i.e., data can move from one device to any other devices instead of moving tier by tier) differential pools (each pool consists of storage devices of a particular type) to suit diverse types of workloads. To explore the internal access patterns and thus efficiently place data in such a fully connected topology, they have developed a Chunk-level storage-aware workload analyzer framework. Access patterns are characterized by a collection of I/O accesses to a data chunk which composed of a set of consecutive data blocks. Taxonomy rules are defined in an extensible manner to assist detecting chunk access patterns. &nbsp;This approach is to enhance the workload profiling accuracy by further partitioning a set of selected chunks and zooming in their interior characteristics. According to the analysis of access pattern changes, the storage manager can adequately distribute/migrate the data chunks across different storage pools. They has improved the initial data placement and migrations of data to the proper storage pools directly and efficiently.&nbsp;</p> <p>Another important issue is how to bridge the giant semantic gap between applications and modern storage systems.&nbsp; Passing a piece of tiny and useful information (I/O access hints) from upper layers to the block storage layer may greatly improve application performance or ease data management in storage systems. This is especially true for heterogeneous storage systems. Since ingesting external access hints will likely involve laborious modifications of legacy I/O stacks, thus making it is very hard to evaluate the effect of access hints. The team has developed a generic and flexible framework to quickly play with a set of access hints and evaluate their impacts on heterogeneous storage systems. The proposed approach contains a new application/user level interface, a file system plugin and a block storage data manager. With this approach, storage systems composed of various storage devices can perform pre-devised data placement, space reallocation and data migration polices assisted by the added access hints. The developed framework supports hints either statically extracted from the existing components (e.g. internal file system data structure) or specially defined and configured by the users (e.g. streaming classification).&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/29/2017<br>      Modified by: David&nbsp;H&nbsp;Du</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many high-end computing and data-intensive applications, such as transaction processing systems, large-scale scientific simulations, and on-demand services are limited by the performance of the underlying memory and storage systems. The limited capacity and bandwidth of widely used DRAM and magnetic disks in the traditional memory/storage hierarchy impose a significant hurdle in scaling these applications to satisfy the increasing growth of data. One promising way to improve the traditional memory/storage hierarchy is to bring in new memory/storage technologies and to fully integrate them with the existing memory/storage hierarchy. Flash memory based Solid State Drives (SSDs) and Non-Volatile Memory (NVRAM) are emerging memory/storage technologies that show tremendous promise to improve the performance of current computer systems. Shingled Magnetic Recording Drives (SMR) have been recently proposed to further improve the magnetic disk space capacity to another level. PI and his team have investigated the possible places for the new memory/storage technologies in memory/storage hierarchies. They investigate effective data placement in the new memory/storage hierarchies to better support high-end computing and data-intensive applications.     Since these new devices have their own special characteristics, the data placement and migration in the newly formed memory/storage hierarchies will be different from the past. In the project, the team emphasizes more on prediction capability of data accesses. This project  provides innovative solutions for prediction-based data placement in the new memory/storage hierarchies. The research outcomes speed up the integration of NVRAM with DRAM as a new memory hierarchy and flash memory-based SSDs and SMRs with the existing storage hierarchy. Providing efficient solutions to the fundamental performance issues has greatly improved the performance of many critical applications including weather forecasting, large-scale scientific simulations, e-commerce, etc. Therefore, the project has successful made huge impact on our daily life. Quickly integrating SMRs into the existing storage hierarchy has also enhanced our capability to handle huge amounts of available data. As storage performance is always critical, solving the issues targeted by the project has positively impacted every layer of the memory and storage hierarchies.  In this big data era, industries have provided multi-tier storage through dynamically placing hot data in the fast tiers and cold data in the slow tiers to reduce the total cost of ownership while keep the performance high. As discussed, recently more new storage devices/technologies have emerged. Each of them has its own unique price-performance tradeoffs and idiosyncrasies with respect to the workloads they prefer to support. Moving data tier by tier may not be efficient and even worse it may lead to unnecessary data movements. Therefore, the team studies the storage architecture with fully connected (i.e., data can move from one device to any other devices instead of moving tier by tier) differential pools (each pool consists of storage devices of a particular type) to suit diverse types of workloads. To explore the internal access patterns and thus efficiently place data in such a fully connected topology, they have developed a Chunk-level storage-aware workload analyzer framework. Access patterns are characterized by a collection of I/O accesses to a data chunk which composed of a set of consecutive data blocks. Taxonomy rules are defined in an extensible manner to assist detecting chunk access patterns.  This approach is to enhance the workload profiling accuracy by further partitioning a set of selected chunks and zooming in their interior characteristics. According to the analysis of access pattern changes, the storage manager can adequately distribute/migrate the data chunks across different storage pools. They has improved the initial data placement and migrations of data to the proper storage pools directly and efficiently.   Another important issue is how to bridge the giant semantic gap between applications and modern storage systems.  Passing a piece of tiny and useful information (I/O access hints) from upper layers to the block storage layer may greatly improve application performance or ease data management in storage systems. This is especially true for heterogeneous storage systems. Since ingesting external access hints will likely involve laborious modifications of legacy I/O stacks, thus making it is very hard to evaluate the effect of access hints. The team has developed a generic and flexible framework to quickly play with a set of access hints and evaluate their impacts on heterogeneous storage systems. The proposed approach contains a new application/user level interface, a file system plugin and a block storage data manager. With this approach, storage systems composed of various storage devices can perform pre-devised data placement, space reallocation and data migration polices assisted by the added access hints. The developed framework supports hints either statically extracted from the existing components (e.g. internal file system data structure) or specially defined and configured by the users (e.g. streaming classification).              Last Modified: 12/29/2017       Submitted by: David H Du]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
