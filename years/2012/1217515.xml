<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: Ontology based Perceptual Organization of Audio-Video Events using Pattern Theory</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>247648.00</AwardTotalIntnAmount>
<AwardAmount>255648</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>It is natural that events of interest in observed scenes manifest themselves across multiple sensing modalities - vision, hearing, smell, etc. The remarkable perceptions of audio-video signals by natural systems, such as humans, also points to superiority of inferences drawn across modalities. It, therefore, seems natural to enhance performance of automated systems by using joint, cross-modal statistical inferences. However, the detection, organization, and understanding of cues and events in real-world scenarios are difficult tasks. This project seeks to develop a pattern-theoretic framework for achieving these goals. The main research items are: (1) development of mathematical quantities to represent audio and visual events and their spatiotemporal relations, (2) use domain-specific ontologies to impose semantic structure and to incorporate prior knowledge, and (3) derive algorithms for Bayesian inferences using efficient adaptations of Markov Chain Monte Carlo sampling. &lt;br/&gt;&lt;br/&gt;The use of pattern theory allows bridging of gaps between raw signals and high-level, domain-dependent semantics, and helps discovers large groups of audio-visual events likely to represent the same underlying event. This effort combines ideas from perceptual organization in computer vision, computational analysis of auditory signals, pattern theory, and prior developments in ontological structures. The methods developed here are applicable to many scenarios that deploy audio and video sensors, including problems of audio annotations of videos, speaker tracking in teleconferencing, and separation of multiple objects in remote surveillance. Broader impact activities involve the development of teaching modules, innovation and entrepreneurial training of the students, and communication of the findings to the community.</AbstractNarration>
<MinAmdLetterDate>08/27/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/13/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217515</AwardID>
<Investigator>
<FirstName>Anuj</FirstName>
<LastName>Srivastava</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anuj Srivastava</PI_FULL_NAME>
<EmailAddress>anuj@stat.fsu.edu</EmailAddress>
<PI_PHON>8506448832</PI_PHON>
<NSF_ID>000395163</NSF_ID>
<StartDate>08/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Florida State University</Name>
<CityName>TALLAHASSEE</CityName>
<ZipCode>323064166</ZipCode>
<PhoneNumber>8506445260</PhoneNumber>
<StreetAddress>874 Traditions Way, 3rd Floor</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790877419</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FLORIDA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Florida State University]]></Name>
<CityName>Tallahassee</CityName>
<StateCode>FL</StateCode>
<ZipCode>323064330</ZipCode>
<StreetAddress><![CDATA[Room 106D, Building OSB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~247648</FUND_OBLG>
<FUND_OBLG>2013~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This collaborative research project centered on applications of Grenander&rsquo;s General Pattern Theory to the representation of complex knowledge associated with human activities in video data.</p> <p><strong>Intellectual Merit</strong></p> <p>Successful automatic understanding of video content is essential for many computer vision-based applications. These applications focus on recognizing human interactions, which are typically understood as events that involve people performing actions with objects or other people. The main challenges are threefold. First, it is difficult to handle the enormous variety of interactions present in different instances of a complex event. The second problem involves rejecting clutter of extraneous objects so that only the participating ones are considered. The third problem stems from errors in classification of objects and actions. How do we leverage the success of deep-learning/machine learning based labeling methods into building a detailed semantic description of a complex event?&nbsp;We explored the&nbsp;combinatorial approach based on Grenander&rsquo;s pattern theory &ndash; the classic version. We have formulated a comprehensive and elegant formulation that can handle all these three kinds of challenges, without the need for an extensive training dataset.</p> <p>The main idea is to discover objects, actions, and their contextual elements, in video sequences and reach graph-theoretic representations of activities. These representations facilitate high-level tasks, such as characterizing, summarizing, and labeling of activities in video segments. Two broad research directions were developed:</p> <p>1.&nbsp; <strong>Inferences on Human Activities from Videos and Sound</strong>: In pattern theory, the mathematical representation of data is a form of a configuration of basic entities, termed generators (or nodes), connected using bonds (or edges) under pre-defined rules. The optimality of a configuration is defined using a global posterior energy that has contributions from both data likelihood and prior knowledge. The choice of generators is dictated by the application context; in cooking videos one typically needs generators for representing objects (spoon, spatula, etc.), materials (bowl, egg, lettuce, etc.), actions (pickup, stir, put, etc.), that constitute a recipe. The presence of generators in a scene is based on spatiotemporal features extracted from video frames using latest learning techniques. The prior knowledge is incorporated in the form on co-occurrence frequencies of generators in the training data. Thus, the detected generators are selectively linked using bonds using a combination of prior, data support and spatiotemporal proximity. The resulting configurations denote interpretations of activities contained in the given video data. Advantages of this framework include incorporation of relevant knowledge to draw efficient scene interpretations, relative immunity to clutter and noise, and scalability to descriptions of long videos towards multiple, simultaneous activities.&nbsp; The framework is also easily generalization to include other sensory modalities such as sound.</p> <p>2.&nbsp; <strong>Rate-Invariance Action Classification</strong>: This effort also focused on statistical analysis of activities in a manner that is invariant to the rate of performance. The basic approach is to extract characteristic features from individual frames, and then represent each observation of an activity as a time-indexed trajectory on a nonlinear feature space. This project developed novel invariant metrics that compare and quantify differences in such manifold paths, and simultaneously temporally register points across trajectories. These metrics contribute in generating statistical summaries and probabilistic classification of observed activities for performing computer vision tasks efficiently. This approach outperformed state-of-the-art results in the classification of human activities and action using 2D or 3D video and audio data.</p> <p>&nbsp;</p> <p><strong>Broader Impacts</strong></p> <p>At FSU, this project supported education and training of three Ph.D. students (including a female) in the general area of high-dimensional statistical data analysis, with placements in academia and industry. It also supported training of three undergraduate researchers during summer months. At USF, this project directly supported education and training of one Ph.D. student, and partially supported and contributed to the education of three other Ph.D. students. It also supported training of two undergraduate researchers.</p> <p>As research outcomes, this projected directly or partly resulted in 8 journal papers, 15+ conferences papers and several book chapters. PI Srivastava finished one graduate-level textbook on &ldquo;Functional and Shape Data Analysis&rdquo; and two edited volumes (&ldquo;3D Face Recognition&rdquo; and &ldquo;Riemannian Computing in Computer Vision&rdquo;) with partial support from this project. Current plans include the development of a textbook to disseminate further the knowledge developed in this project.</p> <p>PI Sarkar along with one of the Ph.D. students participated in the NSF I-Corps program to explore the commercial viability of the ideas developed in this project.</p> <p>The participants received several recognitions for this research, including the best paper award at ICPR 2014. They also presented a tutorial on this pattern-theoretic approach at a major conference. The PIs delivered a number of invited talks at colloquia, workshops, and working groups, to advance the use of pattern theory in applications beyond video data analysis and computer vision.</p> <p>As an outreach, the PIs also collaborated with researchers at two Federal agencies (NIST and NSWC), resulting in joint research efforts and publications.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/17/2016<br>      Modified by: Anuj&nbsp;Srivastava</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481981928875_Figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481981928875_Figure1--rgov-800width.jpg" title="Pattern Theoretic Pipeline"><img src="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481981928875_Figure1--rgov-66x44.jpg" alt="Pattern Theoretic Pipeline"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 1: Object and action generators combine using pattern-theoretic rules to produce connected generator structures that represent an interpretation of the scene. The construction of these interpretation structures are guided by the ontology of the domain.</div> <div class="imageCredit">Fillipe De Douza</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Anuj&nbsp;Srivastava</div> <div class="imageTitle">Pattern Theoretic Pipeline</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982143700_Figure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982143700_Figure2--rgov-800width.jpg" title="Configurations representing videos"><img src="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982143700_Figure2--rgov-66x44.jpg" alt="Configurations representing videos"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 2: Each column depicts a scenario of simultaneously occurring events. The interpretations on the top were generated considered spatial constraints and the bottom row results were produced without such constraints.</div> <div class="imageCredit">Fillipe De Douza</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Anuj&nbsp;Srivastava</div> <div class="imageTitle">Configurations representing videos</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982224469_Figure3--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982224469_Figure3--rgov-800width.jpg" title="Skeletal Trajectories for Action Analysis"><img src="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982224469_Figure3--rgov-66x44.jpg" alt="Skeletal Trajectories for Action Analysis"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 3: Temporal alignment of skeletal shape that is faithful to a rigorous metric definition allow us to overcome temporal speed variations between actions. Recognition of elementary actions is important for overall activity recognition of scenes.</div> <div class="imageCredit">B. Ben Amor</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Anuj&nbsp;Srivastava</div> <div class="imageTitle">Skeletal Trajectories for Action Analysis</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982328547_Figure4--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982328547_Figure4--rgov-800width.jpg" title="Audio-Video Data Inference"><img src="/por/images/Reports/POR/2016/1217515/1217515_10206267_1481982328547_Figure4--rgov-66x44.jpg" alt="Audio-Video Data Inference"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 4: Grenander?s structures probabilistic reasoning on the top k labels scored on CNN and auditory features form semantically consistent interpretations.</div> <div class="imageCredit">Fillipe De Douza</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Anuj&nbsp;Srivastava</div> <div class="imageTitle">Audio-Video Data Inference</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This collaborative research project centered on applications of Grenander?s General Pattern Theory to the representation of complex knowledge associated with human activities in video data.  Intellectual Merit  Successful automatic understanding of video content is essential for many computer vision-based applications. These applications focus on recognizing human interactions, which are typically understood as events that involve people performing actions with objects or other people. The main challenges are threefold. First, it is difficult to handle the enormous variety of interactions present in different instances of a complex event. The second problem involves rejecting clutter of extraneous objects so that only the participating ones are considered. The third problem stems from errors in classification of objects and actions. How do we leverage the success of deep-learning/machine learning based labeling methods into building a detailed semantic description of a complex event? We explored the combinatorial approach based on Grenander?s pattern theory &ndash; the classic version. We have formulated a comprehensive and elegant formulation that can handle all these three kinds of challenges, without the need for an extensive training dataset.  The main idea is to discover objects, actions, and their contextual elements, in video sequences and reach graph-theoretic representations of activities. These representations facilitate high-level tasks, such as characterizing, summarizing, and labeling of activities in video segments. Two broad research directions were developed:  1.  Inferences on Human Activities from Videos and Sound: In pattern theory, the mathematical representation of data is a form of a configuration of basic entities, termed generators (or nodes), connected using bonds (or edges) under pre-defined rules. The optimality of a configuration is defined using a global posterior energy that has contributions from both data likelihood and prior knowledge. The choice of generators is dictated by the application context; in cooking videos one typically needs generators for representing objects (spoon, spatula, etc.), materials (bowl, egg, lettuce, etc.), actions (pickup, stir, put, etc.), that constitute a recipe. The presence of generators in a scene is based on spatiotemporal features extracted from video frames using latest learning techniques. The prior knowledge is incorporated in the form on co-occurrence frequencies of generators in the training data. Thus, the detected generators are selectively linked using bonds using a combination of prior, data support and spatiotemporal proximity. The resulting configurations denote interpretations of activities contained in the given video data. Advantages of this framework include incorporation of relevant knowledge to draw efficient scene interpretations, relative immunity to clutter and noise, and scalability to descriptions of long videos towards multiple, simultaneous activities.  The framework is also easily generalization to include other sensory modalities such as sound.  2.  Rate-Invariance Action Classification: This effort also focused on statistical analysis of activities in a manner that is invariant to the rate of performance. The basic approach is to extract characteristic features from individual frames, and then represent each observation of an activity as a time-indexed trajectory on a nonlinear feature space. This project developed novel invariant metrics that compare and quantify differences in such manifold paths, and simultaneously temporally register points across trajectories. These metrics contribute in generating statistical summaries and probabilistic classification of observed activities for performing computer vision tasks efficiently. This approach outperformed state-of-the-art results in the classification of human activities and action using 2D or 3D video and audio data.     Broader Impacts  At FSU, this project supported education and training of three Ph.D. students (including a female) in the general area of high-dimensional statistical data analysis, with placements in academia and industry. It also supported training of three undergraduate researchers during summer months. At USF, this project directly supported education and training of one Ph.D. student, and partially supported and contributed to the education of three other Ph.D. students. It also supported training of two undergraduate researchers.  As research outcomes, this projected directly or partly resulted in 8 journal papers, 15+ conferences papers and several book chapters. PI Srivastava finished one graduate-level textbook on "Functional and Shape Data Analysis" and two edited volumes ("3D Face Recognition" and "Riemannian Computing in Computer Vision") with partial support from this project. Current plans include the development of a textbook to disseminate further the knowledge developed in this project.  PI Sarkar along with one of the Ph.D. students participated in the NSF I-Corps program to explore the commercial viability of the ideas developed in this project.  The participants received several recognitions for this research, including the best paper award at ICPR 2014. They also presented a tutorial on this pattern-theoretic approach at a major conference. The PIs delivered a number of invited talks at colloquia, workshops, and working groups, to advance the use of pattern theory in applications beyond video data analysis and computer vision.  As an outreach, the PIs also collaborated with researchers at two Federal agencies (NIST and NSWC), resulting in joint research efforts and publications.          Last Modified: 12/17/2016       Submitted by: Anuj Srivastava]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
