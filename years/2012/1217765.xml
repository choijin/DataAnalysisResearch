<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Small: Measurement-based Editing of Reflectance Properties in Photographs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>484498.00</AwardTotalIntnAmount>
<AwardAmount>498498</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project endeavors to answer the fundamental research question whether it is possible to alter the appearance of specific reflectance properties in a photograph of a scene by only partially characterizing the appearance of the scene via measurements.  Specifically, the research team investigates novel appearance editing techniques that minimize the modeling effort, while maintaining maximum editing flexibility and at the same time impose minimal restrictions on the scene, while guaranteeing a physically plausible editing result.  A common application of appearance modeling is to digitally clone a real-world scene, then change some scene properties, and finally revisualize the scene.  Despite the enormous advances in appearance modeling, it still requires significant effort and expertise to create a digital clone of a physical scene.  The effort needed to make the desired alterations to the scene often pales in comparison to the required appearance modeling effort. This project focuses on three reflectance phenomena suited for measurement-based editing: translucency, surface albedo, and surface reflectance.&lt;br/&gt;&lt;br/&gt;This research program has a far-reaching impact beyond computer graphics. It has the potential to impact product development and pre-visualization, virtual reconstruction of archaeological artifacts, forensics, virtual cosmetics, fine-arts and entertainment, and other applications that require the visualization of an object/subject with altered reflectance conditions.  Besides supporting the traditional research roles of graduate students, undergraduate education is enhanced by hands-on research experience and the incorporation of research results into existing courses.</AbstractNarration>
<MinAmdLetterDate>08/15/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/14/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217765</AwardID>
<Investigator>
<FirstName>Pieter</FirstName>
<LastName>Peers</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pieter Peers</PI_FULL_NAME>
<EmailAddress>ppeers@cs.wm.edu</EmailAddress>
<PI_PHON>7572213466</PI_PHON>
<NSF_ID>000554726</NSF_ID>
<StartDate>08/15/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>College of William and Mary</Name>
<CityName>Williamsburg</CityName>
<ZipCode>231878795</ZipCode>
<PhoneNumber>7572213966</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 8795]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>074762238</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLLEGE OF WILLIAM &amp; MARY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>074762238</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[College of William and Mary]]></Name>
<CityName>Williamsburg</CityName>
<StateCode>VA</StateCode>
<ZipCode>231878795</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~154063</FUND_OBLG>
<FUND_OBLG>2013~330435</FUND_OBLG>
<FUND_OBLG>2014~6000</FUND_OBLG>
<FUND_OBLG>2015~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Altering the apparent material properties of objects in photographs such that the appearance of the materials remain natural is a challenging problem. The key challenge is that changing the material properties of an object affects how light is distributed throughout the whole scene, and thus colors in the whole image (including pixels not on the edited object) are affected. Traditional image editing packages do not offer a practical solution for these types of editing operations as they mainly give the user direct control on the color of each pixel separately, and they rely on the user the enforce the correct inter-pixel relations. &nbsp;An alternative strategy to alter the appearance of objects in photographs would be to measure all relevant properties of the photographed scene (such as shape, lighting, and material properties), then change the desired scene property, and finally revisualize the scene by simulating light transport through the scene. &nbsp;Despite the enormous advances in measurement techniques,it still requires significant effort and expertise to create a digital clone of a physical scene. &nbsp;The effort needed to make the desired alterations to the scene often pales in comparison to the required appearance modeling effort. &nbsp;This project investigated the fundamental research question whether it is possible to alter specific appearance properties of a scene by only partially characterizing the appearance via measurements, while guaranteeing a physically plausible editing result.</p> <p><br />The key result of this research endeavor is a novel decomposition of a photograph into different images that each contain the amount of light that has interacted a specified number of times with a user-selected matte surface. &nbsp;For example, our method can decompose a photograph of a room into several images that show the amount of light that has hit a selected (portion of a) wall once, twice, or more times. &nbsp;The number of measurements (expressed in number of photographs that need to be captured), equals the depth of the decomposition. For example, to decompose an image in its first, second, third, and fourth order light interaction components only requires the capture of four photographs. Furthermore, our method does not require any specialized or expensive equipment; only a regular camera and projector. &nbsp;Using this decomposition, we can alter the color of the target surface, while keeping the interreflections throughout the scene consistent. For example, this allows us to recoloring a wall from white to red in a photograph, while automatically adding the physically correct reddish color over the whole room.</p> <p><br />In addition our research made progress in modeling and understanding the appearance of translucent materials. Unlike opaque materials, translucent materials not only directly reflect incident illumination, but also exhibit scattering of light under its surface. Due to this subsurface scattering behavior, translucent materials exhibit a characteristic 'soft' appearance. Many organic materials, such as milk, plants, and skin, exhibit some degree of translucency. &nbsp;Our research has lead to a better understanding on the relation between the degree of translucency, the meso-structure of the underlying shape, and its appearance. &nbsp;This has resulted in a fast method for capturing both the meso-structure and the material properties of translucent materials, enabling us to quickly alter the surface appearance consistently.</p> <p><br />Finally, we have investigated methods for changing the meso-structure of opaque surfaces in a single photograph while retaining a physically plausible appearance. &nbsp;In addition we have addressed related research problems such as easier acquisition of appearance and shape, as well as explored an automated strategy for learning better material appearance models.</p> <p><br />Our research has made advances in computational illumination, image processing, and in appearance modeling and acquisition in the computer graphics and the computer vision field which have been published in several scientific journals and conferences. &nbsp;This project has supported the development of multiple undergraduate and graduate students, and its findings were part of the central thesis of two doctoral dissertations. Its results have been integrated in the relevant undergraduate and graduate courses at the College of William &amp; Mary. The developed methods have potential applications in security, cultural heritage preservation, education, and architecture.</p><br> <p>            Last Modified: 08/15/2016<br>      Modified by: Pieter&nbsp;Peers</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Altering the apparent material properties of objects in photographs such that the appearance of the materials remain natural is a challenging problem. The key challenge is that changing the material properties of an object affects how light is distributed throughout the whole scene, and thus colors in the whole image (including pixels not on the edited object) are affected. Traditional image editing packages do not offer a practical solution for these types of editing operations as they mainly give the user direct control on the color of each pixel separately, and they rely on the user the enforce the correct inter-pixel relations.  An alternative strategy to alter the appearance of objects in photographs would be to measure all relevant properties of the photographed scene (such as shape, lighting, and material properties), then change the desired scene property, and finally revisualize the scene by simulating light transport through the scene.  Despite the enormous advances in measurement techniques,it still requires significant effort and expertise to create a digital clone of a physical scene.  The effort needed to make the desired alterations to the scene often pales in comparison to the required appearance modeling effort.  This project investigated the fundamental research question whether it is possible to alter specific appearance properties of a scene by only partially characterizing the appearance via measurements, while guaranteeing a physically plausible editing result.   The key result of this research endeavor is a novel decomposition of a photograph into different images that each contain the amount of light that has interacted a specified number of times with a user-selected matte surface.  For example, our method can decompose a photograph of a room into several images that show the amount of light that has hit a selected (portion of a) wall once, twice, or more times.  The number of measurements (expressed in number of photographs that need to be captured), equals the depth of the decomposition. For example, to decompose an image in its first, second, third, and fourth order light interaction components only requires the capture of four photographs. Furthermore, our method does not require any specialized or expensive equipment; only a regular camera and projector.  Using this decomposition, we can alter the color of the target surface, while keeping the interreflections throughout the scene consistent. For example, this allows us to recoloring a wall from white to red in a photograph, while automatically adding the physically correct reddish color over the whole room.   In addition our research made progress in modeling and understanding the appearance of translucent materials. Unlike opaque materials, translucent materials not only directly reflect incident illumination, but also exhibit scattering of light under its surface. Due to this subsurface scattering behavior, translucent materials exhibit a characteristic 'soft' appearance. Many organic materials, such as milk, plants, and skin, exhibit some degree of translucency.  Our research has lead to a better understanding on the relation between the degree of translucency, the meso-structure of the underlying shape, and its appearance.  This has resulted in a fast method for capturing both the meso-structure and the material properties of translucent materials, enabling us to quickly alter the surface appearance consistently.   Finally, we have investigated methods for changing the meso-structure of opaque surfaces in a single photograph while retaining a physically plausible appearance.  In addition we have addressed related research problems such as easier acquisition of appearance and shape, as well as explored an automated strategy for learning better material appearance models.   Our research has made advances in computational illumination, image processing, and in appearance modeling and acquisition in the computer graphics and the computer vision field which have been published in several scientific journals and conferences.  This project has supported the development of multiple undergraduate and graduate students, and its findings were part of the central thesis of two doctoral dissertations. Its results have been integrated in the relevant undergraduate and graduate courses at the College of William &amp; Mary. The developed methods have potential applications in security, cultural heritage preservation, education, and architecture.       Last Modified: 08/15/2016       Submitted by: Pieter Peers]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
