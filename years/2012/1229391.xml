<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Development of the Collaborative-Research Augmented Immersive Virtual Environment Laboratory (CRAIVE-Lab)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>300001.00</AwardTotalIntnAmount>
<AwardAmount>300001</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Proposal #: 12-29391&lt;br/&gt;PI(s):  Braasch, Jonas; Chang, Ben; Cutler, Barbra M.; Goebel, Johannes; Radke, Richard&lt;br/&gt;Institution: Rensselaer Polytechnic Institute &lt;br/&gt;Title: MRI/Dev.: Collaborative-Research Augmented Immersive Virtual Environment Laboratory (CRAIVE-Lab)&lt;br/&gt;&lt;br/&gt;Project Proposed:&lt;br/&gt;This project, developing a specialized virtual-reality (VR) system, introduces a new concept for an immersive virtual environment which is optimized for collaborative group activities. The system addresses the auditory and visual senses with equal priority, which is currently only championed by head-mounted display systems. Unlike the latter, this system also enables direct communication between participants without the need for telecommunication devices. The system?s haptic floor display can be used to recreate floor vibrations (e.g., simulate concert stages) with an accuracy that is currently only known for smaller displays (e.g., car simulators).&lt;br/&gt;&lt;br/&gt;The work addresses the need for specialized virtual reality (VR) system for the study and enablement of communication-driven tasks with groups of users immersed in a high-fidelity multi-modal environment located in the same physical space. While current multi-modal VR systems have achieved a high degree of realism, they either focus on the immersion of a single or very small group of users or on presenting material to a larger group of users in a cinema-type environment. In both cases, the systems provide homogeneous visual and acoustic fields. For group communication tasks, inhomogeneous fields that provide personalized visual and acoustic perspectives or each user, could provide better access to relevant information from the VR system?s display and at the same time increase the experiential degree of presence and perceived realism for interactive tasks. The project addresses the technical hurdles that need to be surmounted to establish a large-scale (18mx12mx4.3m), muti-user, muti-perspective, muti-model display. For visual domain, multiple point-of-convergence rendering techniques will be used to (re-)create scenes on a 7-projector display.. For the acoustic domain, a 192-loudspeaker-channel system will be designed for Wave-Field Synthesis (WFS) with the support of Higher-Order-Ambisonic (HoA) sound projection to render inhomogeneous acoustic fields. A haptic display, consisting of 16 platforms elements, will be used to simulate floor vibrations and also to provide infrastructure for other vibrating objects (e.g., handheld devices). An intelligent position-tracking system estimates current user positions and head orientations as well as positioning data for other objects. For the tracking system, a hybrid visual/acoustic sensor system will be used to emulate the human activity of extracting robust information by relying on different modalities.&lt;br/&gt;&lt;br/&gt;Broader Impacts: &lt;br/&gt;With new tools to study human perception, the system will enable research to explore new multi-modal, multi-user data displays that strategically utilize human ability to integrate cross-model sensory information. It will also serve as a platform to study interactions between humans and humanoid intelligent systems that can simulate environments at different degrees of complexity. The instrumentation supports research that will include the development of new interfaces for people with disabilities. The CRAIVE system will be an integrative facility for three school-wide centers, the center for Cognition, Communication, and Culture (CCC), the CCNI supercomputer, and the Experimental Media and Performing Arts Center (EMPC) to their full potential for collaborative research in this area, and to serve as a training platform for students who will engage in one of the enabled research areas.</AbstractNarration>
<MinAmdLetterDate>08/18/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1229391</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Radke</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard Radke</PI_FULL_NAME>
<EmailAddress>rjradke@ecse.rpi.edu</EmailAddress>
<PI_PHON>5182766483</PI_PHON>
<NSF_ID>000185530</NSF_ID>
<StartDate>08/18/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Barbara</FirstName>
<LastName>Cutler</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Barbara M Cutler</PI_FULL_NAME>
<EmailAddress>cutler@cs.rpi.edu</EmailAddress>
<PI_PHON>5182763274</PI_PHON>
<NSF_ID>000494028</NSF_ID>
<StartDate>08/18/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jonas</FirstName>
<LastName>Braasch</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonas Braasch</PI_FULL_NAME>
<EmailAddress>braasj@rpi.edu</EmailAddress>
<PI_PHON>5182763864</PI_PHON>
<NSF_ID>000186127</NSF_ID>
<StartDate>08/18/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Johannes</FirstName>
<LastName>Goebel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Johannes Goebel</PI_FULL_NAME>
<EmailAddress>jeg@rpi.edu</EmailAddress>
<PI_PHON>5182766000</PI_PHON>
<NSF_ID>000565704</NSF_ID>
<StartDate>08/18/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ben</FirstName>
<LastName>Chang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ben Chang</PI_FULL_NAME>
<EmailAddress>changb3@rpi.edu</EmailAddress>
<PI_PHON>5182766000</PI_PHON>
<NSF_ID>000615452</NSF_ID>
<StartDate>08/18/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rensselaer Polytechnic Institute</Name>
<CityName>Troy</CityName>
<ZipCode>121803522</ZipCode>
<PhoneNumber>5182766000</PhoneNumber>
<StreetAddress>110 8TH ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002430742</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RENSSELAER POLYTECHNIC INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002430742</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rensselaer Polytechnic Institute]]></Name>
<CityName>Troy</CityName>
<StateCode>NY</StateCode>
<ZipCode>121803522</ZipCode>
<StreetAddress><![CDATA[110 8th Streeet]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~300001</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Within this project, the Collaborative <em>Research Augmented Immersive Virtual Environment Laboratory (CRAIVE-Lab)</em> was developed and built. The CRAIVE-Lab project addresses the need for a specialized immersive display system to study and enable communication driven tasks with groups of users, who are immersed in a high fidelity multimodal environment. The lab has a useable floor area of 10 m x 12 m with a screen height of 4.3 m. It can host up to 49 people. For the visual domain, an eight-projector front-projection display has been designed to (re)create scenes on a seamless panoramic screen based on 12000 x 1200 pixels. For the acoustic modality, 134 full-range loudspeakers are used to create complex, immersive sound fields. A subset of the 128 loudspeakers is positioned equidistantly around the whole circumference of the lab. The loudspeakers are placed behind the micro-perforated screen material at ear level. The remaining six loudspeakers are mounted on the ceiling grid. An acoustically absorptive curtain is mounted behind the loudspeakers for acoustic treatment. Multiple rendering techniques including Wave Field Synthesis (WFS), Higher-Order Ambisonics, and Virtual Microphone Control (ViMiC) can be utilized to spatialize live or recorded sound with real-time control.</p> <p>The <em>intellectual merit</em> of the CRAIVE-Lab is its unique design to foster collaborative virtual reality tasks. Multiple users can interact with each other or robots in the same space without intrusive devices like VR goggles, 3D glasses or headphones. An intelligent 6-camera tracking system estimates current user positions as well as positioning data for other objects. A 16-channel spherical microphone with additional shotgun microphones is being used to track people and objects acoustically. The microphone system can extract speech and other meaningful sound information using beamforming techniques.</p> <p>One of the <em>broader impacts</em> of the CRAIVE-Lab is its ongoing use as a next-generation classroom. As part of this activity, Rensselaer has been using the lab for regular architecture classes to teach architectural design at human scale. The laboratory also serves as a testbed to study human perception and behavior, and it contributes to the <em>Cognitive and Immersive Systems Laboratory (CISL)</em>, an initiative between Rensselaer and IBM to marry immersive environments with cognitive computing applications.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/31/2016<br>      Modified by: Jonas&nbsp;Braasch</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216183895_CRAIVE_Arch--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216183895_CRAIVE_Arch--rgov-800width.jpg" title="Architecture Studio Class in the CRAIVE-Lab"><img src="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216183895_CRAIVE_Arch--rgov-66x44.jpg" alt="Architecture Studio Class in the CRAIVE-Lab"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Still image from an Architecture Design Studio Demonstration Video (https://youtu.be/y1hC2zePASU)</div> <div class="imageCredit">Rensselaer Polytechnic Institute</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jonas&nbsp;Braasch</div> <div class="imageTitle">Architecture Studio Class in the CRAIVE-Lab</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216349526_CRAIVE3--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216349526_CRAIVE3--rgov-800width.jpg" title="Music Recording Session in the CRAIVE-Lab"><img src="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216349526_CRAIVE3--rgov-66x44.jpg" alt="Music Recording Session in the CRAIVE-Lab"></a> <div class="imageCaptionContainer"> <div class="imageCaption">CRAIVE-Lab Recording Session with J. Braasch, Z. Layton, and P. Oliveros using the Dan Harpole Cistern Simulation (Visual rendering Jeff Carter, live audio rendering J. Braasch et al.).</div> <div class="imageCredit">Rensselaer Polytechnic Institute</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jonas&nbsp;Braasch</div> <div class="imageTitle">Music Recording Session in the CRAIVE-Lab</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216468558_RAckFront--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216468558_RAckFront--rgov-800width.jpg" title="Audio/Visual Rendering Unit for CRAIVE"><img src="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216468558_RAckFront--rgov-66x44.jpg" alt="Audio/Visual Rendering Unit for CRAIVE"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Audio/Visual Rendering Unit for the CRAIVE-Lab</div> <div class="imageCredit">Rensselaer Polytechnic Institute</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jonas&nbsp;Braasch</div> <div class="imageTitle">Audio/Visual Rendering Unit for CRAIVE</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216636382_Cable2--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216636382_Cable2--rgov-800width.jpg" title="Loudspeaker Setup in the CRAIVE-Lab"><img src="/por/images/Reports/POR/2016/1229391/1229391_10203043_1483216636382_Cable2--rgov-66x44.jpg" alt="Loudspeaker Setup in the CRAIVE-Lab"></a> <div class="imageCaptionContainer"> <div class="imageCaption">View from the back to show the loudspeaker mounting and cabling with a total of 2 miles in cables. Photo shows the set up before the acoustic curtain was mounted.</div> <div class="imageCredit">Rensselaer Polytechnic Institute</div> <div class="imageSubmitted">Jonas&nbsp;Braasch</div> <div class="imageTitle">Loudspeaker Setup in the CRAIVE-Lab</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Within this project, the Collaborative Research Augmented Immersive Virtual Environment Laboratory (CRAIVE-Lab) was developed and built. The CRAIVE-Lab project addresses the need for a specialized immersive display system to study and enable communication driven tasks with groups of users, who are immersed in a high fidelity multimodal environment. The lab has a useable floor area of 10 m x 12 m with a screen height of 4.3 m. It can host up to 49 people. For the visual domain, an eight-projector front-projection display has been designed to (re)create scenes on a seamless panoramic screen based on 12000 x 1200 pixels. For the acoustic modality, 134 full-range loudspeakers are used to create complex, immersive sound fields. A subset of the 128 loudspeakers is positioned equidistantly around the whole circumference of the lab. The loudspeakers are placed behind the micro-perforated screen material at ear level. The remaining six loudspeakers are mounted on the ceiling grid. An acoustically absorptive curtain is mounted behind the loudspeakers for acoustic treatment. Multiple rendering techniques including Wave Field Synthesis (WFS), Higher-Order Ambisonics, and Virtual Microphone Control (ViMiC) can be utilized to spatialize live or recorded sound with real-time control.  The intellectual merit of the CRAIVE-Lab is its unique design to foster collaborative virtual reality tasks. Multiple users can interact with each other or robots in the same space without intrusive devices like VR goggles, 3D glasses or headphones. An intelligent 6-camera tracking system estimates current user positions as well as positioning data for other objects. A 16-channel spherical microphone with additional shotgun microphones is being used to track people and objects acoustically. The microphone system can extract speech and other meaningful sound information using beamforming techniques.  One of the broader impacts of the CRAIVE-Lab is its ongoing use as a next-generation classroom. As part of this activity, Rensselaer has been using the lab for regular architecture classes to teach architectural design at human scale. The laboratory also serves as a testbed to study human perception and behavior, and it contributes to the Cognitive and Immersive Systems Laboratory (CISL), an initiative between Rensselaer and IBM to marry immersive environments with cognitive computing applications.              Last Modified: 12/31/2016       Submitted by: Jonas Braasch]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
