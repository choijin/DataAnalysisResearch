<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Observing the world through the lenses of social media</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2013</AwardEffectiveDate>
<AwardExpirationDate>02/29/2020</AwardExpirationDate>
<AwardTotalIntnAmount>499964.00</AwardTotalIntnAmount>
<AwardAmount>595964</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Every day, millions of people across the world take photos and upload them to social media websites. Their goal is to share photos with friends and others, but collectively they are creating vast repositories of visual information about the world. Each photo is an observation of how the world looked at a particular point in time and space. Aggregated together, these photos could provide new sources of observational data for use in disciplines like biology, earth science, social science or history. This project is investigating the algorithms and technologies needed for mining these large collections of photographs and noisy metadata to draw inferences about the physical world. The project has four research thrusts: (1) investigating techniques for identifying and correcting noise in metadata like geo-tags and timestamps, (2) developing algorithms for extracting semantic information from images and metadata, (3) creating methods for robust aggregation of noisy evidence from multiple photos, (4) validating these techniques on interdisciplinary applications in biology, sociology, and earth science.&lt;br/&gt;&lt;br/&gt;The project is laying the foundation for using visual social media as a new source of observational data for a variety of scientific disciplines. The educational component is preparing students for the next generation of "big data" jobs through new undergraduate and graduate courses and online instructional materials. Undergraduate students (particularly from under-represented groups) are recruited to participate in the research program and encouraged to pursue scientific careers. An annual workshop is planned to educate general audiences, particularly senior citizens, about data mining and social media. Source code, datasets, course materials, and other results of the project will be disseminated to the public via the project web site (http://vision.soic.indiana.edu/career/).</AbstractNarration>
<MinAmdLetterDate>02/27/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/24/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1253549</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Crandall</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Crandall</PI_FULL_NAME>
<EmailAddress>djcran@indiana.edu</EmailAddress>
<PI_PHON>8128561115</PI_PHON>
<NSF_ID>000571471</NSF_ID>
<StartDate>02/27/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474083912</ZipCode>
<StreetAddress><![CDATA[901 E Tenth St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~116873</FUND_OBLG>
<FUND_OBLG>2014~109556</FUND_OBLG>
<FUND_OBLG>2015~115999</FUND_OBLG>
<FUND_OBLG>2016~116807</FUND_OBLG>
<FUND_OBLG>2017~120729</FUND_OBLG>
<FUND_OBLG>2018~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Cameras are seemingly everywhere, from smartphones to self-driving cars, microscopes to telescopes, and movie sets to wearable devices, and capture an astonishing quantity and variety of visual data. Each photo uploaded to social media is a record of how the world looked at a particular point in time and space. Video from wearable cameras and eye gaze trackers record what people are doing and paying attention to. Visual data from scientific instruments observe biological and physical processes with unprecedented fidelity. Aggregated together, these observations could provide new sources of data about the state of the world and human behavior, impacting a range of scientific disciplines and consumer applications. However, while today&rsquo;s computers make it easy to generate and store images, their ability to actually understand visual data is limited.&nbsp;This project developed new automatic algorithms for organizing and understanding large collections of imagery, and then applied them to important problems in diverse fields such as psychology, ecology, biology, and earth science.</p> <p>For example, the project developed new techniques for reconstructing three-dimensional scenes from multiple two-dimensional images using deep machine learning and probabilistic graphical models. We applied these techniques to reconstruct major world landmarks using photos uploaded from social media websites and showed how the resulting 3d models could be useful for studies of architecture, archaeology, and city planning. In another application, we created algorithms that model the 3d structure of ice sheets near the Earth&rsquo;s poles from radar images, creating a new technique for earth scientists to track changes in polar ice due to global climate change.</p> <p>The project also developed new algorithms for recognizing the content of images using deep machine learning. We created techniques that estimate where on Earth a photo was taken using the visual information alone, and that identify subtle visual patterns such as changes in architectural styles over time. We investigated using image recognition to identify important content in photos of nature, such as weather and presence of plants and animals. We showed that by automatically identifying these natural phenomena in time-stamped, geotagged imagery from social media, we could give ecologists and biologists a new technique for tracking plants and animals at a continental scale. We also showed that analysis of geotagged social media could help quantify tourist activity, including to identify ecologically-sensitive protected sites that may be in danger from tourism and to measure the effect of wars and other conflicts on tourist activity.</p> <p>A major contribution of the project was to investigate computer vision for wearable camera devices. Wearable cameras are becoming popular both in consumer applications (e.g., GoPro cameras, smart glasses, etc.) and government and industrial applications (e.g., police body cameras, Augmented Reality devices, etc.). This project developed algorithms to solve novel computer vision problems specific to wearable cameras, such as locating and segmenting the hands, identifying actions of the camera wearer, tracking moving people across wearable cameras, automatically inferring the camera motion, and predicting attention (gaze) of the camera wearer. We evaluated these algorithms in multiple applications including self-driving cars (to, for example, predict the movement of other cars and pedestrians and identify potential accidents as early as possible), lifelogging, and assistive devices for people with visual impairments.</p> <p>We also applied these algorithms to video collected by developmental psychologists, who use wearable cameras and eye gaze trackers to study how children learn. Our analyses let us identify patterns in the way that children observe and interact with new objects during important learning moments. We conducted computational experiments in which we tested how training data with different properties, including those similar to the children&rsquo;s views, affects recognition accuracy. Our results suggest that children&rsquo;s views have unique visual properties &ndash; such as a greater diversity of viewpoints -- that may help them learn efficiently. Using these insights from developmental psychology about how children learn so well could help us create the next generation of computer vision algorithms.</p> <p>The project directly involved and educated a total of about 25 undergraduate and graduate students as research assistants and summer research interns. The project supported the development of new online classes in computer vision and artificial intelligence. To help build community around interdisciplinary research areas, the project supported the founding of three new workshop series at major international conferences, in the areas of egocentric computer vision, social media mining for ecology, and computer vision for improving privacy.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/16/2020<br>      Modified by: David&nbsp;Crandall</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Cameras are seemingly everywhere, from smartphones to self-driving cars, microscopes to telescopes, and movie sets to wearable devices, and capture an astonishing quantity and variety of visual data. Each photo uploaded to social media is a record of how the world looked at a particular point in time and space. Video from wearable cameras and eye gaze trackers record what people are doing and paying attention to. Visual data from scientific instruments observe biological and physical processes with unprecedented fidelity. Aggregated together, these observations could provide new sources of data about the state of the world and human behavior, impacting a range of scientific disciplines and consumer applications. However, while today’s computers make it easy to generate and store images, their ability to actually understand visual data is limited. This project developed new automatic algorithms for organizing and understanding large collections of imagery, and then applied them to important problems in diverse fields such as psychology, ecology, biology, and earth science.  For example, the project developed new techniques for reconstructing three-dimensional scenes from multiple two-dimensional images using deep machine learning and probabilistic graphical models. We applied these techniques to reconstruct major world landmarks using photos uploaded from social media websites and showed how the resulting 3d models could be useful for studies of architecture, archaeology, and city planning. In another application, we created algorithms that model the 3d structure of ice sheets near the Earth’s poles from radar images, creating a new technique for earth scientists to track changes in polar ice due to global climate change.  The project also developed new algorithms for recognizing the content of images using deep machine learning. We created techniques that estimate where on Earth a photo was taken using the visual information alone, and that identify subtle visual patterns such as changes in architectural styles over time. We investigated using image recognition to identify important content in photos of nature, such as weather and presence of plants and animals. We showed that by automatically identifying these natural phenomena in time-stamped, geotagged imagery from social media, we could give ecologists and biologists a new technique for tracking plants and animals at a continental scale. We also showed that analysis of geotagged social media could help quantify tourist activity, including to identify ecologically-sensitive protected sites that may be in danger from tourism and to measure the effect of wars and other conflicts on tourist activity.  A major contribution of the project was to investigate computer vision for wearable camera devices. Wearable cameras are becoming popular both in consumer applications (e.g., GoPro cameras, smart glasses, etc.) and government and industrial applications (e.g., police body cameras, Augmented Reality devices, etc.). This project developed algorithms to solve novel computer vision problems specific to wearable cameras, such as locating and segmenting the hands, identifying actions of the camera wearer, tracking moving people across wearable cameras, automatically inferring the camera motion, and predicting attention (gaze) of the camera wearer. We evaluated these algorithms in multiple applications including self-driving cars (to, for example, predict the movement of other cars and pedestrians and identify potential accidents as early as possible), lifelogging, and assistive devices for people with visual impairments.  We also applied these algorithms to video collected by developmental psychologists, who use wearable cameras and eye gaze trackers to study how children learn. Our analyses let us identify patterns in the way that children observe and interact with new objects during important learning moments. We conducted computational experiments in which we tested how training data with different properties, including those similar to the children’s views, affects recognition accuracy. Our results suggest that children’s views have unique visual properties &ndash; such as a greater diversity of viewpoints -- that may help them learn efficiently. Using these insights from developmental psychology about how children learn so well could help us create the next generation of computer vision algorithms.  The project directly involved and educated a total of about 25 undergraduate and graduate students as research assistants and summer research interns. The project supported the development of new online classes in computer vision and artificial intelligence. To help build community around interdisciplinary research areas, the project supported the founding of three new workshop series at major international conferences, in the areas of egocentric computer vision, social media mining for ecology, and computer vision for improving privacy.          Last Modified: 07/16/2020       Submitted by: David Crandall]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
