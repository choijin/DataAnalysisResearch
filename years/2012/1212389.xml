<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Preparing Lattice QCD for Accelerated Computing and Future Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>296997.00</AwardTotalIntnAmount>
<AwardAmount>296997</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03010000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>PHY</Abbreviation>
<LongName>Division Of Physics</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The nature of high performance computing is undergoing rapid change with the advent of general purpose GPU-accelerated architectures.  These changes open new opportunities for scientific discovery at a much reduced capital and energy cost, but to exploit them, new algorithms and methods are required.  &lt;br/&gt;&lt;br/&gt;This project supports the collaborative development and implementation of GPU (Graphical Processing Unit) codes and innovative algorithms for the numerical simulation of the strong interactions of quarks and gluons, namely quantum chromodynamics (QCD), on a lattice.  Specifically, this project supports the development of CUDA (parallel coding model from NVIDIA)-based code modules and OpenMP threading that can be used with the MILC [Multiple Instruction (and Multiple Data) Lattice Calculation] code.  This 200,000-line code suite is used by several research groups around the world and supports some 100 million core-hours of computing at NSF and DOE national centers and laboratories mostly on conventional supercomputers.  Our code development will allow improved performance on GPU clusters and on computers such as Blue Waters at NCSA that contain both GPUs and conventional multicore processors.  Performance models developed will guide the performance tuning and help to plan future calculations and should also be of value in studing the effectiveness of exascale architectures. &lt;br/&gt;&lt;br/&gt;Interpretation of many of the experimental results in particle physics is currently limited by lack of theoretical understanding.  This reduces our ability to determine parameters of the Standard Model and to find evidence for physics beyond the Standard Model through precision experiments.  The calculations that are enabled are thus important for the interpretation of many experiments in elementary particle physics.  Some of these experiments at Fermilab, Cornell, and at SLAC have been completed.  Others such as in Beijing, Geneva, and Japan are still running.  &lt;br/&gt;&lt;br/&gt;We have publicized previous work on code development and performance modeling by presentations at appropriate conference and publication.  We will continue to do so for the current work in order that others can benefit from our methodology and innovations. The MILC code is widely used in benchmarking by several high-performance computing centers.  This in turn has led to several vendors contacting us regarding performance of our code on their current and future chips.  Thus, having GPU-accelerated code available in MILC could help in the evaluation of or co-design of future high-performance computing architectures.  &lt;br/&gt;&lt;br/&gt;Finally this project will contribute to the professional education and training of future scientists via the participation of postdoctoral reseachers.</AbstractNarration>
<MinAmdLetterDate>07/09/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1212389</AwardID>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Gottlieb</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven A Gottlieb</PI_FULL_NAME>
<EmailAddress>sg@indiana.edu</EmailAddress>
<PI_PHON>8128550243</PI_PHON>
<NSF_ID>000166188</NSF_ID>
<StartDate>07/09/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474057105</ZipCode>
<StreetAddress><![CDATA[727 E Third Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramReference>
<Code>7569</Code>
<Text>CYBERINFRASTRUCTURE/SCIENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~98999</FUND_OBLG>
<FUND_OBLG>2013~98999</FUND_OBLG>
<FUND_OBLG>2014~98999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This was a collaborative project conducted by Indiana University and the National Center for Supercomputing Applications (NCSA) at the University of Illinois.&nbsp; The goal of the project was to enhance the software capable of running on graphical processing units (GPUs) for carrying out research in lattice Quantum Chromodynamics (QCD).&nbsp; The software developed in this project to run on the GPUs is now an integral part of the QUDA library and can be found at https://github.com/lattice/quda.&nbsp; The software that is part of the MILC code can be found at https://github.com/milc-qcd/milc_qcd.&nbsp; QUDA is based on CUDA, an extension to the C programming language developed by the video hardware company NVIDIA to ease massively parallel program development for non-video applications on their GPUs.<br /><br />The software developed under this project includes gauge fixing and enhancements to heavy-quark solvers important for running Blue Waters.&nbsp; In addition, software was developed (and development continues) for generalizing MILC QCD software to support study of both quantum chromodynamics and electromagnetism.&nbsp; The inclusion of electromagnetism is important because lattice QCD calculations are now reaching sub-percent level precision on a number of quantities and electromagnetism can be significant on that level.&nbsp; The MILC collaboration has used quenched quantum electrodynamics to study the ratio of the up and down quark masses and would like to be able to do a fully dynamical calculation that avoids the quenched approximation.<br /><br />During the course of this project, four postdoctoral researchers were supported at one time or another.&nbsp; Of the two who were employed the longest, one is doing postdoctoral research in his native Portugal and the other is employed by NVIDIA in his native Germany where he remains a leading developer of QUDA.&nbsp; Two postdocs were also employed for shorter periods of time.&nbsp; One went on to a postdoc at Fermilab and then transitioned to an industrial position in Silicon Valley in data science.&nbsp; The other is still employed at Indiana University and will be working on exascale computing applications with Department of Energy support.</p><br> <p>            Last Modified: 09/07/2017<br>      Modified by: Steven&nbsp;A&nbsp;Gottlieb</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This was a collaborative project conducted by Indiana University and the National Center for Supercomputing Applications (NCSA) at the University of Illinois.  The goal of the project was to enhance the software capable of running on graphical processing units (GPUs) for carrying out research in lattice Quantum Chromodynamics (QCD).  The software developed in this project to run on the GPUs is now an integral part of the QUDA library and can be found at https://github.com/lattice/quda.  The software that is part of the MILC code can be found at https://github.com/milc-qcd/milc_qcd.  QUDA is based on CUDA, an extension to the C programming language developed by the video hardware company NVIDIA to ease massively parallel program development for non-video applications on their GPUs.  The software developed under this project includes gauge fixing and enhancements to heavy-quark solvers important for running Blue Waters.  In addition, software was developed (and development continues) for generalizing MILC QCD software to support study of both quantum chromodynamics and electromagnetism.  The inclusion of electromagnetism is important because lattice QCD calculations are now reaching sub-percent level precision on a number of quantities and electromagnetism can be significant on that level.  The MILC collaboration has used quenched quantum electrodynamics to study the ratio of the up and down quark masses and would like to be able to do a fully dynamical calculation that avoids the quenched approximation.  During the course of this project, four postdoctoral researchers were supported at one time or another.  Of the two who were employed the longest, one is doing postdoctoral research in his native Portugal and the other is employed by NVIDIA in his native Germany where he remains a leading developer of QUDA.  Two postdocs were also employed for shorter periods of time.  One went on to a postdoc at Fermilab and then transitioned to an industrial position in Silicon Valley in data science.  The other is still employed at Indiana University and will be working on exascale computing applications with Department of Energy support.       Last Modified: 09/07/2017       Submitted by: Steven A Gottlieb]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
