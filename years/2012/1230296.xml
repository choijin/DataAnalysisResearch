<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase II:  Single-Channel Stationary/Non-Stationary Speech Extraction for Mobile Phones</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>494250.00</AwardTotalIntnAmount>
<AwardAmount>1096090</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This Small Business Innovation Research (SBIR) Phase II project addresses the problem of everyday noisy environments that limit when and where people can be heard clearly over various communication devices (phones, first-responder radios, voice-over-IP, etc.). Given the inability of single-microphone noise-reduction techniques to handle non-stationary noise (e.g. restaurant noise) the industry's current solution is the use of dual-microphone techniques.  However, dual-microphone techniques cost more, require more hardware, need spatial separation between the noise and speech, and can only process the signal on the transmit side. The proposed technology is a single-microphone software-only solution that effectively handles non-stationary noise coming from any direction and can process the signal on both the transmit and receive sides.  The novelty of the approach is its use of speech-specific characteristics and knowledge of human perception to extract speech from the noisy signal.  The objective of the proposed research is to improve the current method of detecting voice activity, enabling better speech extraction thereby resulting in enhanced speech quality. The resulting technology will be architecture agnostic, cost effective and have superior performance in everyday situations.&lt;br/&gt;&lt;br/&gt;The broader impact/commercial potential of this project is considerable and compelling.  The initial focus will be the mobile phone industry, which is now the single largest user of noise suppression products with a market size that includes nearly all of humanity.  The technology will then be optimized to improve the listening experience for hundreds of millions of potential hearing-aid/cochlear implant users worldwide. According to the World Health Organization, there are 278 million people worldwide that have hearing loss. This number is expected to at least double over the next 30 years due to the growth in the number of senior citizens over 65 years of age and the growth in the number of younger people who are needing hearing aids 20 years sooner than their parents due to loud-music listening habits. Additional markets include first-responder radios, voice-over-IP and military/intelligence/homeland-security. The research required to reduce computational complexity will illuminate the essential aspects of auditory scene analysis needed for improved speech perception.</AbstractNarration>
<MinAmdLetterDate>08/07/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1230296</AwardID>
<Investigator>
<FirstName>Amit</FirstName>
<LastName>Juneja</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amit Juneja</PI_FULL_NAME>
<EmailAddress>amjuneja@gmail.com</EmailAddress>
<PI_PHON>3014057411</PI_PHON>
<NSF_ID>000615280</NSF_ID>
<StartDate>08/07/2012</StartDate>
<EndDate>11/15/2012</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Bartusek</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John Bartusek</PI_FULL_NAME>
<EmailAddress>jbartusek@omni-speech.com</EmailAddress>
<PI_PHON>3014058861</PI_PHON>
<NSF_ID>000627006</NSF_ID>
<StartDate>11/15/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>OmniSpeech</Name>
<CityName>College Pard</CityName>
<ZipCode>207423371</ZipCode>
<PhoneNumber>3014057411</PhoneNumber>
<StreetAddress>c/o Venture Accelerator Program</StreetAddress>
<StreetAddress2><![CDATA[387 Technology Drive, Ste 1105]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832795764</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OMNISPEECH, LLC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[OmniSpeech]]></Name>
<CityName>College Pard</CityName>
<StateCode>MD</StateCode>
<ZipCode>207423371</ZipCode>
<StreetAddress><![CDATA[c/o Venture Accelerator Program]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5373</Code>
<Text>SBIR Phase II</Text>
</ProgramElement>
<ProgramReference>
<Code>152E</Code>
<Text>Cyber-Physical Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>165E</Code>
<Text>SBIR Phase IIB</Text>
</ProgramReference>
<ProgramReference>
<Code>169E</Code>
<Text>SBIR Tech Enhan Partner (TECP)</Text>
</ProgramReference>
<ProgramReference>
<Code>5373</Code>
<Text>SMALL BUSINESS PHASE II</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~494250</FUND_OBLG>
<FUND_OBLG>2014~601840</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>During the time of the Phase II/IIB, we optimized the OmniSpeech single channel speech enhancement technology, OmniClear, for both voice clarity performance and computational efficiency. We also developed a fixed-point C code reference implementation suitable for fixed-point processors, and we benchmarked the computational efficiency of it on development platforms using an ARM Cortex A8/9 processor.   We had an independent testing lab evaluate the voice quality performance of the single channel fixed-point code and compare it to other 2 and 3 microphone Noise Suppression solutions currently available on the market in smartphones.   During the award period we also started work and gained considerable experience working with Acoustic Echo Canceller (AEC) solutions so we could offer a bundled solution (including both AEC and Noise Suppression technologies) to OEM vendors which preferred this type of package.  While working with and testing the AEC, we identified several issues with its performance that required a solution, and after the award period concluded we continued to work on the AEC to address these issues as well as enhance the OmniClear Noise Suppression.</p> <p>Following the award period, we succeeded in integrating OmniClear into a smartphone platform (using Qualcomm Snapdragon processor with Hexagon DSP) so we could make a real time call demonstration available to smartphone OEM vendors for testing and evaluating the technology.  The demo smartphone uses the OmniClear solution in the voice path when making a call.   Following the award period, we also integrated the OmniClear solution into a smartwatch product prototype  (using the ST Microelectronics STM32F7 microcontroller) which is currently under contract with an OEM vendor.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/15/2016<br>      Modified by: John&nbsp;Bartusek</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ During the time of the Phase II/IIB, we optimized the OmniSpeech single channel speech enhancement technology, OmniClear, for both voice clarity performance and computational efficiency. We also developed a fixed-point C code reference implementation suitable for fixed-point processors, and we benchmarked the computational efficiency of it on development platforms using an ARM Cortex A8/9 processor.   We had an independent testing lab evaluate the voice quality performance of the single channel fixed-point code and compare it to other 2 and 3 microphone Noise Suppression solutions currently available on the market in smartphones.   During the award period we also started work and gained considerable experience working with Acoustic Echo Canceller (AEC) solutions so we could offer a bundled solution (including both AEC and Noise Suppression technologies) to OEM vendors which preferred this type of package.  While working with and testing the AEC, we identified several issues with its performance that required a solution, and after the award period concluded we continued to work on the AEC to address these issues as well as enhance the OmniClear Noise Suppression.  Following the award period, we succeeded in integrating OmniClear into a smartphone platform (using Qualcomm Snapdragon processor with Hexagon DSP) so we could make a real time call demonstration available to smartphone OEM vendors for testing and evaluating the technology.  The demo smartphone uses the OmniClear solution in the voice path when making a call.   Following the award period, we also integrated the OmniClear solution into a smartwatch product prototype  (using the ST Microelectronics STM32F7 microcontroller) which is currently under contract with an OEM vendor.             Last Modified: 08/15/2016       Submitted by: John Bartusek]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
