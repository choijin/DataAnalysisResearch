<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Development of a GPU-Enabled, Petascale Active Storage Architecture for Data-Intensive Applications in HPC and Cloud Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Proposal #: 12-29282&lt;br/&gt;PI(s):  Skjellum, Anthony &lt;br/&gt;  Bangalore, Purushotham; Hasan, Ragib; Zhang, Chengcui&lt;br/&gt;Institution: University of Alabama at Birmingham&lt;br/&gt;Title:   MRI/Dev.: A GPU-Enabled, Petascale Active Storage Architecture for Data-Intensive Applications in  HPC and Cloud Environments&lt;br/&gt;Project Proposed:&lt;br/&gt;This project, developing a 2.4 Petabytes (PB) of raw storage instrument to support a variety of research projects in experimental HPC and cloud storage, aims to both increase local resources for scientific computing and act as a testbed for GPU-enabled reliable storage. The instrument enables an increased virtualization of storage, the concurrent access to storage under fault scenarios (e.g., RAID), and a series of data intensive applications. Lessons learned will be leveraged from the existing system in place, whereby the existing system and the new system will be integrated in a way that supports cloud and disaster recovery modes of operation. The project enables the following studies and research projects: &lt;br/&gt;- Studying of effective rates of errors and reliability at highly refined levels and seeking means to identify and manage additional classes of errors (e.g., misdirected writes);&lt;br/&gt;- Creating semi-analytical models to allow tunable storage characteristics within a lifetime-reliability-performance cost space; &lt;br/&gt;- Running applications from data mining (including bioinformatics as drivers for proving the efficacy of the final system), to achieve new science in these data-intensive domains; and&lt;br/&gt;- Conducting computer science research aimed at simplifying use of active storage computation.&lt;br/&gt;Broader Impacts: &lt;br/&gt;This instrumentation increases the institution?s capacity to conduct cutting-edge research in an inexpensive, fast, practical, reliable petascale storage for data-intensive applications. Significant computational power logically close to that storage enables new science. Student training (including underrepresented groups) will be emphasized. The knowledge dissemination through this effort could be significant.</AbstractNarration>
<MinAmdLetterDate>08/21/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/29/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1229282</AwardID>
<Investigator>
<FirstName>Anthony</FirstName>
<LastName>Skjellum</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anthony Skjellum</PI_FULL_NAME>
<EmailAddress>tony-skjellum@utc.edu</EmailAddress>
<PI_PHON>2058074968</PI_PHON>
<NSF_ID>000446874</NSF_ID>
<StartDate>08/21/2012</StartDate>
<EndDate>09/29/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anthony</FirstName>
<LastName>Skjellum</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anthony Skjellum</PI_FULL_NAME>
<EmailAddress>tony-skjellum@utc.edu</EmailAddress>
<PI_PHON>2058074968</PI_PHON>
<NSF_ID>000446874</NSF_ID>
<StartDate>09/29/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Purushotham</FirstName>
<LastName>Bangalore</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Purushotham Bangalore</PI_FULL_NAME>
<EmailAddress>pvbangalore@ua.edu</EmailAddress>
<PI_PHON>2053482979</PI_PHON>
<NSF_ID>000353999</NSF_ID>
<StartDate>08/21/2012</StartDate>
<EndDate>09/29/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Purushotham</FirstName>
<LastName>Bangalore</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Purushotham Bangalore</PI_FULL_NAME>
<EmailAddress>pvbangalore@ua.edu</EmailAddress>
<PI_PHON>2053482979</PI_PHON>
<NSF_ID>000353999</NSF_ID>
<StartDate>09/29/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Chengcui</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Chengcui Zhang</PI_FULL_NAME>
<EmailAddress>czhang02@uab.edu</EmailAddress>
<PI_PHON>2059348606</PI_PHON>
<NSF_ID>000483727</NSF_ID>
<StartDate>08/21/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ragib</FirstName>
<LastName>Hasan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ragib Hasan</PI_FULL_NAME>
<EmailAddress>ragib@uab.edu</EmailAddress>
<PI_PHON>2059348643</PI_PHON>
<NSF_ID>000604882</NSF_ID>
<StartDate>08/21/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Alabama at Birmingham</Name>
<CityName>Birmingham</CityName>
<ZipCode>352940001</ZipCode>
<PhoneNumber>2059345266</PhoneNumber>
<StreetAddress>AB 1170</StreetAddress>
<StreetAddress2><![CDATA[1720 2nd Avenue South]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<StateCode>AL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>063690705</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ALABAMA AT BIRMINGHAM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>808245794</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Alabama at Birmingham]]></Name>
<CityName>Birmingham</CityName>
<StateCode>AL</StateCode>
<ZipCode>352940001</ZipCode>
<StreetAddress><![CDATA[1300 University Blvd.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">The goal of this effort is to develop an architecture with two Petabytes of raw storage instrumented to support a variety of research projects in experimental HPC and cloud storage. &nbsp;One of the key outcomes of this work is the design and deployment of a cost-effective and reliable storage-centric Petascale cloud storage using commodity hardware components and open source software components. The storage system consists of four I/O servers each with two 10-core Intel Xeon CPUs, 128 GB RAM, a K40 NVIDIA GPU, dual SAS cards, two 400GB SSD drives, FDR InfiniBand, and 10Gbps Ethernet network cards. Each I/O server is connected to two disk arrays, each disk array has 48 6TB disk drives. The storage system is accessed through two head nodes that are connected both to the FDR InfiniBand and 10Gbps Ethernet networks. This system uses the Ceph distributed object store and file system to expose the storage units to applications by providing an erasure plugin that uses the GPU-based RAID library (Gibraltar). A GPU-based erasure code plugin for Ceph was developed and evaluated as part of this work. Using the Hadoop plugin for Ceph, we were also able to use this storage cluster to support MapReduce applications. &nbsp;This effort directly supported two Ph.D. student's dissertations, one MS thesis, and several other graduate students were indirectly supported through access to the storage cluster and GPU cluster and also provided valuable training in the design, evaluation, and use of a petascale storage system and related system software. The results of this research effort were published and presented at peer-reviewed conferences and workshops as well as the research results were disseminated through the project website. &nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/29/2016<br>      Modified by: Purushotham&nbsp;Bangalore</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The goal of this effort is to develop an architecture with two Petabytes of raw storage instrumented to support a variety of research projects in experimental HPC and cloud storage.  One of the key outcomes of this work is the design and deployment of a cost-effective and reliable storage-centric Petascale cloud storage using commodity hardware components and open source software components. The storage system consists of four I/O servers each with two 10-core Intel Xeon CPUs, 128 GB RAM, a K40 NVIDIA GPU, dual SAS cards, two 400GB SSD drives, FDR InfiniBand, and 10Gbps Ethernet network cards. Each I/O server is connected to two disk arrays, each disk array has 48 6TB disk drives. The storage system is accessed through two head nodes that are connected both to the FDR InfiniBand and 10Gbps Ethernet networks. This system uses the Ceph distributed object store and file system to expose the storage units to applications by providing an erasure plugin that uses the GPU-based RAID library (Gibraltar). A GPU-based erasure code plugin for Ceph was developed and evaluated as part of this work. Using the Hadoop plugin for Ceph, we were also able to use this storage cluster to support MapReduce applications.  This effort directly supported two Ph.D. student's dissertations, one MS thesis, and several other graduate students were indirectly supported through access to the storage cluster and GPU cluster and also provided valuable training in the design, evaluation, and use of a petascale storage system and related system software. The results of this research effort were published and presented at peer-reviewed conferences and workshops as well as the research results were disseminated through the project website.            Last Modified: 12/29/2016       Submitted by: Purushotham Bangalore]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
