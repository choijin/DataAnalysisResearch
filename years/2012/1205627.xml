<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CI-P:Collaborative Research: Visual entailment data set and challenge for the Language and Vision Community</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
<AwardExpirationDate>10/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>41000.00</AwardTotalIntnAmount>
<AwardAmount>41000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Vision and language provide fundamental means to interpret, learn, and communicate about the world around us. A primary goal of computer vision and natural language processing research is therefore to automatically uncover and analyze the information that images and video, or text and speech, convey about the world.  Both communities are concerned with tasks that require increasingly deeper understanding, including the ability to reason with and draw inferences from this information.  Since vision and language are complementary modalities, there is now also an increasing amount of work at the interface of both fields. However, progress in multimodal analysis requires a tighter collaboration between the two communities, since each currently relies on its own set of techniques, datasets and evaluation criteria. &lt;br/&gt;&lt;br/&gt;This community planning grant explores the need for, feasibility, and usefulness of a "visual entailment" corpus and associated visual entailment recognition task. In natural language, entailment recognition is the problem of determining whether a particular statement can be inferred from a text document. This project explores a novel related problem - visual entailment - where the goal is to determine whether a statement in natural language can be inferred from an image or video.  The outcomes of the project include a novel dataset and prototype research challenge, as well as increased collaboration between the vision and language communities.</AbstractNarration>
<MinAmdLetterDate>06/20/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1205627</AwardID>
<Investigator>
<FirstName>Julia</FirstName>
<LastName>Hockenmaier</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Julia C Hockenmaier</PI_FULL_NAME>
<EmailAddress>juliahmr@illinois.edu</EmailAddress>
<PI_PHON>2173332187</PI_PHON>
<NSF_ID>000237005</NSF_ID>
<StartDate>06/20/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~41000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!--StartFragment--> <p>The aim of this community infrastructure planning grant was to develop a new task at the interface of computer vision and natural language processing, which we called 'visual entailment recognition', to work towards the creation of new data sets for this task, and to bring together the vision and natural language communities to collaborate on this task.</p> <p>The goal of the visual entailment recognition task is to develop systems that can automatically decide what statements can and cannot be inferred from an image or video clip. One aspect of this task is sentence-based image description: i..e the ability to associate images with sentences that describe what is going on in the image.&nbsp;</p> <p>We have produced a large dataset of 30K images that are each associated with 5 sentences.&nbsp;We focused on images that depict a wide variety of everyday events and situations, so these sentences describe the people (or animals), their actions, as well as the location of the images. This data set is publicly available on our website, and is already widely used by researchers in academia and industry to develop new image retrieval and description algorithms that have been covered in e.g. the New York Times and the BBC.</p> <p>We have also given tutorials on image description at CVPR 2014 and EACL 2014. The materials for these tutorials are also available on our websites.</p> <p>Finally, we have held a workshop for vision and language researchers at NAACL 2013.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/12/2015<br>      Modified by: Julia&nbsp;C&nbsp;Hockenmaier</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The aim of this community infrastructure planning grant was to develop a new task at the interface of computer vision and natural language processing, which we called 'visual entailment recognition', to work towards the creation of new data sets for this task, and to bring together the vision and natural language communities to collaborate on this task.  The goal of the visual entailment recognition task is to develop systems that can automatically decide what statements can and cannot be inferred from an image or video clip. One aspect of this task is sentence-based image description: i..e the ability to associate images with sentences that describe what is going on in the image.   We have produced a large dataset of 30K images that are each associated with 5 sentences. We focused on images that depict a wide variety of everyday events and situations, so these sentences describe the people (or animals), their actions, as well as the location of the images. This data set is publicly available on our website, and is already widely used by researchers in academia and industry to develop new image retrieval and description algorithms that have been covered in e.g. the New York Times and the BBC.  We have also given tutorials on image description at CVPR 2014 and EACL 2014. The materials for these tutorials are also available on our websites.  Finally, we have held a workshop for vision and language researchers at NAACL 2013.          Last Modified: 01/12/2015       Submitted by: Julia C Hockenmaier]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
