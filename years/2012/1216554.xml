<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Methods for Stochastic and Nonlinear Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>120000.00</AwardTotalIntnAmount>
<AwardAmount>120000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rosemary Renaut</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The projects described in this proposal are designed to advance the capabilities of optimization methods for a class of stochastic and deterministic optimization problems. The first project focuses on  problems where the objective function is given by an expectation or a loss function. We propose dynamic sample algorithms that attempt to bridge the gap between stochastic and batch  methods. Their essential characteristic is that they adapt the sample size during the progression of the optimization in a manner that leads to low computational effort and high accuracy in the solution, when so desired. The second project deals with the design of new active-set methods for solving constrained optimization and convex regularized L1 problems.  Our work builds on two algorithms recently proposed in the literature: the block active-set method (also called the primal-dual active-set method), and the orthant-wise method  for solving L1 regularized problems. Our new algorithms are provably convergent and applicable to a wider class of applications. The third project addresses the need to improve the robustness of nonlinear optimization methods in the presence of infeasibility. Our first goal is to design an interior point method endowed with infeasibility detection capabilities, and to show how its main mechanism can be extended to other interior point methods. The second goal is to develop a convergence theory that is applicable to both active set and interior point methods consisting of three components:  an optimization phase, a feasibility phase, and a mechanism for transitioning between the two phases.&lt;br/&gt;&lt;br/&gt;The methods developed in this project are useful in big data analysis, which is playing a vital role in genomics, materials science, meteorology, climate modeling and  information science. In all these disciplines, vast amounts of data have become available in the last decade, with the rate of  generation  accelerating exponentially.  The challenge is to process this large amount of information to make inferences and predictions, thereby accelerating our basic understanding of physical and social systems. For example, the complex physics simulations employed in the design of advanced materials, meteorology and climate modeling, require the use of detailed information obtained over a large set of scenarios. The optimization and machine learning methods developed in this project can be integrated in support of such simulations, thereby obviating the need for  extremely complex models that are difficult to study and generalize. Our work has direct impact in genomics and other areas of biology. For example, we plan to investigate its use in metagenomics, specifically de novo assembly of next generation DNA sequencing data.  Sequences  can be tagged with markers, or found in reference data sets like transcriptomes.  A goal is to use this new information to enable faster and more accurate de novo assembly.  In computer science and information technology, our new algorithms will be useful in the development of a new generation of speech recognition and computer vision systems. Speech recognition, which will play an increasingly important role in many technological applications, can only advance by incorporating more data more intelligently, and the algorithms described in this proposal are designed precisely for that purpose.</AbstractNarration>
<MinAmdLetterDate>07/24/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1216554</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Byrd</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard H Byrd</PI_FULL_NAME>
<EmailAddress>richard@cs.colorado.edu</EmailAddress>
<PI_PHON>3034928014</PI_PHON>
<NSF_ID>000369868</NSF_ID>
<StartDate>07/24/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090572</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, 572 UCB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~120000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The reseach supported by this collaborative grant is lead jointly by the PI, Richard Byrd of the University of Colorado, and Jorge Nocedal of Northwestern University. The principal outcome is the development and improved understanding of optimization algorithms relevant to machine learning.&nbsp; Machine learning strives to make predictions by adjusting parameters in a prediction function in away that agrees most closely with a vast collection of examples.&nbsp; The algorithms developed in the grant research are useful in this effort in that they provide a way to adjust these parameters to maximize agreement with the example data.</p> <p>Our work on stochastic optimation is important in this process because it provides a way to adjust the parameters in an optimization without looking at all of the data. It goes beyond existing methods in that it takes into account the curvature of the objective function (agreement with example data). This quasi-Newton approximation of curvature reduces the number of iterations required.</p> <p>We also worked on adding to the objective function a penalty proportional to the sum of the absolute values of the parameters (L1 regularization). This tends to reduce the number of parameters in the prediction function and provide a more robust and effective predictor.&nbsp; However, the fact that the absolute value function is not differentiable is a challenge to standard optimization methods. Our work on "a flexible active set strategy" describes an algorithm for optimizing for a quadratic function with L1 regularization that operates by interleaving steps of the well known ISTA method with steps of the conjugate gradient method.&nbsp; Our work on an "Inexact Successive Quadratic Approximation Method" is for a general smooth objective function with L1 regularization. It provides guidelines for approximately optimizing a quadratic&nbsp; approximation to the original objective plus the L1 term. The resluting method has strong theoretical convergence guarantees and performs well.</p> <p>Finally, together Paul Boggs, we have developed a important improvement to the limited memory BFGS method, which is widely used in many applications including machine learning.&nbsp; As currently used, there are no reliable guidelines for choice of the crucial memory size parameter. We have developed an adaptive for choosing the memory size parameter at each iteration. This approach gives performance close to the best chice of memory most of the time at a cost of a 50 percent increase in computational overhead.</p><br> <p>            Last Modified: 10/27/2016<br>      Modified by: Richard&nbsp;H&nbsp;Byrd</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The reseach supported by this collaborative grant is lead jointly by the PI, Richard Byrd of the University of Colorado, and Jorge Nocedal of Northwestern University. The principal outcome is the development and improved understanding of optimization algorithms relevant to machine learning.  Machine learning strives to make predictions by adjusting parameters in a prediction function in away that agrees most closely with a vast collection of examples.  The algorithms developed in the grant research are useful in this effort in that they provide a way to adjust these parameters to maximize agreement with the example data.  Our work on stochastic optimation is important in this process because it provides a way to adjust the parameters in an optimization without looking at all of the data. It goes beyond existing methods in that it takes into account the curvature of the objective function (agreement with example data). This quasi-Newton approximation of curvature reduces the number of iterations required.  We also worked on adding to the objective function a penalty proportional to the sum of the absolute values of the parameters (L1 regularization). This tends to reduce the number of parameters in the prediction function and provide a more robust and effective predictor.  However, the fact that the absolute value function is not differentiable is a challenge to standard optimization methods. Our work on "a flexible active set strategy" describes an algorithm for optimizing for a quadratic function with L1 regularization that operates by interleaving steps of the well known ISTA method with steps of the conjugate gradient method.  Our work on an "Inexact Successive Quadratic Approximation Method" is for a general smooth objective function with L1 regularization. It provides guidelines for approximately optimizing a quadratic  approximation to the original objective plus the L1 term. The resluting method has strong theoretical convergence guarantees and performs well.  Finally, together Paul Boggs, we have developed a important improvement to the limited memory BFGS method, which is widely used in many applications including machine learning.  As currently used, there are no reliable guidelines for choice of the crucial memory size parameter. We have developed an adaptive for choosing the memory size parameter at each iteration. This approach gives performance close to the best chice of memory most of the time at a cost of a 50 percent increase in computational overhead.       Last Modified: 10/27/2016       Submitted by: Richard H Byrd]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
