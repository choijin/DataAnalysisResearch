<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: PaRSEC: Parallel Runtime Scheduling and Execution Control</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>196980.00</AwardTotalIntnAmount>
<AwardAmount>196980</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Trends in the design of high-performance computing systems are making them more difficult to use effectively.  This EAGER award focuses on the problem of managing massive parallelism while effectively exploiting locality.  Its goal is to develop breakthrough techniques for parallel runtime systems that will support libraries, applications, and other software infrastructure on the next generation of high-performance systems.&lt;br/&gt;&lt;br/&gt;The PI proposes to develop the Parallel Runtime Scheduling and Execution Control system (PaRSEC), which will serve as a prototype for novel ideas about parallel runtime systems.  The PI's approach will be based on scalable directed acyclic graph (DAG) scheduling techniques that will track data dependencies between tasks.  The scheduling structures will be designed to handle billions of tasks running on millions of computational nodes.  The scheduling framework will also have to handle task migration and load balancing to maximize parallelism while preserving data locality.  The prototypes developed as part of this EAGER will provide the foundation for future research on parallel linear algebra routines for extreme-scale computing systems.</AbstractNarration>
<MinAmdLetterDate>08/27/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1244905</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Dongarra</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack J Dongarra</PI_FULL_NAME>
<EmailAddress>dongarra@icl.utk.edu</EmailAddress>
<PI_PHON>8659748295</PI_PHON>
<NSF_ID>000299281</NSF_ID>
<StartDate>08/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName>Knoxville</CityName>
<StateCode>TN</StateCode>
<ZipCode>379960003</ZipCode>
<StreetAddress><![CDATA[1 circle park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~196980</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><!--   @font-face  {font-family:"?? ??";  panose-1:0 0 0 0 0 0 0 0 0 0;  mso-font-charset:128;  mso-generic-font-family:roman;  mso-font-format:other;  mso-font-pitch:fixed;  mso-font-signature:1 134676480 16 0 131072 0;} @font-face  {font-family:"?? ??";  panose-1:0 0 0 0 0 0 0 0 0 0;  mso-font-charset:128;  mso-generic-font-family:roman;  mso-font-format:other;  mso-font-pitch:fixed;  mso-font-signature:1 134676480 16 0 131072 0;}   p.MsoNormal, li.MsoNormal, div.MsoNormal  {mso-style-unhide:no;  mso-style-qformat:yes;  mso-style-parent:";  margin:0in;  margin-bottom:.0001pt;  text-align:justify;  text-justify:inter-ideograph;  mso-pagination:widow-orphan;  font-size:11.0pt;  mso-bidi-font-size:12.0pt;  font-family:"Times New Roman";  mso-fareast-font-family:"?? ??";  mso-fareast-theme-font:minor-fareast;  mso-bidi-font-family:"Times New Roman";  mso-bidi-theme-font:minor-bidi;} .MsoChpDefault  {mso-style-type:export-only;  mso-default-props:yes;  font-family:Cambria;  mso-ascii-font-family:Cambria;  mso-ascii-theme-font:minor-latin;  mso-fareast-font-family:"?? ??";  mso-fareast-theme-font:minor-fareast;  mso-hansi-font-family:Cambria;  mso-hansi-theme-font:minor-latin;  mso-bidi-font-family:"Times New Roman";  mso-bidi-theme-font:minor-bidi;} @page WordSection1  {size:8.5in 11.0in;  margin:1.0in 1.25in 1.0in 1.25in;  mso-header-margin:.5in;  mso-footer-margin:.5in;  mso-paper-source:0;} div.WordSection1  {page:WordSection1;} --></p> <p class="MsoNormal" style="text-align: left;">The purpose of the <em style="mso-bidi-font-style: normal;">Parallel Runtime Scheduler and Execution Controller</em> (PaRSEC) system is to provide a programming environment that facilitates development of scientific and engineering computing software packages for the largest supercomputers. Such supercomputers are built of hundreds of cabinets, each cabinet containing tens of circuit boards / server blades, each board containing multiple processor (CPU) sockets, each socket housing a chip with multiple CPU cores. The total number of cores in the largest systems reaches millions. The boards often also contain more exotic chips, such as Graphic Processing Units (GPUs), which serve as hardware accelerators to speed up critical portions of the computation. The system is interconnected with a high performance network, which connects all computing components.</p> <p class="MsoNormal" style="text-align: left;">PaRSEC&rsquo;s solution to programming such large and complex systems is twofold: 1) PaRSEC defines a programming model that shields the software developer from the complexities of modern supercomputers, and 2) PaRSEC provides the software layer that executes users&rsquo; code on such a system using dataflow principles, i.e., schedules work to cores and moves data around as necessary through the complex hierarchy of the memory system. PaRSEC accomplishes its objectives by representing an algorithm as a collection of tasks, i.e., atomic operations, executed by a single CPU core or a single GPU processing unit. The tasks are connected with edges representing the flow of data between the tasks, and referred to as the task graph, or more formally, a <em style="mso-bidi-font-style: normal;">Direct Acyclic Graph</em> (DAG). DAG scheduling is the main principle of PaRSEC's operation.</p> <p class="MsoNormal" style="text-align: left;">One of the main areas of PaRSEC's application is the field of dense linear algebra, where DAGs are commonly encountered, which cannot be built entirely without exceeding the memory capacity of the hardware. To address this problem, PaRSEC relies on a symbolic representation of the DAG, called a <em style="mso-bidi-font-style: normal;">Parametrized Task Graph</em> (PTG), which allows for describing large DAGs in a compact manner.</p> <p class="MsoNormal" style="text-align: left;">By the end of this project's funding cycl...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The purpose of the Parallel Runtime Scheduler and Execution Controller (PaRSEC) system is to provide a programming environment that facilitates development of scientific and engineering computing software packages for the largest supercomputers. Such supercomputers are built of hundreds of cabinets, each cabinet containing tens of circuit boards / server blades, each board containing multiple processor (CPU) sockets, each socket housing a chip with multiple CPU cores. The total number of cores in the largest systems reaches millions. The boards often also contain more exotic chips, such as Graphic Processing Units (GPUs), which serve as hardware accelerators to speed up critical portions of the computation. The system is interconnected with a high performance network, which connects all computing components. PaRSECÆs solution to programming such large and complex systems is twofold: 1) PaRSEC defines a programming model that shields the software developer from the complexities of modern supercomputers, and 2) PaRSEC provides the software layer that executes usersÆ code on such a system using dataflow principles, i.e., schedules work to cores and moves data around as necessary through the complex hierarchy of the memory system. PaRSEC accomplishes its objectives by representing an algorithm as a collection of tasks, i.e., atomic operations, executed by a single CPU core or a single GPU processing unit. The tasks are connected with edges representing the flow of data between the tasks, and referred to as the task graph, or more formally, a Direct Acyclic Graph (DAG). DAG scheduling is the main principle of PaRSEC's operation. One of the main areas of PaRSEC's application is the field of dense linear algebra, where DAGs are commonly encountered, which cannot be built entirely without exceeding the memory capacity of the hardware. To address this problem, PaRSEC relies on a symbolic representation of the DAG, called a Parametrized Task Graph (PTG), which allows for describing large DAGs in a compact manner. By the end of this project's funding cycle, the PaRSEC system was successfully used to automatically translate an important subset of the PLASMA numerical library for shared-memory systems, to form the core of the DPLASMA library for distributed memory systems with multicore processors and accelerators. The subset includes crucial numerical operations, such as solution of linear systems of equations and least square problems. This objective has been accomplished by providing the serial code from PLASMA (loop nests) to the front-end/compiler layer of PaRSEC, which automatically translates them to the PTG form. The PTG representation of the DAG is then executed by the runtime component of PaRSEC, which takes care of dynamically scheduling work to CPU cores and GPU accelerators and moving the data around as necessary. The performance delivered by PaRSEC for these routines is superior to the performance delivered by the ScaLAPACK software, which&mdash;at the time of this writing&mdash;is the only viable alternative. The impact of PaRSEC has both cyberinfrastructure and educational aspects. In terms of cyberinfrastructure, PaRSEC represents the type of software that most future software libraries and applications will require in order to achieve high performance on tomorrowÆs large scale and highly hybrid supercomputers. The educational aspect of PaRSEC is in automating code development for large-scale machines through a transparent methodology aimed at broadening the user's insight into his/her computational problem and its performance/parallelism/scalability characteristics.       Last Modified: 10/24/2013       Submitted by: Jack J Dongarra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
