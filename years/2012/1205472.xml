<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>II-NEW: Collaborative Research: COMET: A Web Infrastructure for Research and Experimentation in User Interactive Event Driven Testing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>332104.00</AwardTotalIntnAmount>
<AwardAmount>332104</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>User interactive event driven software is pervasive today.  End users interact with applications by pointing, clicking or touching the interface, and as this happens, the programs respond.  Ensuring the dependability of these systems through software testing is paramount, because insufficient testing techniques currently cost the US economy billions of dollars annually.  Yet the flexibility of these systems, which make them appealing to users, also increases the difficulty of testing them. This difficulty has fueled a large body of research on user interactive event driven testing, but the newly developed techniques are often evaluated using isolated case studies and experiments. The user interactive event driven testing community lacks a common set of benchmarks and tools for evaluating their new methods, leading to experimental mismatch; the results of one study are difficult to compare with another. The lack of common benchmarks also means that we cannot easily combine results of multiple studies to build a larger body of knowledge.&lt;br/&gt; &lt;br/&gt;This project reduces the mismatch and is advancing user interactive event driven testing research by developing a shared research and experimentation web infrastructure called COMET.  An initial proof of concept for COMET was developed through earlier support from NSF.  Factors that contribute to the mismatch include the development of platform specific test methods, test harnesses that require customizations for each test subject application and each operating system, and models that are incompatible with one another.  The research will devise new techniques to control the testing environment, contextualizing factors that may affect experimental outcome and will allow for evolution and change of the artifacts over time.  It is building a shared and extensible web infrastructure of benchmarks, tools, models and test artifacts that will enable scientific discovery in the state-of-the-art of user interactive event driven testing.   COMET is a public resource that will be available to a broader community.  Its impact will extend not only to the user interactive event testing research community, but also to others that work with user interfaces such as those who study usability, and to industry and the software testing community at large. The project work will involve both graduate and undergraduate students. Artifacts from the COMET website will be utilized for educational purposes in the classroom.</AbstractNarration>
<MinAmdLetterDate>08/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1205472</AwardID>
<Investigator>
<FirstName>Myra</FirstName>
<LastName>Cohen</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Myra B Cohen</PI_FULL_NAME>
<EmailAddress>mcohen@iastate.edu</EmailAddress>
<PI_PHON>5152948305</PI_PHON>
<NSF_ID>000124813</NSF_ID>
<StartDate>08/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName>Lincoln</CityName>
<StateCode>NE</StateCode>
<ZipCode>685880430</ZipCode>
<StreetAddress><![CDATA[312 N 14th St, Alex Bldg West]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>1714</Code>
<Text>SPECIAL PROJECTS - CISE</Text>
</ProgramReference>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~195838</FUND_OBLG>
<FUND_OBLG>2013~136266</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A large body of research has focused on developing software testing techniques for applications that have user interactive (often graphical) interfaces. However, we lack ways to fairly compare results between researchers, since there has been no common set of benchmarks. In addition, controlling the outcomes of tests so that they produce the same result each time is non-trivial because this type of testing is dependent on complex test harnesses and is environmentally dependent (starting state of application, operating system, tools, and system resources). In this project we built a website called Community Event-based Testing or (COMET) (comet.unl.edu) to collate and share benchmarking artifacts for this purpose. This website allows registered users to download the artifacts in order to repeat our experiments and to compare their new approaches against ours. As users of COMET increase over time, we expect many novel testing techniques to be developed that can be fairly compared between different research groups. Throughout the life of this grant we have also studied fundamental research questions on topics such as test suite repeatability, test suite repair and test oracles.</p> <p><strong>Intellectual Merit:&nbsp; </strong>We have developed a website, and created a common format&nbsp; for user interactive testing artifacts and have included a template to allow members of the broader community to contribute .We have uploaded 29 benchmark programs (including several that were contributed by others in the community). The benchmarks include a variety of applications and include test cases, models, tools and coverage/fault matrices when available. We have made an effort to provide full environmental conditions so that our experiments are repeatable.&nbsp; Almost 200 users have registered on our site which gives unlimited access to the artifacts. To date, 14 papers have been published by authors who have utilized our artifacts. We have also made fundamental research contributions in test suite repair, in reducing test case flakiness and begun to study these issues in the mobile domain. We have published many of these papers in top software engineering venues.</p> <p><strong>Broader Impacts: </strong>During the course of this grant we have involved (and published papers with) both graduate and undergraduate students on this research, some of whom are from underrepresented groups.&nbsp; The research has led to multiple MS/PHD theses and several of the students&nbsp; have gone on to graduate studies at other Universities.&nbsp; Students working on this project have also had the opportunity to attend conferences and present their research to the broader community. &nbsp; We have had industrial partners on some of this work, and many of the registered users on COMET are from industry.&nbsp; PIs Cohen and Memon have given several invited talks&nbsp; both at conferences and at companies and presented a tutorial on controlling test flakiness at ICSE 2013.&nbsp; Finally, both PIs have incorporated material from COMET in their courses at their respective universities.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/14/2016<br>      Modified by: Myra&nbsp;B&nbsp;Cohen</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1205472/1205472_10197929_1479098742996_comet--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1205472/1205472_10197929_1479098742996_comet--rgov-800width.jpg" title="COMET"><img src="/por/images/Reports/POR/2016/1205472/1205472_10197929_1479098742996_comet--rgov-66x44.jpg" alt="COMET"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Community Event Based Testing: http://comet.unl.edu</div> <div class="imageCredit">COMET Project</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Myra&nbsp;B&nbsp;Cohen</div> <div class="imageTitle">COMET</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A large body of research has focused on developing software testing techniques for applications that have user interactive (often graphical) interfaces. However, we lack ways to fairly compare results between researchers, since there has been no common set of benchmarks. In addition, controlling the outcomes of tests so that they produce the same result each time is non-trivial because this type of testing is dependent on complex test harnesses and is environmentally dependent (starting state of application, operating system, tools, and system resources). In this project we built a website called Community Event-based Testing or (COMET) (comet.unl.edu) to collate and share benchmarking artifacts for this purpose. This website allows registered users to download the artifacts in order to repeat our experiments and to compare their new approaches against ours. As users of COMET increase over time, we expect many novel testing techniques to be developed that can be fairly compared between different research groups. Throughout the life of this grant we have also studied fundamental research questions on topics such as test suite repeatability, test suite repair and test oracles.  Intellectual Merit:  We have developed a website, and created a common format  for user interactive testing artifacts and have included a template to allow members of the broader community to contribute .We have uploaded 29 benchmark programs (including several that were contributed by others in the community). The benchmarks include a variety of applications and include test cases, models, tools and coverage/fault matrices when available. We have made an effort to provide full environmental conditions so that our experiments are repeatable.  Almost 200 users have registered on our site which gives unlimited access to the artifacts. To date, 14 papers have been published by authors who have utilized our artifacts. We have also made fundamental research contributions in test suite repair, in reducing test case flakiness and begun to study these issues in the mobile domain. We have published many of these papers in top software engineering venues.  Broader Impacts: During the course of this grant we have involved (and published papers with) both graduate and undergraduate students on this research, some of whom are from underrepresented groups.  The research has led to multiple MS/PHD theses and several of the students  have gone on to graduate studies at other Universities.  Students working on this project have also had the opportunity to attend conferences and present their research to the broader community.   We have had industrial partners on some of this work, and many of the registered users on COMET are from industry.  PIs Cohen and Memon have given several invited talks  both at conferences and at companies and presented a tutorial on controlling test flakiness at ICSE 2013.  Finally, both PIs have incorporated material from COMET in their courses at their respective universities.             Last Modified: 11/14/2016       Submitted by: Myra B Cohen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
