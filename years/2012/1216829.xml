<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small:Conductor: A Run-Time System for Exascale Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>One of the critical problems---if not the critical problem---in&lt;br/&gt;reaching the exascale computing goal by the end of the decade is the&lt;br/&gt;power problem.  Exascale systems have a target power constraint of 20&lt;br/&gt;megawatts even though today's petascale systems---which have&lt;br/&gt;performance at least two orders of magnitude below prospective&lt;br/&gt;exascale systems---generally consume around 5 megawatts.  Hardware&lt;br/&gt;improvements alone will not bridge this gap.&lt;br/&gt;&lt;br/&gt;The PI is developing a run-time system called Conductor to address the&lt;br/&gt;power issue.  The overall goal of Conductor is to produce near-optimal&lt;br/&gt;application performance under a prescribed power bound.  Conductor&lt;br/&gt;carries this out by allocating power both between and within nodes.&lt;br/&gt;First, the PI is designing and implementing a new technique called&lt;br/&gt;power scheduling, which addresses the inter-node case.  The second&lt;br/&gt;part of Conductor addresses the intra-node case with power gating,&lt;br/&gt;which allows powering off of individual components in a more&lt;br/&gt;fine-grain manner than is generally available in architectures today.&lt;br/&gt;Finally, the PI is investigating techniques to allow users to assist&lt;br/&gt;Conductor, through annotations, in achieving high performance in cases&lt;br/&gt;where power scheduling and power gating alone are not sufficient.&lt;br/&gt;&lt;br/&gt;The impact of the research described in this proposal is significant.&lt;br/&gt;Conductor is essential to achieving exascale performance on nontrivial&lt;br/&gt;applications, and it will help push towards the exascale goal.&lt;br/&gt;Achieving exascale performance is an important national priority and&lt;br/&gt;will impact many application domains.  The PI is also integrating the&lt;br/&gt;research ideas into both undergraduate and graduate parallel and&lt;br/&gt;distributed computing courses.</AbstractNarration>
<MinAmdLetterDate>09/11/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/11/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1216829</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Lowenthal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Lowenthal</PI_FULL_NAME>
<EmailAddress>dkl@cs.arizona.edu</EmailAddress>
<PI_PHON>5206268282</PI_PHON>
<NSF_ID>000516447</NSF_ID>
<StartDate>09/11/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arizona</Name>
<CityName>Tucson</CityName>
<ZipCode>857194824</ZipCode>
<PhoneNumber>5206266000</PhoneNumber>
<StreetAddress>888 N Euclid Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>806345617</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARIZONA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072459266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arizona]]></Name>
<CityName/>
<StateCode>AZ</StateCode>
<ZipCode>857210001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>One of the critical problems---if not the critical problem---in<br />reaching the exascale computing goal by the end of the decade is the<br />power problem.&nbsp; Exascale systems have a target power constraint of 20<br />megawatts, and the performance and power consumptions of today's<br />petascale systems are not on a target to achieve this goal.&nbsp; Hardware<br />improvements alone will not bridge this gap.&nbsp; This project developed<br />software infrastructure to address the power/performance gap so that<br />the exascale goal will be more easily reached.<br /><br />The intellectual merit of this work was in several areas.&nbsp; First, we<br />showed that using an overprovisioned supercomputer can lead to better<br />performance across a range of applications, while still meeting a<br />power constraint.&nbsp; In particular, we found that overprovisioning leads<br />to an average speedup of more than 50% over worst-case provisioning,<br />which is the current state-of-the-art.<br /><br />Second, we developed a follow-on run-time system, called Conductor,<br />that helped to allocate power to applications so that performance is<br />improved.&nbsp; Over a set of high-performance computing benchmarks,<br />Conductor leads to a best-case performance improvement of up to 30%<br />and an average improvement of 19%.</p> <p>Third, we developed novel scheduling algorithms for overprovisioned<br />clusters that improve throughput on such machines while maintaining a<br />system-wide power bound.&nbsp; Our results show that our new scheduling<br />policy increases system power utilization while adhering to strict<br />job-level power bounds.&nbsp; It leads to 31% (19% on average) and 54% (36%<br />on average) faster average turnaround time when compared to worst-case<br />provisioning and naive overprovisioning respectively.<br /><br />The broader impact of the research described in this proposal is<br />significant.&nbsp; Conductor is essential to achieving exascale performance<br />on nontrivial applications, and it will help push towards the exascale<br />goal.&nbsp; Achieving exascale performance is an important national<br />priority and will impact many application domains.&nbsp; Notably, parts of<br />our work on power-aware scheduling is being considered for inclusion<br />in the next-generation scheduler being developed at Lawrence Livermore<br />National Laboratory, which has a large user base.&nbsp; In addition,<br />Conductor is in final form and will soon be distributed freely.&nbsp; The PI also integrated the research ideas into both undergraduate and graduate parallel and distributed computing courses.&nbsp;&nbsp; Specifically, both courses now have a power/performance component.</p><br> <p>            Last Modified: 10/20/2017<br>      Modified by: David&nbsp;Lowenthal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ One of the critical problems---if not the critical problem---in reaching the exascale computing goal by the end of the decade is the power problem.  Exascale systems have a target power constraint of 20 megawatts, and the performance and power consumptions of today's petascale systems are not on a target to achieve this goal.  Hardware improvements alone will not bridge this gap.  This project developed software infrastructure to address the power/performance gap so that the exascale goal will be more easily reached.  The intellectual merit of this work was in several areas.  First, we showed that using an overprovisioned supercomputer can lead to better performance across a range of applications, while still meeting a power constraint.  In particular, we found that overprovisioning leads to an average speedup of more than 50% over worst-case provisioning, which is the current state-of-the-art.  Second, we developed a follow-on run-time system, called Conductor, that helped to allocate power to applications so that performance is improved.  Over a set of high-performance computing benchmarks, Conductor leads to a best-case performance improvement of up to 30% and an average improvement of 19%.  Third, we developed novel scheduling algorithms for overprovisioned clusters that improve throughput on such machines while maintaining a system-wide power bound.  Our results show that our new scheduling policy increases system power utilization while adhering to strict job-level power bounds.  It leads to 31% (19% on average) and 54% (36% on average) faster average turnaround time when compared to worst-case provisioning and naive overprovisioning respectively.  The broader impact of the research described in this proposal is significant.  Conductor is essential to achieving exascale performance on nontrivial applications, and it will help push towards the exascale goal.  Achieving exascale performance is an important national priority and will impact many application domains.  Notably, parts of our work on power-aware scheduling is being considered for inclusion in the next-generation scheduler being developed at Lawrence Livermore National Laboratory, which has a large user base.  In addition, Conductor is in final form and will soon be distributed freely.  The PI also integrated the research ideas into both undergraduate and graduate parallel and distributed computing courses.   Specifically, both courses now have a power/performance component.       Last Modified: 10/20/2017       Submitted by: David Lowenthal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
