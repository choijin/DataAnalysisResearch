<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC-NIE Integration: SDNX - Enabling End-to-End Dynamic Science DMZ</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>915900.00</AwardTotalIntnAmount>
<AwardAmount>931880</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Multi-Service Exchange (MSX) platform facilitates an environment where the advanced network functions are extended throughout campus over a Software Defined Networking (SDN) solution, e.g., OpenFlow, de facto creating a dynamic Science DMZ driven entirely by science application workflows. Its architecture decouples the campus network operational mission from science data flows dynamically. MSX makes the protocols and interfaces of the wide-area networks, such as Internet2's ION, DYNES, perfSONAR, and others interoperable with the campus SDN solutions such as OpenFlow.  The MSX platform also provides an option for data landing solutions, facilitated by a wide array of campus-facing NAS and SAN storage interfaces. The platform is also made physically accessible to campus researches for manually initiated, scheduled data transport.&lt;br/&gt;&lt;br/&gt;However, the main focus of this effort is to integrate the advanced network functions with individual science applications, algorithms and workflows. By concentrating predominantly on science applications code, the MSX software development team helps domain scientists extricate themselves from the need to maintain either a deep network-related knowledge or specialized staff. The integration of the selected science applications with advanced network technologies lets them treat any network function as a resource - an integral part of their algorithms and workflows.&lt;br/&gt;&lt;br/&gt;By architecting the Multi Service Exchange in the most flexible, adaptable way, MSX will establish a platform applicable to any campus, providing them with a point from which to develop new (or terminate existing) services and make them available to on-campus and off-campus researchers.</AbstractNarration>
<MinAmdLetterDate>09/07/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1246386</AwardID>
<Investigator>
<FirstName>Jaroslav</FirstName>
<LastName>Flidr</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jaroslav Flidr</PI_FULL_NAME>
<EmailAddress>jflidr@gwu.edu</EmailAddress>
<PI_PHON>5712942320</PI_PHON>
<NSF_ID>000621076</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Voss</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian D Voss</PI_FULL_NAME>
<EmailAddress>bdvoss@umd.edu</EmailAddress>
<PI_PHON>3014056269</PI_PHON>
<NSF_ID>000621255</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate>08/05/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tripti</FirstName>
<LastName>Sinha</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tripti Sinha</PI_FULL_NAME>
<EmailAddress>tsinha@umd.edu</EmailAddress>
<PI_PHON>3014056269</PI_PHON>
<NSF_ID>000621387</NSF_ID>
<StartDate>08/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tripti</FirstName>
<LastName>Sinha</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tripti Sinha</PI_FULL_NAME>
<EmailAddress>tsinha@umd.edu</EmailAddress>
<PI_PHON>3014056269</PI_PHON>
<NSF_ID>000621387</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate>08/05/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Ensor</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian D Ensor</PI_FULL_NAME>
<EmailAddress>bensor@gwu.edu</EmailAddress>
<PI_PHON>2023229079</PI_PHON>
<NSF_ID>000621394</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Gallo</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew K Gallo</PI_FULL_NAME>
<EmailAddress>agallo@gwu.edu</EmailAddress>
<PI_PHON>2029946255</PI_PHON>
<NSF_ID>000621427</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~915900</FUND_OBLG>
<FUND_OBLG>2013~15980</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to maximize end-to-end performance of scientific, data-driven workflows of selected Domain Science groups. At the proposal stage we argued that data-intensive science workflows and algorithms must be integrated with the advanced network functionality, which was largely available in the wide-area networks, and that this must happen at the level of science application. We postulated that the application-level integration with advanced network technologies, combined sometimes with the so-called &ldquo;last mile problem,&rdquo; was addressed insufficiently or not addressed at all. As a result, we claimed that the best solution of this problem would be the deployment and utilization of various SDN technologies, and their integration at the protocol and the API level, thus allowing for the extension of network paths all the way to the Domain Science endpoints. To achieve this goal we focused on seven Domain Science groups with diverse data flow patterns and usage models. Our intention was to work with each group individually, and by addressing specific issues related to each particular workflow, to prove that the integration of endpoints with advanced network functions is the most important component of such optimization. Our key design principle was minimal interference with the established workflows and the groups&rsquo; existing mode of operations. We hoped that by following this rule we would create an open framework applicable to virtually any Domain Science group. We have architected and deployed a high-performance, and high-throughput, multi-tenant system &ndash; Multi Service Exchange (MSX) &ndash; that would provide a unified API and translation capabilities allowing the endpoints to request heterogeneous, SDN-provisioned, end-to-end paths. The analysis of the use cases resulted in three initial functional configurations:</p> <ol> <li><em>a dynamic science DMZ </em>&ndash; where the network path is extended all the way from the data sources to the particular Domain Science resources dynamically, by isolating the network path through campus by the use of SDN technologies </li> <li><em>a science DMZ-in-a-box </em>&ndash; where the Domain Science workflow is collapsed partially or entirely to the MSX platform leaving, if anything, only the storage and post-processing for the on-campus locations </li> <li><em>a scheduled data transport service </em>&ndash; where a Domain Scientist would access the MSX platform&rsquo;s storage interfaces physically, allowing for a high-performance data transport to a remote location. </li> </ol> <p>While all the proposed configurations are technologically sound, only #2 &ndash; <em>science DMZ-in-a-box &ndash; </em>gained traction. It turned out that the<em> dynamic science DMZ </em>approach was somewhat na&iuml;ve:</p> <p>a)&nbsp;&nbsp;&nbsp;&nbsp; Virtually no campus CIO will allow deployment of an SDN technology in their enterprise systems. To date, many open SDN solutions are experimental at best. Vendor-based systems exist, <em>e.g.</em> Cisco ACI, but were entirely out of the scope of this effort.</p> <p>b)&nbsp;&nbsp;&nbsp;&nbsp; The need for this level of SDN integration &ndash; <em>any-to-any</em> &ndash; does not really exist. Most of the research groups want to interact with a very small subset of resources (<em>e.g.</em> an HPC cluster, a storage system, or a remote system such as an instrument, or a cloud infrastructure over WAN), not with each other. This pretty much eliminates the need for a generic SDN solution.</p> <p>c)&nbsp;&nbsp;&nbsp;&nbsp; Most importantly, <em>dynamic science DMZ</em> does not scale economically. Even if a) and b) were not true, extending high-performance networks and API&rsquo;s all the way to every single endpoint would require corresponding upgrades of the endpoints&rsquo; capabilities (<em>e.g. </em>most of the existing research systems do not have 10GE interfac...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to maximize end-to-end performance of scientific, data-driven workflows of selected Domain Science groups. At the proposal stage we argued that data-intensive science workflows and algorithms must be integrated with the advanced network functionality, which was largely available in the wide-area networks, and that this must happen at the level of science application. We postulated that the application-level integration with advanced network technologies, combined sometimes with the so-called "last mile problem," was addressed insufficiently or not addressed at all. As a result, we claimed that the best solution of this problem would be the deployment and utilization of various SDN technologies, and their integration at the protocol and the API level, thus allowing for the extension of network paths all the way to the Domain Science endpoints. To achieve this goal we focused on seven Domain Science groups with diverse data flow patterns and usage models. Our intention was to work with each group individually, and by addressing specific issues related to each particular workflow, to prove that the integration of endpoints with advanced network functions is the most important component of such optimization. Our key design principle was minimal interference with the established workflows and the groupsÆ existing mode of operations. We hoped that by following this rule we would create an open framework applicable to virtually any Domain Science group. We have architected and deployed a high-performance, and high-throughput, multi-tenant system &ndash; Multi Service Exchange (MSX) &ndash; that would provide a unified API and translation capabilities allowing the endpoints to request heterogeneous, SDN-provisioned, end-to-end paths. The analysis of the use cases resulted in three initial functional configurations:  a dynamic science DMZ &ndash; where the network path is extended all the way from the data sources to the particular Domain Science resources dynamically, by isolating the network path through campus by the use of SDN technologies  a science DMZ-in-a-box &ndash; where the Domain Science workflow is collapsed partially or entirely to the MSX platform leaving, if anything, only the storage and post-processing for the on-campus locations  a scheduled data transport service &ndash; where a Domain Scientist would access the MSX platformÆs storage interfaces physically, allowing for a high-performance data transport to a remote location.    While all the proposed configurations are technologically sound, only #2 &ndash; science DMZ-in-a-box &ndash; gained traction. It turned out that the dynamic science DMZ approach was somewhat na&iuml;ve:  a)     Virtually no campus CIO will allow deployment of an SDN technology in their enterprise systems. To date, many open SDN solutions are experimental at best. Vendor-based systems exist, e.g. Cisco ACI, but were entirely out of the scope of this effort.  b)     The need for this level of SDN integration &ndash; any-to-any &ndash; does not really exist. Most of the research groups want to interact with a very small subset of resources (e.g. an HPC cluster, a storage system, or a remote system such as an instrument, or a cloud infrastructure over WAN), not with each other. This pretty much eliminates the need for a generic SDN solution.  c)     Most importantly, dynamic science DMZ does not scale economically. Even if a) and b) were not true, extending high-performance networks and APIÆs all the way to every single endpoint would require corresponding upgrades of the endpointsÆ capabilities (e.g. most of the existing research systems do not have 10GE interfaces). Given their generally tight budgets, very few research groups would be able to afford this.  The Scheduled data transport service seems like an attractive solution. However, it suffers from a chronic deficiency in, for lack of a better word, marketing. In an environment where a significant portion of t...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
