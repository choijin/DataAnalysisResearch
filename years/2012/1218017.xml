<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: SMALL: Collaborative Research: Data Structures for Parallel Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>171946.00</AwardTotalIntnAmount>
<AwardAmount>193946</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops a theory for characterizing the performance of parallel data structures and parallel algorithms that use parallel structures.  Standard metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (critical-path length), do not naturally generalize in the presence of contention on shared data.  Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.&lt;br/&gt;&lt;br/&gt;The specific research goals are as follows:  &lt;br/&gt;(1) Investigate a methodology for designing and analyzing parallel algorithms that use data structures, especially amortized ones. &lt;br/&gt;(2) Design parallel schedulers that ameliorate the contention on parallel data structures. &lt;br/&gt;(3) Design parallel data structures that perform provably well with these schedulers.&lt;br/&gt;&lt;br/&gt;Today parallel computing is ubiquitous.  Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors.  Writing parallel code that provably scales well is challenging and techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code.  This project will develop a theoretical foundation for characterizing the scalability of parallel programs that contend for access to shared data.</AbstractNarration>
<MinAmdLetterDate>07/20/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/12/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1218017</AwardID>
<Investigator>
<FirstName>Kunal</FirstName>
<LastName>Agrawal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kunal Agrawal</PI_FULL_NAME>
<EmailAddress>kunal@cse.wustl.edu</EmailAddress>
<PI_PHON>3149354838</PI_PHON>
<NSF_ID>000555177</NSF_ID>
<StartDate>07/20/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Washington University</Name>
<CityName>Saint Louis</CityName>
<ZipCode>631304862</ZipCode>
<PhoneNumber>3147474134</PhoneNumber>
<StreetAddress>CAMPUS BOX 1054</StreetAddress>
<StreetAddress2><![CDATA[1 Brookings Drive]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<StateCode>MO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MO01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>068552207</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068552207</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Washington University]]></Name>
<CityName>St. Louis</CityName>
<StateCode>MO</StateCode>
<ZipCode>631304899</ZipCode>
<StreetAddress><![CDATA[One Brookings Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~171946</FUND_OBLG>
<FUND_OBLG>2013~14000</FUND_OBLG>
<FUND_OBLG>2014~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Helvetica; color: #323333; -webkit-text-stroke: #323333} span.s1 {font-kerning: none} --> <p class="p1"><span class="s1">The goal of this project is to develop a theory for characterizing the performance of parallel data structures and parallel algorithms that use data structures. &nbsp;Standard metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (length of the critical path), do not naturally generalize in the presence of contention on shared data. Moreover, the standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are used in parallel, in part because the performance depends on properties of the scheduler.</span></p> <p class="p1"><span class="s1">The main outcomes of this project are:</span></p> <p class="p1"><span class="s1">1) A new work-stealing scheduler that sits between the program and the data structure and coordinates accesses in order to remove contention. &nbsp;The scheduler is accompanied by a strong performance theorem for the overall running time of parallel programs that use data structures. The performance theorem relies on easy-to-use metrics (in addition to work and span), and the guarantees readily extend even to amortized data structures.</span></p> <p class="p1"><span class="s1">2) A runtime-system implementation, called BATCHER, that implements the above scheduler and makes it easier to incorporate data structures into a parallel program. One of the key advantages of BATCHER is that it uses batched data structures instead of concurrent data structures. Implementing efficient batched operations (using standard parallel constructs) can be much easier than coping with arbitrary concurrency. BATCHER automatically groups operations into batches, essentially transforming a program that makes concurrent accesses into one that makes batched accesses.&nbsp;</span></p> <p class="p1"><span class="s1">3) A new on-the-fly "determinacy race" detector. Two data accesses are logically parallel if they allowed to be reordered with respect to each other by the scheduler. If two accesses to the same location are logically parallel, and at least one of those accesses is a write, then a determinacy race occurs. Determinacy races lead to nondeterministic program behavior, and as such they are difficult to reason about. This nondeterminacy is often unintential and indicates a bug in the program. A race detector is a tool for discovering determinacy races in a program execution. Some determinacy race detectors rely on shared data structures. This project addresses one of those shared data structures and includes an implementation of the resulting determinacy-race detector.</span></p> <p>4) A new record and replay system for dynamically threaded programs. &nbsp;Parallel programs can be difficult to debug due to inherent non-determinism which makes it difficult to reproduce bugs. &nbsp;A record and replay system is a debugging tool where the tester can run the program over and over again under "record mode" until a bug appears. &nbsp;He can then run the program under "replay mode" to reproduce the bug since under the replay mode, the program makes exactly the same non-deterministic choices as under record. &nbsp;This is the first record and replay tool specially designed for dynamically multithreaded programs.</p><br> <p>            Last Modified: 12/17/2016<br>      Modified by: Kunal&nbsp;Agrawal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project is to develop a theory for characterizing the performance of parallel data structures and parallel algorithms that use data structures.  Standard metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (length of the critical path), do not naturally generalize in the presence of contention on shared data. Moreover, the standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are used in parallel, in part because the performance depends on properties of the scheduler. The main outcomes of this project are: 1) A new work-stealing scheduler that sits between the program and the data structure and coordinates accesses in order to remove contention.  The scheduler is accompanied by a strong performance theorem for the overall running time of parallel programs that use data structures. The performance theorem relies on easy-to-use metrics (in addition to work and span), and the guarantees readily extend even to amortized data structures. 2) A runtime-system implementation, called BATCHER, that implements the above scheduler and makes it easier to incorporate data structures into a parallel program. One of the key advantages of BATCHER is that it uses batched data structures instead of concurrent data structures. Implementing efficient batched operations (using standard parallel constructs) can be much easier than coping with arbitrary concurrency. BATCHER automatically groups operations into batches, essentially transforming a program that makes concurrent accesses into one that makes batched accesses.  3) A new on-the-fly "determinacy race" detector. Two data accesses are logically parallel if they allowed to be reordered with respect to each other by the scheduler. If two accesses to the same location are logically parallel, and at least one of those accesses is a write, then a determinacy race occurs. Determinacy races lead to nondeterministic program behavior, and as such they are difficult to reason about. This nondeterminacy is often unintential and indicates a bug in the program. A race detector is a tool for discovering determinacy races in a program execution. Some determinacy race detectors rely on shared data structures. This project addresses one of those shared data structures and includes an implementation of the resulting determinacy-race detector.  4) A new record and replay system for dynamically threaded programs.  Parallel programs can be difficult to debug due to inherent non-determinism which makes it difficult to reproduce bugs.  A record and replay system is a debugging tool where the tester can run the program over and over again under "record mode" until a bug appears.  He can then run the program under "replay mode" to reproduce the bug since under the replay mode, the program makes exactly the same non-deterministic choices as under record.  This is the first record and replay tool specially designed for dynamically multithreaded programs.       Last Modified: 12/17/2016       Submitted by: Kunal Agrawal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
