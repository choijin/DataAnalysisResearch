<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:Small: Data Triggered Threads for Removing Redundant Execution and Increasing Parallelism</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2012</AwardEffectiveDate>
<AwardExpirationDate>05/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As high-performance general-purpose processors advance further into the chip multiprocessor era, with ever increasing core counts, the industry faces two huge challenges.  The first is how to effectively use that hardware parallelism when much of the available software does not parallelize easily.  The second challenge is managing the power and energy consumed by these processors.  Given constrained power, we can only scale performance if we improve the performance delivered per Watt.  &lt;br/&gt;Data-triggered threads (DTT) is a new programming and execution model designed to address both of these issues.  Any conventional architecture that exploits parallelism does so by initiating new parallel computation based on control flow ? that is, execution (i.e., the core?s program counter) reaches a fork instruction or maybe a pthread create call. DTT instead spawns a new thread when data in memory is changed.  The programmer specifies a function that is not attached to a set of call sites (a place in the program where the function is called), but rather is attached to a variable or even a field in a data structure type.  When that variable is modified by the program, a thread in another core (or multithreading context) is automatically spawned to execute the data-triggered thread, containing code that depends on the changed data.&lt;br/&gt;Data-triggered threads provide two key advantages over traditional mechanisms for describing parallelism.  The first is that the dependent computation becomes available immediately, as soon as the source data is modified.  The second is that the dependent computation need only be executed if the triggering data is actually modified ? in many cases it is not.  This work exploits a huge new opportunity to eliminate redundant computation. In the C SPEC benchmarks, 78% of loads are redundant (meaning the same load fetches the same value as the last time it went to the same address).  The computation which operates on those values is often also redundant. Researchers in computer architecture and related areas have been working to reduce the power consumed by each instruction, and have made steady progress. It is far preferable, though, to just not execute those instructions ? no power or energy optimization will beat that.&lt;br/&gt;This research is exploring a number of opportunities to exploit this new execution model, including: (1) Data triggered threads via architectural support, (2) software-only data triggered threads, (3) data triggered threads to complement existing parallel applications, (4) automatic generation of data triggered threads in the compiler, and (5) programming experience with data triggered threads ? how does DTT code written from scratch differ from programs modified to use DTT, what kind of code will novice programmers produce, and how does that impact the architectural and software runtime implementations.&lt;br/&gt;Generation-to-generation performance scaling of processors is critically important to the national and world economy ? not just to hardware vendors, but also the software industries that sell products every time someone upgrades their system. The data triggered threads programming and execution model represents solutions to the two key barriers to performance scaling. It addresses the parallelism crisis by giving the programmer a new way to express parallelism. It addresses the power problem in the most effective way possible ? by not executing computation that does not need to be done.</AbstractNarration>
<MinAmdLetterDate>05/16/2012</MinAmdLetterDate>
<MaxAmdLetterDate>04/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1219059</AwardID>
<Investigator>
<FirstName>Dean</FirstName>
<LastName>Tullsen</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dean M Tullsen</PI_FULL_NAME>
<EmailAddress>tullsen@cs.ucsd.edu</EmailAddress>
<PI_PHON>8585346181</PI_PHON>
<NSF_ID>000461702</NSF_ID>
<StartDate>05/16/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930404</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Dr.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As the gap between available hardware parallelism and available software parallelism continues to widen (in most environments), the ability to continue to scale performance and increase computational efficiency requires the identification of new sources of parallelism.</p> <p>Data Triggered Threads (DTT) is a new programming and execution model which allows the programmer (or even automatic code generator) to express code such that is triggered not by normal control flow, but rather modification to a triggering data location. &nbsp;In this way, computation becomes available for parallel execution at the earliest possible instant, or in many cases the computation is found to be redundant (because the trigger is not changed) and can be safely skipped.</p> <p>The particular outcomes of this research demonstrate the utility and effectiveness of DTT, with a variety of research results, demonstration vehicles, and mechanisms for producing DTT code.</p> <p>(1) Hardware-supported DTT. &nbsp;With a few minor changes to a processor's instruction set architecture, and relatively small (e.g., hundreds of lines) changes to existing traditional code, the system achieves speedups averaging nearly 50%, and as high as 6X, over traditional serial code (SPEC).</p> <p>(2) &nbsp;Software-only DTT. &nbsp;By emulating the hardware support in software, the user can run DTT on existing systems, and often still see strong results. &nbsp;This is because much of our gain is from skipping redundant computation, in which case the higher overheads of our software approach are also skipped. &nbsp;There are moderate gains with SPEC, but averaging 8X (and as high as 100X) speedup over a wider range of applications.</p> <p>(3) Automatic Generation of DTT. &nbsp;Traditional code, without being transformed by the programmer, can still take advantage of the benefits of DTT, enabling speedup and energy gains, again often as a result of avoiding redundant computation from the original program. &nbsp;In this work, the compiler automatically generate the DTT code from the original. &nbsp;It demonstrates that the automatic compiler framework comes close to replicating the programmer-generated gains. &nbsp;Of particular interest is that the system is nearly as effective at identifying candidates for DTT triggers and code without any help from profiling to identify redundant code regions.</p> <p>(4) DTTs with massive parallelism. &nbsp;To avoid race conditions, the original DTT infrastructure exploited very minimal parallelism. &nbsp;This work redefines the interface, allowing the programmer to express dependences between DTT threads. &nbsp;By expressing even very traditional forms of parallelism in this way, strong advantages can be observed. &nbsp;For example, because the runtime knows the trigger address of every thread, it can create a schedule that balances both load balancing and data locality.</p> <p>(5) &nbsp;Delta Triggered Threads. &nbsp;DTT skips unnecessary redundant computation when the triggering data item is unchanged. &nbsp;Delta triggered threads adds to the approximate computing literature, providing a mechanism that can skip recomputation of outputs when an input change is small. &nbsp;Contrary to some forms of approximate computing, this gives the programmer direct control of approximate triggers, which part of the dependent computation can be deemed unnecessary, and specific tolerances that are used to guard the computation.</p> <p>This work resulted in a number of papers, an award paper (top picks in computer architecture, recognizing the top research publications from the prior year), a number of invited research and distinguished lecture talks at universities and industry locations, and a software infrastructure made available to other users to write DTT code and experiment with further innovations. &nbsp;Among the students who have worked on the project and been mentored by the PI, several are continuing students at UCSD, but one has gone on to a faculty position at NCSU, one has joined an industry startup, and an undergraduate has been accepted to a PhD program.</p><br> <p>            Last Modified: 09/08/2016<br>      Modified by: Dean&nbsp;M&nbsp;Tullsen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As the gap between available hardware parallelism and available software parallelism continues to widen (in most environments), the ability to continue to scale performance and increase computational efficiency requires the identification of new sources of parallelism.  Data Triggered Threads (DTT) is a new programming and execution model which allows the programmer (or even automatic code generator) to express code such that is triggered not by normal control flow, but rather modification to a triggering data location.  In this way, computation becomes available for parallel execution at the earliest possible instant, or in many cases the computation is found to be redundant (because the trigger is not changed) and can be safely skipped.  The particular outcomes of this research demonstrate the utility and effectiveness of DTT, with a variety of research results, demonstration vehicles, and mechanisms for producing DTT code.  (1) Hardware-supported DTT.  With a few minor changes to a processor's instruction set architecture, and relatively small (e.g., hundreds of lines) changes to existing traditional code, the system achieves speedups averaging nearly 50%, and as high as 6X, over traditional serial code (SPEC).  (2)  Software-only DTT.  By emulating the hardware support in software, the user can run DTT on existing systems, and often still see strong results.  This is because much of our gain is from skipping redundant computation, in which case the higher overheads of our software approach are also skipped.  There are moderate gains with SPEC, but averaging 8X (and as high as 100X) speedup over a wider range of applications.  (3) Automatic Generation of DTT.  Traditional code, without being transformed by the programmer, can still take advantage of the benefits of DTT, enabling speedup and energy gains, again often as a result of avoiding redundant computation from the original program.  In this work, the compiler automatically generate the DTT code from the original.  It demonstrates that the automatic compiler framework comes close to replicating the programmer-generated gains.  Of particular interest is that the system is nearly as effective at identifying candidates for DTT triggers and code without any help from profiling to identify redundant code regions.  (4) DTTs with massive parallelism.  To avoid race conditions, the original DTT infrastructure exploited very minimal parallelism.  This work redefines the interface, allowing the programmer to express dependences between DTT threads.  By expressing even very traditional forms of parallelism in this way, strong advantages can be observed.  For example, because the runtime knows the trigger address of every thread, it can create a schedule that balances both load balancing and data locality.  (5)  Delta Triggered Threads.  DTT skips unnecessary redundant computation when the triggering data item is unchanged.  Delta triggered threads adds to the approximate computing literature, providing a mechanism that can skip recomputation of outputs when an input change is small.  Contrary to some forms of approximate computing, this gives the programmer direct control of approximate triggers, which part of the dependent computation can be deemed unnecessary, and specific tolerances that are used to guard the computation.  This work resulted in a number of papers, an award paper (top picks in computer architecture, recognizing the top research publications from the prior year), a number of invited research and distinguished lecture talks at universities and industry locations, and a software infrastructure made available to other users to write DTT code and experiment with further innovations.  Among the students who have worked on the project and been mentored by the PI, several are continuing students at UCSD, but one has gone on to a faculty position at NCSU, one has joined an industry startup, and an undergraduate has been accepted to a PhD program.       Last Modified: 09/08/2016       Submitted by: Dean M Tullsen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
