<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Emotional Sophistication - Studies of Facial Expressions in Decision Making</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>166977.00</AwardTotalIntnAmount>
<AwardAmount>166977</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Social and economic decisions cannot be fully explained by "rational" attempts to maximize monetary gain, even in very simple game-theoretic scenarios. Complex emotional processes such as anger, guilt or generosity act as hidden forces that lead to observable actions. Such "non-rational" motivations can drive our own decisions and they affect our beliefs about what motivates others' decisions as well. The goal of this project is to use automatic measurements of dynamic facial expressions, in combination with other measurements such as functional MRI (fMRI) and eye-tracking, to investigate the role of non-rational motivations in social decision making. The core of the approach is to use state-of-the-art computer vision techniques to extract facial actions from video in real-time while participants interact with a computer or with each other, in some cases viewing live video of each others' faces. The investigators will use powerful statistical machine learning techniques to make inferences about  the participants' internal emotional states during the interactions.  The goal is to use the inferences concerning emotional state (a) to predict participants' behavior; (b) to explain why a decision is made in terms of the hidden forces driving it; and (c) to build autonomous agents that can use this information to drive their interactions with humans. &lt;br/&gt;&lt;br/&gt;This multidisciplinary project contributes to several fields such as psychology, neuroscience, and economics. First, it develops new methodologies to study decision processes. Second, it uses these methods to test hypotheses about social decision-making and to bridge the gap between observable actions and the internal states that generated them. Third, the investigators intend to make available a  dataset and toolset that should be an extremely useful for other investigators analyzing facial expression in multiple contexts. Additionally, automatic and on-line decoding of internal motivational states lays the groundwork for "affectively-aware" interactive computers, or artificial systems that can make inferences about the emotions and intentions of their users. Through the development of these systems, this project will make a significant contribution to the growing field of human-machine interaction.&lt;br/&gt;&lt;br/&gt;[Supported by Perception, Action and Cognition, Decision, Risk and Management Sciences, and Robust Intelligence]</AbstractNarration>
<MinAmdLetterDate>08/27/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1232639</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Sanfey</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan Sanfey</PI_FULL_NAME>
<EmailAddress>asanfey@u.arizona.edu</EmailAddress>
<PI_PHON>5206211477</PI_PHON>
<NSF_ID>000487030</NSF_ID>
<StartDate>08/27/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Clayton</FirstName>
<LastName>Morrison</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Clayton T Morrison</PI_FULL_NAME>
<EmailAddress>claytonm@email.arizona.edu</EmailAddress>
<PI_PHON>5206216609</PI_PHON>
<NSF_ID>000558879</NSF_ID>
<StartDate>08/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arizona</Name>
<CityName>Tucson</CityName>
<ZipCode>857194824</ZipCode>
<PhoneNumber>5206266000</PhoneNumber>
<StreetAddress>888 N Euclid Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>806345617</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARIZONA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072459266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arizona]]></Name>
<CityName>Tucson</CityName>
<StateCode>AZ</StateCode>
<ZipCode>857210001</ZipCode>
<StreetAddress><![CDATA[888 N Euclid Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1321</Code>
<Text>Decision, Risk &amp; Mgmt Sci</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>6867</Code>
<Text>INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>7298</Code>
<Text>COLLABORATIVE RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~166977</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project is to use automatic measurements of facial expressions, in combination with other measurements such as fMRI and EMG, to investigate the role of non-rational motivations in social decision making. For example, if someone playing a game believes their partner is cheating them, they may make a different decision than they otherwise would have made, and this can be predicted by the player&rsquo;s facial expression. &nbsp;Our approach uses state-of-the-art computer vision techniques to extract facial actions from video in real-time and then uses this information to infer people&rsquo;s emotional state while they are making decisions. &nbsp;This has led to the ability to accurately predict human actions based on facial expression while making decision under different conditions.</p> <p>This targeted interdisciplinary project contributed to psychology, neuroscience, and economics in multiple ways. First, we developed new methodologies, including statistical methods reliably associate events in facial actions to people&rsquo;s actions in order to model how people make decisions under a variety of conditions. Second, we used our approach to test hypotheses about the role of emotion in social decision-making as well as the role that external cues such as the face can play in making decisions.</p> <p>The techniques developed during this project also contribute fundamentally to the design of &ldquo;affectively-aware&rdquo; interactive computer agents. &nbsp;In order for computer agents to interact with humans closer to the way humans naturally interact with each other, computer agents must be able to interpret human facial expressions in the context of different decision-making situations. &nbsp;Examples of where affectively-aware computer agents are of great social benefit include systems for medical diagnosis, psychological and drug assessment (e.g., for pain, depression, anxiety), home assistants that provide health interventions, electronic tutoring systems that react to estimates of students&rsquo; understanding or engagement, and robots for child- and elder- care.</p><br> <p>            Last Modified: 04/23/2017<br>      Modified by: Clayton&nbsp;T&nbsp;Morrison</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project is to use automatic measurements of facial expressions, in combination with other measurements such as fMRI and EMG, to investigate the role of non-rational motivations in social decision making. For example, if someone playing a game believes their partner is cheating them, they may make a different decision than they otherwise would have made, and this can be predicted by the player?s facial expression.  Our approach uses state-of-the-art computer vision techniques to extract facial actions from video in real-time and then uses this information to infer people?s emotional state while they are making decisions.  This has led to the ability to accurately predict human actions based on facial expression while making decision under different conditions.  This targeted interdisciplinary project contributed to psychology, neuroscience, and economics in multiple ways. First, we developed new methodologies, including statistical methods reliably associate events in facial actions to people?s actions in order to model how people make decisions under a variety of conditions. Second, we used our approach to test hypotheses about the role of emotion in social decision-making as well as the role that external cues such as the face can play in making decisions.  The techniques developed during this project also contribute fundamentally to the design of "affectively-aware" interactive computer agents.  In order for computer agents to interact with humans closer to the way humans naturally interact with each other, computer agents must be able to interpret human facial expressions in the context of different decision-making situations.  Examples of where affectively-aware computer agents are of great social benefit include systems for medical diagnosis, psychological and drug assessment (e.g., for pain, depression, anxiety), home assistants that provide health interventions, electronic tutoring systems that react to estimates of students? understanding or engagement, and robots for child- and elder- care.       Last Modified: 04/23/2017       Submitted by: Clayton T Morrison]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
