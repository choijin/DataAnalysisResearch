<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Data and Software Preservation for Open Science (DASPOS)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>1800627.00</AwardTotalIntnAmount>
<AwardAmount>2065046</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03010000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>PHY</Abbreviation>
<LongName>Division Of Physics</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scientific data is being collected at a higher rate than ever.  A striking example is in elementary particle physics (EPP) experiments, where collaborations of thousands of scientists collect huge amounts of data at the Large Hadron Collider (LHC), but other disciplines have and are continuing to observe similar growth. &lt;br/&gt;&lt;br/&gt;The complexity and time frame of these experiments is such that the full scientific potential can only be realized when the data remains accessible and analyzable through extended periods. &lt;br/&gt;&lt;br/&gt;The possibility of successfully meeting this goal for the necessary long term data preservation requires a novel approach that will make the data and necessary software management more solid and survivable. The long term data preservation will become an even more critical issue as present experimental efforts evolve and the Big Data paradigm develops. The initial efforts of the US community to analyze the large volume of LHC data is being satisfied by the Open Science Grid project, designed to facilitate such large and distributed experiments. DASPOS provides an opportunity to continue to study today's analysis over the long term. &lt;br/&gt;&lt;br/&gt;The DASPOS project incorporates the present state-of-the-art knowledge in working with data in EPP. This project aims to provide a generic technological framework where the basic difficulties are identified and solved with the aim of advancing these goals simultaneously for several disciplines, which should facilitate the emergence of commonalities and standards.&lt;br/&gt;&lt;br/&gt;The milestones and the work plan are well structured and adapted to present knowledge. Intense communications via workshops is planned and is a natural path for the goal of inclusion of the various disciplines. The intention to document these workshops is a valuable component of this project, as are concrete goals such as prototypes and software challenges.&lt;br/&gt;&lt;br/&gt;The DASPOS project is not only sound but also timely. The recent dynamics in data preservation and large data management is now reaching several countries and funding agencies. In fact, several projects at national levels are now installed, including for instance, the PREDON project, financed by CNRS-France in 2012 to prepare a multidisciplinary novel approach to big data management. &lt;br/&gt;&lt;br/&gt;Other similar initiatives are under study in Germany and Italy. It is no question that synergies will emerge at an international scale, and is also clear that DASPOS will play a pioneering and leading role in this context. &lt;br/&gt;&lt;br/&gt;In conclusion, the impact and merit of the DASPOS proposal is innovative and potentially transformational. This can be an historical opportunity to make a significant advance in the scientific data management in the context of the "Big Data" challenge.</AbstractNarration>
<MinAmdLetterDate>09/14/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1247316</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Gardner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>Robert Gardner</PI_FULL_NAME>
<EmailAddress>rwg@hep.uchicago.edu</EmailAddress>
<PI_PHON>7738349885</PI_PHON>
<NSF_ID>000458001</NSF_ID>
<StartDate>09/14/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Hildreth</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Hildreth</PI_FULL_NAME>
<EmailAddress>hildreth.2@nd.edu</EmailAddress>
<PI_PHON>5746316458</PI_PHON>
<NSF_ID>000090746</NSF_ID>
<StartDate>09/14/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Douglas</FirstName>
<LastName>Thain</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Douglas Thain</PI_FULL_NAME>
<EmailAddress>dthain@nd.edu</EmailAddress>
<PI_PHON>5746316845</PI_PHON>
<NSF_ID>000341714</NSF_ID>
<StartDate>09/14/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Neubauer</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark S Neubauer</PI_FULL_NAME>
<EmailAddress>msn@illinois.edu</EmailAddress>
<PI_PHON>2172443913</PI_PHON>
<NSF_ID>000509237</NSF_ID>
<StartDate>09/14/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jaroslaw</FirstName>
<LastName>Nabrzyski</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jaroslaw Nabrzyski</PI_FULL_NAME>
<EmailAddress>naber@nd.edu</EmailAddress>
<PI_PHON>5746312400</PI_PHON>
<NSF_ID>000510570</NSF_ID>
<StartDate>09/14/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Notre Dame</Name>
<CityName>NOTRE DAME</CityName>
<ZipCode>465565708</ZipCode>
<PhoneNumber>5746317432</PhoneNumber>
<StreetAddress>940 Grace Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>824910376</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NOTRE DAME DU LAC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>048994727</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Notre Dame]]></Name>
<CityName>Notre Dame</CityName>
<StateCode>IN</StateCode>
<ZipCode>465565612</ZipCode>
<StreetAddress><![CDATA[940 Grace Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1221</Code>
<Text>HEP-High Energy Physics</Text>
</ProgramElement>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>7244</Code>
<Text>COMPUTATIONAL PHYSICS</Text>
</ProgramElement>
<ProgramElement>
<Code>7553</Code>
<Text>PHYSICS AT THE INFO FRONTIER</Text>
</ProgramElement>
<ProgramElement>
<Code>7726</Code>
<Text>Data Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7569</Code>
<Text>CYBERINFRASTRUCTURE/SCIENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8048</Code>
<Text>Data Infrstr Bldg Blocks-DIBBs</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~1800627</FUND_OBLG>
<FUND_OBLG>2016~264419</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The DASPOS project (<span style="text-decoration: underline;">D</span>ata and <span style="text-decoration: underline;">S</span>oftware <span style="text-decoration: underline;">P</span>reservation for <span style="text-decoration: underline;">O</span>pen <span style="text-decoration: underline;">S</span>cience) was a multi-disciplinary effort between six universities and the Fermi and Brookhaven national laboratories whose aim was to understand the problems associated with knowledge preservation in data-heavy sciences like experimental particle physics, astrophysics, genomics, etc.&nbsp; The team included physicists, digital librarians, computer scientists, and other experts from different fields of research.&nbsp;</p> <p>Stated simply, the intellectual goals of the project were the following: (1) determine <em>what</em> needs to be saved in order to preserve the different aspects of a complicated data analysis with many processing steps for reproducibility and re-use, (2) determine <em>how</em> to save these elements in a manner that they could be archived, searchable, and re-useable, and (3) demonstrate a prototype preservation system that could do this.&nbsp; The research followed several different paths. One aspect focused on how to capture all of the necessary information to re-run a given process, including the operating system, the input data, all of the external database connections, etc.&nbsp; Several solutions to this were explored, including ones based on tracing the system calls of the process to find all of the necessary dependencies, and several based on linux containers.&nbsp; The relative performances of the different techniques were assessed, with the linux container approach (embodied, for example, by Docker containers) given the slight edge due to ease of use and available infrastructure.&nbsp; &nbsp;A second aspect of the research was to understand how to describe what was being done in a computational step as part of an analysis.&nbsp; This would be necessary for the material to be searched for and retrieved from an archive, or so another person could understand what was done and re-use some elements.&nbsp; The studies performed on this aspect of the project resulted in several new metadata vocabularies, including one that describes a "computational step" in a complex analysis, and one that describes a "detector final state" in High Energy Physics (HEP), the first such description to be recorded.&nbsp; In collaboration with the IT and SIS groups at CERN, we have been involved in building the CERN Analysis Preservation Portal (CAP) and the REANA analysis platform, both of which incorporate DASPOS research and represent the achievement of the original goals of the proposal.&nbsp; The CAP will allow individual researchers to store a wealth of pertinent information about their analysis, some of it collected automatically from their LHC experiment.&nbsp; Executables, scripts, and data can also be stored.&nbsp; In particular, individual computational steps can be described and captured, currently using container technology.&nbsp; The metadata description used to archive the information is based on the DASPOS work.&nbsp; The REANA analysis back-end is able to re-assemble complete analysis workflows based on the archived information and re-instantiate them using workflow engines implemented by the DASPOS and CERN teams.&nbsp; The infrastructure required is quite generic and includes many commondity elements that can orchestrate container-based applications on distributed high-throughput computing systems. We have demonstrated the functionality of this system using sample analyses from the LHCb, ATLAS, and CMS experiments at the LHC.&nbsp; The analyses preserved in the CAP portal can be re-run inside of the REANA infrastructure and produce identical results to the original processing.</p> <p>The broader impacts of this project are many.&nbsp; DASPOS was conceived from the beginning to be an interdisciplinary project with an emphasis on HEP.&nbsp; As such, the workshops that were conducted involved a variety of disciplines and computational problems, with an explicit focus to discover commonaility in computational approaches that could be preserved with the same basic infrastructure.&nbsp; For example, although it was derived in the context of HEP computation, the "computational step" metadata description is completely generic and based on the well-known PROV ontology, making it widely applicable.&nbsp; The core technologies of the REANA platform are all standard commodity elements and are, as such, widely available.&nbsp; Anyone with a fairly powerful laptop can set up and run the REANA infrastructure. Nothing in its structure is explicitly tied to HEP, including the workflow and computation descritpion language.&nbsp; It can be, therefore, easily adapted to other disciplines. We have take the additional step of incorporating the commands and structure of the Common Workflow Language, widely used, for example, in biological computation, into the REANA fabric. This should make adoption by other discplines that much easier.&nbsp; &nbsp;Another aspect of the DASPOS project that should not be overlooked is the training of graduate students.&nbsp; Ten student participated in various aspects of this project, coming from computer science and physics.&nbsp; The interdisciplinary nature of the work has provided them with very broad skills and an understanding of the scientific tools that the other discplines have at their disposal.&nbsp; All the students who have graduated remain in the STEM workforce and are able to use their highly-valued skill sets in a variety of applications.</p> <p>Visit www.daspos.crc.nd.edu for more information.</p><br> <p>            Last Modified: 12/30/2017<br>      Modified by: Michael&nbsp;Hildreth</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1247316/1247316_10214905_1514497948700_CAP-REANA--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1247316/1247316_10214905_1514497948700_CAP-REANA--rgov-800width.jpg" title="The REANA Project"><img src="/por/images/Reports/POR/2017/1247316/1247316_10214905_1514497948700_CAP-REANA--rgov-66x44.jpg" alt="The REANA Project"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The structure of the REANA project, showing the various components of the execution engine, and several commodity elements.</div> <div class="imageCredit">Tibor Simka</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Michael&nbsp;Hildreth</div> <div class="imageTitle">The REANA Project</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The DASPOS project (Data and Software Preservation for Open Science) was a multi-disciplinary effort between six universities and the Fermi and Brookhaven national laboratories whose aim was to understand the problems associated with knowledge preservation in data-heavy sciences like experimental particle physics, astrophysics, genomics, etc.  The team included physicists, digital librarians, computer scientists, and other experts from different fields of research.   Stated simply, the intellectual goals of the project were the following: (1) determine what needs to be saved in order to preserve the different aspects of a complicated data analysis with many processing steps for reproducibility and re-use, (2) determine how to save these elements in a manner that they could be archived, searchable, and re-useable, and (3) demonstrate a prototype preservation system that could do this.  The research followed several different paths. One aspect focused on how to capture all of the necessary information to re-run a given process, including the operating system, the input data, all of the external database connections, etc.  Several solutions to this were explored, including ones based on tracing the system calls of the process to find all of the necessary dependencies, and several based on linux containers.  The relative performances of the different techniques were assessed, with the linux container approach (embodied, for example, by Docker containers) given the slight edge due to ease of use and available infrastructure.   A second aspect of the research was to understand how to describe what was being done in a computational step as part of an analysis.  This would be necessary for the material to be searched for and retrieved from an archive, or so another person could understand what was done and re-use some elements.  The studies performed on this aspect of the project resulted in several new metadata vocabularies, including one that describes a "computational step" in a complex analysis, and one that describes a "detector final state" in High Energy Physics (HEP), the first such description to be recorded.  In collaboration with the IT and SIS groups at CERN, we have been involved in building the CERN Analysis Preservation Portal (CAP) and the REANA analysis platform, both of which incorporate DASPOS research and represent the achievement of the original goals of the proposal.  The CAP will allow individual researchers to store a wealth of pertinent information about their analysis, some of it collected automatically from their LHC experiment.  Executables, scripts, and data can also be stored.  In particular, individual computational steps can be described and captured, currently using container technology.  The metadata description used to archive the information is based on the DASPOS work.  The REANA analysis back-end is able to re-assemble complete analysis workflows based on the archived information and re-instantiate them using workflow engines implemented by the DASPOS and CERN teams.  The infrastructure required is quite generic and includes many commondity elements that can orchestrate container-based applications on distributed high-throughput computing systems. We have demonstrated the functionality of this system using sample analyses from the LHCb, ATLAS, and CMS experiments at the LHC.  The analyses preserved in the CAP portal can be re-run inside of the REANA infrastructure and produce identical results to the original processing.  The broader impacts of this project are many.  DASPOS was conceived from the beginning to be an interdisciplinary project with an emphasis on HEP.  As such, the workshops that were conducted involved a variety of disciplines and computational problems, with an explicit focus to discover commonaility in computational approaches that could be preserved with the same basic infrastructure.  For example, although it was derived in the context of HEP computation, the "computational step" metadata description is completely generic and based on the well-known PROV ontology, making it widely applicable.  The core technologies of the REANA platform are all standard commodity elements and are, as such, widely available.  Anyone with a fairly powerful laptop can set up and run the REANA infrastructure. Nothing in its structure is explicitly tied to HEP, including the workflow and computation descritpion language.  It can be, therefore, easily adapted to other disciplines. We have take the additional step of incorporating the commands and structure of the Common Workflow Language, widely used, for example, in biological computation, into the REANA fabric. This should make adoption by other discplines that much easier.   Another aspect of the DASPOS project that should not be overlooked is the training of graduate students.  Ten student participated in various aspects of this project, coming from computer science and physics.  The interdisciplinary nature of the work has provided them with very broad skills and an understanding of the scientific tools that the other discplines have at their disposal.  All the students who have graduated remain in the STEM workforce and are able to use their highly-valued skill sets in a variety of applications.  Visit www.daspos.crc.nd.edu for more information.       Last Modified: 12/30/2017       Submitted by: Michael Hildreth]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
