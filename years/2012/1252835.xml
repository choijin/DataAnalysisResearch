<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Learning Scalable Models for Grounded Semantic Parsing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>D.  Langendoen</SignBlockName>
<PO_EMAI>dlangend@nsf.gov</PO_EMAI>
<PO_PHON>7032925088</PO_PHON>
</ProgramOfficer>
<AbstractNarration>One core challenge in natural language research is to do robust, wide coverage semantic analysis.  Recently, there has been progress towards solving this problem by developing algorithms for learning semantic parsers that map sentences to rich, logical representations of their meaning.  State-of-the-art approaches have reached the level where they can, with sufficient training data, be used to learn highly accurate parsers for many different natural languages on a number of benchmark problems. However, the general applicability of this work has been limited by the focus on somewhat idealized conditions, where the application domain is of limited size, sentences are analyzed in isolation, and there is an exclusive focus on database query applications.&lt;br/&gt;&lt;br/&gt;This CAREER project aims to build a framework for grounded semantic parsing that solves these challenges by reasoning about a sentence's possible meanings given its linguistic and situated context. This type of reasoning is necessary for extending existing learning approaches to fundamentally new applications, such as conversational understanding.  However, it is also be crucial for the next generation of semantic parsers that learn from easily gathered data and scale to domains that are several orders of magnitude more complex than previously considered.&lt;br/&gt;&lt;br/&gt;The project will extend the PI's educational and outreach efforts, including the creation of freely shared online content for introductory and advanced semantics topics. It will also enable new initiatives by the PI to increase diversity in computer science study and research, by supporting efforts to motivate students through early exposure to exciting language understanding problems.</AbstractNarration>
<MinAmdLetterDate>02/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1252835</AwardID>
<Investigator>
<FirstName>Luke</FirstName>
<LastName>Zettlemoyer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Luke Zettlemoyer</PI_FULL_NAME>
<EmailAddress>lsz@cs.washington.edu</EmailAddress>
<PI_PHON>2066851227</PI_PHON>
<NSF_ID>000581613</NSF_ID>
<StartDate>02/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way, Room CSE101]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~92141</FUND_OBLG>
<FUND_OBLG>2014~95774</FUND_OBLG>
<FUND_OBLG>2015~99687</FUND_OBLG>
<FUND_OBLG>2016~103914</FUND_OBLG>
<FUND_OBLG>2017~108484</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>One core challenge in natural language research is to do robust, wide coverage semantic analysis. Recently, there has been progress towards solving this problem by developing algorithms for learning semantic parsers that map sentences to rich, logical representations of their meaning. State-of-the-art approaches have reached the level where they can, with sufficient training data, be used to learn highly accurate parsers for many different natural languages on a number of benchmark problems. However, the general applicability of this work has been limited by the focus on somewhat idealized conditions, where the application domain is of limited size, sentences are analyzed in isolation, and there is an exclusive focus on database query applications.</span><br /><br /><span>This CAREER project developed a new framework for grounded semantic parsing that solves these challenges by reasoning about a sentence's possible meanings given its linguistic and situated context. This type of reasoning is necessary for extending existing learning approaches to fundamentally new applications, such as conversational understanding. However, it is also be crucial for the next generation of semantic parsers that learn from easily gathered data and scale to domains that are several orders of magnitude more complex than previously considered.</span></p> <p>We developed a number of new methods for reasoning about meaning in context, including both&nbsp;linguistic and programmatic. This work also took place during the deep learning revolution in NLP, allow us to develop and release some of the very first neural semantic parsers, which enabled much better performance with (perhaps surprisingly) even less training data. This change and the generality of the deep learning methods also allowed us to consider new forms of semantic supervision, and develop many of the first methods that could map natural language (e.g. English questions) directly to executable source code (e.g. SQL). Again, we made strong advances on improving the accuracy of these systems while also developing new methods to train with fewer labeled examples, using open source code repositories and other sources of supervision as much as possible. Finally, in the later years, we generalized these techniques to problems and question answer answering and reaching comprehension, where the meaning representations are more implicit in the structure of the neural models.&nbsp;</p> <p>This project also has a significant educational integration. We developed in course and tutorial material to provide an on ramp to the research area. These materials were originally designed for CCG parsing (a rich, linguistically informed grammar formalism) and later redone for the more modern neural semantic parsing approaches. These materials were incorporated into the courses at the University of Washington and shared more broadly through conference tutorials and open source materials.&nbsp;</p><br> <p>            Last Modified: 06/11/2021<br>      Modified by: Luke&nbsp;Zettlemoyer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ One core challenge in natural language research is to do robust, wide coverage semantic analysis. Recently, there has been progress towards solving this problem by developing algorithms for learning semantic parsers that map sentences to rich, logical representations of their meaning. State-of-the-art approaches have reached the level where they can, with sufficient training data, be used to learn highly accurate parsers for many different natural languages on a number of benchmark problems. However, the general applicability of this work has been limited by the focus on somewhat idealized conditions, where the application domain is of limited size, sentences are analyzed in isolation, and there is an exclusive focus on database query applications.  This CAREER project developed a new framework for grounded semantic parsing that solves these challenges by reasoning about a sentence's possible meanings given its linguistic and situated context. This type of reasoning is necessary for extending existing learning approaches to fundamentally new applications, such as conversational understanding. However, it is also be crucial for the next generation of semantic parsers that learn from easily gathered data and scale to domains that are several orders of magnitude more complex than previously considered.  We developed a number of new methods for reasoning about meaning in context, including both linguistic and programmatic. This work also took place during the deep learning revolution in NLP, allow us to develop and release some of the very first neural semantic parsers, which enabled much better performance with (perhaps surprisingly) even less training data. This change and the generality of the deep learning methods also allowed us to consider new forms of semantic supervision, and develop many of the first methods that could map natural language (e.g. English questions) directly to executable source code (e.g. SQL). Again, we made strong advances on improving the accuracy of these systems while also developing new methods to train with fewer labeled examples, using open source code repositories and other sources of supervision as much as possible. Finally, in the later years, we generalized these techniques to problems and question answer answering and reaching comprehension, where the meaning representations are more implicit in the structure of the neural models.   This project also has a significant educational integration. We developed in course and tutorial material to provide an on ramp to the research area. These materials were originally designed for CCG parsing (a rich, linguistically informed grammar formalism) and later redone for the more modern neural semantic parsing approaches. These materials were incorporated into the courses at the University of Washington and shared more broadly through conference tutorials and open source materials.        Last Modified: 06/11/2021       Submitted by: Luke Zettlemoyer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
