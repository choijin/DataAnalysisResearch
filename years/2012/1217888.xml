<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Goal-Driven Autonomy</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>263870.00</AwardTotalIntnAmount>
<AwardAmount>263870</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Goal-driven autonomy (GDA) is a reflective model of reasoning about goals to control the focus of an agent's activities by dynamically resolving unexpected discrepancies in the world state, which frequently arise when solving tasks in complex environments. This project is motivated by two observations about GDA agents. First, to perform well, comprehensive GDA agents require substantial domain knowledge; however, few techniques have been investigated for learning this knowledge. Second, while existing GDA agents have demonstrated good performance in a variety of tasks, understanding and generalizing their successes has been hindered by a gap between the kinds of domains that these agents aim to model and the representations that they use; for instance, the bulk of current research on GDA agents assumes STRIPS representations of the agent's goals and actions. &lt;br/&gt;&lt;br/&gt;This project aims to study GDA agents that are capable of learning expectations, explanations, and goals. This project aims to develop methods that enable creation of GDA agents that can autonomously act and learn to: (1) identify situations where discrepancies take place between what they expect and what actually has happened; (2) explain the discrepancy; (3) decide which goals to try to achieve as a result of these explanations; and (4) act to accomplish these goals. In this work, the objective of each agent is to maximize its expected return as defined in reinforcement learning. This approach fits naturally with the 4-step GDA cycle, facilitates studying properties about GDA using the well-defined reinforcement learning framework, and enables the adoption of representation formalisms such as stochastic policies (i.e., probability distributions of state-action pairs), which are naturally suited to represent GDA agent's actions in the domains that GDA agents aim to interact with. This project aims to develop representational methods that combine FOL (First Order Logic) literals and actions with probabilities as the basis to represent GDA elements.&lt;br/&gt;&lt;br/&gt;The potential Broader Impact of this research is significant due to the potentially large and widespread applications of goal-driven autonomy. With the pervasive presence of autonomous computing devices and software, there is an increasingly pressing need for technology that enables systems to recognize discrepancies in what they expect from their 'worlds', diagnose them, and then adjust themselves. This is a ubiquitous problem in all areas of computer science. For example, in the general area of ambient intelligence, automated systems, such as an air quality control system, must monitor and control a variety of devices; it is very difficult, if not impossible, for a programmer to foresee all potential situations that such a system will encounter. Another example is cyber security where given the openness that characterizes current networks and the continuous integration of new technologies and services into them, it is not feasible to implement counter measures for all potential threats in advance; instead, an agent-based system must continuously monitor the overall network, learn and reason about expectations, and act autonomously when discrepancies are encountered. &lt;br/&gt;&lt;br/&gt;This project includes a vigorous educational component. Specifically, it plans to (1) regularly involve undergraduate students in developing and testing carefully scoped components of the project; (2) create a course on adaptive and self-aware GDA agents that transcends traditional boundaries in courses on agents, reinforcement learning, and planning; and (3) create and disseminate testbeds for GDA agents that include not only the project's GDA agents but also simulations and agent-simulation interfaces. Creating and disseminating testbeds will help remediate the lack of systems and agent-simulation interfaces that has been a repeated stumbling block for teaching about GDA agents.</AbstractNarration>
<MinAmdLetterDate>07/31/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217888</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Heflin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey Heflin</PI_FULL_NAME>
<EmailAddress>heflin@cse.lehigh.edu</EmailAddress>
<PI_PHON>6107586533</PI_PHON>
<NSF_ID>000241473</NSF_ID>
<StartDate>06/20/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hector</FirstName>
<LastName>Munoz-Avila</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hector Munoz-Avila</PI_FULL_NAME>
<EmailAddress>hem4@lehigh.edu</EmailAddress>
<PI_PHON>6107583797</PI_PHON>
<NSF_ID>000236759</NSF_ID>
<StartDate>07/31/2012</StartDate>
<EndDate>06/20/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Lehigh University</Name>
<CityName>Bethlehem</CityName>
<ZipCode>180153005</ZipCode>
<PhoneNumber>6107583021</PhoneNumber>
<StreetAddress>Alumni Building 27</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>808264444</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LEHIGH UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068570936</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Lehigh University]]></Name>
<CityName>Bethlehem</CityName>
<StateCode>PA</StateCode>
<ZipCode>180153084</ZipCode>
<StreetAddress><![CDATA[19 Memorial Drive West]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~263870</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The topic of autonomy has gained attention over the past years because of the tremendous progress in robotic platforms such as self-driving vehicles. This project aimed at studying high-level degrees of autonomy.&nbsp;This project studied underlying principles of Goal-driven autonomy (GDA) agents. GDA agents can (1) identify situations where discrepancies take place between what they expect and what actually happens, (2) explain the discrepancy, (3) decide which goals to achieve as a result of these explanations, and (4) act to accomplish these goals. GDA agents are able to act in complex environments, which exhibit nondeterministic action outcomes and are dynamic with changes occurring independently of the agent&rsquo;s own actions. The project resulted in a new way for agents to self-monitor if the agent is working within expected parameters. We call these informed expectations, which take into account the trajectory of the agent as opposed to the most recent action taken. We defined informed expectations for different variants of agents including those using ontologies and those using hierarchical knowledge. We developed algorithms capable of learning expectations, eliciting explanations for expectation failures and for goal formulation.&nbsp;Our work on goal-driven autonomy has formalized a robust model for goal reasoning, one where planning and execution are intertwined and can lead to a new generation of autonomous systems that are robust not only on physical tasks such as navigation but cognitive tasks such as goal reasoning.&nbsp;This project provided the means to educate PhD students, including a female student, MS students, some of whom went on the pursue their PhD, and undergraduate students, some of whom went on the graduate school.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/27/2017<br>      Modified by: Jeffrey&nbsp;Heflin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The topic of autonomy has gained attention over the past years because of the tremendous progress in robotic platforms such as self-driving vehicles. This project aimed at studying high-level degrees of autonomy. This project studied underlying principles of Goal-driven autonomy (GDA) agents. GDA agents can (1) identify situations where discrepancies take place between what they expect and what actually happens, (2) explain the discrepancy, (3) decide which goals to achieve as a result of these explanations, and (4) act to accomplish these goals. GDA agents are able to act in complex environments, which exhibit nondeterministic action outcomes and are dynamic with changes occurring independently of the agent?s own actions. The project resulted in a new way for agents to self-monitor if the agent is working within expected parameters. We call these informed expectations, which take into account the trajectory of the agent as opposed to the most recent action taken. We defined informed expectations for different variants of agents including those using ontologies and those using hierarchical knowledge. We developed algorithms capable of learning expectations, eliciting explanations for expectation failures and for goal formulation. Our work on goal-driven autonomy has formalized a robust model for goal reasoning, one where planning and execution are intertwined and can lead to a new generation of autonomous systems that are robust not only on physical tasks such as navigation but cognitive tasks such as goal reasoning. This project provided the means to educate PhD students, including a female student, MS students, some of whom went on the pursue their PhD, and undergraduate students, some of whom went on the graduate school.             Last Modified: 09/27/2017       Submitted by: Jeffrey Heflin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
