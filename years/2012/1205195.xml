<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CI-ADDO-EN: Collaborative Research: 3D Dynamic Multimodal Spontaneous Emotion Corpus for Automated Facial Behavior and Emotion Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>110001.00</AwardTotalIntnAmount>
<AwardAmount>110001</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Emotion is the complex psycho-physiological experience of an individual's state of mind.  It affects every aspect of rational thinking, learning, decision making, and psychomotor ability.  Emotion modeling and recognition is playing an increasingly important role in many research areas, including human computer interaction, robotics, artificial intelligence, and advanced technologies for education and learning.  Current emotion-related research, however, is impeded by a lack of a large spontaneous emotion data corpus.  With few exceptions, emotion databases are limited in terms of size, sensor modalities, labeling, and elicitation methods.  Most rely on posed emotions, which may bear little resemblance to what occurs in the contexts wherein the emotions are really triggered.  In this project the PIs will address these limitations by developing a multimodal and multidimensional corpus of dynamic spontaneous emotion and facial expression data, with labels and feature derivatives, from approximately 200 subjects of different ethnicities and ages, using sensors of different modalities.  To these ends, they will acquire a 6-camera wide-range 3D dynamic imaging system to capture ultra high-resolution facial geometric data and video texture data, which will allow them to examine the fine structure change as well as the precise time course for spontaneous expressions.  Video data will be accompanied by other sensor modalities, including thermal, audio and physiological sensors.  An IR thermal camera will allow real time recording of facial temperature, while an audio sensor will record the voices of both subject and experimenter. The physiological sensor will measure skin conductivity and related physiological signals.  Tools and methods to facilitate and simplify use of the dataset will be provided.  The entire dataset, including metadata and associated software, will be stored in a public depository and made available for research in computer vision, affective computing, human computer interaction, and related fields.&lt;br/&gt;&lt;br/&gt;Intellectual Merit &lt;br/&gt;This research will involve construction of a corpus of spontaneous multi-dimensional and multimodal emotion and facial expression data, which is significantly larger than any that currently exist.  To elicit natural and spontaneous emotions from subjects, the PIs will employ five approaches using physical experience, film clips, cold pressor, relived memories tasks, and interview formats.  The database will employ sensors of different modalities including high resolution 2D/3D video cameras, infrared thermal cameras, audio sensors, and physiological sensors.  The video data will be labeled according to a number of categories, including AU labeling and emotion labeling from self-report and perceptual judgments of na√Øve observers.  Comprehensive emotion labeling will include dimensional approaches (e.g., valence, arousal), discrete emotions (e.g., joy, anger, smile controls), anatomic methods (e.g., FACS), and paralinguistic signaling (e.g., back-channeling).  Additional features will be derived from the raw data, including 2D/3D facial feature points, head pose, and audio parameters.&lt;br/&gt;&lt;br/&gt;Broader Impact &lt;br/&gt;Project outcomes will immediately benefit researchers in computer vision and emotion modeling and recognition, because the database will allow them to train and validate their facial expression and emotion recognition algorithms.  The new corpus will facilitate the study of multimodal fusion from audio, video, geometric, thermal, and physical responses.  It will contribute to the development of a comprehensive understanding of mechanisms involving human behavior, and will allow enhancements to human computer interaction (e.g., through emotion-sensitive and socially intelligent interfaces), robotics, artificial intelligence, and cognitive science.  The work will likely also significantly impact research in diverse other fields such as psychology, biometrics, medicine/life science, law-enforcement, education, entrainment, and social science.</AbstractNarration>
<MinAmdLetterDate>06/28/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/28/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1205195</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Cohn</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey F Cohn</PI_FULL_NAME>
<EmailAddress>jeffcohn@pitt.edu</EmailAddress>
<PI_PHON>4126248825</PI_PHON>
<NSF_ID>000211710</NSF_ID>
<StartDate>06/28/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pittsburgh</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152133203</ZipCode>
<PhoneNumber>4126247400</PhoneNumber>
<StreetAddress>300 Murdoch Building</StreetAddress>
<StreetAddress2><![CDATA[3420 Forbes Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004514360</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF PITTSBURGH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004514360</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pittsburgh]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152132303</ZipCode>
<StreetAddress><![CDATA[123 University Place]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~110001</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Human communication is multimodal.&nbsp; Emotion and communicative intent are expressed in the &ldquo;packaging&rdquo; of facial expression, direction of gaze, head pose, their dynamics over time, and the physiology of their embodiment. Yet, computational approaches to behavior for most part are limited to a single modality, such as face or voice. &nbsp;The lack of multimodal approaches to behavior understanding arises in large part from lack of adequate training data necessary for algorithm development and validation. To train computer algorithms that can detect, represent, and understand multimodal behavior, large, well-annotated databases, or corpuses, of naturalistic multimodal behavior in relevant contexts are essential. To meet the needs for such data, we observed over 140 participants in varied, well-validated emotion inductions.&nbsp;</p> <p>Participants were ages 18 to 66 years, 59% female, and racially and ethnically diverse (65% non-Euro-American).&nbsp; Emotion inductions, seamlessly led by a professional actor and director, &nbsp;included interview, film clip viewing, cold pressor test, startle, threat, and others selected to elicit a wide range of emotion responses. During the tasks, participant behavior was comprehensively recorded using both 3D and 2D video, audio microphone, infrared camera, and physiology sensors (heart rate, respiration, skin temperature, and blood pressure).&nbsp; Self-reported emotion was collected throughout the session.&nbsp;</p> <p>Facial expression was annotated offline using the Facial Action Coding System (FACS). FACS defines more than 30 anatomically based action units (AUs) that individually and in combination can describe nearly all-possible facial expressions.&nbsp; AUs were coded with high reliability in each video frame. Thirty-four AUs were coded for occurrence; 8 AUs were coded for intensity on a 6-point ordinal scale.</p> <p>In addition to 3D and 2D video, audio, and physiology, the corpus includes derived features from the 3D, 2D, and infrared sensors, and baseline results for detection of AU occurrence and intensity. The entire corpus will be made available to the research community for basic research in computational behavioral science, computer vision, psychology, and related fields, and for applied research in affective computing, marketing, mental health, education, and related fields.&nbsp; Derivations of the database include synthesized non-frontal facial expression with which to train, test, and compare alternative approaches to face alignment and AU detection across a wide range of camera orientations. Subsets of the data have already made possible development of algorithms for 3D tracking and alignment of facial expression from 2D video, international competition in automatic facial expression recogntion, and validation of algorithms for detection of heart rate from 2D video recordings with moderate to large head rotation. These corpuses address the needs of basic and applied research in multiple domains, contribute to research infrastructure, and already are having significant impact in multiple domains.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/15/2016<br>      Modified by: Jeffrey&nbsp;F&nbsp;Cohn</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1205195/1205195_10185449_1479228461333_OutcomesReportfigure--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1205195/1205195_10185449_1479228461333_OutcomesReportfigure--rgov-800width.jpg" title="Example of multimodal data in the corpus."><img src="/por/images/Reports/POR/2016/1205195/1205195_10185449_1479228461333_OutcomesReportfigure--rgov-66x44.jpg" alt="Example of multimodal data in the corpus."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Sample data sequences from a participant including original 2D texture (first row), shaded 3D model (second row), textured 3D model (third row), thermal image (fourth row), physiology (fifth row: respiration rate, blood pressure, EDA, heart rate) and co-occurring action units (last row).</div> <div class="imageCredit">Zhang, X., et al., Multimodal spontaneous human emotion corpus for human behavior analysis, in IEEE International Conference on Computer Vision and Pattern Recognition. 2016: Las Vegas, NV.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jeffrey&nbsp;F&nbsp;Cohn</div> <div class="imageTitle">Example of multimodal data in the corpus.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Human communication is multimodal.  Emotion and communicative intent are expressed in the "packaging" of facial expression, direction of gaze, head pose, their dynamics over time, and the physiology of their embodiment. Yet, computational approaches to behavior for most part are limited to a single modality, such as face or voice.  The lack of multimodal approaches to behavior understanding arises in large part from lack of adequate training data necessary for algorithm development and validation. To train computer algorithms that can detect, represent, and understand multimodal behavior, large, well-annotated databases, or corpuses, of naturalistic multimodal behavior in relevant contexts are essential. To meet the needs for such data, we observed over 140 participants in varied, well-validated emotion inductions.   Participants were ages 18 to 66 years, 59% female, and racially and ethnically diverse (65% non-Euro-American).  Emotion inductions, seamlessly led by a professional actor and director,  included interview, film clip viewing, cold pressor test, startle, threat, and others selected to elicit a wide range of emotion responses. During the tasks, participant behavior was comprehensively recorded using both 3D and 2D video, audio microphone, infrared camera, and physiology sensors (heart rate, respiration, skin temperature, and blood pressure).  Self-reported emotion was collected throughout the session.   Facial expression was annotated offline using the Facial Action Coding System (FACS). FACS defines more than 30 anatomically based action units (AUs) that individually and in combination can describe nearly all-possible facial expressions.  AUs were coded with high reliability in each video frame. Thirty-four AUs were coded for occurrence; 8 AUs were coded for intensity on a 6-point ordinal scale.  In addition to 3D and 2D video, audio, and physiology, the corpus includes derived features from the 3D, 2D, and infrared sensors, and baseline results for detection of AU occurrence and intensity. The entire corpus will be made available to the research community for basic research in computational behavioral science, computer vision, psychology, and related fields, and for applied research in affective computing, marketing, mental health, education, and related fields.  Derivations of the database include synthesized non-frontal facial expression with which to train, test, and compare alternative approaches to face alignment and AU detection across a wide range of camera orientations. Subsets of the data have already made possible development of algorithms for 3D tracking and alignment of facial expression from 2D video, international competition in automatic facial expression recogntion, and validation of algorithms for detection of heart rate from 2D video recordings with moderate to large head rotation. These corpuses address the needs of basic and applied research in multiple domains, contribute to research infrastructure, and already are having significant impact in multiple domains.             Last Modified: 11/15/2016       Submitted by: Jeffrey F Cohn]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
