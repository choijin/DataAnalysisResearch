<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: 'Houston We Have A Solution': Novel Speech Processing Advancements for Analysis of Large Asynchronous Multi-Channel Audio Corpora</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>12/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>365199.00</AwardTotalIntnAmount>
<AwardAmount>393309</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is focused on developing new speech processing techniques which will transform access to large asynchronous multi-channel and diverse collections of multimedia materials. In particular, the algorithms developed are being employed to create a novel multi-source and multi-scale event reconstruction system that brings together the massive archives of the Apollo lunar missions, to create experiential interaction with historical materials. Specific research advancements are focused on state of the art acoustic environment analysis, speech recognition including keyword spotting, speaker identification under adverse conditions, multimodal content alignment, and automated linking for events and entities from spoken content. Specifically, the research is developing: (i) new techniques for noise- and channel-robust acoustic processing, exploiting missing-features concepts with novel feature extraction and compensation techniques, (ii) a new articulatory framework for speech recognition for robustness to variations in speech production, (iii) environmental "sniffing" techniques to automatically characterize acoustic environments to improve robustness, and (iv) automatic detection of novel task-specific audio-events. Since the data is asynchronous, unique speech analytics techniques are being formulated to address the large number of "local loop" intercom circuits in the NASA Mission Control Center, audio recorded onboard the two Apollo spacecrafts during specific mission events, and space-to-ground radio circuits. The specific speech, language, and knowledge extraction advancements will be integrated into a new automated evaluation model that reflects specific challenges encountered in the event reconstruction task. This platform will be deployed and evaluated by actual users from the Science and Engineering Education Center (SEEC) of the University of Texas at Dallas. &lt;br/&gt;&lt;br/&gt;Integration of robust speech processing algorithms with event reconstruction systems will have a direct and immediate impact on education, society, and government organizations. Working with NASA's Apollo mission data allows for the development of speech technology for challenging audio that contains severe communication channel artifacts, cross-talk/static/tones, and low signal-to-noise ratios. The software being developed in this project will be made available to any non-profit organization for use in audio/video search (download with training modules). Students working on senior design teams will also develop a Contact Science station to be deployed in Dallas, TX and overseen by the University of Texas in Dallas Science and Engineering Education Center to illustrate and assess student use of the advancements.  As a lasting legacy for this project, this project team includes eminent historians of human space flight, who will explore opportunities to deploy this event reconstruction system in a museum setting where it can support both scholarship and public engagement, and we will make the system itself available on an open-source basis to support other researchers.</AbstractNarration>
<MinAmdLetterDate>08/09/2012</MinAmdLetterDate>
<MaxAmdLetterDate>01/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1219130</AwardID>
<Investigator>
<FirstName>John H. L.</FirstName>
<LastName>Hansen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John H. L. Hansen</PI_FULL_NAME>
<EmailAddress>John.Hansen@utdallas.edu</EmailAddress>
<PI_PHON>9728832910</PI_PHON>
<NSF_ID>000197424</NSF_ID>
<StartDate>08/09/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Abhijeet</FirstName>
<LastName>Sangwan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abhijeet Sangwan</PI_FULL_NAME>
<EmailAddress>abhijeet.sangwan@utdallas.edu</EmailAddress>
<PI_PHON>9728832313</PI_PHON>
<NSF_ID>000611593</NSF_ID>
<StartDate>08/09/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Dallas</Name>
<CityName>Richardson</CityName>
<ZipCode>750803021</ZipCode>
<PhoneNumber>9728832313</PhoneNumber>
<StreetAddress>800 W. Campbell Rd., AD15</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX32</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>800188161</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT DALLAS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The University of Texas at Dallas]]></Name>
<CityName>Richardson</CityName>
<StateCode>TX</StateCode>
<ZipCode>750803021</ZipCode>
<StreetAddress><![CDATA[800 West Campbell Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX32</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~365199</FUND_OBLG>
<FUND_OBLG>2016~28110</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span style="text-decoration: underline;"><strong>Part A: Technical Description:</strong><br /></span>This project has focused on developing new speech/speaker/language processing techniques which transform access to large asynchronous multi-channel diverse collections of multimedia materials with application to NASA Apollo program (<a href="https://app.exploreapollo.org/">https://app.exploreapollo.org/</a>). In particular, the algorithms developed have been employed to create a novel multi-source interactive event system that brings together the massive archives of Apollo lunar missions, emphasizing Apollo-11, to create a unique experiential interaction with historical materials. The specific research accomplishments center around state-of-the-art audio based acoustic diarization (i.e., who spoke what when and how), environmental sniffing/analysis, speech recognition including keyword spotting, speaker identification and tracking under adverse conditions, speaker state and conversational interaction assessment (sentiment, word count, conversational leader/follower), and multimodal content alignment. Accomplished research includes: (i) new techniques for robust acoustic diarization processing, (ii) novel unsupervised speech activity detection; (iii) speaker clustering, identification, tracking; (iv) hot-spot detection using automatic speech recognition and keyword spotting; (v) speaker state assessment including sentiment analysis; (vi) conversational interaction analysis; and (vii) corpus development including both audio hardware advancements and software pipeline processing for digitizing Apollo 30-track tapes and massive audio archive knowledge extraction. &nbsp;Since data is asynchronous, unique speech analytic techniques were formulated to address &ldquo;local loop&rdquo; intercom circuits in NASA Mission Control Center and space-to-ground radio circuits. The specific speech, language, and knowledge extraction advancements were integrated into a new automated evaluation model that reflects specific challenges encountered in event reconstruction. The foundations for this platform included K-12/student feedback support from the Science and Engineering Education Center (SEEC) and UG Senior Design Teams at The University of Texas at Dallas.</p> <p>From an engineering perspective, this project required CRSS-UTDallas to re-engineer NASA&rsquo;s SoundScriber 30-track audio playback system (Serial No. 2, built in 1960&rsquo;s) to support simultaneous 30-track digitizing of Apollo analog tapes. A one-of-kind 30-track read-head was designed, installed/tested, and employed to capture 19,000hours of Apollo audio data by CRSS-UTDallas. This corpus consists of all human communications from Apollo-11, most of Apollo-13, Apollo-1, and Gemini-8. After four months of 24/7 digitizing, this data was organized based on channel loops from each NASA Historical Recorder track and synchronized based on mission elapsed time(MET). A new massive diarization pipeline was established to provide automatic transcript generation for the entire corpus (+6month effort). An efficient audio/transcript review system was also established to allow NASA Export-Control to effectively review and approve for release all data (+1000hrs already approved;2/1/2017). All approved data will be released to the general public (<a href="http://archive.org/">http://archive.org</a>). CRSS-UTDallas also worked closely with their collaborators at Univ. of Maryland on content generation and document linking to support their efforts for the Explore Apollo experience. The Explore Apollo interactive web experience (<a href="https://app.exploreapollo.org/">https://app.exploreapollo.org/</a>) was demonstrated during Engineer&rsquo;s Week at the Ross Perot Museum of Science and Technology (Dallas,TX) in 2015,16,17, which included formal K-12 student feedback as part of UTDallas &ldquo;The Heroes behind the Heroes of Apollo-11: Role of STEM&rdquo; event. Finally, CRSS-UTDallas developed 5 Challenge Tasks entitled <strong>&ldquo;Fearless Steps&rdquo;</strong>&ndash;that employed a subset of Apollo-11 for open domain research challenges using real naturalistic data for the speech/language community for (i) speech activity detection (SAD), (ii) speaker identification (SID), (iii) audio diarization, (iv) automatic speech recognition (ASR), and (v) joint topic &amp; sentiment detection and tracking. Collectively, the research and corpus realized through this project have advanced speech/language algorithms/technology, provided extensive outreach engagements for STEM/K-12 student interaction, as well as providing the speech &amp; language community a unique complex and carefully organized massive audio corpus of one of mankind&rsquo;s most important engineering accomplishments.</p> <p><strong><span style="text-decoration: underline;">Part B: Project&rsquo;s Broader Impact</span></strong><span style="text-decoration: underline;">:<br /></span>The integration of robust speech processing algorithms with event reconstruction systems from this study continues to have a direct impact on education, society, and government organizations. Working with NASA&rsquo;s Apollo mission data has allowed for development of speech technology for challenging audio that contains severe communication channel artifacts, multi-speaker back-loops, cross-talk/static/tones, and low signal-to-noise ratios. Students working on senior design teams have also developed the &ldquo;Heroes behind the Heroes of Apollo-11: Role of STEM&rdquo; experience through the Explore Apollo interactive web experience (<a href="https://app.exploreapollo.org/">https://app.exploreapollo.org/</a>) along with continued advice and feedback from UTDallas SEEC (Science and Engineering Education Center) which will be freely distributed to any group/educational organization to help promote STEM fields. As a lasting legacy from this project, the project team has developed &ldquo;Fearless Steps&rdquo; challenge corpus for researchers wishing to address real-world multi-speaker team based voice communications, and continues to work with eminent historians of human space flight, as well as libraries/museums, to deploy this event reconstruction system in museum settings where it can support both scholarship and public engagement. All materials from the Explore Apollo interactive web experience are available on an open-source basis to support other researchers.</p> <p><span style="text-decoration: underline;"><strong>Key Words:</strong></span> Acoustic Modeling; Environmental Sniffing; Knowledge Integration, Metadata Extraction, Event Reconstruction, Multi-Channel Audio, Conversational Interaction Analysis.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/07/2017<br>      Modified by: John H. L.&nbsp;Hansen</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1219130/1219130_10199192_1498793230066_Image1-HighLevelProjectOverview--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1219130/1219130_10199192_1498793230066_Image1-HighLevelProjectOverview--rgov-800width.jpg" title="(Overview) High level Speech/Speaker/Language Project Structure"><img src="/por/images/Reports/POR/2017/1219130/1219130_10199192_1498793230066_Image1-HighLevelProjectOverview--rgov-66x44.jpg" alt="(Overview) High level Speech/Speaker/Language Project Structure"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig.1: High level structure for the project ?Houston, We have a Solution: Novel Speech Processing Advancements for Analysis and Linking of Large Asynchronous Multi-Channel Audio Corpora";  (UTDallas) University of Texas at Dallas; (CRSS) Center for Robust Speech Systems</div> <div class="imageCredit">CRSS-UTDallas</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">John H. L.&nbsp;Hansen</div> <div class="imageTitle">(Overview) High level Speech/Speaker/Language Project Structure</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1219130/1219130_10199192_1498793568161_Image2-CRSS_UTDallas-ApolloCorpusDevelopment--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1219130/1219130_10199192_1498793568161_Image2-CRSS_UTDallas-ApolloCorpusDevelopment--rgov-800width.jpg" title="Fig.2: CRSS-UTDallas Apollo Corpus Development"><img src="/por/images/Reports/POR/2017/1219130/1219130_10199192_1498793568161_Image2-CRSS_UTDallas-ApolloCorpusDevelopment--rgov-66x44.jpg" alt="Fig.2: CRSS-UTDallas Apollo Corpus Development"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig.2: (a) prior state of NASA SoundScriber Play-Back hardware (Sept.?12); (b) CRSS-UTDallas program to upgrade SoundScriber; 30-track digitizing solution: new read head plus new cabling solution; massive APOLLO analog tapes collection; transcript diarization pipeline. Current state (June?17).</div> <div class="imageCredit">CRSS-UTDallas</div> <div class="imageSubmitted">John H. L.&nbsp;Hansen</div> <div class="imageTitle">Fig.2: CRSS-UTDallas Apollo Corpus Development</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502073709531_Fig3-PPT-File--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502073709531_Fig3-PPT-File--rgov-800width.jpg" title="Fig. 3 (Apollo-11 Stages) Study on speaker ID for Apollo-11 mission"><img src="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502073709531_Fig3-PPT-File--rgov-66x44.jpg" alt="Fig. 3 (Apollo-11 Stages) Study on speaker ID for Apollo-11 mission"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig. 3: mission stages where audio was extracted for speech analysis and speaker recognition assessment. These phases were analyzed over all Mission Controllers voice engagement using CRSS-UTDallas digitized 30-track audio loops. All audio contained in CRSS-UTDallas ?Fearless Steps? corpus release.</div> <div class="imageCredit">CRSS-UTDallas</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">John H. L.&nbsp;Hansen</div> <div class="imageTitle">Fig. 3 (Apollo-11 Stages) Study on speaker ID for Apollo-11 mission</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502077634509_Fig4-PPT-File_rev2--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502077634509_Fig4-PPT-File_rev2--rgov-800width.jpg" title="Fig. 4: (CRSS-UTDallas Launch Pad Web Interface - Apollo)"><img src="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502077634509_Fig4-PPT-File_rev2--rgov-66x44.jpg" alt="Fig. 4: (CRSS-UTDallas Launch Pad Web Interface - Apollo)"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig. 4: (CRSS-UTDallas Launch Pad Web Interface - Apollo) The front page ?Launch Pad? for web users to experience the Apollo archive through parallel audio streams, pictures, and stories; plus example Apollo Moment Pop-Up information for web users https://app.exploreapollo.org/ .</div> <div class="imageCredit">CRSS-UTDallas</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">John H. L.&nbsp;Hansen</div> <div class="imageTitle">Fig. 4: (CRSS-UTDallas Launch Pad Web Interface - Apollo)</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502079562388_Fig5-PPT-File--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502079562388_Fig5-PPT-File--rgov-800width.jpg" title="Fig. 5:  CRSS-UTDallas at Perot Museum (Feb. 27, 2016) for ENGINEER&rsquo;s WEEK. Interactive STEM Web-System for Audio/Image System being used by K-12 visitors:"><img src="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502079562388_Fig5-PPT-File--rgov-66x44.jpg" alt="Fig. 5:  CRSS-UTDallas at Perot Museum (Feb. 27, 2016) for ENGINEER&rsquo;s WEEK. Interactive STEM Web-System for Audio/Image System being used by K-12 visitors:"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig.5: Interactive STEM Web-System for Audio/Image System used by K-12 visitors: ?The Heroes behind the Heroes of Apollo-11?. CRSS-UTDallas (+7 UTD UG Design Teams)   participated in ENGINEER?s WEEK at Ross Perot Museum of Science & Technology in 2015,16,17; between 1200-2000 students attended daily</div> <div class="imageCredit">CRSS-UTDallas</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">John H. L.&nbsp;Hansen</div> <div class="imageTitle">Fig. 5:  CRSS-UTDallas at Perot Museum (Feb. 27, 2016) for ENGINEER?s WEEK. Interactive STEM Web-System for Audio/Image System being used by K-12 visitors:</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502080626431_Fig6-PPT-File_rev2--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502080626431_Fig6-PPT-File_rev2--rgov-800width.jpg" title="Fig. 6: Fig. 6 (Speaker/Conversation):  Analysis of speaker conversations for Apollo-11."><img src="/por/images/Reports/POR/2017/1219130/1219130_10199192_1502080626431_Fig6-PPT-File_rev2--rgov-66x44.jpg" alt="Fig. 6: Fig. 6 (Speaker/Conversation):  Analysis of speaker conversations for Apollo-11."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fig.6: Sample machine learning technology/algorithm knowledge extraction for the identified speaker and word sequence, the following statistics are determined:  (a) Most Frequent Words used; (b) Number of conversations initiated by each speaker; (c) Duration spoken by each speaker; (d) Chord Diagram</div> <div class="imageCredit">CRSS-UTDallas</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">John H. L.&nbsp;Hansen</div> <div class="imageTitle">Fig. 6: Fig. 6 (Speaker/Conversation):  Analysis of speaker conversations for Apollo-11.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Part A: Technical Description: This project has focused on developing new speech/speaker/language processing techniques which transform access to large asynchronous multi-channel diverse collections of multimedia materials with application to NASA Apollo program (https://app.exploreapollo.org/). In particular, the algorithms developed have been employed to create a novel multi-source interactive event system that brings together the massive archives of Apollo lunar missions, emphasizing Apollo-11, to create a unique experiential interaction with historical materials. The specific research accomplishments center around state-of-the-art audio based acoustic diarization (i.e., who spoke what when and how), environmental sniffing/analysis, speech recognition including keyword spotting, speaker identification and tracking under adverse conditions, speaker state and conversational interaction assessment (sentiment, word count, conversational leader/follower), and multimodal content alignment. Accomplished research includes: (i) new techniques for robust acoustic diarization processing, (ii) novel unsupervised speech activity detection; (iii) speaker clustering, identification, tracking; (iv) hot-spot detection using automatic speech recognition and keyword spotting; (v) speaker state assessment including sentiment analysis; (vi) conversational interaction analysis; and (vii) corpus development including both audio hardware advancements and software pipeline processing for digitizing Apollo 30-track tapes and massive audio archive knowledge extraction.  Since data is asynchronous, unique speech analytic techniques were formulated to address "local loop" intercom circuits in NASA Mission Control Center and space-to-ground radio circuits. The specific speech, language, and knowledge extraction advancements were integrated into a new automated evaluation model that reflects specific challenges encountered in event reconstruction. The foundations for this platform included K-12/student feedback support from the Science and Engineering Education Center (SEEC) and UG Senior Design Teams at The University of Texas at Dallas.  From an engineering perspective, this project required CRSS-UTDallas to re-engineer NASA?s SoundScriber 30-track audio playback system (Serial No. 2, built in 1960?s) to support simultaneous 30-track digitizing of Apollo analog tapes. A one-of-kind 30-track read-head was designed, installed/tested, and employed to capture 19,000hours of Apollo audio data by CRSS-UTDallas. This corpus consists of all human communications from Apollo-11, most of Apollo-13, Apollo-1, and Gemini-8. After four months of 24/7 digitizing, this data was organized based on channel loops from each NASA Historical Recorder track and synchronized based on mission elapsed time(MET). A new massive diarization pipeline was established to provide automatic transcript generation for the entire corpus (+6month effort). An efficient audio/transcript review system was also established to allow NASA Export-Control to effectively review and approve for release all data (+1000hrs already approved;2/1/2017). All approved data will be released to the general public (http://archive.org). CRSS-UTDallas also worked closely with their collaborators at Univ. of Maryland on content generation and document linking to support their efforts for the Explore Apollo experience. The Explore Apollo interactive web experience (https://app.exploreapollo.org/) was demonstrated during Engineer?s Week at the Ross Perot Museum of Science and Technology (Dallas,TX) in 2015,16,17, which included formal K-12 student feedback as part of UTDallas "The Heroes behind the Heroes of Apollo-11: Role of STEM" event. Finally, CRSS-UTDallas developed 5 Challenge Tasks entitled "Fearless Steps"&ndash;that employed a subset of Apollo-11 for open domain research challenges using real naturalistic data for the speech/language community for (i) speech activity detection (SAD), (ii) speaker identification (SID), (iii) audio diarization, (iv) automatic speech recognition (ASR), and (v) joint topic &amp; sentiment detection and tracking. Collectively, the research and corpus realized through this project have advanced speech/language algorithms/technology, provided extensive outreach engagements for STEM/K-12 student interaction, as well as providing the speech &amp; language community a unique complex and carefully organized massive audio corpus of one of mankind?s most important engineering accomplishments.  Part B: Project?s Broader Impact: The integration of robust speech processing algorithms with event reconstruction systems from this study continues to have a direct impact on education, society, and government organizations. Working with NASA?s Apollo mission data has allowed for development of speech technology for challenging audio that contains severe communication channel artifacts, multi-speaker back-loops, cross-talk/static/tones, and low signal-to-noise ratios. Students working on senior design teams have also developed the "Heroes behind the Heroes of Apollo-11: Role of STEM" experience through the Explore Apollo interactive web experience (https://app.exploreapollo.org/) along with continued advice and feedback from UTDallas SEEC (Science and Engineering Education Center) which will be freely distributed to any group/educational organization to help promote STEM fields. As a lasting legacy from this project, the project team has developed "Fearless Steps" challenge corpus for researchers wishing to address real-world multi-speaker team based voice communications, and continues to work with eminent historians of human space flight, as well as libraries/museums, to deploy this event reconstruction system in museum settings where it can support both scholarship and public engagement. All materials from the Explore Apollo interactive web experience are available on an open-source basis to support other researchers.  Key Words: Acoustic Modeling; Environmental Sniffing; Knowledge Integration, Metadata Extraction, Event Reconstruction, Multi-Channel Audio, Conversational Interaction Analysis.          Last Modified: 08/07/2017       Submitted by: John H. L. Hansen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
