<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: SMALL: Collaborative Research: Data Structures for Parallel Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>138994.00</AwardTotalIntnAmount>
<AwardAmount>138994</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops a theory for characterizing the performance of parallel data structures and parallel algorithms that use parallel structures.  Standard metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (critical-path length), do not naturally generalize in the presence of contention on shared data.  Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.&lt;br/&gt;&lt;br/&gt;The specific research goals are as follows:  &lt;br/&gt;(1) Investigate a methodology for designing and analyzing parallel algorithms that use data structures, especially amortized ones. &lt;br/&gt;(2) Design parallel schedulers that ameliorate the contention on parallel data structures. &lt;br/&gt;(3) Design parallel data structures that perform provably well with these schedulers.&lt;br/&gt;&lt;br/&gt;Today parallel computing is ubiquitous.  Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors.  Writing parallel code that provably scales well is challenging, and techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code.  This project will develop a theoretical foundation for characterizing the scalability of parallel programs that contend for access to shared data.</AbstractNarration>
<MinAmdLetterDate>07/20/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1218188</AwardID>
<Investigator>
<FirstName>Jeremy</FirstName>
<LastName>Fineman</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeremy T Fineman</PI_FULL_NAME>
<EmailAddress>jfineman@cs.georgetown.edu</EmailAddress>
<PI_PHON>2026875141</PI_PHON>
<NSF_ID>000610554</NSF_ID>
<StartDate>07/20/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgetown University</Name>
<CityName>Washington</CityName>
<ZipCode>200571789</ZipCode>
<PhoneNumber>2026250100</PhoneNumber>
<StreetAddress>37th &amp; O St N W</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049515844</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGETOWN UNIVERSITY (THE)</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049515844</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgetown University]]></Name>
<CityName/>
<StateCode>DC</StateCode>
<ZipCode>200571789</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~138994</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project is to develop a theory for characterizing the performance of parallel data structures and parallel algorithms that use data structures. &nbsp;Standard metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (length of the critical path), do not naturally generalize in the presence of contention on shared data. Moreover, the standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are used in parallel, in part because the performance depends on properties of the scheduler.</p> <p>The main outcomes of this project are:</p> <p>1) A new work-stealing scheduler that interacts well with certain types of data structures. The scheduler is accompanied by a strong performance theorem for the overall running time of parallel programs that uses certain types of data structures. The performance theorem relies on easy-to-use metrics (in addition to work and span), and the guarantees readily extend even to amortized data structures.</p> <p>2) A runtime-system implementation, called BATCHER, that implements the above scheduler and makes it easier to incorporate data structures into a parallel program. One of the key advantages of BATCHER is that it uses batched data structures instead of concurrent data structures. Implementing efficient batched operations (using standard parallel constructs) can be much easier than coping with arbitrary concurrency. BATCHER automatically groups operations into batches, essentially transforming a program that makes concurrent accesses into one that makes batched accesses.&nbsp;</p> <p>3) A new on-the-fly "determinacy race" detector. Two data accesses are logically parallel if they allowed to be reordered with respect to each other by the scheduler. If two accesses to the same location are logically parallel, and at least one of those accesses is a write, then a determinacy race occurs. Determinacy races lead to nondeterministic program behavior, and as such they are difficult to reason about. This nondeterminacy is often unintential and indicates a bug in the program. A race detector is a tool for discovering determinacy races in a program execution. Some determinacy race detectors rely on shared data structures. This project addresses one of those shared data structures and includes an implementation of the resulting determinacy-race detector.</p><br> <p>            Last Modified: 10/28/2015<br>      Modified by: Jeremy&nbsp;T&nbsp;Fineman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project is to develop a theory for characterizing the performance of parallel data structures and parallel algorithms that use data structures.  Standard metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (length of the critical path), do not naturally generalize in the presence of contention on shared data. Moreover, the standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are used in parallel, in part because the performance depends on properties of the scheduler.  The main outcomes of this project are:  1) A new work-stealing scheduler that interacts well with certain types of data structures. The scheduler is accompanied by a strong performance theorem for the overall running time of parallel programs that uses certain types of data structures. The performance theorem relies on easy-to-use metrics (in addition to work and span), and the guarantees readily extend even to amortized data structures.  2) A runtime-system implementation, called BATCHER, that implements the above scheduler and makes it easier to incorporate data structures into a parallel program. One of the key advantages of BATCHER is that it uses batched data structures instead of concurrent data structures. Implementing efficient batched operations (using standard parallel constructs) can be much easier than coping with arbitrary concurrency. BATCHER automatically groups operations into batches, essentially transforming a program that makes concurrent accesses into one that makes batched accesses.   3) A new on-the-fly "determinacy race" detector. Two data accesses are logically parallel if they allowed to be reordered with respect to each other by the scheduler. If two accesses to the same location are logically parallel, and at least one of those accesses is a write, then a determinacy race occurs. Determinacy races lead to nondeterministic program behavior, and as such they are difficult to reason about. This nondeterminacy is often unintential and indicates a bug in the program. A race detector is a tool for discovering determinacy races in a program execution. Some determinacy race detectors rely on shared data structures. This project addresses one of those shared data structures and includes an implementation of the resulting determinacy-race detector.       Last Modified: 10/28/2015       Submitted by: Jeremy T Fineman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
