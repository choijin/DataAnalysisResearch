<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Small: Towards Exposing and Mitigating End-to-End TCP Performance and Fairness Issues in Data Center Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>While TCP was designed to work in any environment, it has a long history of problems whenever introduced in newer environments (e.g., wireless networks, satellite networks, high-capacity networks) that it was not explicitly designed and tested for.  A similar new frontier that challenges TCP is the data center and cloud environments that have become popular in recent times.  This project will conduct research on exposing end-to-end challenges that TCP faces in the modern cloud computing and data center environments and propose solutions to address them.  &lt;br/&gt;&lt;br/&gt;Specifically, it focuses on three major issues surrounding TCP in data center and cloud environments: First, in virtualized cloud environments, when multiple VMs share the CPU, CPU access latency for each VM (i.e., the interval during which a VM waits for the CPU) is in the order of tens/hundreds of milliseconds and can be orders of magnitude higher than the typical sub-millisecond network RTTs.  This high RTT causes significant reduction in TCP throughput.  Second, in multirooted data center networks, under certain conditions, TCP connections sharing a common link exhibit severe unfairness.  Finally, in data center networks today, equal-cost multipath routing (ECMP) is often used to split traffic across multiple paths, but it can potentially cause significant load imbalance.  Researchers have refrained from suggesting packet-level multipath routing in the past because of its ability to cause reordering that may reduce TCP throughput. It is however not clear whether this poor interactions with multipath routing exists even under symmetric topologies such as fattrees, or more generally, multi-rooted tree topologies. &lt;br/&gt;&lt;br/&gt;Intellectual Merit: The goal of this project is to comprehensively investigate these afore-mentioned three major issues discussed above, propose new solutions to address the problems, and finally, validate these solutions and hypotheses using real prototype implementations and through extensive evaluations.  (1) To address TCP's performance deterioration in virtualized cloud environments, the project will explore a new approach called transport function delegation, where certain TCP functions are delegated to the driver domain or hypervisor.  (2) It will conduct extensive experimentation using real testbeds to expose, and characterize the unfairness issue, and also propose and evaluate classic solutions (e.g., RED) and a new routing algorithm called equal-length routing to mitigate this problem. (3) It will revisit the conventional wisdom that fine-grained multi-path traffic splitting protocols interact poorly with TCP, in the context of data center networks which have regular topologies such as multi-rooted trees. &lt;br/&gt;&lt;br/&gt;Broader Impact: The broader impact of this research comprises of the following: (1) It will help improve the key aspects of TCP such as performance and fairness in data center and cloud environments, that will benefit all data center systems and applications.  (2) Results of this research will be transferred to industry. (3) Results of this research will be integrated into courses such as operating systems, computer networks, and cloud computing. It will also provide training to graduate students and several Ph.D. theses are expected to come out of this research. (4) It will include participation of minorities (under-represented minorities and women).</AbstractNarration>
<MinAmdLetterDate>08/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/29/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1219004</AwardID>
<Investigator>
<FirstName>Dongyan</FirstName>
<LastName>Xu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dongyan Xu</PI_FULL_NAME>
<EmailAddress>dxu@cs.purdue.edu</EmailAddress>
<PI_PHON>7654946182</PI_PHON>
<NSF_ID>000295392</NSF_ID>
<StartDate>05/29/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dongyan</FirstName>
<LastName>Xu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dongyan Xu</PI_FULL_NAME>
<EmailAddress>dxu@cs.purdue.edu</EmailAddress>
<PI_PHON>7654946182</PI_PHON>
<NSF_ID>000295392</NSF_ID>
<StartDate>08/06/2012</StartDate>
<EndDate>05/29/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ramana</FirstName>
<LastName>Kompella</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ramana R Kompella</PI_FULL_NAME>
<EmailAddress>Kompella@cs.purdue.edu</EmailAddress>
<PI_PHON>7654944600</PI_PHON>
<NSF_ID>000501961</NSF_ID>
<StartDate>08/06/2012</StartDate>
<EndDate>05/29/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072107</ZipCode>
<StreetAddress><![CDATA[305 N. University St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Datacenters are critical infrastructures that enable cloud computing applications and services. In a datacenter, system and network virtualization has been widely adopted as the methodology for creating virtual infrastructures on behalf of cloud-hosted tenants, on top of shared physical machines and networks. An unexpected problem arises from the resource-sharing nature of such virtual infrastructures (e.g., virtual machines or VMs): the VM scheduling latency and overhead negatively affects the throughput of (and fairness among) network connections between VMs. Furthermore, such a problem can be generalized to the end-to-end I/O path that involves both network and end-system resources. In this project, we aim at addressing this class of network I/O performance and fairness problems arising from virtualization in a datacenter.&nbsp;</p> <p>On the intellectual merit front, we have developed a suite of solutions that address different aspects of the end-to-end I/O path: (1) <strong>vSlicer</strong> is an adaptive VM scheduler that differentiates between computation-intensive and I/O-intensive VMs, with the goal of improving network throughput of the latter while maintaining CPU-sharing fairness; (2) <strong>vTurbo</strong> involves dedicating a CPU core for fast I/O event processing on behalf of all VMs in the same physical machine; (3) <strong>vPipe</strong> is an I/O-piping method that creates shortcuts on end-to-end I/O paths (e.g., network to disk, network to network, and disk to disk) for improved throughput, latency, and application performance; (4) <strong>vFair</strong> is a storage I/O scheduling framework that achieves storage resource sharing fairness among all tenant VMs; (5) <strong>vHaul</strong> enables minimum-disruption live migration of cloud applications across datacenter networks; (6) <strong>vRead</strong> optimizes data read operation performance for tenant VMs when accessing the Hadoop distributed file system; (7)<strong> StorM </strong>enables software-defined network middle-boxes between tenant VMs and backend storage for deploying tenant-specific reliability and security services for cloud data.</p> <p>On the broader impact front, three PhD students have been trained and graduated in the past three years. The project also involved two undergraduate students who gained valuable research experience during their junior and senior years. Results from this project have been integrated into both undergraduate and graduate operating systems courses (e.g., in CPU scheduling, interrupt handling, and network protocol stack chapters) taught by the PI. This project also led to fruitful collaborations with industry (e.g., Microsoft, AT&amp;T, VMware, and IBM). Software artifacts developed by project members have been shared with our industry partners. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><br> <p>            Last Modified: 06/07/2017<br>      Modified by: Dongyan&nbsp;Xu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Datacenters are critical infrastructures that enable cloud computing applications and services. In a datacenter, system and network virtualization has been widely adopted as the methodology for creating virtual infrastructures on behalf of cloud-hosted tenants, on top of shared physical machines and networks. An unexpected problem arises from the resource-sharing nature of such virtual infrastructures (e.g., virtual machines or VMs): the VM scheduling latency and overhead negatively affects the throughput of (and fairness among) network connections between VMs. Furthermore, such a problem can be generalized to the end-to-end I/O path that involves both network and end-system resources. In this project, we aim at addressing this class of network I/O performance and fairness problems arising from virtualization in a datacenter.   On the intellectual merit front, we have developed a suite of solutions that address different aspects of the end-to-end I/O path: (1) vSlicer is an adaptive VM scheduler that differentiates between computation-intensive and I/O-intensive VMs, with the goal of improving network throughput of the latter while maintaining CPU-sharing fairness; (2) vTurbo involves dedicating a CPU core for fast I/O event processing on behalf of all VMs in the same physical machine; (3) vPipe is an I/O-piping method that creates shortcuts on end-to-end I/O paths (e.g., network to disk, network to network, and disk to disk) for improved throughput, latency, and application performance; (4) vFair is a storage I/O scheduling framework that achieves storage resource sharing fairness among all tenant VMs; (5) vHaul enables minimum-disruption live migration of cloud applications across datacenter networks; (6) vRead optimizes data read operation performance for tenant VMs when accessing the Hadoop distributed file system; (7) StorM enables software-defined network middle-boxes between tenant VMs and backend storage for deploying tenant-specific reliability and security services for cloud data.  On the broader impact front, three PhD students have been trained and graduated in the past three years. The project also involved two undergraduate students who gained valuable research experience during their junior and senior years. Results from this project have been integrated into both undergraduate and graduate operating systems courses (e.g., in CPU scheduling, interrupt handling, and network protocol stack chapters) taught by the PI. This project also led to fruitful collaborations with industry (e.g., Microsoft, AT&amp;T, VMware, and IBM). Software artifacts developed by project members have been shared with our industry partners.                Last Modified: 06/07/2017       Submitted by: Dongyan Xu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
