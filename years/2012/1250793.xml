<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Small: DA: DCM: Labeling the World</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2013</AwardEffectiveDate>
<AwardExpirationDate>03/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>750000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project aims to  leverage the massive corpus of online photos, text, and maps to create a semantic 3D labeled model of the world, e.g., detailed representations of the world's top cultural and historical sites.  While breakthroughs in computer vision enable creating detailed 3D models from millions of online 2D images,  the resulting models capture only geometry. Consequently, they lack semantics; they don't provide information about the contents of the scene.  The vast treasure trove of online text such as Wikipedia meticulously catalogs the scenes that are captured in  photos and models.  Modern Natural Language Processing (NLP) techniques can now process such data, opening up the opportunity to extract knowledge from the online text corpus and use it to label 3D geometry.  This project seeks to jointly analyze the massive corpus of online text, maps, and photos to create labeled 3D models of the world's sites.  Achieving this goal will require fundamental research advances at the interface of natural language processing and computer vision that impact both the scientific research community and the world at large. &lt;br/&gt;&lt;br/&gt;The project addresses two key technical challenges: (1) automatic scene labeling:  mapping semantics onto geometry, and (2) solving the 3D jigsaw puzzle:  mapping pieces of geometry into the world.  Many clues to these mapping problems lie in the text and other online datasources such as floorplans.  Other clues lie in the content of the photos.  Decoding this mapping therefore involves an interplay between NLP and computer vision.  The key research advances center around new ways to jointly leverage computer vision and NLP to solve problems to solve challenging problems in both fields, specifically, 1) recognizing objects through joint NLP and 3D visual analysis, 2) placing objects in the world by correlating geometry with spatial text in maps and webpages, and 3) using semantics to improve geometry by augmenting visual cues with textual spatial relations.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The primary research outcomes are: (1) technology for creating labeled 3D models at a massive scale, and (2) labeled models for many top tourist sites.  Both the algorithms and models will be made freely available for the research community.  These algorithms and models will provide the foundation for a range of exciting applications of major practical impact on the world at large.  The resulting tools could make it possible for resources such as Wikipedia to link the text directly to 3D models and vice-versa, with attendant benefits to online learning and education.  The same technology could enable automated labeling of 2D photographs.  In the context of real-time applications (e.g., augmented reality), the technology could provide visual overlays and instant feedback on what you are currently looking at, and enable augmented reality-style guided tours.  Other applications include using labeled geometry for navigation (walking directions), and converting images to text for the visually impaired. The research is tightly integrated into education and training of students at the University of Washington. Additional information about the project can be found at: http://grail.cs.washington.edu/projects/label3d/</AbstractNarration>
<MinAmdLetterDate>04/11/2013</MinAmdLetterDate>
<MaxAmdLetterDate>04/11/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1250793</AwardID>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Seitz</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven M Seitz</PI_FULL_NAME>
<EmailAddress>seitz@cs.washington.edu</EmailAddress>
<PI_PHON>2066169431</PI_PHON>
<NSF_ID>000195084</NSF_ID>
<StartDate>04/11/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Luke</FirstName>
<LastName>Zettlemoyer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Luke Zettlemoyer</PI_FULL_NAME>
<EmailAddress>lsz@cs.washington.edu</EmailAddress>
<PI_PHON>2066851227</PI_PHON>
<NSF_ID>000581613</NSF_ID>
<StartDate>04/11/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName/>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~750000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-c706a20f-1ab6-4a2e-231e-140733a5219c"> </span></p> <p dir="ltr"><span>The goal of this project was to develop algorithms and systems to create semantic 3D models of the world&rsquo;s objects, places, and things. &nbsp;Achieving this goal required fundamental research advances at the interface of computer vision and natural language processing that impact both the scientific research community and the world at large.</span></p> <p dir="ltr"><span>One major thrust was to mine the massive corpus of online photos, text, and maps, to infer large scale semantic 3D models. &nbsp;For example, in the 3D Wikipedia work, we developed an approach for analyzing Wikipedia and other text, together with online photos, to produce annotated 3D models of famous tourist sites. &nbsp;A key result was a 3D tour visualization of landmark like the Pantheon and St. Peter&rsquo;s Cathedral in 3D, using a model that is reconstructed from the online photos, and automatically annotated with descriptive text from Wikipedia. &nbsp;Notable technical contributions included a novel method of mapping large indoor spaces by analyzing annotated maps of a site, together with Internet photos. &nbsp;We also developed a new technique for automatically creating 3D time-lapse videos from thousands of Internet photos taken over many years. &nbsp;Our results show photorealistic time-lapses of receding glaciers, changing city scapes, long term archaeological excavations, and many other striking examples.</span></p> <p dir="ltr"><span>In a similar vein of correlating online photos with text data, we developed a system &ldquo;PhotoRecall&rdquo; for searching your personal photos using an extremely wide range of text queries, including dates and holidays (</span><span>Halloween</span><span>), named and categorical places (</span><span>Empire State Building</span><span> or </span><span>park</span><span>), events and occasions (</span><span>Radiohead concert</span><span> or </span><span>wedding</span><span>), activities (</span><span>skiing</span><span>), object categories (</span><span>whales</span><span>), attributes (</span><span>outdoors</span><span>), and object instances (</span><span>Mona Lisa</span><span>), and any combination of these -- all with </span><span>no </span><span>manual labeling required. We accomplish this by correlating information in your photos -- the timestamps, GPS locations, and image pixels -- to information mined from the Internet. </span></p> <p dir="ltr"><span>We introduced novel techniques for building 3D CAD models of homes, offices, and other interiors using commodity mobile phones and tablets. &nbsp;Our techniques are able to infer not just floor plans and room layouts, but to accurately reproduce the placement of furniture items. &nbsp;Key contributions include the design of a multi-model user interface combined with 3D reconstruction, object recognition, and alignment algorithms working in tandem.</span></p> <p dir="ltr"><span>Finally, we developed a fully-automated technique that reconstructs a 3D room and furniture CAD model from a single photo, capturing walls, chairs, windows, bookshelves, tables, chairs, etc. &nbsp;</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 07/06/2017<br>      Modified by: Steven&nbsp;M&nbsp;Seitz</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The goal of this project was to develop algorithms and systems to create semantic 3D models of the world?s objects, places, and things.  Achieving this goal required fundamental research advances at the interface of computer vision and natural language processing that impact both the scientific research community and the world at large. One major thrust was to mine the massive corpus of online photos, text, and maps, to infer large scale semantic 3D models.  For example, in the 3D Wikipedia work, we developed an approach for analyzing Wikipedia and other text, together with online photos, to produce annotated 3D models of famous tourist sites.  A key result was a 3D tour visualization of landmark like the Pantheon and St. Peter?s Cathedral in 3D, using a model that is reconstructed from the online photos, and automatically annotated with descriptive text from Wikipedia.  Notable technical contributions included a novel method of mapping large indoor spaces by analyzing annotated maps of a site, together with Internet photos.  We also developed a new technique for automatically creating 3D time-lapse videos from thousands of Internet photos taken over many years.  Our results show photorealistic time-lapses of receding glaciers, changing city scapes, long term archaeological excavations, and many other striking examples. In a similar vein of correlating online photos with text data, we developed a system "PhotoRecall" for searching your personal photos using an extremely wide range of text queries, including dates and holidays (Halloween), named and categorical places (Empire State Building or park), events and occasions (Radiohead concert or wedding), activities (skiing), object categories (whales), attributes (outdoors), and object instances (Mona Lisa), and any combination of these -- all with no manual labeling required. We accomplish this by correlating information in your photos -- the timestamps, GPS locations, and image pixels -- to information mined from the Internet.  We introduced novel techniques for building 3D CAD models of homes, offices, and other interiors using commodity mobile phones and tablets.  Our techniques are able to infer not just floor plans and room layouts, but to accurately reproduce the placement of furniture items.  Key contributions include the design of a multi-model user interface combined with 3D reconstruction, object recognition, and alignment algorithms working in tandem. Finally, we developed a fully-automated technique that reconstructs a 3D room and furniture CAD model from a single photo, capturing walls, chairs, windows, bookshelves, tables, chairs, etc.            Last Modified: 07/06/2017       Submitted by: Steven M Seitz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
