<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Reinforcement Learning by Mirror Descent</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>449991.00</AwardTotalIntnAmount>
<AwardAmount>449991</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A fundamental challenge in machine learning is the design of computational agents that, rather than being explicitly programmed, autonomously learn complex tasks in stochastic real-world environments.  Past approaches, such as reinforcement learning algorithms for solving Markov decision processes, scale poorly to large state spaces. The proposed research addresses this curse of dimensionality by investigating a novel framework combining reinforcement learning and online convex optimization, in particular mirror descent and related algorithms.  Mirror descent scales significantly better than classical first-order gradient descent in high-dimensional state spaces, by using a distance-generating function specific to a particular state space geometry.&lt;br/&gt;&lt;br/&gt;The proposed framework enables several significant algorithmic advances in the design of autonomous machine learning agents: a new class of first-order mirror-descent based methods for learning sparse solutions to Markov decision processes will be developed that scale significantly significantly better than previous second-order methods; novel hierarchical methods for solving semi-Markov decision processes will be investigated; and finally, applications to a variety of high-dimensional Markov decision processes will be explored.&lt;br/&gt;&lt;br/&gt;The anticipated outcomes of the proposed work include foundational advances in designing autonomous agents that learn to solve sequential decision-making problems, which will impact a large number of target applications from manufacturing to robotics and scheduling. The educational goal includes the development of a graduate-level course in online convex optimization for sequential decision-making, as well as interdisciplinary tutorials to enhance the cross-fertilization of ideas from applied mathematics and optimization to machine learning and artificial intelligence.</AbstractNarration>
<MinAmdLetterDate>07/16/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/16/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1216467</AwardID>
<Investigator>
<FirstName>Sridhar</FirstName>
<LastName>Mahadevan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sridhar Mahadevan</PI_FULL_NAME>
<EmailAddress>mahadeva@cs.umass.edu</EmailAddress>
<PI_PHON>4135453140</PI_PHON>
<NSF_ID>000203723</NSF_ID>
<StartDate>07/16/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Computer Science Department]]></Name>
<CityName>Amherst</CityName>
<StateCode>MA</StateCode>
<ZipCode>010039264</ZipCode>
<StreetAddress><![CDATA[140 Governors Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~449991</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A major challenge in developing autonomous artificial intelligent systems is enabling them to learn successfully from trial and error. Agents, whether biological or artificial, must of necessity take decisions in the absence of complete information. These decisions may often be incorrect. A successful agent is one that is able to learn through experience how to make better decisions. The field of reinforcement learning studies algorithms that can give agents the ability to learn optimal decision making over time. For over three decades, the field has struggled to develop algorithms that are stable under any condition. The most popular algorithm, temporal difference learning, was long known to produce instabilities under certain conditions.&nbsp;</p> <p>This project led to a key breakthrough in designing a new class of temporal difference learning methods that are truly stable under a wide range of input conditions. The new method was based on a dual space model, whereby the perceived state and decision is mapped into a new dual space where gradient updates are undertaken. These are then mapped back to the original space. The mapping from primal to dual space can be chosen to make the algorithm particularly robust and scalable in high dimensional problems.&nbsp;</p> <p>The project also showed the empirical success of the new algorithms and undertook a computational study of their performance given finite number of samples. Further work on scaling these methods to new problems is ongoing.&nbsp;</p><br> <p>            Last Modified: 09/04/2016<br>      Modified by: Sridhar&nbsp;Mahadevan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A major challenge in developing autonomous artificial intelligent systems is enabling them to learn successfully from trial and error. Agents, whether biological or artificial, must of necessity take decisions in the absence of complete information. These decisions may often be incorrect. A successful agent is one that is able to learn through experience how to make better decisions. The field of reinforcement learning studies algorithms that can give agents the ability to learn optimal decision making over time. For over three decades, the field has struggled to develop algorithms that are stable under any condition. The most popular algorithm, temporal difference learning, was long known to produce instabilities under certain conditions.   This project led to a key breakthrough in designing a new class of temporal difference learning methods that are truly stable under a wide range of input conditions. The new method was based on a dual space model, whereby the perceived state and decision is mapped into a new dual space where gradient updates are undertaken. These are then mapped back to the original space. The mapping from primal to dual space can be chosen to make the algorithm particularly robust and scalable in high dimensional problems.   The project also showed the empirical success of the new algorithms and undertook a computational study of their performance given finite number of samples. Further work on scaling these methods to new problems is ongoing.        Last Modified: 09/04/2016       Submitted by: Sridhar Mahadevan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
