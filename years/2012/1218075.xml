<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Ensuring Reliability and Portability of Scientific Software for Heterogeneous Architectures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>499857.00</AwardTotalIntnAmount>
<AwardAmount>507857</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nina Amla</SignBlockName>
<PO_EMAI>namla@nsf.gov</PO_EMAI>
<PO_PHON>7032927991</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Numerical results of scientific computations are stored in computers as&lt;br/&gt;floating-point numbers, an approximation of real numbers that accounts for&lt;br/&gt;the fact that a computer's storage is limited. This need for approximation&lt;br/&gt;has the unfortunate side effect that floating-point numbers don't abide by&lt;br/&gt;common laws of arithmetic known from high school, such as the associativity&lt;br/&gt;of addition. As a consequence, apparently equivalent implementations of&lt;br/&gt;floating-point operations on computer hardware may produce very different&lt;br/&gt;results, such as when the order of operands of an addition is changed by a&lt;br/&gt;compiler. Programs generically written for high-performance parallel computing&lt;br/&gt;platforms are likely to be compiled using different floating-point&lt;br/&gt;implementations and schedulings, as the executable resulting from the&lt;br/&gt;compilation depends on the available hardware. Such parallel scientific&lt;br/&gt;programs are therefore susceptible to reliability and portability issues&lt;br/&gt;that can range from simple deviations in precision to drastic changes of&lt;br/&gt;program control flow when moving from one architecture to another.&lt;br/&gt;The results of this research will be tools and techniques to help scientists &lt;br/&gt;find bugs more effectively in such programs. This research has important implications &lt;br/&gt;for the reliability of important scientific programs such as those used in biomedical &lt;br/&gt;imaging applications, climate modelling, and vehicle design. &lt;br/&gt;&lt;br/&gt;This project develops rigorous methods for analyzing parallel scientific&lt;br/&gt;code, specifically written using the now emerging OpenCL parallel&lt;br/&gt;programming standard. The goal is to detect potential sources of&lt;br/&gt;reliability and portability deficiencies in such code that are due to&lt;br/&gt;dependencies of the floating-point behavior on the underlying hardware,&lt;br/&gt;which may be unknown to the programmer. Traditional reliability methods&lt;br/&gt;such as program testing and debugging are ineffective for parallel OpenCL&lt;br/&gt;programs, because program behavior may vary across runs, making after-test&lt;br/&gt;behavior uncertain. For these reasons, the investigators will use rigorous&lt;br/&gt;analysis methods that are not solely based on program execution. Instead,&lt;br/&gt;the program is formally modeled as a transition system; the model is&lt;br/&gt;encoded symbolically, using logical formula representations that can often&lt;br/&gt;compactly represent the set of executions of the program without executing&lt;br/&gt;it. The program model is then analyzed for portability violations and&lt;br/&gt;program errors using floating point-capable decision procedures and model&lt;br/&gt;checkers. To achieve scalability, the investigators plan to exploit the&lt;br/&gt;highly symmetric and parametric form of OpenCL programs, where identical&lt;br/&gt;operations are performed by many computational threads in Single&lt;br/&gt;Instruction Multiple Data (SIMD) style.</AbstractNarration>
<MinAmdLetterDate>07/03/2012</MinAmdLetterDate>
<MaxAmdLetterDate>04/15/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1218075</AwardID>
<Investigator>
<FirstName>Miriam</FirstName>
<LastName>Leeser</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Miriam E Leeser</PI_FULL_NAME>
<EmailAddress>mel@ece.neu.edu</EmailAddress>
<PI_PHON>6173733814</PI_PHON>
<NSF_ID>000194950</NSF_ID>
<StartDate>07/03/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Wahl</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Wahl</PI_FULL_NAME>
<EmailAddress>wahl@ccs.neu.edu</EmailAddress>
<PI_PHON>6173733100</PI_PHON>
<NSF_ID>000608439</NSF_ID>
<StartDate>07/03/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northeastern University</Name>
<CityName>BOSTON</CityName>
<ZipCode>021155005</ZipCode>
<PhoneNumber>6173733004</PhoneNumber>
<StreetAddress>360 HUNTINGTON AVE</StreetAddress>
<StreetAddress2><![CDATA[177-500]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001423631</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHEASTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001423631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northeastern University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>021155005</ZipCode>
<StreetAddress><![CDATA[360 HUNTINGTON AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~499857</FUND_OBLG>
<FUND_OBLG>2013~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to investigate to what extent <strong>calculations</strong> on computers, i.e. the processing of purely numeric data such as in scientific computing, depends on the computational platform on which the calculation runs. The background is that, while mathematics defines unambiguously what the output of arithmetic operations such as + is , computers cannot implement mathematical arithmetic, for a variety of reasons. Instead, they must approximate what happens in mathematics. The most widely used approximation of real-number arithmetic used in computers is known as floating-point arithmetic.</p> <p>If the result of calculations depends heavily on the hardware implementation of the "floating-point calculator" used on the machine, we have a portability problem: moving a calculation that was programmed, say, on a desktop PC, to a high-performance graphics computer (without changing the input data) can change the result. The changes in the result can then affect the entire computation, say if the program makes <strong>decisions</strong> based on the numeric result. In extreme cases, a binary "yes/no" output can depend purely on non-mathematical artifacts, such as the machine, compiler flags, etc. This situation renders numeric programs usable only by computer and numerics experts, which excludes many scientists that depend on such programs for simulations.</p> <p>In this project we accomplished the following:</p> <ol> <li>We confirmed the frequency and severity of this problem, especially for heterogeneous platforms where hardware differences are significant. Platform dependencies are frequent, and so are situations where overall program behavior (rather than just a single numeric result) is affected by such dependencies. We experimented with numeric programs run on PCs and GPU computers, manufactured by diverse hardware companies such as Intel, NVidia, and AMD.</li> <li>We designed a procedure that can help (the many) non-numerics experts and non=floating-point experts detect the sensitivity of programs they plan to use to the above problem. The procedure will test whether, for a given input, the outcome depends unreasonably on the computational platform (meaning that the outcome variations due to platform differences can be large), and it can also find critical such inputs for the program.</li> <li>We designed a procedure that can assist users in "fixing" their programs. To fix such a program means to modify it such that the platform dependencies go away. This is not necessarily required for all kinds of computations. But whenever portability and reproducibility is a prime concern, it can be enforced by disabling extra features that the compiler and the platform may offer, which would render the computed result irreproducible. Those features of compilers and hardware are often offered in the name of performance: they are intended to speed up the computation. Therefore, disabling them across the entire program can have a non-trivial performance cost. Our procedure therefore identifies code segments in the program that contribute most significantly to the platform dependence of an output later in the code. The user can then disable those features only for the (short) code segment thus identified. Performance-enhancing improvements can still be applied to the rest of the code, without affecting&nbsp;reproducibility and portability of the code.</li> </ol> <p>We have published this work both in the form of scientific papers presented in leading international meetings, as well as in the form of tools that can be downloaded and used by others, academics and practitioners alike.</p> <ol> </ol><br> <p>            Last Modified: 08/27/2016<br>      Modified by: Thomas&nbsp;Wahl</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to investigate to what extent calculations on computers, i.e. the processing of purely numeric data such as in scientific computing, depends on the computational platform on which the calculation runs. The background is that, while mathematics defines unambiguously what the output of arithmetic operations such as + is , computers cannot implement mathematical arithmetic, for a variety of reasons. Instead, they must approximate what happens in mathematics. The most widely used approximation of real-number arithmetic used in computers is known as floating-point arithmetic.  If the result of calculations depends heavily on the hardware implementation of the "floating-point calculator" used on the machine, we have a portability problem: moving a calculation that was programmed, say, on a desktop PC, to a high-performance graphics computer (without changing the input data) can change the result. The changes in the result can then affect the entire computation, say if the program makes decisions based on the numeric result. In extreme cases, a binary "yes/no" output can depend purely on non-mathematical artifacts, such as the machine, compiler flags, etc. This situation renders numeric programs usable only by computer and numerics experts, which excludes many scientists that depend on such programs for simulations.  In this project we accomplished the following:  We confirmed the frequency and severity of this problem, especially for heterogeneous platforms where hardware differences are significant. Platform dependencies are frequent, and so are situations where overall program behavior (rather than just a single numeric result) is affected by such dependencies. We experimented with numeric programs run on PCs and GPU computers, manufactured by diverse hardware companies such as Intel, NVidia, and AMD. We designed a procedure that can help (the many) non-numerics experts and non=floating-point experts detect the sensitivity of programs they plan to use to the above problem. The procedure will test whether, for a given input, the outcome depends unreasonably on the computational platform (meaning that the outcome variations due to platform differences can be large), and it can also find critical such inputs for the program. We designed a procedure that can assist users in "fixing" their programs. To fix such a program means to modify it such that the platform dependencies go away. This is not necessarily required for all kinds of computations. But whenever portability and reproducibility is a prime concern, it can be enforced by disabling extra features that the compiler and the platform may offer, which would render the computed result irreproducible. Those features of compilers and hardware are often offered in the name of performance: they are intended to speed up the computation. Therefore, disabling them across the entire program can have a non-trivial performance cost. Our procedure therefore identifies code segments in the program that contribute most significantly to the platform dependence of an output later in the code. The user can then disable those features only for the (short) code segment thus identified. Performance-enhancing improvements can still be applied to the rest of the code, without affecting reproducibility and portability of the code.   We have published this work both in the form of scientific papers presented in leading international meetings, as well as in the form of tools that can be downloaded and used by others, academics and practitioners alike.         Last Modified: 08/27/2016       Submitted by: Thomas Wahl]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
