<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Numerical algebra and statistical inference</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>150001.00</AwardTotalIntnAmount>
<AwardAmount>150001</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The investigators have two aims in this proposal that fall at the interface of numerical algebra and statistical inference. The first aim is to extend the use of randomized approximation in a variety of dimension reduction methods that rely on numerical linear algebra both supervised and unsupervised as well as linear and nonlinear and develop a statistical bases for these methods in addition to the computational motivation of being applicable to massive data. The other motivation is to extend these statistical methods for dimension reduction to multiway data using numerical multilinear algebra, a recent new development in numerical analysis. These projects will increase interaction between statistical inference and numerical analysis and benefit both fields, providing new perspectives to how we view and perform data analysis.&lt;br/&gt;&lt;br/&gt;Numerical methods with statistical implications are central to a variety of technologies used by the general population. These technologies include Google's pagerank algorithm, genetic methods used to find genetic variation related to disease, compressing of medical images for storage and treatment, as well as applications in geostatistics. In all the previous cases the fundamental idea is to condense massive data in a useful summary with respect to a desired goal. The two ideas in this proposal are (1) to study how numerical methods that scale to the massive data generated in modern scientific, engineering, and social applications impose statistical assumptions or models on the data, (2) to study more complex interactions or properties of the data than examined in current methods. The motivation behind the first aim is to understand how numerical approximations required for computational scaling as we collect more data impact the information that can be extracted from these data -- for what type of data and applications do certain numerical approximations work well. The motivation behind the second aim is to go beyond the broad category of standard statistical methods take into account the relation between pairs of objects -- two web pages that are linked for Google's pagerank, the correlation between two genes or two loci in genetics applications. The question behind this aim is whether richer sources of information can be extracted by examining the links between three web pages or three loci. The research involved in this aim consists of the development of computationally efficient algebraic methods to extract this information and understanding the statistical models implemented by these methods.</AbstractNarration>
<MinAmdLetterDate>06/13/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/08/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1209155</AwardID>
<Investigator>
<FirstName>Sayan</FirstName>
<LastName>Mukherjee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sayan Mukherjee</PI_FULL_NAME>
<EmailAddress>sayan@stat.duke.edu</EmailAddress>
<PI_PHON>9196684747</PI_PHON>
<NSF_ID>000169747</NSF_ID>
<StartDate>06/13/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277080001</ZipCode>
<StreetAddress><![CDATA[PO Box 90251]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~80820</FUND_OBLG>
<FUND_OBLG>2013~62450</FUND_OBLG>
<FUND_OBLG>2014~6731</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span class="s1">In the current era of information, large amounts of complex&nbsp; high dimensional data are</span>&nbsp;routinely generated across science&nbsp;<span class="s1">and engineering. Understanding the underlying structure of these data and using this s</span>tructure to model scientific problems and make inference is a a fundamental problem in&nbsp; a variety of applications in statistics, applied mathematics, computer science, and the natural and social sciences. As the size of data sets increase the problem of statistical inference and the problem of computational feasibility become inextricably linked. Typically, statisticians have developed procedures and theory for estimators, for example the subspace that captures the maximal variation in data drawn from a population. Numerical analysis or computational mathematics is then used to provide efficient algorithms with provable guarantees to compute these estimates. A classic example of this interplay is principal components analysis which is used in a variety of data analysis simulations and algorithms forimplementing singular value decomposition, the standard way of computing principal components.</p> <p class="p1"><span class="s1">The investigators had two&nbsp; aims in this proposal that fall at the interface of numerical algebra an</span>d statistical inference. The first aim was to extend the use of randomized approximation in a variety of dimension reduction methods -- both supervised and unsupervised as well as linear and nonlinear -- and develop a statistical bases for these methods in addition to the computational motivation of being applicable to massive data. The other motivation was to develop extend these statistical methods for dimension reduction to multiway data using multilinear numerical algebra. These projects will increase interaction between statistical inference and numerical analysis and&nbsp; benefit both fields, providing new perspectives to how we view and perform data analysis. &nbsp;</p> <p class="p1">Intellectual merit: T<span class="s1">he proposed work investigated several fundamental problems of data analysis at the cross-sectio</span>n of numerical algebra and statistical methods for dimension reduction. &nbsp;The proposed work introduced four methods for dimension reduction on massive &nbsp;data sets using approximate randomized algorithms, an idea developed in numerical analysis. A statistical interpretation of the randomization and its use in the methods will be developed.&nbsp;<span class="s1">Second, the proposeded work explored novel applications of the rich family of methods in multiline&nbsp;</span>ar algebra to a variety of dimension reduction methods.</p> <p class="p2">The investigation of these problems explored the synergy between the fields of numerical analysis and statistical inference. The intellectual merit of the research was in the innovative new data analysis methods resulting from the integration of methodologies and techniques from these two complimentary fields.&nbsp;</p> <p class="p1">The research from this proposal led to statistical methods for very fast dimension reduction which were applied to problems in quantitative genetics as well as statistical genetics. In quantitative genetics a paper as well as software for inference of the additive genetic covariance of high-dimensional traits was developed, this software is being used by animal and plan breeders. In statistical genetics, software for inference of population structure in massive data was developed.</p> <p class="p1">&nbsp;</p> <p class="p1"><span class="s1">Broader impact:&nbsp;</span>Data analysis is a fundamental problem in statistical and computational sciences. The proposed work can made a contribution to our understanding and manipulation of modern data.&nbsp;</p> <p class="p1"><span class="s1">The proposed work was multi-disciplinary in nature, and will involve fields includin...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the current era of information, large amounts of complex  high dimensional data are routinely generated across science and engineering. Understanding the underlying structure of these data and using this structure to model scientific problems and make inference is a a fundamental problem in  a variety of applications in statistics, applied mathematics, computer science, and the natural and social sciences. As the size of data sets increase the problem of statistical inference and the problem of computational feasibility become inextricably linked. Typically, statisticians have developed procedures and theory for estimators, for example the subspace that captures the maximal variation in data drawn from a population. Numerical analysis or computational mathematics is then used to provide efficient algorithms with provable guarantees to compute these estimates. A classic example of this interplay is principal components analysis which is used in a variety of data analysis simulations and algorithms forimplementing singular value decomposition, the standard way of computing principal components. The investigators had two  aims in this proposal that fall at the interface of numerical algebra and statistical inference. The first aim was to extend the use of randomized approximation in a variety of dimension reduction methods -- both supervised and unsupervised as well as linear and nonlinear -- and develop a statistical bases for these methods in addition to the computational motivation of being applicable to massive data. The other motivation was to develop extend these statistical methods for dimension reduction to multiway data using multilinear numerical algebra. These projects will increase interaction between statistical inference and numerical analysis and  benefit both fields, providing new perspectives to how we view and perform data analysis.   Intellectual merit: The proposed work investigated several fundamental problems of data analysis at the cross-section of numerical algebra and statistical methods for dimension reduction.  The proposed work introduced four methods for dimension reduction on massive  data sets using approximate randomized algorithms, an idea developed in numerical analysis. A statistical interpretation of the randomization and its use in the methods will be developed. Second, the proposeded work explored novel applications of the rich family of methods in multiline ar algebra to a variety of dimension reduction methods. The investigation of these problems explored the synergy between the fields of numerical analysis and statistical inference. The intellectual merit of the research was in the innovative new data analysis methods resulting from the integration of methodologies and techniques from these two complimentary fields.  The research from this proposal led to statistical methods for very fast dimension reduction which were applied to problems in quantitative genetics as well as statistical genetics. In quantitative genetics a paper as well as software for inference of the additive genetic covariance of high-dimensional traits was developed, this software is being used by animal and plan breeders. In statistical genetics, software for inference of population structure in massive data was developed.   Broader impact: Data analysis is a fundamental problem in statistical and computational sciences. The proposed work can made a contribution to our understanding and manipulation of modern data.  The proposed work was multi-disciplinary in nature, and will involve fields including numerical analysis,  randomized algorithms, and statistics. Applications to biomedical sciences as well as agriculture were develeoped.                   Last Modified: 10/24/2015       Submitted by: Sayan Mukherjee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
