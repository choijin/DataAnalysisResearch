<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Function of Auditory Feedback Processing During Speech</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>530000.00</AwardTotalIntnAmount>
<AwardAmount>530000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Uri Hasson</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The goal of speaking is to produce the right sounds that convey an intended message. Accordingly, speakers monitor their sound output and use this auditory feedback to further adjust their speech production. Drs. Nagarajan and Houde hypothesized that the brain not only generates the motor signals that control the speech production but also generates a prediction of what this speech should sound like and performs an on-going comparison during speaking in order to dynamically adjust speech production. Whole-brain magnetoencephalographic imaging (MEG-I) experiments will be performed to monitor subjects' auditory neural activity as they hear themselves speak. The first tests task and feature specificity of the feedback prediction. If the prediction encodes only task-relevant acoustic features (i.e., pitch for a singing task), then auditory cortical activity will depend only on the acoustic goal for that task. The second tests the importance of categorical identity in the process of comparing feedback with the prediction. If feedback is altered enough to change the meaning of a word (e.g., when /bad/ is altered to /dad/), this is expected to have a much larger impact on auditory cortical activity than non-categorical alterations. These experiments are expected to improve our understanding of how the brain uses auditory feedback to maintain accuracy in speech production.&lt;br/&gt;&lt;br/&gt;The proposed research activity also has broader impact.  First, these expected findings may further contribute to a better understanding of and effective treatments for speech dysfunctions.  For example, accurate models of brain networks used to control speaking form the basis for testable hypotheses about neural origins of speech disorders such as stuttering or spasmodic dysphonia. Second, the research project will provide a special opportunity to train and educate graduate students and postdoctoral fellows in the use of real-time speech alteration and MEG-I techniques which are only available at very few US institutions.  Outreach with collaborators at San Francisco State University, and to the San Francisco Unified School district through the NSF funded Science Education Partnership (SEP) program will provide research experience to their students, specifically students from socioeconomically disadvantaged minorities who are under-represented in the sciences. The research team will also participate in the big data sharing effort by making the data and analysis tools available to support efforts to make use of real data in the teaching of STEM-related courses and to enable participation in discovery science by those who would otherwise have no access to such data.</AbstractNarration>
<MinAmdLetterDate>09/03/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1262297</AwardID>
<Investigator>
<FirstName>Srikantan</FirstName>
<LastName>Nagarajan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Srikantan Nagarajan</PI_FULL_NAME>
<EmailAddress>sri@ucsf.edu</EmailAddress>
<PI_PHON>4154764982</PI_PHON>
<NSF_ID>000350328</NSF_ID>
<StartDate>09/03/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Houde</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John F Houde</PI_FULL_NAME>
<EmailAddress>houde@phy.ucsf.edu</EmailAddress>
<PI_PHON>4152179332</PI_PHON>
<NSF_ID>000164988</NSF_ID>
<StartDate>09/03/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Chang</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward F Chang</PI_FULL_NAME>
<EmailAddress>changed@neurosurg.ucsf.edu</EmailAddress>
<PI_PHON>4154762977</PI_PHON>
<NSF_ID>000596881</NSF_ID>
<StartDate>09/03/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Francisco</Name>
<CityName>San Francisco</CityName>
<ZipCode>941034249</ZipCode>
<PhoneNumber>4154762977</PhoneNumber>
<StreetAddress>1855 Folsom St Ste 425</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>094878337</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, SAN FRANCISCO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Francisco]]></Name>
<CityName>San Francisco</CityName>
<StateCode>CA</StateCode>
<ZipCode>941034249</ZipCode>
<StreetAddress><![CDATA[1855 Folsom St Ste 425]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1699</Code>
<Text>Cognitive Neuroscience</Text>
</ProgramElement>
<ProgramReference>
<Code>1699</Code>
<Text>COGNEURO</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~530000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goals of the project were to further develop our novel state feedback control (SFC) model for speech production and to test key hypotheses this model using the functional brain imaging modality of magnetoencephalography (MEG). We have combined our model with another dominant model for articulatory control of speech called task-dynamics (TADA). Our quantitative model for speech motor control accounts for all of our behavioral and neural data to date. Based on our model, we hypothesized that the brain not only generates the motor signals that control the speech production but also generates a prediction of what this speech should sound like and performs an on-going comparison during speaking in order to dynamically adjust speech production.</p> <p>Our results showed that even in natural conditions speech has unpredictable variations (i.e., our speech output is never produced exactly as we intended it), and that this causes unpredictable variations in auditory feedback. Speakers are constantly correcting for these unpredictable deviations from their intended output, depending on the task. We have found evidence that responses in auditory cortices drive these online corrective compensation through a larger speech motor control network. These results are all explained with our novel quantitative model for speech production.</p> <p>The biggest impact of the project has been on the field of speech motor control research, in that key modeling work and experimental findings allow us to elaborate the state feedback model (SFC) model of speech motor control. The impact on the broader field of speech research of this work will relate how speech perceptual systems are used during control of speaking.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/06/2017<br>      Modified by: Srikantan&nbsp;Nagarajan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goals of the project were to further develop our novel state feedback control (SFC) model for speech production and to test key hypotheses this model using the functional brain imaging modality of magnetoencephalography (MEG). We have combined our model with another dominant model for articulatory control of speech called task-dynamics (TADA). Our quantitative model for speech motor control accounts for all of our behavioral and neural data to date. Based on our model, we hypothesized that the brain not only generates the motor signals that control the speech production but also generates a prediction of what this speech should sound like and performs an on-going comparison during speaking in order to dynamically adjust speech production.  Our results showed that even in natural conditions speech has unpredictable variations (i.e., our speech output is never produced exactly as we intended it), and that this causes unpredictable variations in auditory feedback. Speakers are constantly correcting for these unpredictable deviations from their intended output, depending on the task. We have found evidence that responses in auditory cortices drive these online corrective compensation through a larger speech motor control network. These results are all explained with our novel quantitative model for speech production.  The biggest impact of the project has been on the field of speech motor control research, in that key modeling work and experimental findings allow us to elaborate the state feedback model (SFC) model of speech motor control. The impact on the broader field of speech research of this work will relate how speech perceptual systems are used during control of speaking.           Last Modified: 10/06/2017       Submitted by: Srikantan Nagarajan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
