<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI-Small: Collaborative Research: Assistive Robotics for Grasping and Manipulation using Novel Brain Computer Interfaces</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>784998.00</AwardTotalIntnAmount>
<AwardAmount>800498</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Irina Dolinskaya</SignBlockName>
<PO_EMAI>idolinsk@nsf.gov</PO_EMAI>
<PO_PHON>7032927078</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This is a collaborative proposal (with UC Davis) which is aimed at making concrete some of the major goals of Assistive Robotics. A team of experts has been brought together from the fields of signal processing and control, robotic grasping, and rehabilitative medicine to create a field-deployable assistive robotic system that will allow severely disabled patients to control a robot arm/hand system to perform complex grasping and manipulation tasks using novel Brain Muscle Computer Interfaces (BMCI). Further, the intent of this effort is not just technology-driven, but is also driven by clear and necessary clinical needs, and will be evaluated on how well it meets these clinical requirements.  Validation will be performed at the Department of Regenerative and Rehabilitation Medicine at Columbia University on a diverse set of disabled users who will provide important feedback on the technology being developed, and this feedback will be used to iterate on the system design and implementation.&lt;br/&gt;&lt;br/&gt;Intellectual Merit: The intellectual merit of this proposal includes:&lt;br/&gt;o Novel research in Human Machine Interfaces that has the potential to be transformative in eliciting rich, multi-degree-of-freedom signal content from simple and non-invasive surface electromyographic (sEMG) sensors.&lt;br/&gt;o Development of smart adaptive software that employs machine learning algorithms that can continually monitor user performance, and then automatically calibrate and tune system parameters based on system performance.&lt;br/&gt;o Data driven methods for real-time grasp planning algorithms that can be used with both known and unknown objects.&lt;br/&gt;o Methods for finding pose-robust grasps that are tolerant of errors in sensing.&lt;br/&gt;o Evaluation of an underactuated hand as a grasping device for certain application tasks.&lt;br/&gt;o Integration of 3D vision with real-time grasp planning.&lt;br/&gt;o Scientific evaluation at the clinical level of the impact of these new technologies on the disabled population.&lt;br/&gt;&lt;br/&gt;Broader Impacts: The broader impacts of this proposal include:&lt;br/&gt;o Development of a complete system to aid the severely disabled population with tetraplegia.&lt;br/&gt;o Extensions of this technlogy to others lacking motor control function including multiple sclerosis, stroke, amyotrophic lateral sclerosis (ALS or Lou Gehrig disease), cerebral palsy, and muscular dystrophy.&lt;br/&gt;o New technology that can extend the reach and impact of the field of Assistive Robotics.&lt;br/&gt;o Major extensions to the open-source GraspIt! software system that will allow many other researchers to leverage the results of this project.&lt;br/&gt;o Educational thrusts that will bring together engineering students, clinicians and the disabled population to extend the reach and scope of Assistive Robotics.&lt;br/&gt;o New directions in Human Machine Interfaces that can extend beyond the disabled population and into a variety of other applications.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208153</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Allen</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter K Allen</PI_FULL_NAME>
<EmailAddress>allen@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397000</PI_PHON>
<NSF_ID>000183444</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joel</FirstName>
<LastName>Stein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joel Stein</PI_FULL_NAME>
<EmailAddress>js1165@columbia.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000605052</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~784998</FUND_OBLG>
<FUND_OBLG>2013~8000</FUND_OBLG>
<FUND_OBLG>2016~7500</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed new technology that can extend the reach and impact of the field of Assistive Robotic grasping.&nbsp; The new technologies developed include a novel sEMG sensor device, an augmented reality user interface (UI),&nbsp; a novel&nbsp; shared autonomy, Human-in-the Loop (HitL) paradigm, and new grasp planning algorithms and methods that make grasping more robust amid clutter and novel objects.&nbsp; This work forms the foundation for a flexible, human-in-the-loop system that allows users to grasp known and unknown objects in cluttered spaces using practical human-robot interaction paradigms that have the potential to bring HITL assistive devices out of research and into the lives of those that need them.</p> <p>Two prototype assistive robotic systems were developed to aid the disabled population with tetraplegia and other motor impairments (stroke).&nbsp; The first was an assistive robotic grasping system that allows motor impaired individuals to interact with the system in a HitL manner, including the use of a novel cranio-facial electromyography input device. &nbsp;The system uses an augmented reality interface that allows users to plan grasps online that match their intents. &nbsp;This system was tested on a severely impaired C3-C4&nbsp; spinal&nbsp; cord&nbsp; injury&nbsp; patient&nbsp; in California&nbsp; who successfully used the system to grasp a number of objects at Columbia.&nbsp; An improved version of this system was also used by 5 healthy individuals to pick up a variety of objects with different grasps from different directions.&nbsp;&nbsp;</p> <p>The second system used a non-invasive EEG interface with Rapid Serial Visual Presentation (RSVP) to grasp objects. &nbsp;The EEG interface allows online generation of a flexible number of options. This online planning framework allows the user to direct the planner towards grasps that reflect their intent for using the grasped object by successively selecting grasps that approach the desired approach direction of the hand. &nbsp;The EEG interface is used to recognize the user's preference among a set of options presented by the planner and the system requires almost no learning by the subject.&nbsp; Experimental results included trials with three subjects who were able to successfully use the system to grasp and pick up objects in a cluttered scene. Further experiments used five subjects who were able to grasp and pick up objects in cluttered scenes.&nbsp; It was also tested with two robotic arms demonstrating the portability of the system.</p> <p>These systems proved the viability of a HitL virtual interface for assistive grasping. &nbsp;Shared autonomy as demonstrated in these system is a viable way of using robots to perform complex tasks for the disabled and healthy populations.&nbsp; The feasibility of using other input devices with an assistive grasping system interface was also investigated, using able-bodied individuals to define a set of quantitative metrics that could be used to assess an assistive grasping system.&nbsp; Using these measurements, a generalized benchmark for evaluating the effectiveness of any arbitrary input device into a HitL grasping system was developed. &nbsp;These results provide insight into how interface devices perform for assistive grasping and highlight the potential of sEMG control.</p> <p>Occlusion can severely impair robotic grasping and the grasping system uses a new shape completion method to alleviate this. &nbsp;Shape completion is&nbsp; accomplished &nbsp;through&nbsp; the&nbsp; use&nbsp; of&nbsp; a&nbsp; 3D&nbsp; Convolutional&nbsp; Neural Network (CNN). The network is trained on an open source dataset of 3D model exemplars&nbsp; captured&nbsp; from varying&nbsp; viewpoints developed in this project.&nbsp; At runtime, a pointcloud&nbsp; captured from a single point of view is fed into the CNN, which fills in the occluded regions of the scene, allowing grasps to be planned.&nbsp; Shape completion is rapid because the computational costs are&nbsp; borne&nbsp; during&nbsp; offline&nbsp; training.&nbsp; The quality of completions was measured based on several factors. These include&nbsp; whether&nbsp; or&nbsp; not&nbsp; the&nbsp; object&nbsp; being&nbsp; completed&nbsp; existed&nbsp; in the&nbsp; training&nbsp; data&nbsp; and&nbsp; how&nbsp; many&nbsp; object&nbsp; models&nbsp; were&nbsp; used&nbsp; to train&nbsp; the&nbsp; network.&nbsp; Also measured was the&nbsp; ability&nbsp; of&nbsp; the&nbsp; network to&nbsp; generalize&nbsp; to&nbsp; novel&nbsp; objects. &nbsp;Finally, experimentation was&nbsp; done&nbsp; both&nbsp; in&nbsp; simulation&nbsp; and&nbsp; on&nbsp; actual&nbsp; robotic&nbsp; hardware&nbsp; to explore&nbsp; the&nbsp; relationship&nbsp; between&nbsp; completion&nbsp; quality&nbsp; and&nbsp; the utility&nbsp; of&nbsp; the&nbsp; completed&nbsp; mesh&nbsp; model&nbsp; for&nbsp; grasping.&nbsp; Results showed this completion method to outperform other methods.</p> <p>A final study addressed the issue that there currently exist no practical tools to identify functional movements in the upper extremities (UEs) of stroke patients. This absence has limited the precise therapeutic dosing of patients recovering from stroke.&nbsp; This proof-of-principle study was aimed at developing an accurate approach for classifying UE functional movement primitives, which comprise functional movements. Data were generated from inertial measurement units (IMUs) placed on upper body segments of older healthy individuals and chronic stroke patients. Subjects performed activities commonly trained during rehabilitation after stroke. In healthy control and stroke participants, this approach identified functional movement primitives embedded in training activities with, on average, 80% precision. This approach may support functional movement dosing in stroke rehabilitation</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/23/2019<br>      Modified by: Peter&nbsp;K&nbsp;Allen</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1208153/1208153_10211765_1548257258284_2016_NRI_Poster32x48--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1208153/1208153_10211765_1548257258284_2016_NRI_Poster32x48--rgov-800width.jpg" title="Assistive Robotics for Grasping and Manipulation using Novel Brain-Computer Interfaces"><img src="/por/images/Reports/POR/2019/1208153/1208153_10211765_1548257258284_2016_NRI_Poster32x48--rgov-66x44.jpg" alt="Assistive Robotics for Grasping and Manipulation using Novel Brain-Computer Interfaces"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Assistive Robotic grasping systems: 1) uses a novel sEMG sensor that has been tested on both healthy and disabled individuals and 2) uses non-invasive EEG signals, and has been tested on healthy individuals.  Both connect to an online robotic grasping system also developed under this grant.</div> <div class="imageCredit">Peter Allen Columbia University</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Assistive Robotics for Grasping and Manipulation using Novel Brain-Computer Interfaces</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1208153/1208153_10211765_1546446513082_sensorpic--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1208153/1208153_10211765_1546446513082_sensorpic--rgov-800width.jpg" title="Capture, Learning, and Classification of Upper Extremity Movement Primitives"><img src="/por/images/Reports/POR/2019/1208153/1208153_10211765_1546446513082_sensorpic--rgov-66x44.jpg" alt="Capture, Learning, and Classification of Upper Extremity Movement Primitives"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Instrumented subject during feeding task.  In healthy control and stroke participants, functional movement primitives embedded in training activities were identified with, on average, 80% precision. This approach may support functional movement dosing in stroke rehabilitation</div> <div class="imageCredit">Peter Allen</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Capture, Learning, and Classification of Upper Extremity Movement Primitives</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1208153/1208153_10211765_1546444111551_CopyofShape_Completion_Columbia_Robotics_Demo_Day--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1208153/1208153_10211765_1546444111551_CopyofShape_Completion_Columbia_Robotics_Demo_Day--rgov-800width.jpg" title="Shape Completion Enabled Robotic Grasping"><img src="/por/images/Reports/POR/2019/1208153/1208153_10211765_1546444111551_CopyofShape_Completion_Columbia_Robotics_Demo_Day--rgov-66x44.jpg" alt="Shape Completion Enabled Robotic Grasping"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Online Grasp Planning using Shape completion uses a  3D  CNN. The network is trained on a dataset  of 3D  exemplars  captured  from varying  viewpoints.  At  runtime  a pointcloud  from a single view is fed into the CNN which fills in the occluded regions of the scene allowing grasps to be planned</div> <div class="imageCredit">Columbia Robotics Laboraotry</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Shape Completion Enabled Robotic Grasping</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed new technology that can extend the reach and impact of the field of Assistive Robotic grasping.  The new technologies developed include a novel sEMG sensor device, an augmented reality user interface (UI),  a novel  shared autonomy, Human-in-the Loop (HitL) paradigm, and new grasp planning algorithms and methods that make grasping more robust amid clutter and novel objects.  This work forms the foundation for a flexible, human-in-the-loop system that allows users to grasp known and unknown objects in cluttered spaces using practical human-robot interaction paradigms that have the potential to bring HITL assistive devices out of research and into the lives of those that need them.  Two prototype assistive robotic systems were developed to aid the disabled population with tetraplegia and other motor impairments (stroke).  The first was an assistive robotic grasping system that allows motor impaired individuals to interact with the system in a HitL manner, including the use of a novel cranio-facial electromyography input device.  The system uses an augmented reality interface that allows users to plan grasps online that match their intents.  This system was tested on a severely impaired C3-C4  spinal  cord  injury  patient  in California  who successfully used the system to grasp a number of objects at Columbia.  An improved version of this system was also used by 5 healthy individuals to pick up a variety of objects with different grasps from different directions.    The second system used a non-invasive EEG interface with Rapid Serial Visual Presentation (RSVP) to grasp objects.  The EEG interface allows online generation of a flexible number of options. This online planning framework allows the user to direct the planner towards grasps that reflect their intent for using the grasped object by successively selecting grasps that approach the desired approach direction of the hand.  The EEG interface is used to recognize the user's preference among a set of options presented by the planner and the system requires almost no learning by the subject.  Experimental results included trials with three subjects who were able to successfully use the system to grasp and pick up objects in a cluttered scene. Further experiments used five subjects who were able to grasp and pick up objects in cluttered scenes.  It was also tested with two robotic arms demonstrating the portability of the system.  These systems proved the viability of a HitL virtual interface for assistive grasping.  Shared autonomy as demonstrated in these system is a viable way of using robots to perform complex tasks for the disabled and healthy populations.  The feasibility of using other input devices with an assistive grasping system interface was also investigated, using able-bodied individuals to define a set of quantitative metrics that could be used to assess an assistive grasping system.  Using these measurements, a generalized benchmark for evaluating the effectiveness of any arbitrary input device into a HitL grasping system was developed.  These results provide insight into how interface devices perform for assistive grasping and highlight the potential of sEMG control.  Occlusion can severely impair robotic grasping and the grasping system uses a new shape completion method to alleviate this.  Shape completion is  accomplished  through  the  use  of  a  3D  Convolutional  Neural Network (CNN). The network is trained on an open source dataset of 3D model exemplars  captured  from varying  viewpoints developed in this project.  At runtime, a pointcloud  captured from a single point of view is fed into the CNN, which fills in the occluded regions of the scene, allowing grasps to be planned.  Shape completion is rapid because the computational costs are  borne  during  offline  training.  The quality of completions was measured based on several factors. These include  whether  or  not  the  object  being  completed  existed  in the  training  data  and  how  many  object  models  were  used  to train  the  network.  Also measured was the  ability  of  the  network to  generalize  to  novel  objects.  Finally, experimentation was  done  both  in  simulation  and  on  actual  robotic  hardware  to explore  the  relationship  between  completion  quality  and  the utility  of  the  completed  mesh  model  for  grasping.  Results showed this completion method to outperform other methods.  A final study addressed the issue that there currently exist no practical tools to identify functional movements in the upper extremities (UEs) of stroke patients. This absence has limited the precise therapeutic dosing of patients recovering from stroke.  This proof-of-principle study was aimed at developing an accurate approach for classifying UE functional movement primitives, which comprise functional movements. Data were generated from inertial measurement units (IMUs) placed on upper body segments of older healthy individuals and chronic stroke patients. Subjects performed activities commonly trained during rehabilitation after stroke. In healthy control and stroke participants, this approach identified functional movement primitives embedded in training activities with, on average, 80% precision. This approach may support functional movement dosing in stroke rehabilitation             Last Modified: 01/23/2019       Submitted by: Peter K Allen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
