<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: A Hierarchical Approach to Unsupervised Feature Discovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Contrary to some depictions in popular media, humans are still far better than any computer program at understanding the visual world around them. If we understood how the visual system does this, perhaps better artificial vision systems could be built. The goal of this project is to understand how the brain represents the visual world and why. Following a mantra famously credited to Richard Feynman -- What I cannot create, I do not understand -- this project's approach is to create a computer system that learns from natural input (images, videos), assuming that the visual system operates with the goal of efficiently representing the world. These representations will then be compared to measurements of visual neurons. The long term goal is to understand the functional roles of the early visual processing layers in the human visual pathway.&lt;br/&gt;&lt;br/&gt;The model is based on the efficient coding hypothesis, in which the early visual pathway serves to capture the statistical structure of its visual inputs by efficiently coding visual information in its outputs. Most computational models following this hypothesis have focused on modeling only one or two visual layers. In this project, Cottrell's group proposes a hierarchical information processing model, which concurs with the efficient coding hypothesis, yet provides the most complete description so far of the early visual processing layers. In this model, the visual inputs are first compressed to reduce noise using Sparse Principal Components Analysis (SPCA), then the data dimensions are expanded to capture the statistical structure of the visual inputs using overcomplete Sparse Coding. A nonlinear activation function then formats the outputs of this layer for the next layer up, and the whole process is repeated. Preliminary work shows that the resulting hierarchical model can learn visual features exhibiting the receptive field properties of neurons in the early visual pathway, including retinal ganglion cells, LGN, V1 simple and complex cells, and V2 cells.</AbstractNarration>
<MinAmdLetterDate>09/13/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/13/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1219252</AwardID>
<Investigator>
<FirstName>Garrison</FirstName>
<LastName>Cottrell</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Garrison W Cottrell</PI_FULL_NAME>
<EmailAddress>gary@cs.ucsd.edu</EmailAddress>
<PI_PHON>8585346640</PI_PHON>
<NSF_ID>000314090</NSF_ID>
<StartDate>09/13/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930404</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Dr.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>It is well-known that children learn to talk. What is less appreciated is that they also learn to see. Babies see with about 20-200 visual acuity; that is, they are legally blind. Over the course of development, their visual acuity improves and their ability to discriminate visual inputs, for example, the faces of their parents and others, objects, scenes, and eventually, words. This work investigated hypotheses concerning how the visual system develops through self-organization, how we learn to discriminate facial expressions, how we learn to discriminate faces from one another, how we learn to read, and how we learn to discriminate objects and scenes.&nbsp;</p> <p>To accomplish this, we developed neural network models that learn to perform the same tasks people do. These models are brain-like in that they use components that are like brain cells in that they are highly connected to one another, and spread excitation and inhibition through connections in a manner inspired by brain cells, or neurons. We developed algorithms for learning in these systems that can explain both retinal representations as well as auditory representations. We expanded these algorithms to not just explain how our retinal representations may have developed, but also how early visual processing in the cortex could have developed merely by being exposed to the visual world.&nbsp;</p> <p>We further created neural network models that could learn to read the 850 words of basic English, that could distinguish the six basic emotional expressions (happy, sad, afraid, surprised, angry and disgusted), that could distinguish visual scenes from one another, and that could recognize objects in 1000 different categories. We showed how experience and innate abilities could combine to explain individual differences in people's abilities to discriminate faces and other visual objects that are highly similar to one another, such as birds, dogs, and cars. We are not alone in these endeavors - many researchers funded by NSF, are focused upon trying to solve these visual tasks so that we can develop self-driving cars, understand how our kids learn to see, and how to develop robots that see the world much as we do.&nbsp;</p><br> <p>            Last Modified: 01/16/2017<br>      Modified by: Garrison&nbsp;W&nbsp;Cottrell</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ It is well-known that children learn to talk. What is less appreciated is that they also learn to see. Babies see with about 20-200 visual acuity; that is, they are legally blind. Over the course of development, their visual acuity improves and their ability to discriminate visual inputs, for example, the faces of their parents and others, objects, scenes, and eventually, words. This work investigated hypotheses concerning how the visual system develops through self-organization, how we learn to discriminate facial expressions, how we learn to discriminate faces from one another, how we learn to read, and how we learn to discriminate objects and scenes.   To accomplish this, we developed neural network models that learn to perform the same tasks people do. These models are brain-like in that they use components that are like brain cells in that they are highly connected to one another, and spread excitation and inhibition through connections in a manner inspired by brain cells, or neurons. We developed algorithms for learning in these systems that can explain both retinal representations as well as auditory representations. We expanded these algorithms to not just explain how our retinal representations may have developed, but also how early visual processing in the cortex could have developed merely by being exposed to the visual world.   We further created neural network models that could learn to read the 850 words of basic English, that could distinguish the six basic emotional expressions (happy, sad, afraid, surprised, angry and disgusted), that could distinguish visual scenes from one another, and that could recognize objects in 1000 different categories. We showed how experience and innate abilities could combine to explain individual differences in people's abilities to discriminate faces and other visual objects that are highly similar to one another, such as birds, dogs, and cars. We are not alone in these endeavors - many researchers funded by NSF, are focused upon trying to solve these visual tasks so that we can develop self-driving cars, understand how our kids learn to see, and how to develop robots that see the world much as we do.        Last Modified: 01/16/2017       Submitted by: Garrison W Cottrell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
