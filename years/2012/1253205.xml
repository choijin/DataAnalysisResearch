<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Information Theory Beyond Capacity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>595238.00</AwardTotalIntnAmount>
<AwardAmount>625238</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Unprecedented technological progress in the last decades makes information theory an ever more exciting and important discipline. The modern world is swarming with information streams pervading the radio, wires, fiber optic cables, and on-chip networks. Yet we are unable to answer the most basic questions such as the impact of delay on the capacity of multiple-antenna wireless channels, or the fundamental principles of protecting computation networks from local process variation in silicon chip fabrication.  As such, the main purpose of this project is to advance the state-of-the-art in the fundamental limits of delay-constrained wireless communication. Computation of the impact of delay constraint in wireless communication will allow assessments of the degree of suboptimality of currently employed systems and industry standards, and likewise shed light on novel and higher-performing wireless systems.&lt;br/&gt;&lt;br/&gt;The progress on non-asymptotic information theory is inseparable from understanding of non-Shannon information measures and their data-processing properties on general (non-linear) graphs.  The progress on this topic is expected to provide the theoretical tools required for exploration of complex information processing systems including non-communication ones, such as fault-tolerant chips and noise-resistant circuits.  Advanced converse techniques and graph-based data-processing will open information theory to new fields and is expected to reinvigorate the progress in the converse bounds for multi-terminal (network) problems.&lt;br/&gt;&lt;br/&gt;The curriculum will be broadly disseminated through online resources, OpenCourseWare and MITx/edX.  The analysis of real-world communication systems also presents a rich field for undergraduate research opportunities (UROPs). Popularizing finite blocklength results is likewise expected to have industrial impact, especially in areas related to wireless and time-critical communication. The compiled performance charts and delay-constrained analysis will guide the design of next-generation mobile standards and help in fair assessment of intellectual property.</AbstractNarration>
<MinAmdLetterDate>12/19/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/05/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1253205</AwardID>
<Investigator>
<FirstName>Yury</FirstName>
<LastName>Polyanskiy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yury Polyanskiy</PI_FULL_NAME>
<EmailAddress>yp@mit.edu</EmailAddress>
<PI_PHON>6172531000</PI_PHON>
<NSF_ID>000623445</NSF_ID>
<StartDate>12/19/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394307</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7935</Code>
<Text>COMM &amp; INFORMATION THEORY</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7363</Code>
<Text>RES IN NETWORKING TECH &amp; SYS</Text>
</ProgramReference>
<ProgramReference>
<Code>7556</Code>
<Text>CONFERENCE AND WORKSHOPS</Text>
</ProgramReference>
<ProgramReference>
<Code>7935</Code>
<Text>COMM &amp; INFORMATION THEORY</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~225373</FUND_OBLG>
<FUND_OBLG>2015~242147</FUND_OBLG>
<FUND_OBLG>2017~157718</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Classical theory of information transmission, as developed by Nyquist, Wiener and, of course, Claude Shannon layed out thefoundation of the digital world. This foundation supports all the wired, optical and radio networks that are currentlyin operation. In the last decade, however, the demands on these networks has shifted towards more stringent real-timeresponsiveness (lower latency) and energy efficiency. This was in part due to mobile devices (and interactive workloads)acquiring a much larger fraction of the total digital traffic.<br />The classical theory, however, primarily focused on the question of capacity of data links. The main highlight of it,&nbsp;Shannon's channel coding theorem, established a mathematical formula for the highest possible communication rate achievable across agiven noisy medium. In particular, when engineers build new systems they are able to benchmark the actual speed of theirsystem against the fundamental Shannon limit. Achieving the Shannon limit, however, requires sacrificing two otherimportant qualities. First, instead of transmitting newly arrived data instanteneously, the Shannon-optimal system hasto wait for a very long time until a very large number of new data bits is available. Only then it can transmit all thislarge block in the next (also very long) transmission session. Clearly, such an approach is not feasible when new data needs tobe delivered immediately (for example, if it is part of a real-time control mechanism, or an interactive conversation). Second,when a very long block is received, to clean it up from the corruption caused by the noise, is computationallyexpensive (in proportion to its length, or even some power of its length, depending on the error-correcting algorithmused).<br />Most systems designed in practice, thus are not Shannon-optimal and hence benchmarking them against Shannon's limit isnot completely informative. So a new theory, suited for the modern \textit{finite blocklength} demands was needed. The workunder this proposal has built some parts of this new theory (and many researchers, in the USA and around the world,have build other parts).<br />The work spanned several different directions: the data compression, the channel coding, the energy efficiency and manymore. One of the principal achievements is in establishing dependable bounds and approximations for how differentreal-world constraints can impact the fundamental information-theoretic limits. In data compression, we introduced theconcept of variable-length codecs which allow small error. We also characterized (alas, only as the order of magnitude)the amount of backoff from the Shannon limit that is required by any system that is delivering its data with a certain(non-zero) distortion.&nbsp;<br />Most of the work, however, focused on channel coding. In this area, we have established guidelines for building&nbsp;asynchronous communication systems subject to delay constraints. An all-around study of themultiple-antenna channels was undertaken. We investigated the influence of multi-path fading, the dynamics of the fadingprocess, presence of the channel state information. A special attention was paid to questions of powercontrol under the low-latency constraints. In all of these instances, numerical validation demonstrates&nbsp;that our theoretical predictions have precision suitable for the use of the practicing system designer.All of the code needed for benchmarking these diverse systems, has been put into aunified and well-documented online repository, openly available for engineers to use.&nbsp;<br />Finally, a second part of the work on this proposal concerned a more abstract notions of core information theory. Wehave developed methods for working with non-Shannon information measures, studied their dissipation properties.Particularly, we provided tools for analyzing complex information-processing networks. Applications of these tools,which are now actively explored, have already established new results in combinatorial statistics and machine learning.</p><br> <p>            Last Modified: 03/23/2020<br>      Modified by: Yury&nbsp;Polyanskiy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Classical theory of information transmission, as developed by Nyquist, Wiener and, of course, Claude Shannon layed out thefoundation of the digital world. This foundation supports all the wired, optical and radio networks that are currentlyin operation. In the last decade, however, the demands on these networks has shifted towards more stringent real-timeresponsiveness (lower latency) and energy efficiency. This was in part due to mobile devices (and interactive workloads)acquiring a much larger fraction of the total digital traffic. The classical theory, however, primarily focused on the question of capacity of data links. The main highlight of it, Shannon's channel coding theorem, established a mathematical formula for the highest possible communication rate achievable across agiven noisy medium. In particular, when engineers build new systems they are able to benchmark the actual speed of theirsystem against the fundamental Shannon limit. Achieving the Shannon limit, however, requires sacrificing two otherimportant qualities. First, instead of transmitting newly arrived data instanteneously, the Shannon-optimal system hasto wait for a very long time until a very large number of new data bits is available. Only then it can transmit all thislarge block in the next (also very long) transmission session. Clearly, such an approach is not feasible when new data needs tobe delivered immediately (for example, if it is part of a real-time control mechanism, or an interactive conversation). Second,when a very long block is received, to clean it up from the corruption caused by the noise, is computationallyexpensive (in proportion to its length, or even some power of its length, depending on the error-correcting algorithmused). Most systems designed in practice, thus are not Shannon-optimal and hence benchmarking them against Shannon's limit isnot completely informative. So a new theory, suited for the modern \textit{finite blocklength} demands was needed. The workunder this proposal has built some parts of this new theory (and many researchers, in the USA and around the world,have build other parts). The work spanned several different directions: the data compression, the channel coding, the energy efficiency and manymore. One of the principal achievements is in establishing dependable bounds and approximations for how differentreal-world constraints can impact the fundamental information-theoretic limits. In data compression, we introduced theconcept of variable-length codecs which allow small error. We also characterized (alas, only as the order of magnitude)the amount of backoff from the Shannon limit that is required by any system that is delivering its data with a certain(non-zero) distortion.  Most of the work, however, focused on channel coding. In this area, we have established guidelines for building asynchronous communication systems subject to delay constraints. An all-around study of themultiple-antenna channels was undertaken. We investigated the influence of multi-path fading, the dynamics of the fadingprocess, presence of the channel state information. A special attention was paid to questions of powercontrol under the low-latency constraints. In all of these instances, numerical validation demonstrates that our theoretical predictions have precision suitable for the use of the practicing system designer.All of the code needed for benchmarking these diverse systems, has been put into aunified and well-documented online repository, openly available for engineers to use.  Finally, a second part of the work on this proposal concerned a more abstract notions of core information theory. Wehave developed methods for working with non-Shannon information measures, studied their dissipation properties.Particularly, we provided tools for analyzing complex information-processing networks. Applications of these tools,which are now actively explored, have already established new results in combinatorial statistics and machine learning.       Last Modified: 03/23/2020       Submitted by: Yury Polyanskiy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
