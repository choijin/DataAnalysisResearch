<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CI-P: REFT - A Reconfigurable Execution Framework Testbed for data-driven and extreme scale computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2012</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>98820.00</AwardTotalIntnAmount>
<AwardAmount>98820</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The field of supercomputing is experiencing a rapid change in system structure, programming models, and software environments in response to advances in application requirements and in underlying enabling technologies. Traditional parallel programming approaches have relied on static resource allocation and task scheduling through programming interfaces such as MPI and OpenMP. These methods are reaching their efficiency and scalability limits on the new emerging classes of systems, spurring the creation of innovative dynamic strategies and software tools, including advanced runtime system software and programming interfaces that use them. To accelerate adoption of these next-generation methods, a unique environment is being created and operated that provides a comprehensive ensemble of state-of-the-art runtime system software and programming interfaces. Taken from previous research and development projects, some at the host institution, Indiana University, and others from premiere research organizations across the nation, these execution systems are integrated in a single supported Reconfigurable Execution Framework Testbed (REFT) and made available to parallel application algorithm developers as well as researchers in advanced tools for parallel computing. The basic REFT hardware capabilities include a medium- scale heterogeneous Linux cluster with multi-core sockets, high-bandwidth interconnect, and mass storage; field-programmable gate arrays; and instrumentation for power measurement. ParalleX-based HPX-3, ETI SWARM, Berkeley GasNet, Rice University?s Habanero, Illinois? Charm++, Cray Chapel, IBM X-10, and UPC among other programming and execution models comprise the major components of this unique facility.&lt;br/&gt;&lt;br/&gt;Supercomputing is making a sharp corner turn in form, function, and methodologies.  Unfortunately, few in the field are skilled in the use of the emerging execution and programming models that are becoming increasingly critical to effectively utilizing supercomputers to deliver quality science for extreme-scale applications?either those at the highest end of the performance spectrum (Petaflops currently and Exaflops at the end of the decade) or strong-scaled fixed-size problems. REFT serves the NSF computational science community by dramatically lowering the barrier to training, experimentation, and adoption of new dynamic execution methods and systems. It provides full documentation, on-line tutorials, in-house classes, and workshops for skill development and community building for the broad US HPC community to accelerate application, evaluation, and exploitation. As a repository for competing and complementary software environments it provides a single site for conducting comparative studies by end-users to establish best practices. As an NSF resource, it serves to expedite and further goals of computational science by enabling effective application of the next generation Petaflops-class computer systems of millions of cores and eventual Exascale systems with billion-way concurrency.</AbstractNarration>
<MinAmdLetterDate>05/30/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/30/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1205518</AwardID>
<Investigator>
<FirstName>Craig</FirstName>
<LastName>Stewart</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Craig A Stewart</PI_FULL_NAME>
<EmailAddress>stewart@iu.edu</EmailAddress>
<PI_PHON>8128554240</PI_PHON>
<NSF_ID>000188281</NSF_ID>
<StartDate>05/30/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Lumsdaine</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew Lumsdaine</PI_FULL_NAME>
<EmailAddress>al75@uw.edu</EmailAddress>
<PI_PHON>2065431695</PI_PHON>
<NSF_ID>000420340</NSF_ID>
<StartDate>05/30/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Sterling</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Sterling</PI_FULL_NAME>
<EmailAddress>tron@indiana.edu</EmailAddress>
<PI_PHON>8128564597</PI_PHON>
<NSF_ID>000119080</NSF_ID>
<StartDate>05/30/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Maciej</FirstName>
<LastName>Brodowicz</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maciej Brodowicz</PI_FULL_NAME>
<EmailAddress>simultac@gmail.com</EmailAddress>
<PI_PHON>8122878110</PI_PHON>
<NSF_ID>000492001</NSF_ID>
<StartDate>05/30/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Link</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Link</PI_FULL_NAME>
<EmailAddress>mrlink@iu.edu</EmailAddress>
<PI_PHON>8128556339</PI_PHON>
<NSF_ID>000563705</NSF_ID>
<StartDate>05/30/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName/>
<StateCode>IN</StateCode>
<ZipCode>474021847</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~98820</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>With the emergence of data-centric scientific exploration, there is a need to revisit architectural decisions that were made in support of compute-centric scientific applications. Unfortunately, it is very expensive to develop wholly custom computer architectures and even a significant capital expense for investigators to use FPGA to test different hardware approaches. The purpose of this planning grant was to assess the specific needs of the community to inform the design of a community infrastructure in support of reconfigurable computing for data-centric scientific explorations.</p> <p>While these reconfigurable approaches hold significant promise, it is very clear that they requrie non-trivial investment in terms of effort. There are open questions as to the appropriate levels of abstractions for reconfiguration and sharing in the sense that users of custom hardware should not have to become experts in, eg, FPGA programming.</p> <p>Experts in graph algorithms are not likely to also be experts in FPGA programming. Similarly, experts in FPGA programming are not likely to be also experts in combinatorics. Only through working together can an effort like this one succeed. Community infrastructure such as that to be proposed can serve as a nexus to bring together groups of researchers with complementary expertise. Ideally, these exploratory will yield building blocks for future experimentation.</p><br> <p>            Last Modified: 08/19/2015<br>      Modified by: Thomas&nbsp;Sterling</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ With the emergence of data-centric scientific exploration, there is a need to revisit architectural decisions that were made in support of compute-centric scientific applications. Unfortunately, it is very expensive to develop wholly custom computer architectures and even a significant capital expense for investigators to use FPGA to test different hardware approaches. The purpose of this planning grant was to assess the specific needs of the community to inform the design of a community infrastructure in support of reconfigurable computing for data-centric scientific explorations.  While these reconfigurable approaches hold significant promise, it is very clear that they requrie non-trivial investment in terms of effort. There are open questions as to the appropriate levels of abstractions for reconfiguration and sharing in the sense that users of custom hardware should not have to become experts in, eg, FPGA programming.  Experts in graph algorithms are not likely to also be experts in FPGA programming. Similarly, experts in FPGA programming are not likely to be also experts in combinatorics. Only through working together can an effort like this one succeed. Community infrastructure such as that to be proposed can serve as a nexus to bring together groups of researchers with complementary expertise. Ideally, these exploratory will yield building blocks for future experimentation.       Last Modified: 08/19/2015       Submitted by: Thomas Sterling]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
