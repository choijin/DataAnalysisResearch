<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BCC-SBE: Seeing Speech: Building a Community</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>243272.00</AwardTotalIntnAmount>
<AwardAmount>255272</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Yellen</SignBlockName>
<PO_EMAI>jyellen@nsf.gov</PO_EMAI>
<PO_PHON>7032928759</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Human language integrates several complex systems. Sound is the most accessible of these, with measurable acoustic and articulatory properties, yet it is poorly understood because it involves multiple interacting modalities: lips, velum, tongue, and larynx all conspire to give each sound its unique properties. Understanding how sound works in language is important to basic linguistic and cognitive sciences as well as to applied research such as speech sciences and automatic speech recognition.&lt;br/&gt;&lt;br/&gt;Technological advances make collecting articulatory data relatively trivial, while technology for annotation and analysis lags behind.  In response, we are developing (i) a software suite for simultaneous extraction and automatic analysis of articulatory speech data (UltraPraat), integrated seamlessly with the premier software for acoustic analysis of speech (Praat); and (ii), a database coupling articulatory and acoustic speech data (UltraSpeech) to support development and evaluation of theories of acoustic and articulatory phonetics, based on an existing database such as TIMIT.&lt;br/&gt;&lt;br/&gt;The success of such a venture depends on both the quality of execution and on how well it is received by the community. The goal of this project is to determine the key properties for both software and database that, if developed, would be most beneficial to the community of users.  This project will create a prototype of the analytic software, develop the community of potential users, and refine software and database specifications to best meet the community's needs. The outcomes will be a working prototype, a community of researchers who understand its benefits, and a set of specifications for the infrastructure.&lt;br/&gt;&lt;br/&gt;This project brings together language researchers with a wide range of interests in human articulation. The discussions will ensure that the infrastructure developed to fill the current gap in articulatory data and analysis software will provide as much benefit as possible to all the related fields in technology and the sciences; they may also give rise to new research synergies. The prototype will be made available publicly, so others may begin using the results of this project for research, even before the full model is complete. UltraPraat and UltraSpeech have the potential for transforming research in all domains of oral tract articulation, such as the diversity of human language sounds, language acquisition and endangered languages, speech deficits, foreign language pronunciation, oral language for the profoundly deaf, speech recognition and synthesis software, and how musicians shape sounds while playing wind instruments.</AbstractNarration>
<MinAmdLetterDate>09/25/2012</MinAmdLetterDate>
<MaxAmdLetterDate>01/15/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1244687</AwardID>
<Investigator>
<FirstName>Diana</FirstName>
<LastName>Archangeli</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Diana B Archangeli</PI_FULL_NAME>
<EmailAddress>dba@email.arizona.edu</EmailAddress>
<PI_PHON>5207418100</PI_PHON>
<NSF_ID>000427375</NSF_ID>
<StartDate>09/25/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arizona</Name>
<CityName>Tucson</CityName>
<ZipCode>857194824</ZipCode>
<PhoneNumber>5206266000</PhoneNumber>
<StreetAddress>888 N Euclid Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>806345617</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARIZONA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072459266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arizona]]></Name>
<CityName>Tucson</CityName>
<StateCode>AZ</StateCode>
<ZipCode>857210001</ZipCode>
<StreetAddress><![CDATA[888 N Euclid]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8068</Code>
<Text>Data Infrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~243272</FUND_OBLG>
<FUND_OBLG>2013~12000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Ultrasound imaging can be used to make video images of the tongue&rsquo;s movements as a person speaks. These images are very useful for basic research in understanding how language sounds are articulated. Ultrasound imaging of the tongue&rsquo;s movements has practical applications as well, in understanding the nature of disordered speech, in speech therapy, in second language research and instruction, and in automated speech recognition.</p> <p>&nbsp;</p> <p>One challenge with ultrasound language data is that it produces a large number of images (roughly 30-120 per second of speech, depending on the hardware used), which creates different types of management problems. Keeping track of the relationships between the images frames, their corresponding time frames in the video and audio, the ID of the participant whose speech is recorded, tracing information, the word, segment and phonological environment to which the tongue image belongs, and other potential labels and meta-information becomes very difficult as the size of these collections increase. Having the data organized facilitates access to the information for further research, either within data collected for one project or comparing data across different projects.</p> <p>&nbsp;</p> <p>To address this challenge, we developed UATracker, an online tool which allows labs to share aspects of the data that they choose with a broader audience. UATracker has two main parts; a database, a single SQLite file with the information that the tool is responsible for organizing, and the main program, written in Python, a graphical user interface and the engine that intermediates between the database and the user interface. The database stores and relates image files, audio files, textgrid files, and trace files (data extracted from the image files). The user interface allows the user to easily search for and download specific image-trace-audio triplets for use in a research project.</p> <p>&nbsp;</p> <p>A second challenge is that ultrasound data can only be collected in labs that have the appropriate equipment, yet there are research applications (such as speech recognition applications) in labs that do not otherwise have need for the equipment or expertise to collect data. Making data more broadly available through a database will address this problem.</p> <p>&nbsp;</p> <p>To solve this problem, we collected ultrasound images from eight native speakers of American English. The stimuli were fifty different sentences taken from 5 of the Harvard Sentences lists (H6, H7, H8, H9, H10).&nbsp; Each sentence was repeated 6 times, resulting in each participant reading a total of 300 sentences. Textgrids were created for the audio and image file pairs, first to divide each image file containing 300 sentences into 300 separate image files, then to identify the sounds within each of the 300 sentence files.</p> <p>&nbsp;</p> <p>Project funding did not include funding for serving the data to the public; we are still trying to find a way to do this.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/12/2017<br>      Modified by: Diana&nbsp;B&nbsp;Archangeli</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Ultrasound imaging can be used to make video images of the tongue?s movements as a person speaks. These images are very useful for basic research in understanding how language sounds are articulated. Ultrasound imaging of the tongue?s movements has practical applications as well, in understanding the nature of disordered speech, in speech therapy, in second language research and instruction, and in automated speech recognition.     One challenge with ultrasound language data is that it produces a large number of images (roughly 30-120 per second of speech, depending on the hardware used), which creates different types of management problems. Keeping track of the relationships between the images frames, their corresponding time frames in the video and audio, the ID of the participant whose speech is recorded, tracing information, the word, segment and phonological environment to which the tongue image belongs, and other potential labels and meta-information becomes very difficult as the size of these collections increase. Having the data organized facilitates access to the information for further research, either within data collected for one project or comparing data across different projects.     To address this challenge, we developed UATracker, an online tool which allows labs to share aspects of the data that they choose with a broader audience. UATracker has two main parts; a database, a single SQLite file with the information that the tool is responsible for organizing, and the main program, written in Python, a graphical user interface and the engine that intermediates between the database and the user interface. The database stores and relates image files, audio files, textgrid files, and trace files (data extracted from the image files). The user interface allows the user to easily search for and download specific image-trace-audio triplets for use in a research project.     A second challenge is that ultrasound data can only be collected in labs that have the appropriate equipment, yet there are research applications (such as speech recognition applications) in labs that do not otherwise have need for the equipment or expertise to collect data. Making data more broadly available through a database will address this problem.     To solve this problem, we collected ultrasound images from eight native speakers of American English. The stimuli were fifty different sentences taken from 5 of the Harvard Sentences lists (H6, H7, H8, H9, H10).  Each sentence was repeated 6 times, resulting in each participant reading a total of 300 sentences. Textgrids were created for the audio and image file pairs, first to divide each image file containing 300 sentences into 300 separate image files, then to identify the sounds within each of the 300 sentence files.     Project funding did not include funding for serving the data to the public; we are still trying to find a way to do this.                   Last Modified: 04/12/2017       Submitted by: Diana B Archangeli]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
