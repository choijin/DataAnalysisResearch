<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Dense and Sparse Methods in High-Dimensional Data Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>159995.00</AwardTotalIntnAmount>
<AwardAmount>159995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many methods for high-dimensional data analysis begin with the assumption that the parameter of interest is, in some sense, sparse.  Furthermore, the performance of many of these methods depends on the sparsity of the underlying parameters.  However, statistical methods for checking sparsity assumptions and determining the implications of the absence or near-absence of sparsity are lacking.  The driving goal of this project is to develop practical statistical tools for identifying situations where the relevant parameters are in fact sparse, or where sparse methods for high-dimensional data analysis may be applied effectively.  Problems considered in this project will primarily be studied within the context of the linear model and the Gaussian location model.  Methods will be assessed by decision theoretic-like criteria (e.g. asymptotic minimaxity).  A null model based on dense (non-sparse) signals and dense estimation and prediction methods will be developed and thoroughly studied.  This will provide a rich framework for sparsity testing, where the aim is to identify settings in which sparse methods are likely to be successful.  Specific sparsity testing procedures will be proposed and analyzed.  &lt;br/&gt;&lt;br/&gt;High-dimensional data analysis is one of the most active areas of current statistical research.  Much of this research has been driven by technological advances that have enabled researchers to collect vast datasets with relative ease in a variety of scientific disciplines, including astrophysics, geological sciences, molecular biology, and genomics.  In high-dimensional datasets, many features are measured for each unit of observation (e.g. thousands of gene expression levels may be measured for each individual in a genomic study).  Sparsity plays a major role in much of the research on high-dimensional data analysis.  Broadly speaking, sparsity measures the degree to which a specified outcome may be described by relatively few features.  Sparse methods for high-dimensional data analysis attempt to leverage sparsity in the underlying dataset and have proven to be very effective in many applications, especially in engineering and signal processing.  On the other hand, the performance of sparse methods has been more mixed in other important applications where high-dimensional data are abundant, such as genomics.  In this project, the investigator will develop statistical methods for characterizing and identifying situations where sparse methods can be successfully applied.  This will be achieved by developing tools for determining the level of sparsity in high-dimensional datasets.  These methods, when applied to a given dataset, will help researchers determine the validity of subsequent statistical analyses and the potential benefits of using sparse methods for these analyses.  This research is likely to have significant implications for understanding reproducibility in high-dimensional data analysis and broad applications in the analysis of genomic data.  The methods developed during the course of this project will be utilized in collaborative work with highly experienced researchers in genomics.</AbstractNarration>
<MinAmdLetterDate>07/02/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/02/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208785</AwardID>
<Investigator>
<FirstName>Lee</FirstName>
<LastName>Dicker</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lee Dicker</PI_FULL_NAME>
<EmailAddress>ldicker@stat.rutgers.edu</EmailAddress>
<PI_PHON>8484457668</PI_PHON>
<NSF_ID>000576426</NSF_ID>
<StartDate>07/02/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName/>
<StateCode>NJ</StateCode>
<ZipCode>089018559</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~159995</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>High-dimensional data analysis is one of the most active areas of research in statistics, with broad applications &ndash; from the biological sciences, to business and industrial statistics, to geophysics.&nbsp; The outcomes from this project shed new light into important aspects of high-dimensional data analysis.&nbsp; Especially in challenging areas where standard statistical conditions (like consistency of parameter estimation) may not be valid, but it is still desirable and important to draw reliable conclusions from the data.&nbsp;</p> <p>&nbsp;Many existing results for the statistics of high-dimensional data analysis rely on &ldquo;sparsity&rdquo; assumptions, which essentially ensure that a given problem involving high-dimensional data has a simple solution.&nbsp; This project largely addressed problems where this optimistic viewpoint &ndash; the existence of a simple solution that can be derived from the available data &ndash; may not hold.&nbsp;</p> <p>&nbsp;The primary outcomes from this projects fall into (roughly) three categories.&nbsp; First, new results on a classical and widely used method for analyzing &ldquo;non-sparse&rdquo; problems (ridge regression) were derived.&nbsp; These results shed new light into the optimality of ridge regression and provide practical insights into when it may be most effectively employed.&nbsp; Second, new measures of sparsity were studied and methods for determining whether or not a given problem can be solved using &ldquo;sparse methods&rdquo; were proposed.&nbsp; Loosely speaking, this line of work tackled the problem of how to determine if a given high-dimensional data analysis problem is &ldquo;easy&rdquo; or &ldquo;hard.&rdquo; Finally, an alternative approach to determining the degree of difficulty of non-sparse problems &ndash; residual variance estimation &ndash; was investigated.&nbsp; During the course of the project, the PI and collaborators derived a broad range of theoretical results on residual variance estimation for high-dimensional data analysis.&nbsp; While most of results from this project were theoretical in nature, the outcomes related to residual variance estimation will likely have broad applications in modern genetics &ndash; specifically, in heritability estimation.&nbsp;</p> <p>To date, the project has resulted in 9 published or accepted publications and 2 publications under review or in revision. &nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/29/2016<br>      Modified by: Lee&nbsp;Dicker</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ High-dimensional data analysis is one of the most active areas of research in statistics, with broad applications &ndash; from the biological sciences, to business and industrial statistics, to geophysics.  The outcomes from this project shed new light into important aspects of high-dimensional data analysis.  Especially in challenging areas where standard statistical conditions (like consistency of parameter estimation) may not be valid, but it is still desirable and important to draw reliable conclusions from the data.    Many existing results for the statistics of high-dimensional data analysis rely on "sparsity" assumptions, which essentially ensure that a given problem involving high-dimensional data has a simple solution.  This project largely addressed problems where this optimistic viewpoint &ndash; the existence of a simple solution that can be derived from the available data &ndash; may not hold.    The primary outcomes from this projects fall into (roughly) three categories.  First, new results on a classical and widely used method for analyzing "non-sparse" problems (ridge regression) were derived.  These results shed new light into the optimality of ridge regression and provide practical insights into when it may be most effectively employed.  Second, new measures of sparsity were studied and methods for determining whether or not a given problem can be solved using "sparse methods" were proposed.  Loosely speaking, this line of work tackled the problem of how to determine if a given high-dimensional data analysis problem is "easy" or "hard." Finally, an alternative approach to determining the degree of difficulty of non-sparse problems &ndash; residual variance estimation &ndash; was investigated.  During the course of the project, the PI and collaborators derived a broad range of theoretical results on residual variance estimation for high-dimensional data analysis.  While most of results from this project were theoretical in nature, the outcomes related to residual variance estimation will likely have broad applications in modern genetics &ndash; specifically, in heritability estimation.   To date, the project has resulted in 9 published or accepted publications and 2 publications under review or in revision.            Last Modified: 10/29/2016       Submitted by: Lee Dicker]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
