<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Dexterous Manipulation Using Predictive Thin-Shell Modeling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>498894.00</AwardTotalIntnAmount>
<AwardAmount>506894</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Reid Simmons</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Grasping and manipulation of deformable objects presents a host of new research challenges that are much more demanding than for rigid objects. A particular challenge is to fully understand the physics of deformation and to model deformable objects in a way that can be used by real robotic systems in the presence of noise and uncertainty and with real-time constraints. This project will use offline simulation to predict states of deformable objects modeled as thin-shells (i.e. cloth, fabric, clothing) that can then be recognized by a robotic vision/grasping system to pick up and manipulate these objects. The Intellectual Merit includes a significant step forward in the synthesis of state-of-the-art numerical computation of plasto-elastica with a database-driven manipulation approach to allow robots to manipulate deformable objects.  This includes fabric simulation technology that provides the requisite level of accuracy at speeds amenable to online computation parallel to the robotic grasping task.  &lt;br/&gt;&lt;br/&gt;The Broader Impacts of this research include 1) creation of an open source, extensible, 3-D database of deformable objects for dissemination to the robotics and graphics communities, 2) extending the range of working environments for the emerging field of personal robotic assistants, and 3)  developing and educating a new class of scientists who bridge the fields of computer graphics, computational mechanics, and robotics.</AbstractNarration>
<MinAmdLetterDate>08/31/2012</MinAmdLetterDate>
<MaxAmdLetterDate>04/15/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217904</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Allen</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter K Allen</PI_FULL_NAME>
<EmailAddress>allen@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397000</PI_PHON>
<NSF_ID>000183444</NSF_ID>
<StartDate>08/31/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Eitan</FirstName>
<LastName>Grinspun</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eitan Grinspun</PI_FULL_NAME>
<EmailAddress>eitan@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397057</PI_PHON>
<NSF_ID>000320766</NSF_ID>
<StartDate>08/31/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~498894</FUND_OBLG>
<FUND_OBLG>2014~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Robotic manipulation of deformable objects is a challenging problem especially because of the complexity of the many different ways an object can deform. Searching within such a high dimensional state space makes it difficult to recognize, track, and manipulate deformable objects. In this project, we have developed a feed-forward, model-driven, predictive approach to address this challenge, using a pre-computed, simulated database of deformable object models. The models are detailed, robust, and simple to construct, and using a physics-based approach, can predict accurately, in simulation, the behavior of the objects, which can then be used in a real physical setting. This bridges the gap between the simulation world and the real physical world. The feed-forward, model-driven, predictive approach takes advantages of the simulation and generates a large number of instances for learning approaches, which not only alleviates the burden of the data collection, but also makes domain adaptation of the methods to other applications easier and faster.&nbsp; To validate this approach, we developed a comprehensive pipeline for manipulating clothing as in a typical laundry task, which includes garment pose detection, re-grasping, folding and ironing. &nbsp;In this task, mesh models of common deformable garments are simulated with the garments picked up in multiple different poses under gravity, and stored in an indexable database for fast and efficient retrieval. First, the database is used to estimate poses of garments in an arbitrary positions. A full featured 3D volumetric model of the garment is constructed in real-time and novel volumetric features are then used to obtain the most similar model in the database to predict the object category and pose. Second, the database can significantly benefit the manipulation of deformable objects via non-rigid registration, providing accurate correspondences between the reconstructed object model and the database models. Third, the accurate model simulation can also be used to predict re-grasping points, and to optimize the trajectories for manipulation of deformable objects, such as the folding of garments. Extensive experimental results have shown the methods work quite well on a variety of different clothing. &nbsp;&nbsp;Further, we have achieved significant improvements over previous approaches in pose detection and recognition, in terms of accuracy and speed.&nbsp;&nbsp;&nbsp; We have also developed novel vision algorithms fusing depth and 2-D imagery for detection and classification of wrinkles, which have been used to implement an ironing algorithm on a real robot. Finally, our work has shown how the interactions between computer vision, simulation, machine learning, 3-D modeling, and robotics can solve difficult grasping and manipulation tasks, and that these methods can be applied to other areas of deformable grasping including food handling/preparation and wiring tasks.</p><br> <p>            Last Modified: 11/16/2016<br>      Modified by: Eitan&nbsp;Grinspun</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478030745359_BaxterFlowChart-eps-converted-to--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478030745359_BaxterFlowChart-eps-converted-to--rgov-800width.jpg" title="Deformable Grasping Pipeline"><img src="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478030745359_BaxterFlowChart-eps-converted-to--rgov-66x44.jpg" alt="Deformable Grasping Pipeline"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The overall pipeline of robotic manipulation of deformable objects.</div> <div class="imageCredit">Peter Allen</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Deformable Grasping Pipeline</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031154551_flowchart_iros2014-eps-converted-to--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031154551_flowchart_iros2014-eps-converted-to--rgov-800width.jpg" title="Pose Estimation Pipeline"><img src="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031154551_flowchart_iros2014-eps-converted-to--rgov-66x44.jpg" alt="Pose Estimation Pipeline"></a> <div class="imageCaptionContainer"> <div class="imageCaption">In the offline training stage (the red rectangle), we extract a tailored binary feature from the simulated database, and learn a weighted Hamming distance from additional calibrated data collected from the Kinect. In the on</div> <div class="imageCredit">Peter Allen</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Pose Estimation Pipeline</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031430521_visexample-eps-converted-to--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031430521_visexample-eps-converted-to--rgov-800width.jpg" title="Pose Recognition results"><img src="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031430521_visexample-eps-converted-to--rgov-66x44.jpg" alt="Pose Recognition results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Visual examples of the pose recognition result of our method. The garment is picked up via a griper of the Baxter robot.  From left to right, each example shows the color image, input depth image, reconstructed model, matched simulated model, groundtruth simulated model, and the predicted grasping</div> <div class="imageCredit">Peter Allen</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Pose Recognition results</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031977658_final_results-eps-converted-to--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031977658_final_results-eps-converted-to--rgov-800width.jpg" title="Regrasping and Unfolding Results"><img src="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478031977658_final_results-eps-converted-to--rgov-66x44.jpg" alt="Regrasping and Unfolding Results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Unfolding procedure. Left to right: a snapshot of initial pick up, a 3Dreconstructed mesh, a predicted mesh from database, the predicted mesh with weighted Gaussian distribution distance, predictedregrasping point on the 3D reconstructed mesh, snapshot of regrasping, and a snapshot of unfolding</div> <div class="imageCredit">Peter Allen</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Regrasping and Unfolding Results</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478032623554_Good_Examples-eps-converted-to--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478032623554_Good_Examples-eps-converted-to--rgov-800width.jpg" title="Folding Examples"><img src="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478032623554_Good_Examples-eps-converted-to--rgov-66x44.jpg" alt="Folding Examples"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Successful folding examples.The first row of each group is from the simulation and the second row is from the real world (Green tape shows the original garment contour position). TOPGROUP: Long-sleeve shirt folding with 3 steps. MIDDLE GR</div> <div class="imageCredit">Peter Allen</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Folding Examples</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478033241392_Picture1--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478033241392_Picture1--rgov-800width.jpg" title="Robotic Ironing"><img src="/por/images/Reports/POR/2016/1217904/1217904_10209259_1478033241392_Picture1--rgov-66x44.jpg" alt="Robotic Ironing"></a> <div class="imageCaptionContainer"> <div class="imageCaption">TOP: A Baxter robot is ironing a piece of cloth on the table,assisted by a Kinect sensor and two light sources. BOTTOM: Samples ofsmooth region (left) and non-smooth region (right). The smooth regions are defined as height bumps, which can be removed by iterative pulling thecloth boundary.</div> <div class="imageCredit">Peter Allen</div> <div class="imageSubmitted">Peter&nbsp;K&nbsp;Allen</div> <div class="imageTitle">Robotic Ironing</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Robotic manipulation of deformable objects is a challenging problem especially because of the complexity of the many different ways an object can deform. Searching within such a high dimensional state space makes it difficult to recognize, track, and manipulate deformable objects. In this project, we have developed a feed-forward, model-driven, predictive approach to address this challenge, using a pre-computed, simulated database of deformable object models. The models are detailed, robust, and simple to construct, and using a physics-based approach, can predict accurately, in simulation, the behavior of the objects, which can then be used in a real physical setting. This bridges the gap between the simulation world and the real physical world. The feed-forward, model-driven, predictive approach takes advantages of the simulation and generates a large number of instances for learning approaches, which not only alleviates the burden of the data collection, but also makes domain adaptation of the methods to other applications easier and faster.  To validate this approach, we developed a comprehensive pipeline for manipulating clothing as in a typical laundry task, which includes garment pose detection, re-grasping, folding and ironing.  In this task, mesh models of common deformable garments are simulated with the garments picked up in multiple different poses under gravity, and stored in an indexable database for fast and efficient retrieval. First, the database is used to estimate poses of garments in an arbitrary positions. A full featured 3D volumetric model of the garment is constructed in real-time and novel volumetric features are then used to obtain the most similar model in the database to predict the object category and pose. Second, the database can significantly benefit the manipulation of deformable objects via non-rigid registration, providing accurate correspondences between the reconstructed object model and the database models. Third, the accurate model simulation can also be used to predict re-grasping points, and to optimize the trajectories for manipulation of deformable objects, such as the folding of garments. Extensive experimental results have shown the methods work quite well on a variety of different clothing.   Further, we have achieved significant improvements over previous approaches in pose detection and recognition, in terms of accuracy and speed.    We have also developed novel vision algorithms fusing depth and 2-D imagery for detection and classification of wrinkles, which have been used to implement an ironing algorithm on a real robot. Finally, our work has shown how the interactions between computer vision, simulation, machine learning, 3-D modeling, and robotics can solve difficult grasping and manipulation tasks, and that these methods can be applied to other areas of deformable grasping including food handling/preparation and wiring tasks.       Last Modified: 11/16/2016       Submitted by: Eitan Grinspun]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
