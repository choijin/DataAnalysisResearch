<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: 3D Gaze Control for Assistive Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>101331.00</AwardTotalIntnAmount>
<AwardAmount>101331</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07020000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CBET</Abbreviation>
<LongName>Div Of Chem, Bioeng, Env, &amp; Transp Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michele Grimm</SignBlockName>
<PO_EMAI>mgrimm@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>PI: Zhang, Xiaoli and Nelson, Carl A.&lt;br/&gt;Proposal Number: 1264496 &amp; 1264504&lt;br/&gt;&lt;br/&gt;The objective of this research is to develop a novel 3D gaze control system to intuitively connect elderly or disabled people with robotic assistants. The communication will take place through a 3D gaze as the robot control signal. We will focus on basic Activities of Daily Living, including retrieving an object to the user, and moving an object from one place to another, which enable the individual to live independently. Tasks include: (1) a study of 3D gaze estimation based on eye modeling and simulation with experimental data; (2) investigation and development of a 3D gaze control framework to enable the target selection relative to real-world, everyday objects on which assistive tasks are performed; (3) establishment of a gaze control platform for testing and evaluating the proposed 3D gaze control model with a wide range of assistive robot applications.&lt;br/&gt;&lt;br/&gt;Intellectual Merit: The project investigates a novel 3D gaze control concept for robotic assistants with the goal of increasing the level of independence for motor impaired people due to diseases or senescence. The research also proposes a novel gaze control model which uses gaze-extracted features to guide a robot for object identification and operation, achieving simple and natural human-robot interaction. Finally, the project seeks to develop a novel 3D gaze estimation system to ensure accuracy and reliability, which is currently not well explored. This project seeks to provide solutions for a wide spectrum of robotic assistive applications.&lt;br/&gt;&lt;br/&gt;Broader Impacts: By introducing 3D gaze as the control signal into communication between elderly or disabled people and robotic assistants, the broader impacts of this project are to build a simple and natural control interface to motor impaired people and increase the level of living independence. Persons with disabilities, especially students with disabilities at Wilkes University, will be actively involved in the development of the proposed work including feedback in the form of interviews, surveys, and participation in testing and evaluating the working system. Results, outcomes, software tools, benchmarks, and educational materials will be disseminated through a project web site, as well as through journal and conference publications. A new course on assistive robotic technology is being developed and taught at Wilkes University.</AbstractNarration>
<MinAmdLetterDate>09/25/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/25/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1264504</AwardID>
<Investigator>
<FirstName>Carl</FirstName>
<LastName>Nelson</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carl A Nelson</PI_FULL_NAME>
<EmailAddress>cnelson5@unl.edu</EmailAddress>
<PI_PHON>4024724128</PI_PHON>
<NSF_ID>000490839</NSF_ID>
<StartDate>09/25/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName>Lincoln</CityName>
<StateCode>NE</StateCode>
<ZipCode>685880430</ZipCode>
<StreetAddress><![CDATA[312 N 14th St., Alex Bldg West]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5342</Code>
<Text>Disability &amp; Rehab Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>010E</Code>
<Text>DISABILITY RES &amp; HOMECARE TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~101331</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was focused on developing methods by which individuals with severe disabilities can control assistive robots. In particular, the emphasis was placed on the ability to use eye gaze as a primary mode of communication with a robot (i.e., assuming that other communication options are affected by impairment).&nbsp;</p> <p>In a first phase of the research, we sought to demonstrate that the eyes can convey a richness of information, including different characteristics for intentional vs. observational gaze, suitable for robot control. A control system was developed based on fuzzy logic, using the eye gaze vector along with blinking as inputs, allowing control of robot speed and direction corresponding with "intention" interpreted from the gaze data.</p> <p>In a second phase of the research, we sought to develop a gaze-based framework for specific commands (e.g., go to object, pick up object, etc.). Using a hierarchical menu framework as the basis, several simple shapes (e.g., circle, square, triangle) were used as primitives in the menu structure. It was demonstrated that users could trace these shapes with their eye gaze, and the software developed as part of this project was capable of repeatable and reliable detection and interpretation of these menu commands. Passing these commands along to a small assistive robot resulted in the desired task performance.</p> <p>In addition to the scientific contributions of this project described above (algorithm and software development), it is envisioned that this work can lead to improved human-robot interfaces for the severely disabled, enabling these individuals to more actively engage with their environment and perform healthy activities of daily living. This can provide a key component of quality of life.</p><br> <p>            Last Modified: 11/29/2016<br>      Modified by: Carl&nbsp;A&nbsp;Nelson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was focused on developing methods by which individuals with severe disabilities can control assistive robots. In particular, the emphasis was placed on the ability to use eye gaze as a primary mode of communication with a robot (i.e., assuming that other communication options are affected by impairment).   In a first phase of the research, we sought to demonstrate that the eyes can convey a richness of information, including different characteristics for intentional vs. observational gaze, suitable for robot control. A control system was developed based on fuzzy logic, using the eye gaze vector along with blinking as inputs, allowing control of robot speed and direction corresponding with "intention" interpreted from the gaze data.  In a second phase of the research, we sought to develop a gaze-based framework for specific commands (e.g., go to object, pick up object, etc.). Using a hierarchical menu framework as the basis, several simple shapes (e.g., circle, square, triangle) were used as primitives in the menu structure. It was demonstrated that users could trace these shapes with their eye gaze, and the software developed as part of this project was capable of repeatable and reliable detection and interpretation of these menu commands. Passing these commands along to a small assistive robot resulted in the desired task performance.  In addition to the scientific contributions of this project described above (algorithm and software development), it is envisioned that this work can lead to improved human-robot interfaces for the severely disabled, enabling these individuals to more actively engage with their environment and perform healthy activities of daily living. This can provide a key component of quality of life.       Last Modified: 11/29/2016       Submitted by: Carl A Nelson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
