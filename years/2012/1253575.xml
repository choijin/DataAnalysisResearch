<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Application-Agnostic, Distributed-Aware Cloud Platforms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>410486.00</AwardTotalIntnAmount>
<AwardAmount>426486</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Cloud Computing has radically changed how businesses run their applications by allowing a huge number of computers to be economically shared by many different users.  The applications running inside these cloud data centers are growing in size and complexity.  Even a relatively straightforward web application is likely to be composed of multiple interacting service components such as a web server, a database, and a data cache. The result is a complicated distributed application that may exhibit performance bottlenecks or consistency requirements between components. Unfortunately, existing resource management and reliability tools consider these components individually, and are often unaware of the important relations between them.&lt;br/&gt;  &lt;br/&gt;This work is predicated on the belief that future data centers must be application-agnostic, yet distributed-computing-aware. The goal of this research project is to enable cloud platforms to balance this trade-off, improving performance, reliability, and efficiency. Its contributions will be twofold: 1) tools for cloud providers to better understand the applications running on their infrastructure, and 2) new resource management and reliability algorithms that offer customers stronger guarantees. These will be achieved with application-agnostic solutions built into the virtualization layer, that are still capable of understanding the structure and dependencies of distributed applications. &lt;br/&gt;&lt;br/&gt;Broader Impact: This work will improve the efficiency and reliability of data centers running web applications used by millions of people daily. By attacking these challenges at the virtualization layer, we will provide flexible solutions fit for a wide variety of environments. The project also contains a significant educational component to enhance the understanding of cloud computing challenges for students at the high school through graduate levels.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/14/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1253575</AwardID>
<Investigator>
<FirstName>Timothy</FirstName>
<LastName>Wood</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Timothy Wood</PI_FULL_NAME>
<EmailAddress>timwood@gwu.edu</EmailAddress>
<PI_PHON>2029941918</PI_PHON>
<NSF_ID>000621583</NSF_ID>
<StartDate>01/14/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043990498</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520058</ZipCode>
<StreetAddress><![CDATA[801 22nd Street NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~90954</FUND_OBLG>
<FUND_OBLG>2014~163511</FUND_OBLG>
<FUND_OBLG>2016~84905</FUND_OBLG>
<FUND_OBLG>2017~87116</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 11.0px 'Helvetica Neue'; color: #000000; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 11.0px 'Helvetica Neue'; color: #000000; -webkit-text-stroke: #000000; min-height: 12.0px} span.s1 {font-kerning: none} --> <p class="p1"><span class="s1">Cloud Computing has radically changed how businesses run their applications by allowing a huge number of computers to be economically shared by many different users. The applications running inside these cloud data centers are growing in size and complexity. Even a relatively straightforward web application is likely to be composed of multiple interacting service components such as a web server, a database, and a data cache. The result is a complicated distributed application that may exhibit performance bottlenecks or consistency requirements between components. Unfortunately, existing resource management and reliability tools consider these components individually, and are often unaware of the important relations between them.</span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">This project was predicated on the belief that future data centers must be application-agnostic, yet distributed-computing-aware. Cloud data centers rely on virtualization to partition servers into isolated components, but there are benefits and drawbacks of sending information across the virtualization abstraction layer. We have explored these trade-offs in the context of memory management (VEE 14), flash storage (Cloud 14, Super Computing 15,&nbsp;IC2E 16) and CPU scheduling (CCGrid 14 and PER 15), with a focus on distributed cloud applications. For example, in our Mortar system presented at VEE 2014 we demonstrated how spare memory could be efficiently allocated to other virtual machines, while still allowing it to be quickly reclaimed when a more important virtual machine needed it.<span>&nbsp; </span>We extended this principle to spare CPU time in our CCGrid 14 paper, which allowed idle resources to be used for processor intensive big data tasks without causing interference on other virtual machines. We have also explored how network function virtualization offers a new opportunity to customize network management for cloud applications in a transparent way (NSDI 14,&nbsp;ICAC 16 Best Paper).<span>&nbsp;</span></span></p> <p class="p2"><span class="s1">&nbsp;</span></p> <p class="p1"><span class="s1">The work has resulted in several open source extensions to popular cloud software such as Xen and Hadoop, over twenty publications, and a best paper award. We have collaborated closely with industrial researchers at IBM and AT&amp;T to encourage deployment of our techniques. A diverse body of students, including four PhD students, eight undergraduate students, three MS students, and three high school students, were involved with the research projects.<span>&nbsp; </span>The PI also developed new course materials for a distributed systems course and ran an outreach program to local high schools and undergraduate colleges to introduce students from a broad background to the technology behind cloud computing.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 06/12/2019<br>      Modified by: Timothy&nbsp;Wood</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Cloud Computing has radically changed how businesses run their applications by allowing a huge number of computers to be economically shared by many different users. The applications running inside these cloud data centers are growing in size and complexity. Even a relatively straightforward web application is likely to be composed of multiple interacting service components such as a web server, a database, and a data cache. The result is a complicated distributed application that may exhibit performance bottlenecks or consistency requirements between components. Unfortunately, existing resource management and reliability tools consider these components individually, and are often unaware of the important relations between them.   This project was predicated on the belief that future data centers must be application-agnostic, yet distributed-computing-aware. Cloud data centers rely on virtualization to partition servers into isolated components, but there are benefits and drawbacks of sending information across the virtualization abstraction layer. We have explored these trade-offs in the context of memory management (VEE 14), flash storage (Cloud 14, Super Computing 15, IC2E 16) and CPU scheduling (CCGrid 14 and PER 15), with a focus on distributed cloud applications. For example, in our Mortar system presented at VEE 2014 we demonstrated how spare memory could be efficiently allocated to other virtual machines, while still allowing it to be quickly reclaimed when a more important virtual machine needed it.  We extended this principle to spare CPU time in our CCGrid 14 paper, which allowed idle resources to be used for processor intensive big data tasks without causing interference on other virtual machines. We have also explored how network function virtualization offers a new opportunity to customize network management for cloud applications in a transparent way (NSDI 14, ICAC 16 Best Paper).    The work has resulted in several open source extensions to popular cloud software such as Xen and Hadoop, over twenty publications, and a best paper award. We have collaborated closely with industrial researchers at IBM and AT&amp;T to encourage deployment of our techniques. A diverse body of students, including four PhD students, eight undergraduate students, three MS students, and three high school students, were involved with the research projects.  The PI also developed new course materials for a distributed systems course and ran an outreach program to local high schools and undergraduate colleges to introduce students from a broad background to the technology behind cloud computing.          Last Modified: 06/12/2019       Submitted by: Timothy Wood]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
