<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Design of Efficient Saddle Point Algorithms for Large-scale/Complex Geometry Convex Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>george hazelrigg</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This award provides funding for theoretical and software development of novel algorithms for processing large-scale optimization models arising in Signal Processing,&lt;br/&gt;Medical Image Reconstruction, Machine Learning, and high-dimensional Statistical inference, where huge and steadily growing amounts of underlying data result in the&lt;br/&gt;necessity to process optimization problems with hundreds of thousands of variables and constraints. In addition, some of applications, such as low rank matrix&lt;br/&gt;approximations, lead to problems with difficult geometry, which amplifies significantly the challenges caused by sheer problem sizes. These challenges will be&lt;br/&gt;met via developing algorithms with cheap iterations utilizing state-of-the-art approaches, primarily bilinear saddle point reformulation of the problem of interest&lt;br/&gt;combined with duality-based handling difficult geometry and accelerating algorithms via various types of randomization. Theoretical and algorithmic developments&lt;br/&gt;will be adjusted to several generic applications (sparsity- and low-rank oriented Signal Processing and Machine Learning, extensions of total variation-based Image&lt;br/&gt;processing, and some others) and will be aimed at developing optimization techniques with good theoretical performance guarantees and visible practical potential;&lt;br/&gt;the latter will be validated by extensive numerical experimentation with both simulated and real life problems.&lt;br/&gt;&lt;br/&gt;If successful, the research will advance theory and practice of optimization by enriching its abilities to process large-scale/complex geometry problems and thus will&lt;br/&gt;contribute significantly to the computational toolboxes in Signal Processing, Image Reconstruction, Machine Learning, and some other subject domains. As a byproduct,&lt;br/&gt;the research will contribute to recent tendency of bridging the corresponding research communities, with clear mutual benefits. In addition, the results of&lt;br/&gt;the research could form the base of new Ph.D.-level optimization courses.</AbstractNarration>
<MinAmdLetterDate>07/18/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/18/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1232623</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Shapiro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander Shapiro</PI_FULL_NAME>
<EmailAddress>ashapiro@isye.gatech.edu</EmailAddress>
<PI_PHON>4048946544</PI_PHON>
<NSF_ID>000261260</NSF_ID>
<StartDate>07/18/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Arkadi</FirstName>
<LastName>Nemirovski</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arkadi S Nemirovski</PI_FULL_NAME>
<EmailAddress>nemirovs@isye.gatech.edu</EmailAddress>
<PI_PHON>4043850769</PI_PHON>
<NSF_ID>000486169</NSF_ID>
<StartDate>07/18/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue, NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>5514</Code>
<Text>OPERATIONS RESEARCH</Text>
</ProgramElement>
<ProgramReference>
<Code>072E</Code>
<Text>NETWORKS &amp; QUEUING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>073E</Code>
<Text>OPTIMIZATION &amp; DECISION MAKING</Text>
</ProgramReference>
<ProgramReference>
<Code>077E</Code>
<Text>SIMULATION MODELS</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong><span>The primary goal</span></strong><span><span>&nbsp;</span></span><span>of the research project was to develop novel efficient optimization techniques for large-scale optimization&nbsp;problems on domains with difficult geometry,&nbsp;meaning that the only allowed domain-related operation is&nbsp;minimizing over the domain the simplest possible - just linear - functions. Problems of this type arise in many important Machine Learning and Signal Processing applications and typically are beyond the grasp of existing optimization techniques.</span></p> <p><em><span>This and related goals were successfully achieved.</span></em><span><em><span>&nbsp;</span></em></span><span>The results are presented in 19 papers in peer-reviewed professional outlets (12 published, 2 accepted, 3 submitted&nbsp; journal papers, 2 papers in NIPS Conference proceedings) and were the primary subject of 13, and the secondary subject of 7 more, talks at international conferences.</span></p> <p><strong><span>Impact on project's discipline.</span></strong><span><span>&nbsp;</span></span><span>We believe our research has contributed significantly to the basic theory of large-scale convex optimization. This includes</span></p> <p><span>--- developing novel Norm-regularized Conditional Gradient &nbsp;and Composite Mirror Prox algorithms aimed at handling ``composite,'' in some precise sense, large-scale optimization problems frequently arising in Machine Learning and Signal Processing &nbsp;(multiclass classification, recovery of low rank matrix models, total variation based image reconstruction, image decomposition, to name just a few);</span></p> <p><span>--- discovery,<span>&nbsp;</span><em><span>for the first time</span></em>, algorithms capable to handle difficult geometry<span>&nbsp;</span><em><span>nonsmooth</span></em><span>&nbsp;</span>convex minimization and (smooth and nonsmooth alike) convex-concave<span>&nbsp;</span><em><span>saddle point</span></em><span>&nbsp;</span>problems (in the past, difficult geometry techniques were known solely for smooth convex minimization). The resulting algorithms are essentially novel and obey provably best possible under the circumstances efficiency estimates. Design of these algorithms required developing several novel theoretical concepts, most notably, Fenchel-type representations of monotone vector fields and induced by these representations ``algorithmic duality,'' on one hand, and decomposition techniques, on the other hand, all this -- for problems with convex structure, including saddle point ones. We believe that these concepts are of significant interest and potential by their own right.</span></p> <p><em><span>To judge on computational power of our methods</span></em><span>, note that we were able to process in reasonable time on medium performance computers:</span></p> <p><span>&nbsp; &nbsp;--- classification with 20,860 examples of dimension 65,536 each, falling into 1043 classes,</span></p> <p><span>&nbsp; &nbsp;--- matrix recovery with 32,000 x 32,000 (L2 fit) and 16,000 x 16,000 (uniform and spectral norm fits) matrices,</span></p> <p><span>&nbsp; &nbsp;--- matrix game with huge (97,082,021,465 x 97,082,021,465) matrix.</span></p> <p><strong><span>Impact on other disciplines.</span></strong><span><strong><span>&nbsp;</span></strong></span><span>Specific goals and priorities driving our research project take their origin mainly in the needs of the ``state-of-the-art optimization customers,'' most notably, Machine Learning and Signal Processing, and problems arising in these applications were the primary test beds for the novel algorithms stemming from the project. We strongly believe that the project extended significantly the ``computational toolbox'' of these and related disciplines.</span></p> <p><span>A significant development of project's last year (not foreseen in the initial research agenda) was discovery of novel ways t...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary goal of the research project was to develop novel efficient optimization techniques for large-scale optimization problems on domains with difficult geometry, meaning that the only allowed domain-related operation is minimizing over the domain the simplest possible - just linear - functions. Problems of this type arise in many important Machine Learning and Signal Processing applications and typically are beyond the grasp of existing optimization techniques.  This and related goals were successfully achieved. The results are presented in 19 papers in peer-reviewed professional outlets (12 published, 2 accepted, 3 submitted  journal papers, 2 papers in NIPS Conference proceedings) and were the primary subject of 13, and the secondary subject of 7 more, talks at international conferences.  Impact on project's discipline. We believe our research has contributed significantly to the basic theory of large-scale convex optimization. This includes  --- developing novel Norm-regularized Conditional Gradient  and Composite Mirror Prox algorithms aimed at handling ``composite,'' in some precise sense, large-scale optimization problems frequently arising in Machine Learning and Signal Processing  (multiclass classification, recovery of low rank matrix models, total variation based image reconstruction, image decomposition, to name just a few);  --- discovery, for the first time, algorithms capable to handle difficult geometry nonsmooth convex minimization and (smooth and nonsmooth alike) convex-concave saddle point problems (in the past, difficult geometry techniques were known solely for smooth convex minimization). The resulting algorithms are essentially novel and obey provably best possible under the circumstances efficiency estimates. Design of these algorithms required developing several novel theoretical concepts, most notably, Fenchel-type representations of monotone vector fields and induced by these representations ``algorithmic duality,'' on one hand, and decomposition techniques, on the other hand, all this -- for problems with convex structure, including saddle point ones. We believe that these concepts are of significant interest and potential by their own right.  To judge on computational power of our methods, note that we were able to process in reasonable time on medium performance computers:     --- classification with 20,860 examples of dimension 65,536 each, falling into 1043 classes,     --- matrix recovery with 32,000 x 32,000 (L2 fit) and 16,000 x 16,000 (uniform and spectral norm fits) matrices,     --- matrix game with huge (97,082,021,465 x 97,082,021,465) matrix.  Impact on other disciplines. Specific goals and priorities driving our research project take their origin mainly in the needs of the ``state-of-the-art optimization customers,'' most notably, Machine Learning and Signal Processing, and problems arising in these applications were the primary test beds for the novel algorithms stemming from the project. We strongly believe that the project extended significantly the ``computational toolbox'' of these and related disciplines.  A significant development of project's last year (not foreseen in the initial research agenda) was discovery of novel ways to apply Convex Optimization in High-Dimensional Statistics. Specifically, we found that saddle point techniques (which, according to our original plan, laid the foundation to project's algorithmic developments), can be directly utilized in statistical context, primarily, in hypotheses testing. As applied to several basic ``observation schemes,'' these techniques yield provably near optimal statistical inferences. The major good news here is that within its well understood (and reasonably wide) applicability bounds, the approach can handle pretty general hypotheses, and both near optimal inference and its risk are given by an efficient computation. In contrast to this, with the traditional approach, near-optimal statistical inferences stem from ``cl...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
