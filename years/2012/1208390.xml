<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI-Small: Contextually Grounded Collaborative Discourse for Mediating Shared Basis in Situated Human Robot Dialogue</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>957000.00</AwardTotalIntnAmount>
<AwardAmount>981000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In human robot dialogue, although human partners and robots are co-present in a shared environment, they have completely mismatched capabilities in perceiving and reasoning about the environment. Their knowledge and representations of the shared world are drastically different. In addition, the shared environment is full of uncertainties and unexpected events. Humans and robots may have different capabilities in attending and responding to these uncertainties. All of these contribute to a misaligned perceptual basis between a human and a robot, which jeopardizes their collaborative activities and task performance. To enable situated human robot dialogue, a critical component is to develop techniques that will support mediating the shared perceptual basis for effective conversation and task completion. &lt;br/&gt;&lt;br/&gt;The objective of this National Robotics Initiative project is to develop a novel framework that tightly integrates high level language and dialogue processing with low level sensing and control systems and contextually grounds the collaborative discourse to mediate shared perceptual basis. By capturing grounded symbolic representations as well as continuous representations of the internal configuration of a robotic system and continuous information sensed from the changing environment, the framework allows the robot to promptly modify its execution without interrupting the on-going tasks. It further enables collaborations between humans and robots to mediate a shared perceptual basis and support efficient interaction in a highly dynamic environment.&lt;br/&gt;&lt;br/&gt;This project will provide insight as to how the misaligned perceptual basis between a human and a robot should be mediated through a collaborative process and how such a process should be integrated to produce intelligent and collaborative robot behaviors. The expected results will benefit many applications such as manufacturing, service, assistive technology, and search and rescue.</AbstractNarration>
<MinAmdLetterDate>09/11/2012</MinAmdLetterDate>
<MaxAmdLetterDate>02/22/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208390</AwardID>
<Investigator>
<FirstName>Ning</FirstName>
<LastName>Xi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ning Xi</PI_FULL_NAME>
<EmailAddress>xin@egr.msu.edu</EmailAddress>
<PI_PHON>5174321925</PI_PHON>
<NSF_ID>000099369</NSF_ID>
<StartDate>09/11/2012</StartDate>
<EndDate>02/01/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joyce</FirstName>
<LastName>Chai</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joyce Y Chai</PI_FULL_NAME>
<EmailAddress>chaijy@umich.edu</EmailAddress>
<PI_PHON>7347648505</PI_PHON>
<NSF_ID>000477137</NSF_ID>
<StartDate>09/11/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName/>
<StateCode>MI</StateCode>
<ZipCode>488241046</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~957000</FUND_OBLG>
<FUND_OBLG>2014~8000</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In human-robot communication, robots need to ground human language to their own representations of perception and action. However, although humans and robots are co-present, their representations of the shared world and joint tasks are significantly mismatched due to disparities of their knowledge and perceptual capabilities. Such disparities make language grounding rather difficult. To address this issue, this project has conducted a systematic investigation on the role of collaboration in language communication between humans and robots and developed several collaborative models for grounded language processing.</p> <p>Our studies have shown that when robots have limited perceptual capabilities, grounding language to perception often cannot be succeeded by one attempt. It is achieved by a collaborative process between humans and robots through multiple episodes. To establish a shared perceptual basis, humans and robots will need to make extra efforts to collaborate with each other and strive for a common ground for language grounding. Based on these observations, this project has developed several collaborative models using graph-based approaches, where a language graph captures collaborative linguistic discourse and a vision graph represents the perceived environment. Language grounding becomes a problem of finding the best match between the language graph and the vision graph that maximizes the overall compatibility between the two graphs. Our empirical results have shown that, graphs can capture rich semantic relations (e.g., spatial relations, group relations) between linguistic expressions as collaborative discourse unfolds. By taking advantage of these relations, the graph-based approaches can compensate for visual recognition errors and mitigate perceptual differences between humans and robots. Robots also need to have the ability to generate linguistic expressions to refer to objects in the environment. Our experiments have shown that, traditional approaches that generate one single description often fail as humans cannot identify the corresponding target objects due to mismatched representations. To address this problem, &nbsp;this project has developed collaborative referring expression generation models that generate smaller episodes one at a time based on human feedback to mediate perceptual differences. These collaborative models allow humans to identify target objects with a significantly higher accuracy.</p> <p>This project has also explored connections between language and robotic actions. Most robotic arms are programmed with primitive operations such as move to, open gripper, and close gripper. To perform an action, a discrete controller is often first applied to find a sequence of primitive operations. These primitive operations are then passed to a continuous planner and translated into trajectories of arm motors. Thus a critical question is how to connect language commands with the corresponding sequence of primitive operations. To address this question, this project has focused on the representation of grounded verb semantics for concrete result action verbs (i.e., those which denote changes to the world) and the acquisition of such representation through human-robot communication. Humans can teach robots new actions through step-by-step language instructions based on known actions and operations. At the end of teaching, robots capture the change of world states as they experience during teaching and represent the corresponding new verb with the expected goal state of the world. This state-based representation allows a planner to automatically compute a sequence of low-level primitive operations when the learned verbs are applied in new situations. To handle uncertainties of the environment, this project has also developed collaborative approaches that allow the robot to ask intelligent questions and acquire more reliable grounded representations.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/29/2018<br>      Modified by: Joyce&nbsp;Y&nbsp;Chai</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In human-robot communication, robots need to ground human language to their own representations of perception and action. However, although humans and robots are co-present, their representations of the shared world and joint tasks are significantly mismatched due to disparities of their knowledge and perceptual capabilities. Such disparities make language grounding rather difficult. To address this issue, this project has conducted a systematic investigation on the role of collaboration in language communication between humans and robots and developed several collaborative models for grounded language processing.  Our studies have shown that when robots have limited perceptual capabilities, grounding language to perception often cannot be succeeded by one attempt. It is achieved by a collaborative process between humans and robots through multiple episodes. To establish a shared perceptual basis, humans and robots will need to make extra efforts to collaborate with each other and strive for a common ground for language grounding. Based on these observations, this project has developed several collaborative models using graph-based approaches, where a language graph captures collaborative linguistic discourse and a vision graph represents the perceived environment. Language grounding becomes a problem of finding the best match between the language graph and the vision graph that maximizes the overall compatibility between the two graphs. Our empirical results have shown that, graphs can capture rich semantic relations (e.g., spatial relations, group relations) between linguistic expressions as collaborative discourse unfolds. By taking advantage of these relations, the graph-based approaches can compensate for visual recognition errors and mitigate perceptual differences between humans and robots. Robots also need to have the ability to generate linguistic expressions to refer to objects in the environment. Our experiments have shown that, traditional approaches that generate one single description often fail as humans cannot identify the corresponding target objects due to mismatched representations. To address this problem,  this project has developed collaborative referring expression generation models that generate smaller episodes one at a time based on human feedback to mediate perceptual differences. These collaborative models allow humans to identify target objects with a significantly higher accuracy.  This project has also explored connections between language and robotic actions. Most robotic arms are programmed with primitive operations such as move to, open gripper, and close gripper. To perform an action, a discrete controller is often first applied to find a sequence of primitive operations. These primitive operations are then passed to a continuous planner and translated into trajectories of arm motors. Thus a critical question is how to connect language commands with the corresponding sequence of primitive operations. To address this question, this project has focused on the representation of grounded verb semantics for concrete result action verbs (i.e., those which denote changes to the world) and the acquisition of such representation through human-robot communication. Humans can teach robots new actions through step-by-step language instructions based on known actions and operations. At the end of teaching, robots capture the change of world states as they experience during teaching and represent the corresponding new verb with the expected goal state of the world. This state-based representation allows a planner to automatically compute a sequence of low-level primitive operations when the learned verbs are applied in new situations. To handle uncertainties of the environment, this project has also developed collaborative approaches that allow the robot to ask intelligent questions and acquire more reliable grounded representations.           Last Modified: 01/29/2018       Submitted by: Joyce Y Chai]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
