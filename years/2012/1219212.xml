<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Making sense of incomplete sensor data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>427097.00</AwardTotalIntnAmount>
<AwardAmount>427097</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project focuses on how to reveal the whole from only partial measurements. Often, physical variables can only be incompletely measured. For example, the state-of-the-art techniques to record brain activity (multi-electrode recordings of local field potentials or fMRI measurements) give only a partial account of the activity patterns of large numbers of neurons. In the course of this project, Fritz Sommer, Ph.D., and his collaborators from the University of California, investigate and develop methods for recovering complex multi-dimensional data structure from incomplete measurements. In the general case, when measurements are made (or "subsampled") at a rate lower than a mathematical limit called the Nyquist limit, the original signal cannot be fully reconstructed. However, the recent theory of compressed sensing (CS) demonstrated that subsampling below the Nyquist limit can be lossless if the signal to be compressed has sparse structure. The established theory of CS explains when full recovery from incomplete measurements is possible and provides efficient algorithms for full data reconstruction.  This result is extremely relevant because many important classes of sensor signals, such as natural images and sounds, have an approximately sparse structure.&lt;br/&gt;&lt;br/&gt;The current project explores whether the principle of CS can allow reconstruction of data structure from measurements that subsample an unknown signal in an unknown fashion. Standard CS cannot reconstruct the signal in such situations because the algorithm requires knowledge of how the signal was subsampled and what its structure is. Dr. Sommer and his team plan to develop methods for data reconstruction that can be performed with the subsampled data alone. The idea is to combine CS, a principle about measuring a signal with sparse structure, with sparse coding (SC), a principle for learning efficient representations of signals with sparse structure. Preliminary results suggest that this combination of methods, called adaptive compressed sensing (ACS), can indeed "learn" the map for recovering the full data from the subsamples alone (Isely et al. 2011). The team will investigate under what conditions a similar result holds for the large class of real-world sensor data that are not exactly sparse but can be well-approximated by sparse representations. Also being developed are methods on how to draw inferences and make decisions based on incomplete measurements. In particular, a pilot investigation in collaboration with Dr. Bosco Tjan's lab at the University of Southern California will explore whether ACS can be applied to improve the decoding of fMRI data.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1219212</AwardID>
<Investigator>
<FirstName>Friedrich</FirstName>
<LastName>Sommer</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Friedrich T Sommer</PI_FULL_NAME>
<EmailAddress>fsommer@berkeley.edu</EmailAddress>
<PI_PHON>5106427251</PI_PHON>
<NSF_ID>000490098</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Regents of the University of California]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947200001</ZipCode>
<StreetAddress><![CDATA[571 Evans Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~427097</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="layoutArea"> <div class="column"> <p><span>Sparse coding or sparse dictionary learning has been widely used to recover underlying structure in many kinds of natural data, such as images, sound and other measurement data. The result of sparse coding is a decomposition of the original data into different components, just based on the intrinsic statistical struture. Often these components directly reveal interpretable substructure in the data, and thus sparse coding can be a very valuable tool in data analysis.&nbsp;</span>Specifically, sparse coding reveals a product structure of the data - a dictionary, that is, typical structure primitives of the signals, multiplied with sparse coefficients, specifying which data instances contain certain primitives.&nbsp;</p> <p>&nbsp;</p> <p><span>&nbsp;</span>Intellectual merit:</p> <p>In this grant we investigated in theory, under which circumstances the decomposition provided by sparse coding is universal; that is, when sparse codes and dictionaries are unique (up to natural symmetries). If the uniqueness condition is violated, the result of a sparse coding analysis cannot be trusted. We were able to derive bounds on the sample sizes guaranteeing such uniqueness under various assumptions for how training data are generated. Whenever the conditions to one of our theorems are met, any sparsity-constrained learning algorithm that succeeds in reconstructing the data recovers the original sparse codes and dictionary.</p> <p>Further we were able to generalize the uniqueness guarantee to the case where the data are approximated by the sparse decomposition with finite precision. This generalization is crucial to any application of sparse decomposition methods to real-world measurement, in which finite precision is inherent.&nbsp; Finally, we were able to demionstrate impacts that these results have in practice, by analyzing concrete data sets. In particular, we demonstrated the practical value of sparse signal decomposition in the analysis of human electrocorticography (ECoG) data, as well as multielectrode recordings in the hippocampus of rodent.&nbsp;</p> <p>&nbsp;</p> <p>Broader impact:</p> <p>The theoretical foundation of the application of sparse coding in data analysis has broad implications to many field of science, engineering and technology, where the structure of multivariate data needs to be scrutinized. Since the theoretical results are broad and not restricted to a particular algorithm, it should unlock new research opportunities in methods development for data analysis, including undergraduate research.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> </div> </div> </div><br> <p>            Last Modified: 12/29/2017<br>      Modified by: Friedrich&nbsp;T&nbsp;Sommer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Sparse coding or sparse dictionary learning has been widely used to recover underlying structure in many kinds of natural data, such as images, sound and other measurement data. The result of sparse coding is a decomposition of the original data into different components, just based on the intrinsic statistical struture. Often these components directly reveal interpretable substructure in the data, and thus sparse coding can be a very valuable tool in data analysis. Specifically, sparse coding reveals a product structure of the data - a dictionary, that is, typical structure primitives of the signals, multiplied with sparse coefficients, specifying which data instances contain certain primitives.       Intellectual merit:  In this grant we investigated in theory, under which circumstances the decomposition provided by sparse coding is universal; that is, when sparse codes and dictionaries are unique (up to natural symmetries). If the uniqueness condition is violated, the result of a sparse coding analysis cannot be trusted. We were able to derive bounds on the sample sizes guaranteeing such uniqueness under various assumptions for how training data are generated. Whenever the conditions to one of our theorems are met, any sparsity-constrained learning algorithm that succeeds in reconstructing the data recovers the original sparse codes and dictionary.  Further we were able to generalize the uniqueness guarantee to the case where the data are approximated by the sparse decomposition with finite precision. This generalization is crucial to any application of sparse decomposition methods to real-world measurement, in which finite precision is inherent.  Finally, we were able to demionstrate impacts that these results have in practice, by analyzing concrete data sets. In particular, we demonstrated the practical value of sparse signal decomposition in the analysis of human electrocorticography (ECoG) data, as well as multielectrode recordings in the hippocampus of rodent.      Broader impact:  The theoretical foundation of the application of sparse coding in data analysis has broad implications to many field of science, engineering and technology, where the structure of multivariate data needs to be scrutinized. Since the theoretical results are broad and not restricted to a particular algorithm, it should unlock new research opportunities in methods development for data analysis, including undergraduate research.                 Last Modified: 12/29/2017       Submitted by: Friedrich T Sommer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
