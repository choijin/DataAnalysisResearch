<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Differentially-Private Machine Learning with Applications to Biomedical Informatics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>490625.00</AwardTotalIntnAmount>
<AwardAmount>490625</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Machine learning on large-scale patient medical records can lead to the discovery of novel population-wide patterns enabling advances in genetics, disease mechanisms, drug discovery, healthcare policy, and public health. However, concerns over patient privacy prevent biomedical researchers from running their algorithms on large volumes of patient data, creating a barrier to important new discoveries through machine-learning. &lt;br/&gt;&lt;br/&gt;The goal of this project is to address this barrier by developing privacy-preserving tools to query, cluster, classify and analyze medical databases. In particular, the project aims to ensure differential privacy --- a formal mathematical notion of privacy designed by cryptographers which has gained considerable attention in the systems, algorithms, machine-learning and data-mining communities in recent years.  The primary challenge in applying differentially-private machine learning tools to biomedical informatics is the lack of statistical efficiency, or the large number of samples required.&lt;br/&gt;&lt;br/&gt;The project will overcome this challenge by drawing on insights obtained from the PI's expertise to develop differentially-private and highly statistically-efficient machine learning tools for classification and clustering. The proposed research will advance the state-of-the-art in privacy-preserving data analysis by combining insights from differential privacy, statistics, machine learning, and database algorithms. &lt;br/&gt;&lt;br/&gt;The proposed research is closely tied to the development of the undergraduate and graduate curricula at UCSD, feeding into the PI's new undergraduate machine learning class, a new graduate learning theory class, and updates to an algorithm design and analysis class.  The corresponding materials will be publicly disseminated through the PI's website.  The PI is strongly committed to increasing the participation of women and minorities, and will engage in outreach activities to attract and retain women in computer science.</AbstractNarration>
<MinAmdLetterDate>02/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/30/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1253942</AwardID>
<Investigator>
<FirstName>Kamalika</FirstName>
<LastName>Chaudhuri</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kamalika Chaudhuri</PI_FULL_NAME>
<EmailAddress>kamalika@cs.ucsd.edu</EmailAddress>
<PI_PHON>8588227703</PI_PHON>
<NSF_ID>000573596</NSF_ID>
<StartDate>02/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>San Diego</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930404</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~98670</FUND_OBLG>
<FUND_OBLG>2014~103184</FUND_OBLG>
<FUND_OBLG>2015~107082</FUND_OBLG>
<FUND_OBLG>2016~112254</FUND_OBLG>
<FUND_OBLG>2017~69435</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A great deal of data analysis is currently carried out on sensitive data, such as medical information, financial records and search logs, and it has been shown that ,odels trained on this kind of data have the potential of leaking sensitive information about the people in the training set. The goal of this project is design, analyze and implement of novel machine learning algorithms that do not leak such information. Specifically, we aim to design algorithms that enforce a rigorous guarantee of differential privacy -- that ensures that the participation of a single person in the dataset does not change the probability of any outcome by much.&nbsp;</p> <p>The project has led to the development of private algorithms for a large number of tasks. This includes generic private algorithmic techniques, and private machine learning algorithms for classification, stochastic optimization and Bayesian inference. Additionally, we have branched out beyond differential privacy, and designed mechanisms that offer privacy on correlated data -- such as those encountered in location privacy applications.</p> <p>The project has also partially supported three graduate students including a woman graduate student; one of the students has already completed her PhD, and the other two are on their way to graduation. Support from the project has enabled the students to attend conferences and give talks on their work. Finally, in addition to multiple invited talks, support from the project has enabled the PI to give two invited tutorials on privacy at major signal processing and machine learning venues.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/29/2019<br>      Modified by: Kamalika&nbsp;Chaudhuri</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A great deal of data analysis is currently carried out on sensitive data, such as medical information, financial records and search logs, and it has been shown that ,odels trained on this kind of data have the potential of leaking sensitive information about the people in the training set. The goal of this project is design, analyze and implement of novel machine learning algorithms that do not leak such information. Specifically, we aim to design algorithms that enforce a rigorous guarantee of differential privacy -- that ensures that the participation of a single person in the dataset does not change the probability of any outcome by much.   The project has led to the development of private algorithms for a large number of tasks. This includes generic private algorithmic techniques, and private machine learning algorithms for classification, stochastic optimization and Bayesian inference. Additionally, we have branched out beyond differential privacy, and designed mechanisms that offer privacy on correlated data -- such as those encountered in location privacy applications.  The project has also partially supported three graduate students including a woman graduate student; one of the students has already completed her PhD, and the other two are on their way to graduation. Support from the project has enabled the students to attend conferences and give talks on their work. Finally, in addition to multiple invited talks, support from the project has enabled the PI to give two invited tutorials on privacy at major signal processing and machine learning venues.           Last Modified: 08/29/2019       Submitted by: Kamalika Chaudhuri]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
