<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Design and Implementation of Algorithms for an Experimental High-Radix Network Switching system</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>149888.00</AwardTotalIntnAmount>
<AwardAmount>149888</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Technology and economic trends in computer hardware have led to the widespread adoption of extremely large clusters of multicore servers as today?s supercomputers. As applications are modified to exploit the increased parallelism of these additional nodes and cores, the application?s network messages typically become both smaller and more frequent.  Although the bandwidth of the interconnect networks in current supercomputer systems is almost keeping up with increases in compute performance, there has been little improvement in the overhead of sending messages, and correspondingly little improvement in the throughput of the network when sending short messages.  As a consequence of these two trends, many applications scale poorly on existing supercomputing systems, and many more applications are expected to scale poorly as the industry moves forward with even larger supercomputer systems. &lt;br/&gt;&lt;br/&gt;In this study, TACC is investigating a possible solution to these issues with a new network switch and network interface architecture that can sustain full network bandwidth using very short messages.  This project is investigating the programming models required to use this network efficiently and is evaluating the performance of the new interconnect network in direct comparison with a modern (quad-data-rate Infiniband) interconnect that is widely used in current supercomputers.&lt;br/&gt;&lt;br/&gt;The evaluation of the new system is being conducted through a number of case studies. Implementations of algorithms known to exhibit poor parallel scaling on standard systems due to network performance limitations are being ported to the new system and instrumented with timers and hardware performance counters to document the detailed performance characteristics. This study will provide an initial evaluation of the technical viability of the current implementation of this new architecture for these algorithms, and is expected to provide recommendations for future enhancements to the architecture.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1240652</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>McCalpin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John McCalpin</PI_FULL_NAME>
<EmailAddress>mccalpin@tacc.utexas.edu</EmailAddress>
<PI_PHON>5122323754</PI_PHON>
<NSF_ID>000539973</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787137726</ZipCode>
<StreetAddress><![CDATA[PO Box 7726]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>21</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX21</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>L121</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~149888</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>For the past decade, most supercomputers used in high performance scientific computing have not been custom-designed systems, but have been assembled as "clusters" of hundreds (initially) to thousands (currently) of general-purpose computers (referred to as "compute nodes"), often linked together by a specialized high-performance network.&nbsp; The most popular high-performance networks in recent systems are designed to deliver messages from one compute node to another in exactly the same order in which they were sent.&nbsp; This has the advantage of making software easier to write, but has the disadvantage that messages are more likely to interfere with each other, causing subsequent messages to be delayed, and therefore slowing down the programs that are waiting on those messages.&nbsp;&nbsp; On clusters made up of thousands of compute nodes, it is not unusual for these "in order" networks to deliver only 1/10 to 1/2 of the expected data transfer rates, and it is not generally possible to predict what level of performance will be provided by a specific combination of programs running on the system.&nbsp; Both the loss of performance and the inability to predict the loss of performance reduce the productivity of supercomputers using these networks, and discourage users from running applications that spend a large fraction of their time transferring data across the network.</p> <p>For this project, we evaluated the issues involved in using a network that does not guarantee that messages are delivered in order, and that uses what amounts to random routing of messages to minimize the ability of messages to interfere with each other.&nbsp; This enables much higher utilization of the network, but means that extra work is required if the program needs to receive the messages in the same order that they were sent.&nbsp;&nbsp; Most programs don't require all messages to arrive in order, but almost all programs require that some specific sequences of messages arrive in the order in which they were sent.&nbsp;&nbsp; Rather than forcing all messages to arrive in order, we considered what additional hardware and software mechanisms were required to deliver particular messages in order, but only when "in-order delivery" was requested.</p> <p>From previous work we knew that we could force messages to be delivered in order by sending the first message, waiting for an acknowledgment from the recipient, then sending the second message.&nbsp; We also knew that these extra delays while waiting for acknowledments could (in some cases) cause a greater loss in performance than using a network with guaranteed "in-order" delivery.&nbsp; What we discovered during this project was that certain mathematical proofs about the theoretical properties of message-passing systems could be used to point us to a much more efficient way to provide in order delivery when it is needed.&nbsp; Instead of waiting for round trip acknowledgments, most of the time we are able to combine the use of "sequence numbers" added to the messages with the use of multiple receiving buffers to eliminate the round trip synchronizations.&nbsp; The "sequence numbers" are set by the sending compute node and are used by either hardware or software on the network interface of the receiving compute node to specify the order in which the messages should be delivered to the processor(s). We combine this with the use of multiple buffers to allow a compute node to continue working after sending a message and then to send additional messages to the receiving compute node without waiting to find out if the receiving node had finished reading the first set of messages.&nbsp; Although round trips are still required in some special cases (such as coordinating the startup of a program running on multiple compute nodes), our analysis shows that the overhead required to enable in-order message delivery should almost always be much smalle...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ For the past decade, most supercomputers used in high performance scientific computing have not been custom-designed systems, but have been assembled as "clusters" of hundreds (initially) to thousands (currently) of general-purpose computers (referred to as "compute nodes"), often linked together by a specialized high-performance network.  The most popular high-performance networks in recent systems are designed to deliver messages from one compute node to another in exactly the same order in which they were sent.  This has the advantage of making software easier to write, but has the disadvantage that messages are more likely to interfere with each other, causing subsequent messages to be delayed, and therefore slowing down the programs that are waiting on those messages.   On clusters made up of thousands of compute nodes, it is not unusual for these "in order" networks to deliver only 1/10 to 1/2 of the expected data transfer rates, and it is not generally possible to predict what level of performance will be provided by a specific combination of programs running on the system.  Both the loss of performance and the inability to predict the loss of performance reduce the productivity of supercomputers using these networks, and discourage users from running applications that spend a large fraction of their time transferring data across the network.  For this project, we evaluated the issues involved in using a network that does not guarantee that messages are delivered in order, and that uses what amounts to random routing of messages to minimize the ability of messages to interfere with each other.  This enables much higher utilization of the network, but means that extra work is required if the program needs to receive the messages in the same order that they were sent.   Most programs don't require all messages to arrive in order, but almost all programs require that some specific sequences of messages arrive in the order in which they were sent.   Rather than forcing all messages to arrive in order, we considered what additional hardware and software mechanisms were required to deliver particular messages in order, but only when "in-order delivery" was requested.  From previous work we knew that we could force messages to be delivered in order by sending the first message, waiting for an acknowledgment from the recipient, then sending the second message.  We also knew that these extra delays while waiting for acknowledments could (in some cases) cause a greater loss in performance than using a network with guaranteed "in-order" delivery.  What we discovered during this project was that certain mathematical proofs about the theoretical properties of message-passing systems could be used to point us to a much more efficient way to provide in order delivery when it is needed.  Instead of waiting for round trip acknowledgments, most of the time we are able to combine the use of "sequence numbers" added to the messages with the use of multiple receiving buffers to eliminate the round trip synchronizations.  The "sequence numbers" are set by the sending compute node and are used by either hardware or software on the network interface of the receiving compute node to specify the order in which the messages should be delivered to the processor(s). We combine this with the use of multiple buffers to allow a compute node to continue working after sending a message and then to send additional messages to the receiving compute node without waiting to find out if the receiving node had finished reading the first set of messages.  Although round trips are still required in some special cases (such as coordinating the startup of a program running on multiple compute nodes), our analysis shows that the overhead required to enable in-order message delivery should almost always be much smaller than the benefit that we expect from using an "out of order" network that operates more efficiently (because of reduced interference between messag...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
