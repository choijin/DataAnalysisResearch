<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRCNS: Collaborative Research: Neural Correlates of Hierarchical Reinforcement Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>43337.00</AwardTotalIntnAmount>
<AwardAmount>43337</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>aude oliva</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Research on human behavior has long emphasized its hierarchical structure:  Simple actions group together into subtask sequences, and these in turn cohere to bring about higher-level goals.  This hierarchical structure is critical to humans' unique ability to tackle complex, large-scale tasks, since it allows such tasks to be decomposed or broken down into more manageable parts. While some progress has been made toward understanding the origins and mechanisms of hierarchical behavior, key questions remain: How are task-subtask-action hierarchies initially assembled through learning?  How does learning operate within such hierarchies, allowing adaptive hierarchical behavior to take shape?  How do the relevant learning and action-selection processes play out in neural hardware?  &lt;br/&gt;&lt;br/&gt;To pursue these questions, the present proposal will leverage ideas emerging from the computational framework of Hierarchical Reinforcement Learning (HRL). HRL builds on a highly successful machine-learning paradigm known as reinforcement learning (RL), extending it to include task-subtask-action hierarchies. Recent neuroscience and behavioral research has suggested that standard RL mechanisms may be directly relevant to reward-based learning in humans and animals. The present proposal hypothesizes that the mechanisms introduced in computational HRL may be similarly relevant, providing insight into the cognitive and neural underpinnings of hierarchical behavior. &lt;br/&gt;&lt;br/&gt;The project brings together two computational cognitive neuroscientists and a computer scientist with expertise in machine learning.  The proposed research, which includes both computational modeling and human functional neuroimaging and behavioral studies, pursues a set of hypotheses drawn directly from HRL research.  A first set of hypotheses relates to the question of how complex tasks are decomposed into manageable subtasks. Here, fMRI and computational work will leverage the idea, drawn from HRL research, that useful decompositions "carve" tasks at points identifiable through graph-theoretic measures of centrality.  A second set of hypotheses relates to the question of how learning occurs within hierarchies.  Here, fMRI and modeling work will pursue the idea that hierarchical learning may be driven by reward prediction errors akin to those arising within the HRL framework.  The project as a whole aims to construct a biologically constrained neural network model, translating computational HRL into an account of how the brain supports hierarchically structured behavior.</AbstractNarration>
<MinAmdLetterDate>09/13/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/16/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208051</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Barto</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew G Barto</PI_FULL_NAME>
<EmailAddress>barto@cs.umass.edu</EmailAddress>
<PI_PHON>4135452109</PI_PHON>
<NSF_ID>000201472</NSF_ID>
<StartDate>09/13/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Amherst</CityName>
<StateCode>MA</StateCode>
<ZipCode>010039264</ZipCode>
<StreetAddress><![CDATA[Computer Sci, 140 Governors Dr]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramReference>
<Code>7327</Code>
<Text>CRCNS</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~12990</FUND_OBLG>
<FUND_OBLG>2013~30347</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>Human behavior displays hierarchical structure: Goal-directed tasks (e.g., make coffee) comprise discrete subtasks (e.g., add cream), which in turn comprise basic actions (e.g., pick up spoon). It is a longstanding challenge in both psychology and neuroscience to understand the computational processes and neural mechanisms that underlie behavioral hierarchy. In the present project, we pursued this objective by leveraging recent developments in machine learning. Earlier work has established a link between human decision making and a machine-learning framework known as reinforcement learning (RL), wherein action selection is shaped by reward feedback. We investigated the relevance of an extension of RL known as hierarchical reinforcement learning (HRL), which generalizes the machinery of RL to engage hierarchical structure. Using behavioral and fMRI research, as well as computational analysis, we investigated whether human hierarchical behavior might arise from processes akin to those involved in HRL. Two initial experiments using functional magnetic resonance imaging (fMRI), reported in the journals </span><span>Neuron </span><span>and </span><span>Journal of Neuroscience</span><span>, provided evidence for neural prediction-error signals specifically predicted by HRL. The uncovering of these signals provides insight into how the brain selects actions within an established task/subtask hierarchy. Building on this foundation, the focus of the project turned to the question of how such action hierarchies might emerge through learning. In a formal analysis published in </span><span>PLoS Computational Biology</span><span>, we specified a method for identifying the optimal form for the action hierarchy in any given task domain. Accompanying behavioral experiments provided evidence that human learners approximate the optimal solution. In a subsequent analysis, published in </span><span>Philosophical Transactions of the Royal Society</span><span>, we established a connection between optimal task decomposition and the principle of 'efficient coding,' which has been widely applied in neuroscientific studies of perception. From here, the project inquired into the specific learning procedures whereby hierarchical task decompositions might emerge. Our hypothesis, formalized in analyses presented in </span><span>Neural Information Processing Systems</span><span>, was that hierarchical structure might be identified as a by-product of prospective coding, that is, representation of the current situation in terms of its predictions for future events. This hypothesis led to novel predictions concerning event coding in prefrontal cortex and hippocampus, which were confirmed in an fMRI study reported in </span><span>Nature Neuroscience </span><span>and </span><span>Hippocampus</span><span>. A follow-up study focusing on the classic planning task referred to as the 'Tower of Hanoi,' is currently underway. In a final piece, the project examined how multi- step planning plays out over time. In work reported in the journal </span><span>PNAS</span><span>, we presented evidence that the temporal profile of multi-step decisions is well captured by 'drift-diffusion' models that have been highly successful in explaining simpler, one-step actions.</span></p> </div> </div> </div> </div><br> <p>            Last Modified: 12/13/2015<br>      Modified by: Andrew&nbsp;G&nbsp;Barto</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     Human behavior displays hierarchical structure: Goal-directed tasks (e.g., make coffee) comprise discrete subtasks (e.g., add cream), which in turn comprise basic actions (e.g., pick up spoon). It is a longstanding challenge in both psychology and neuroscience to understand the computational processes and neural mechanisms that underlie behavioral hierarchy. In the present project, we pursued this objective by leveraging recent developments in machine learning. Earlier work has established a link between human decision making and a machine-learning framework known as reinforcement learning (RL), wherein action selection is shaped by reward feedback. We investigated the relevance of an extension of RL known as hierarchical reinforcement learning (HRL), which generalizes the machinery of RL to engage hierarchical structure. Using behavioral and fMRI research, as well as computational analysis, we investigated whether human hierarchical behavior might arise from processes akin to those involved in HRL. Two initial experiments using functional magnetic resonance imaging (fMRI), reported in the journals Neuron and Journal of Neuroscience, provided evidence for neural prediction-error signals specifically predicted by HRL. The uncovering of these signals provides insight into how the brain selects actions within an established task/subtask hierarchy. Building on this foundation, the focus of the project turned to the question of how such action hierarchies might emerge through learning. In a formal analysis published in PLoS Computational Biology, we specified a method for identifying the optimal form for the action hierarchy in any given task domain. Accompanying behavioral experiments provided evidence that human learners approximate the optimal solution. In a subsequent analysis, published in Philosophical Transactions of the Royal Society, we established a connection between optimal task decomposition and the principle of 'efficient coding,' which has been widely applied in neuroscientific studies of perception. From here, the project inquired into the specific learning procedures whereby hierarchical task decompositions might emerge. Our hypothesis, formalized in analyses presented in Neural Information Processing Systems, was that hierarchical structure might be identified as a by-product of prospective coding, that is, representation of the current situation in terms of its predictions for future events. This hypothesis led to novel predictions concerning event coding in prefrontal cortex and hippocampus, which were confirmed in an fMRI study reported in Nature Neuroscience and Hippocampus. A follow-up study focusing on the classic planning task referred to as the 'Tower of Hanoi,' is currently underway. In a final piece, the project examined how multi- step planning plays out over time. In work reported in the journal PNAS, we presented evidence that the temporal profile of multi-step decisions is well captured by 'drift-diffusion' models that have been highly successful in explaining simpler, one-step actions.           Last Modified: 12/13/2015       Submitted by: Andrew G Barto]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
