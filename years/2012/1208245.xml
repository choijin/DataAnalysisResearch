<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI-Small: Expert-Apprentice Collaboration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>746924.00</AwardTotalIntnAmount>
<AwardAmount>746924</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recent advances in robot platforms have outpaced our ability to effectively program robots to accomplish useful tasks, often in complex environments that they share with humans.   In order for flexible, general purpose robots to become widespread e.g., in teaching skills to children, assisting the elderly, there must be a way of interacting with them beyond programming. Teaching by demonstration offers a potentially powerful and practical approach to realizing the promise of large scale personal robotics in a wide range of applications. In teaching by demonstration, the expert (human), demonstrates the task on different hardware than what the apprentice or student (robot) uses. &lt;br/&gt;&lt;br/&gt;The project aims to develop visual feature-based methods that allow robots to teach humans and learn from them by unifying apprenticeship learning, learning by demonstration (or by imitation) and teaching humans, taking into account the differences between experts and apprentices. The resulting system will be evaluated on a PR2 robot (mostly on grasping and manipulation tasks). The scientific advances resulting from the project in learning from demonstration and imitation learning, both general techniques with broad applicability, will greatly simplify the programming of robots which would make it easier for non-expert users to perform this important task which currently requires considerable expertise in robotics as well as computer science. &lt;br/&gt;&lt;br/&gt;Broader impacts of the research include development of new robotics curricula, enhanced opportunities for research-based interdisciplinary training at the intersection of computer vision, machine learning, and robotics,  outreach  activities (including participation in a public school robotics instruction program). All of the results of the research, including publications, open-source software and datasets, will be made freely available to the larger scientific and academic community.</AbstractNarration>
<MinAmdLetterDate>09/11/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/19/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208245</AwardID>
<Investigator>
<FirstName>Carlo</FirstName>
<LastName>Tomasi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carlo Tomasi</PI_FULL_NAME>
<EmailAddress>tomasi@cs.duke.edu</EmailAddress>
<PI_PHON>9196606539</PI_PHON>
<NSF_ID>000107168</NSF_ID>
<StartDate>09/11/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ronald</FirstName>
<LastName>Parr</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ronald Parr</PI_FULL_NAME>
<EmailAddress>parr@cs.duke.edu</EmailAddress>
<PI_PHON>9196606537</PI_PHON>
<NSF_ID>000188767</NSF_ID>
<StartDate>09/11/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277080129</ZipCode>
<StreetAddress><![CDATA[Computer Science Department]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~746924</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>This grant has resulted in advances in the research needed to let people and robots interact in natural and seamless ways. This interaction is crucial both in manufacturing and in situations were individuals are assisted by robots in the home or other environments. Currently, robots are painstakingly programmed by highly skilled robotics experts and programmers to accomplish precisely defined tasks in tightly controlled environments. The research funded by this grant is a significant step towards a more natural, less expensive, and more flexible method for people and robots to work together.</span></p> <p><span>Specifically, this research provides initial developments that enable robots to observe people perform a task and learn how the task should be performed, even if some of the conditions under which it is to be carried out change. Some of these developments concern passive observation: Robots can discover what in a given environment is an &ldquo;object:&rdquo; something that can be manipulated, moved, and possibly be applied to a task. They can also track how a person uses her hands to accomplish some task.</span></p> <p><span>Other developments use these passive observations by learning a policy that a person is following while accomplishing a task. Rather than a verbatim description of the exact motions involved, the inferred policy encapsulates principles and goals that underlie what is being done. This inference provides a crucial level of indirection that allows performing similar tasks in different scenarios and surroundings, thereby providing a degree of flexibility and resilience to environmental changes.</span></p> <p><span>By facilitating person-robot interaction, this research has the added potential to ease the training of the workforce to be employed in manufacturing, and to make robotic applications easier and safer to deploy in both factory and home.</span></p> <p><span>Four graduate and one undergraduate student worked on research related to this project. Of the four graduate students, one is currently employed as a researcher and works on topics closely related to this research. Another has been admitted to law school and plans to work on the intersection of legal and technical aspects of human-robot interaction. Two more graduate students are pursuing their PhDs in computer science in areas closely related to the research developed under this grant. The undergraduate student is now a PhD candidate.</span></p> <p><span>Outcomes from the research carried out during this grant were published in various venues and are also being incorporated in courses on machine learning, artificial intelligence, and computer vision, at both the graduate and undergraduate level. Physical infrastructure built for this research continues to be used by students even after the expiration of the grant.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 12/04/2017<br>      Modified by: Carlo&nbsp;Tomasi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This grant has resulted in advances in the research needed to let people and robots interact in natural and seamless ways. This interaction is crucial both in manufacturing and in situations were individuals are assisted by robots in the home or other environments. Currently, robots are painstakingly programmed by highly skilled robotics experts and programmers to accomplish precisely defined tasks in tightly controlled environments. The research funded by this grant is a significant step towards a more natural, less expensive, and more flexible method for people and robots to work together.  Specifically, this research provides initial developments that enable robots to observe people perform a task and learn how the task should be performed, even if some of the conditions under which it is to be carried out change. Some of these developments concern passive observation: Robots can discover what in a given environment is an "object:" something that can be manipulated, moved, and possibly be applied to a task. They can also track how a person uses her hands to accomplish some task.  Other developments use these passive observations by learning a policy that a person is following while accomplishing a task. Rather than a verbatim description of the exact motions involved, the inferred policy encapsulates principles and goals that underlie what is being done. This inference provides a crucial level of indirection that allows performing similar tasks in different scenarios and surroundings, thereby providing a degree of flexibility and resilience to environmental changes.  By facilitating person-robot interaction, this research has the added potential to ease the training of the workforce to be employed in manufacturing, and to make robotic applications easier and safer to deploy in both factory and home.  Four graduate and one undergraduate student worked on research related to this project. Of the four graduate students, one is currently employed as a researcher and works on topics closely related to this research. Another has been admitted to law school and plans to work on the intersection of legal and technical aspects of human-robot interaction. Two more graduate students are pursuing their PhDs in computer science in areas closely related to the research developed under this grant. The undergraduate student is now a PhD candidate.  Outcomes from the research carried out during this grant were published in various venues and are also being incorporated in courses on machine learning, artificial intelligence, and computer vision, at both the graduate and undergraduate level. Physical infrastructure built for this research continues to be used by students even after the expiration of the grant.          Last Modified: 12/04/2017       Submitted by: Carlo Tomasi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
