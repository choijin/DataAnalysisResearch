<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: PROTEUS: A Practical and Rigorous Toolkit for Privacy</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2013</AwardEffectiveDate>
<AwardExpirationDate>01/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>659996.00</AwardTotalIntnAmount>
<AwardAmount>659996</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dan Cosley</SignBlockName>
<PO_EMAI>dcosley@nsf.gov</PO_EMAI>
<PO_PHON>7032928832</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Statistical privacy, or the problem of disclosing aggregate statistics about data collected from individuals while ensuring the privacy of individual level sensitive properties, is an important problem in today's age of big data. The key challenge in statistical privacy is that applications for data collection and analysis operate on varied kinds of data, and have diverse requirements for the information that must be kept secret, and the adversaries that they must tolerate. Thus, application domain experts, who are frequently not experts in privacy, cannot directly use an existing, general-purpose privacy definition. Instead, they must develop a new privacy definition or customize an existing one. Currently there exist no rigorous techniques to customize privacy to applications.&lt;br/&gt;&lt;br/&gt;This project builds PROTEUS, a general-purpose toolkit for developing rigorous privacy definitions and mechanisms that can be customized to applications. The cornerstone of PROTEUS is a novel privacy framework that allows customized privacy protection by explicitly listing the secrets to be protected, enumerating the (potentially infinite set of) possible adversaries, and ensuring rigorous bounds on the information disclosed to each adversary about every secret. Novel theoretical tools in PROTEUS include methods to reason about privacy for correlated data, privacy against realistic adversaries, and techniques to express and enforce personalized privacy preferences. These tools result in practical privacy mechanisms for publishing social science survey data, social network analysis, and analysis of user-activity streams. Broader impacts of this project include developing new courses in privacy and big-data management, as well as technology transfer to the US Census.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/23/2013</MinAmdLetterDate>
<MaxAmdLetterDate>02/02/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1253327</AwardID>
<Investigator>
<FirstName>Ashwin</FirstName>
<LastName>Machanavajjhala</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ashwin Machanavajjhala</PI_FULL_NAME>
<EmailAddress>ashwin@cs.duke.edu</EmailAddress>
<PI_PHON>9196843030</PI_PHON>
<NSF_ID>000624095</NSF_ID>
<StartDate>01/23/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName/>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress><![CDATA[2200 W. Main St, Suite 710]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~124280</FUND_OBLG>
<FUND_OBLG>2014~260123</FUND_OBLG>
<FUND_OBLG>2016~135816</FUND_OBLG>
<FUND_OBLG>2017~139777</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we developed general purpose tools for developing rigorous privacy definitions and mechanisms for sharing sensitive data. Our contributions are three-fold. First, we <strong>developed the theory behind customizable privacy frameworks </strong>like Pufferfish, Blowfish, One sided Privacy and Capacity Bounded Privacy. These privacy frameworks extend differential privacy, the gold standard for privacy, to application settings where the data are not a single table, when the sensitive information can not be capture by a single row,&nbsp; when there are constraints/correlations between individual records that an adversary can use to unravel the privacy guarantee, when protecting one-sided secrets or when tolerating adversaries with limited learning power. Second, we <strong>developed mechanisms with customized privacy </strong>for several applications. We used Pufferfish to model Title 13 legislation that governs how data about people and establishments is shared by the US Census Bureau. We developed privacy mechanisms that match this customized privacy definition. The error resulting from these mechanisms were evaluated on a real linked employee and employer data product and compared to existing statistical disclosure limitation methods. These mechanisms are now being used by the US Census Bureau for a new version of the Business Dynamics Statistics (BDS) data product to be released in 2020. We also used Blowfish to correctly define and analyze privacy in scenarios where differentially private algorithms are composed with secure multiparty computation primitives. We used one-sided privacy to model privacy of individuals in IoT systems and investigated the use of capacity bounded privacy in the context of machine learnt adversaries. Third, we developed several <strong>general-purpose systems</strong> for private data analytics. We built DPT, a system for releasing synthetic data from database that track location trajectories of individuals. We also developed systems for differentially private answering of SQL queries over untrusted database servers by combining differential privacy with secure multiparty computation and homomorphic encryption with end to end privacy guarantees. The project helped trained two PhD students at Duke University. Both these students have finished their degrees, one of whom has gone on to become an Assistant Professor at University of Waterloo. This project also led to the development on innovative and award-winning courses on data science and privacy at Duke University, as well as a tutorial on Differential Privacy that was presented at top tier database conferences. All teaching material is publicly available.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/06/2019<br>      Modified by: Ashwin&nbsp;Machanavajjhala</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we developed general purpose tools for developing rigorous privacy definitions and mechanisms for sharing sensitive data. Our contributions are three-fold. First, we developed the theory behind customizable privacy frameworks like Pufferfish, Blowfish, One sided Privacy and Capacity Bounded Privacy. These privacy frameworks extend differential privacy, the gold standard for privacy, to application settings where the data are not a single table, when the sensitive information can not be capture by a single row,  when there are constraints/correlations between individual records that an adversary can use to unravel the privacy guarantee, when protecting one-sided secrets or when tolerating adversaries with limited learning power. Second, we developed mechanisms with customized privacy for several applications. We used Pufferfish to model Title 13 legislation that governs how data about people and establishments is shared by the US Census Bureau. We developed privacy mechanisms that match this customized privacy definition. The error resulting from these mechanisms were evaluated on a real linked employee and employer data product and compared to existing statistical disclosure limitation methods. These mechanisms are now being used by the US Census Bureau for a new version of the Business Dynamics Statistics (BDS) data product to be released in 2020. We also used Blowfish to correctly define and analyze privacy in scenarios where differentially private algorithms are composed with secure multiparty computation primitives. We used one-sided privacy to model privacy of individuals in IoT systems and investigated the use of capacity bounded privacy in the context of machine learnt adversaries. Third, we developed several general-purpose systems for private data analytics. We built DPT, a system for releasing synthetic data from database that track location trajectories of individuals. We also developed systems for differentially private answering of SQL queries over untrusted database servers by combining differential privacy with secure multiparty computation and homomorphic encryption with end to end privacy guarantees. The project helped trained two PhD students at Duke University. Both these students have finished their degrees, one of whom has gone on to become an Assistant Professor at University of Waterloo. This project also led to the development on innovative and award-winning courses on data science and privacy at Duke University, as well as a tutorial on Differential Privacy that was presented at top tier database conferences. All teaching material is publicly available.          Last Modified: 05/06/2019       Submitted by: Ashwin Machanavajjhala]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
