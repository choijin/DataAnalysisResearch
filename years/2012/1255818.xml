<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Cost-Effective Reliability Screening, Binning, and In-Field Adaptation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2013</AwardEffectiveDate>
<AwardExpirationDate>03/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>189000.00</AwardTotalIntnAmount>
<AwardAmount>189000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sankar Basu</SignBlockName>
<PO_EMAI>sabasu@nsf.gov</PO_EMAI>
<PO_PHON>7032927843</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In the semiconductor industry, one way of measuring reliability is the expected lifetime of a product. As semiconductor manufacturing technologies continue to advance, ensuring reliability has become one of the major challenges in the industry. The ability to ensure reliability is essential for the success of a product line as well as the success of the industry. &lt;br/&gt;&lt;br/&gt;This project proposes to develop a first cost-effective solution for screening unreliable parts before product shipment to the customer. The project proposes to develop software tools and methodologies for predicting the expected life time of a part. Further, the project proposes to develop a second cost-effective solution for continuously monitoring and improving reliability while a part is being used in the field. Additional software tools and methodologies will be developed for building models for such continuous monitoring. These monitoring models are then used for extending the life time of a part by automatic adjusting the operational parameters such as speed and voltage of the part based on its current health condition.  &lt;br/&gt;   &lt;br/&gt;The research is integrated with educational activities to develop course and tutorial materials for broad impact, a state-of-the-art laboratory for education, and a research program to attract undergraduate and underrepresented students. The research strives to achieve a comprehensive understanding of state-of-the-art industrial practices and to accomplish multidisciplinary studies merging knowledge from reliability physics, semiconductor testing, and data mining. Knowledge discovered through this research will provide the industry with a clear direction on how to cope with the present and future reliability challenges.</AbstractNarration>
<MinAmdLetterDate>01/28/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/16/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1255818</AwardID>
<Investigator>
<FirstName>Li-Chung</FirstName>
<LastName>Wang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Li-Chung Wang</PI_FULL_NAME>
<EmailAddress>licwang@ece.ucsb.edu</EmailAddress>
<PI_PHON>8058866017</PI_PHON>
<NSF_ID>000492566</NSF_ID>
<StartDate>01/28/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Barbara</Name>
<CityName>Santa Barbara</CityName>
<ZipCode>931062050</ZipCode>
<PhoneNumber>8058934188</PhoneNumber>
<StreetAddress>Office of Research</StreetAddress>
<StreetAddress2><![CDATA[Rm 3227 Cheadle Hall]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA24</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>094878394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA BARBARA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[ELEG]]></Name>
<CityName>Santa Barbara</CityName>
<StateCode>CA</StateCode>
<ZipCode>931062050</ZipCode>
<StreetAddress><![CDATA[2164 Harold Frank Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA24</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8081</Code>
<Text>Failure Resistant Systems(FRS)</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~126000</FUND_OBLG>
<FUND_OBLG>2014~31500</FUND_OBLG>
<FUND_OBLG>2015~31500</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was proposed to develop the tools and methodologies for building data-driven models aiming to improve the quality and reliability of semiconductor parts. The models are built based on on-chip or off-chip test measurements and can be used to predict that a part is defective. Along the course of the research, we discovered that for the intended test applications, building such models involves solving an unsupervised learning problem such as outlier analysis. And the main challenge for solving such a problem is the subjectivity involved in choosing an analytics tool and its modeling parameters. This subjectivity makes the model development process non-robust.</p> <p>The non-robustness is reflected in two undesirable phenomena observed in our experiments based on test data collected from multiple automotive product lines. First, behavior established with a model built on some initial dataset does not hold when the model is applied to a future dataset. A typical example is that an outlier model capturing very few parts as outliers in the initial dataset classifies unreasonably many more parts as outliers when it is applied to a future dataset. Second, models derived from different analytics tools disagree on their prediction. For example, different outlier models built using different methods can disagree largely on which parts should be the top outliers.</p> <p>In the research, we developed two novel approaches to overcome the non-robustness issue. The first approach is called consistency check. Consistency check takes advantage of the fact that test data collected in semiconductor manufacturing process is organized as a collection of wafers and ideally data characteristics on each wafer are expected to be similar. Consistency check quantifies the wafer-to-wafer noises and utilizes the noises to select admissible models. For example, an outlier model is admissible only if its outlier classification is invariant to the wafer-to-wafer noises. In our experiments, we demonstrate that the behavior of admissible models becomes much more consistent across different datasets. Furthermore, the admissible models given by different methods have much less disagreement. &nbsp;&nbsp;&nbsp;&nbsp;</p> <p>The second approach developed through the research is called applicability check. Throughout the research we came to realize that it is difficult, if not impossible, to determine the best outlier analysis method in a given test application context. The best method depends on the characteristics of the test data and can differ from wafer to wafer. The applicability check quantifies how applicable an outlier analysis method is to a given test dataset from a wafer. With applicability check, a best method can be determined for each wafer of data separately. Applicability check allows the possibility that none of the methods under consideration are applicable to a given wafer of data. In this situation, a warning can be issued to trigger further manual investigation. While consistency check tries to resolve differences among different methods, applicability check tries to determine which method is more trustable when different methods produce models that disagree with each other.</p> <p>The practical benefits of the two approaches have been demonstrated through extensive experiments based on test data from multiple production lines of semiconductor parts sold to the automotive market. While the approaches were developed for analytics of semiconductor manufacturing test data, they have a broader impact to the area of unsupervised learning in general. Take novelty detection as an example. Determination of a novel sample depends on the choice of model parameters and hence is subjective. Because of this subjectivity, it is challenging to determine that there is no novel sample in a given dataset. The consistency check enables automatic determination of no outlier in a dataset, i.e. no novel sample, and hence can be used to improve the robustness of novelty detection. The applicability check provides a practical solution to the situation where no single best learning algorithm exists, a situation suggested by the no-free-lunch theorem in machine learning. Therefore, the applicability check can be seen as a practical approach to mitigate the no-free-lunch situation in data analytics. Extension of the approaches to general learning problem settings requires further research. However, the experiment results and research findings provide a foundation to enable such exploration. Findings from this research have been published in several peer-reviewed papers and presented in leading test conferences. Knowledge discovered through the research was transferred to the leading semiconductor companies through research review presentations. Graduate students involved in the research have done multiple internships in Semiconductor Research Corporation member companies to carry out technology transfer of our research results into their respective production flows.</p><br> <p>            Last Modified: 06/30/2017<br>      Modified by: Li-Chung&nbsp;Wang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was proposed to develop the tools and methodologies for building data-driven models aiming to improve the quality and reliability of semiconductor parts. The models are built based on on-chip or off-chip test measurements and can be used to predict that a part is defective. Along the course of the research, we discovered that for the intended test applications, building such models involves solving an unsupervised learning problem such as outlier analysis. And the main challenge for solving such a problem is the subjectivity involved in choosing an analytics tool and its modeling parameters. This subjectivity makes the model development process non-robust.  The non-robustness is reflected in two undesirable phenomena observed in our experiments based on test data collected from multiple automotive product lines. First, behavior established with a model built on some initial dataset does not hold when the model is applied to a future dataset. A typical example is that an outlier model capturing very few parts as outliers in the initial dataset classifies unreasonably many more parts as outliers when it is applied to a future dataset. Second, models derived from different analytics tools disagree on their prediction. For example, different outlier models built using different methods can disagree largely on which parts should be the top outliers.  In the research, we developed two novel approaches to overcome the non-robustness issue. The first approach is called consistency check. Consistency check takes advantage of the fact that test data collected in semiconductor manufacturing process is organized as a collection of wafers and ideally data characteristics on each wafer are expected to be similar. Consistency check quantifies the wafer-to-wafer noises and utilizes the noises to select admissible models. For example, an outlier model is admissible only if its outlier classification is invariant to the wafer-to-wafer noises. In our experiments, we demonstrate that the behavior of admissible models becomes much more consistent across different datasets. Furthermore, the admissible models given by different methods have much less disagreement.       The second approach developed through the research is called applicability check. Throughout the research we came to realize that it is difficult, if not impossible, to determine the best outlier analysis method in a given test application context. The best method depends on the characteristics of the test data and can differ from wafer to wafer. The applicability check quantifies how applicable an outlier analysis method is to a given test dataset from a wafer. With applicability check, a best method can be determined for each wafer of data separately. Applicability check allows the possibility that none of the methods under consideration are applicable to a given wafer of data. In this situation, a warning can be issued to trigger further manual investigation. While consistency check tries to resolve differences among different methods, applicability check tries to determine which method is more trustable when different methods produce models that disagree with each other.  The practical benefits of the two approaches have been demonstrated through extensive experiments based on test data from multiple production lines of semiconductor parts sold to the automotive market. While the approaches were developed for analytics of semiconductor manufacturing test data, they have a broader impact to the area of unsupervised learning in general. Take novelty detection as an example. Determination of a novel sample depends on the choice of model parameters and hence is subjective. Because of this subjectivity, it is challenging to determine that there is no novel sample in a given dataset. The consistency check enables automatic determination of no outlier in a dataset, i.e. no novel sample, and hence can be used to improve the robustness of novelty detection. The applicability check provides a practical solution to the situation where no single best learning algorithm exists, a situation suggested by the no-free-lunch theorem in machine learning. Therefore, the applicability check can be seen as a practical approach to mitigate the no-free-lunch situation in data analytics. Extension of the approaches to general learning problem settings requires further research. However, the experiment results and research findings provide a foundation to enable such exploration. Findings from this research have been published in several peer-reviewed papers and presented in leading test conferences. Knowledge discovered through the research was transferred to the leading semiconductor companies through research review presentations. Graduate students involved in the research have done multiple internships in Semiconductor Research Corporation member companies to carry out technology transfer of our research results into their respective production flows.       Last Modified: 06/30/2017       Submitted by: Li-Chung Wang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
