<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Disparity and Correspondence in Human Stereo Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2013</AwardEffectiveDate>
<AwardExpirationDate>02/28/2017</AwardExpirationDate>
<AwardTotalIntnAmount>427622.00</AwardTotalIntnAmount>
<AwardAmount>435907</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>catherine arrington</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Due to their different locations, our left and right eyes have slightly different views of the world. It has been known since the 1830's that a horizontal disparity (shift) between the position of an image on the retinas of the left and right eyes is sufficient for humans to see that image in depth. Combining the two retinal images allows us to see stereoscopically the full three-dimensional volume of scenes (and even some movies and comic books). Stereoscopic vision helps people to navigate, to locate, identify and grasp objects, to judge distances, and to drive vehicles. The goal of this research project is to understand how the brain interprets retinal disparities as cues to stereoscopic depth. Horizontal disparities are relative easy to interpret; this is because of the separation of the eyes, when we are in upright posture, is horizontal. However, in naturalistic scenes disparities can have any direction, not just horizontal, and interpreting them is complicated by the variety of spatial arrangements among objects can influence the disparity directions we encounter. Therefore, understanding how humans see depth requires that we understand how the brain analyzes this two-dimensional (horizontal and vertical) disparity signal. The investigator will measure the depth that people perceive as they view displays containing several patterns, each with its own disparity direction. &lt;br/&gt;&lt;br/&gt;The knowledge gained from these studies will help scientists understand how humans and other animals combine images from the two eyes in order to recover information not available from either eye alone and how perception of 3-D space can be distorted when this information is combined incorrectly. Learning about the brain's strategies in achieving normal 3-D vision will help explain why visual areas of the brain are organized the way they are and aid in the design of treatments for impaired stereoscopic vision (from, for example, amblyopia and strabismus, or "lazy eye"). It will also help in the design of more effective artificial depth-sensing systems and improve machine-vision algorithms for object recognition, robotics, and visual prosthetic devices.</AbstractNarration>
<MinAmdLetterDate>02/12/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1257096</AwardID>
<Investigator>
<FirstName>Bart</FirstName>
<LastName>Farell</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bart Farell</PI_FULL_NAME>
<EmailAddress>bfarell@syr.edu</EmailAddress>
<PI_PHON>3154432807</PI_PHON>
<NSF_ID>000624261</NSF_ID>
<StartDate>02/12/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Syracuse University</Name>
<CityName>SYRACUSE</CityName>
<ZipCode>132441200</ZipCode>
<PhoneNumber>3154432807</PhoneNumber>
<StreetAddress>OFFICE OF SPONSORED PROGRAMS</StreetAddress>
<StreetAddress2><![CDATA[211 Lyman Hall]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY24</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002257350</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SYRACUSE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002257350</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Syracuse University]]></Name>
<CityName>Syracuse</CityName>
<StateCode>NY</StateCode>
<ZipCode>132445290</ZipCode>
<StreetAddress><![CDATA[621 Skytop Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY24</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~427622</FUND_OBLG>
<FUND_OBLG>2014~8285</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The visual world casts two-dimensional (2-D) images on the retinas at the backs of our eyes, but we see the world as three-dimensional.&nbsp; Seeing the world as fully 3-D requires the brain to interpret and combine the many cues to depth contained in those retinal images.&nbsp; A powerful cue is stereopsis.&nbsp; Stereopsis depends on the differences between the images cast on the retinas of the left and right eyes. It&rsquo;s what gives 3-D movies their extra realism.&nbsp; These images differ because the eyes are not in the same viewing location, but instead are horizontally separated.&nbsp; By comparing the retinal images of the left and right eyes, the visual system can infer the relative depth of the objects in a scene and under optimal conditions it can do this with very high precision.&nbsp; <br /><br />Since work done in the 1830&rsquo;s, it has been known that horizontal disparities (shifts) in the locations of image features between the two eyes are all that is needed for us to see stereoscopic depth.&nbsp; However, we and others have shown that horizontal disparities are not the whole story.&nbsp; For some visual patterns horizontal disparity does not play the expected role.&nbsp; Instead, the brain favors a disparity direction for these patterns that varies with the pattern&rsquo;s orientation. This is the case with 1-D patterns, such as lines, edges, and gratings.&nbsp; As a result, horizontal disparities allow us to see the stereoscopic depth of some patterns and a mixture of both horizontal and vertical disparities are used for other patterns. But the visual system could use horizontal disparities in all cases, so why does it uses other strategies as well?&nbsp; Our hypothesis is that there is in fact only a single strategy.&nbsp; The aim of our research is to provide an alternative to current theories about how the human visual system combines the two eyes&rsquo; retinal images and uses mismatches in various directions&mdash;not just horizontal&mdash;to compute stereoscopic depth.<br /><br />Our experiments used all combinations of pairs of 1-D and 2-D patterns and asked how the human visual system uses the disparities of these stimuli to arrive at a perception of stereoscopic depth.&nbsp; We manipulated the directions and magnitudes of these disparities and had people judge which of the two patterns appeared the nearest to them.&nbsp; We found that seeing the relative depth between two 1-D patterns depended on different disparity information than when one pattern was 1-D and the other was 2-D.&nbsp; And the disparity computations were different again when both patterns were 2-D.&nbsp; To explore the sources of these differences, we tested people using visual displays containing both relevant and irrelevant patterns.&nbsp; We gave our subjects the task of judging the depth of the relevant patterns and ignoring the irrelevant ones.&nbsp; Our interest was in the effect of the disparities of the unattended stimuli on subjects' depth judgments, and how these effects depended on the disparities of the relevant stimuli.&nbsp; The data revealed visual processes that cannot be isolated under usual viewing conditions, but depended on the low-level &lsquo;pre-attentive&rsquo; processing that only the irrelevant stimuli receive. <br /><br />The data showed that relevant and irrelevant disparities contribute differently to the perception of depth.&nbsp; This finding allowed us to say that the brain analyzes disparity in two stages.&nbsp; One stage is global, operates without attention, and combines information about disparity direction from all the patterns in the display, whether they are relevant to the task or not.&nbsp; The average disparity direction is then used for depth calculations in a second stage.&nbsp; The second stage is attentional and compares the sizes of disparities in the average direction.&nbsp; These two stages together give us conscious perception of the relative depth between objects.<br /><br />Stereoscopic vision, according to current theories, depends on a &lsquo;hard-wired&rsquo; system for decoding depth from horizontal disparities.&nbsp; Our research overturns this important assumption and implies that the visual system can carry out its analysis of disparity in directions that vary between different visual scenes.&nbsp; This raises a question: If stereo depth depends on cues that vary from one visual scene to another, how do we gain a stable and accurate view of the world?&nbsp; The data hint that stability and accuracy are not assured.&nbsp; Instead, they depend on statistical regularities in visual scenes&mdash;and are easily by-passed in scenes that are unusual and violate the expectations we have learned to see with.<br /><br />Our experiments will result in a better understanding of how humans see the position and shape of objects around them.&nbsp; Some of our results could also help in the design of better therapies for people with imbalanced vision between their left and right eyes.&nbsp; The data will also be useful for designing better visual capacities into robots and other machine systems.</p><br> <p>            Last Modified: 04/30/2017<br>      Modified by: Bart&nbsp;Farell</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The visual world casts two-dimensional (2-D) images on the retinas at the backs of our eyes, but we see the world as three-dimensional.  Seeing the world as fully 3-D requires the brain to interpret and combine the many cues to depth contained in those retinal images.  A powerful cue is stereopsis.  Stereopsis depends on the differences between the images cast on the retinas of the left and right eyes. It?s what gives 3-D movies their extra realism.  These images differ because the eyes are not in the same viewing location, but instead are horizontally separated.  By comparing the retinal images of the left and right eyes, the visual system can infer the relative depth of the objects in a scene and under optimal conditions it can do this with very high precision.    Since work done in the 1830?s, it has been known that horizontal disparities (shifts) in the locations of image features between the two eyes are all that is needed for us to see stereoscopic depth.  However, we and others have shown that horizontal disparities are not the whole story.  For some visual patterns horizontal disparity does not play the expected role.  Instead, the brain favors a disparity direction for these patterns that varies with the pattern?s orientation. This is the case with 1-D patterns, such as lines, edges, and gratings.  As a result, horizontal disparities allow us to see the stereoscopic depth of some patterns and a mixture of both horizontal and vertical disparities are used for other patterns. But the visual system could use horizontal disparities in all cases, so why does it uses other strategies as well?  Our hypothesis is that there is in fact only a single strategy.  The aim of our research is to provide an alternative to current theories about how the human visual system combines the two eyes? retinal images and uses mismatches in various directions&mdash;not just horizontal&mdash;to compute stereoscopic depth.  Our experiments used all combinations of pairs of 1-D and 2-D patterns and asked how the human visual system uses the disparities of these stimuli to arrive at a perception of stereoscopic depth.  We manipulated the directions and magnitudes of these disparities and had people judge which of the two patterns appeared the nearest to them.  We found that seeing the relative depth between two 1-D patterns depended on different disparity information than when one pattern was 1-D and the other was 2-D.  And the disparity computations were different again when both patterns were 2-D.  To explore the sources of these differences, we tested people using visual displays containing both relevant and irrelevant patterns.  We gave our subjects the task of judging the depth of the relevant patterns and ignoring the irrelevant ones.  Our interest was in the effect of the disparities of the unattended stimuli on subjects' depth judgments, and how these effects depended on the disparities of the relevant stimuli.  The data revealed visual processes that cannot be isolated under usual viewing conditions, but depended on the low-level ?pre-attentive? processing that only the irrelevant stimuli receive.   The data showed that relevant and irrelevant disparities contribute differently to the perception of depth.  This finding allowed us to say that the brain analyzes disparity in two stages.  One stage is global, operates without attention, and combines information about disparity direction from all the patterns in the display, whether they are relevant to the task or not.  The average disparity direction is then used for depth calculations in a second stage.  The second stage is attentional and compares the sizes of disparities in the average direction.  These two stages together give us conscious perception of the relative depth between objects.  Stereoscopic vision, according to current theories, depends on a ?hard-wired? system for decoding depth from horizontal disparities.  Our research overturns this important assumption and implies that the visual system can carry out its analysis of disparity in directions that vary between different visual scenes.  This raises a question: If stereo depth depends on cues that vary from one visual scene to another, how do we gain a stable and accurate view of the world?  The data hint that stability and accuracy are not assured.  Instead, they depend on statistical regularities in visual scenes&mdash;and are easily by-passed in scenes that are unusual and violate the expectations we have learned to see with.  Our experiments will result in a better understanding of how humans see the position and shape of objects around them.  Some of our results could also help in the design of better therapies for people with imbalanced vision between their left and right eyes.  The data will also be useful for designing better visual capacities into robots and other machine systems.       Last Modified: 04/30/2017       Submitted by: Bart Farell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
