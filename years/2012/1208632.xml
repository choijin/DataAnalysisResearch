<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI-Small: Perceptually Inspired Dynamics for Robot Arm Motion</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>799889.00</AwardTotalIntnAmount>
<AwardAmount>799889</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Reid Simmons</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>In order for robots to collaborate efficiently and effectively with humans, the human perception of their movement must be considered in motion creation.  Because a human collaborator will interpret the movements of a robot (even subconsciously), robot motion synthesis algorithms that do not consider the human observer may create motions that are perceived incorrectly, interpreted negatively (e.g. as being angry or threatening), or at least miss out on the opportunity to use this subtle communication channel effectively. The key idea of this project is to develop an understanding of human perception of movement that can be applied to the development of robot trajectory planning and control algorithms. The team will use human subjects experiments to understand and evaluate the interpretation of movements and apply these findings in robotics and motion synthesis. The research plan interleaves empirical studies of how people interpret motions, algorithm development to create methods that generate robot motions in a controllable manner, and contextualized deployments that allow the PIs to evaluate the success of the methods. The success of the project will provide a deeper understanding of how people interpret movements, new algorithms for synthesizing robot movements, and demonstrations of the potential applications of collaborative robots.&lt;br/&gt;&lt;br/&gt;Broader Impact: Perceptually inspired robot motion synthesis algorithms will enable robots to collaborate more effectively with people. It will enable more communicative robots that can serve as teachers and guides; more approachable and acceptable robots that can work in domestic situations such as elder care; more cooperative robots that can work as assistants to workers; and easier to instruct robots that can be trained by non-experts. This project will enhance the education and outreach efforts of hte PIs by connecting empirical human studies to the technical challenges of robot trajectory planning.</AbstractNarration>
<MinAmdLetterDate>08/07/2012</MinAmdLetterDate>
<MaxAmdLetterDate>02/18/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208632</AwardID>
<Investigator>
<FirstName>Nicola</FirstName>
<LastName>Ferrier</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicola J Ferrier</PI_FULL_NAME>
<EmailAddress>ferrier@uchicago.edu</EmailAddress>
<PI_PHON>6088867846</PI_PHON>
<NSF_ID>000096025</NSF_ID>
<StartDate>08/07/2012</StartDate>
<EndDate>02/18/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Gleicher</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael L Gleicher</PI_FULL_NAME>
<EmailAddress>gleicher@cs.wisc.edu</EmailAddress>
<PI_PHON>6082632874</PI_PHON>
<NSF_ID>000192407</NSF_ID>
<StartDate>08/07/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bilge</FirstName>
<LastName>Mutlu</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bilge D Mutlu</PI_FULL_NAME>
<EmailAddress>bilge@cs.wisc.edu</EmailAddress>
<PI_PHON>6082623822</PI_PHON>
<NSF_ID>000546805</NSF_ID>
<StartDate>08/07/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 6401]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>161202122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>Madison</CityName>
<StateCode>WI</StateCode>
<ZipCode>537061685</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~799889</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In order for robots to collaborate effectively with people, both the robots and their human collaborators must be able to interpret each other's movements. This project has explored how people interpret arm movements, and used the understanding gained to enable robots both to move in ways that people can understand and to interpret peoples' movements. The project's outcomes fall into three major topics:<br /><br />First, the project has developed an approach that create movements for robot arms such that human observers can better predict the intent of the robots' actions. The approach considers how people interpret arm motions and uses this to guide an optimization-based motion synthesis method. Experiments showed that robots that moved according to these methods were easier for people to collaborate with.<br /><br />Second, the project has developed methods that enable a person to directly control a robot arm in real-time by mapping the person's movements to the robot arm. Our methods provide the user with the feeling of direct control, yet produce feasible robot motions. Experiments show that systems based on our methods are easier for inexperienced users to operate than traditional robot control methods.<br /><br />Third, the project has developed methods that enable a robot to interpret the movements of a person performing a task. These methods can infer the geometric constraints involved in an action based on measurements of the movements and forces that occur when a person performs the action.<br /><br />Additionally, the project has created a number of other outcomes including methods for displaying robot movements, methods that use eye movements in animated characters to communicate about physical tasks, and methods for creating camera movements that best show movements. <br /><br />These outcomes from the project will help enable new robotic systems where robots work together with people. Such systems may have broad societal impact by enabling applications such as robotic assistants to the elderly, improved automation systems for manufacturing and commerce, and even robot helpers for household chores and entertainment. The direct impact of the project includes spreading our scientific results through publications and talks, training students to work in robotics-related fields, and engaging the public to broaden interest in robotics.</p><br> <p>            Last Modified: 10/21/2018<br>      Modified by: Michael&nbsp;L&nbsp;Gleicher</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133413920_1-mico-3paths--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133413920_1-mico-3paths--rgov-800width.jpg" title="Mico - 3 Paths"><img src="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133413920_1-mico-3paths--rgov-66x44.jpg" alt="Mico - 3 Paths"></a> <div class="imageCaptionContainer"> <div class="imageCaption">3 different paths for robot movement compared.</div> <div class="imageCredit">Daniel Rakita and Michael Gleicher</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Michael&nbsp;L&nbsp;Gleicher</div> <div class="imageTitle">Mico - 3 Paths</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133506437_2-mico-timelapse--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133506437_2-mico-timelapse--rgov-800width.jpg" title="Mico Timelapse"><img src="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133506437_2-mico-timelapse--rgov-66x44.jpg" alt="Mico Timelapse"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Showing a legible motion in a laboratory setting with a synthetic timelapse.</div> <div class="imageCredit">Daniel Rakita and Michael Gleicher</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Michael&nbsp;L&nbsp;Gleicher</div> <div class="imageTitle">Mico Timelapse</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133585609_3-Teleop-IMG_0234--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133585609_3-Teleop-IMG_0234--rgov-800width.jpg" title="Teleoperation"><img src="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133585609_3-Teleop-IMG_0234--rgov-66x44.jpg" alt="Teleoperation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">User tele-operating a UR5 robot using or RelaxedIK-based mimicry system.</div> <div class="imageCredit">Daniel Rakita and Michael Gleicher</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Michael&nbsp;L&nbsp;Gleicher</div> <div class="imageTitle">Teleoperation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133664949_4-camera_teaser--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133664949_4-camera_teaser--rgov-800width.jpg" title="Camera Teleoperation Schematic"><img src="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133664949_4-camera_teaser--rgov-66x44.jpg" alt="Camera Teleoperation Schematic"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Schematic explaining our teleoperation system with autonomous camera control.</div> <div class="imageCredit">Daniel Rakita and Michael Gleicher</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Michael&nbsp;L&nbsp;Gleicher</div> <div class="imageTitle">Camera Teleoperation Schematic</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133736050_5-synopsis--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133736050_5-synopsis--rgov-800width.jpg" title="Robot Motion Synopsis"><img src="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133736050_5-synopsis--rgov-66x44.jpg" alt="Robot Motion Synopsis"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Example of our robot motion synopsis approach to show a robot action in a still image.</div> <div class="imageCredit">Daniel Rakita and Michael Gleicher</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Michael&nbsp;L&nbsp;Gleicher</div> <div class="imageTitle">Robot Motion Synopsis</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133821392_6-Differentactions--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133821392_6-Differentactions--rgov-800width.jpg" title="Different action with tongs"><img src="/por/images/Reports/POR/2018/1208632/1208632_10198375_1540133821392_6-Differentactions--rgov-66x44.jpg" alt="Different action with tongs"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Image showing many different physical actions performed with our instrumented tongs input device.</div> <div class="imageCredit">Guru Subramani and Michael Gleicher</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Michael&nbsp;L&nbsp;Gleicher</div> <div class="imageTitle">Different action with tongs</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In order for robots to collaborate effectively with people, both the robots and their human collaborators must be able to interpret each other's movements. This project has explored how people interpret arm movements, and used the understanding gained to enable robots both to move in ways that people can understand and to interpret peoples' movements. The project's outcomes fall into three major topics:  First, the project has developed an approach that create movements for robot arms such that human observers can better predict the intent of the robots' actions. The approach considers how people interpret arm motions and uses this to guide an optimization-based motion synthesis method. Experiments showed that robots that moved according to these methods were easier for people to collaborate with.  Second, the project has developed methods that enable a person to directly control a robot arm in real-time by mapping the person's movements to the robot arm. Our methods provide the user with the feeling of direct control, yet produce feasible robot motions. Experiments show that systems based on our methods are easier for inexperienced users to operate than traditional robot control methods.  Third, the project has developed methods that enable a robot to interpret the movements of a person performing a task. These methods can infer the geometric constraints involved in an action based on measurements of the movements and forces that occur when a person performs the action.  Additionally, the project has created a number of other outcomes including methods for displaying robot movements, methods that use eye movements in animated characters to communicate about physical tasks, and methods for creating camera movements that best show movements.   These outcomes from the project will help enable new robotic systems where robots work together with people. Such systems may have broad societal impact by enabling applications such as robotic assistants to the elderly, improved automation systems for manufacturing and commerce, and even robot helpers for household chores and entertainment. The direct impact of the project includes spreading our scientific results through publications and talks, training students to work in robotics-related fields, and engaging the public to broaden interest in robotics.       Last Modified: 10/21/2018       Submitted by: Michael L Gleicher]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
