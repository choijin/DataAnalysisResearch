<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHB: Type I (EXP): High Accuracy Motion Analysis using Commodity Depth Camera for Clinical Lower-Extremity Assessment</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>566303.00</AwardTotalIntnAmount>
<AwardAmount>577303</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Abstract&lt;br/&gt;&lt;br/&gt;Physical therapy has been widely adopted to treat a wide range of medical conditions, from disabilities, injuries, to crippling diseases such as arthritis, lower back pain, or cerebral palsy. A central issue in physical therapy is the assessment of a patient's kinematic performance. With technology advancement, physical therapists can now utilize commercial motion capture (mocap) system to quantify the 3D mechanics common to many conditions they treat.  The 3D kinematic feedback leads to more accurate diagnosis and faster progression. Unfortunately the current state of the art for technology to measure 3D motion is expensive, cumbersome, and not widely available to physical therapists. Thus a strong need remains for the development of a portable, low cost, mocap system to give physical therapists precise 3D measurements of movement dysfunction. &lt;br/&gt;&lt;br/&gt;This research aims to use the depth information from a single depth camera (SDC) to capture high-accuracy motion data comparable to that from a commercial mocap system. While depth-camera-based pose tracking solutions exist, such as the Kinect system from Microsoft, medical assessment demands almost an order-high accuracy than that for digital entertainment. Based on prior medical studies, this project's goal is to achieve a positional error of less than 10mm and angular error less than 1 degree. That is four to 10 times better than the current state of the art in SDC mocap.  For the scope of this project, a new motion capture framework will be developed and thoroughly evaluated for medical applications, in particular to help physician, physical therapists, and other medical professionals to diagnosis, treat, and monitor patients with movement disorders in the lower-extremity.</AbstractNarration>
<MinAmdLetterDate>09/04/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1231545</AwardID>
<Investigator>
<FirstName>Ruigang</FirstName>
<LastName>Yang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ruigang Yang</PI_FULL_NAME>
<EmailAddress>ryang@cs.uky.edu</EmailAddress>
<PI_PHON>8592573886</PI_PHON>
<NSF_ID>000129435</NSF_ID>
<StartDate>09/04/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Shapiro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Shapiro</PI_FULL_NAME>
<EmailAddress>rshap01@uky.edu</EmailAddress>
<PI_PHON>8592579420</PI_PHON>
<NSF_ID>000541379</NSF_ID>
<StartDate>09/04/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Noehren</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian W Noehren</PI_FULL_NAME>
<EmailAddress>b.noehren@uky.edu</EmailAddress>
<PI_PHON>8592579420</PI_PHON>
<NSF_ID>000541946</NSF_ID>
<StartDate>09/04/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Kentucky Research Foundation</Name>
<CityName>Lexington</CityName>
<ZipCode>405260001</ZipCode>
<PhoneNumber>8592579420</PhoneNumber>
<StreetAddress>109 Kinkead Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<StateCode>KY</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>KY06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>939017877</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF KENTUCKY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007400724</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Kentucky Research Foundation]]></Name>
<CityName>Lexington</CityName>
<StateCode>KY</StateCode>
<ZipCode>405260001</ZipCode>
<StreetAddress><![CDATA[500 S Limestone 109 Kinkead Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Kentucky</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>KY06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramElement>
<ProgramElement>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramElement>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8061</Code>
<Text>SCH Type I:  EXP</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~566303</FUND_OBLG>
<FUND_OBLG>2013~11000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span style="font-family: Calibri; font-size: small;">The ultimate goal of this project is to develop a single 3D camera system that can be deployed in physical therapy and other medical clinics to provide </span><em><span style="font-family: Calibri; font-size: small;">precise, reliable, </span></em><span style="font-family: Calibri; font-size: small;">and </span><em><span style="font-family: Calibri; font-size: small;">anatomically-correct</span></em><span style="font-family: Calibri; font-size: small;"> 3D kinematic feedback to assist in the diagnosis and rehabilitation of common lower extremity pathologies. </span><span style="font-family: Calibri; font-size: small;">&nbsp;</span><span style="font-family: Calibri; font-size: small;">It can be used to reduce treatment times by helping patient and clinicians visualize correct motion versus incorrect motion, and to allow the physical therapist to adjust more precisely faulty movement mechanics.</span></p> <p><span style="font-family: Calibri; font-size: small;">Through years of research, we have demonstrated that high-accuracy body motion can be estimated even from a single depth camera. In particular we have developed a novel (generative) articulated pose estimation algorithm that </span><em><span style="font-family: Calibri; font-size: small;">does not require explicit point correspondences and captures the subject's shape automatically during the pose estimation process</span></em><span style="font-family: Calibri; font-size: small;">.</span><span style="font-family: Calibri; font-size: small;">&nbsp; </span><span style="font-family: Calibri; font-size: small;">Our algorithm relates the observed data with our model using Gaussian Mixture Model (GMM), without explicitly building point correspondences. The pose is then estimated through probability density estimation with articulated deformation model parameterized using exponential maps. Consequently, the algorithm is less sensitive to local minimum and well handles fast and complex motions.</span></p> <p><span style="font-family: Calibri; font-size: small;">Also in a laboratory setting anatomically-relevant measurements for lower extremity movement, measured with a depth camera, have been obtained. The accuracy is within 3-5 degrees from the ground truth, which is deemed acceptable to use in a clinical setting to quantify body movement. </span><span style="font-family: Calibri; font-size: small;">&nbsp;</span><span style="font-family: Calibri; font-size: small;">The prototype system can provide objective information about the subjects&rsquo; movement while performing common tasks that are difficult to capture and measure with clinical tools such as walking and running.</span><span style="font-family: Calibri; font-size: small;">&nbsp; </span><span style="font-family: Calibri; font-size: small;">This information would be used by medical professionals to help document the success of rehabilitation interventions at improving function.</span><span style="font-family: Calibri; font-size: small;">&nbsp; </span></p> <p><strong>&nbsp;</strong><em>&nbsp;</em></p> <p>&nbsp;</p><br> <p>            Last Modified: 12/15/2016<br>      Modified by: Ruigang&nbsp;Yang</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1231545/1231545_10210479_1481835979069_mocap-result--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1231545/1231545_10210479_1481835979069_mocap-result--rgov-800width.jpg" title="Motion capture results"><img src="/por/images/Reports/POR/2016/1231545/1231545_10210479_1481835979069_mocap-result--rgov-66x44.jpg" alt="Motion capture results"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The proposed novel motion capture method is able to find accurate pose under many challenging conditions. The green lines show the estimated skeletons.</div> <div class="imageCredit">Mao Ye and Ruigang Yang</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Ruigang&nbsp;Yang</div> <div class="imageTitle">Motion capture results</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1231545/1231545_10210479_1481835708994_squat-system--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1231545/1231545_10210479_1481835708994_squat-system--rgov-800width.jpg" title="Validation of our proposed system"><img src="/por/images/Reports/POR/2016/1231545/1231545_10210479_1481835708994_squat-system--rgov-66x44.jpg" alt="Validation of our proposed system"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A diagram showing our result validation scheme. We first scan a subject's legs with markers. Then we capture the subject's movement with both a marker-based system and our novel single camera system. Finally we compare the difference.</div> <div class="imageCredit">Univeristy of Kentucky</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Ruigang&nbsp;Yang</div> <div class="imageTitle">Validation of our proposed system</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The ultimate goal of this project is to develop a single 3D camera system that can be deployed in physical therapy and other medical clinics to provide precise, reliable, and anatomically-correct 3D kinematic feedback to assist in the diagnosis and rehabilitation of common lower extremity pathologies.  It can be used to reduce treatment times by helping patient and clinicians visualize correct motion versus incorrect motion, and to allow the physical therapist to adjust more precisely faulty movement mechanics.  Through years of research, we have demonstrated that high-accuracy body motion can be estimated even from a single depth camera. In particular we have developed a novel (generative) articulated pose estimation algorithm that does not require explicit point correspondences and captures the subject's shape automatically during the pose estimation process.  Our algorithm relates the observed data with our model using Gaussian Mixture Model (GMM), without explicitly building point correspondences. The pose is then estimated through probability density estimation with articulated deformation model parameterized using exponential maps. Consequently, the algorithm is less sensitive to local minimum and well handles fast and complex motions.  Also in a laboratory setting anatomically-relevant measurements for lower extremity movement, measured with a depth camera, have been obtained. The accuracy is within 3-5 degrees from the ground truth, which is deemed acceptable to use in a clinical setting to quantify body movement.  The prototype system can provide objective information about the subjects? movement while performing common tasks that are difficult to capture and measure with clinical tools such as walking and running.  This information would be used by medical professionals to help document the success of rehabilitation interventions at improving function.                Last Modified: 12/15/2016       Submitted by: Ruigang Yang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
