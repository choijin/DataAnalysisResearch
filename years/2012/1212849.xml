<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Large: Collaborative Research: Reconstructive recognition: Uniting statistical scene understanding and physics-based visual reasoning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>545454.00</AwardTotalIntnAmount>
<AwardAmount>545454</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is creating a novel paradigm for computer vision, termed "reconstructive recognition", that incorporates the strongest elements of previous machine learning-based recognition efforts and the strongest elements of previous reconstruction efforts based on radiometric reasoning. The goal is to provide a new foundation for machine perception, and the potential for a transformative advance in applications of computer vision. The project seeks novel physics-based methods for recognition as well as novel learning-based methods for interpreting pixel values in terms of the physics of a scene. The agenda is structured around four aims: Aim I develops generalized reconstructive processes that unify the recovery of shape, materials, motion and illumination. Aim II focuses on supervised visual learning methods that exploit such reconstructive image representations. Aim III pursues unsupervised discovery of reconstructive representations that converge to be similar to the engineered models of Aim I. Finally, Aim IV introduces well-defined challenge problems that focus the field and serve as measurable proxies for progress in computer vision applications that have high potential impact on society. &lt;br/&gt;&lt;br/&gt;There is a significant broader impact to this project, not least being the improvement in computer vision pedagogy that ensues from a reunification of the currently divergent recognition and reconstruction views of the field. More broadly, this project pursues critical steps toward a future where machines can see, a future that will bring changes to robotics, human-computer interfaces, security, and autonomous navigation, to name a few.</AbstractNarration>
<MinAmdLetterDate>09/12/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/12/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1212849</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Freeman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William Freeman</PI_FULL_NAME>
<EmailAddress>wtf@ai.mit.edu</EmailAddress>
<PI_PHON>6172538828</PI_PHON>
<NSF_ID>000418387</NSF_ID>
<StartDate>09/12/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394307</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~545454</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-7e1334b3-8f0a-8eb4-195b-752bf568fdae"> </span></p> <p dir="ltr"><span>Through the course of the award, we developed numerous methods and models that make significant contributions to physics-based visual perception.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>First of all, we investigated how visual cues can be utilized to infer physical properties. We developed models that could infer physical properties such as objects&rsquo; materials and stiffness. Such models and methods provide a convenient yet accurate way to measure such properties, which are conventionally hard and expensive to acquire. In addition, we developed a series of work that aims to recover object&rsquo;s 3D shape, which is the most fundamental physical property of an object, &nbsp;using a single image. As the 3D shape of an object is hard to acquire, this line of work provides a practical workaround, which could be a good starting point for robots or autonomous vehicles to have better environment perception.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Then, We investigated how physical events can be helpful in both high-level and low-level vision. For high-level vision, our outcomes are focused on utilizing auditory signals for better object perception and physical reasoning about objects through videos. Specifically, we provided models that are able to utilize audio signals to facilitate visual understanding tasks such as learning visual representations, material classification, and shape inference. Such results can be used as a cheap yet effective approach to facilitate traditional vision-based algorithms, or even when visual signals are not present. For low-level vision, we investigated the physical light transport and provided methods can create smart camouflage for objects and image the hidden scene around a corner using commodity RGB cameras. In particular, we designed algorithms that could camouflage objects in arbitrary environments, making them hard to detect by humans. Moreover, we analyzed the geometrical light transport for occlusions and developed methods that could infer the angular location of objects moving behind a corner. </span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/16/2018<br>      Modified by: William&nbsp;Freeman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Through the course of the award, we developed numerous methods and models that make significant contributions to physics-based visual perception.    First of all, we investigated how visual cues can be utilized to infer physical properties. We developed models that could infer physical properties such as objects? materials and stiffness. Such models and methods provide a convenient yet accurate way to measure such properties, which are conventionally hard and expensive to acquire. In addition, we developed a series of work that aims to recover object?s 3D shape, which is the most fundamental physical property of an object,  using a single image. As the 3D shape of an object is hard to acquire, this line of work provides a practical workaround, which could be a good starting point for robots or autonomous vehicles to have better environment perception.    Then, We investigated how physical events can be helpful in both high-level and low-level vision. For high-level vision, our outcomes are focused on utilizing auditory signals for better object perception and physical reasoning about objects through videos. Specifically, we provided models that are able to utilize audio signals to facilitate visual understanding tasks such as learning visual representations, material classification, and shape inference. Such results can be used as a cheap yet effective approach to facilitate traditional vision-based algorithms, or even when visual signals are not present. For low-level vision, we investigated the physical light transport and provided methods can create smart camouflage for objects and image the hidden scene around a corner using commodity RGB cameras. In particular, we designed algorithms that could camouflage objects in arbitrary environments, making them hard to detect by humans. Moreover, we analyzed the geometrical light transport for occlusions and developed methods that could infer the angular location of objects moving behind a corner.                Last Modified: 04/16/2018       Submitted by: William Freeman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
