<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:Large:Collaborative Research:Unified Runtime for Supporting Hybrid Programming Models on Heterogeneous Architecture</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
<AwardExpirationDate>06/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>371903.00</AwardTotalIntnAmount>
<AwardAmount>371903</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Most of the traditional High-End Computing (HEC) applications and&lt;br/&gt;current petascale applications are written using the Message Passing&lt;br/&gt;Interface (MPI) programming model. Some of these applications are run&lt;br/&gt;in MPI+OpenMP mode.  However, it can be very difficult to use MPI or&lt;br/&gt;MPI+OpenMP and maintain performance for applications which demonstrate&lt;br/&gt;irregular and dynamic communication patterns.  The Partitioned Global&lt;br/&gt;Address Space (PGAS) programming model presents a flexible way for&lt;br/&gt;these applications to express parallelism.  Accelerators introduce&lt;br/&gt;additional programming models: CUDA, OpenCL or OpenACC.  Thus, the&lt;br/&gt;emerging heterogeneous architectures require support for various&lt;br/&gt;hybrid programming models: MPI+OpenMP, MPI+PGAS, and MPI+PGAS+OpenMP&lt;br/&gt;with extended APIs for multiple levels of parallelism.  Unfortunately,&lt;br/&gt;there is no unified runtime which delivers the best performance and&lt;br/&gt;scalability for all of these hybrid programming models for a range of&lt;br/&gt;applications on current and next-generation HEC systems.  This leads&lt;br/&gt;to the following broad challenge: "Can a unified runtime for hybrid&lt;br/&gt;programming model be designed which can provide benefits that are&lt;br/&gt;greater than the sum of its parts?"&lt;br/&gt;&lt;br/&gt;A synergistic and comprehensive research plan, involving computer&lt;br/&gt;scientists from The Ohio State University (OSU) and Ohio Supercomputer&lt;br/&gt;Center (OSC) and computational scientists from the Texas Advanced&lt;br/&gt;Computing Center (TACC) and San Diego Supercomputer Center (SDSC),&lt;br/&gt;University of California San Diego (UCSD), is proposed to address the&lt;br/&gt;above broad challenge with innovative solutions.  The investigators&lt;br/&gt;will specifically address the following challenges: 1) What are the&lt;br/&gt;requirements and limitations of using hybrid programming models for a&lt;br/&gt;set of petascale applications?  2) What features and mechanisms are&lt;br/&gt;needed in a unified runtime?  3) How can the unified runtime and&lt;br/&gt;associated extension to programming model APIs be designed and&lt;br/&gt;implemented?  4) How can candidate petascale applications be&lt;br/&gt;redesigned to take advantage of proposed unified runtime?  and 5) What&lt;br/&gt;kind of benefits (in terms of performance, scalability and&lt;br/&gt;productivity) can be achieved by the proposed approach?  The research&lt;br/&gt;will be driven by a set of applications from established NSF&lt;br/&gt;computational science researchers running large scale simulations on&lt;br/&gt;Ranger and other systems at OSC, SDSC and OSU.  The proposed designs&lt;br/&gt;will be integrated into the open-source MVAPICH2 library.  The&lt;br/&gt;established national-scale training and outreach programs at TACC,&lt;br/&gt;SDSC and OSC will be used to disseminate the results of this research.</AbstractNarration>
<MinAmdLetterDate>05/16/2012</MinAmdLetterDate>
<MaxAmdLetterDate>04/07/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1213057</AwardID>
<Investigator>
<FirstName>Karl</FirstName>
<LastName>Schulz</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karl W Schulz</PI_FULL_NAME>
<EmailAddress>karl@ices.utexas.edu</EmailAddress>
<PI_PHON>5122321124</PI_PHON>
<NSF_ID>000370250</NSF_ID>
<StartDate>05/16/2012</StartDate>
<EndDate>04/07/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>William</FirstName>
<LastName>Barth</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William L Barth</PI_FULL_NAME>
<EmailAddress>bbarth@tacc.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000596862</NSF_ID>
<StartDate>04/07/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>William</FirstName>
<LastName>Barth</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William L Barth</PI_FULL_NAME>
<EmailAddress>bbarth@tacc.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000596862</NSF_ID>
<StartDate>05/16/2012</StartDate>
<EndDate>04/07/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jerome</FirstName>
<LastName>Vienne</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jerome F Vienne</PI_FULL_NAME>
<EmailAddress>viennej@tacc.utexas.edu</EmailAddress>
<PI_PHON>5124759411</PI_PHON>
<NSF_ID>000658593</NSF_ID>
<StartDate>04/07/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787137726</ZipCode>
<StreetAddress><![CDATA[PO Box 7726]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>21</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX21</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~371903</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Message Passing Interface (MPI) has been the most popular<br />programming model for developing parallel scientific applications on<br />modern High-End Computing (HEC) systems. A hybrid model combining MPI<br />with the shared-memory OpenMP model is also widely used (referred to<br />as MPI+OpenMP), where typically a single MPI process is used per<br />compute server or socket with OpenMP "threads" running on each CPU<br />core in the server/socket. On the other hand, Partitioned Global<br />Address Space (PGAS) programming models are an attractive alternative<br />for designing applications with irregular communication patterns. They<br />improve programmability by providing a shared memory abstraction<br />globally while exposing data placement control required for<br />performance. It is widely believed that a hybrid programming model<br />(MPI+X, where X is a PGAS model) is optimal for many scientific<br />computing problems, especially for exascale computing. Specialize<br />computing devices, such as NVIDIA GPUs and Intel Many Integrated Cores<br />(MIC) introduce additional programming models such as CUDA, OpenCL or<br />OpenACC to express parallelism within them. Thus, the emerging<br />heterogeneous architectures for HEC systems require support for a range<br />of hybrid programming models: MPI+OpenMP, MPI+PGAS, and<br />MPI+PGAS+OpenMP with extended APIs for multiple levels of parallelism.<br /><br />Unfortunately, there is no unified runtime layer to orchestrate the<br />communication and synchronization operations for these models which<br />delivers the best performance and scalability for a range of<br />applications on current and next-generation HEC systems. This leads to<br />the following broad challenge: "Can a unified runtime for hybrid<br />programming model be designed which can provide benefits that are<br />greater than the sum of its parts?"<br /><br />In this project we have addressed this challenge in five areas: 1)<br />Developed a unified runtime supporting MPI, PGAS (OpenSHMEM, UPC,<br />UPC++ and CAF), Hybrid MPI+PGAS, and MPI+CUDA; 2) Designed efficient<br />algorithms for one-sided and collective communications for PGAS<br />models; 3) Developed high-performance GPU-aware and MIC-aware<br />communications algorithms which leverage latest hardware features; 4)<br />Co-Designed MPI+PGAS and MPI+CUDA applications and demonstrated<br />performance advantages; and 5) Developed benchmarks for new hybrid<br />models.<br /><br />Contributions were made in all five areas and evaluated with a range<br />of science applications including - Graph500, CNTK Deep Learning<br />Framework, Mizan Graph Processing Framework, COSMO Weather prediction,<br />AWP-ODC Seismic modeling, and an Out-of-Core Sort. Some highlights of<br />these results are:<br /><br />* An optimized Graph500 design using hybrid MPI+OpenSHMEM model demonstrated<br />a 13X factor of improvement in comparison with the pure MPI version <br />on 16,000 cores.<br /><br />* An Out-of-Core Sort design using hybrid MPI+OpenSHMEM models reduced<br />the runtime by 45% on 8,192 processes compared with the existing<br />design and an improvement of 7X on 1,024 cores in comparison with<br />Hadoop.<br /><br />* A Scalable GPU-Aware implementation of the CNTK Deep Learning<br />framework that runs 10-18% faster than the prior implementation on<br />benchmark datasets.<br /><br />* A new MPI memory registration approach in the unified runtime which<br />reduces the amount of pinned memory required by the library by as much<br />as eleven-fold.<br /><br />* GPU-aware halo-data exchange and new GPU-kernel supported<br />collective communication algorithms reduce communication operations<br />for the COSMO Weather prediction application by 20% and 30%, <br />respectively.<br /><br />The results of this research (new designs, performance results,<br />benchmarks, etc.) have been made available to the community through<br />MVAPICH2, MVAPICH2-X and MVAPICH2-GDR libraries (1.9, 2.0, 2.1, and<br />2.2 release series including alpha, beta, RC and GA versions). The latest<br />versions of these libraries are currently running on many large-scale<br />XSEDE systems including TACC Stampede and SDSC Comet. Currently,the<br />MVAPICH2 libraries are being used by more than 2,675 organizations in<br />83 countries. The MVAPICH2 libraries and the associated enhancements<br />are being used by a large number of users of these systems.<br /><br />In each of these releases, information about the tuned designs for<br />various components (such as point-to-point, collectives, GPU-GPU<br />communication, etc.) has been shared with the MVAPICH2 user community<br />through mailing lists. The applications-based tuning results have been<br />made available to the community through the "Best Practices" link of<br />the MVAPICH project web page. In order to achieve direct face-to-face<br />discussion with MVAPICH2 users and get their feedback, in 2013 we<br />started holding an MVAPICH2 User Group Meeting (MUG) each year in<br />August in Columbus, Ohio. This meeting has been continuing successfully<br />for the last four years and has helped to disseminate the results of<br />this research to a wider community.&nbsp; In addition to the software<br />distribution and the MUG events, the results have been presented at<br />various conferences and events through talks and tutorials.&nbsp; Multiple<br />Ph.D and Masters students have performed research work and received<br />their Ph.D and M.S. degrees as a part of this project.</p><br> <p>            Last Modified: 10/12/2016<br>      Modified by: William&nbsp;L&nbsp;Barth</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Message Passing Interface (MPI) has been the most popular programming model for developing parallel scientific applications on modern High-End Computing (HEC) systems. A hybrid model combining MPI with the shared-memory OpenMP model is also widely used (referred to as MPI+OpenMP), where typically a single MPI process is used per compute server or socket with OpenMP "threads" running on each CPU core in the server/socket. On the other hand, Partitioned Global Address Space (PGAS) programming models are an attractive alternative for designing applications with irregular communication patterns. They improve programmability by providing a shared memory abstraction globally while exposing data placement control required for performance. It is widely believed that a hybrid programming model (MPI+X, where X is a PGAS model) is optimal for many scientific computing problems, especially for exascale computing. Specialize computing devices, such as NVIDIA GPUs and Intel Many Integrated Cores (MIC) introduce additional programming models such as CUDA, OpenCL or OpenACC to express parallelism within them. Thus, the emerging heterogeneous architectures for HEC systems require support for a range of hybrid programming models: MPI+OpenMP, MPI+PGAS, and MPI+PGAS+OpenMP with extended APIs for multiple levels of parallelism.  Unfortunately, there is no unified runtime layer to orchestrate the communication and synchronization operations for these models which delivers the best performance and scalability for a range of applications on current and next-generation HEC systems. This leads to the following broad challenge: "Can a unified runtime for hybrid programming model be designed which can provide benefits that are greater than the sum of its parts?"  In this project we have addressed this challenge in five areas: 1) Developed a unified runtime supporting MPI, PGAS (OpenSHMEM, UPC, UPC++ and CAF), Hybrid MPI+PGAS, and MPI+CUDA; 2) Designed efficient algorithms for one-sided and collective communications for PGAS models; 3) Developed high-performance GPU-aware and MIC-aware communications algorithms which leverage latest hardware features; 4) Co-Designed MPI+PGAS and MPI+CUDA applications and demonstrated performance advantages; and 5) Developed benchmarks for new hybrid models.  Contributions were made in all five areas and evaluated with a range of science applications including - Graph500, CNTK Deep Learning Framework, Mizan Graph Processing Framework, COSMO Weather prediction, AWP-ODC Seismic modeling, and an Out-of-Core Sort. Some highlights of these results are:  * An optimized Graph500 design using hybrid MPI+OpenSHMEM model demonstrated a 13X factor of improvement in comparison with the pure MPI version  on 16,000 cores.  * An Out-of-Core Sort design using hybrid MPI+OpenSHMEM models reduced the runtime by 45% on 8,192 processes compared with the existing design and an improvement of 7X on 1,024 cores in comparison with Hadoop.  * A Scalable GPU-Aware implementation of the CNTK Deep Learning framework that runs 10-18% faster than the prior implementation on benchmark datasets.  * A new MPI memory registration approach in the unified runtime which reduces the amount of pinned memory required by the library by as much as eleven-fold.  * GPU-aware halo-data exchange and new GPU-kernel supported collective communication algorithms reduce communication operations for the COSMO Weather prediction application by 20% and 30%,  respectively.  The results of this research (new designs, performance results, benchmarks, etc.) have been made available to the community through MVAPICH2, MVAPICH2-X and MVAPICH2-GDR libraries (1.9, 2.0, 2.1, and 2.2 release series including alpha, beta, RC and GA versions). The latest versions of these libraries are currently running on many large-scale XSEDE systems including TACC Stampede and SDSC Comet. Currently,the MVAPICH2 libraries are being used by more than 2,675 organizations in 83 countries. The MVAPICH2 libraries and the associated enhancements are being used by a large number of users of these systems.  In each of these releases, information about the tuned designs for various components (such as point-to-point, collectives, GPU-GPU communication, etc.) has been shared with the MVAPICH2 user community through mailing lists. The applications-based tuning results have been made available to the community through the "Best Practices" link of the MVAPICH project web page. In order to achieve direct face-to-face discussion with MVAPICH2 users and get their feedback, in 2013 we started holding an MVAPICH2 User Group Meeting (MUG) each year in August in Columbus, Ohio. This meeting has been continuing successfully for the last four years and has helped to disseminate the results of this research to a wider community.  In addition to the software distribution and the MUG events, the results have been presented at various conferences and events through talks and tutorials.  Multiple Ph.D and Masters students have performed research work and received their Ph.D and M.S. degrees as a part of this project.       Last Modified: 10/12/2016       Submitted by: William L Barth]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
