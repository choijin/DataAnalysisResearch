<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps:  BlindNav: Indoor Navigation for the Visually Impaired</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>03/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rathindra DasGupta</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>There are currently very few ways for the blind to navigate a new indoor space without the assistance of a fully-sighted person. The technology proposed by this project is designed to enable a visually-impaired individual to find their way through large indoor environments such as airports, train stations and shopping malls by recognizing semantic and salient visual features of the environment. There is no prior visit or mapping of the environment required, and there is no need to deploy or utilize any special infrastructure like WiFi access points or infrared beacons. Researchers plan to use publically available architectural lay-outs and information about the location of ships, tracks, gates and other visual cues. The platform is a cell-phone mounted on a necklace that provides turn-by-turn directions through an audio-voice command interface. This technology is designed to process video from the cell phone camera in real-time using text and logo detection, localization based on prior knowledge of the layout and integration of accelerometer and visual odometry.&lt;br/&gt;&lt;br/&gt;The blind and visually-impaired population in the United States is large and expected to grow in the future. If successfully implemented, this technology could have broader reaching applications, including many location-based services such as aiding those with spatial learning difficulties or guiding users to a specific location. The project team has the expertise required to develop this technology at a relatively rapid rate and economical cost.</AbstractNarration>
<MinAmdLetterDate>09/25/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/25/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1265129</AwardID>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>09/25/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Independent mobility is a critical function of humans and it is almost universally taken for granted by fully sighted people. This process involves localization, navigation, and obstacle avoidance. While obstacle avoidance might be partially solved using canes or guide dogs, and outdoor navigation is facilitated by GPS systems, indoor navigation remains an intractable challenge. The I-Corps team proposes a new product, BlindNav, which enables a visually impaired person to find her way in large indoor environments like airports, train stations, and malls by recognizing semantic and salient visual features of the environment. No prior visits to the train station or mapping of the station with SLAM techniques is needed. Instead &nbsp;architectural lay-outs available publicly and information about location of shops, tracks, gates, and other visual cues are used. &nbsp;The platform is a cell-phone mounted on a necklace and providing turn by turn directions through a voice interface. &nbsp;BlindNav processes video from the cell phone camera in real-time using three main technologies: text and logo detection, localization based on prior knowledge of the layout (like vertical lines), integration of vanishing point detection and accelerometer data, and short-range visual odometry to mitigate hard visibility conditions.&nbsp;<span> </span>There is currently no way for a blind person to navigate in a new space without the assistance of a fully-sighted person. The team aims to revolutionize the world for blind people by providing an unprecedented independence. The integration of &ldquo;accessibility&rdquo; software such as audio-feedback touch screens has put mobile technology such as smartphones and tablets in the hands of the visually impaired. &nbsp;The team has partnered with several members of the blind community towards developing a simple, intuitive audio interface and furthermore, a grounds root adoption. BlindNav is scheduled for nationwide commercial launch in October 2013.&nbsp;<span> </span>Currently, in pre-alpha development, the team is leveraging experience in computer vision to port cutting-edge, proven research to the cell phone. Because, the key to BlindNav&rsquo;s success is The team&rsquo;s integration filter which combines multiple open source computer vision tools towards localization, all IP is owned by the company. &nbsp;The team has implemented text detection and vertical line detection on the cell phone to date.&nbsp;With the revolution of Apple's universal access software-which makes mobile software usable for the disabled, other companies have followed suit, thus changing the entire landscape of assistive technology. &nbsp;The team's product would be the only product of its kind, and would offer a unique and elegant solution an affordable price.&nbsp;</p><br> <p>            Last Modified: 07/11/2014<br>      Modified by: Kostas&nbsp;Daniilidis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Independent mobility is a critical function of humans and it is almost universally taken for granted by fully sighted people. This process involves localization, navigation, and obstacle avoidance. While obstacle avoidance might be partially solved using canes or guide dogs, and outdoor navigation is facilitated by GPS systems, indoor navigation remains an intractable challenge. The I-Corps team proposes a new product, BlindNav, which enables a visually impaired person to find her way in large indoor environments like airports, train stations, and malls by recognizing semantic and salient visual features of the environment. No prior visits to the train station or mapping of the station with SLAM techniques is needed. Instead  architectural lay-outs available publicly and information about location of shops, tracks, gates, and other visual cues are used.  The platform is a cell-phone mounted on a necklace and providing turn by turn directions through a voice interface.  BlindNav processes video from the cell phone camera in real-time using three main technologies: text and logo detection, localization based on prior knowledge of the layout (like vertical lines), integration of vanishing point detection and accelerometer data, and short-range visual odometry to mitigate hard visibility conditions.  There is currently no way for a blind person to navigate in a new space without the assistance of a fully-sighted person. The team aims to revolutionize the world for blind people by providing an unprecedented independence. The integration of "accessibility" software such as audio-feedback touch screens has put mobile technology such as smartphones and tablets in the hands of the visually impaired.  The team has partnered with several members of the blind community towards developing a simple, intuitive audio interface and furthermore, a grounds root adoption. BlindNav is scheduled for nationwide commercial launch in October 2013.  Currently, in pre-alpha development, the team is leveraging experience in computer vision to port cutting-edge, proven research to the cell phone. Because, the key to BlindNavÆs success is The teamÆs integration filter which combines multiple open source computer vision tools towards localization, all IP is owned by the company.  The team has implemented text detection and vertical line detection on the cell phone to date. With the revolution of Apple's universal access software-which makes mobile software usable for the disabled, other companies have followed suit, thus changing the entire landscape of assistive technology.  The team's product would be the only product of its kind, and would offer a unique and elegant solution an affordable price.        Last Modified: 07/11/2014       Submitted by: Kostas Daniilidis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
