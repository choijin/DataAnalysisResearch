<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Mid-Scale: DA: Collaborative Research: Big Tensor Mining: Theory, Scalable Algorithms and Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/01/2012</AwardEffectiveDate>
<AwardExpirationDate>11/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>866846.00</AwardTotalIntnAmount>
<AwardAmount>866846</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Tensors are multi-dimensional generalizations of matrices, and so can have non-numeric entries. Extremely large and sparse coupled tensors arise in numerous important applications that require the analysis of large, diverse, and partially related data. The effective analysis of coupled tensors requires the development of algorithms and associated software that can identify the core relations that exist among the different tensor modes, and scale to extremely large datasets. The objective of this project is to develop theory and algorithms for (coupled) sparse and low-rank tensor factorization, and associated scalable software toolkits to make such analysis possible. The research in the project is centered on three major thrusts. The first is designed to make novel theoretical contributions in the area of coupled tensor factorization, by developing multi-way compressed sensing methods for dimensionality reduction with perfect latent model reconstruction. Methods to handle missing values, noisy input, and coupled data will also be developed. The second thrust focuses on algorithms and scalability on modern architectures, which will enable the efficient analysis of coupled tensors with millions and billions of non-zero entries, using the map-reduce paradigm, as well as hybrid multicore architectures. An open-source coupled tensor factorization toolbox (HTF- Hybrid Tensor Factorization) will be developed that will provide robust and high-performance implementations of these algorithms. Finally, the third thrust focuses on evaluating and validating the effectiveness of these coupled factorization algorithms on a NeuroSemantics application whose goal is to understand how human brain activity correlates with text reading &amp; understanding by analyzing fMRI and MEG brain image datasets obtained while reading various text passages.&lt;br/&gt;&lt;br/&gt;Given triplets of facts (subject-verb-object), like ('Washington' 'is the capital of' 'USA'), can we find patterns, new objects, new verbs, anomalies? Can we correlate these with brain scans of people reading these words, to discover which parts of the brain get activated, say, by tool-like nouns ('hammer'), or action-like verbs ('run')? &lt;br/&gt;We propose a unified "coupled tensor" factorization framework to systematically mine such datasets. Unique challenges in these settings include &lt;br/&gt;(a) tera- and peta-byte scaling issues, &lt;br/&gt;(b) distributed fault-tolerant computation, &lt;br/&gt;(c) large proportions of missing data, and &lt;br/&gt;(d) insufficient theory and methods for big sparse tensors. &lt;br/&gt;The Intellectual Merit of this effort is exactly the solution to the above four challenges.&lt;br/&gt;&lt;br/&gt;The Broader Impact is the derivation of new scientific hypotheses on how the brain works and how it processes language (from the never-ending language learning (NELL) and NeuroSemantics projects) and the development of scalable open source software for coupled tensor factorization. Our tensor analysis methods can also be used in many other settings, including recommendation systems and computer-network intrusion/anomaly detection.&lt;br/&gt;&lt;br/&gt;KEYWORDS:&lt;br/&gt;Data mining; map/reduce; read-the-web; neuro-semantics; tensors.</AbstractNarration>
<MinAmdLetterDate>09/13/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/13/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1247632</AwardID>
<Investigator>
<FirstName>Nikolaos</FirstName>
<LastName>Sidiropoulos</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nikolaos D Sidiropoulos</PI_FULL_NAME>
<EmailAddress>nikos@virginia.edu</EmailAddress>
<PI_PHON>4349246075</PI_PHON>
<NSF_ID>000228544</NSF_ID>
<StartDate>09/13/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>George</FirstName>
<LastName>Karypis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George Karypis</PI_FULL_NAME>
<EmailAddress>karypis@cs.umn.edu</EmailAddress>
<PI_PHON>6126267524</PI_PHON>
<NSF_ID>000386588</NSF_ID>
<StartDate>09/13/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[117 Pleasant Street SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramElement>
<Code>L218</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>170E</Code>
<Text>Interagency Agreements</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~866846</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Tensors, also known as multi-way arrays, are functions of three or more indices&nbsp;(i,j,k,?)&nbsp;&ndash; similar to matrices, which are functions of two indices&nbsp;(r,c)&nbsp;for (row,column). Tensors have recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining and machine learning. The objective of this collaborative research project was to explore ways of parallelizing and scaling up tensor computations to very large datasets, paying special attention to identifiability, application-specific constraints such as non-negativity, and with an eye on important applications in web, topic, and language mining, brain imaging and neuro-semantics. &nbsp;</p> <p><strong>Intellectual merit:</strong> Outcomes can be classified as contributions to i) theory, ii) algorithms, and iii) cross-disciplinary applications. In terms of i) theory, outcomes include important new results on uniqueness of tensor and non-negative matrix factorization; randomized multi-way tensor compressed sensing; provably identifiable correlated topic mining from pairwise word co-occurrences without anchor words; and a fundamental new link between tensors and probability theory: showing that it is possible to estimate the joint distribution of a very large number of random variables from knowledge of only third-order marginalized distributions, provided that the random variables are only moderately dependent. This in a way allows, under certain conditions, to circumvent the `curse of dimensionality' in high-dimensional statistical estimation. On the algorithms front, novel parallel tensor and coupled matrix-tensor decomposition and sampling algorithms were derived, along with a hybrid alternating optimization &ndash; alternating direction method of multipliers (AO-ADMM) framework that performs constrained matrix/tensor factorization at the cost of unconstrained ALS, enabling plug-and-play exploration. SPLATT, a C library with shared- and distributed-memory parallelism for CPD and TUCKER tensor decompositions was developed, that can support tensors with arbitrarily large number of modes. On shared-memory systems, SPLATT averages almost 30-fold speedup compared to baseline schemes when using 16 threads, and reaches over 80-fold speedup on the NELL-2 dataset. On distributed-memory systems, SPLATT can scale to systems with over 2000 cores. SPLATT is now publicly available. On the cross-disciplinary applications front, collaborative work between the partner institutions has yielded novel approaches and exciting discoveries in neuro-semantics, translation-invariant word embedding, multi-lingual word embedding, topic mining, fMRI-MEG/EEG fusion for high spatio-temporal resolution functional brain imaging, and historical data disaggregation. The proposed new approach for multi-lingual word embedding outperforms the prior state-of-art on multilingual tasks, matches it on monolingual tasks, and scales linearly in the number of languages. Numerous joint journal and conference papers reporting original research were published in prestigious, top-tier venues, spanning signal processing, high performance computing, data mining, and natural language processing.</p> <p>&nbsp;</p> <p><strong>Broader impacts:</strong> Research under this project was inherently cross-disciplinary, bringing together signal processing, factor analysis, data mining, machine learning, neuro-semantics, language learning, and high-performance computing. As such, it has an inherent value in terms of permeating and creating knowledge across fields. The project has been a true collaboration, with numerous papers co-authored from co-PIs, students and postdocs from the collaborating institutions and beyond. A comprehensive tutorial overview paper (essentially a monograph at 42 double-column 10-point pages) has been written with the explicit goal of bridging the ground between the tensor communities in signal processing and machine learning. This is already highly appreciated, in the top-100 IEEExplore downloads for many months in a row, and rapidly growing citations. Several other focused tutorials were published as well, and complemented by plenary, keynote, and tutorial talks. Many students and three postdocs have worked on this grant. One former student and one former postdoc are now faculty members, and two more are prospective faculty. Software libraries have been made publicly available to facilitate public use, and they are already extensively used by industry and government researchers. In addition, a repository of publicly available large and sparse tensor datasets has been created and made publicly available.</p><br> <p>            Last Modified: 02/28/2018<br>      Modified by: Nikolaos&nbsp;D&nbsp;Sidiropoulos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Tensors, also known as multi-way arrays, are functions of three or more indices (i,j,k,?) &ndash; similar to matrices, which are functions of two indices (r,c) for (row,column). Tensors have recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining and machine learning. The objective of this collaborative research project was to explore ways of parallelizing and scaling up tensor computations to very large datasets, paying special attention to identifiability, application-specific constraints such as non-negativity, and with an eye on important applications in web, topic, and language mining, brain imaging and neuro-semantics.    Intellectual merit: Outcomes can be classified as contributions to i) theory, ii) algorithms, and iii) cross-disciplinary applications. In terms of i) theory, outcomes include important new results on uniqueness of tensor and non-negative matrix factorization; randomized multi-way tensor compressed sensing; provably identifiable correlated topic mining from pairwise word co-occurrences without anchor words; and a fundamental new link between tensors and probability theory: showing that it is possible to estimate the joint distribution of a very large number of random variables from knowledge of only third-order marginalized distributions, provided that the random variables are only moderately dependent. This in a way allows, under certain conditions, to circumvent the `curse of dimensionality' in high-dimensional statistical estimation. On the algorithms front, novel parallel tensor and coupled matrix-tensor decomposition and sampling algorithms were derived, along with a hybrid alternating optimization &ndash; alternating direction method of multipliers (AO-ADMM) framework that performs constrained matrix/tensor factorization at the cost of unconstrained ALS, enabling plug-and-play exploration. SPLATT, a C library with shared- and distributed-memory parallelism for CPD and TUCKER tensor decompositions was developed, that can support tensors with arbitrarily large number of modes. On shared-memory systems, SPLATT averages almost 30-fold speedup compared to baseline schemes when using 16 threads, and reaches over 80-fold speedup on the NELL-2 dataset. On distributed-memory systems, SPLATT can scale to systems with over 2000 cores. SPLATT is now publicly available. On the cross-disciplinary applications front, collaborative work between the partner institutions has yielded novel approaches and exciting discoveries in neuro-semantics, translation-invariant word embedding, multi-lingual word embedding, topic mining, fMRI-MEG/EEG fusion for high spatio-temporal resolution functional brain imaging, and historical data disaggregation. The proposed new approach for multi-lingual word embedding outperforms the prior state-of-art on multilingual tasks, matches it on monolingual tasks, and scales linearly in the number of languages. Numerous joint journal and conference papers reporting original research were published in prestigious, top-tier venues, spanning signal processing, high performance computing, data mining, and natural language processing.     Broader impacts: Research under this project was inherently cross-disciplinary, bringing together signal processing, factor analysis, data mining, machine learning, neuro-semantics, language learning, and high-performance computing. As such, it has an inherent value in terms of permeating and creating knowledge across fields. The project has been a true collaboration, with numerous papers co-authored from co-PIs, students and postdocs from the collaborating institutions and beyond. A comprehensive tutorial overview paper (essentially a monograph at 42 double-column 10-point pages) has been written with the explicit goal of bridging the ground between the tensor communities in signal processing and machine learning. This is already highly appreciated, in the top-100 IEEExplore downloads for many months in a row, and rapidly growing citations. Several other focused tutorials were published as well, and complemented by plenary, keynote, and tutorial talks. Many students and three postdocs have worked on this grant. One former student and one former postdoc are now faculty members, and two more are prospective faculty. Software libraries have been made publicly available to facilitate public use, and they are already extensively used by industry and government researchers. In addition, a repository of publicly available large and sparse tensor datasets has been created and made publicly available.       Last Modified: 02/28/2018       Submitted by: Nikolaos D Sidiropoulos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
