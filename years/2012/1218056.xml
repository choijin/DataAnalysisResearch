<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Collaborative Research: Real-Time Captioning by Groups of Non-Experts for Deaf and Hard of Hearing Students</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>80646.00</AwardTotalIntnAmount>
<AwardAmount>129554</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many deaf and hard of hearing students use real-time captioning to participate in education.  Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute.   But professional captionists are expensive and must be arranged in advance in blocks of at least an hour.  Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms.  In this collaborative effort involving the University of Rochester and Rochester Institute of Technology, the PIs will address these issues by blending human- and machine-powered captioning to produce captions on demand, in real time, for low cost.  The PIs' approach is for multiple non-experts and ASR to collectively caption speech in under 5 seconds, with the help of interfaces which encourage quick, incomplete captioning of live audio.  Because non-experts cannot keep up with natural speaking rates, new algorithms will merge incomplete captions in real time. (While the sequence alignment problem can be solved exactly with dynamic programming, existing approaches are too slow, are not robust to input error, and do not incorporate natural language semantics.)  Systematically varying audio saliency will encourage complete coverage of speech.  Non-expert captions will train ASR engines in real time, so that ASR may improve during a lecture. (Traditional approaches for ASR training assume that training occurs offline.)  The quikCaption mobile application will embody these ideas and will be iteratively designed with deaf and hard of hearing students at the National Technical Institute of the Deaf (NTID) via design sessions, lab studies and in-class deployments.  Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers.  They may be local (in the classroom) or remote.  Captionists may have experience from prior quikCaption sessions, or novice crowd workers recruited on demand from existing marketplaces (e.g., Mechanical Turk).  A flexible worker pool will allow real-time captions to be available on demand at low cost and for only as long as needed.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This research will dramatically improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged.  Real-time captioning will also be useful in other settings such as school programs, artistic performances, and political events.  Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population; hearing people may benefit because captioning is a first step in automatic translation of aural speech.  The algorithms developed as part of this project for real-time merging of incomplete natural language will likely be adaptable for other applications such as collaborative translation or communication over noisy mediums.</AbstractNarration>
<MinAmdLetterDate>08/08/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/18/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1218056</AwardID>
<Investigator>
<FirstName>Raja</FirstName>
<LastName>Kushalnagar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raja Kushalnagar</PI_FULL_NAME>
<EmailAddress>raja.kushalnagar@gallaudet.edu</EmailAddress>
<PI_PHON>5854448753</PI_PHON>
<NSF_ID>000591743</NSF_ID>
<StartDate>08/08/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rochester Institute of Tech</Name>
<CityName>ROCHESTER</CityName>
<ZipCode>146235603</ZipCode>
<PhoneNumber>5854757987</PhoneNumber>
<StreetAddress>1 LOMB MEMORIAL DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002223642</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ROCHESTER INSTITUTE OF TECHNOLOGY (INC)</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002223642</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rochester Institute of Tech]]></Name>
<CityName>Rochester</CityName>
<StateCode>NY</StateCode>
<ZipCode>146235603</ZipCode>
<StreetAddress><![CDATA[1 Lomb Memorial Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~80646</FUND_OBLG>
<FUND_OBLG>2013~16908</FUND_OBLG>
<FUND_OBLG>2014~16000</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span style="font-family: Calibri; font-size: small;">The goals of this project&nbsp;was to analyze and improve usability of on-demand, real-time speech-to-text for deaf and hard of hearing consumers in academic and entertainment settings. Real-time speech-to-text services provide access to social environments (academic, entertainment, etc) for deaf and hard of hearing people world-wide. </span></p> <p><span style="font-family: Calibri; font-size: small;"><span>Over 5% of the world&rsquo;s population - 360 million people world-wide have disabling hearing loss, including one third of those who are 65 plus. A major impact of disabling hearing loss is that the individual has difficulty in understanding other speakers, or in conversing with them, and their access in&nbsp;most social environments is limited. Many deaf and hard of hearing people,&nbsp;provided they are literate,&nbsp;can benefit from real-time speech to text services, in which speech is transcribed to text in real-time.&nbsp;</span><span><span>&nbsp;</span></span></span></p> <p><span style="font-family: Calibri; font-size: small;">The project funded eight studies on several&nbsp;distinct parameters of&nbsp;speech-to-text services, interfaces or accessibility. The findings provide insights into how the deaf and hard of hearing people&rsquo;s reading process differed from hearing people&rsquo;s listening process, and on how speech-to-text is perceived differently from speech.&nbsp;</span></p> <p>The studies <span style="font-family: Calibri; font-size: small;">showed that there are separate parameters unique to speech-text that vary by environment, and are completely different from speech parameters. They also show that there are many speech parameters that are not adequately captured by speech-text interfaces and show that visual-tactile interfaces are viable approaches to enhancing speech-to-text. &nbsp;</span><span style="font-family: Calibri; font-size: small;">&nbsp;The information gleaned from those studies have been disseminated to accessible computing researchers, developers, and accessible service providers. </span></p> <p><span style="font-family: Calibri; font-size: small;">The project findings have expanded knowledge of how deaf and hard of hearing people use and experience speech-to-text services, and provided a significant framework for user interface and user experience researchers and developers.&nbsp;The&nbsp;findings provide them with&nbsp;a better understanding of speech-text parameters, and how these parameters shape the development and presentation of modern speech-to-text services, such as television or classroom captioning. Our findings and guidelines can be used to further improve existing and new speech-to-text services and interfaces in a wide variety of settings. </span></p><br> <p>            Last Modified: 09/12/2016<br>      Modified by: Raja&nbsp;Kushalnagar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goals of this project was to analyze and improve usability of on-demand, real-time speech-to-text for deaf and hard of hearing consumers in academic and entertainment settings. Real-time speech-to-text services provide access to social environments (academic, entertainment, etc) for deaf and hard of hearing people world-wide.   Over 5% of the world?s population - 360 million people world-wide have disabling hearing loss, including one third of those who are 65 plus. A major impact of disabling hearing loss is that the individual has difficulty in understanding other speakers, or in conversing with them, and their access in most social environments is limited. Many deaf and hard of hearing people, provided they are literate, can benefit from real-time speech to text services, in which speech is transcribed to text in real-time.    The project funded eight studies on several distinct parameters of speech-to-text services, interfaces or accessibility. The findings provide insights into how the deaf and hard of hearing people?s reading process differed from hearing people?s listening process, and on how speech-to-text is perceived differently from speech.   The studies showed that there are separate parameters unique to speech-text that vary by environment, and are completely different from speech parameters. They also show that there are many speech parameters that are not adequately captured by speech-text interfaces and show that visual-tactile interfaces are viable approaches to enhancing speech-to-text.   The information gleaned from those studies have been disseminated to accessible computing researchers, developers, and accessible service providers.   The project findings have expanded knowledge of how deaf and hard of hearing people use and experience speech-to-text services, and provided a significant framework for user interface and user experience researchers and developers. The findings provide them with a better understanding of speech-text parameters, and how these parameters shape the development and presentation of modern speech-to-text services, such as television or classroom captioning. Our findings and guidelines can be used to further improve existing and new speech-to-text services and interfaces in a wide variety of settings.        Last Modified: 09/12/2016       Submitted by: Raja Kushalnagar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
