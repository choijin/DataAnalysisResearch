<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Characterizing feature selectivity and invariance in deep neural architectures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>528000.00</AwardTotalIntnAmount>
<AwardAmount>528000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goals of this CAREER proposal are to help elucidate the principles that make robust object recognition possible. Object recognition is a problem that must be solved by all living organisms, from single-cell organisms to humans. Although the physical signals for recognition based on chemical events, light or sound waves are different, the computational requirements for analyzing these events appear to be similar. Specifically, there are two main properties that any system that mediates robust object recognition must have. The first property is known as "invariance." It endows neurons with a similar response to the same object observed from different viewpoints. The second property is known as "selectivity." Selectivity requires that neurons produce different responses to potentially quite similar objects (such as different faces) even when presented from similar viewpoints. It is straightforward to make detectors that are invariant but not selective or selective but not invariant. The difficulty lies in making detectors that are both selective and invariant. &lt;br/&gt;&lt;br/&gt;This CAREER project will develop statistical methods for simultaneously characterizing both the invariance properties of neurons and their selectivity to specific features in the environment. The developed methods will have three distinguishing characteristics. First, it will be possible to recover new types of invariance without any prior assumptions of what the dominant type of invariance is for any given neuron or brain region. Second, they will make it possible to characterize imperfect and approximate types of invariance. Third, the methods will be geared towards stimuli typical of the natural sensory environment that are rich in objects and elicit robust responses from neurons from all stages of sensory processing. These three properties of the developed methods will make it possible to simultaneously study multiple neurons both within and across different regions, without the need to adjust stimuli to a particular neuron or brain region. Application of the developed methods to responses of neurons that mediate visual and auditory object recognition in the brain will help reveal the common principles of sensory processing in the brain and may ultimately lead to improved designs of artificial recognition systems, including sensory prostheses.&lt;br/&gt;&lt;br/&gt;This research will be integrated into education and outreach activities involving K-12 students, undergraduate and graduate students. The educational component will help integrate knowledge acquired in computer science, physics, and neuroscience, training a new generation of scientists that are proficient in these disciplines. Outreach to local schools and museums, as well as the creation of an online course will help reach a diverse range of students both locally and worldwide.</AbstractNarration>
<MinAmdLetterDate>09/05/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1254123</AwardID>
<Investigator>
<FirstName>Tatyana</FirstName>
<LastName>Sharpee</LastName>
<PI_MID_INIT>O</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tatyana O Sharpee</PI_FULL_NAME>
<EmailAddress>sharpee@salk.edu</EmailAddress>
<PI_PHON>8584534100</PI_PHON>
<NSF_ID>000330861</NSF_ID>
<StartDate>09/05/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>The Salk Institute for Biological Studies</Name>
<CityName>LA JOLLA</CityName>
<ZipCode>920371002</ZipCode>
<PhoneNumber>8584534100</PhoneNumber>
<StreetAddress>10010 N TORREY PINES RD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078731668</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SALK INSTITUTE FOR BIOLOGICAL STUDIES, SAN DIEGO, CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Salk Institute for Biological Studies]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920371002</ZipCode>
<StreetAddress><![CDATA[10010 North Torrey Pines Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1045</Code>
<Text>CAREER: FACULTY EARLY CAR DEV</Text>
</ProgramElement>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~453000</FUND_OBLG>
<FUND_OBLG>2017~75000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>When signals are first received in the brain, they are coded in terms of simple features. For visual stimuli, such features could be segments of object contours; auditory stimuli are initially represented in terms of individual frequency components. However, subsequent transformation begin to represent object parts in ways that historically has been difficult to characterize and describe precisely.&nbsp; This project has produced a set of methods that address this problem and make it possible to characterize the secondary transformations that occur in the brain. Using these methods, we found several principles according to which visual and auditory stimuli are transformed in the brain. For visual stimuli, we found that signals representing individual small edges are bound together in groups four: two nearby parallel edges and two perpendicular edges that suppress the neural responses if present. This motif of four edges is then repeated in ways that varied across neurons. Some visual neurons integrated signals from these motifs over a patch a visual scenes; other neurons computed differences between patches. These second-order neurons make it possible for us to discern boundaries between objects of similar brightness, for example, by detecting changes in textures between the objects. &nbsp;For auditory stimuli, we found that selectivity to more complex auditory waveforms is achieved in ways quite different from those used in vision. In vision, current evidence indicated that selectivity to complex visual feature is achieved through integration of signals that represent object parts. For auditory stimuli, we found that this invariance is achieved primarily by exclusion, i.e. by making neurons increasing less sensitive to stimuli that contradict their primary auditory event of interest.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/06/2018<br>      Modified by: Tatyana&nbsp;O&nbsp;Sharpee</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ When signals are first received in the brain, they are coded in terms of simple features. For visual stimuli, such features could be segments of object contours; auditory stimuli are initially represented in terms of individual frequency components. However, subsequent transformation begin to represent object parts in ways that historically has been difficult to characterize and describe precisely.  This project has produced a set of methods that address this problem and make it possible to characterize the secondary transformations that occur in the brain. Using these methods, we found several principles according to which visual and auditory stimuli are transformed in the brain. For visual stimuli, we found that signals representing individual small edges are bound together in groups four: two nearby parallel edges and two perpendicular edges that suppress the neural responses if present. This motif of four edges is then repeated in ways that varied across neurons. Some visual neurons integrated signals from these motifs over a patch a visual scenes; other neurons computed differences between patches. These second-order neurons make it possible for us to discern boundaries between objects of similar brightness, for example, by detecting changes in textures between the objects.  For auditory stimuli, we found that selectivity to more complex auditory waveforms is achieved in ways quite different from those used in vision. In vision, current evidence indicated that selectivity to complex visual feature is achieved through integration of signals that represent object parts. For auditory stimuli, we found that this invariance is achieved primarily by exclusion, i.e. by making neurons increasing less sensitive to stimuli that contradict their primary auditory event of interest.          Last Modified: 12/06/2018       Submitted by: Tatyana O Sharpee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
