<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC-NIE Integration: Bringing Distributed High Throughput Computing to the Network with Lark</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>573344.00</AwardTotalIntnAmount>
<AwardAmount>573344</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Lark project provides network-aware scheduling for distributed computing resources, allowing workload management schedulers to make network-aware provisioning decisions and providing mechanisms for individual batch jobs to describe and request the networking topology required. Specifically, Lark will integrate the capabilities of the perfSONAR, a network performance monitoring tool, and DYNES, a cyber-instrument for allocation of bandwidth guarantees, into distributed computing grid middleware such as Condor and glideinWMS. By using a multi-site IPv6 test-bed and collaborating with a Nebraska project to provide small starter clusters to local colleges, the project aims to produce production-quality technology that could be deployed into the Open Science Grid (OSG).&lt;br/&gt;&lt;br/&gt;This work will enable better trade-offs between selection of a site to execute jobs, data location decisions, performance, and security in large distributed computing environments, hence providing more effective use of limited resources. This project addresses a clear and immediate need in the Large Hadron Collider (LHC) computing environment, as network-aware scheduling will increase the amount of analysis the LHC experiment collaborations can perform. By integrating this work into the widely-used Condor high throughput computing software and into the OSG stack, the benefits will become available not only to LHC researchers, but to a number of science and industry projects that rely upon distributed high throughput computing.</AbstractNarration>
<MinAmdLetterDate>09/07/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1245864</AwardID>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Tannenbaum</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd Tannenbaum</PI_FULL_NAME>
<EmailAddress>tannenba@cs.wisc.edu</EmailAddress>
<PI_PHON>6082637132</PI_PHON>
<NSF_ID>000334476</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Bockelman</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian P Bockelman</PI_FULL_NAME>
<EmailAddress>bbockelman@morgridge.org</EmailAddress>
<PI_PHON>4027504235</PI_PHON>
<NSF_ID>000539290</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName/>
<StateCode>NE</StateCode>
<ZipCode>685880150</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~573344</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Lark project works to provide network-aware scheduling for distributed computing resources by promoting the network as a first-class resource managed by the distributed system (in the same way that CPUs, storages and memory are managed) and allowing the network topology to be actively modified. The primary focus is in the domain of high throughput computing and batch systems, which have historically treated the network as an unmanaged, shared resource with infinite capacity. High throughput computing is an important technique for advancing research across a variety of disciplines, from the social sciences to physics and engineering. &nbsp;Improved network integration will ease the transition of these computing workloads from local computing clusters to resources distributed across the campus, region, or world.</p> <p>The project resulted in a new approach for integrating the computing and network layer. We developed extensions to the HTCondor distributed batch computing system to give individual batch jobs an address on the network; this was done without the substantial&nbsp;overhead of a virtual machine approach. We then developed a&nbsp;custom network controller that can reason about the set of&nbsp;running jobs and adjust network topology accordingly. Further, we have enhanced an existing file transfer application (GridFTP) to interact with the same controller, allowing co-scheduling of jobs and transfers over the wide-area network.</p> <p>The network testbed for this&nbsp;project provided a platform to develop IPv6 enhancements for several widely-used pieces of software (HTCondor, GridFTP, and Xrootd) running in the Open Science Grid. &nbsp;The testbed also provided a mechanism for testing and hardening on-campus IPv6 deployments, particularly at Nebraska. Nebraska now runs the enhanced versions of all three applications in production. The Lark sites (Nebraska and Wisconsin) are now leaders in IPv6 deployment within the computing community for the Large Hadron Collider (LHC).</p> <p>The network testbed provided our researchers with the ability to setup a full range of complex topologies seen on modern networks. This was essential to the development of the next-generation endpoint discovery and connection negotiation protocol&nbsp;within the HTCondor software, significantly improving its dual-stack capabilities. This will benefit the LHC community, as resources such as CERN and the San Diego Supercomputing Center will prefer IPv6-based traffic in the future.&nbsp;Improved network statistics and simple network-aware algorithms were added to the HTCondor scheduler and matchmaker; we hope this will&nbsp;serve as a base for future work.</p> <p>In addition to the running applications produced, the results of this work are disseminated through presentations at HTCondor Week, an overview paper in CCGrid 2015, and will be featured in an upcoming PhD dissertation.&nbsp;Further, the project provided critical training and development opportunities for our professional staff and students.</p><br> <p>            Last Modified: 12/08/2015<br>      Modified by: Brian&nbsp;P&nbsp;Bockelman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Lark project works to provide network-aware scheduling for distributed computing resources by promoting the network as a first-class resource managed by the distributed system (in the same way that CPUs, storages and memory are managed) and allowing the network topology to be actively modified. The primary focus is in the domain of high throughput computing and batch systems, which have historically treated the network as an unmanaged, shared resource with infinite capacity. High throughput computing is an important technique for advancing research across a variety of disciplines, from the social sciences to physics and engineering.  Improved network integration will ease the transition of these computing workloads from local computing clusters to resources distributed across the campus, region, or world.  The project resulted in a new approach for integrating the computing and network layer. We developed extensions to the HTCondor distributed batch computing system to give individual batch jobs an address on the network; this was done without the substantial overhead of a virtual machine approach. We then developed a custom network controller that can reason about the set of running jobs and adjust network topology accordingly. Further, we have enhanced an existing file transfer application (GridFTP) to interact with the same controller, allowing co-scheduling of jobs and transfers over the wide-area network.  The network testbed for this project provided a platform to develop IPv6 enhancements for several widely-used pieces of software (HTCondor, GridFTP, and Xrootd) running in the Open Science Grid.  The testbed also provided a mechanism for testing and hardening on-campus IPv6 deployments, particularly at Nebraska. Nebraska now runs the enhanced versions of all three applications in production. The Lark sites (Nebraska and Wisconsin) are now leaders in IPv6 deployment within the computing community for the Large Hadron Collider (LHC).  The network testbed provided our researchers with the ability to setup a full range of complex topologies seen on modern networks. This was essential to the development of the next-generation endpoint discovery and connection negotiation protocol within the HTCondor software, significantly improving its dual-stack capabilities. This will benefit the LHC community, as resources such as CERN and the San Diego Supercomputing Center will prefer IPv6-based traffic in the future. Improved network statistics and simple network-aware algorithms were added to the HTCondor scheduler and matchmaker; we hope this will serve as a base for future work.  In addition to the running applications produced, the results of this work are disseminated through presentations at HTCondor Week, an overview paper in CCGrid 2015, and will be featured in an upcoming PhD dissertation. Further, the project provided critical training and development opportunities for our professional staff and students.       Last Modified: 12/08/2015       Submitted by: Brian P Bockelman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
