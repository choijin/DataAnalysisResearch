<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: GraphLab 2: An Abstraction and System for Large-Scale Parallel Machine Learning on Natural Graphs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With the growth of the Web and improvements in data collection technology in Science, datasets have been rapidly increasing in size and complexity, necessitating comparable scaling of machine learning algorithms. However, designing and implementing efficient parallel machine learning algorithms is challenging and time consuming. To address this challenge, we recently released GraphLab, a framework providing an expressive and efficient high-level abstraction satisfying the needs of a broad range of machine learning algorithms.  The performance of our system has attracted significant attention, receiving thousands of downloads from many universities and companies.&lt;br/&gt;&lt;br/&gt;Currently, GraphLab only addresses batch processing in multicore settings.  In this project, we are developing GraphLab 2: addressing the much more challenging online and distributed settings, tackling: 1) Cloud-based distributed machine learning.  2) Natural graphs, with very high-degree vertices that are not amenable to graph partitioning methods.  3) Online tasks, where data and queries are streaming over time.  4) Off-core computation, since huge problems may not fit into memory, even across the cloud.&lt;br/&gt;&lt;br/&gt;One of the key contributions of the project is the continual dissemination and transfer of our technology.  Our open-source software releases will continue to enable large-scale machine learning applications in science and engineering.&lt;br/&gt;&lt;br/&gt;Our ambitious broader impact goals, beyond theory and systems, include the development of a new curriculum focused on preparing students for the industrial and scientific needs in this field.  Our proposed courses include "Machine Learning on the Web" and "Cloud Computing for Big Machine Learning and Data Mining."</AbstractNarration>
<MinAmdLetterDate>09/07/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1258741</AwardID>
<Investigator>
<FirstName>Carlos</FirstName>
<LastName>Guestrin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Carlos Guestrin</PI_FULL_NAME>
<EmailAddress>guestrin@cs.washington.edu</EmailAddress>
<PI_PHON>2065431695</PI_PHON>
<NSF_ID>000221609</NSF_ID>
<StartDate>09/07/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName/>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>6892</Code>
<Text>CI REUSE</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>With a constantly lowering cost of storage and bandwidth, acquisition of large datasets in domains such as physics, biology, social sciences, economics, and medicine has seen widespread adoption. These trends are accompanied by a corresponding increase in the number of processors available in commodity computing hardware. There is a need for Machine Learning approaches to scale to these large datasets, by utilizing the parallelism in computations that can be afforded by multiple processors and machines. Unfortunately, existing Machine Learning algorithms have been designed for serial computations, and deploying them on parallel and distributed hardware for large datasets often requires time-consuming reimplementation for specific models, handling challenges in data storage, and results in poor scaling properties.</p> <p>&nbsp;</p> <p>In this project, we studied and developed novel methods and systems to accelerate the application of machine learning techniques in the real world. In particular, we have developed:</p> <ul> <li>New machine learning algorithms that can handle huge amounts of data.</li> <li>Built large-scale open-source implementations of systems for machine learning.</li> </ul> <p>&nbsp;</p> <p>The impact of this project has been in three main areas:</p> <ul> <li>New algorithms that outperform the existing state-of-the-art for machine learning on large datasets.</li> <li>Three new major open-source projects, XGBoost, MXNet, and TVM. These proejcts have seen tremendous adoption and have become de facto standards in industry.</li> <li>Education programs for machine learning that have been made accessible to over 100,000 people around the world.</li> </ul> <p>&nbsp;</p> <p>Nowadays, we interact with computing systems every day. These systems are tightly integrated with every aspect of our lives, and manage all of our world's information. By providing more effective, large-scale machine learning, we can get "more value out of our data", and make better personal, political and social decisions. Our software releases and published algorithms will enable such analyses at a scale that has never been possible before.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/26/2018<br>      Modified by: Carlos&nbsp;Guestrin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ With a constantly lowering cost of storage and bandwidth, acquisition of large datasets in domains such as physics, biology, social sciences, economics, and medicine has seen widespread adoption. These trends are accompanied by a corresponding increase in the number of processors available in commodity computing hardware. There is a need for Machine Learning approaches to scale to these large datasets, by utilizing the parallelism in computations that can be afforded by multiple processors and machines. Unfortunately, existing Machine Learning algorithms have been designed for serial computations, and deploying them on parallel and distributed hardware for large datasets often requires time-consuming reimplementation for specific models, handling challenges in data storage, and results in poor scaling properties.     In this project, we studied and developed novel methods and systems to accelerate the application of machine learning techniques in the real world. In particular, we have developed:  New machine learning algorithms that can handle huge amounts of data. Built large-scale open-source implementations of systems for machine learning.      The impact of this project has been in three main areas:  New algorithms that outperform the existing state-of-the-art for machine learning on large datasets. Three new major open-source projects, XGBoost, MXNet, and TVM. These proejcts have seen tremendous adoption and have become de facto standards in industry. Education programs for machine learning that have been made accessible to over 100,000 people around the world.      Nowadays, we interact with computing systems every day. These systems are tightly integrated with every aspect of our lives, and manage all of our world's information. By providing more effective, large-scale machine learning, we can get "more value out of our data", and make better personal, political and social decisions. Our software releases and published algorithms will enable such analyses at a scale that has never been possible before.          Last Modified: 07/26/2018       Submitted by: Carlos Guestrin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
