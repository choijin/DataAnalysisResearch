<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Prototype Dense Motion Capture for Large-Scale Deformable-Scene Tracking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>72834.00</AwardTotalIntnAmount>
<AwardAmount>72834</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Richard Voyles</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This EAGER proposal, developing a dense deformable object tracking method for testing and ground-truthing highly dynamic maneuvers of robotic vehicles, offers to prototype a radically new approach to instrumentation for highly agile machines of many types. Using heterogeneous fusion of numerous sensing modalities, the PI proposes to create a high resolution, high bandwidth map of deformable and articulated bodies such as cars with suspensions, humans, legged robots, and such for the performance evaluation and algorithm development for these highly dynamic research artifacts.&lt;br/&gt;&lt;br/&gt;Broader Impacts: This work will facilitate a wide variety of research, including studies of agile robotics in dynamic environments. The impacts to robotic science, especially agile and fast moving robots, are clear. The system will also have impact on any other science, such as human motion analysis, ergonomics and medical rehabilitation, where accurate, dense tracking and mapping of deformable scenes is required.</AbstractNarration>
<MinAmdLetterDate>09/10/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/10/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1249409</AwardID>
<Investigator>
<FirstName>Gabriel</FirstName>
<LastName>Sibley</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gabriel T Sibley</PI_FULL_NAME>
<EmailAddress>gsibley@colorado.edu</EmailAddress>
<PI_PHON>3034927514</PI_PHON>
<NSF_ID>000674922</NSF_ID>
<StartDate>09/10/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520058</ZipCode>
<StreetAddress><![CDATA[801 22nd ST NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~72834</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Outcomes &ndash; Intellectual Merit:</p> <p>This NSF EAGER grant led to a successful a prototype of a motion-capture facility for large-scale dense 3D dynamic and deformable scenes.&nbsp; This EAGER grant contributed to the publication of four papers [1][2][3][4].&nbsp; The prototype proves that in real-time we can track large volumes on the order of 200m^3 at sub-centimeter resolutions, building highly accurate scene models that can serve as ground-truth.&nbsp; We developed prof-of-concept environment tracking and modeling algorithms for a new type of real-time dense-motion-capture system.&nbsp; The success of this prototype has catalyzed a follow on major research instrumentation grant to development a dense-motion capture system capable of real-time, high-resolution, deformable-scene modeling covering 200m^3 at sub cm accuracies.&nbsp; This instrument is currently being developed under NSF MRI: &ldquo;Development of Large-Scale Dense Scene Capture and Tracking Instrument&rdquo;</p> <p>We are interested in fast, agile mobile robots operating in dynamic environments with deformable objects. Imagine fast-flying vehicles avoiding waving branches while navigating through dense moving vegetation; or a troupe of running robots traversing soft, pliable terrain, such as mud, sand or snow &ndash; all while estimating, modeling and predicting complex ground interactions. If robots are to move quickly and confidently through such highly-dynamic and deformable environments, then we need to devise better perception, planning and control algorithms. Rigorously testing these algorithms in controlled experiments will require a new kind of motion-capture technology that can provide dense ground-truth measurements of a changing 3D scene. Tracking deformable environments necessitates a live dense-3D map that is updated in real-time.</p> <p>This EAGER grant has directly enabled the development of a system for rapid capture and construction of large dynamic high-resolution virtual environments that duplicate specific real-world environments, including deformable?objects, with unprecedented density of detail. When finished, this will enable a wide variety of new research. Examples include: experimentally validating novel perception, planning and control algorithms of agile mobile robots, particularly those that operate in dynamic environments with deformable objects, require ground truth representation of those environments. Validating computational tools for tether?dynamics and control for flexible multi-body systems require the capture of their configuration?in a large environment. Study of human motion for biomechanics, physical therapy, and exercise science applications requires accurate capture of dynamically changing deformable human shapes?in a large environment. Image-guided surgical procedures require capture of localized dense?patient anatomical surface that are registered to surgical instruments in a larger surgical?environment. In human visual perception and navigation, an accurate, dense model of the surrounding environment (including objects in motion), would dramatically advance the state of eye movement analysis by enabling fast, automated, and objective coding of the objects people see as they move through an environment. The study of foot deformations is enabled by dense-shape-capture during walking and running on real sediments will shed light on the evolution of our uniquely human anatomy and gait, and the biomechanics of barefoot walking and running.</p> <p>Outcomes &ndash; Broader Impacts:</p> <p>&nbsp;</p> <p>This EAGER grant developed a prototype for a large-scale, dense motion capture system for dynamic and deformable scenece.&nbsp; The full-scale instrument is now under development and will be housed in the GWU Motion Capture and Analysis Laboratory (MOCA). &nbsp;MOCA represents a collaboration of a dedicated researchers and educators across the university an...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Outcomes &ndash; Intellectual Merit:  This NSF EAGER grant led to a successful a prototype of a motion-capture facility for large-scale dense 3D dynamic and deformable scenes.  This EAGER grant contributed to the publication of four papers [1][2][3][4].  The prototype proves that in real-time we can track large volumes on the order of 200m^3 at sub-centimeter resolutions, building highly accurate scene models that can serve as ground-truth.  We developed prof-of-concept environment tracking and modeling algorithms for a new type of real-time dense-motion-capture system.  The success of this prototype has catalyzed a follow on major research instrumentation grant to development a dense-motion capture system capable of real-time, high-resolution, deformable-scene modeling covering 200m^3 at sub cm accuracies.  This instrument is currently being developed under NSF MRI: "Development of Large-Scale Dense Scene Capture and Tracking Instrument"  We are interested in fast, agile mobile robots operating in dynamic environments with deformable objects. Imagine fast-flying vehicles avoiding waving branches while navigating through dense moving vegetation; or a troupe of running robots traversing soft, pliable terrain, such as mud, sand or snow &ndash; all while estimating, modeling and predicting complex ground interactions. If robots are to move quickly and confidently through such highly-dynamic and deformable environments, then we need to devise better perception, planning and control algorithms. Rigorously testing these algorithms in controlled experiments will require a new kind of motion-capture technology that can provide dense ground-truth measurements of a changing 3D scene. Tracking deformable environments necessitates a live dense-3D map that is updated in real-time.  This EAGER grant has directly enabled the development of a system for rapid capture and construction of large dynamic high-resolution virtual environments that duplicate specific real-world environments, including deformable?objects, with unprecedented density of detail. When finished, this will enable a wide variety of new research. Examples include: experimentally validating novel perception, planning and control algorithms of agile mobile robots, particularly those that operate in dynamic environments with deformable objects, require ground truth representation of those environments. Validating computational tools for tether?dynamics and control for flexible multi-body systems require the capture of their configuration?in a large environment. Study of human motion for biomechanics, physical therapy, and exercise science applications requires accurate capture of dynamically changing deformable human shapes?in a large environment. Image-guided surgical procedures require capture of localized dense?patient anatomical surface that are registered to surgical instruments in a larger surgical?environment. In human visual perception and navigation, an accurate, dense model of the surrounding environment (including objects in motion), would dramatically advance the state of eye movement analysis by enabling fast, automated, and objective coding of the objects people see as they move through an environment. The study of foot deformations is enabled by dense-shape-capture during walking and running on real sediments will shed light on the evolution of our uniquely human anatomy and gait, and the biomechanics of barefoot walking and running.  Outcomes &ndash; Broader Impacts:     This EAGER grant developed a prototype for a large-scale, dense motion capture system for dynamic and deformable scenece.  The full-scale instrument is now under development and will be housed in the GWU Motion Capture and Analysis Laboratory (MOCA).  MOCA represents a collaboration of a dedicated researchers and educators across the university and the greater Washington, DC area in diverse disciplines including anthropology, computer science, psychology, exercise sciences, orthopaedic surgery,...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
