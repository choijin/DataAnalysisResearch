<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Constituents and heads in prosody perception: A comparative study</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>100357.00</AwardTotalIntnAmount>
<AwardAmount>100357</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Joan Maling</SignBlockName>
<PO_EMAI>jmaling@nsf.gov</PO_EMAI>
<PO_PHON>7032928046</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates how prosody is used in spoken language to signal how words are grouped into phrases and how each word and phrase contributes information to the discourse. In text these functions are signaled through punctuation and text enhancement (eg., boldface), but in spoken language pitch, timing, loudness, voice quality, and other properties of speech convey such information. Linguists propose that underlying the prosody of all languages is a universal structure, the Headed Constituent (HC), which groups syllables and words together, with one prominent element per constituent designated the Head element. These constituents determine how speech sounds are coordinated (at a physical level), and how speech is mapped onto syntactic and semantic structures that determine utterance meaning. &lt;br/&gt;&lt;br/&gt;Prosodic constituents are examined in English, Spanish, and French, three languages that are known to differ not only in their prosody (eg., intonation and rhythmic patterns), but also in the associations linking prosody to syntax and semantics. Experiments conducted in Illinois, Barcelona and Lyon will show how how listeners perceive the prosodic phrasing and prominence patterns of an utterance when presented with speech samples that differ in their phonetic properties (pitch and timing), and in syntactic and semantic features. A novel method of real-time, auditory transcription with non-expert listeners and conversational speech samples is intended to best approximate conditions of normal language use. Parallel experiments on English, Spanish, and French will provide critical evidence regarding the universality of the Headed Constituent as the structure that underlies prosodic form, and will also shed light on differences among languages in the role of prosody in communicating linguistic meaning. &lt;br/&gt;&lt;br/&gt;Project findings will contribute valuable benchmark data on how prosody functions in conversational speech, with future applications in clinical settings to identify speech disorders involving prosody, in second language teaching, and in development of speech technologies for human-computer interfaces.</AbstractNarration>
<MinAmdLetterDate>02/05/2013</MinAmdLetterDate>
<MaxAmdLetterDate>02/05/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1251134</AwardID>
<Investigator>
<FirstName>Caroline</FirstName>
<LastName>Smith</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Caroline L Smith</PI_FULL_NAME>
<EmailAddress>caroline@unm.edu</EmailAddress>
<PI_PHON>5052777417</PI_PHON>
<NSF_ID>000252322</NSF_ID>
<StartDate>02/05/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of New Mexico</Name>
<CityName>Albuquerque</CityName>
<ZipCode>871310001</ZipCode>
<PhoneNumber>5052774186</PhoneNumber>
<StreetAddress>1700 Lomas Blvd. NE, Suite 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Mexico</StateName>
<StateCode>NM</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NM01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>868853094</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NEW MEXICO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>784121725</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of New Mexico]]></Name>
<CityName/>
<StateCode>NM</StateCode>
<ZipCode>871310001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Mexico</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NM01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~100357</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project investigated the prosody of conversational speech. Prosody refers to the properties of speech that communicate which words are most important, and how words group together to communicate meaning. These properties are different from those that distinguish one sound from another; the properties associated with prosody most often include intonation (changes in the pitch of a speaker&rsquo;s voice), variation in loudness, and variation in the durational properties of the speech. For example, speakers often slow down or lengthen sounds to emphasize that a word is important, or to indicate that they are at the end of a group of words. They may also change the pitch of their voice for similar reasons. We refer to these properties as the acoustic correlates of prosody.</p> <p>The goal of the larger, collaborative project was to look at similarities and differences in how listeners interpret prosody in English, French, and Spanish. The part of the project reported on here was concerned with French; a research group in Lyon, France, helped us to recruit French students to participate in our study. We were specifically interested in how speakers of the different languages use different ways of communicating which words are most important. In order to do this, we used special software developed for this project at the University of Illinois, in which participants listen to recorded speech and at the same time read a printed transcript of that speech on a computer screen. As they listen to the recording, the participants click on the words shown on the screen to indicate which ones seem most important to them, depending on how the recording sounds.</p> <p>&nbsp;In English, speakers can emphasize any word in a sentence by making it louder and longer, and listeners interpret these changes accordingly. English speakers have a great deal of freedom as to which word or words in a sentence they emphasize in order to communicate the particular meaning that they wish to convey. French speakers do not manipulate acoustic correlates of prosody as much, and our results show that French listeners&rsquo; perception of which words are important is correspondingly less influenced by acoustic factors such as duration, compared to English listeners. This is especially true when they are told to pay attention to the <em>sound</em> of the words than instructions to pay attention to the <em>meaning</em> of the words.</p> <p>&nbsp;An additional topic of investigation for French was the differences in prosody between speech that has been recorded in a laboratory compared to speech produced as part of a conversation between friends. Much research on prosody has been done on &ldquo;laboratory speech&rdquo; because it enables researchers to study specific structures that they are interested in, by specifying what their experimental participants are to say. In contrast, this project studied speech produced in ordinary, unplanned conversations that had been previously recorded in Paris by other researchers. We analyzed the acoustic modifications that the speakers in these conversations used when they wanted to add extra emphasis to a word (to &ldquo;focus&rdquo; on it in the conversation), and found that many of the acoustic modifications that have been observed in laboratory speech are not present in the conversational speech. We also looked at the prosody produced in dislocations, a specific sentence structure that is commonly used in spoken French. Dislocations are when a word is moved from its usual place in a sentence to either the beginning or end of the sentence. An example in English would be, &ldquo;Ice cream, I really like.&rdquo; This kind of change in word order is frequent in spoken French, and previous research of laboratory speech has reported that speakers treat the dislocated word (&ldquo;ice cream&rdquo;) as a separate phrase, with a pause between it and the rest of the sentence. Our analysis of dislocations that speakers happened to produce in conversation showed that they rarely pause after a dislocated word, and do not lengthen it much either, as would be expected if it is a separate phrase. At most they may raise the pitch of their voice at the end of the dislocated word, which indicates that are going to continue speaking.</p> <p>Investigating these specific aspects of how French speakers use prosody enables us to get a better understanding of the differences among different languages, and the differences between reading aloud in a laboratory compared with spontaneous conversation. Identifying these differences is useful for someone learning how to speak French, and can also be useful in programming computers to interpret spoken French, such as is needed by devices like Siri or Alexa that respond to spoken commands.</p><br> <p>            Last Modified: 01/09/2018<br>      Modified by: Caroline&nbsp;L&nbsp;Smith</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project investigated the prosody of conversational speech. Prosody refers to the properties of speech that communicate which words are most important, and how words group together to communicate meaning. These properties are different from those that distinguish one sound from another; the properties associated with prosody most often include intonation (changes in the pitch of a speaker?s voice), variation in loudness, and variation in the durational properties of the speech. For example, speakers often slow down or lengthen sounds to emphasize that a word is important, or to indicate that they are at the end of a group of words. They may also change the pitch of their voice for similar reasons. We refer to these properties as the acoustic correlates of prosody.  The goal of the larger, collaborative project was to look at similarities and differences in how listeners interpret prosody in English, French, and Spanish. The part of the project reported on here was concerned with French; a research group in Lyon, France, helped us to recruit French students to participate in our study. We were specifically interested in how speakers of the different languages use different ways of communicating which words are most important. In order to do this, we used special software developed for this project at the University of Illinois, in which participants listen to recorded speech and at the same time read a printed transcript of that speech on a computer screen. As they listen to the recording, the participants click on the words shown on the screen to indicate which ones seem most important to them, depending on how the recording sounds.   In English, speakers can emphasize any word in a sentence by making it louder and longer, and listeners interpret these changes accordingly. English speakers have a great deal of freedom as to which word or words in a sentence they emphasize in order to communicate the particular meaning that they wish to convey. French speakers do not manipulate acoustic correlates of prosody as much, and our results show that French listeners? perception of which words are important is correspondingly less influenced by acoustic factors such as duration, compared to English listeners. This is especially true when they are told to pay attention to the sound of the words than instructions to pay attention to the meaning of the words.   An additional topic of investigation for French was the differences in prosody between speech that has been recorded in a laboratory compared to speech produced as part of a conversation between friends. Much research on prosody has been done on "laboratory speech" because it enables researchers to study specific structures that they are interested in, by specifying what their experimental participants are to say. In contrast, this project studied speech produced in ordinary, unplanned conversations that had been previously recorded in Paris by other researchers. We analyzed the acoustic modifications that the speakers in these conversations used when they wanted to add extra emphasis to a word (to "focus" on it in the conversation), and found that many of the acoustic modifications that have been observed in laboratory speech are not present in the conversational speech. We also looked at the prosody produced in dislocations, a specific sentence structure that is commonly used in spoken French. Dislocations are when a word is moved from its usual place in a sentence to either the beginning or end of the sentence. An example in English would be, "Ice cream, I really like." This kind of change in word order is frequent in spoken French, and previous research of laboratory speech has reported that speakers treat the dislocated word ("ice cream") as a separate phrase, with a pause between it and the rest of the sentence. Our analysis of dislocations that speakers happened to produce in conversation showed that they rarely pause after a dislocated word, and do not lengthen it much either, as would be expected if it is a separate phrase. At most they may raise the pitch of their voice at the end of the dislocated word, which indicates that are going to continue speaking.  Investigating these specific aspects of how French speakers use prosody enables us to get a better understanding of the differences among different languages, and the differences between reading aloud in a laboratory compared with spontaneous conversation. Identifying these differences is useful for someone learning how to speak French, and can also be useful in programming computers to interpret spoken French, such as is needed by devices like Siri or Alexa that respond to spoken commands.       Last Modified: 01/09/2018       Submitted by: Caroline L Smith]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
