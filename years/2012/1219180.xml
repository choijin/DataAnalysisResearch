<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:Small: Automatic Generation of Hardware Threads on Programmable Fabrics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>399999.00</AwardTotalIntnAmount>
<AwardAmount>415999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>tao li</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The gap between the speed of processors and that of memory has been widening for decades and will continue to widen with current technology forecasts. This major obstacle to high-performance computing is traditional overcome using techniques that exploit data locality in memory, such as cache memories. However, a very important class of applications commonly referred to as irregular applications, do not exhibit any locality. Examples of such applications include: (1) sparse linear algebra, widely used in science, engineering, medicine, finances, economic modeling etc; and (2) graph algorithms, used in the modeling and analysis of large data such as social networks and the ab-initio construction of genomes from sequenced material etc. Hardware supported multithreaded execution has been shown to mask the latency to memory, and hence can boost the effective parallelism, by suspending a thread waiting for the result of a memory operation and resuming it when the results are available. By doing so the utilization of the computational units is raised to near 100% resulting in a tremendous speedup of the computation.&lt;br/&gt;&lt;br/&gt;This research aims at generating customized hardware for multithreaded execution on configurable devices such as FPGAs. FPGAs (Field Programmable Gate Arrays) are integrated circuits on which arbitrary digital hardware circuits can be configured and reconfigured under software control. Toward this goal, CHAT (Customized Hardware Accelerated Threads) is being developed as a tool that generates a custom multithreaded FPGA processor design tailored for a particular application, based on the C programming language specification of the application. Preliminary results show a potential speedup greater than 10x over traditional memory hierarchy approaches for some irregular applications. The technical deliverables of this project will be: (1) an open-source distributed version of CHAT implemented on high-performance machines with FPGA accelerators; and (2) a detailed analysis of the performance benefits of various compile-time optimizations on various applications.</AbstractNarration>
<MinAmdLetterDate>08/02/2012</MinAmdLetterDate>
<MaxAmdLetterDate>05/03/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1219180</AwardID>
<Investigator>
<FirstName>Walid</FirstName>
<LastName>Najjar</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Walid A Najjar</PI_FULL_NAME>
<EmailAddress>najjar@cs.ucr.edu</EmailAddress>
<PI_PHON>9518274406</PI_PHON>
<NSF_ID>000439876</NSF_ID>
<StartDate>08/02/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Riverside</Name>
<CityName>RIVERSIDE</CityName>
<ZipCode>925210217</ZipCode>
<PhoneNumber>9518275535</PhoneNumber>
<StreetAddress>Research &amp; Economic Development</StreetAddress>
<StreetAddress2><![CDATA[245 University Office Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>44</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA44</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>627797426</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Riverside]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>925210001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>41</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA41</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~399999</FUND_OBLG>
<FUND_OBLG>2013~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>One of the enduring problems in computer design is the large gap between the speed of memory and that of the computing cores such as CPUs and GPUs. &nbsp;In other words, the computing core would have to wait thousands of cycles for the arrival of a data element it fetched from memory.&nbsp;The gap is often referred to as the&nbsp;<em>memory wall</em>.</span></p> <p><span>Traditional computers, such as CPUs and GPUs, mitigate this gap by relying on a large hierarchy of cache memories. These caches exploit the inherent locality in the access to the data by caching copies of segments of the memory on the same chip as the processing core. This approach is extremely effective on applications that exhibit a large degree of locality. However, most applications dealing with "<em>big data" </em>are highly irregular, meaning that they exhibit very poor locality, hence the cannot benefit from the large cache hierarchy. Furthermore, these large cache hierarchies occupy over 80% of the chip area, in high-end servers, and have become the main consumer of its power budget.</span></p> <p><span>&nbsp;</span>In this project we have examined and re-evaluated hardware multithreading as a computational execution model suitable for large scale big data applications. Unlike traditional computers using CPUs or GPUs, this model does not rely on a hierarchy of caches to mask the very large memory latency. Instead, the hardware-supported multithreading is used to <em>mask</em> memory latency. This latency masking mechanism has the advantage of supporting a very large degree of parallelism which exactly what big data applications require.&nbsp;Relying on a wide variety of applications, ranging from bioinformatics, to sparse linear algebra to databases, we have shown that this execution model is capable off delivering a much higher throughput while requiring a substantially smaller power budget.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/28/2017<br>      Modified by: Walid&nbsp;A&nbsp;Najjar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ One of the enduring problems in computer design is the large gap between the speed of memory and that of the computing cores such as CPUs and GPUs.  In other words, the computing core would have to wait thousands of cycles for the arrival of a data element it fetched from memory. The gap is often referred to as the memory wall.  Traditional computers, such as CPUs and GPUs, mitigate this gap by relying on a large hierarchy of cache memories. These caches exploit the inherent locality in the access to the data by caching copies of segments of the memory on the same chip as the processing core. This approach is extremely effective on applications that exhibit a large degree of locality. However, most applications dealing with "big data" are highly irregular, meaning that they exhibit very poor locality, hence the cannot benefit from the large cache hierarchy. Furthermore, these large cache hierarchies occupy over 80% of the chip area, in high-end servers, and have become the main consumer of its power budget.   In this project we have examined and re-evaluated hardware multithreading as a computational execution model suitable for large scale big data applications. Unlike traditional computers using CPUs or GPUs, this model does not rely on a hierarchy of caches to mask the very large memory latency. Instead, the hardware-supported multithreading is used to mask memory latency. This latency masking mechanism has the advantage of supporting a very large degree of parallelism which exactly what big data applications require. Relying on a wide variety of applications, ranging from bioinformatics, to sparse linear algebra to databases, we have shown that this execution model is capable off delivering a much higher throughput while requiring a substantially smaller power budget.          Last Modified: 08/28/2017       Submitted by: Walid A Najjar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
