<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Small: DCM: ESCE: Condensate Database for Efficient Anomaly Detection and Quality Assurance of Massive Cryospheric Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2013</AwardEffectiveDate>
<AwardExpirationDate>04/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>655718</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This award is for addressing the problem of the increasing amount of cryospheric data being generated at an unprecedented rate, and in particular, for the discoverability and higher-level data analytics such as anomaly detection in these big data flows. This work builds on the results from the previously NSF-awarded project aimed at the development of "datarods technology", in which the data are organized in temporal columns (aka "datarods") that include multiple time stamps and sensor values stored sequentially but not necessarily as a fixed time interval. These will lead to investigation and development of innovative techniques for the construction of "Condensate Databases" that are much smaller (but yet capture key characteristics) of the original datasets. The challenges lie in the fact that cryospheric data are massive and diverse, and have "normal" and "abnormal" patterns spanning a wide range of spatial and temporal scales. Three main areas of the study are: (1) adaptive neighborhood-based thresholding in both space and time; (2) compressive-domain pattern detection and change analysis; and (3) hybrid condensation of multi-modal, multi-scale cryospheric data. The techniques and software developed will be shared publicly via the National Snow and Ice Data Center (NSIDC) existing support platform, and publicized via scientific publications and presentations at top-tier conferences such as the America Geophysical Union meetings. The next generation of scientists and technical professionals will be trained in the interdisciplinary domain of Big Data management, data analytics, and scientific discovery through the student curriculum integration, distance learning, and education workshops.</AbstractNarration>
<MinAmdLetterDate>05/15/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1251257</AwardID>
<Investigator>
<FirstName>Qin</FirstName>
<LastName>Lv</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qin Lv</PI_FULL_NAME>
<EmailAddress>Qin.Lv@Colorado.EDU</EmailAddress>
<PI_PHON>3034928821</PI_PHON>
<NSF_ID>000189035</NSF_ID>
<StartDate>05/15/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Gallaher</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David W Gallaher</PI_FULL_NAME>
<EmailAddress>david.gallaher@nsidc.org</EmailAddress>
<PI_PHON>3034921827</PI_PHON>
<NSF_ID>000531579</NSF_ID>
<StartDate>05/15/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090572</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street Room 479]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>5407</Code>
<Text>Polar Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>02XX</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~655718</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary goal of the project was to overcome the I/O bottleneck when analyzing massive remotely-sensed data sets. The project developed methods to create &ldquo;condensate databases&rdquo; that are substantially smaller than the original data sets but still captured key spatio-temporal norms and changes. The databases enable rapid scientific analyses, anomaly detection, and quality assurance (QA) using only in-memory data or limited I/O requests. In particular, visual and statistical QA tools were developed for flagging spatial and temporal discontinuities within massive satellite data sets. The results from this effort have practical applications for assessing data quality across multiple scientific domains.</p> <p><br />Our three-step method includes a series of algorithms that (1) mask-out unnecessary or noisy data, (2) determine&nbsp; statistical norms, which are then used to (3) reduce the data volume by eliminating normative values and retaining only the most significant indicators of change. Furthermore, we have developed a novel clustering-based solution for unsupervised detection of contextual anomalies in remotely sensed data. The main contributions of this work include the design of an unsupervised anomaly detection solution that (1) requires no prior knowledge of the data set, (2) identifies contextual outliers that differ from their spatial-temporal neighbors; and (3) groups contextual outliers into anomalous events to reveal possible underlying processes.</p> <p>&nbsp;</p> <p>The condensation methods successfully reduced massive data set sizes by up to two orders of magnitude.&nbsp; Our development was informed by condensing multiple different data sets, from low-noise level-4 data such as sea ice concentrations to highly variable Special Sensor Microwave Imager/Sounder (SSMI/S) brightness temperatures and NOAA/AVHRR 5 km and 25 km skin temperatures. In each case, we condensed data for both the north and south polar regions, spanning both the Arctic Ocean and all of Antarctica. We also experimented with global data from NASA&rsquo;s Soil Moisture Active/Passive satellite (SMAP). As a final demonstration of the methods, we condensed 17 years of MODIS land surface temperatures (LSTs) that covered the continent of Antarctica at a 1 km resolution. The resulting condensed LST database contains only 2% of the source data yet retains the points of greatest interest. Once generated, the databases can be reused indefinitely.</p> <p><br />Using the methods we have developed, we have been able to identify and validate diverse data quality issues due to systematic or random errors as well as significant natural events. Several significant results are immediately apparent, and perhaps the most striking was the system&rsquo;s ability to QA data sets. Systematic quality problems were identified that had heretofore been undetected. The techniques and results produced in this project have been disseminated via scientific publications, conference presentations, and education materials. We expect our concept and techniques of condensate databases can apply widely to different application domains with massive datasets where efficient anomaly detection and quality assurance are essential.&nbsp; &nbsp;</p><br> <p>            Last Modified: 07/11/2018<br>      Modified by: Qin&nbsp;Lv</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary goal of the project was to overcome the I/O bottleneck when analyzing massive remotely-sensed data sets. The project developed methods to create "condensate databases" that are substantially smaller than the original data sets but still captured key spatio-temporal norms and changes. The databases enable rapid scientific analyses, anomaly detection, and quality assurance (QA) using only in-memory data or limited I/O requests. In particular, visual and statistical QA tools were developed for flagging spatial and temporal discontinuities within massive satellite data sets. The results from this effort have practical applications for assessing data quality across multiple scientific domains.   Our three-step method includes a series of algorithms that (1) mask-out unnecessary or noisy data, (2) determine  statistical norms, which are then used to (3) reduce the data volume by eliminating normative values and retaining only the most significant indicators of change. Furthermore, we have developed a novel clustering-based solution for unsupervised detection of contextual anomalies in remotely sensed data. The main contributions of this work include the design of an unsupervised anomaly detection solution that (1) requires no prior knowledge of the data set, (2) identifies contextual outliers that differ from their spatial-temporal neighbors; and (3) groups contextual outliers into anomalous events to reveal possible underlying processes.     The condensation methods successfully reduced massive data set sizes by up to two orders of magnitude.  Our development was informed by condensing multiple different data sets, from low-noise level-4 data such as sea ice concentrations to highly variable Special Sensor Microwave Imager/Sounder (SSMI/S) brightness temperatures and NOAA/AVHRR 5 km and 25 km skin temperatures. In each case, we condensed data for both the north and south polar regions, spanning both the Arctic Ocean and all of Antarctica. We also experimented with global data from NASA?s Soil Moisture Active/Passive satellite (SMAP). As a final demonstration of the methods, we condensed 17 years of MODIS land surface temperatures (LSTs) that covered the continent of Antarctica at a 1 km resolution. The resulting condensed LST database contains only 2% of the source data yet retains the points of greatest interest. Once generated, the databases can be reused indefinitely.   Using the methods we have developed, we have been able to identify and validate diverse data quality issues due to systematic or random errors as well as significant natural events. Several significant results are immediately apparent, and perhaps the most striking was the system?s ability to QA data sets. Systematic quality problems were identified that had heretofore been undetected. The techniques and results produced in this project have been disseminated via scientific publications, conference presentations, and education materials. We expect our concept and techniques of condensate databases can apply widely to different application domains with massive datasets where efficient anomaly detection and quality assurance are essential.          Last Modified: 07/11/2018       Submitted by: Qin Lv]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
