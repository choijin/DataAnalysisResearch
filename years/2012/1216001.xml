<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Multiterminal Video Coding: From Theory to Practice</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>149644.00</AwardTotalIntnAmount>
<AwardAmount>149644</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Multiterminal (MT) video coding considers the problem of separate compression and joint decompression of multiple correlated video sources. Driven by a host of emerging applications (e.g., in network communications, distributed video sensor networks and camera arrays, and distributed inference), MT video coding has become a very active area of research in the past ten years.&lt;br/&gt;&lt;br/&gt;This research addresses both the theory of MT source coding and practice of MT video coding, with the aim of using the theory to guide practice. It involves: 1) identifying new classes of problems of quadratic Gaussian MT source coding with tight sum-rate bound or giving sufficient conditions for sum-rate tightness; 2) taking an approximation approach when sum-rate tightness cannot be proved; 3) performing MT source code constructions based on Slepian-Wolf coded quantization for random coding and hybrid random-structured coding; and 4) spearheading the practical application of MT video coding for camera arrays and distributed video sensor networks, with focus on depth camera assisted MT video coding.</AbstractNarration>
<MinAmdLetterDate>08/28/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/28/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1216001</AwardID>
<Investigator>
<FirstName>Zixiang</FirstName>
<LastName>Xiong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zixiang Xiong</PI_FULL_NAME>
<EmailAddress>zx@ece.tamu.edu</EmailAddress>
<PI_PHON>9798628683</PI_PHON>
<NSF_ID>000408759</NSF_ID>
<StartDate>08/28/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M Engineering Experiment Station</Name>
<CityName>College Station</CityName>
<ZipCode>778454645</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy S</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>847205572</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A&amp;M ENGINEERING EXPERIMENT STATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas Engineering Experiment Station]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778454645</ZipCode>
<StreetAddress><![CDATA[TEES State Headquarters Bldg.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~149644</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In the past year, progress was made in image denoising through the use of adaptive boosting. In the past decade, much progress has been made in image denoising due to the use of low-rank representation and sparse coding. In the meanwhile, state-of-the-art algorithms also rely on an iteration step to boost the denoising performance. However, the boosting step is fixed or non-adaptive. In this work, we perform rank-1 based fixed-point analysis, then, guided by our analysis, we develop the first adaptive boosting (AB) algorithm, whose convergence is guaranteed. Preliminary results on the same image dataset show that AB uniformly outperforms existing denoising algorithms on every image and at each noise level, with more gains at higher noise levels.</p> <p>Another area of work lies in texture-free large-area depth recovery for planar surfaces, in which we devised a texture-free depth enhancement algorithm for large-area depth recovery. Our proposed algorithm identifies a large-area depth missing region, and iteratively segments its contour by setting different initial pixels in each iteration. Coordinate transformation is used to analyze the distribution of each contour segment. By examining distributions of all contour segments, statistical histogram analysis is applied in our approach to select contour pixels. Then, selected pixels are projected into the world coordinate system, and multiple linear regression is utilized for surface function approximation. Missing depth values of a large-area depth missing region are recovered with guidance of the approximated surface function. Quantitative and qualitative evaluations over state-of-the-art depth enhancement methods demonstrate the effectiveness and superiority of our method. Being texture-free, the proposed method has the flexibility of being merged into traditional depth enhancement methods.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/29/2016<br>      Modified by: Zixiang&nbsp;Xiong</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the past year, progress was made in image denoising through the use of adaptive boosting. In the past decade, much progress has been made in image denoising due to the use of low-rank representation and sparse coding. In the meanwhile, state-of-the-art algorithms also rely on an iteration step to boost the denoising performance. However, the boosting step is fixed or non-adaptive. In this work, we perform rank-1 based fixed-point analysis, then, guided by our analysis, we develop the first adaptive boosting (AB) algorithm, whose convergence is guaranteed. Preliminary results on the same image dataset show that AB uniformly outperforms existing denoising algorithms on every image and at each noise level, with more gains at higher noise levels.  Another area of work lies in texture-free large-area depth recovery for planar surfaces, in which we devised a texture-free depth enhancement algorithm for large-area depth recovery. Our proposed algorithm identifies a large-area depth missing region, and iteratively segments its contour by setting different initial pixels in each iteration. Coordinate transformation is used to analyze the distribution of each contour segment. By examining distributions of all contour segments, statistical histogram analysis is applied in our approach to select contour pixels. Then, selected pixels are projected into the world coordinate system, and multiple linear regression is utilized for surface function approximation. Missing depth values of a large-area depth missing region are recovered with guidance of the approximated surface function. Quantitative and qualitative evaluations over state-of-the-art depth enhancement methods demonstrate the effectiveness and superiority of our method. Being texture-free, the proposed method has the flexibility of being merged into traditional depth enhancement methods.          Last Modified: 08/29/2016       Submitted by: Zixiang Xiong]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
