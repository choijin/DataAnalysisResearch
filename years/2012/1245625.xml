<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Validating Proof Comprehension Tests in Mathematics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Myles Boylan</SignBlockName>
<PO_EMAI>mboylan@nsf.gov</PO_EMAI>
<PO_PHON>7032928670</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Intellectual Merit:  The primary means for teaching advanced mathematics is through the presentation of mathematical proofs. However, both in the classroom and in mathematics education research, students' understanding of these proofs is rarely assessed. The goal of this project is to develop a student comprehension assessment model of theoretical proofs. Specifically, the project is engaged in generating and validating proof comprehension tests for three proofs in a transition-to-proof course.&lt;br/&gt;&lt;br/&gt;The project has three phases. The first stage is developing open-ended proof comprehension assessment questions for three proofs using the Mejia-Ramos model.  A small sample of undergraduate mathematics majors answers these questions in a semi-structured interview and their responses go into the creation a large repository of multiple-choice assessment items. In the second stage a large sample of mathematics majors completes these multiple-choice tests for the three proofs.  Using their responses, items that correlate highly with other items in the tests are dropped using a form of factor analysis to produce a reduced version of the multiple-choice tests for these proofs. Finally, a small sample of undergraduate mathematics majors is interviewed as they complete both the open-ended and the short multiple-choice versions of the assessment tests. This stage seeks to verify that the multiple-choice tests are a valid indicator of the test taker's understanding of the three proofs. The result of this process is expected to be three short, multiple-choice proof comprehension tests that are valid indicators of students' understanding of the proofs being studied.&lt;br/&gt;&lt;br/&gt;Broader impact: Better assessments of the extent to which STEM students comprehend the proofs they read are needed to improve the teaching and learning of proof at the university level.  The failure of some university students to successfully understand mathematical proofs prevents them from entering STEM disciplines. In the particular case of prospective teachers of mathematics, this failure prevents them from gaining the content knowledge they need to teach mathematics effectively. &lt;br/&gt;&lt;br/&gt;The generation of proof comprehension tests serves urgent needs for mathematicians, mathematics majors, and mathematics education researchers. For mathematics faculty, incomplete assessment of students' comprehension of proofs means they do not receive feedback both on the general quality of their lectures and on specific areas of proofs that students may have found confusing. For mathematics majors, assessment of their comprehension of the proofs they are asked to complete encourages them to invest time in studying these proofs; and meaningful assessment items direct their attention to important aspects of the proof that they may not ordinarily consider. Valid proof comprehension tests are also useful to researchers in undergraduate mathematics education by allowing them to measure the effectiveness of different teaching techniques related to proof presentation and systematically address questions about what aspects of proof students find confusing.</AbstractNarration>
<MinAmdLetterDate>09/12/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/12/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1245625</AwardID>
<Investigator>
<FirstName>Keith</FirstName>
<LastName>Weber</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Keith H Weber</PI_FULL_NAME>
<EmailAddress>keith.weber@gse.rutgers.edu</EmailAddress>
<PI_PHON>8489320804</PI_PHON>
<NSF_ID>000200438</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jimmy</FirstName>
<LastName>de la Torre</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jimmy de la Torre</PI_FULL_NAME>
<EmailAddress>j.delatorre@rutgers.edu</EmailAddress>
<PI_PHON>7329320150</PI_PHON>
<NSF_ID>000300073</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Juan Pablo</FirstName>
<LastName>Mejia-Ramos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Juan Pablo Mejia-Ramos</PI_FULL_NAME>
<EmailAddress>pablo.mejia@gse.rutgers.edu</EmailAddress>
<PI_PHON>7329320150</PI_PHON>
<NSF_ID>000548880</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName/>
<StateCode>NJ</StateCode>
<ZipCode>089018559</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7513</Code>
<Text>TUES-Type 1 Project</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0413</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Numerous researchers and influential organizations emphasize that students should engage in the activity of proving throughout their mathematics education (from elementary school to university). In particular, students should be able to successfully understand these rigorous type of mathematical argument and justification. In many cases, failing to do so prevents students from succeeding in STEM disciplines, and in the particular case of prospective teachers, the failure to successfully understand the proofs that they read prevents them from gaining the content knowledge they need to teach mathematics effectively.</p> <p>However, both in the classroom and in mathematics education research, students&rsquo; understanding of proofs is rarely assessed, and this is partly due to the dearth of valid assessments that measure such understandings. In order to improve the teaching and learning of mathematical proof at all levels, we need better assessments of what students comprehend from the proofs that they read.&nbsp;</p> <p>In this project we used a theoretical proof comprehension assessment model to generate and validate reliable proof comprehension tests for three proofs in an undergraduate mathematics course. The careful and methodical process by which we generated these tests led to the development of high quality assessment instruments that are now available for mathematics instructors and mathematics education researchers to use. Furthermore, as part of this process we were able to investigate the dimensionality of students&rsquo; proof comprehension, their levels of comprehension of these proofs, and the extent to which these instruments may be measuring more general competencies.</p> <p>This project was an important proof of concept, one that has already encouraged mathematics educators around the world to follow our methodology to develop and validate their own proof comprehension tests, so that instructors and researchers will eventually have access to a large battery of these tests to use in their classrooms and their own research studies.</p> <p>The generation of the kind of proof comprehension tests we generated in this project serves urgent needs for instructors, students, and mathematics education researchers. For mathematics instructors, these tests provide formative feedback on their presentations. An instructor can have their students take our tests after he or she has presented the corresponding proofs, or simply have students answer specific test items using clickers or mobile devices in the classroom. Students&rsquo; performance on these tests could alert instructors to areas of the proof that their students did not understand, as well as provide more general feedback to the overall quality of their presentation. For students, knowing they will be assessed on their comprehension of the proofs they are asked to read will encourage them to invest time in studying these proofs. Our meaningful assessment items will also direct their attention to important aspects of the proof that they may not ordinarily consider. Finally, our valid proof comprehension tests will be useful to mathematics education researchers by allowing them to measure the effectiveness of different teaching techniques related to proof presentation, and to systematically address questions about what aspects of proof students find confusing.</p><br> <p>            Last Modified: 11/30/2017<br>      Modified by: Juan Pablo&nbsp;Mejia-Ramos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Numerous researchers and influential organizations emphasize that students should engage in the activity of proving throughout their mathematics education (from elementary school to university). In particular, students should be able to successfully understand these rigorous type of mathematical argument and justification. In many cases, failing to do so prevents students from succeeding in STEM disciplines, and in the particular case of prospective teachers, the failure to successfully understand the proofs that they read prevents them from gaining the content knowledge they need to teach mathematics effectively.  However, both in the classroom and in mathematics education research, students? understanding of proofs is rarely assessed, and this is partly due to the dearth of valid assessments that measure such understandings. In order to improve the teaching and learning of mathematical proof at all levels, we need better assessments of what students comprehend from the proofs that they read.   In this project we used a theoretical proof comprehension assessment model to generate and validate reliable proof comprehension tests for three proofs in an undergraduate mathematics course. The careful and methodical process by which we generated these tests led to the development of high quality assessment instruments that are now available for mathematics instructors and mathematics education researchers to use. Furthermore, as part of this process we were able to investigate the dimensionality of students? proof comprehension, their levels of comprehension of these proofs, and the extent to which these instruments may be measuring more general competencies.  This project was an important proof of concept, one that has already encouraged mathematics educators around the world to follow our methodology to develop and validate their own proof comprehension tests, so that instructors and researchers will eventually have access to a large battery of these tests to use in their classrooms and their own research studies.  The generation of the kind of proof comprehension tests we generated in this project serves urgent needs for instructors, students, and mathematics education researchers. For mathematics instructors, these tests provide formative feedback on their presentations. An instructor can have their students take our tests after he or she has presented the corresponding proofs, or simply have students answer specific test items using clickers or mobile devices in the classroom. Students? performance on these tests could alert instructors to areas of the proof that their students did not understand, as well as provide more general feedback to the overall quality of their presentation. For students, knowing they will be assessed on their comprehension of the proofs they are asked to read will encourage them to invest time in studying these proofs. Our meaningful assessment items will also direct their attention to important aspects of the proof that they may not ordinarily consider. Finally, our valid proof comprehension tests will be useful to mathematics education researchers by allowing them to measure the effectiveness of different teaching techniques related to proof presentation, and to systematically address questions about what aspects of proof students find confusing.       Last Modified: 11/30/2017       Submitted by: Juan Pablo Mejia-Ramos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
