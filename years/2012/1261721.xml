<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF21 DIBBS: The Data Exacell</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>7600000.00</AwardTotalIntnAmount>
<AwardAmount>8914035</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Amy Walton</SignBlockName>
<PO_EMAI>awalton@nsf.gov</PO_EMAI>
<PO_PHON>7032924538</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Pittsburgh Supercomputing Center (PSC) will carry out an  accelerated, development pilot project to create, deploy and test software building blocks and hardware implementing functionalities specifically designed to support data-analytic capabilities for data intensive scientific research. Building on the successful Data Supercell (DSC) technology which replaced a conventional tape-based archive with a disk-based system to economically provide the much lower latency and higher bandwidth data success necessary for data-intensive activities, PSC will implement and bring to production quality additional functionalities important to such work.  These include improved local performance, additional abilities for remote data access and storage, enhanced data integrity, data tagging and improved manageability.  PSC will work with partners in diverse fields of science, initially chosen from biology, astronomy and computer science, who will provide scientific and technology drivers and system validation.  The project will leverage current NSF/CI investments in data analytics systems at PSC.  Those investments include DSC, Blacklight (an SGI UV1000 with 2Ã—16TB of hardware-enabled cache-coherent shared memory), and Sherlock (a YarcData ?Urika? graph-analytic appliance which also supports a globally accessible shared memory), both very capable for data analytic applications. Their tight coupling to the pilot storage system will allow synergistic development of analytical capabilities with development of increasingly sophisticated mechanisms for data handling. Working with the new, multi-petabyte data store, they will constitute a system specifically optimized for data intensive work as contrasted with conventional HPC systems. Blacklight will be upgraded with more powerful technology, specifically architected to satisfy the more demanding needs of data analytics in years 3,4. When successful, PSC will engage the NSF to consider larger-scale deployment aiming at exascale capacity.</AbstractNarration>
<MinAmdLetterDate>09/26/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/09/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1261721</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Levine</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael J Levine</PI_FULL_NAME>
<EmailAddress>levine@psc.edu</EmailAddress>
<PI_PHON>4122684960</PI_PHON>
<NSF_ID>000311748</NSF_ID>
<StartDate>09/26/2013</StartDate>
<EndDate>04/21/2016</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ralph</FirstName>
<LastName>Roskies</LastName>
<PI_MID_INIT>Z</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ralph Z Roskies</PI_FULL_NAME>
<EmailAddress>roskies@psc.edu</EmailAddress>
<PI_PHON>4122684960</PI_PHON>
<NSF_ID>000196046</NSF_ID>
<StartDate>09/26/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nicholas</FirstName>
<LastName>Nystrom</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicholas Nystrom</PI_FULL_NAME>
<EmailAddress>nystrom@psc.edu</EmailAddress>
<PI_PHON>4128609095</PI_PHON>
<NSF_ID>000216722</NSF_ID>
<StartDate>05/09/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Nicholas</FirstName>
<LastName>Nystrom</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicholas Nystrom</PI_FULL_NAME>
<EmailAddress>nystrom@psc.edu</EmailAddress>
<PI_PHON>4128609095</PI_PHON>
<NSF_ID>000216722</NSF_ID>
<StartDate>09/26/2013</StartDate>
<EndDate>05/09/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>JRay</FirstName>
<LastName>Scott</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>JRay Scott</PI_FULL_NAME>
<EmailAddress>scott@psc.edu</EmailAddress>
<PI_PHON>4122685835</PI_PHON>
<NSF_ID>000224442</NSF_ID>
<StartDate>03/10/2017</StartDate>
<EndDate>05/09/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>JRay</FirstName>
<LastName>Scott</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>JRay Scott</PI_FULL_NAME>
<EmailAddress>scott@psc.edu</EmailAddress>
<PI_PHON>4122685835</PI_PHON>
<NSF_ID>000224442</NSF_ID>
<StartDate>09/26/2013</StartDate>
<EndDate>04/21/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James Taylor</PI_FULL_NAME>
<EmailAddress>james@taylorlab.org</EmailAddress>
<PI_PHON>4045748771</PI_PHON>
<NSF_ID>000212822</NSF_ID>
<StartDate>09/26/2013</StartDate>
<EndDate>10/20/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Sommerfield</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jason Sommerfield</PI_FULL_NAME>
<EmailAddress>jasons@psc.edu</EmailAddress>
<PI_PHON>4122689606</PI_PHON>
<NSF_ID>000776420</NSF_ID>
<StartDate>05/09/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7726</Code>
<Text>Data Cyberinfrastructure</Text>
</ProgramElement>
<ProgramElement>
<Code>NX12</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8048</Code>
<Text>Data Infrstr Bldg Blocks-DIBBs</Text>
</ProgramReference>
<ProgramReference>
<Code>8237</Code>
<Text>CISE Interagency Agreements</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~4902601</FUND_OBLG>
<FUND_OBLG>2014~1246850</FUND_OBLG>
<FUND_OBLG>2015~2697399</FUND_OBLG>
<FUND_OBLG>2016~67185</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Data Exacell (DXC) was a research&nbsp;pilot project at the Pittsburgh Supercomputing Center (PSC) to create, deploy, and test software and hardware data infrastructure building blocks to enable the coupling of data analytics with innovative storage for scientific research. The DXC project developed and disseminated building blocks and engaged deeply with the research community, particularly with researchers from fields that traditionally have not used high performance computing (HPC), to create valuable new applications. The building blocks developed by the DXC were reused to create production infrastructure of great national value, and many of the pilot applications became important community applications that are now in widespread use.</p> <p>The key outcomes of the Data Exacell project were the data infrastructure building blocks it generated, which were subsequently and successfully reused in other prominent infrastructure and research projects and made available through open source licenses and public repositories. Specific building blocks with high potential for reuse were:</p> <ol> <li>SLASH2 wide-area, network-friendly, distributed filesystem,</li> <li>Weldable Overlay Knack File System (WOKFS) toolkit,</li> <li>psync multiple-stream, rsync-compatible file transport tool,</li> <li>DXC optimizations to the AdaptFS filesystem for in-filesystem analytics,</li> <li>Virtual machine (VM) support for distributed applications,</li> <li>LDAP VM authentication for group-specific hosts and VMs, and</li> <li>heterogeneous scheduling for regular CPU nodes, large-memory CPU nodes, GPUs, and Hadoop.</li> </ol> <p>Of those, building blocks 1&ndash;4 are distributed under an open source license on GitHub, and SLASH2 was deployed at PSC, the Texas Advanced Computing Center (TACC), the National Radio Astronomy Observatory (NRAO), the Minnesota Supercomputing Institute (MSI), and the University of Wyoming. Building blocks 5&ndash;7 were reused to design and implement PSC&rsquo;s Bridges system.</p> <p>Pilot applications in data-intensive research areas were used to motivate, test, demonstrate, and improve the DXC building blocks. The pilot projects spanned diverse research areas including genomics, data integration and fusion, machine learning for multimedia data, radio astronomy, causal analysis for biomedical big data, analysis of streaming social media data with specific application to epidemiology, computational notebooks for teaching digital scholarship, and prototyping the Brain Image Library. The pilot applications were selected according to their ability to advance research through: high data volume, variety, and/or velocity; novel approaches to data management or organization; novel approaches to data integration or fusion; integration of data analytic components into workflows; and complementarity to other DXC pilot applications.</p> <p>The Data Exacell project made a profound impact on the principal discipline of converged architecture for high-performance data analytics (HPDA) leveraging novel storage. This was shown most clearly by transitioning ideas proven in the Data Exacell to the large-scale production architecture of Bridges, which went on to serve 15,179 users at 726 institutions working on over 1,883 projects (statistics as of January 2019). Spanning a heterogeneous collection of compute nodes with a parallel filesystem proved to be a game-changing advance that enabled applications and gateways and has been replicated in other prominent HPC+HPDA systems worldwide.</p> <p>The Data Exacell also positively impacted other disciplines by providing a unique hardware and software architecture on which applications could be built, especially those requiring data-intensive computing and different kinds of processing. Three prominent and highly successful examples that arose from Data Exacell pilot applications are the following:</p> <ul> <li><em>Genomics, Causal Discovery, and Machine Learning:</em> For genomics, including application and democratization of causal discovery algorithms and application of machine learning, the Pittsburgh Genome Resource Repository (PGRR) and TCGA Expedition were built using the Data Exacell.</li> <li><em>Bioinformatics and Data-Intensive Workflows:</em> Galaxy, a workflow framework serving an extremely popular gateway for bioinformatics, was extended using the Data Exacell to allow launching genome sequence assembly jobs on the DXC&rsquo;s large-memory nodes.</li> <li><em>Neuroscience: </em>The Brain Image Library (BIL) was piloted on the Data Exacell. The Brain Image Library is accepting unique, high-resolution, confocal fluorescence microscopy data for mouse, rat, and marmoset brains. The BIL project team is curating the data and ensuring that accurate, useful metadata are maintained, so that datasets and data products can be provided on demand to researchers needing them.</li> </ul> <p>The Data Exacell project also included diverse outreach activities to broaden engagement and workforce development. PSC delivered 15 XSEDE HPC Monthly Workshops on Big Data that featured topics involving high-performance data analytics directly related to the Data Exacell. Topics included graph analytics with RDF and SPARQL, Hadoop, Spark, and artificial intelligence. All the workshops consisted of interactive, HD video instruction, originated at PSC and telecast to remote locations, with hands-on exercises to ensure that participants emerged ready to apply skills learned to their own research problems. The workshops reached 5,015 participants at 64 institutions.</p> <p>The Data Exacell also supported undergraduate and high school internships through which students developed skills in big data, system monitoring, filesystems, wide-area networking, and HPC.</p> <p>&nbsp;</p> <div id="_mcePaste" class="mcePaste" style="position: absolute; left: -10000px; top: 1266px; width: 1px; height: 1px; overflow: hidden;"> <p class="MsoNormal"><span style="font-size: 10.0pt; line-height: 107%;">The Data Exacell also supported undergraduate and high school internships through which students developed skills in big data, system monitoring, filesystems, wide-area networking, and high performance computing.</span></p> </div><br> <p>            Last Modified: 03/28/2019<br>      Modified by: Nicholas&nbsp;Nystrom</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Data Exacell (DXC) was a research pilot project at the Pittsburgh Supercomputing Center (PSC) to create, deploy, and test software and hardware data infrastructure building blocks to enable the coupling of data analytics with innovative storage for scientific research. The DXC project developed and disseminated building blocks and engaged deeply with the research community, particularly with researchers from fields that traditionally have not used high performance computing (HPC), to create valuable new applications. The building blocks developed by the DXC were reused to create production infrastructure of great national value, and many of the pilot applications became important community applications that are now in widespread use.  The key outcomes of the Data Exacell project were the data infrastructure building blocks it generated, which were subsequently and successfully reused in other prominent infrastructure and research projects and made available through open source licenses and public repositories. Specific building blocks with high potential for reuse were:  SLASH2 wide-area, network-friendly, distributed filesystem, Weldable Overlay Knack File System (WOKFS) toolkit, psync multiple-stream, rsync-compatible file transport tool, DXC optimizations to the AdaptFS filesystem for in-filesystem analytics, Virtual machine (VM) support for distributed applications, LDAP VM authentication for group-specific hosts and VMs, and heterogeneous scheduling for regular CPU nodes, large-memory CPU nodes, GPUs, and Hadoop.   Of those, building blocks 1&ndash;4 are distributed under an open source license on GitHub, and SLASH2 was deployed at PSC, the Texas Advanced Computing Center (TACC), the National Radio Astronomy Observatory (NRAO), the Minnesota Supercomputing Institute (MSI), and the University of Wyoming. Building blocks 5&ndash;7 were reused to design and implement PSC?s Bridges system.  Pilot applications in data-intensive research areas were used to motivate, test, demonstrate, and improve the DXC building blocks. The pilot projects spanned diverse research areas including genomics, data integration and fusion, machine learning for multimedia data, radio astronomy, causal analysis for biomedical big data, analysis of streaming social media data with specific application to epidemiology, computational notebooks for teaching digital scholarship, and prototyping the Brain Image Library. The pilot applications were selected according to their ability to advance research through: high data volume, variety, and/or velocity; novel approaches to data management or organization; novel approaches to data integration or fusion; integration of data analytic components into workflows; and complementarity to other DXC pilot applications.  The Data Exacell project made a profound impact on the principal discipline of converged architecture for high-performance data analytics (HPDA) leveraging novel storage. This was shown most clearly by transitioning ideas proven in the Data Exacell to the large-scale production architecture of Bridges, which went on to serve 15,179 users at 726 institutions working on over 1,883 projects (statistics as of January 2019). Spanning a heterogeneous collection of compute nodes with a parallel filesystem proved to be a game-changing advance that enabled applications and gateways and has been replicated in other prominent HPC+HPDA systems worldwide.  The Data Exacell also positively impacted other disciplines by providing a unique hardware and software architecture on which applications could be built, especially those requiring data-intensive computing and different kinds of processing. Three prominent and highly successful examples that arose from Data Exacell pilot applications are the following:  Genomics, Causal Discovery, and Machine Learning: For genomics, including application and democratization of causal discovery algorithms and application of machine learning, the Pittsburgh Genome Resource Repository (PGRR) and TCGA Expedition were built using the Data Exacell. Bioinformatics and Data-Intensive Workflows: Galaxy, a workflow framework serving an extremely popular gateway for bioinformatics, was extended using the Data Exacell to allow launching genome sequence assembly jobs on the DXC?s large-memory nodes. Neuroscience: The Brain Image Library (BIL) was piloted on the Data Exacell. The Brain Image Library is accepting unique, high-resolution, confocal fluorescence microscopy data for mouse, rat, and marmoset brains. The BIL project team is curating the data and ensuring that accurate, useful metadata are maintained, so that datasets and data products can be provided on demand to researchers needing them.   The Data Exacell project also included diverse outreach activities to broaden engagement and workforce development. PSC delivered 15 XSEDE HPC Monthly Workshops on Big Data that featured topics involving high-performance data analytics directly related to the Data Exacell. Topics included graph analytics with RDF and SPARQL, Hadoop, Spark, and artificial intelligence. All the workshops consisted of interactive, HD video instruction, originated at PSC and telecast to remote locations, with hands-on exercises to ensure that participants emerged ready to apply skills learned to their own research problems. The workshops reached 5,015 participants at 64 institutions.  The Data Exacell also supported undergraduate and high school internships through which students developed skills in big data, system monitoring, filesystems, wide-area networking, and HPC.     The Data Exacell also supported undergraduate and high school internships through which students developed skills in big data, system monitoring, filesystems, wide-area networking, and high performance computing.        Last Modified: 03/28/2019       Submitted by: Nicholas Nystrom]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
