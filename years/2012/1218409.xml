<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Small: Collaborative Research: Diffractive masks and algorithms for light field capture</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Diffractive masks and algorithms for light field capture&lt;br/&gt;PIs: Ramesh Raskar, MIT Media Lab  Alyosha Molnar, Cornell ECE&lt;br/&gt;&lt;br/&gt;Advanced imaging and display technology requires integrated, low cost systems able to efficiently capture and characterize light from 3-D scenes. In particular, a 3-D scene can be described by the collection of light rays it generates, called the light field.  This research combines concepts from mask-based light-field imaging with angle sensitive pixels (ASPs).  While mask-based light-field capture is much better understood mathematically, and masks are cheaper to manufacture and more easily modified on-the-fly, diffractive ASPs provide smaller, denser light field sensors, and provide naturally compressible outputs.  This project combines the physics and signal processing of these approaches to enable optical imaging systems that capture more information than normal cameras while reducing the system's complexity.  This work broadly impacts diverse applications spanning consumer imaging and displays, machine vision and automation, scientific/medical imaging and displays, robotic surgery, surveillance and remote sensing.&lt;br/&gt;&lt;br/&gt;3-D images and video can be captured by measuring the combined spatial and angular distribution of light (the light field).  This research combines two techniques for light-field capture: mask-based light-field imaging and diffractive angle sensitive pixels (ASPs).  A critical element of this work is the development of a mathematical framework that maps between conventional geometric light fields and the diffractive optics upon which ASPs rely.  A second element is constructing hybrid systems based on this mathematics, leveraging diffractive effects in mask design, and combining masks with ASPs in single light-field cameras.  This work also combines formalisms in existing light field methods with knowledge about real 3-D scene statistics to develop optimal (in the sense of usability and compressibility) basis sets for sampling and encoding the light-field. All of these aspects combine to reduce the size, cost and complexity of light field cameras, while simultaneously enhancing their capabilities.</AbstractNarration>
<MinAmdLetterDate>07/31/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/23/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1218409</AwardID>
<Investigator>
<FirstName>Alyosha</FirstName>
<LastName>Molnar</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alyosha C Molnar</PI_FULL_NAME>
<EmailAddress>am699@cornell.edu</EmailAddress>
<PI_PHON>6072555014</PI_PHON>
<NSF_ID>000498150</NSF_ID>
<StartDate>07/31/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148535401</ZipCode>
<StreetAddress><![CDATA[327 Phillips Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~83960</FUND_OBLG>
<FUND_OBLG>2013~85703</FUND_OBLG>
<FUND_OBLG>2014~80337</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project started out being focused on hybrid techniques for light-field imaging, combining diffractive pixels, dappled masks, inverse theory, and sparse reconstruction.&nbsp; Whereas traditional imaging systems ultimately just capture a map of light intensity at a 2-d plane (either on film or an electronic image sensor) behind a lens, light-field imaging also captures information about the incident angle of incoming light rays.&nbsp; This additional information primarily provides information about 3-D structure in scenes, both in terms of depth, and by enabling computational refocusing and synthetic parallax.&nbsp; Light-field information can be captured in multiple ways, and the resulting data can also be processed to reconstruct 3-D scenes in multiple ways.&nbsp; All approaches to light-field capture, however, lose some information about the imaged scene, which has to be filled in, or smoothed over, by subsequent computational image processing.&nbsp;</p> <p>In a joint effort with MIT, we sought, first, to answer two questions : 1) what is the best way to capture light field information (in terms of minimum information loss, and physical size of camera) and 2) how do we best reconstruct light fields given such capture?&nbsp; These two questions are related, since a good capture mechanism will focus on capturing the kind of data that is most useful for 3-D reconstruction.&nbsp; Our goal was to answer these questions, and use the result to build higher performance, more efficient light-field imaging systems.</p> <p>What we found was that one of our existing techniques (diffractive angle sensitive pixels, or ASPs) when arranged properly, are very close to optimum for natural light-field scenes.&nbsp; In particular ASP arrays provide a smooth trade-off between angular and spatial information for different parts of a scene that are closer to/farther from a camera&rsquo;s focal depth, where most other techniques have a fixed trade-off between angular and spatial resolution.&nbsp; At the same time, ASPs provide an inherently sparse representation of the light-fields of most real scenes, reducing computational overhead in reconstruction.&nbsp; Based on this work, we both know the limits on light field capture, and have a good sense for how to build an optimized ASP-based light-field sensor.</p> <p>We also developed three different techniques for reconstruction based on such ASPs, ranging from very fast linear reconstruction to much slower, but higher resolution sparse reconstruction, as well as an intermediate, fast technique using deep learning.&nbsp; Critically, these latter two techniques can accurately fill in the light-field for scenes with a mix of depths relative to the focal depth of a camera, without giving up significant information in the space-angle trade-off.</p> <p>We also concluded that while our capture techniques were already near optimal for passive light fields, they could be enhanced by combining them with existing active 3-D imaging techniques, especially time-of-flight (TOF).&nbsp; The resulting extraction &ldquo;depth fields&rdquo; which we explored both experimentally and theoretically, provides a number of benefits, but especially helps in cases where one is seeking to extract the depth of objects that are partially occluded by other, closer reflective objects (such as leaves of a plant).</p> <p>In addition to advancing the state-of-the-art in 2-D imaging and computer vision, this work provided technical education and research opportunities for more than half a dozen students, including multiple undergraduates, master&rsquo;s and PhD students, most of whom have now joined the technological work force.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/10/2017<br>      Modified by: Alyosha&nbsp;C&nbsp;Molnar</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484078333521_Fig1_LF_NSF--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484078333521_Fig1_LF_NSF--rgov-800width.jpg" title="Concept Figure Lightfield Reconstruction"><img src="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484078333521_Fig1_LF_NSF--rgov-66x44.jpg" alt="Concept Figure Lightfield Reconstruction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Prototype angle sensitive pixel camera (left). The data recorded by the camera prototypecan be processed to recover a high-resolution 4D light field (center). As seen in the close-ups on the right, parallax is recovered from a single camera image using sparse reconstruction.</div> <div class="imageCredit">M Hirsch, S Sivaramakrishnan, S Jayasuriya, A Wang, A Molnar, R Raskar, G Wetzstein, " A Switchable Light Field Camera Architecture with Angle Sensitive Pixels and Dictionary-based Sparse Coding". To be presented at International Conference on Computational Photography (ICCP) May 2014</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Concept Figure Lightfield Reconstruction</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484078898777_Fig2_LF_NSF--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484078898777_Fig2_LF_NSF--rgov-800width.jpg" title="Description of light-field capture and reconstruction quality."><img src="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484078898777_Fig2_LF_NSF--rgov-66x44.jpg" alt="Description of light-field capture and reconstruction quality."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Top: Illustration of ASP sensor layout (left) and sampled spatio-angular frequencies (right). Middle and Bottom: comparing reconstruction accuracy of ASP array vs other forms of light-field capture across focal conditions.</div> <div class="imageCredit">6)M Hirsch, S Sivaramakrishnan, S Jayasuriya, A Wang, A Molnar, R Raskar, G Wetzstein, " A Switchable Light Field Camera Architecture with Angle Sensitive Pixels and Dictionary-based Sparse Coding". To be presented at International Conference on Computational Photography (ICCP) May 2014</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Description of light-field capture and reconstruction quality.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484079135331_Fig3_LF_NSF--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484079135331_Fig3_LF_NSF--rgov-800width.jpg" title="Deep Neural Net for Light-Field Reconstruction"><img src="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484079135331_Fig3_LF_NSF--rgov-66x44.jpg" alt="Deep Neural Net for Light-Field Reconstruction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The figure shows the 2-branch architecture for light-field reconstruction using a deep neural net.</div> <div class="imageCredit">Suren Jayasuriya's Thesis</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Deep Neural Net for Light-Field Reconstruction</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484079600924_Fig4_LF_NSF--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484079600924_Fig4_LF_NSF--rgov-800width.jpg" title="Example of Depth Field reconstructions"><img src="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484079600924_Fig4_LF_NSF--rgov-66x44.jpg" alt="Example of Depth Field reconstructions"></a> <div class="imageCaptionContainer"> <div class="imageCaption">a) Captured scene, b-e) Digital refocusing on different focal planes for the depth mapof the scene, showing how depth field imaging can break the tradeoff between aperture and depth of field for range imaging.</div> <div class="imageCredit">3)Jayasuriya, S. , Pediredla, A., Sivaramakrishnan, S., Molnar, A., Veeraraghavan, A.? Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging?, 2015 International Conference on 3D Vision (3DV), Lyon, France, Oct 2015.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Example of Depth Field reconstructions</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484080222802_Fig5_LF_NSF--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484080222802_Fig5_LF_NSF--rgov-800width.jpg" title="Example of Depth Fields for TOF through Occluders"><img src="/por/images/Reports/POR/2017/1218409/1218409_10195863_1484080222802_Fig5_LF_NSF--rgov-66x44.jpg" alt="Example of Depth Fields for TOF through Occluders"></a> <div class="imageCaptionContainer"> <div class="imageCaption">(a) Scene with toy monkey partly occluded by a plant in the foreground, (b) computational refocusing and (c) depth finding is corrupted by occlusion, (d) histogram of TOF depth shows plant and monkey, (e) like (c) after removal of plant pixels shows sharp depth image of monkey, (f) Comparison.</div> <div class="imageCredit">3)Jayasuriya, S. , Pediredla, A., Sivaramakrishnan, S., Molnar, A., Veeraraghavan, A.? Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging?, 2015 International Conference on 3D Vision (3DV), Lyon, France, Oct 2015.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Example of Depth Fields for TOF through Occluders</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project started out being focused on hybrid techniques for light-field imaging, combining diffractive pixels, dappled masks, inverse theory, and sparse reconstruction.  Whereas traditional imaging systems ultimately just capture a map of light intensity at a 2-d plane (either on film or an electronic image sensor) behind a lens, light-field imaging also captures information about the incident angle of incoming light rays.  This additional information primarily provides information about 3-D structure in scenes, both in terms of depth, and by enabling computational refocusing and synthetic parallax.  Light-field information can be captured in multiple ways, and the resulting data can also be processed to reconstruct 3-D scenes in multiple ways.  All approaches to light-field capture, however, lose some information about the imaged scene, which has to be filled in, or smoothed over, by subsequent computational image processing.   In a joint effort with MIT, we sought, first, to answer two questions : 1) what is the best way to capture light field information (in terms of minimum information loss, and physical size of camera) and 2) how do we best reconstruct light fields given such capture?  These two questions are related, since a good capture mechanism will focus on capturing the kind of data that is most useful for 3-D reconstruction.  Our goal was to answer these questions, and use the result to build higher performance, more efficient light-field imaging systems.  What we found was that one of our existing techniques (diffractive angle sensitive pixels, or ASPs) when arranged properly, are very close to optimum for natural light-field scenes.  In particular ASP arrays provide a smooth trade-off between angular and spatial information for different parts of a scene that are closer to/farther from a camera?s focal depth, where most other techniques have a fixed trade-off between angular and spatial resolution.  At the same time, ASPs provide an inherently sparse representation of the light-fields of most real scenes, reducing computational overhead in reconstruction.  Based on this work, we both know the limits on light field capture, and have a good sense for how to build an optimized ASP-based light-field sensor.  We also developed three different techniques for reconstruction based on such ASPs, ranging from very fast linear reconstruction to much slower, but higher resolution sparse reconstruction, as well as an intermediate, fast technique using deep learning.  Critically, these latter two techniques can accurately fill in the light-field for scenes with a mix of depths relative to the focal depth of a camera, without giving up significant information in the space-angle trade-off.  We also concluded that while our capture techniques were already near optimal for passive light fields, they could be enhanced by combining them with existing active 3-D imaging techniques, especially time-of-flight (TOF).  The resulting extraction "depth fields" which we explored both experimentally and theoretically, provides a number of benefits, but especially helps in cases where one is seeking to extract the depth of objects that are partially occluded by other, closer reflective objects (such as leaves of a plant).  In addition to advancing the state-of-the-art in 2-D imaging and computer vision, this work provided technical education and research opportunities for more than half a dozen students, including multiple undergraduates, master?s and PhD students, most of whom have now joined the technological work force.          Last Modified: 01/10/2017       Submitted by: Alyosha C Molnar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
