<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:   Constituents and heads in prosody perception:   A comparative study</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>202581.00</AwardTotalIntnAmount>
<AwardAmount>202581</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Joan Maling</SignBlockName>
<PO_EMAI>jmaling@nsf.gov</PO_EMAI>
<PO_PHON>7032928046</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates how prosody is used in spoken language to signal how words are grouped into phrases and how each word and phrase contributes information to the discourse. In text these functions are signaled through punctuation and text enhancement (eg., boldface), but in spoken language pitch, timing, loudness, voice quality, and other properties of speech convey such information. Linguists propose that underlying the prosody of all languages is a universal structure, the Headed Constituent (HC), which groups syllables and words together, with one prominent element per constituent designated the Head element. These constituents determine how speech sounds are coordinated (at a physical level), and how speech is mapped onto syntactic and semantic structures that determine utterance meaning. &lt;br/&gt;&lt;br/&gt;Prosodic constituents are examined in English, Spanish, and French, three languages that are known to differ not only in their prosody (eg., intonation and rhythmic patterns), but also in the associations linking prosody to syntax and semantics. Experiments conducted in Illinois, Barcelona and Lyon will show how how listeners perceive the prosodic phrasing and prominence patterns of an utterance when presented with speech samples that differ in their phonetic properties (pitch and timing), and in syntactic and semantic features. A novel method of real-time, auditory transcription with non-expert listeners and conversational speech samples is intended to best approximate conditions of normal language use. Parallel experiments on English, Spanish, and French will provide critical evidence regarding the universality of the Headed Constituent as the structure that underlies prosodic form, and will also shed light on differences among languages in the role of prosody in communicating linguistic meaning. &lt;br/&gt;&lt;br/&gt;Project findings will contribute valuable benchmark data on how prosody functions in conversational speech, with future applications in clinical settings to identify speech disorders involving prosody, in second language teaching, and in development of speech technologies for human-computer interfaces.</AbstractNarration>
<MinAmdLetterDate>02/05/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1251343</AwardID>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Cole</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer Cole</PI_FULL_NAME>
<EmailAddress>jennifer.cole1@northwestern.edu</EmailAddress>
<PI_PHON>8474917020</PI_PHON>
<NSF_ID>000383964</NSF_ID>
<StartDate>02/05/2013</StartDate>
<EndDate>07/13/2016</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jose</FirstName>
<LastName>Hualde</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jose I Hualde</PI_FULL_NAME>
<EmailAddress>jihualde@illinois.edu</EmailAddress>
<PI_PHON>2173333390</PI_PHON>
<NSF_ID>000388774</NSF_ID>
<StartDate>07/13/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jose</FirstName>
<LastName>Hualde</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jose I Hualde</PI_FULL_NAME>
<EmailAddress>jihualde@illinois.edu</EmailAddress>
<PI_PHON>2173333390</PI_PHON>
<NSF_ID>000388774</NSF_ID>
<StartDate>02/05/2013</StartDate>
<EndDate>07/13/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress><![CDATA[1901 South First St., Suite A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~202581</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Prosody in spoken language serves a similar function as punctuation and font stylizing in text, lending texture to spoken utterances through patterns of pitch, tempo and intensity modulation that extend over phrases of varying word length. The goal of this project was to understand how listeners perceive prosody, in terms of identifying words that stand out as prominent in an utterance, and the grouping words into phrases. Both of these functions (prominence, phrasing) are essential for comprehending sentences and larger discourse segments, especially as speech is naturally produced, in a continuous flow of words.&nbsp; In a series of experiments with native speakers of English, French and Spanish, we find differences among languages, and among individuals, in the way listeners attend to pitch, timing and loudness in spoken utterances, and how they make use of those acoustic dimensions as cues for identifying prominent words and phrasal word groupings. In addition, we find differences in the perception of&nbsp; prosody that reflect listeners&rsquo; expectations about word prominence and phrasing based on the prosodic patterns that are most typical in their native language. In other words, we find evidence that the prosodic form of an utterance is on one hand perceived based on cues present in the speech signal, but is also in some contexts an auditory hallucination, as patterns are perceived for which little or no cues exist in the in the speech signal. By tracking signal-driven and illusory prosodic features in languages with very different prosodic systems, we determine that prosody perceived as an auditory illusion reflects listener bias from past experience with their native language.</p> <p>To investigate how listeners perceive prominent words and phrasal grouping in ordinary speech, we developed a novel experimental paradigm using a simple transcription task, Rapid Prosody Transcription (RPT), and a web-based platform (LMEDS) for collecting prosodic transcriptions over the internet from remote participants, whom we recruit through a crowd-sourcing platform. This experimental platform enabled us to collect perceptual ratings of prosody from over one hundred participants, using speech samples from existing databases of recorded conversational speech. The RPT method and the LMEDS web platform not only supplied data for our cross-linguistic research on prosody, but also offer valuable tools for future efforts to build prosodically annotated speech databases for the development of computer speech technologies, in any spoken language. We conducted extensive validation tests of the RPT method, with statistical modeling of individual listener behavior, which demonstrate that RPT is a reliable, efficient and cost-effective way to obtain coarse prosodic annotations of speech such as those needed to develop computer prosodically natural computer speech technologies.</p> <p>&nbsp;</p> <p>This project looks at differences in how listeners hear prosody in English, Spanish and French because these languages differ in their prosodic sound patterns (e.g., the pattern of pitch rises and falls across an utterance), and in the linguistic meaning that is conveyed through those prosodic patterns. For example, a pitch rise-fall contour on a word in an English sentence can mark a semantic focus or emphasis on that word, while prosody does not play exactly the same focus-marking function in Spanish and French. Simply stated, our research asks whether speakers of these languages listen for prosodic cues in the speech signal in a similar fashion, or whether their listening strategies differ in a way that reflects the prosodic &ldquo;accent&rdquo; of each language. Our evidence from listeners&rsquo; ratings of prosody reveal striking similarities in listeners&rsquo; sensitivity to the range prosodic cues present in speech in all three languages, which suggests a universal basis to prosody, perhaps originating in auditory processing. At the same time, we find that English speakers pay more attention to prosodic cues in speech when judging word prominence, which is interesting because among the studied languages, English is the only one where prosodic marking is the primary strategy for marking the semantic focus of a sentence. Our findings suggest that a listener&rsquo;s perceptual strategy is tailored to allow more attentio to cues that are critical for comprehending speech in their language. In other words, speakers of different languages listen differently to the prosody in their respective languages.</p> <p>In sum, we have shown that regardless of language, listeners in our study are sensitive to prosodic sound patterns expressed in pitch, tempo, and intensity, but that sensitivity levels differ in degree among these languages. We also ask if&nbsp; listeners from the same language group in our study use similar listening strategies. Here the answer is a surprising no! For the 32 English speakers in our study, there were substantial differences, with some attending to all available cues to prominent words, and others relying exclusively on cues having to do with timing measures. This analysis shows that sensitivity to pitch cues varies substantially among listeners.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/30/2017<br>      Modified by: Jose&nbsp;I&nbsp;Hualde</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Prosody in spoken language serves a similar function as punctuation and font stylizing in text, lending texture to spoken utterances through patterns of pitch, tempo and intensity modulation that extend over phrases of varying word length. The goal of this project was to understand how listeners perceive prosody, in terms of identifying words that stand out as prominent in an utterance, and the grouping words into phrases. Both of these functions (prominence, phrasing) are essential for comprehending sentences and larger discourse segments, especially as speech is naturally produced, in a continuous flow of words.  In a series of experiments with native speakers of English, French and Spanish, we find differences among languages, and among individuals, in the way listeners attend to pitch, timing and loudness in spoken utterances, and how they make use of those acoustic dimensions as cues for identifying prominent words and phrasal word groupings. In addition, we find differences in the perception of  prosody that reflect listeners? expectations about word prominence and phrasing based on the prosodic patterns that are most typical in their native language. In other words, we find evidence that the prosodic form of an utterance is on one hand perceived based on cues present in the speech signal, but is also in some contexts an auditory hallucination, as patterns are perceived for which little or no cues exist in the in the speech signal. By tracking signal-driven and illusory prosodic features in languages with very different prosodic systems, we determine that prosody perceived as an auditory illusion reflects listener bias from past experience with their native language.  To investigate how listeners perceive prominent words and phrasal grouping in ordinary speech, we developed a novel experimental paradigm using a simple transcription task, Rapid Prosody Transcription (RPT), and a web-based platform (LMEDS) for collecting prosodic transcriptions over the internet from remote participants, whom we recruit through a crowd-sourcing platform. This experimental platform enabled us to collect perceptual ratings of prosody from over one hundred participants, using speech samples from existing databases of recorded conversational speech. The RPT method and the LMEDS web platform not only supplied data for our cross-linguistic research on prosody, but also offer valuable tools for future efforts to build prosodically annotated speech databases for the development of computer speech technologies, in any spoken language. We conducted extensive validation tests of the RPT method, with statistical modeling of individual listener behavior, which demonstrate that RPT is a reliable, efficient and cost-effective way to obtain coarse prosodic annotations of speech such as those needed to develop computer prosodically natural computer speech technologies.     This project looks at differences in how listeners hear prosody in English, Spanish and French because these languages differ in their prosodic sound patterns (e.g., the pattern of pitch rises and falls across an utterance), and in the linguistic meaning that is conveyed through those prosodic patterns. For example, a pitch rise-fall contour on a word in an English sentence can mark a semantic focus or emphasis on that word, while prosody does not play exactly the same focus-marking function in Spanish and French. Simply stated, our research asks whether speakers of these languages listen for prosodic cues in the speech signal in a similar fashion, or whether their listening strategies differ in a way that reflects the prosodic "accent" of each language. Our evidence from listeners? ratings of prosody reveal striking similarities in listeners? sensitivity to the range prosodic cues present in speech in all three languages, which suggests a universal basis to prosody, perhaps originating in auditory processing. At the same time, we find that English speakers pay more attention to prosodic cues in speech when judging word prominence, which is interesting because among the studied languages, English is the only one where prosodic marking is the primary strategy for marking the semantic focus of a sentence. Our findings suggest that a listener?s perceptual strategy is tailored to allow more attentio to cues that are critical for comprehending speech in their language. In other words, speakers of different languages listen differently to the prosody in their respective languages.  In sum, we have shown that regardless of language, listeners in our study are sensitive to prosodic sound patterns expressed in pitch, tempo, and intensity, but that sensitivity levels differ in degree among these languages. We also ask if  listeners from the same language group in our study use similar listening strategies. Here the answer is a surprising no! For the 32 English speakers in our study, there were substantial differences, with some attending to all available cues to prominent words, and others relying exclusively on cues having to do with timing measures. This analysis shows that sensitivity to pitch cues varies substantially among listeners.           Last Modified: 09/30/2017       Submitted by: Jose I Hualde]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
