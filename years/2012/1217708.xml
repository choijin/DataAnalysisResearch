<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: SMALL: Collaborative Research: Data Structures for Parallel Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>138999.00</AwardTotalIntnAmount>
<AwardAmount>138999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops a theory for characterizing the performance of parallel data structures and parallel algorithms that use parallel structures.  Standard metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (critical-path length), do not naturally generalize in the presence of contention on shared data.  Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.&lt;br/&gt;&lt;br/&gt;The specific research goals are as follows:  &lt;br/&gt;(1) Investigate a methodology for designing and analyzing parallel algorithms that use data structures, especially amortized ones. &lt;br/&gt;(2) Design parallel schedulers that ameliorate the contention on parallel data structures. &lt;br/&gt;(3) Design parallel data structures that perform provably well with these schedulers.&lt;br/&gt;&lt;br/&gt;Today parallel computing is ubiquitous.  Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors.  Writing parallel code that provably scales well is challenging and techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code.  This project will develop a theoretical foundation for characterizing the scalability of parallel programs that contend for access to shared data.</AbstractNarration>
<MinAmdLetterDate>07/20/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217708</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Bender</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael A Bender</PI_FULL_NAME>
<EmailAddress>bender@cs.stonybrook.edu</EmailAddress>
<PI_PHON>6316327835</PI_PHON>
<NSF_ID>000092778</NSF_ID>
<StartDate>07/20/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Stony Brook]]></Name>
<CityName>Stony Brook</CityName>
<StateCode>NY</StateCode>
<ZipCode>117944400</ZipCode>
<StreetAddress><![CDATA[Department of Computer Science]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~138999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed a theory for characterizing the performance of parallel algorithms that use parallel data structures. Common metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (critical-path length), do not naturally generalize when there is contention on shared data. Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.</p> <p>This research project resulted in the following major outcomes:&nbsp;</p> <p>(1) The PIs developed methodologies for designing and analyzing parallel algorithms that use data structures, including amortized data structures. &nbsp;Some of these approaches even apply to programs that store and access data from (traditional or solid state) disk.</p> <p>(2) The PIs implemented and published new parallel schedulers that ameliorate the contention on parallel data structures.</p> <p>(3) The PIs designed parallel data structures that perform provably well with these schedulers.</p> <p>These outcomes are significant because today parallel computing is ubiquitous. &nbsp;Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors. &nbsp;Writing parallel code that provably scales well is challenging, however, because techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code. This project helped develop theoretical techniques for accessing shared data to make it easier to write provably good scalable parallel code.<br /><br /><br /><br /><br /></p><br> <p>            Last Modified: 10/30/2016<br>      Modified by: Michael&nbsp;A&nbsp;Bender</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed a theory for characterizing the performance of parallel algorithms that use parallel data structures. Common metrics for parallel algorithms, such as "work" (total amount of computation) and "span" (critical-path length), do not naturally generalize when there is contention on shared data. Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.  This research project resulted in the following major outcomes:   (1) The PIs developed methodologies for designing and analyzing parallel algorithms that use data structures, including amortized data structures.  Some of these approaches even apply to programs that store and access data from (traditional or solid state) disk.  (2) The PIs implemented and published new parallel schedulers that ameliorate the contention on parallel data structures.  (3) The PIs designed parallel data structures that perform provably well with these schedulers.  These outcomes are significant because today parallel computing is ubiquitous.  Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors.  Writing parallel code that provably scales well is challenging, however, because techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code. This project helped develop theoretical techniques for accessing shared data to make it easier to write provably good scalable parallel code.            Last Modified: 10/30/2016       Submitted by: Michael A Bender]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
