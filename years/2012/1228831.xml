<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Nimble Assessments: Tools for the Design and Analysis of Interactive Assessments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>799999.00</AwardTotalIntnAmount>
<AwardAmount>799999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Connie Della-Piana</SignBlockName>
<PO_EMAI>cdellapi@nsf.gov</PO_EMAI>
<PO_PHON>7032925309</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This full-scale project is developing and adapting innovative methodologies and tools to facilitate the design of interactive assessments of student learning processes that are mediated by technology.  Building on previous work, the project incorporates three components and capabilities into an existing platform that makes it suitable for implementation in a wide range of learning environments.  The components are authoring tools, crowd-sourcing resources, and automated support for data abstraction.&lt;br/&gt;  &lt;br/&gt;A team of researchers and developers from Stanford University lead the effort and are developing the following products (1) an authoring language to develop interactive assessments of student learning; (2) a crowd sourcing dashboard to facilitate validation of measures; and (3) a data abstraction platform to assist in the collection and analysis of the rich corpus of data associated with these types of assessments.&lt;br/&gt;&lt;br/&gt;With an increased interest in the development and use of a wide range of ways to assess student learning, this full scale research and development project focuses on the development of an assessment infrastructure that supports the creation and use of new forms of assessments and analyses.   The main platform is designed to support subsequent extensions and feature addition in response to the dynamic nature of knowledge about student learning and new approaches to measuring student outcomes.</AbstractNarration>
<MinAmdLetterDate>09/24/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/24/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1228831</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Schwartz</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel L Schwartz</PI_FULL_NAME>
<EmailAddress>daniel.schwartz@stanford.edu</EmailAddress>
<PI_PHON>6507361514</PI_PHON>
<NSF_ID>000470265</NSF_ID>
<StartDate>09/24/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>943052023</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0412</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~799999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A good education yields many positive outcomes: knowledge, for one, but also the ability to make educated choices in the future.&nbsp; Once students leave the confines of school, they need to choose whether, how, and what to learn on their own, because no education can fully anticipate the future.&nbsp; Currently, there are few assessments of whether students have learned to choose well.&nbsp; This NSF project developed the computational infrastructure to develop, deploy, and evaluate people&rsquo;s learning choices.&nbsp; This infrastructure has made it possible to create choice-based assessments and deliver them at scale.&nbsp; Choice-based assessments have detected important learning catalysts that are invisible to current measurement approaches.&nbsp; The choice-based assessments come in the form of 10-15 minute video games that are fun to play but also include telling decision points.&nbsp; For example, in one game, players create posters for a fun fair.&nbsp; Their posters are submitted to a focus group of agents that have artificial intelligence, so the agents can provide useful feedback on the graphic qualities of the posters.&nbsp; Players have a choice of asking the agents for positive feedback or critical feedback.&nbsp; Regardless of age, students who choose more constructive criticism learn more about graphical design principles from the game than those who chose more positive feedback, despite the informational equivalence of the positive and negative feedback.&nbsp; Additionally, there is a strong correlation between standardized tests of academic achievement and students&rsquo; willingness to choose constructive criticism.&nbsp; Given the choice-based assessments, it is possible to evaluate the effectiveness of instruction for influencing important learning choices. For example, one study demonstrated that a science curriculum that emphasized seeking criticism to improve a product led to transfer in another context such that students were more likely to seek constructive criticism in the choice-based assessments. The effect was most pronounced for low-achieving students, who were likely to shy away from constructive criticism without the instruction.&nbsp; By creating assessments of people&rsquo;s choices with respect to learning, it is possible to conduct research on which choices are beneficial for learning and what features of instruction lead to those choices.&nbsp; Thus far, the work has produced and validated assessments of choices to engage in critical thinking, to seek constructive criticism, to explore multiple alternatives, to conduct systematic testing, to plan ahead, and to persist and share.</p><br> <p>            Last Modified: 12/21/2015<br>      Modified by: Daniel&nbsp;L&nbsp;Schwartz</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/1228831/1228831_10218025_1450499196666_Posterlet_screenshot--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/1228831/1228831_10218025_1450499196666_Posterlet_screenshot--rgov-800width.jpg" title="Posterlet Screenshot"><img src="/por/images/Reports/POR/2015/1228831/1228831_10218025_1450499196666_Posterlet_screenshot--rgov-66x44.jpg" alt="Posterlet Screenshot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">In the Posterlet game, students designed posters for a Fall Fun Fair and chose whether to receive positive or negative feedback on their creations from a set of animal critics.</div> <d...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A good education yields many positive outcomes: knowledge, for one, but also the ability to make educated choices in the future.  Once students leave the confines of school, they need to choose whether, how, and what to learn on their own, because no education can fully anticipate the future.  Currently, there are few assessments of whether students have learned to choose well.  This NSF project developed the computational infrastructure to develop, deploy, and evaluate peopleÃ†s learning choices.  This infrastructure has made it possible to create choice-based assessments and deliver them at scale.  Choice-based assessments have detected important learning catalysts that are invisible to current measurement approaches.  The choice-based assessments come in the form of 10-15 minute video games that are fun to play but also include telling decision points.  For example, in one game, players create posters for a fun fair.  Their posters are submitted to a focus group of agents that have artificial intelligence, so the agents can provide useful feedback on the graphic qualities of the posters.  Players have a choice of asking the agents for positive feedback or critical feedback.  Regardless of age, students who choose more constructive criticism learn more about graphical design principles from the game than those who chose more positive feedback, despite the informational equivalence of the positive and negative feedback.  Additionally, there is a strong correlation between standardized tests of academic achievement and studentsÃ† willingness to choose constructive criticism.  Given the choice-based assessments, it is possible to evaluate the effectiveness of instruction for influencing important learning choices. For example, one study demonstrated that a science curriculum that emphasized seeking criticism to improve a product led to transfer in another context such that students were more likely to seek constructive criticism in the choice-based assessments. The effect was most pronounced for low-achieving students, who were likely to shy away from constructive criticism without the instruction.  By creating assessments of peopleÃ†s choices with respect to learning, it is possible to conduct research on which choices are beneficial for learning and what features of instruction lead to those choices.  Thus far, the work has produced and validated assessments of choices to engage in critical thinking, to seek constructive criticism, to explore multiple alternatives, to conduct systematic testing, to plan ahead, and to persist and share.       Last Modified: 12/21/2015       Submitted by: Daniel L Schwartz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
