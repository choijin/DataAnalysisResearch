<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Topical Positioning System (TPS) for Informed Reading of Web Pages</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>499752.00</AwardTotalIntnAmount>
<AwardAmount>499752</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This work addresses the challenge of increasing the critical literacy of people looking for information on the Web, including information regarding healthcare, policy, or any other broadly discussed topic. The proposed research on Topical Positioning System "TPS" drives the vision of developing a browser tool that shows a person whether the web page in front of them discusses a provocative topic, whether the material is presented in a heavily biased way, whether it represents an outlier (fringe) idea, and how its discussion of issues relates to the broader context and to information presented in "familiar" sources. This research applies and extends text analysis and comparison techniques to this problem. It uses statistical language modeling, topic modeling, machine learning, and link analysis techniques to represent Web pages and clusters of Web pages. It requires both off-line pre-processing to organize web-scale collections and on-line, query-time fine-tuning of the organization for presentation in a TPS browser add-on. &lt;br/&gt;&lt;br/&gt;This research will be the foundation of class projects as well as graduate student research, exposing a large number of students to issues of web-based search and critical evaluation of information. More importantly, however, this work has the potential to impact many people, helping them make more informed choices in response to what they read on the Web and elsewhere. The results of this project will be published in peer-reviewed venues and listed at the project Web site (http://ciir.cs.umass.edu/research/tps).  A freely available TPS browser add-on will be available at this Web site.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1217281</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Allan</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James M Allan</PI_FULL_NAME>
<EmailAddress>allan@cs.umass.edu</EmailAddress>
<PI_PHON>4135453240</PI_PHON>
<NSF_ID>000209290</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Amherst</CityName>
<StateCode>MA</StateCode>
<ZipCode>010039242</ZipCode>
<StreetAddress><![CDATA[OGCA, 70 Butterfield Terrace]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~499752</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Search engines have made it easy for people to find useful information; however, those search engines have also made it equally easy for people to stumble upon advice and comments that are of dubious or even negative value. This project&rsquo;s goal was to develop and foster technologies to assist people in critical evaluation of the material they find on the internet, and to help them understand why a page is educative or why it is not. It is driven by a vision of a &ldquo;Topical Positioning System&rdquo; (TPS) tool that shows a person whether the web page in front of them discusses a provocative topic, whether the material is presented in a heavily biased way, and whether it represents an outlier (fringe) idea, with the ultimate goal of visualizing how its discussion of issues relates to the broader context and to information presented in authoritative sources.&nbsp; The motivating goal is ultimately not to present the person with a filter or grade for each web page, but to help them recognize that there is a larger discussion and to critically evaluate how the page in question is positioned in that discussion.</p> <p>This project demonstrated that controversy detection was possible using Wikipedia as a source of "topics" and using automatic estimates of controversy derived from the edit history of those articles (Dori-Hacohen and Allan 2013 and 2015). It was extended to demonstrate that Twitter feeds could also be used to identify controversial topics. It demonstrated that intensities of controversy among related pages are not independent and proposed a stacked model which exploits the dependencies among related pages to improve classification of controversial web pages when compared to examining each page in isolation (Dori-Hacohen, Jensen, and Allan 2016).</p> <p>Following on from that effort, we undertook work to model controversy more directly and more formally, resulting in some new approaches (Jang, Foley, Dori-Hacohen, and Allan 2016), including a formal model that encompasses the Wikipedia and Twitter models. Based on insights from social science research, it also introduced a language modeling approach to this problem. We extensively evaluated different methods of creating controversy language models based on a diverse set of public datasets including Wikipedia, Web and News corpora.</p> <p>The primary result of the project has been a clear demonstration that it is possible to detect controversy in arbitrary web pages with reasonable accuracy using different sources to derive labeled training data. The work has laid the foundation for leveraging summarization technologies to explain the nature of controversy and how a page is positioned in the larger discourse.</p> <p>An additional outcome of this project has been the creation of a startup company to deploy this technology to recognize changes in controversial material in public media. The startup, founded by one of the PhD students (now graduated) whose work was funded by this project, was supported by an NSF I-Corps grant and is currently in the midst of its ongoing customer discovery process.</p><br> <p>            Last Modified: 12/28/2017<br>      Modified by: James&nbsp;M&nbsp;Allan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Search engines have made it easy for people to find useful information; however, those search engines have also made it equally easy for people to stumble upon advice and comments that are of dubious or even negative value. This project?s goal was to develop and foster technologies to assist people in critical evaluation of the material they find on the internet, and to help them understand why a page is educative or why it is not. It is driven by a vision of a "Topical Positioning System" (TPS) tool that shows a person whether the web page in front of them discusses a provocative topic, whether the material is presented in a heavily biased way, and whether it represents an outlier (fringe) idea, with the ultimate goal of visualizing how its discussion of issues relates to the broader context and to information presented in authoritative sources.  The motivating goal is ultimately not to present the person with a filter or grade for each web page, but to help them recognize that there is a larger discussion and to critically evaluate how the page in question is positioned in that discussion.  This project demonstrated that controversy detection was possible using Wikipedia as a source of "topics" and using automatic estimates of controversy derived from the edit history of those articles (Dori-Hacohen and Allan 2013 and 2015). It was extended to demonstrate that Twitter feeds could also be used to identify controversial topics. It demonstrated that intensities of controversy among related pages are not independent and proposed a stacked model which exploits the dependencies among related pages to improve classification of controversial web pages when compared to examining each page in isolation (Dori-Hacohen, Jensen, and Allan 2016).  Following on from that effort, we undertook work to model controversy more directly and more formally, resulting in some new approaches (Jang, Foley, Dori-Hacohen, and Allan 2016), including a formal model that encompasses the Wikipedia and Twitter models. Based on insights from social science research, it also introduced a language modeling approach to this problem. We extensively evaluated different methods of creating controversy language models based on a diverse set of public datasets including Wikipedia, Web and News corpora.  The primary result of the project has been a clear demonstration that it is possible to detect controversy in arbitrary web pages with reasonable accuracy using different sources to derive labeled training data. The work has laid the foundation for leveraging summarization technologies to explain the nature of controversy and how a page is positioned in the larger discourse.  An additional outcome of this project has been the creation of a startup company to deploy this technology to recognize changes in controversial material in public media. The startup, founded by one of the PhD students (now graduated) whose work was funded by this project, was supported by an NSF I-Corps grant and is currently in the midst of its ongoing customer discovery process.       Last Modified: 12/28/2017       Submitted by: James M Allan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
