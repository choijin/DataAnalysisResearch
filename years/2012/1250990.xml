<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>BIGDATA: Small: DCM: JetStream: A Flexible Distributed System for Online and In-Place Data Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>700000.00</AwardTotalIntnAmount>
<AwardAmount>700000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many research and commercial endeavors are experiencing dramatic transformations through the use of Big Data, wherein large data repositories are collected and analyzed to reveal trends, correlation, and information that may not be apparent in smaller samples. Current approaches assume centralizing the repository, which may be a poor fit in environments where the data generation rate exceeds the network capabilities. In this project, the PIs investigate system architectures for both real-time and historical analysis of geographically distributed data, combined with research in adaptively reducing data volumes to optimize bandwidth capabilities. This combination allows better use of the computation and storage associated with smarter end devices, including, but not limited to, distributed sensors, smart meters, and even full servers, without requiring network upgrades. Given the historical trends of the growth of computation and storage versus the capacity limits of wide-area networks, this research enables more data collection and analysis to be performed at a lower overall system cost.  Further, the ability to dynamically adapt data precision and fidelity to available network bandwidth allows systems to gracefully and automatically improve performance in the presence of higher-capacity networks. The research enables the collection and analysis of data that is currently left unanalyzed because of network constraints. Such data can include finer-granularity usage data, which could indicate actionable steps to reducing household energy consumption, or it could include a greater  olume of debugging and monitoring data, which could better predict system failures or provide greater insight than with current methods.</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1250990</AwardID>
<Investigator>
<FirstName>Vivek</FirstName>
<LastName>Pai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vivek Pai</PI_FULL_NAME>
<EmailAddress>vivek@cs.princeton.edu</EmailAddress>
<PI_PHON>6092582086</PI_PHON>
<NSF_ID>000122168</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Freedman</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael J Freedman</PI_FULL_NAME>
<EmailAddress>mfreed@cs.princeton.edu</EmailAddress>
<PI_PHON>6092589179</PI_PHON>
<NSF_ID>000500977</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[4 New South Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~700000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The number and variety of devices producing data streams has exploded in recent years, from software logs to handheld phones to aerial cameras. These sources generate data at rates, sizes, and complexities never before seen, including from locations that may be spread all over the world. This has led to significant complexity and difficulty in processing this data in a timely or cost-effective manner.</p> <p>To meet these needs, this multi-year research project focused on designing, building, and evaluating systems and algorithmic techniques that address key challenges in this new environment of exploding data volumes, including across the wide area.&nbsp; At its core, this project explored how data processing systems can employ approximation as a first-class systems principle to achieve time-sensitive computational results.</p> <p>Two major thrusts of research were performed, focusing distinctly on wide-area and datacenter data analytics.</p> <p>To address wide-area data analytics, we developed the Jetstream system, a system that allows real-time analysis of large, widely-distributed changing data sets. Traditional approaches to distributed analytics require users to specify in advance which data is to be backhauled to a central location for analysis. This is a poor match for domains where available bandwidth is scarce and it is infeasible to collect all potentially useful data.&nbsp; Jetstream addresses bandwidth limits in two ways, both of which are explicit in its programming model. The system incorporates structured storage in the form of OLAP data cubes, so data can be stored for analysis near where it is generated. Using cubes, queries can aggregate data in ways and locations of their choosing. The system also includes adaptive filtering and other transformations that adjusts data quality to match available bandwidth (and thus ensure timely streaming of data under bandwidth constraints).&nbsp; Through detailed evaluation of our prototype, we demonstrate that Jetstream's adaptive control mechanisms are responsive enough to keep end-to-end latency within a few seconds, even when available bandwidth fluctuates wildly.</p> <p>We further addressed the timely processing of wide-area data in the VideoStorm system, focusing specifically on scheduling and managing machine vision algorithms on real-time video streams. VideoStorm's goal was to develop a video analytics system that scales to processing thousands of live video streams over large clusters. Given video analytics queries that perform vision signal processing on incoming video frames, VideoStorm contains a scheduler that efficiently generates resource-quality profile for a query for its different possible configurations, and then jointly maximizes the quality and minimizes the lag of streaming video queries.&nbsp; This therefore allows the schedule to make tradeoffs between quality and latency (lag) between these video analytics applications based on their needs.&nbsp; Developed in collaboration with Microsoft Research, the VideoStorm system was deployed in live trials in both Bellevue, WA and Cambridge, UK.</p> <p>To address datacenter analytics, we explored system problems of adaptively controlling data accuracy, particularly in multi-tenant settings with many tenants seeking to use datacenter resources.&nbsp; Much like VideoStorm explored for video analytics how quality or latency are not fixed constraints but rather tradeoffs, our research explored applying similar concepts to machine learning and SQL analytics.</p> <p>In particular, SLAQ demonstrated a cluster-scheduling system for machine learning (ML) training jobs. We observe that in ML training, many iterations are typically computed in order to continually improve ML model quality. Yet in exploratory settings, better models can be obtained faster by directing resources to jobs with the most potential for improvement. Towards this end, when allocating cluster resources, SLAQ explores the quality-runtime trade-offs across multiple jobs to maximize system-wide quality improvement. To do so, SLAQ leverages the iterative nature of ML training algorithms, by collecting quality and resource usage information from concurrent jobs, and then generating highly-tailored quality-improvement predictions for future iterations. Experiments show that SLAQ achieves an average quality improvement of up to 73% and an average delay reduction of up to 44% on a large set of ML training jobs, compared to resource fairness schedulers.</p> <p>ReLAQS applied a similar approach to SQL approximate query processing, introducing a cluster scheduling system for online aggregation that reduces latency by assigning resources to queries with the most potential for improvement. ReLAQS utilizes the approximate results each query returns to periodically estimate how much progress each concurrent query is currently making. It then uses this information to predict how much progress each query is expected to make in the near future and redistributes resources in real-time to maximize the overall quality of the answers returned across the cluster. Experiments show that ReLAQS achieves a reduction in latency of up to 47% compared to traditional fair schedulers.</p> <p>Both SLAQ and ReLAQS won Best Paper awards at their peer-reviewed publication venues.</p><br> <p>            Last Modified: 12/10/2019<br>      Modified by: Michael&nbsp;J&nbsp;Freedman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The number and variety of devices producing data streams has exploded in recent years, from software logs to handheld phones to aerial cameras. These sources generate data at rates, sizes, and complexities never before seen, including from locations that may be spread all over the world. This has led to significant complexity and difficulty in processing this data in a timely or cost-effective manner.  To meet these needs, this multi-year research project focused on designing, building, and evaluating systems and algorithmic techniques that address key challenges in this new environment of exploding data volumes, including across the wide area.  At its core, this project explored how data processing systems can employ approximation as a first-class systems principle to achieve time-sensitive computational results.  Two major thrusts of research were performed, focusing distinctly on wide-area and datacenter data analytics.  To address wide-area data analytics, we developed the Jetstream system, a system that allows real-time analysis of large, widely-distributed changing data sets. Traditional approaches to distributed analytics require users to specify in advance which data is to be backhauled to a central location for analysis. This is a poor match for domains where available bandwidth is scarce and it is infeasible to collect all potentially useful data.  Jetstream addresses bandwidth limits in two ways, both of which are explicit in its programming model. The system incorporates structured storage in the form of OLAP data cubes, so data can be stored for analysis near where it is generated. Using cubes, queries can aggregate data in ways and locations of their choosing. The system also includes adaptive filtering and other transformations that adjusts data quality to match available bandwidth (and thus ensure timely streaming of data under bandwidth constraints).  Through detailed evaluation of our prototype, we demonstrate that Jetstream's adaptive control mechanisms are responsive enough to keep end-to-end latency within a few seconds, even when available bandwidth fluctuates wildly.  We further addressed the timely processing of wide-area data in the VideoStorm system, focusing specifically on scheduling and managing machine vision algorithms on real-time video streams. VideoStorm's goal was to develop a video analytics system that scales to processing thousands of live video streams over large clusters. Given video analytics queries that perform vision signal processing on incoming video frames, VideoStorm contains a scheduler that efficiently generates resource-quality profile for a query for its different possible configurations, and then jointly maximizes the quality and minimizes the lag of streaming video queries.  This therefore allows the schedule to make tradeoffs between quality and latency (lag) between these video analytics applications based on their needs.  Developed in collaboration with Microsoft Research, the VideoStorm system was deployed in live trials in both Bellevue, WA and Cambridge, UK.  To address datacenter analytics, we explored system problems of adaptively controlling data accuracy, particularly in multi-tenant settings with many tenants seeking to use datacenter resources.  Much like VideoStorm explored for video analytics how quality or latency are not fixed constraints but rather tradeoffs, our research explored applying similar concepts to machine learning and SQL analytics.  In particular, SLAQ demonstrated a cluster-scheduling system for machine learning (ML) training jobs. We observe that in ML training, many iterations are typically computed in order to continually improve ML model quality. Yet in exploratory settings, better models can be obtained faster by directing resources to jobs with the most potential for improvement. Towards this end, when allocating cluster resources, SLAQ explores the quality-runtime trade-offs across multiple jobs to maximize system-wide quality improvement. To do so, SLAQ leverages the iterative nature of ML training algorithms, by collecting quality and resource usage information from concurrent jobs, and then generating highly-tailored quality-improvement predictions for future iterations. Experiments show that SLAQ achieves an average quality improvement of up to 73% and an average delay reduction of up to 44% on a large set of ML training jobs, compared to resource fairness schedulers.  ReLAQS applied a similar approach to SQL approximate query processing, introducing a cluster scheduling system for online aggregation that reduces latency by assigning resources to queries with the most potential for improvement. ReLAQS utilizes the approximate results each query returns to periodically estimate how much progress each concurrent query is currently making. It then uses this information to predict how much progress each query is expected to make in the near future and redistributes resources in real-time to maximize the overall quality of the answers returned across the cluster. Experiments show that ReLAQS achieves a reduction in latency of up to 47% compared to traditional fair schedulers.  Both SLAQ and ReLAQS won Best Paper awards at their peer-reviewed publication venues.       Last Modified: 12/10/2019       Submitted by: Michael J Freedman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
