<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative Research: Using PDE Descriptions to Generate Code Precisely Tailored to Energy-Constrained Systems Including Large GPU Accelerated Clusters</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>24915.00</AwardTotalIntnAmount>
<AwardAmount>24915</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Modern computer system architectures are forcing computational scientists to move scientific applications&lt;br/&gt;from traditional homogeneous cpu-based systems to heterogeneous multi-core/accelerator architectures.   &lt;br/&gt;Obtaining performance in the presence of accelerators requires close attention to&lt;br/&gt;the memory hierarchy and chip-level parallelism to reach even a modest fraction&lt;br/&gt;of the potential performance. As a result, coding tasks which were once the province of&lt;br/&gt;lone graduate students in a single discipline now require interdisciplinary teams of people. &lt;br/&gt;Project Chemora will explore the design of a new application framework for automatically&lt;br/&gt;creating highly optimized code for high-end computational machines. The system&lt;br/&gt;will use as input a set of partial differential equations (PDEs) that describe a&lt;br/&gt;problem, it will then construct a machine-specific abstract performance model, and using these&lt;br/&gt;it will generate well-tuned code and execution configurations for accelerated&lt;br/&gt;(e.g., hybrid CPU/GPU) computing clusters at various scales.  Chemora will&lt;br/&gt;improve programmability in this simplified domain by decoupling the science and&lt;br/&gt;computer science at a high level, thereby reducing the complexity and number of issues scientists need to&lt;br/&gt;collectively understand and allowing individual scientists in the team to focus on their area of&lt;br/&gt;specialty. Chemora will improve performance (both wallclock time and energy) for&lt;br/&gt;systems with both simple and complex sets of equations by making use of detailed&lt;br/&gt;information describing the problem and machine, and will provide improved load&lt;br/&gt;balancing through the AMPI framework.&lt;br/&gt;&lt;br/&gt;The Chemora project  has chosen the Einstein equations as the primary science driver because&lt;br/&gt;these equations are one of the more complex PDE systems, one with many&lt;br/&gt;hundreds of terms, and a problem scale that is challenging to optimize for most&lt;br/&gt;compilers. Achieving this vision for a general scientific problem would indeed&lt;br/&gt;be a "Grand Challenge" in computational science, but in order to give our&lt;br/&gt;research a sharper focus we have chosen as a science driver the&lt;br/&gt;simulation of Intermediate mass ratio Binary Black Hole (IBBH) systems. Such&lt;br/&gt;systems, consisting of a black hole of mass 100 to 1,000 solar masses orbited by&lt;br/&gt;a smaller black hole of mass 5 to 20 solar masses are expected to be important&lt;br/&gt;sources of gravitational waves for advanced Laser Interferometer Gravitational&lt;br/&gt;Wave Observatory (LIGO) and the Einstein Telescope (ET). Accurate modeling of&lt;br/&gt;the waveforms from IBBH systems will be necessary in order to extract&lt;br/&gt;gravitational wave signals using template-matching data analysis techniques.</AbstractNarration>
<MinAmdLetterDate>08/15/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/15/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1265451</AwardID>
<Investigator>
<FirstName>Gengbin</FirstName>
<LastName>Zheng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gengbin Zheng</PI_FULL_NAME>
<EmailAddress>gzheng@illinois.edu</EmailAddress>
<PI_PHON>2173003023</PI_PHON>
<NSF_ID>000485464</NSF_ID>
<StartDate>08/15/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Urbana</CityName>
<StateCode>IL</StateCode>
<ZipCode>618012311</ZipCode>
<StreetAddress><![CDATA[1205 W Clark St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~24915</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>The MPI-based runtime on which Cactus-Carpet is built faces challenges at&nbsp;</span><br /><span>different scales, ranging from small clusters to large scale supercomputers.&nbsp;</span><br /><span>First, the new generation of parallel scientific applications are complex,&nbsp;</span><br /><span>involve simulation of dynamically varying systems, and use adaptive techniques,&nbsp;</span><br /><span>such as multiple timestepping and adaptive refinements. The conventional&nbsp;</span><br /><span>implementations of the MPI standard do not support the dynamic nature of these&nbsp;</span><br /><span>applications well. As a result, application performance suffers.</span><br /><span>In this project, we introduced an introspective runtime, Adaptive MPI, to&nbsp;</span><br /><span>tackle these challenges. Adaptive MPI, or AMPI, provides a multi-threaded&nbsp;</span><br /><span>implementation of MPI on top of Charm++, a parallel programming paradim with&nbsp;</span><br /><span>migratable objects. In order to apply AMPI to Cactus, we modified Cactus&nbsp;</span><br /><span>codebase to handle the global and static variables in order to run each MPI&nbsp;</span><br /><span>rank in a user-level thread in AMPI. We also enabled user-level thread migration&nbsp;</span><br /><span>with a System V context-based user-level thread implementation and an isomalloc</span><br /><span>scheme that reserves thread's address space across processors for migration.&nbsp;</span><br /><span>Based on the adaptive runtime enabled in the AMPI load balancing framework,&nbsp;</span><br /><span>we deployed a greedy-based load balancing algorithms for Cactus. With a&nbsp;</span><br /><span>Cactus benchmark TOV_PUGH, we demonstrated that using the load balancing&nbsp;</span><br /><span>feature, we achived about 11% of performance gain running on 8 cores.</span></p><br> <p>            Last Modified: 11/30/2015<br>      Modified by: Gengbin&nbsp;Zheng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The MPI-based runtime on which Cactus-Carpet is built faces challenges at  different scales, ranging from small clusters to large scale supercomputers.  First, the new generation of parallel scientific applications are complex,  involve simulation of dynamically varying systems, and use adaptive techniques,  such as multiple timestepping and adaptive refinements. The conventional  implementations of the MPI standard do not support the dynamic nature of these  applications well. As a result, application performance suffers. In this project, we introduced an introspective runtime, Adaptive MPI, to  tackle these challenges. Adaptive MPI, or AMPI, provides a multi-threaded  implementation of MPI on top of Charm++, a parallel programming paradim with  migratable objects. In order to apply AMPI to Cactus, we modified Cactus  codebase to handle the global and static variables in order to run each MPI  rank in a user-level thread in AMPI. We also enabled user-level thread migration  with a System V context-based user-level thread implementation and an isomalloc scheme that reserves thread's address space across processors for migration.  Based on the adaptive runtime enabled in the AMPI load balancing framework,  we deployed a greedy-based load balancing algorithms for Cactus. With a  Cactus benchmark TOV_PUGH, we demonstrated that using the load balancing  feature, we achived about 11% of performance gain running on 8 cores.       Last Modified: 11/30/2015       Submitted by: Gengbin Zheng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
