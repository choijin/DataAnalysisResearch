<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Collaborative Research: Programming Interface And Runtime For Self-Tuning Scalable C/C++ Data Structures</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/13/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A key challenge in developing multi-threaded applications on modern architectures is correctly synchronizing data shared among the threads while avoiding excessive performance penalties. Unsafe low-level synchronization mechanisms can easily introduce errors (e.g. race conditions and deadlock) that are extremely difficult to debug. At the same time, application performance and scalability are frequently compromised due to inefficient implementations of synchronous operations on shared data.    &lt;br/&gt;&lt;br/&gt;This research develops a library of highly concurrent scalable data containers with associated programming interface and optimization support to significantly enhance the productivity and performance of multi-threaded C/C++ applications on multicore architectures. The library provides an easy to use and composable interface similar to that of C++ Standard Template Library (STL) and enhances each container type with internal support for non-blocking synchronization of their data accesses, thereby providing better safety and performance than traditional blocking synchronization by eliminating hazards such as deadlock, livelock, and priority inversion, and by being highly scalable in supporting large numbers of threads. A higher level programming interface, similar to that of OpenMP, is supported by a preprocessing compiler associated with the runtime to ease the transition of existing sequential or multi-threaded C/C++ applications to using the non-blocking synchronous template library and to provide optimization and tuning support for the use of the library abstractions. The developed deliverables are expected to demonstrate a seamless integration of developer input, compiler optimization, and multicore runtimes to support systematic migration of C/C++ applications to continuously evolving architectures. &lt;br/&gt;&lt;br/&gt;The scalable template library and the associated programming interface and tuning support is expected to provide an immense productivity and performance boost for developers of high-end scientific and systems applications, including branch and bound, graph analysis, complex scene rendering, and goal propagation in autonomous embedded systems. The developed programming techniques and tools can enable the transformation of such applications into software that is substantially more reliable, efficient, and scalable than existing state of the art. The software techniques is also expected to be employed as an educational toolkit in the teaching of programming languages, compilers, systems software, and parallel programming courses.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>01/29/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1261584</AwardID>
<Investigator>
<FirstName>Qing</FirstName>
<LastName>Yi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qing Yi</PI_FULL_NAME>
<EmailAddress>qyi@uccs.edu</EmailAddress>
<PI_PHON>7192553066</PI_PHON>
<NSF_ID>000492904</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Colorado Springs</Name>
<CityName>Colorado Springs</CityName>
<ZipCode>809183733</ZipCode>
<PhoneNumber>7192553153</PhoneNumber>
<StreetAddress>1420, Austin Bluffs Parkway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>186192829</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Colorado Springs]]></Name>
<CityName>Colorado Springs</CityName>
<StateCode>CO</StateCode>
<ZipCode>809183733</ZipCode>
<StreetAddress><![CDATA[1420 Austin Bluffs Parkway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 2"> <div class="layoutArea"> <div class="column"> <p><span>This funded research addresses challenges in the design, optimization, and tool support of scalable concurrent data structures in C/C++ applications. Specifically it focuses on&nbsp;programming and compiler support to 1) &nbsp;automatically convert sequential data abstractions to efficient implementations of highly concurrent implementations using lock-free synchronization mechanisms, and &nbsp;2) optimization and tuning support for porting architecture-sensitive implementations of library abstractions and their use cases in large applications to different architectures. The prototype deliverables accordingly include 1)&nbsp;</span>a lock-free synchronization compiler, which automatically modifies sequential data structure implementations to support concurrent operations using non-blocking synchronizations, &nbsp;and 2) a source-to-source data abstraction replacement compiler, which support the automated exchange of data structure implementations and the optimization of their synchronizations to enhance application performance. &nbsp;The outcome also includes a preliminary adapter specification language to model the semantic properties of data abstractions in multi-threaded applications and thereby to enable these abstractions to be used interchangeably in large applications.</p> <p>The lock-free synchronization compiler has been tested by using it to automate synchronization of 8 commonly used data structures. Here our auto-generated non-blocking synchronizations can perform comparably or better than existing state-of-the-art manual implementations for all data structures tested. The compiler-generated code can perform highly efficiently because it automatically integrates a combination of state-of-the practice lock-free synchronization techniques based on the underlying code that need to be synchronized, through automated program analysis of the data abstraction implementation. The data abstraction replacement compiler has been tested by using it to support the exchange of data structure implementations in the PARSEC benchmarks. The compiler was able to automate the exchange of all uses of pre-defined queue, map, and synchronization abstractions in PARSEC (they are used in 10 of the 13 available benchmarks). However, speedups were observed on for 6 of the 10 benchmarks optimized. The main reason being low contentions among concurrent uses of the task queue abstractions in the original application design.</p> <p>The main findings of the research are two folds. First, there is a large potential in optimizing the implementation of entire data abstractions, each represented by a group of related data members and encapsulated operations on these data. The potential is reflected by the significant improvement attained by automatically synchronizing sequential data abstractions from their sequential implementations, beyond what is possible via previous state-of-the-art. Second, the implementations of different abstractions in a large application are often intertwined with one another, and changing the implementation of an individual abstraction in isolation is often not rewarding, as reflected by the lack of speedups attained by our compiler when exchanging individual abstraction implementations in isolation. It is important to collectively consider the hierarchy of abstractions and their parallelization dertails in a cohesive fashion, in order to improve the overall efficiency of a large application.</p> </div> </div> </div><br> <p>            Last Modified: 10/23/2018<br>      Modified by: Qing&nbsp;Yi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    This funded research addresses challenges in the design, optimization, and tool support of scalable concurrent data structures in C/C++ applications. Specifically it focuses on programming and compiler support to 1)  automatically convert sequential data abstractions to efficient implementations of highly concurrent implementations using lock-free synchronization mechanisms, and  2) optimization and tuning support for porting architecture-sensitive implementations of library abstractions and their use cases in large applications to different architectures. The prototype deliverables accordingly include 1) a lock-free synchronization compiler, which automatically modifies sequential data structure implementations to support concurrent operations using non-blocking synchronizations,  and 2) a source-to-source data abstraction replacement compiler, which support the automated exchange of data structure implementations and the optimization of their synchronizations to enhance application performance.  The outcome also includes a preliminary adapter specification language to model the semantic properties of data abstractions in multi-threaded applications and thereby to enable these abstractions to be used interchangeably in large applications.  The lock-free synchronization compiler has been tested by using it to automate synchronization of 8 commonly used data structures. Here our auto-generated non-blocking synchronizations can perform comparably or better than existing state-of-the-art manual implementations for all data structures tested. The compiler-generated code can perform highly efficiently because it automatically integrates a combination of state-of-the practice lock-free synchronization techniques based on the underlying code that need to be synchronized, through automated program analysis of the data abstraction implementation. The data abstraction replacement compiler has been tested by using it to support the exchange of data structure implementations in the PARSEC benchmarks. The compiler was able to automate the exchange of all uses of pre-defined queue, map, and synchronization abstractions in PARSEC (they are used in 10 of the 13 available benchmarks). However, speedups were observed on for 6 of the 10 benchmarks optimized. The main reason being low contentions among concurrent uses of the task queue abstractions in the original application design.  The main findings of the research are two folds. First, there is a large potential in optimizing the implementation of entire data abstractions, each represented by a group of related data members and encapsulated operations on these data. The potential is reflected by the significant improvement attained by automatically synchronizing sequential data abstractions from their sequential implementations, beyond what is possible via previous state-of-the-art. Second, the implementations of different abstractions in a large application are often intertwined with one another, and changing the implementation of an individual abstraction in isolation is often not rewarding, as reflected by the lack of speedups attained by our compiler when exchanging individual abstraction implementations in isolation. It is important to collectively consider the hierarchy of abstractions and their parallelization dertails in a cohesive fashion, in order to improve the overall efficiency of a large application.          Last Modified: 10/23/2018       Submitted by: Qing Yi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
