<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRCNS: Cortical representation of phonetic, syntactic and semantic information during speech perception and language comprehension</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>888774.00</AwardTotalIntnAmount>
<AwardAmount>888774</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The overarching goal of this project is to discover how language-related information is represented and processed in the human brain. To address this issue we propose to use a novel computational modeling approach, voxel-wise modeling. Voxel-wise modeling draws from the principles of nonlinear system identification, and it provides an efficient method for using complex data sets collected under naturalistic conditions to test multiple hypotheses about language representation. The specific research plan is divided into three aims, each targeted at a different form of language-related information. Aim 1 will reveal how low-level features of speech, such as spectral power, spectral modulation and phonemic structure, are represented across human cortex. Subjects will passively listen to human speech while hemodynamic brain activity is recorded by functional MRI. Voxel-wise modeling will then be used to determine how each point in the brain (i.e., each voxel, or volumetric pixel) is tuned for these various features. Using analogous methods, Aim 2 will reveal how syntactic and semantic features are represented across cortex. Finally, Aim 3 will reveal how language-related information is represented when it is delivered by auditory versus visual modalities. In this case speech and video stimuli will be used. Separate models will be estimated for data recorded during auditory and visual stimulation, and voxel-wise tuning will be compared across modalities. The voxel-wise computational models developed under this proposal will reveal how these various types of language-related information are represented across the cortical surface. These models will also provide clear predictions about how the brain will respond to novel speech stimuli. The results of the proposed research will have broad impacts on clinical problems related to speech perception and production, and they could form the basis of powerful brain decoding device that would enable neurological patients to communicate by thought alone.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1208203</AwardID>
<Investigator>
<FirstName>Frederic</FirstName>
<LastName>Theunissen</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Frederic E Theunissen</PI_FULL_NAME>
<EmailAddress>theunissen@berkeley.edu</EmailAddress>
<PI_PHON>5106431531</PI_PHON>
<NSF_ID>000463441</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Griffiths</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas L Griffiths</PI_FULL_NAME>
<EmailAddress>tomg@princeton.edu</EmailAddress>
<PI_PHON>6176422701</PI_PHON>
<NSF_ID>000488584</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Gallant</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack Gallant</PI_FULL_NAME>
<EmailAddress>gallant@berkeley.edu</EmailAddress>
<PI_PHON>5106433573</PI_PHON>
<NSF_ID>000440177</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Regents of the University of California]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947201650</ZipCode>
<StreetAddress><![CDATA[3115 Tolman Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7327</Code>
<Text>CRCNS</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~423718</FUND_OBLG>
<FUND_OBLG>2014~219201</FUND_OBLG>
<FUND_OBLG>2015~149330</FUND_OBLG>
<FUND_OBLG>2016~96525</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Human language production and comprehension is a complex cognitive process that is mediated by a constellation of specialized brain areas that represent different aspects of language. Language itself is a complex multi-scale signal that can be represented as a sound spectrogram, phonemes, morphemes, articulatory features, syntax, semantics and narrative. The human brain represents all these aspects of language, though the precise nature of these representations is poorly understood. The overarching goal of this proposal was to systematically map the representation of these various language-related features across the surface of the human cerebral cortex, and to relate the semantic maps in particular to visual semantic maps obtained previously by our lab (see Huth, A. G., Nishimoto, S., Vu, A. T., &amp; Gallant, J. L. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. <em>Neuron</em>, <em>76</em>, 1210-1224, 2012.). This project resulted in one landmark paper in Nature, one important paper in the Journal of Neuroscience, and a third paper soon to be submitted to a premier neuroscience Journal.</p> <p>&nbsp;</p> <p>The first paper to come out of this work (Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E., &amp; Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. <em>Nature</em>, <em>532</em>, 453-458, 2016.) describes how narrative lexical semantic information in natural stories is represented across the human cerebral cortex. It was known previously that the meaning of language is represented in cortical regions collectively known as the &ldquo;semantic system&rdquo;, but this system has not been mapped comprehensively. To address this we applied a voxel-wise modeling that we had developed originally for vision (see Huth, 2012 paper cited above) to functional MRI language comprehension data obtained while participants listened to natural narrative speech. We found that the semantic system is organized into intricate patterns that are highly consistent across individuals. We further developed a novel generative model to map these patterns quantitatively and to create a detailed semantic atlas of the human brain. This study clearly demonstrates that data-driven methods&mdash;commonplace in studies of human neuroanatomy and functional connectivity&mdash;provide a powerful and efficient means for mapping functional representations in the brain.</p> <p>&nbsp;</p> <p>The second paper to come out of this work (de Heer, W. A., Huth, A. G., Griffiths, T. L., Gallant, J. L., &amp; Theunissen, F. E. The hierarchical cortical organization of human speech processing. <em>Journal of Neuroscience</em>, <em>37</em>, 6539-6557, 2017.) describes how three important properties of language -- spectral information, articulatory information, and semantic information -- are represented across the surface of human cerebral cortex. The data come from voxel-wise modeling of functional MRI language comprehension data obtained while participants listened to natural narrative speech. Consistent with previous studies, we found that speech comprehension involves hierarchical representations starting in primary auditory areas and moving laterally on the temporal lobe. Our data also show that both hemispheres are equally and actively involved in speech perception and interpretation. Further, responses as early in the auditory hierarchy as in STS are more correlated with semantic than spectral representations. These results illustrate the importance of using natural speech in neurolinguistic research. Our methodology also provides an efficient way to simultaneously test multiple specific hypotheses about the representations of speech without using block designs and segmented or synthetic speech.</p> <p>&nbsp;</p> <p>The third paper, which is in the final stages of preparation, examines the relationship between the cortical representation of language semantics and that of visual semantics. Information about visual categories is represented across much of anterior occipital cortex (see Huth, 2012 paper cited above). Information about semantic categories in narrative speech is represented nearby, across much of the posterior temporal and parietal lobes (see Huth, 2016 paper cited above). These two types of representation meet at the boundary of visual cortex. In this study we investigated the spatial relationship between these types of representation by examining voxel-wise models that predict BOLD fMRI responses based on semantic properties of visual and linguistic stimuli. Direct comparison of visual and linguistic models on a voxel-by-voxel basis shows that some voxels near the boundary of visual cortex are selectively activated by both movies and stories, and that they have similar semantic selectivity in both modalities. However, closer examination of representations of specific categories reveals a more complex organization: for nearly any visual category that is represented on the posterior side of the border, an adjacent representation of the same linguistic category is found on the anterior side of the border. This suggests that the boundary of visual cortex constitutes both an anatomical and functional interface between unimodal representations of visual information and amodal representations of related information. This observation has important implications for the diagnosis and treatment of visual and language disorders.</p><br> <p>            Last Modified: 01/27/2018<br>      Modified by: Jack&nbsp;Gallant</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Human language production and comprehension is a complex cognitive process that is mediated by a constellation of specialized brain areas that represent different aspects of language. Language itself is a complex multi-scale signal that can be represented as a sound spectrogram, phonemes, morphemes, articulatory features, syntax, semantics and narrative. The human brain represents all these aspects of language, though the precise nature of these representations is poorly understood. The overarching goal of this proposal was to systematically map the representation of these various language-related features across the surface of the human cerebral cortex, and to relate the semantic maps in particular to visual semantic maps obtained previously by our lab (see Huth, A. G., Nishimoto, S., Vu, A. T., &amp; Gallant, J. L. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron, 76, 1210-1224, 2012.). This project resulted in one landmark paper in Nature, one important paper in the Journal of Neuroscience, and a third paper soon to be submitted to a premier neuroscience Journal.     The first paper to come out of this work (Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E., &amp; Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532, 453-458, 2016.) describes how narrative lexical semantic information in natural stories is represented across the human cerebral cortex. It was known previously that the meaning of language is represented in cortical regions collectively known as the "semantic system", but this system has not been mapped comprehensively. To address this we applied a voxel-wise modeling that we had developed originally for vision (see Huth, 2012 paper cited above) to functional MRI language comprehension data obtained while participants listened to natural narrative speech. We found that the semantic system is organized into intricate patterns that are highly consistent across individuals. We further developed a novel generative model to map these patterns quantitatively and to create a detailed semantic atlas of the human brain. This study clearly demonstrates that data-driven methods&mdash;commonplace in studies of human neuroanatomy and functional connectivity&mdash;provide a powerful and efficient means for mapping functional representations in the brain.     The second paper to come out of this work (de Heer, W. A., Huth, A. G., Griffiths, T. L., Gallant, J. L., &amp; Theunissen, F. E. The hierarchical cortical organization of human speech processing. Journal of Neuroscience, 37, 6539-6557, 2017.) describes how three important properties of language -- spectral information, articulatory information, and semantic information -- are represented across the surface of human cerebral cortex. The data come from voxel-wise modeling of functional MRI language comprehension data obtained while participants listened to natural narrative speech. Consistent with previous studies, we found that speech comprehension involves hierarchical representations starting in primary auditory areas and moving laterally on the temporal lobe. Our data also show that both hemispheres are equally and actively involved in speech perception and interpretation. Further, responses as early in the auditory hierarchy as in STS are more correlated with semantic than spectral representations. These results illustrate the importance of using natural speech in neurolinguistic research. Our methodology also provides an efficient way to simultaneously test multiple specific hypotheses about the representations of speech without using block designs and segmented or synthetic speech.     The third paper, which is in the final stages of preparation, examines the relationship between the cortical representation of language semantics and that of visual semantics. Information about visual categories is represented across much of anterior occipital cortex (see Huth, 2012 paper cited above). Information about semantic categories in narrative speech is represented nearby, across much of the posterior temporal and parietal lobes (see Huth, 2016 paper cited above). These two types of representation meet at the boundary of visual cortex. In this study we investigated the spatial relationship between these types of representation by examining voxel-wise models that predict BOLD fMRI responses based on semantic properties of visual and linguistic stimuli. Direct comparison of visual and linguistic models on a voxel-by-voxel basis shows that some voxels near the boundary of visual cortex are selectively activated by both movies and stories, and that they have similar semantic selectivity in both modalities. However, closer examination of representations of specific categories reveals a more complex organization: for nearly any visual category that is represented on the posterior side of the border, an adjacent representation of the same linguistic category is found on the anterior side of the border. This suggests that the boundary of visual cortex constitutes both an anatomical and functional interface between unimodal representations of visual information and amodal representations of related information. This observation has important implications for the diagnosis and treatment of visual and language disorders.       Last Modified: 01/27/2018       Submitted by: Jack Gallant]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
