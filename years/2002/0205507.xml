<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ITR:    Analysis of Complex Audio-Visual Events Using Spatially Distributed Sensors</AwardTitle>
<AwardEffectiveDate>10/01/2002</AwardEffectiveDate>
<AwardExpirationDate>09/30/2008</AwardExpirationDate>
<AwardTotalIntnAmount>1065659.00</AwardTotalIntnAmount>
<AwardAmount>1065659</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Stephen Griffin</SignBlockName>
</ProgramOfficer>
<AbstractNarration>We propose to develop a comprehensive framework for the joint analysis of audio-visual signals obtained&lt;br/&gt;from spatially distributed microphones and cameras. We desire solutions to the audio-visual sensing problem that will scale to an arbitrary number of cameras and microphones and can address challenging environments in which there are multiple speech and nonspeech sound sources and multiple moving people and objects.  Recently it has become relatively inexpensive to deploy tens or even hundreds of cameras and microphones in an environment. Many applications could benefit from ability to sense in both modalities.&lt;br/&gt;&lt;br/&gt;There are two levels at which joint audio-visual analysis can take place. At the signal level, the challenge&lt;br/&gt;is to develop representations that capture the rich dependency structure in the joint signal and deal success-fully issues such as variable sampling rates and varying temporal delays between cues. At the spatial level the challenge is to compensate for the distortions introduced by the sensor location and pool information across sensors to recover 3-D information about the spatial environment.&lt;br/&gt;&lt;br/&gt;For many applications, it is highly desirable if the solution method is self-calibrating, and does not&lt;br/&gt;require an extensive manual calibration process every time a new sensor is added or an old sensor is moved&lt;br/&gt;or replaced. Removing the burden of manual calibration also makes it possible to exploit ad hoc sensor&lt;br/&gt;networks which could arise, for example, from wearable microphones and cameras.&lt;br/&gt;&lt;br/&gt;We propose to address the following four research topics:&lt;br/&gt;&lt;br/&gt;1. Representations and learning methods for signal level fusion.&lt;br/&gt;2. Volumetric techniques for fusing spatially distributed audio-visual data.&lt;br/&gt;3. Self-calibration of distributed microphone-camera systems&lt;br/&gt;4. Applications of audio-visual sensing.&lt;br/&gt;&lt;br/&gt;For example, this proposal includes considerable work on lip and facial analysis to improve voice&lt;br/&gt;communications.</AbstractNarration>
<MinAmdLetterDate>09/24/2002</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2006</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0205507</AwardID>
<Investigator>
<FirstName>Irfan</FirstName>
<LastName>Essa</LastName>
<EmailAddress>irfan@cc.gatech.edu</EmailAddress>
<StartDate>09/24/2002</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Brandstein</LastName>
<EmailAddress>msb@hrl.harvard.edu</EmailAddress>
<StartDate>09/24/2002</StartDate>
<EndDate>08/13/2004</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Rehg</LastName>
<EmailAddress>rehg@cc.gatech.edu</EmailAddress>
<StartDate>09/24/2002</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>1687</Code>
<Text>ITR MEDIUM (GROUP) GRANTS</Text>
</ProgramElement>
<ProgramReference>
<Code>1655</Code>
<Text>INFORMATION MANAGEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0102</Code>
</Appropriation>
<Appropriation>
<Code>0103</Code>
</Appropriation>
<Appropriation>
<Code>0104</Code>
</Appropriation>
<Appropriation>
<Code>0105</Code>
</Appropriation>
<Appropriation>
<Code>0106</Code>
</Appropriation>
</Award>
</rootTag>
