<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ITR:    Bayesian Learning at the Syntax-Semantics Interface</AwardTitle>
<AwardEffectiveDate>09/01/2002</AwardEffectiveDate>
<AwardExpirationDate>08/31/2005</AwardExpirationDate>
<AwardAmount>406000</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana D. Korelsky</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Bayesian Learning at the Syntax-Semantics Interface&lt;br/&gt;Abstract&lt;br/&gt;&lt;br/&gt;Children easily learn features of novel verbs from small numbers of scene-utterance pairs. For example, after encountering a few examples of "breaking" an object, they infer that break might require an object, e.g., John broke the glass. They also learn semantic properties. Children and adults can then generalize to other scene instances representing break.   This project hypothesizes that children combine syntactic and semantic evidence to learn verb features, using a probabilistic method called Bayesian inference. &lt;br/&gt;&lt;br/&gt;The project's first goal is to implement a computational model that can induce probability distributions on features from a very small number of scene-utterance pairs. This model will make explicit all the information sources used. Second, the project will confirm which cues are actually used by human learners in certain settings. The experimental method matches the computer model's predictions empirically, by presenting adult and child learners with training sequences of novel verbs used across varying syntactic and semantic feature situations.  This project's results will advance adaptable computer systems and information-filtering,  both in terms of robustness to noise and an ability to learn from a small number of examples. These results will improve the construction of a key component of natural language processing engines: the dictionary.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/30/2002</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2004</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0218852</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Berwick</LastName>
<EmailAddress>berwick@ai.mit.edu</EmailAddress>
<StartDate>08/30/2002</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jesse</FirstName>
<LastName>Snedeker</LastName>
<EmailAddress>snedeker@wjh.harvard.edu</EmailAddress>
<StartDate>08/30/2002</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1686</Code>
<Text>ITR SMALL GRANTS</Text>
</ProgramElement>
<ProgramReference>
<Code>1654</Code>
<Text>HUMAN COMPUTER INTERFACE</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>RES EXPER FOR UNDERGRAD-SUPPLT</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
