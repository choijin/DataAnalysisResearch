<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Auditory Source Event Perception</AwardTitle>
<AwardEffectiveDate>09/01/2002</AwardEffectiveDate>
<AwardExpirationDate>08/31/2006</AwardExpirationDate>
<AwardTotalIntnAmount>210000.00</AwardTotalIntnAmount>
<AwardAmount>215550</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040500</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher T. Kello</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The child's cry that wakes you in night is not hard to identify.  The footfalls and hoof beats in a western movie supply crucial realism to the moviegoer.  Things that make noise are tightly woven into the fabric of human experience.  Nevertheless, we know very little about the actual perception of auditory events.  This is true despite our extensive knowledge of how ears work, for example, and our extensive knowledge about the basic auditory capabilities of ourselves and other animals.  The specialized area of speech perception has made some advances, but the perception of our environment involves sounds other than speech, and the study of human speech has provided few insights into how we perceive complex acoustic events.&lt;br/&gt;The impact of a shoe on a wooden floor will sound differently than the same shoe on a metal floor.  Acoustic properties of auditory events reflect physical and biomechanical properties of their sources.  To understand auditory event perception, the funded project will identify the physical and biomechanical properties of source events and then identify relations between these source properties and the acoustic properties of the resulting sound.  With this database in hand, Dr. Pastore and his colleagues will next conduct experiments to identify the sound attributes that listeners correctly (or incorrectly) associate with source properties.  Specifically, the funded project employs this source-sound-perception approach to investigate human gait as a class of auditory events.  Although the proposed research focuses on human gait as a source of sounds, the overarching goal is to develop research procedures to study perception of almost any auditory event.&lt;br/&gt;This basic research has broad implications.  For example, virtual-reality technology has fostered interests in the development of auditory virtual-reality capabilities.  The funded basic research will accomplish the crucial first step to make such applications possible.  This research could also contribute to the development of effective surveillance systems for research, commercial, and military use.  Moreover, it will identify how listening skills may be improved with training and which attributes of sounds are most important to aid the hearing impaired. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/14/2002</MinAmdLetterDate>
<MaxAmdLetterDate>05/29/2003</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0213666</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Pastore</LastName>
<EmailAddress>pastore@binghamton.edu</EmailAddress>
<StartDate>08/14/2002</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Binghamton</Name>
<CityName>BINGHAMTON</CityName>
<ZipCode>139026000</ZipCode>
<PhoneNumber>6077776136</PhoneNumber>
<StreetAddress>4400 VESTAL PKWY E</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1180</Code>
<Text>HUMAN COGNITION &amp; PERCEPTION</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1180</Code>
<Text>HUMAN COGNITION &amp; PERCEPTION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0102</Code>
</Appropriation>
<Appropriation>
<Code>0103</Code>
</Appropriation>
</Award>
</rootTag>
