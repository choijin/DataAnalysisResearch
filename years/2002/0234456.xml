<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DIVER:  Distributed Collaborative Analysis of Video Records in the Human Sciences</AwardTitle>
<AwardEffectiveDate>09/15/2002</AwardEffectiveDate>
<AwardExpirationDate>08/31/2003</AwardExpirationDate>
<AwardTotalIntnAmount>97133.00</AwardTotalIntnAmount>
<AwardAmount>97133</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gregg Solomon</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This award supports the  design and prototype development of a Web-based application for collaborative analysis of video records in the human sciences.  This  application, DIVER (Digital Interactive Video Exploration and Reflection), addresses a  critical need for .collaboratory. functions in the human sciences, where common tools  and a shared corpus of human interaction datasets can be used for investigating questions  of theory  and practice.   The proposed  effort will leverage an NSF-MRI  Award (NSF-0216334) for related work and capitalize on an NSF-funded Center for Innovative Learning  Technologies (CILT) workshop on Digital Video  Inquiry in late November 2002.   The  CILT  workshop  will  enable  the introduction of   the  tool  to  a  community  of  NSF  project  researchers, graduate students, and teacher-researchers, and to engage them in the further  design and development of DIVER. This feedback is crucial for an application that is  intended to help constitute and serve a community of users.  The  DIVER  application  will  consist  of  a  server  holding  analyses  of  video  data,  consisting of video sources and a series of annotated selections cropped in both time and space. Viewers of a DIVER analysis will be able to respond using threaded discussions attached  to  particular  scenes  from the  original  analysis,  and  they  can  return  to  the  base video clip to create their own selections as their responses warrant.   Several substantial technical issues, primary among them  the manipulation of multimedia records in a web browser environment, and the storage, indexing, and retrieval  of complex annotated  multimedia documents need to be resolved.  &lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/14/2002</MinAmdLetterDate>
<MaxAmdLetterDate>09/14/2002</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0234456</AwardID>
<Investigator>
<FirstName>Roy</FirstName>
<LastName>Pea</LastName>
<EmailAddress>roypea@stanford.edu</EmailAddress>
<StartDate>09/14/2002</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
</Institution>
<ProgramElement>
<Code>1666</Code>
<Text>RESEARCH ON LEARNING &amp; EDUCATI</Text>
</ProgramElement>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9237</Code>
<Text>SMALL GRANTS-EXPLORATORY RSRCH</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
