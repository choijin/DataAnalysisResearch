<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Integrated Sensing:  Active Stereoscopic Visual Search Driven by Natural Scene Statistics</AwardTitle>
<AwardEffectiveDate>09/01/2002</AwardEffectiveDate>
<AwardExpirationDate>08/31/2005</AwardExpirationDate>
<AwardTotalIntnAmount>249999.00</AwardTotalIntnAmount>
<AwardAmount>249999</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Vittal S. Rao</SignBlockName>
</ProgramOfficer>
<AbstractNarration>ABSTRACT&lt;br/&gt;&lt;br/&gt;"ACTIVE STEREOSCOPIC VISUAL SEARCH DRIVEN BY NATURAL SCENE STATISTICS"&lt;br/&gt;&lt;br/&gt;Alan C. Bovik, Lawrence K. Cormack, J. Ghosh&lt;br/&gt; &lt;br/&gt;The primary thrust of this proposal is to develop methods based on the natural statistics of stereoscopic images that will enable the  design and implementation of  the next generation of foveated, fixating machine vision systems that are capable of efficient and intelligent visual search, by exploiting and applying knowledge about human fixation and search mechanisms.&lt;br/&gt; We summarize the intention of our proposal via the following key goals:&lt;br/&gt;Goal 1: To develop a quantitative description of human active stereo vision as a function of natural scene statistics in a variety of three-dimensional visual search and learning tasks. Our emphasis will be on developing statistical models of stereo primitives that attract low-level visual attention based on a unique and in-depth statistical analysis. We feel that statistical models based on natural scene statistics have a very good chance of succeeding where deterministic models have failed.&lt;br/&gt;Goal 2: To train a state-of-the-art foveated, fixating active computer vision system (named FOVEA) to search and to learn to search 4-D (space-time) scenes.  To do this, back-end artificial neural networks trained on telepresent human search patterns will be used. The statistical models and extracted statistical stereoprimitives discovered as part of the research in Goal 1 will be used as a priori knowledge to improve the configuration and learning of the networks. We envision that these experiments will result in smart active machine vision protocols for exploring, searching, and interacting with 4-D environments, while giving new insights into visual cognitive processes.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/14/2002</MinAmdLetterDate>
<MaxAmdLetterDate>07/23/2004</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0225451</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Bovik</LastName>
<EmailAddress>bovik@ece.utexas.edu</EmailAddress>
<StartDate>08/14/2002</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joydeep</FirstName>
<LastName>Ghosh</LastName>
<EmailAddress>jghosh@utexas.edu</EmailAddress>
<StartDate>08/14/2002</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lawrence</FirstName>
<LastName>Cormack</LastName>
<EmailAddress>Cormack@mail.utexas.edu</EmailAddress>
<StartDate>08/14/2002</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
</Institution>
<FoaInformation>
<Code>0206000</Code>
<Name>Telecommunications</Name>
</FoaInformation>
<ProgramElement>
<Code>1518</Code>
<Text>CONTROL, NETWORKS, &amp; COMP INTE</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
