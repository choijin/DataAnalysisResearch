<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Large: Collaborative Research: HCPN: Hybrid Circuit/Packet Networking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>12/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>1800002.00</AwardTotalIntnAmount>
<AwardAmount>1816002</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Brassil</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Ever-larger data centers are powering the cloud computing revolution, but the scale of these installations is currently limited by the ability to provide sufficient internal network connectivity. Delivering scalable packet-switched interconnects that can support the continually increasing data rates required between literally hundreds of thousands of servers is an extremely challenging problem that is only getting harder. This project leverages microsecond optical circuit-switch technology to develop a hybrid switching paradigm that spans the gap between traditional circuit switching and full-fledged packet switching, achieving a level of performance and scale not previously attainable. This will result in a hybrid switch whose optical switching capacity is orders of magnitude larger than the electrical packet switch, yet whose performance from an end-to-end perspective is largely indistinguishable from a giant (electrical) packet switch.&lt;br/&gt;&lt;br/&gt;The research provides a quantitative baseline for hybrid network design across a wide range of present and future technologies. The project will consist of five parts: i) traffic characterization to identify the class of network traffic that a circuit switch can support as well as the partitioning of the traffic between the circuit and packet portions of the network; ii) circuit scheduling to enable the circuit switch to rapidly multiplex a set of circuits across a large set of data center traffic flows; iii) traffic conditioning to reduce the variability of traffic at the end hosts, easing the demands placed on switch scheduling; iv) a prototype hybrid network that can use an optical circuit switch that operates three orders of magnitude faster than existing solutions; and v) a trend analysis to understand the tradeoffs resulting from potential future technology advances.&lt;br/&gt;&lt;br/&gt;The work stands to dramatically improve data center networks, significantly reducing operating costs and increasing energy efficiency. The research material will be incorporated into courses, helping to train the next generation of computer networking scientists and engineers. The PIs will also continue ongoing outreach to high school students, both through the UCSD COSMOS summer program and through talks delivered at local high schools.</AbstractNarration>
<MinAmdLetterDate>08/08/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/12/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1314921</AwardID>
<Investigator>
<FirstName>George</FirstName>
<LastName>Papen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George Papen</PI_FULL_NAME>
<EmailAddress>gpapen@ucsd.edu</EmailAddress>
<PI_PHON>8588221728</PI_PHON>
<NSF_ID>000337263</NSF_ID>
<StartDate>08/08/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Ford</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph E Ford</PI_FULL_NAME>
<EmailAddress>jeford@ucsd.edu</EmailAddress>
<PI_PHON>8585347891</PI_PHON>
<NSF_ID>000455556</NSF_ID>
<StartDate>08/08/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alex</FirstName>
<LastName>Snoeren</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alex C Snoeren</PI_FULL_NAME>
<EmailAddress>snoeren@cs.ucsd.edu</EmailAddress>
<PI_PHON>8588222289</PI_PHON>
<NSF_ID>000482412</NSF_ID>
<StartDate>08/08/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>George</FirstName>
<LastName>Porter</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George Porter</PI_FULL_NAME>
<EmailAddress>gmporter@cs.ucsd.edu</EmailAddress>
<PI_PHON>8588226818</PI_PHON>
<NSF_ID>000553493</NSF_ID>
<StartDate>08/08/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7363</Code>
<Text>RES IN NETWORKING TECH &amp; SYS</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~599999</FUND_OBLG>
<FUND_OBLG>2014~616001</FUND_OBLG>
<FUND_OBLG>2015~600002</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Today's large-scale "cloud" services are incredibly power hungry, using energy sufficient to power every household in New York City twice over, and that percentage is expected to double in the next 10 years[1]. &nbsp;According to the NRDC, "In 2013, U.S. data centers consumed an estimated 91 billion kilowatt-hours of electricity, equivalent to the annual output of 34 large (500-megawatt) coal-fired power plants. Data center electricity consumption is projected to increase to roughly 140 billion kilowatt-hours annually by 2020, the equivalent annual output of 50 power plants, costing American businesses $13 billion annually in electricity bills and emitting nearly 100 million metric tons of carbon pollution per year.[2]" &nbsp;The enormous energy demands of these datacenters limits their growth potential and results in unnecessary operational expense. &nbsp;The good news is that there are many improvements that can be made in datacenters to raise their efficiency. &nbsp;In some cases, commercial providers run their systems at only 10 or 20% utilization due in large part to the inability for the underlying network to scale to meet bandwidth requirements.</p> <p>Fundamentally, the packet-switching technology underlying current data-center interconnects limits their ability to scale: implementing control- and data-planes necessary to forward packets individually is costly at present, and will rapidly cease to be feasible as link data rates continue to increase.</p> <p>The overall goal of this project has been to leverage microsecond optical circuit-switch technology to develop a hybrid switching paradigm that spans the gap between traditional circuit switching and full-fledged packet switching, achieving a level of performance and scale not previously attainable.&nbsp; We have designed and built a hybrid switch whose optical switching capacity is orders of magnitude larger than the electrical packet switch, yet whose performance from an end-to-end perspective is largely indistinguishable from a giant (electrical) packet switch. &nbsp;A key aspect of this project has been demonstrating a system-level control plane for hybrid networks capable of leveraging both circuit- and packet-switching.</p> <p>Our work has resulted in a characterization of commercial datacenter workloads, in particular multiple clusters at Facebook.&nbsp; Our findings have been used by our project as well as other projects to design an interconnect supporting commercial workloads.&nbsp; We have explored circuit switching in the context of a new, novel, non-crossbar &ldquo;Selector Switch&rdquo; architecture.&nbsp; A Selector Switch switches entire groups of input ports between entirely disjoint network matchings, rather than switching traffic from one input port to a single output port.&nbsp; We have studied the fundamental scaling limitations of MEMS-based optical switches, and found that they scale as a function of resolvable states, rather than ports.&nbsp; This means that by selecting among matchings, rather than individual input/output port matchings, we can build a Selector Switch that scales to 1000s of ports with existing technology, without requiring expensive optical amplification or telecom-grade transceivers.&nbsp; We have designed and build a 61-port Selector Switch prototype.</p> <p>Paired with a Selector Switch-based architecture, we have demonstrated how to build a Top-of-Rack switch that supports hybrid packet/circuit networks.&nbsp; The resulting prototype, REACToR, relies on a trio of mechanisms to support hybrid networks.&nbsp; First, it explicitly &ldquo;pulls&rdquo; packets from attached servers based on foreknowledge of which circuits will be established at a given time.&nbsp; This eliminates the need to maintain large packet buffers in the ToR.&nbsp; Second, REACToR classifies packets based on a centralized circuit schedule, directing low-latency packets to a separate packet-switched network to reduce overall latency.&nbsp; Finally, REACToR plays a key role in ensuring datacenter-wide synchronization.&nbsp; We have designed and build a 16-port FPGA-based REACToR prototype.</p> <p>We have designed two centralized circuit-switch scheduling algorithms, Solstice and Albedo.&nbsp; Solstice operates by processing an input demand matrix, representing a snapshot of the overall datacenter workload demand.&nbsp; The Solstice algorithm is a computationally tractable heuristic for maximizing the bandwidth through the network, operating closely to non-tractable optimal solutions.&nbsp; We next designed Albedo, which extends the assumptions of Solstice by assuming that traffic could be indirected, or sent through intermediate nodes.&nbsp; By allowing some traffic to travel over what would otherwise be unused paths, Albedo is able to increase the total amount of traffic sent through the network as compared to Solstice.&nbsp; We have implemented both Solstice and Albedo in software.</p> <p>The net result of this project is a new strategy for supporting high bandwidth networks in the future, sidestepping the impending limitations of all-packet-switched networks.&nbsp; By switching some datacenter traffic optically, future datacenters will be able to support higher bandwidths, which will result in faster computations, fewer servers waiting for the data they need (and thus less server energy usage), and higher overall efficiency.&nbsp; The result of higher datacenter efficiency is a dramatic reduction in overall US energy usage (since datacenters account for 2.5% of US energy usage), and the removal of a barrier that companies encounter today, namely growing their networks to meet their customer demands.</p> <p>[1]&nbsp;https://energy.gov/eere/buildings/data-centers-and-servers</p> <p>[2]&nbsp;https://www.nrdc.org/resources/americas-data-centers-consuming-and-wasting-growing-amounts-energy</p><br> <p>            Last Modified: 04/23/2017<br>      Modified by: George&nbsp;Porter</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1314921/1314921_10265162_1492990127375_SelectorSwitch--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1314921/1314921_10265162_1492990127375_SelectorSwitch--rgov-800width.jpg" title="Selector Switch Prototype"><img src="/por/images/Reports/POR/2017/1314921/1314921_10265162_1492990127375_SelectorSwitch--rgov-66x44.jpg" alt="Selector Switch Prototype"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A Selector Switch switches groups of input ports to one of a small set of matchings, thereby connecting that input set to a set of output ports.  The Selector Switch design is fundamentally more scalable than a crossbar switch.</div> <div class="imageCredit">George Porter</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">George&nbsp;Porter</div> <div class="imageTitle">Selector Switch Prototype</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1314921/1314921_10265162_1492989716246_reactor--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1314921/1314921_10265162_1492989716246_reactor--rgov-800width.jpg" title="REACToR Switch"><img src="/por/images/Reports/POR/2017/1314921/1314921_10265162_1492989716246_reactor--rgov-66x44.jpg" alt="REACToR Switch"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The REACToR switch connects servers to a hybrid packet circuit network</div> <div class="imageCredit">George Papen</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">George&nbsp;Porter</div> <div class="imageTitle">REACToR Switch</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today's large-scale "cloud" services are incredibly power hungry, using energy sufficient to power every household in New York City twice over, and that percentage is expected to double in the next 10 years[1].  According to the NRDC, "In 2013, U.S. data centers consumed an estimated 91 billion kilowatt-hours of electricity, equivalent to the annual output of 34 large (500-megawatt) coal-fired power plants. Data center electricity consumption is projected to increase to roughly 140 billion kilowatt-hours annually by 2020, the equivalent annual output of 50 power plants, costing American businesses $13 billion annually in electricity bills and emitting nearly 100 million metric tons of carbon pollution per year.[2]"  The enormous energy demands of these datacenters limits their growth potential and results in unnecessary operational expense.  The good news is that there are many improvements that can be made in datacenters to raise their efficiency.  In some cases, commercial providers run their systems at only 10 or 20% utilization due in large part to the inability for the underlying network to scale to meet bandwidth requirements.  Fundamentally, the packet-switching technology underlying current data-center interconnects limits their ability to scale: implementing control- and data-planes necessary to forward packets individually is costly at present, and will rapidly cease to be feasible as link data rates continue to increase.  The overall goal of this project has been to leverage microsecond optical circuit-switch technology to develop a hybrid switching paradigm that spans the gap between traditional circuit switching and full-fledged packet switching, achieving a level of performance and scale not previously attainable.  We have designed and built a hybrid switch whose optical switching capacity is orders of magnitude larger than the electrical packet switch, yet whose performance from an end-to-end perspective is largely indistinguishable from a giant (electrical) packet switch.  A key aspect of this project has been demonstrating a system-level control plane for hybrid networks capable of leveraging both circuit- and packet-switching.  Our work has resulted in a characterization of commercial datacenter workloads, in particular multiple clusters at Facebook.  Our findings have been used by our project as well as other projects to design an interconnect supporting commercial workloads.  We have explored circuit switching in the context of a new, novel, non-crossbar "Selector Switch" architecture.  A Selector Switch switches entire groups of input ports between entirely disjoint network matchings, rather than switching traffic from one input port to a single output port.  We have studied the fundamental scaling limitations of MEMS-based optical switches, and found that they scale as a function of resolvable states, rather than ports.  This means that by selecting among matchings, rather than individual input/output port matchings, we can build a Selector Switch that scales to 1000s of ports with existing technology, without requiring expensive optical amplification or telecom-grade transceivers.  We have designed and build a 61-port Selector Switch prototype.  Paired with a Selector Switch-based architecture, we have demonstrated how to build a Top-of-Rack switch that supports hybrid packet/circuit networks.  The resulting prototype, REACToR, relies on a trio of mechanisms to support hybrid networks.  First, it explicitly "pulls" packets from attached servers based on foreknowledge of which circuits will be established at a given time.  This eliminates the need to maintain large packet buffers in the ToR.  Second, REACToR classifies packets based on a centralized circuit schedule, directing low-latency packets to a separate packet-switched network to reduce overall latency.  Finally, REACToR plays a key role in ensuring datacenter-wide synchronization.  We have designed and build a 16-port FPGA-based REACToR prototype.  We have designed two centralized circuit-switch scheduling algorithms, Solstice and Albedo.  Solstice operates by processing an input demand matrix, representing a snapshot of the overall datacenter workload demand.  The Solstice algorithm is a computationally tractable heuristic for maximizing the bandwidth through the network, operating closely to non-tractable optimal solutions.  We next designed Albedo, which extends the assumptions of Solstice by assuming that traffic could be indirected, or sent through intermediate nodes.  By allowing some traffic to travel over what would otherwise be unused paths, Albedo is able to increase the total amount of traffic sent through the network as compared to Solstice.  We have implemented both Solstice and Albedo in software.  The net result of this project is a new strategy for supporting high bandwidth networks in the future, sidestepping the impending limitations of all-packet-switched networks.  By switching some datacenter traffic optically, future datacenters will be able to support higher bandwidths, which will result in faster computations, fewer servers waiting for the data they need (and thus less server energy usage), and higher overall efficiency.  The result of higher datacenter efficiency is a dramatic reduction in overall US energy usage (since datacenters account for 2.5% of US energy usage), and the removal of a barrier that companies encounter today, namely growing their networks to meet their customer demands.  [1] https://energy.gov/eere/buildings/data-centers-and-servers  [2] https://www.nrdc.org/resources/americas-data-centers-consuming-and-wasting-growing-amounts-energy       Last Modified: 04/23/2017       Submitted by: George Porter]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
