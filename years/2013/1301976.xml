<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Medium: Algorithmic Crowdsourcing Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>999977.00</AwardTotalIntnAmount>
<AwardAmount>999977</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Crowdsourcing refers to the paradigm of eliciting work, typically in small pieces, from a global population of workers, each making dynamic decisions about which task to complete next, and for whom. Crowdsourcing systems are inherently algorithmic because the coordination task would otherwise be overwhelming. Algorithms take the place of the management and incentive structure of traditional firms, responsible for optimizing workflows, determining payments, matching workers with tasks, and learning models of workers and tasks.&lt;br/&gt;&lt;br/&gt;With the potential to transform the way in which productive effort is allocated, crowdsourcing is already finding application across a broad range of domains, such as citizen science, the transcription of documents, and collecting training data for use in machine learning for bridging the gap to human-level intelligence.  The goal of this project is to develop a cohesive theory of algorithms and incentives for the design of crowdsourcing systems.&lt;br/&gt;&lt;br/&gt;The proposed research addresses key challenges in harnessing decentralized resources and capabilities for productive work, and seeks to develop a theoretical framework to guide the design of crowdsourcing systems that align incentives, are adaptive and robust, and achieve high performance for low cost.  The investigators are interested in the roles of matching and pricing, in developing methods for eliciting high quality, unverifiable contributions, and in embracing realistic models of human behavior.&lt;br/&gt;&lt;br/&gt;The investigators will  organize workshops on social computing at upcoming Conferences on Electronic Commerce (EC13 and EC14) and they continue to recruit women and other under-represented groups to join the project.</AbstractNarration>
<MinAmdLetterDate>08/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1301976</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Parkes</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David C Parkes</PI_FULL_NAME>
<EmailAddress>parkes@eecs.harvard.edu</EmailAddress>
<PI_PHON>6173848130</PI_PHON>
<NSF_ID>000210273</NSF_ID>
<StartDate>08/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yiling</FirstName>
<LastName>Chen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yiling Chen</PI_FULL_NAME>
<EmailAddress>yiling@seas.harvard.edu</EmailAddress>
<PI_PHON>6174953298</PI_PHON>
<NSF_ID>000508042</NSF_ID>
<StartDate>08/09/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yaron</FirstName>
<LastName>Singer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yaron Singer</PI_FULL_NAME>
<EmailAddress>yaron@seas.harvard.edu</EmailAddress>
<PI_PHON>6174961447</PI_PHON>
<NSF_ID>000629528</NSF_ID>
<StartDate>08/09/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>021383846</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7932</Code>
<Text>COMPUT GAME THEORY &amp; ECON</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7932</Code>
<Text>COMPUT GAME THEORY &amp; ECON</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~784213</FUND_OBLG>
<FUND_OBLG>2015~215764</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="section"> <div class="section"> <div class="layoutArea"> <div class="column"> <div class="page" title="Page 14"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>Crowdsourcing and human computation continue to attach great attention from industry, both from end users such as large web firms like Google, Microsoft and Amazon, but also from intermediary firms such as Upwork and Figure Eight.&nbsp;Crowdsourcing is a vital component to enable&nbsp; progress on AI research as a source of data and helping to democratize the development of algorithms so that they fairly represent different groups. Progress towards the research goals outlined in this research can have a positive effect on technologies within these industries, better aligning incentives with what is important for workers while leading to systems with improved optimality properties.&nbsp;</span></p> </div> </div> </div> </div> <p>In this research, we have advanced the theory, algorithms, and empirical understanding of algorithmic crowdsourcing.</p> <p>In regard to pricing and allocation, we have developed algorithms with which to elicit and aggregate information while respecting budget constraints. For example, we have methods to learn about costs, and correlations between costs and the quality and value of information, during the process of crowdsourcing--- reasoning and the quality of workers, and the effort they have put into tasks, and providing good approaches to exploration.&nbsp;</p> <p>In regard to peer scoring, which addresses the challenge of how to reward high quality inputs when contributions are unverifiable, we have made substantial advances in the scope and robustness of the methods. Through our work we can score worker contributions through the appropriate use of correlations between responses and can also handle for the first time participants with different tastes (so that they interpret the world in different ways). The schemes we develop are robust to collusive behavior, and can be used, for example, to elicit labeled data for training the next generation of AI technology or for adding user-generated content to platforms that provide information about the communities in which we live and work. We have shown, through this work, how to learn latent taste and quality of participants while at the same time carefully aligning incentives. Further, we show how to learn the relationship between feature vectors and labels, opening up additional opportunites and connections with machine learning algorithms.</p> </div> </div> </div> </div> </div> <div class="page" title="Page 2"> <div class="section"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>In regard to the design of crowdsourcing systems</span>, we have conducted extensive empirical work to understand the role of bonuses and other incentives to improve worker satisfaction and system performance, including in regard to context switching. We have developed new methods for regression and active learning, allowing for uncertainty about the quality of different participants, and&nbsp; considering that future assignments can be made contingent on past behavior. We have also made fundamental advances in regard to understanding how to optimze from noisy data. We introduce and study the problem of optimization from samples, looking to understand when efficient learnability together with efficient approximability of optimization implies that optimization from samples is possible. Along with positive results,&nbsp;we also attain a series of surprising observations about what is not possible,&nbsp;including for coverage problems and natural optimization frameworks.&nbsp;<br /><br />The broader impacts of the work have come through extensive training and professional development, with nine Ph.D. students and postdoctoral fellows continuing into faculty positions in computer science departments and business schools. We have also mentored numerous undergraduate students in research, many of whom have continued to graduate school. The results have been widely disseminated to communities of interest, including through the organization of workshops and events (including interdisciplinary conferences), editing special tracks of journals, and preprints of our papers are all freely available online. Industrial partners have also taken interest in the work, looking to test new methods for scoring and rewarding user-generated content, for example to map platforms.</p> </div> </div> </div> </div> </div><br> <p>            Last Modified: 01/27/2019<br>      Modified by: David&nbsp;C&nbsp;Parkes</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[          Crowdsourcing and human computation continue to attach great attention from industry, both from end users such as large web firms like Google, Microsoft and Amazon, but also from intermediary firms such as Upwork and Figure Eight. Crowdsourcing is a vital component to enable  progress on AI research as a source of data and helping to democratize the development of algorithms so that they fairly represent different groups. Progress towards the research goals outlined in this research can have a positive effect on technologies within these industries, better aligning incentives with what is important for workers while leading to systems with improved optimality properties.       In this research, we have advanced the theory, algorithms, and empirical understanding of algorithmic crowdsourcing.  In regard to pricing and allocation, we have developed algorithms with which to elicit and aggregate information while respecting budget constraints. For example, we have methods to learn about costs, and correlations between costs and the quality and value of information, during the process of crowdsourcing--- reasoning and the quality of workers, and the effort they have put into tasks, and providing good approaches to exploration.   In regard to peer scoring, which addresses the challenge of how to reward high quality inputs when contributions are unverifiable, we have made substantial advances in the scope and robustness of the methods. Through our work we can score worker contributions through the appropriate use of correlations between responses and can also handle for the first time participants with different tastes (so that they interpret the world in different ways). The schemes we develop are robust to collusive behavior, and can be used, for example, to elicit labeled data for training the next generation of AI technology or for adding user-generated content to platforms that provide information about the communities in which we live and work. We have shown, through this work, how to learn latent taste and quality of participants while at the same time carefully aligning incentives. Further, we show how to learn the relationship between feature vectors and labels, opening up additional opportunites and connections with machine learning algorithms.            In regard to the design of crowdsourcing systems, we have conducted extensive empirical work to understand the role of bonuses and other incentives to improve worker satisfaction and system performance, including in regard to context switching. We have developed new methods for regression and active learning, allowing for uncertainty about the quality of different participants, and  considering that future assignments can be made contingent on past behavior. We have also made fundamental advances in regard to understanding how to optimze from noisy data. We introduce and study the problem of optimization from samples, looking to understand when efficient learnability together with efficient approximability of optimization implies that optimization from samples is possible. Along with positive results, we also attain a series of surprising observations about what is not possible, including for coverage problems and natural optimization frameworks.   The broader impacts of the work have come through extensive training and professional development, with nine Ph.D. students and postdoctoral fellows continuing into faculty positions in computer science departments and business schools. We have also mentored numerous undergraduate students in research, many of whom have continued to graduate school. The results have been widely disseminated to communities of interest, including through the organization of workshops and events (including interdisciplinary conferences), editing special tracks of journals, and preprints of our papers are all freely available online. Industrial partners have also taken interest in the work, looking to test new methods for scoring and rewarding user-generated content, for example to map platforms.            Last Modified: 01/27/2019       Submitted by: David C Parkes]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
