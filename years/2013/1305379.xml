<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>II-NEW: BOLD: Big Data and Optical Lightpaths-Driven Networked Systems Research Infrastructure</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>900000.00</AwardTotalIntnAmount>
<AwardAmount>900000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ann Von Lehmen</SignBlockName>
<PO_EMAI>avonlehm@nsf.gov</PO_EMAI>
<PO_PHON>7032924756</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will deploy an optically networked systems research infrastructure named BOLD that integrates high-performance, low-power, optical networking devices and programmable packet switches to enable transformative and inter-disciplinary research.  It enables experimental exploration of different architectural design choices and it will potentially suggest new approaches to integrated hardware-software designs.  At the hardware level, it enables research on novel optical network devices that can provide powerful communication capabilities, possibly tailored to the specific needs of big data applications, as well as experimentation with the device prototypes under real application traffic. BOLD enables a broad range of transformative big data-driven research. At the system software level, it enables research on storage, network, and application control software that are designed from the ground up to coordinate for optimal performance. At the application level, BOLD motivates research on how fundamental algorithms for big data applications should be designed to leverage the new network capabilities. BOLD has the unique potential to bridge the gap between nano-photonics researchers, networked systems researchers, and big data application researchers, creating inter-disciplinary research opportunities. Furthermore, because BOLD is designed to operate alongside Rice?s existing NSF-funded computing infrastructures BOLD can support big data experiments at a substantial scale.&lt;br/&gt;&lt;br/&gt;Results from the inter-disciplinary research enabled by BOLD will lead to future big data processing system architectures that dramatically speed up a wide range of computational scientific discoveries. Because optical networking devices are unique in that they consume very little power, yet can support enormous data rates, BOLD will inspire a new class of high performance, high energy efficiency system architectures. Research enabled by BOLD could inform the design of future nation-wide networking infrastructures by showing how optical networks can be harnessed as shared ?cloud? resources. BOLD will serve as a platform for the training and education of numerous undergraduate and graduate students, including under-represented groups, in cutting edge big data-driven research.</AbstractNarration>
<MinAmdLetterDate>08/26/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1305379</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Symes</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William W Symes</PI_FULL_NAME>
<EmailAddress>symes@rice.edu</EmailAddress>
<PI_PHON>7133485997</PI_PHON>
<NSF_ID>000456209</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Cox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan Cox</PI_FULL_NAME>
<EmailAddress>alc@rice.edu</EmailAddress>
<PI_PHON>7133485730</PI_PHON>
<NSF_ID>000277789</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Jermaine</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher M Jermaine</PI_FULL_NAME>
<EmailAddress>Christopher.m.jermaine@rice.edu</EmailAddress>
<PI_PHON>7133483028</PI_PHON>
<NSF_ID>000439407</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>T. S. Eugene</FirstName>
<LastName>Ng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>T. S. Eugene Ng</PI_FULL_NAME>
<EmailAddress>eugeneng@rice.edu</EmailAddress>
<PI_PHON>7133484389</PI_PHON>
<NSF_ID>000428452</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Qianfan</FirstName>
<LastName>Xu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qianfan Xu</PI_FULL_NAME>
<EmailAddress>qianfan@rice.edu</EmailAddress>
<PI_PHON>7133484820</PI_PHON>
<NSF_ID>000536535</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 MAIN ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~900000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The hardware and software infrastructure we have developed in this project has enabled numerous new research in software defined networking technologies for hybrid packet and circuit network control. It has also enabled new research in the design of big data processing frameworks and in accelerating big data frameworks such as Spark and Hadoop by using optical networking technologies.<br /><br />The infrastructure has enabled us to develop a new data center network architecture called flat-tree that leverages small port-count converter switches to convert topologies dynamically. By changing the configurations of the converter switches, cables are rewired to different outgoing connections, as if they were unplugged and replugged manually. Using this technique to build a network, it becomes possible to have random-graph-like performance at various scales with Clos-like implementation simplicity. We have designed and implemented the network architecture and the control system. Experiments with real data center traffic traces show that flat-tree is able to optimize various workloads with different topology options. In our testbed, the traffic reaches the maximal throughput in 2.5s after a topology change, proving the feasibility of converting topology at run time. The network core bandwidth is increased by 27.6% just by converting the topology from Clos to approximate random graph. This improvement can be translated into acceleration of applications as we observe reduced communication time in Spark and Hadoop jobs.<br /><br />The infrastructure has also enabled research on task placement and circuit scheduling algorithms for structured application traffic flows that are so crucial to big data workloads. We have developed Sunflow to handle structured traffic flows (a.k.a. Coflows) from distributed data processing applications in a circuit switched network. We have proved that the performance of this algorithm is within a factor-of-two to the optimal. We further demonstrate that under realistic traffic, performance of Sunflow is on average within 1.03x&mdash; to optimal. We find that Coflows on average finish just as fast in a Sunflow-scheduled optical circuit switched network as in a comparable packet switched network employing the state-of-the-art Coflow scheduler.&nbsp; In addition to our focus on designing network scheduling algorithms with predetermined Coflow placement, i.e. the endpoints of subflows within a Coflow are preset, we have also studied the underlying Coflow placement problem and its decisive impact on scheduling efficiency. At the intra-Coflow level, constituent flows are related and therefore their placement decisions are dependent. At the inter-Coflow level, placing a new Coflow may introduce contentions with existing Coflows, which changes communication efficiency. We have developed 2D-Placement, the first placement algorithm for Coflows that considers Coflow's inter-flow relationship. Under realistic traffic, 2D-Placement improves the average Coflow completion time by up to 26% when compared with the state-of-the-art placement algorithm.<br /><br />The infrastructure has also enabled research on new circuit switching based multicast solutions. We have developed a new network architecture called HyperOptics to eliminate the circuit reconfiguration delay in using optics for multicast by directly interconnecting top of rack switches by low cost optical splitters, thereby eliminating the need for optical switches. The ToRs are organized to form the connectivity of a regular graph. We have shown that this architecture is scalable and efficient for multicasts. We have shown that running multicasts using this architecture can on average be 2.1x&mdash; faster than on an optical circuit switched network. We have also studied the problem of multicast traffic scheduling in order to take advantage of high bandwidth optical circuits and the ability to multi-hop through ToR switches. In hybrid data centers, the bandwidth of a circuit switch port is much larger than the bandwidth of a server NIC port. This means that in order to achieve high utilization of the circuit switch bandwidth, the scheduling algorithm must wisely share the bandwidth among the multicast traffic from multiple servers. We have developed a scheduling algorithm for multicast data transfer in a high-bandwidth circuit switch. The algorithm adopts multi-hopping and segmented transfer as the approaches to (1) fully utilize the high bandwidth, (2) overcome the fanout limit of the point-to-multipoint circuits and (3) effectively reduce the average demand completion time. We have shown that our algorithm outperforms the state-of-the-art scheduling algorithm by up to 13.4x.<br /><br />This project has provided many exciting opportunities for graduate student training in cutting edge networking technologies. The project has also supported students from under-represented minority groups. Specifically, this project has provided training opportunities to two female graduate students. Four graduate students partially supported by this project have received the M.S. degree, two of whom are expected to receive the Ph.D. degree shortly after the completion of this project.</p><br> <p>            Last Modified: 09/07/2017<br>      Modified by: T. S. Eugene&nbsp;Ng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The hardware and software infrastructure we have developed in this project has enabled numerous new research in software defined networking technologies for hybrid packet and circuit network control. It has also enabled new research in the design of big data processing frameworks and in accelerating big data frameworks such as Spark and Hadoop by using optical networking technologies.  The infrastructure has enabled us to develop a new data center network architecture called flat-tree that leverages small port-count converter switches to convert topologies dynamically. By changing the configurations of the converter switches, cables are rewired to different outgoing connections, as if they were unplugged and replugged manually. Using this technique to build a network, it becomes possible to have random-graph-like performance at various scales with Clos-like implementation simplicity. We have designed and implemented the network architecture and the control system. Experiments with real data center traffic traces show that flat-tree is able to optimize various workloads with different topology options. In our testbed, the traffic reaches the maximal throughput in 2.5s after a topology change, proving the feasibility of converting topology at run time. The network core bandwidth is increased by 27.6% just by converting the topology from Clos to approximate random graph. This improvement can be translated into acceleration of applications as we observe reduced communication time in Spark and Hadoop jobs.  The infrastructure has also enabled research on task placement and circuit scheduling algorithms for structured application traffic flows that are so crucial to big data workloads. We have developed Sunflow to handle structured traffic flows (a.k.a. Coflows) from distributed data processing applications in a circuit switched network. We have proved that the performance of this algorithm is within a factor-of-two to the optimal. We further demonstrate that under realistic traffic, performance of Sunflow is on average within 1.03x&mdash; to optimal. We find that Coflows on average finish just as fast in a Sunflow-scheduled optical circuit switched network as in a comparable packet switched network employing the state-of-the-art Coflow scheduler.  In addition to our focus on designing network scheduling algorithms with predetermined Coflow placement, i.e. the endpoints of subflows within a Coflow are preset, we have also studied the underlying Coflow placement problem and its decisive impact on scheduling efficiency. At the intra-Coflow level, constituent flows are related and therefore their placement decisions are dependent. At the inter-Coflow level, placing a new Coflow may introduce contentions with existing Coflows, which changes communication efficiency. We have developed 2D-Placement, the first placement algorithm for Coflows that considers Coflow's inter-flow relationship. Under realistic traffic, 2D-Placement improves the average Coflow completion time by up to 26% when compared with the state-of-the-art placement algorithm.  The infrastructure has also enabled research on new circuit switching based multicast solutions. We have developed a new network architecture called HyperOptics to eliminate the circuit reconfiguration delay in using optics for multicast by directly interconnecting top of rack switches by low cost optical splitters, thereby eliminating the need for optical switches. The ToRs are organized to form the connectivity of a regular graph. We have shown that this architecture is scalable and efficient for multicasts. We have shown that running multicasts using this architecture can on average be 2.1x&mdash; faster than on an optical circuit switched network. We have also studied the problem of multicast traffic scheduling in order to take advantage of high bandwidth optical circuits and the ability to multi-hop through ToR switches. In hybrid data centers, the bandwidth of a circuit switch port is much larger than the bandwidth of a server NIC port. This means that in order to achieve high utilization of the circuit switch bandwidth, the scheduling algorithm must wisely share the bandwidth among the multicast traffic from multiple servers. We have developed a scheduling algorithm for multicast data transfer in a high-bandwidth circuit switch. The algorithm adopts multi-hopping and segmented transfer as the approaches to (1) fully utilize the high bandwidth, (2) overcome the fanout limit of the point-to-multipoint circuits and (3) effectively reduce the average demand completion time. We have shown that our algorithm outperforms the state-of-the-art scheduling algorithm by up to 13.4x.  This project has provided many exciting opportunities for graduate student training in cutting edge networking technologies. The project has also supported students from under-represented minority groups. Specifically, this project has provided training opportunities to two female graduate students. Four graduate students partially supported by this project have received the M.S. degree, two of whom are expected to receive the Ph.D. degree shortly after the completion of this project.       Last Modified: 09/07/2017       Submitted by: T. S. Eugene Ng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
