<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: small: Numerical Linear Algebra Methods for Efficient Data Exploration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>340386.00</AwardTotalIntnAmount>
<AwardAmount>348386</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jack S. Snoeyink</SignBlockName>
<PO_EMAI>jsnoeyin@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The amount of information that is becoming available every day is expanding at an exponential rate and this is starting to render ineffective standard techniques for data exploration.  Though high-dimensional datasets present great mathematical challenges, in practice the related difficulties are mitigated by the fact that not all the measured variables are important for an understanding of the underlying phenomenon.  The "dimensionality reduction techniques" considered in this project address this issue. Their principle is to combine the variables into a smaller set onto which the data is projected before attempting a solution of the original problem. &lt;br/&gt;&lt;br/&gt;The  problem of dimension reduction gives rise to many interesting mathematical and algorithmic challenges to linear algebra specialists. In particular, one of the difficulties faced by current techniques is that existing algorithms are often too costly when dealing with very large data sets.  For example, a number of methods are based on a form of Principal Component Analysis (PCA) which becomes exceedingly expensive as the number of variables (features) and the number of samples increase.  A similar calculation is also required for graph-based approaches such as the Locally Linear Embedding (LLE), or Laplacean eigenmaps.  In addition, in many applications data sets are frequently updated, e.g., by adding or deleting data items, a situation for which standard matrix algorithms are not adapted. The proposed work will tackle a few of these challenges.  It will focus on the development of computationally efficient dimension reduction methods and related techniques, by exploiting ideas from computational linear algebra. Multilevel or divide and conquer techniques are quite common in other areas of scientific computing but have received relatively little attention in data mining. The proposed work puts methods of this type at the forefront.   &lt;br/&gt;&lt;br/&gt;One of the broader impacts of the proposed work is that it will help promote interest in problems related to the current information revolution because its research theme blends mathematical methods, good algorithmic practices, and applications requiring effective numerical methods. The applications under consideration in this work are all of great relevance to many of the new challenges of society (social networks, commerce, and security).  Finally, the software resulting from this research will be broadly disseminated to join an excellent pool of existing web sites that provide tools and repositories related to data exploration.</AbstractNarration>
<MinAmdLetterDate>08/07/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/13/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1318597</AwardID>
<Investigator>
<FirstName>Yousef</FirstName>
<LastName>Saad</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yousef Saad</PI_FULL_NAME>
<EmailAddress>saad@umn.edu</EmailAddress>
<PI_PHON>6126247804</PI_PHON>
<NSF_ID>000303745</NSF_ID>
<StartDate>08/07/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554550159</ZipCode>
<StreetAddress><![CDATA[200 Union Street SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~340386</FUND_OBLG>
<FUND_OBLG>2014~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary goal of this project was&nbsp; to explore new numerical linear algebra algorithms to help address various issues in&nbsp; machine learning and data mining.&nbsp; One of the main&nbsp; challenges in applications&nbsp; that deal with big&nbsp; data sets is&nbsp; to reduce the&nbsp; dimensionality of data.&nbsp;&nbsp; Dimension reduction is sought&nbsp; for the dual&nbsp; goal of&nbsp; lowering computational costs and minimizing the effects of noise and redundancy.<br /><br />In many problems in machine learning and other fields dealing with data, dimension&nbsp; reduction is achieved by finding a good approximation to a given original matrix, that has the property of being of low rank. The canonical approach for performing this task is the Singular Value Decomposition (SVD).&nbsp; Since the SVD is at the heart of many dimension reduction methods, one of the first ideas we explored in the&nbsp; project was that of finding effective ways of updating the it.&nbsp; In many applications, data sets under consideration undergo frequent changes e.g., when data items are&nbsp; added or removed.&nbsp; For example, in information retrieval, a few documents (columns) or terms (rows) may be added (or removed) regularly.&nbsp; The question then is to&nbsp; compute an approximate SVD from the SVD of the previous data set.&nbsp; Standard algorithms become too costly and specialized ones are needed for `updating' the current SVD rather than recomputing it.&nbsp; The project contributed new techniques in this category that can be significantly faster than exisiting ones.<br /><br />While the SVD provides a mathematically well-grounded approach to the problem of finding a low-rank approximation to a matrix, it tends to be computationally costly, and as a&nbsp; result researchers have recently began to&nbsp; explore inexpensive alternatives based on stochastic&nbsp; approaches that can work well for&nbsp; large data sets.&nbsp; These "randomization" methods obtain the needed approximations&nbsp; by performing a projection-type technique using just one subspace with a&nbsp; basis drawn at random. The method is quite effective in a number of cases but we found that it is also&nbsp; possible to use matrices from information theory and Error Correcting Codes for this task.&nbsp; The key idea is that Codewords generated using generator matrix from linear codes (random&nbsp; generator matrices) and dual BCH codes can be used in place of random Gaussian matrices.&nbsp; It&nbsp; turns out&nbsp; that this approach&nbsp; requires generating&nbsp; far fewer&nbsp; random numbers than&nbsp; Gaussian matrices&nbsp; or other structured&nbsp; random matrices&nbsp; while the precision that can be achieved is comparable to that obtained random with Gaussian matrices.&nbsp; This work showed the importance of using tools from diverse horizons to solve new classes of problems encountered in data related applications.<br /><br />This project also studied&nbsp; an important problem that&nbsp; is seldom addressed in&nbsp; applications, namely the problem of&nbsp; finding the approximate rank of&nbsp; a given matrix. Indeed, when one&nbsp; uses a dimension reduction technique, e.g.,&nbsp; one that is based on principal&nbsp; component analysis (PCA), the dimension is&nbsp; often guessed from certain practical considerations, rather than&nbsp; computed or estimated.&nbsp; We have developed a&nbsp; few computationally inexpensive techniques to estimate the&nbsp; approximate rank of a matrix.&nbsp; These techniques exploit&nbsp; approximate spectral densities also known as&nbsp; Densities of States (DOS), which are probability&nbsp; density distributions that measure the likelihood of finding eigenvalues of the matrix at a given point on a real&nbsp; line.&nbsp; Integrating the DOS of a matrix over an interval gives the eigenvalue count in<br />that interval and therefore&nbsp; the rank can be determined by&nbsp; selecting an appropriate interval over which to&nbsp; integrate. The methods are fast enough&nbsp; that the cost of obtaining an estimated rank can&nbsp; be negligible in most practical applications.&nbsp; As a&nbsp; by-product of this work on rank estimation, we explored&nbsp; the many uses of trace estimation techniques.&nbsp; Estimating the trace of a linear operator, e.g., the exponential or log of a given matrix, is an ubiquitous tool.&nbsp; The potential applications in statistical methods, such&nbsp; as parameter estimation are of great current importance.&nbsp;&nbsp; This project contributed a few methods based on&nbsp; the Lanczos algorithm along with a rigorous convergence analysis.<br /><br />The demand for graduates that are&nbsp; trained in numerical methods and methods that are at the interface between&nbsp; linear algebra, machine learning, and optimization, is growing fast.&nbsp; In terms of broader impacts, the project supported 2 students and&nbsp; (partially) a post-doc, as well as 3 student summer interns who gained expertise in some of these areas.</p><br> <p>            Last Modified: 09/08/2017<br>      Modified by: Yousef&nbsp;Saad</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary goal of this project was  to explore new numerical linear algebra algorithms to help address various issues in  machine learning and data mining.  One of the main  challenges in applications  that deal with big  data sets is  to reduce the  dimensionality of data.   Dimension reduction is sought  for the dual  goal of  lowering computational costs and minimizing the effects of noise and redundancy.  In many problems in machine learning and other fields dealing with data, dimension  reduction is achieved by finding a good approximation to a given original matrix, that has the property of being of low rank. The canonical approach for performing this task is the Singular Value Decomposition (SVD).  Since the SVD is at the heart of many dimension reduction methods, one of the first ideas we explored in the  project was that of finding effective ways of updating the it.  In many applications, data sets under consideration undergo frequent changes e.g., when data items are  added or removed.  For example, in information retrieval, a few documents (columns) or terms (rows) may be added (or removed) regularly.  The question then is to  compute an approximate SVD from the SVD of the previous data set.  Standard algorithms become too costly and specialized ones are needed for `updating' the current SVD rather than recomputing it.  The project contributed new techniques in this category that can be significantly faster than exisiting ones.  While the SVD provides a mathematically well-grounded approach to the problem of finding a low-rank approximation to a matrix, it tends to be computationally costly, and as a  result researchers have recently began to  explore inexpensive alternatives based on stochastic  approaches that can work well for  large data sets.  These "randomization" methods obtain the needed approximations  by performing a projection-type technique using just one subspace with a  basis drawn at random. The method is quite effective in a number of cases but we found that it is also  possible to use matrices from information theory and Error Correcting Codes for this task.  The key idea is that Codewords generated using generator matrix from linear codes (random  generator matrices) and dual BCH codes can be used in place of random Gaussian matrices.  It  turns out  that this approach  requires generating  far fewer  random numbers than  Gaussian matrices  or other structured  random matrices  while the precision that can be achieved is comparable to that obtained random with Gaussian matrices.  This work showed the importance of using tools from diverse horizons to solve new classes of problems encountered in data related applications.  This project also studied  an important problem that  is seldom addressed in  applications, namely the problem of  finding the approximate rank of  a given matrix. Indeed, when one  uses a dimension reduction technique, e.g.,  one that is based on principal  component analysis (PCA), the dimension is  often guessed from certain practical considerations, rather than  computed or estimated.  We have developed a  few computationally inexpensive techniques to estimate the  approximate rank of a matrix.  These techniques exploit  approximate spectral densities also known as  Densities of States (DOS), which are probability  density distributions that measure the likelihood of finding eigenvalues of the matrix at a given point on a real  line.  Integrating the DOS of a matrix over an interval gives the eigenvalue count in that interval and therefore  the rank can be determined by  selecting an appropriate interval over which to  integrate. The methods are fast enough  that the cost of obtaining an estimated rank can  be negligible in most practical applications.  As a  by-product of this work on rank estimation, we explored  the many uses of trace estimation techniques.  Estimating the trace of a linear operator, e.g., the exponential or log of a given matrix, is an ubiquitous tool.  The potential applications in statistical methods, such  as parameter estimation are of great current importance.   This project contributed a few methods based on  the Lanczos algorithm along with a rigorous convergence analysis.  The demand for graduates that are  trained in numerical methods and methods that are at the interface between  linear algebra, machine learning, and optimization, is growing fast.  In terms of broader impacts, the project supported 2 students and  (partially) a post-doc, as well as 3 student summer interns who gained expertise in some of these areas.       Last Modified: 09/08/2017       Submitted by: Yousef Saad]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
