<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Collaborative Research: Modeling and Analyzing Big Data on Peta- and Exascale Distributed Systems Supported by MapReduce Methodologies</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>69038.00</AwardTotalIntnAmount>
<AwardAmount>69038</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Current petascale platforms can perform large-scale simulations and generate massive amounts of data at unprecedented rates. These rates are expected to increase as exascale platforms are introduced. The generation of more and more data presents new challenges for scientists who struggle with the analysis, sorting, and selection of scientifically meaningful results. When very large amounts of data records are located across a large number of nodes in a distributed memory system, even a small number of comparisons can be costly or even impossible. Therefore, new methodologies are necessary to analyze large scientific datasets at scale.&lt;br/&gt;&lt;br/&gt;The goal of this project is to develop a transformative analysis method to model the properties of large scientific datasets in a distributed manner on petascale systems today and exascale systems in the future. The research activity includes (1) the design of new algorithms for encoding properties embedded in distributed data in a parallel manner by using space reduction techniques; (2) the design of new algorithms for clustering and classifying these properties by using distributed paradigms such as MapReduce; (3) the deployment of the algorithms for diverse datasets in structural biology and astronomy; and (4) the tuning of the algorithms for both result performance and accuracy on emerging storage technologies. &lt;br/&gt;&lt;br/&gt;The analysis method will provide the scientific community with infrastructures and instrumentations to identify features that can be used to predict class memberships; find recurrent patterns in datasets; and identify class memberships from a specific feature or property. By effectively and accurately capturing scientific information in a scalable manner, these infrastructures and instrumentations will break the traditional constraint of data centralization and allow scientists to overcome the difficulties associated with the fully distributed nature of the data considered.&lt;br/&gt;&lt;br/&gt;The project's educational component promotes training and learning in computational modeling and analysis techniques as well as data-intensive algorithms and platforms by involving undergraduate and graduate students in research activities and integrating big data analytics into the undergraduate curriculum at the University of Delaware. The research-based educational materials developed in this project will be made available to the scientific community through the project portal and through tutorials at XSEDE and Supercomputing (SC) conferences.</AbstractNarration>
<MinAmdLetterDate>06/24/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/24/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1318417</AwardID>
<Investigator>
<FirstName>Pietro</FirstName>
<LastName>Cicotti</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pietro Cicotti</PI_FULL_NAME>
<EmailAddress>pcicotti@sdsc.edu</EmailAddress>
<PI_PHON>8588225465</PI_PHON>
<NSF_ID>000623655</NSF_ID>
<StartDate>06/24/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~69038</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Empowered by the MapReduce programming model, in this project, we developed a comprehensive analytics approach for modeling and analyzing large scientific datasets in a distributed manner on supercomputers. Our approach breaks the traditional constraint of analyzing data in a post-mortem, centralized fashion. We applied our approach to a diverse set of datasets in different scientific domains including data in computational chemistry (e.g., molecular ensembles from docking simulations and trajectory snapshots from folding simulations) and data in bioinformatics (e.g., genome data). Each different data type is amenable to a distributed analysis with the MapReduce programming model that clusters or classifies the data own &ldquo;space&rdquo; (e.g., protein-ligand interaction, protein structure, or DNA sequences). To this end, we transformed the way we describe and analyze data by embracing four research components: data structures, analysis algorithms, computing paradigms, and storage technologies. The key questions that were of great priority in our project were:</p> <p>(1)&nbsp;&nbsp; Can we encode big data properties in a parallel manner when data are distributed across nodes of a supercomputers, so that we can still effectively and accurately capture properties while pursuing scalability?</p> <p>(2)&nbsp;&nbsp; Can we design new algorithms for big data analysis in distributed environments using distributed programming models, such as MapReduce?</p> <p>(3)&nbsp;&nbsp; Can we achieve scalable performance and accurate results with a MapReduce framework when applied to diverse data such as, for example, structural biology and bioinformatics, on emerging supercomputer technologies?</p> <p>The overarching goal of this project was to answer these questions by designing, developing, and assessing new analytics algorithms on top of a new and efficient MapReduce framework capable of analyzing big data on cutting-edge technologies. We used our analytics approach (i.e., the combination of our analytics algorithms and MapReduce framework) on state-of-the-art supercomputers.&nbsp; More specifically, we pursued these three research objectives:</p> <p>(1)&nbsp;&nbsp; Define and implement new methodologies for capturing, encoding, and modeling sophisticated data properties across distributed sites for a broader set of datatypes (e.g., in structural biology and bioinformatics).</p> <p>(2)&nbsp;&nbsp; Design and implement new algorithms for data analysis and integrate these algorithms into a distributed framework based on the MapReduce programming model.</p> <p>(3)&nbsp;&nbsp; Study and tune both accuracy and performance of our MapReduce-based approach on state-of-the-art computing platforms for real datasets.</p> <p>Our outcomes were delivered to the computer science community and other scientific communities in computational chemistry and bioinformatics. The main achievements are:</p> <p>(1)&nbsp;&nbsp; The release of the open source code for the implementation of a new MapReduce over MPI framework that allows significantly larger datasets to be analyzed in parallel, achieving large performance gains.</p> <p>(2)&nbsp;&nbsp; For the study of large protein-ligand docking datasets in computational chemistry, we demonstrated that our analytics approach can capture the geometrical properties of ligand conformations in drug design simulations more effectively, while predicting near-native ligand conformations more accurately than traditional methods do, including the hierarchical clustering and energy-based scoring methods.</p> <p>(3)&nbsp;&nbsp; For the study of large protein folding trajectories in computational chemistry, we demonstrated our approach&rsquo;s effectiveness by successfully applying it to two case studies in protein folding simulations. For these two case studies, we demonstrated (1) the ability of our approach to identify transformations of individual proteins&rsquo; secondary structures, for example from a single &alpha;-helix to a single &beta;-strand, and (2) the capability of our approach to capture the relative position of a protein&rsquo;s secondary structures with respect to each other over the trajectory, as the simulation evolves and without slowing down the simulation itself.</p> <p>(4)&nbsp;&nbsp; For the study of extremely large DNA datasets in bioinformatics, we could process unprecedented&nbsp;datasets (up to 24 TB of DNA datasets) on supercomputers and count 22-mers of the 24 TB dataset with 3,072 processes in about one hour. In previous work, the same test took up to several days.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/29/2017<br>      Modified by: Pietro&nbsp;Cicotti</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Empowered by the MapReduce programming model, in this project, we developed a comprehensive analytics approach for modeling and analyzing large scientific datasets in a distributed manner on supercomputers. Our approach breaks the traditional constraint of analyzing data in a post-mortem, centralized fashion. We applied our approach to a diverse set of datasets in different scientific domains including data in computational chemistry (e.g., molecular ensembles from docking simulations and trajectory snapshots from folding simulations) and data in bioinformatics (e.g., genome data). Each different data type is amenable to a distributed analysis with the MapReduce programming model that clusters or classifies the data own "space" (e.g., protein-ligand interaction, protein structure, or DNA sequences). To this end, we transformed the way we describe and analyze data by embracing four research components: data structures, analysis algorithms, computing paradigms, and storage technologies. The key questions that were of great priority in our project were:  (1)   Can we encode big data properties in a parallel manner when data are distributed across nodes of a supercomputers, so that we can still effectively and accurately capture properties while pursuing scalability?  (2)   Can we design new algorithms for big data analysis in distributed environments using distributed programming models, such as MapReduce?  (3)   Can we achieve scalable performance and accurate results with a MapReduce framework when applied to diverse data such as, for example, structural biology and bioinformatics, on emerging supercomputer technologies?  The overarching goal of this project was to answer these questions by designing, developing, and assessing new analytics algorithms on top of a new and efficient MapReduce framework capable of analyzing big data on cutting-edge technologies. We used our analytics approach (i.e., the combination of our analytics algorithms and MapReduce framework) on state-of-the-art supercomputers.  More specifically, we pursued these three research objectives:  (1)   Define and implement new methodologies for capturing, encoding, and modeling sophisticated data properties across distributed sites for a broader set of datatypes (e.g., in structural biology and bioinformatics).  (2)   Design and implement new algorithms for data analysis and integrate these algorithms into a distributed framework based on the MapReduce programming model.  (3)   Study and tune both accuracy and performance of our MapReduce-based approach on state-of-the-art computing platforms for real datasets.  Our outcomes were delivered to the computer science community and other scientific communities in computational chemistry and bioinformatics. The main achievements are:  (1)   The release of the open source code for the implementation of a new MapReduce over MPI framework that allows significantly larger datasets to be analyzed in parallel, achieving large performance gains.  (2)   For the study of large protein-ligand docking datasets in computational chemistry, we demonstrated that our analytics approach can capture the geometrical properties of ligand conformations in drug design simulations more effectively, while predicting near-native ligand conformations more accurately than traditional methods do, including the hierarchical clustering and energy-based scoring methods.  (3)   For the study of large protein folding trajectories in computational chemistry, we demonstrated our approach?s effectiveness by successfully applying it to two case studies in protein folding simulations. For these two case studies, we demonstrated (1) the ability of our approach to identify transformations of individual proteins? secondary structures, for example from a single &alpha;-helix to a single &beta;-strand, and (2) the capability of our approach to capture the relative position of a protein?s secondary structures with respect to each other over the trajectory, as the simulation evolves and without slowing down the simulation itself.  (4)   For the study of extremely large DNA datasets in bioinformatics, we could process unprecedented datasets (up to 24 TB of DNA datasets) on supercomputers and count 22-mers of the 24 TB dataset with 3,072 processes in about one hour. In previous work, the same test took up to several days.           Last Modified: 11/29/2017       Submitted by: Pietro Cicotti]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
