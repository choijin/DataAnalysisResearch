<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TWC: Small: A Choice Architecture for Mobile Privacy and Security</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anita  Nikolich</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Mobile devices (e.g., smartphones and tablets) allow users to execute rich third-party applications that are capable of making extensive use of device hardware and personal data.  This poses security risks, as applications may perform undesirable operations such as deleting data, damaging hardware, or even directly incurring charges on the user's phone bill.  Mobile devices also pose privacy risks, as they store sensitive personal information that may be accessed and shared inappropriately.&lt;br/&gt;&lt;br/&gt;Empowering users to decide how resources on their mobile devices are accessed (i.e., "granting permission") is an important challenge for the future of mobile computing.  Our research has shown that existing mechanisms are ineffective: users frequently grant permissions because they either do not understand them, are habituated to them, or feel that they have no other choice.  This research project aims to identify and study potential solutions to these problems.&lt;br/&gt;&lt;br/&gt;This project develops a user-centered approach to mobile device permission requests.  The project is conducting human-subjects experiments to design and validate more effective mechanisms for regulating privacy- or security-sensitive actions.  The research agenda involves minimizing habituation to security warnings by substituting them with protected widgets (i.e., "trusted UI") or audit mechanisms, when possible; improving the design of security warnings, because alternative permission-granting mechanisms are sometimes inappropriate; and integrating these mechanisms into a platform that improves system security by taking a user-centered approach to granting permissions.  If successful, this project could help develop a secure foundation for future generations of mobile devices.</AbstractNarration>
<MinAmdLetterDate>08/13/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1318680</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Wagner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Wagner</PI_FULL_NAME>
<EmailAddress>daw@cs.berkeley.edu</EmailAddress>
<PI_PHON>5106422758</PI_PHON>
<NSF_ID>000651735</NSF_ID>
<StartDate>08/13/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Serge</FirstName>
<LastName>Egelman</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Serge M Egelman</PI_FULL_NAME>
<EmailAddress>egelman@icsi.berkeley.edu</EmailAddress>
<PI_PHON>5106662900</PI_PHON>
<NSF_ID>000553035</NSF_ID>
<StartDate>07/28/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947201776</ZipCode>
<StreetAddress><![CDATA[Soda Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Mobile devices pose new security and privacy risks because they allow third-party applications to access an unprecedented amount of user data, such as audio and video recordings, address book contacts, and location trails. To mitigate these risks, the major mobile platforms have tasked users with regulating access to device resources; when an application requires access to certain data, the user is asked to approve or deny the request. This poses an interesting usable security problem because it is predicated on users noticing and understanding the information presented to them. In our initial research, we showed that this assumption is often incorrect: users frequently grant access because they either do not understand the requests, are habituated to them, or feel that they have no other choice.</p> <p>We performed research into developing a more user-centered approach to granting applications access to mobile device resources. We conducted several&nbsp; human subjects studies to empirically demonstrate how these permission-granting mechanisms can be improved, and then validated our findings. Our research agenda centered around minimizing habituation to security warnings by substituting them with protected widgets (i.e., "trusted UI") or audit/feedback mechanisms, when possible; improving the design of security warnings, because alternative permission-granting mechanisms are sometimes inappropriate; and integrating these mechanisms into a platform that improves system security by taking a user-centered approach by automatically learning from users' previous decisions.</p> <p>Several side projects came out of this research, including studies that examined ways of better communicating data capture on continuous sensiting platforms (e.g., video capture within the home on "smart" devices), as well as studies to better understand why users refuse to use certain security features on their devices, so that the usability of those features can be improved. However, the main thrust of this research focused on re-engineering mobile platforms to better support users' privacy preferences. In doing this, we performed several studies to understand how existing systems make privacy decisions, in order to identify shortcomings.</p> <p>In one study of app developers, we observed that few developers used a feature that allowed them to better explain an app's access to sensitive data. Worse, we observed that when they did use the feature, any explanation, regardless of whether or not it actually imparted useful information, resulted in users' increased willingness to provide data to apps. In another study, we built extensive instrumentation into the Android platform so that we could observe how often and under what circumstances apps access protected user information. We found that it occurs far more frequently than users expect: thousands of times per day, and usually when the device is not even in use.</p> <p>To address several of these privacy problems, we built an AI that can automatically detect when an app's access to sensitive user data is likely to defy users' expectations and take appropriate action (such as blocking the app or directly asking the user). We performed several field studies in which real users used our system on their phones; we found that it reduced privacy violations (as compared to the status quo) by a factor of four. As part of this research, we also developed a user interface that allows a user to understand the privacy decisions being made by the AI on his or her behalf, so that any errors can be corrected, and then the system can learn from them to make fewer errors in the future.</p> <p>This work has had additional broader impacts on industry and policymakers. Our instrumentation to monitor apps' access to sensitive user data is now being used by regulators and others to investigate potential app privacy violations. We have also transformed it into an automatic testbed that we are using to analyze mobile apps and then inform the general public via our website (https://www.appcensus.mobi). We are also pursuing commercialization possibilities for both the instrumentation to monitor app behaviors, as well as the AI component.</p><br> <p>            Last Modified: 11/27/2017<br>      Modified by: Serge&nbsp;M&nbsp;Egelman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Mobile devices pose new security and privacy risks because they allow third-party applications to access an unprecedented amount of user data, such as audio and video recordings, address book contacts, and location trails. To mitigate these risks, the major mobile platforms have tasked users with regulating access to device resources; when an application requires access to certain data, the user is asked to approve or deny the request. This poses an interesting usable security problem because it is predicated on users noticing and understanding the information presented to them. In our initial research, we showed that this assumption is often incorrect: users frequently grant access because they either do not understand the requests, are habituated to them, or feel that they have no other choice.  We performed research into developing a more user-centered approach to granting applications access to mobile device resources. We conducted several  human subjects studies to empirically demonstrate how these permission-granting mechanisms can be improved, and then validated our findings. Our research agenda centered around minimizing habituation to security warnings by substituting them with protected widgets (i.e., "trusted UI") or audit/feedback mechanisms, when possible; improving the design of security warnings, because alternative permission-granting mechanisms are sometimes inappropriate; and integrating these mechanisms into a platform that improves system security by taking a user-centered approach by automatically learning from users' previous decisions.  Several side projects came out of this research, including studies that examined ways of better communicating data capture on continuous sensiting platforms (e.g., video capture within the home on "smart" devices), as well as studies to better understand why users refuse to use certain security features on their devices, so that the usability of those features can be improved. However, the main thrust of this research focused on re-engineering mobile platforms to better support users' privacy preferences. In doing this, we performed several studies to understand how existing systems make privacy decisions, in order to identify shortcomings.  In one study of app developers, we observed that few developers used a feature that allowed them to better explain an app's access to sensitive data. Worse, we observed that when they did use the feature, any explanation, regardless of whether or not it actually imparted useful information, resulted in users' increased willingness to provide data to apps. In another study, we built extensive instrumentation into the Android platform so that we could observe how often and under what circumstances apps access protected user information. We found that it occurs far more frequently than users expect: thousands of times per day, and usually when the device is not even in use.  To address several of these privacy problems, we built an AI that can automatically detect when an app's access to sensitive user data is likely to defy users' expectations and take appropriate action (such as blocking the app or directly asking the user). We performed several field studies in which real users used our system on their phones; we found that it reduced privacy violations (as compared to the status quo) by a factor of four. As part of this research, we also developed a user interface that allows a user to understand the privacy decisions being made by the AI on his or her behalf, so that any errors can be corrected, and then the system can learn from them to make fewer errors in the future.  This work has had additional broader impacts on industry and policymakers. Our instrumentation to monitor apps' access to sensitive user data is now being used by regulators and others to investigate potential app privacy violations. We have also transformed it into an automatic testbed that we are using to analyze mobile apps and then inform the general public via our website (https://www.appcensus.mobi). We are also pursuing commercialization possibilities for both the instrumentation to monitor app behaviors, as well as the AI component.       Last Modified: 11/27/2017       Submitted by: Serge M Egelman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
