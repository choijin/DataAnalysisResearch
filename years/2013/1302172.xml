<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Medium: Collaborative Research: A Heterogeneous Inference Framework for 3D Modeling and Rendering of Sites</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>600000.00</AwardTotalIntnAmount>
<AwardAmount>600000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Organizing and using 3D data related to physical sites is important in many applications such as historical reconstruction, architectural design, and urban planning.  However, no method has been developed that exploits the full range of data types available for such sites.  Useful data often comes from historical sources, and requires substantial processing to be useful.  Some of this processing can be automated, but some of it must be done by humans.  An as-yet unsolved problem is how to coordinate human effort to efficiently carry out this process.  In the current project the PIs will address quantitative and qualitative accuracy issues in reconstructing 3D sites so as to allow for input and participation by different populations in building data sets, and will demonstrate a variety of applications using a heterogeneous 3D site representation. Specifically, the work will make the following contributions: new techniques for annotating heterogeneous input will be developed, balancing automated and human input; new techniques for coordinating digital computation, human computation, and machine learning will be devised; new tools for architectural analysis and design, and for material weathering analysis, will be developed based on the new 3D representation; and new ideas for storytelling from 3D data will be demonstrated.  Project outcomes will include a new organization of heterogeneous data for 3D sites, new insights into the relative contributions of automated techniques and human computation in the domain of 3D site data (which will be applicable to other challenging problems involving large complex data sets), new algorithms for reconstructing 3D models, and new techniques for conducting studies in architecture and in cultural heritage.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This research will have a strong impact on architectural-design and cultural heritage documentation, interpretation and communication.  The various phases of the project will involve students at both the graduate and undergraduate levels, and in diverse disciplines including computer science, architecture, and art history.  The PIs will produce teaching modules based on this work targeted at computer science, architecture, and cultural heritage.</AbstractNarration>
<MinAmdLetterDate>05/16/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/16/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302172</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Aliaga</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel G Aliaga</PI_FULL_NAME>
<EmailAddress>aliaga@cs.purdue.edu</EmailAddress>
<PI_PHON>7654967943</PI_PHON>
<NSF_ID>000483177</NSF_ID>
<StartDate>05/16/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Neville</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer Neville</PI_FULL_NAME>
<EmailAddress>neville@cs.purdue.edu</EmailAddress>
<PI_PHON>7654944600</PI_PHON>
<NSF_ID>000171629</NSF_ID>
<StartDate>05/16/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072107</ZipCode>
<StreetAddress><![CDATA[305 N. University St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~600000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we developed novel approaches to combining multiple sources of information in order to reconstruct 3D urban current and historical places. In particular, our process allows us to take a partial description of a 3D scene, from different data sources, and "fill-in" the missing parts with plausible results. Moreover, the reconstruction, since procedural, can then be efficiently used for analysis and/or generation of similar content. Our methodology makes use of deep learning, optimization, and several heuristics.</p> <p>For example, in one subproject we automatically convert either a digital sketch or a photograph of a building into a complete 3D procedural model. The main steps include recognizing the basic component types, parameter estimation, and then synthesis of the 3D model (Figure 1).</p> <p>In another example work, we automatically discover a &ldquo;program&rdquo; that can segment, label and then generate any provided building model (e.g., downloaded from the web), even though the input building model has no semantic or hierarchical information. Essentially, we look for symmetries and patterns and then organize them (Figure 2).</p> <p>An interesting spin-off of this project is the ability to predict how changes of an urban location affect the local weather. Here, the inference our project enables is not only to obtain the urban form but also the interactions between the local weather and the urban shape and material. This has led to an exciting collaboration with WUDAPT, an international grassroots effort to model city and weather worldwide (Figure 3).</p> <p>Finally, during our project we also made significant improvements to the field of machine learning. We developed novel machine learning methods for heterogeneous domains and only partial observations. In addition, we have also been working extensively to develop deep learning methods for graphs and for structured data with multiple views/sources.</p> <p>Altogether, our project has been received with significant interest and lateral translation of this work to the urban modeling community as one way to use the multitude of heterogeneous inputs to help with urban planning and prediction. Further, the project has supported at least partially seven graduate students and one post-doc. Moreover, we have obtained excellent traction publication wise with the work funded by this grant producing 3 SIGGRAPH/TOG papers, an ICCV paper, Computer Graphics Forum paper, 3DV paper, two SIGGRAPH courses (in 2015 and in 2017), a AAAI paper, a large grant from IARPA to further build upon this work of this proposal, and additional papers as well.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/20/2017<br>      Modified by: Daniel&nbsp;G&nbsp;Aliaga</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203107765_sketch--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203107765_sketch--rgov-800width.jpg" title="Figure 1: Sketching to Building Model"><img src="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203107765_sketch--rgov-66x44.jpg" alt="Figure 1: Sketching to Building Model"></a> <div class="imageCaptionContainer"> <div class="imageCaption">(included in figure)</div> <div class="imageCredit">Nishida et al. 2016, ACM Transactions on Graphics</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;G&nbsp;Aliaga</div> <div class="imageTitle">Figure 1: Sketching to Building Model</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203201232_ipm--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203201232_ipm--rgov-800width.jpg" title="Figure 2: Arbitrary 3D Model to Procedural Model"><img src="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203201232_ipm--rgov-66x44.jpg" alt="Figure 2: Arbitrary 3D Model to Procedural Model"></a> <div class="imageCaptionContainer"> <div class="imageCaption">(included in figure)</div> <div class="imageCredit">Ilke Demir et al. 2015, ACM Transactions on Graphics</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;G&nbsp;Aliaga</div> <div class="imageTitle">Figure 2: Arbitrary 3D Model to Procedural Model</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203272418_weather--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203272418_weather--rgov-800width.jpg" title="Figure 3: Weather to Procedural Model"><img src="/por/images/Reports/POR/2017/1302172/1302172_10245916_1511203272418_weather--rgov-66x44.jpg" alt="Figure 3: Weather to Procedural Model"></a> <div class="imageCaptionContainer"> <div class="imageCaption">(included in figure)</div> <div class="imageCredit">Garcia-Dorado et al. 2017, ACM Transactions on Graphics</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Daniel&nbsp;G&nbsp;Aliaga</div> <div class="imageTitle">Figure 3: Weather to Procedural Model</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we developed novel approaches to combining multiple sources of information in order to reconstruct 3D urban current and historical places. In particular, our process allows us to take a partial description of a 3D scene, from different data sources, and "fill-in" the missing parts with plausible results. Moreover, the reconstruction, since procedural, can then be efficiently used for analysis and/or generation of similar content. Our methodology makes use of deep learning, optimization, and several heuristics.  For example, in one subproject we automatically convert either a digital sketch or a photograph of a building into a complete 3D procedural model. The main steps include recognizing the basic component types, parameter estimation, and then synthesis of the 3D model (Figure 1).  In another example work, we automatically discover a "program" that can segment, label and then generate any provided building model (e.g., downloaded from the web), even though the input building model has no semantic or hierarchical information. Essentially, we look for symmetries and patterns and then organize them (Figure 2).  An interesting spin-off of this project is the ability to predict how changes of an urban location affect the local weather. Here, the inference our project enables is not only to obtain the urban form but also the interactions between the local weather and the urban shape and material. This has led to an exciting collaboration with WUDAPT, an international grassroots effort to model city and weather worldwide (Figure 3).  Finally, during our project we also made significant improvements to the field of machine learning. We developed novel machine learning methods for heterogeneous domains and only partial observations. In addition, we have also been working extensively to develop deep learning methods for graphs and for structured data with multiple views/sources.  Altogether, our project has been received with significant interest and lateral translation of this work to the urban modeling community as one way to use the multitude of heterogeneous inputs to help with urban planning and prediction. Further, the project has supported at least partially seven graduate students and one post-doc. Moreover, we have obtained excellent traction publication wise with the work funded by this grant producing 3 SIGGRAPH/TOG papers, an ICCV paper, Computer Graphics Forum paper, 3DV paper, two SIGGRAPH courses (in 2015 and in 2017), a AAAI paper, a large grant from IARPA to further build upon this work of this proposal, and additional papers as well.             Last Modified: 11/20/2017       Submitted by: Daniel G Aliaga]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
