<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Three Dimensional Headphone Audio for Music, Gaming, Entertainment and Telepresence</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>179999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This Small Business Innovation Research Phase I Project will create immersive three-dimensional (3D) sound for users engaged in gaming, listening to music or watching cinema. Using their two ears, humans are able to localize a sound source in a complex scene and infer the ambience. Unfortunately, when sound is reproduced over speakers or headphones it is almost impossible for the listener to perceive the sound location with any clarity or precision. This is because the cues that the brain uses for inferring source location are lost. Over the past decade the proposers, when at the University of Maryland, have studied the problem of how it would be possible to recreate a 3D sound environment virtually. The research, funded by NSF resulted in a fundamental understanding, as well as demonstrations, efficient software and technology for the capture, reproduction, recreation and rendering of 3D sound. This includes efficient approximations for the room impulse response and the Head Related Transfer Function. In the proposed Phase I R&amp;D, these technologies will be prepared for commercial introduction, addressing those issues raised in customer discovery ¨C maintaining audio quality through the signal processing chain, incorporation into existing tools used by developers, and creation of low-latency telepresence.&lt;br/&gt;&lt;br/&gt;The broader impact/commercial potential of this project will be significant. Because of the smartphone/tablet revolution, and the availability of movie, music, and gaming content on the cloud, there are over a billion consumers who enjoy music, games, movies and other media on their mobile devices. However, most users settle for lackluster sound produced over headphones. Current headphone sound is simply unable to produce the engaging sound experience that high©\end music systems, movie theaters, or live events deliver. Success of the proposed project will allow entertainment content creators to achieve unmatched realism in sound presentation over headphones. Combined with our sound capture hardware, immersive reproduction of live events such as concerts and sporting events can be achieved. Our technology will allow companies to make headphone listening more immersive and reach this growing consumer base. They will all be positively impacted by the technology. Because of the large market, the intellectual property, and the quality of our team, we anticipate that a large and successful business can be built around this technology with SBIR funding. This company will add to the nations¡¯ dominance in the fields of mobile technologies, gaming, entertainment, and immersive simulation.</AbstractNarration>
<MinAmdLetterDate>06/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>01/03/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1315656</AwardID>
<Investigator>
<FirstName>Adam</FirstName>
<LastName>O'Donovan</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adam E O'Donovan</PI_FULL_NAME>
<EmailAddress>adam.o@visisonics.com</EmailAddress>
<PI_PHON>3014558108</PI_PHON>
<NSF_ID>000566170</NSF_ID>
<StartDate>06/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>VisiSonics Corporation</Name>
<CityName>Highland</CityName>
<ZipCode>207779797</ZipCode>
<PhoneNumber>3013322507</PhoneNumber>
<StreetAddress>6800 Koandah Gardens</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>962585456</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VISISONICS CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[VisiSonics Corporation]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207770001</ZipCode>
<StreetAddress><![CDATA[387 Technology Drive; Suite 3122]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<ProgramReference>
<Code>9139</Code>
<Text>INFORMATION INFRASTRUCTURE &amp; TECH APPL</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~150000</FUND_OBLG>
<FUND_OBLG>2014~29999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Body">This Small Business Innovation Research Phase 1 and Phase 1b Project aimed to create technology, based on university research, for creating and capturing immersive three-dimensional (3D) sound for users engaged in gaming, virtual reality, listening to music, watching cinema, or remotely experiencing an event. Using just the streams of sound received at their two ears, humans are able to perceive a rich 3D world, segregated into sound objects, their locations, the ambience of the room in which they exist, and can perform complex sound scene analysis. Unfortunately, when sound is reproduced over speakers or headphones, &nbsp;it is almost impossible for the listener to perceive the sound scene in its three-dimensional richness with any clarity or precision. This is because the cues that the brain uses for inferring things like source location are lost when a natural scene is reproduced. Consequentially, the experience of sound is &ldquo;flat&rdquo; and &ldquo;in the head.&rdquo;&nbsp;</p> <p class="Body">Our research seeks to create devices for capture of real scenes via microphone arrays that retain these cues, and develop a 3D rendering engine for virtual scenes.</p> <p class="Body">Phase 1 and Phase 1b made significant progress in developing these, and it was decided to continue this project and apply for a Phase II project.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/29/2015<br>      Modified by: Adam&nbsp;E&nbsp;O'donovan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[This Small Business Innovation Research Phase 1 and Phase 1b Project aimed to create technology, based on university research, for creating and capturing immersive three-dimensional (3D) sound for users engaged in gaming, virtual reality, listening to music, watching cinema, or remotely experiencing an event. Using just the streams of sound received at their two ears, humans are able to perceive a rich 3D world, segregated into sound objects, their locations, the ambience of the room in which they exist, and can perform complex sound scene analysis. Unfortunately, when sound is reproduced over speakers or headphones,  it is almost impossible for the listener to perceive the sound scene in its three-dimensional richness with any clarity or precision. This is because the cues that the brain uses for inferring things like source location are lost when a natural scene is reproduced. Consequentially, the experience of sound is "flat" and "in the head."  Our research seeks to create devices for capture of real scenes via microphone arrays that retain these cues, and develop a 3D rendering engine for virtual scenes. Phase 1 and Phase 1b made significant progress in developing these, and it was decided to continue this project and apply for a Phase II project.          Last Modified: 05/29/2015       Submitted by: Adam E O'donovan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
