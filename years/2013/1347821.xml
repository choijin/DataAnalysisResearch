<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Community Implementation: WIDER:Data Explorer and Assessment Resources for Faculty</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>422603.00</AwardTotalIntnAmount>
<AwardAmount>440723</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ellen Carpenter</SignBlockName>
<PO_EMAI>elcarpen@nsf.gov</PO_EMAI>
<PO_PHON>7032925104</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A collaborative PI team from Kansas State University and the American Association of Physics Teachers is creating a national model for improving STEM higher education through a community-based web resource to help physics faculty transform their teaching by incorporating teaching methods and assessments based on research into classroom learning. Recognizing that higher education struggles to find ways to evaluate and improve instruction, the project is using a "bottom-up" approach in which discipline-based teachers/researchers develop, share, and aggregate assessment data.&lt;br/&gt;&lt;br/&gt;Research-based assessment instruments have had a major impact on physics education reform. They provide universal and convincing measures of student understanding that instructors can use to assess and improve their teaching. These instruments can transform teaching practice by informing instructors about their teaching efficacy so that they can improve it. At the same time, their widespread use can transform researchers' understanding of the impact of educational transformation by providing large quantities of data that compare teaching practices across a broad range of institutions and student populations.&lt;br/&gt;&lt;br/&gt;The results of preliminary studies suggest that physics faculty members are eager to use their cognitive resources as scientists to explore big data and compare their students' assessment results to those of other students like their own.  However, the preliminary studies also suggest that many instructors who use these instruments do not know how to interpret the results or how to use them to improve their teaching. Further, because local results are known only to individual instructors, researchers do not have access to this pool of data.  For these reasons, the project is turning the private practice of administering assessment instruments into a community practice of interpreting assessment results in the context of a large community of educators using similar practices in similar settings, comparing results, and using them to transform teaching practices both for individual faculty members and for departments as a whole. &lt;br/&gt;&lt;br/&gt;The PI team is expanding a prototype database developed as part of a previous NSF grant (WIDER DUE-1256352) into a community forum and data explorer that allow instructors and researchers to easily upload, discuss, and compare their data in an intuitive, interactive, and informative way. The data explorer features an intuitive user interface inviting exploration and discovery, interactive one-click analysis tools, a scalable database, and robust data security. This system will be incorporated into the PER User's Guide, an NSF-funded (NSDL DUE-0840853, TUES DUE-1245490) project that provides online resources for physics faculty about research-based teaching methods and assessments.&lt;br/&gt;&lt;br/&gt;The intellectual merit of this project resides in the easily accessible user interface that is designed to enable faculty members to engage with their students' assessment data and the national dataset. Instructors can generate reports for themselves to interpret their own results in order to improve their teaching, to explain what they are doing to colleagues, and to include in teaching portfolios and promotion and tenure reports that demonstrate their teaching efficacy. When fully operational, the database is expected to include results from hundreds of colleges and universities.&lt;br/&gt;&lt;br/&gt;The Broader Impacts of the project lie in the access to an unprecedented amount of assessment data that opens the doors for physics education researchers and faculty to answer questions about students' learning that were previously inaccessible.</AbstractNarration>
<MinAmdLetterDate>09/13/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1347821</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Hsu</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William H Hsu</PI_FULL_NAME>
<EmailAddress>bhsu@ksu.edu</EmailAddress>
<PI_PHON>7852368247</PI_PHON>
<NSF_ID>000260913</NSF_ID>
<StartDate>09/13/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Eleanor</FirstName>
<LastName>Sayre</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eleanor C Sayre</PI_FULL_NAME>
<EmailAddress>esayre@ksu.edu</EmailAddress>
<PI_PHON>7585322124</PI_PHON>
<NSF_ID>000530688</NSF_ID>
<StartDate>09/13/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Eugene</FirstName>
<LastName>Vasserman</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eugene Y Vasserman</PI_FULL_NAME>
<EmailAddress>eyv@ksu.edu</EmailAddress>
<PI_PHON>7855327944</PI_PHON>
<NSF_ID>000584026</NSF_ID>
<StartDate>09/13/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Kansas State University</Name>
<CityName>Manhattan</CityName>
<ZipCode>665061100</ZipCode>
<PhoneNumber>7855326804</PhoneNumber>
<StreetAddress>2 FAIRCHILD HALL</StreetAddress>
<StreetAddress2><![CDATA[1601 VATTIER STREET]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Kansas</StateName>
<StateCode>KS</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>KS01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>929773554</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>KANSAS STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041146432</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Kansas State University]]></Name>
<CityName>Manhattan</CityName>
<StateCode>KS</StateCode>
<ZipCode>665061103</ZipCode>
<StreetAddress><![CDATA[2 Fairchild Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Kansas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>KS01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1133</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>1998</Code>
<Text>IUSE</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0413</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~422603</FUND_OBLG>
<FUND_OBLG>2014~18120</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-579523c4-fe86-2649-bbc0-60b0d79fe784"> <p dir="ltr"><span>Research-based assessments have had a major impact on physics education reform by providing a universal and convincing measure of student understanding that instructors can use to assess and improve the effectiveness of their teaching. Studies using these instruments consistently show that research-based teaching methods lead to dramatic improvements in students&rsquo; conceptual understanding of physics. For faculty who know about and use these assessments, the results often help them to recognize the deficiencies in their students&rsquo; learning and motivate them to improve their teaching. Before this project, one of these assessments, the Force Concept Inventory, a test of basic concepts of forces and acceleration, had been given to thousands of students throughout the world, but dozens of similar assessments in other areas of physics were not widely known or easily accessible. Our research showed that faculty wanted to use these assessments, but did not know about most of them and/or did not know where to find them. This project has created a repository, where over 70 research-based assessments are available for download by verified educators. Over 21,000 assessments have been downloaded from our site, including translations in 27 languages. We have created a downloadable implementation guide for each assessment, with detailed information about content, implementation, and analysis. We have summarized and compared the content of these assessments in an article called &ldquo;Research-based Assessment Instruments in Physics and Astronomy.&rdquo;</span></p> <br /> <p dir="ltr"><span>Our research also showed that even when faculty use these assessments, they often don&rsquo;t know how to interpret the results, and find the analysis of their results to be time-consuming. To address these issues, we created the Data Explorer, where faculty can get instant analysis and visualization of their assessments results and get comparisons to national averages. Over 125 faculty members have uploaded their data to the Data Explorer, resulting in over 900 data sets. Feedback from users suggests that it is helping faculty to think about their teaching in new ways. For example, one user wrote, &ldquo;I am embarrassed to say that until now I have only concentrated on individual students and on course averages. &nbsp;I have not divided my data by gender to compare the men and women, but because it's so easy here, I did it. &nbsp;And then I was embarrassed to see that I had a huge gender gap on the FCI and on the FMCE. I needed this! Now I will be humbled again and listen even more carefully to the women to figure out what I need to be doing.&rdquo;</span></p> <br /> <p dir="ltr"><span>In preparation for the next stage of the project, we have developed early prototypes for new features to support physics department chairs with departmental assessment. We conducted market research, consisting of interviews with 9 physics department chairs in which we showed them our prototypes and asked how they would use them. Chairs were extremely enthusiastic about the prototypes, and many said that their departments would pay for such features, suggesting a model for financial sustainability for the Data Explorer in the future.</span></p> </span></p><br> <p>            Last Modified: 11/27/2017<br>      Modified by: Eleanor&nbsp;C&nbsp;Sayre</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Research-based assessments have had a major impact on physics education reform by providing a universal and convincing measure of student understanding that instructors can use to assess and improve the effectiveness of their teaching. Studies using these instruments consistently show that research-based teaching methods lead to dramatic improvements in students? conceptual understanding of physics. For faculty who know about and use these assessments, the results often help them to recognize the deficiencies in their students? learning and motivate them to improve their teaching. Before this project, one of these assessments, the Force Concept Inventory, a test of basic concepts of forces and acceleration, had been given to thousands of students throughout the world, but dozens of similar assessments in other areas of physics were not widely known or easily accessible. Our research showed that faculty wanted to use these assessments, but did not know about most of them and/or did not know where to find them. This project has created a repository, where over 70 research-based assessments are available for download by verified educators. Over 21,000 assessments have been downloaded from our site, including translations in 27 languages. We have created a downloadable implementation guide for each assessment, with detailed information about content, implementation, and analysis. We have summarized and compared the content of these assessments in an article called "Research-based Assessment Instruments in Physics and Astronomy."   Our research also showed that even when faculty use these assessments, they often don?t know how to interpret the results, and find the analysis of their results to be time-consuming. To address these issues, we created the Data Explorer, where faculty can get instant analysis and visualization of their assessments results and get comparisons to national averages. Over 125 faculty members have uploaded their data to the Data Explorer, resulting in over 900 data sets. Feedback from users suggests that it is helping faculty to think about their teaching in new ways. For example, one user wrote, "I am embarrassed to say that until now I have only concentrated on individual students and on course averages.  I have not divided my data by gender to compare the men and women, but because it's so easy here, I did it.  And then I was embarrassed to see that I had a huge gender gap on the FCI and on the FMCE. I needed this! Now I will be humbled again and listen even more carefully to the women to figure out what I need to be doing."   In preparation for the next stage of the project, we have developed early prototypes for new features to support physics department chairs with departmental assessment. We conducted market research, consisting of interviews with 9 physics department chairs in which we showed them our prototypes and asked how they would use them. Chairs were extremely enthusiastic about the prototypes, and many said that their departments would pay for such features, suggesting a model for financial sustainability for the Data Explorer in the future.        Last Modified: 11/27/2017       Submitted by: Eleanor C Sayre]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
