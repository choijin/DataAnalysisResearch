<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Viewpoint Tracking via Acceleration Stabilized with Computer Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>75000.00</AwardTotalIntnAmount>
<AwardAmount>75000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project addresses a problem which has kept virtual reality from widespread use. Some 15 years ago high-capability graphics cards in PCs reduced the cost of computing for virtual environments from hundreds-of-thousands of dollars to (today) hundreds of dollars. Low-cost head-mounted displays have just appeared. The similar advance in viewpoint tracking has not occurred; accurate, low-latency wide-area viewpoint tracking remains very costly. Virtual reality demands stereo 60 frames per second per eye and system latencies below 50 ms. This research is developing a novel system to provide accurate, low-latency viewpoint tracking to meet these requirements with consumer-cost components. The research is based upon a recently demonstrated proof-of-concept system. A standard RGB-Depth camera sits on the user's head. Pose is calculated by matching images against an environment model. A Kalman filter integrates rotational velocity and linear acceleration from a cheap high-speed inertial measurement unit (IMU) to update the pose estimate many times between frames. This not only gives low-latency pose readings, it also improves initial values for the next camera calculation. The depth images and reconstruction software are concurrently used to incrementally build/update the depth model of the environment for the camera matching. &lt;br/&gt;&lt;br/&gt;The current research is demonstrating the system's potential. To work completely successfully, both conceptual and algorithmic advances are in process. IMU calibrations are being improved. Temperature and dynamic bias must be compensated in the calibration to improve stimation and reduce jitter. Using multiple cameras to reduce overall noise and handle difficult cases (such as blank walls) are being addressed with new algorithms and evaluated. The merging of new and modeled data is computationally expensive. The feasibility demo uses two GPUs, one for rendering; one for tracking. Ways are being invented to do it on one.  Additional future research includes tracking dynamic objects and incorporating object recognition (e.g., such as a desk or chairs) to improve estimates. Widespread access to virtual reality may well open new, unexpected creative uses of the technology. The research is inventing the proof-of-concept system forward to one that can make this exciting leap.</AbstractNarration>
<MinAmdLetterDate>08/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/06/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1347208</AwardID>
<Investigator>
<FirstName>Frederick</FirstName>
<LastName>Brooks, Jr</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>Frederick P Brooks, Jr</PI_FULL_NAME>
<EmailAddress>brooks@cs.unc.edu</EmailAddress>
<PI_PHON>9199621931</PI_PHON>
<NSF_ID>000254881</NSF_ID>
<StartDate>08/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275993175</ZipCode>
<StreetAddress><![CDATA[201 S Columbia St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~75000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Immersive virtual reality was proposed by Ivan Sutherland in 1965, and a prototype demonstrated by him in 1968. &nbsp;Over the years, there's been lots of hype, &nbsp;lots of progress, and substantial corporate and military use. &nbsp;Until recently VR systems have been too costly to get much public use.</p> <p>Three things are required for a head-mounted immersive VR system: &nbsp;a binocular head-mounted visual and audio display system, a graphic computer capable of generating complex images at ~30 frames a second with end-to-end system latency of ~50 msec, and a fast system for tracking the position and pose of the two eyes. &nbsp;New head-mounted display systems such as the Oculus Rift have brought high performance and consumer prices. &nbsp;Graphics add-on cards for personal computers have brought fast performance and consumer prices for &nbsp;computational image generation.</p> <p>There has not heretofore been a consumer-price-level solution to the eye and limb tracking problem. &nbsp;This research investigated a two-part solution which used consumer-priced accelerometers and gyroscopes to measure linear and angular accelerations and velocities. &nbsp;These enable dead-reckoning navigation, such as that used to navigate the <em>U.S.S Nautilus </em>under the polar ice to the North Pole. &nbsp;</p> <p>Such navigation is subject to cumulative drift. &nbsp;For a system using consumer-grade sensors, this is a serious problem. &nbsp;This research undertook to harness computer-vision techniques to generate, over hundreds of milliseconds, high accuracy key frames which would provide benchmarks for the correction of drift.</p> <p>A key concept was that the system would as it ran bootstrap up the model of the visible surround from which the key frames would be generated.</p> <p>A running prototype of such a system, without model bootstrapping, was built and a conference paper published. &nbsp;The student doing this work for his Ph.D. thesis decided to join a company interested in doing very similar work. &nbsp;He therefore took his M.S. degree and joined the company full time, thus terminating this grant work.</p><br> <p>            Last Modified: 11/19/2014<br>      Modified by: Frederick&nbsp;P&nbsp;Brooks, Jr</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Immersive virtual reality was proposed by Ivan Sutherland in 1965, and a prototype demonstrated by him in 1968.  Over the years, there's been lots of hype,  lots of progress, and substantial corporate and military use.  Until recently VR systems have been too costly to get much public use.  Three things are required for a head-mounted immersive VR system:  a binocular head-mounted visual and audio display system, a graphic computer capable of generating complex images at ~30 frames a second with end-to-end system latency of ~50 msec, and a fast system for tracking the position and pose of the two eyes.  New head-mounted display systems such as the Oculus Rift have brought high performance and consumer prices.  Graphics add-on cards for personal computers have brought fast performance and consumer prices for  computational image generation.  There has not heretofore been a consumer-price-level solution to the eye and limb tracking problem.  This research investigated a two-part solution which used consumer-priced accelerometers and gyroscopes to measure linear and angular accelerations and velocities.  These enable dead-reckoning navigation, such as that used to navigate the U.S.S Nautilus under the polar ice to the North Pole.    Such navigation is subject to cumulative drift.  For a system using consumer-grade sensors, this is a serious problem.  This research undertook to harness computer-vision techniques to generate, over hundreds of milliseconds, high accuracy key frames which would provide benchmarks for the correction of drift.  A key concept was that the system would as it ran bootstrap up the model of the visible surround from which the key frames would be generated.  A running prototype of such a system, without model bootstrapping, was built and a conference paper published.  The student doing this work for his Ph.D. thesis decided to join a company interested in doing very similar work.  He therefore took his M.S. degree and joined the company full time, thus terminating this grant work.       Last Modified: 11/19/2014       Submitted by: Frederick P Brooks, Jr]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
