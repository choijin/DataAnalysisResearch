<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: AF: Medium: Learning and Matrix Reconstruction with the Max-Norm and Related Factorization Norms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2013</AwardEffectiveDate>
<AwardExpirationDate>03/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>899986.00</AwardTotalIntnAmount>
<AwardAmount>915986</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Matrix learning is fundamental in many learning problems.  These include problems that can be directly formulated as learning some unknown matrix, as well as a broader class of learning problems involving a matrix of parameters.  The most direct matrix learning problem is matrix completion, completing unseen entries in a partially observed matrix. Matrix completion has recently received much attention both in practice in collaborative filtering (notably through the Netflix challenge), and theoretical analysis as an extension to compressed sensing.  Matrix learning has also been used for clustering, transfer and multi-task learning, and similarity learning.&lt;br/&gt;&lt;br/&gt;The dominant approach to matrix learning in recent years, especially in the context of matrix completion, has used the matrix trace norm (developed in part by the PI on this award).  Indeed, trace norm-based methods enjoy much success in a variety of applications.  This project develops and studies alternative matrix norms to the trace-norm, most importantly the promising max-norm.&lt;br/&gt;&lt;br/&gt;Learning with the max-norm was initially presented in 2004 (along with the trace norm), but has not received the same attention, despite many theoretical and empirical advantages.  This project identifies domains where the max-norm and related norms can be beneficial, develops computational methods for using these norms, and promotes the adoption of these norms.  A central aim is to develop optimization methods for max-norm regularized problems that are as efficient as the corresponding methods for trace-norm regularized problems, such as singular value thresholding and LR-type methods.  Beyond matrix completion, the project applies the max-norm both to problems where the trace-norm has been previously applied, and in novel settings.  Novel applications include clustering, binary hashing, crowdsourcing, modeling rankings by a population, and similarity learning.&lt;br/&gt; &lt;br/&gt;Research under this project links the machine learning and theory-of-computation research communities (where SDP relaxations essentially corresponding to the max-norm have played a significant role in recent years).  The project forms bridges between the communities, enabled in part by cross-disciplinary tutorials.  Through collaboration with sociologists the PIs reach out to the social sciences, and increase the broad impact of the work by presenting it in an approachable and useable way to this audience.</AbstractNarration>
<MinAmdLetterDate>06/21/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/30/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302662</AwardID>
<Investigator>
<FirstName>Nathan</FirstName>
<LastName>Srebro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nathan Srebro</PI_FULL_NAME>
<EmailAddress>nati@ttic.edu</EmailAddress>
<PI_PHON>7738347493</PI_PHON>
<NSF_ID>000489181</NSF_ID>
<StartDate>06/21/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yury</FirstName>
<LastName>Makarychev</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yury Makarychev</PI_FULL_NAME>
<EmailAddress>yury@ttic.edu</EmailAddress>
<PI_PHON>7738342515</PI_PHON>
<NSF_ID>000597155</NSF_ID>
<StartDate>06/21/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Toyota Technological Institute at Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372803</ZipCode>
<PhoneNumber>7738340409</PhoneNumber>
<StreetAddress>6045 S Kenwood Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>127228927</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Toyota Technological Institute at Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606372902</ZipCode>
<StreetAddress><![CDATA[6045 S Kenwood Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~400000</FUND_OBLG>
<FUND_OBLG>2014~196549</FUND_OBLG>
<FUND_OBLG>2015~319437</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The award supported research on the use of advanced matrixfactorization techniques for machine learning and data analysis, aswell as foundational research on deep learning, viewing matrixfactorization as a model for understanding deep learning.&nbsp; Researchunder the award involved both improved theoretical understanding, andthe use of this theoretical understanding to obtain improvedtechniques and methods.&nbsp;&nbsp;</p> <p><br />In the first few years of the award research focused on improved SDPrelaxation techniques for matrix and cut problems, and borrowingtheoretical ideas to improve methods used in practice.&nbsp; In particular,influenced by our theoretical understanding of matrix problems, wesuggest the use of asymmetric, as opposed to symmetric, binary hashesfor approximate search and retrieval problems over large data sets,and demonstrated how our techniques greatly improve in terms of speed,memory footprint and accuracy, over prior approaches, boththeoretically and empirically.&nbsp;&nbsp;</p> <p>In later years of the award, with the growing importance of deeplearning, research pivoted toward the use of matrix factorization as amodel for many of the phenomena encountered in deep learning.Influenced and guided by our understanding of matrix factorizationproblems, we embarked on an analogous study of deep learning models.Also here our improved theoretical understanding lead tomethodological improvements: we used a norm developed as part of ourstudy of generalization properties of deep models, the Path-Norm, toguide development of an improved optimization method, Path-SGD, whichwe demonstrated leads to faster training and more accurate models.&nbsp;&nbsp;</p> <p>In the final years of the project, we studied the role of the specificchoice of optimization algorithm in providing "implicitregularization" for deep learning, which we argued plays a centralrole in ensuring generalization.&nbsp; We again used matrix factorizationas a guide, and first demonstrated this implicit regularization effectin an over-parametrized matrix factorization model.&nbsp; We then went onto study also other deep models, including convolutional deep models.Our research established the importance of implicit regularization inunderstanding deep learning, and has lead to numerous studies bymultiple research groups.</p> <p>With the rising importance of deep learning in multiple scientific andtechnological domains, it is becoming increasingly important andurgent to obtain a better foundations understanding of how and when itworks, and perhaps more important when it might fail.&nbsp; Ourwork on understanding deep learning provides a foundational step inthis direction, which can be used both to improve our trust in deeplearning, and to further improve deep learning methods.&nbsp; Such advancedwill have significant and broad impacts.</p> <p>The award also supported collaboration with social scientists, mostlythrough post-doctoral fellow Nandana Sengupta, and the publication ofseveral research papers and surveys aimed at social scientists.&nbsp; Inthese publications we discuss and demonstrate the use of advancedmatrix factorization techniques in surveys and data analysis in aneffort to better introduce them to a wider audience.</p> <p>The award supported training of seven PhD students, twoundergraduate students, and two post-docs.&nbsp;&nbsp;</p><br> <p>            Last Modified: 07/10/2019<br>      Modified by: Nathan&nbsp;Srebro</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The award supported research on the use of advanced matrixfactorization techniques for machine learning and data analysis, aswell as foundational research on deep learning, viewing matrixfactorization as a model for understanding deep learning.  Researchunder the award involved both improved theoretical understanding, andthe use of this theoretical understanding to obtain improvedtechniques and methods.     In the first few years of the award research focused on improved SDPrelaxation techniques for matrix and cut problems, and borrowingtheoretical ideas to improve methods used in practice.  In particular,influenced by our theoretical understanding of matrix problems, wesuggest the use of asymmetric, as opposed to symmetric, binary hashesfor approximate search and retrieval problems over large data sets,and demonstrated how our techniques greatly improve in terms of speed,memory footprint and accuracy, over prior approaches, boththeoretically and empirically.    In later years of the award, with the growing importance of deeplearning, research pivoted toward the use of matrix factorization as amodel for many of the phenomena encountered in deep learning.Influenced and guided by our understanding of matrix factorizationproblems, we embarked on an analogous study of deep learning models.Also here our improved theoretical understanding lead tomethodological improvements: we used a norm developed as part of ourstudy of generalization properties of deep models, the Path-Norm, toguide development of an improved optimization method, Path-SGD, whichwe demonstrated leads to faster training and more accurate models.    In the final years of the project, we studied the role of the specificchoice of optimization algorithm in providing "implicitregularization" for deep learning, which we argued plays a centralrole in ensuring generalization.  We again used matrix factorizationas a guide, and first demonstrated this implicit regularization effectin an over-parametrized matrix factorization model.  We then went onto study also other deep models, including convolutional deep models.Our research established the importance of implicit regularization inunderstanding deep learning, and has lead to numerous studies bymultiple research groups.  With the rising importance of deep learning in multiple scientific andtechnological domains, it is becoming increasingly important andurgent to obtain a better foundations understanding of how and when itworks, and perhaps more important when it might fail.  Ourwork on understanding deep learning provides a foundational step inthis direction, which can be used both to improve our trust in deeplearning, and to further improve deep learning methods.  Such advancedwill have significant and broad impacts.  The award also supported collaboration with social scientists, mostlythrough post-doctoral fellow Nandana Sengupta, and the publication ofseveral research papers and surveys aimed at social scientists.  Inthese publications we discuss and demonstrate the use of advancedmatrix factorization techniques in surveys and data analysis in aneffort to better introduce them to a wider audience.  The award supported training of seven PhD students, twoundergraduate students, and two post-docs.         Last Modified: 07/10/2019       Submitted by: Nathan Srebro]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
