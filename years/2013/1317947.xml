<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Small: Collaborative Research: Active Sensing for Robotic Cameramen</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>433711.00</AwardTotalIntnAmount>
<AwardAmount>433711</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With advances in camera technologies, and as cloud storage, network bandwidth and protocols become available, visual media are becoming ubiquitous. Video recording became de facto universal means of instruction for a wide range of applications such as physical exercise, technology, assembly, or cooking. This project addresses the scientific and technological challenges of video shooting in terms of coverage and optimal views planning while leaving high level aspects including creativity to the video editing and post-production stages. &lt;br/&gt;&lt;br/&gt;Camera placement and novel view selection challenges are modeled as optimization problems that minimize the uncertainty in the location of actors and objects, maximize coverage and effective appearance resolution, and optimize object detection for the sake of semantic annotation of the scene. New probabilistic models capture long range correlations when the trajectories of actors are only partially observable.  Quality of potential novel views is modeled in terms of resolution that is optimized by maximizing the coverage of a 3D orientation histogram while an active view selection process for object detection minimizes a dynamic programming objective function capturing the loss due to classification error as well as the resources spent for each view.&lt;br/&gt;&lt;br/&gt;The project advances active sensing and perception and provides the technology for further automation on video capturing. Such technology has broader impact on the production of education videos for online courses as well as in telepresence applications. Research results are integrated into robotics and digital media programs addressing K-12 students.</AbstractNarration>
<MinAmdLetterDate>09/12/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/12/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317947</AwardID>
<Investigator>
<FirstName>Sampath</FirstName>
<LastName>Kannan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sampath Kannan</PI_FULL_NAME>
<EmailAddress>kannan@central.cis.upenn.edu</EmailAddress>
<PI_PHON>2158989514</PI_PHON>
<NSF_ID>000194422</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~433711</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>When searching for an object in an image, traditional as well as modern approaches slide a filter window per object part over the entire image. Biological systems rather use an attentional scan path based on extracted saliency in the image. The main result of this project has been a novel approach for object detection that actvely selects where to spend computational resources.&nbsp;</p> <p>Object detection is regarded as a planning problem where the policy determines which object part filter should ne applied next and where. Such a policy should minimize the number of filter operations and simultaneously the expectation of the misclassification error. Finding such a policy can be reduced to a dynamic program.</p> <p>Applying this active selection strategy of where to apply part filters has resulted in acceleration between 5 and 30 times compared to the sliding window approach without sacrificing precision and recall.&nbsp;</p> <p>In a second application of active perception, a move selection policy has been studied in the context of absolute semantic localization. Given landmarks on a known map, a new control scheme has been devised that makes a robot move in such a way that the error in localization and the energy spent in the moves is minimized.&nbsp;</p><br> <p>            Last Modified: 02/01/2016<br>      Modified by: Kostas&nbsp;Daniilidis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1317947/1317947_10278888_1454309604599_exp_bottle--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1317947/1317947_10278888_1454309604599_exp_bottle--rgov-800width.jpg" title="Active Deformable Part Models"><img src="/por/images/Reports/POR/2016/1317947/1317947_10278888_1454309604599_exp_bottle--rgov-66x44.jpg" alt="Active Deformable Part Models"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The input image and the final probability of a bottle been images at a pixel location are shown at the top and bottom right, respectively. The rest of the top row shows the probability for a bottle to be at each pixel location. Bottom row shows which filter should be applied (one color per filter).</div> <div class="imageCredit">Menglong Zhu, Nikolay Atanasov, George Pappas, Kostas Daniilidis</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Kostas&nbsp;Daniilidis</div> <div class="imageTitle">Active Deformable Part Models</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ When searching for an object in an image, traditional as well as modern approaches slide a filter window per object part over the entire image. Biological systems rather use an attentional scan path based on extracted saliency in the image. The main result of this project has been a novel approach for object detection that actvely selects where to spend computational resources.   Object detection is regarded as a planning problem where the policy determines which object part filter should ne applied next and where. Such a policy should minimize the number of filter operations and simultaneously the expectation of the misclassification error. Finding such a policy can be reduced to a dynamic program.  Applying this active selection strategy of where to apply part filters has resulted in acceleration between 5 and 30 times compared to the sliding window approach without sacrificing precision and recall.   In a second application of active perception, a move selection policy has been studied in the context of absolute semantic localization. Given landmarks on a known map, a new control scheme has been devised that makes a robot move in such a way that the error in localization and the energy spent in the moves is minimized.        Last Modified: 02/01/2016       Submitted by: Kostas Daniilidis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
