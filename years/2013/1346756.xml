<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Memory Models: Specification and Verification in a Concurrency Intermediate Verification Language (CIVL) Framework</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nina Amla</SignBlockName>
<PO_EMAI>namla@nsf.gov</PO_EMAI>
<PO_PHON>7032927991</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The nation's economic progress and national security are critically dependent on maintaining a trajectory of steady advances in computing. Such advances are crucially dependent on the use of multiple processing units that are programmed using parallel programming languages. As these parallel processing systems find uses in critical applications such as national security infrastructures, medical devices, airplanes, and computing installations that predict our weather, they must be highly reliable as well as energy efficient. Unfortunately, today's multi-processors are extremely difficult to reliably employ and to efficiently program using current parallel programming languages. In addition to the generally recognized difficulty of writing parallel programs, one of the central unresolved difficulties is the development of a clearly defined shared memory semantics that allows sufficient parallelism.  This semantics dictates how the computing elements exchange data, as well as how compilers can safely optimize parallel programs.&lt;br/&gt;&lt;br/&gt;This work primarily focuses on addressing critical problems relating to concurrent shared memory interactions in parallel programs. It helps advance the current state of the art by developing a collection of mathematical models for clearly defining these interactions. These mathematical models will form the bedrock for developing parallel processors as well as compilers that reliably translate user intentions into correctly functioning computing systems. A central emphasis of our work is that it uniformly addresses the multiplicity of parallel processing element types as well as computer languages by erecting these mathematical models based on a Concurrency Intermediate Verification Language. An equally important feature of this work is that this understanding of memory consistency models directly translates into rigorous error-checking tools to avoid egregious mistakes in deployed computer software. A key aspect of this project is the development of such error-checking tools for parallel programs and demonstrating the effectiveness of these tools on realistic programs acquired from national labs and industrial partners.</AbstractNarration>
<MinAmdLetterDate>07/24/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1346756</AwardID>
<Investigator>
<FirstName>Ganesh</FirstName>
<LastName>Gopalakrishnan</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ganesh L Gopalakrishnan</PI_FULL_NAME>
<EmailAddress>ganesh@cs.utah.edu</EmailAddress>
<PI_PHON>8015813568</PI_PHON>
<NSF_ID>000160895</NSF_ID>
<StartDate>07/24/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Zvonimir</FirstName>
<LastName>Rakamaric</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zvonimir Rakamaric</PI_FULL_NAME>
<EmailAddress>zvonimir@cs.utah.edu</EmailAddress>
<PI_PHON>8015816903</PI_PHON>
<NSF_ID>000623290</NSF_ID>
<StartDate>07/24/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName/>
<StateCode>UT</StateCode>
<ZipCode>841129205</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Advances in computing crucially dependent on the use of multiple processing units that are programmed using parallel programming languages. As these parallel processing systems find uses in critical applications such as national security infrastructures, medical devices, airplanes, and computing installations that predict our weather, they must be highly reliable as well as energy efficient. Unfortunately, today&rsquo;s parallel programming languages as well as multi-processors are extremely difficult to reliably employ, as well as efficiently program. In addition to the generally recognized difficulty of writing parallel programs, one of the central unresolved difficulties is that of a clearly defined and well specified shared memory semantics, which dictates how the computing elements exchange data, as well as how compilers can safely optimize parallel programs.</p> <p>The main contribution of this project was exploring topics related to the analysis and verification of parallel programs while taking the semantics of shared memory into account. We devised several main contributions. First, we developed a formal model for a common and widely used parallel programming paradigm called pthreads. We implemented this model as an extension of our software verifier called SMACK. Using this extension, we were able to verify numerous real-world parallel programs, and in particular we focused on Linux device drivers. In the process, our tool automatically discovered several bugs related to parallelism and shared memory. Also, we competed twice in the annual software verification competition, and won several medals both times.</p> <p>As our second main thrust, we carefully studied how shared memory operates in graphics processing units (GPUs). GPUs achieve higher computational efficiency than CPUs&nbsp;by employing simpler cores, and hide memory latency by switching away from&nbsp;stalled threads. GPUs also follow a weak memory consistency model, and as such are very hard to program correctly. While CPUs also have such weak memory rules, &nbsp;the rules followed by GPUs are much less&nbsp;well understood. As a part of this project, we subjected a number of different GPU architecture to targeted stress-testing in order to observe potentially erroneous memory behaviors. In the process, we have shown that GPU code can in fact end up fetching&nbsp;stale data, which was not what programmers would expect.</p> <p>Building on top of the lessons learned from our second thrust, we realized that despite the growing popularity of GPU parallel programming, there is a lack of portable synchronization primitives that are needed to perform certain kinds of parallel computations. Hence, as our final main contribution, we developed such a portable synchronization primitive. We empirically and formally validated its correctness under weak memory models as implemented by various GPU architectures. Since then, we have been working on trying to incorporate our primitive into the next GPU programming language standard, in order to make it available to a multitude of GPU programmers.</p> <p>Finally, we explored improving the core search algorithm of the CIVL verification engine. We implemented several search strategies and we empirically evaluated them on a number of CIVL benchmarks. Our prototype implementation showed promising results, and we hope that it will be incorporated into the main CIVL engine.</p> <p>We disseminated our research results by publishing around ten research papers and reports. We also implemented tools and extensions stemming from this research, and we disseminated those to the academic community as well as practitioners in national labs and industry. We made all of the developed tools publicly available and open-source projects. As a part of this project, we also trained a generation of students and post-doctoral researchers in the areas of parallel programming and shared memory semantics where the current shortage of such manpower is crippling national productivity as well as innovation.</p><br> <p>            Last Modified: 10/26/2016<br>      Modified by: Zvonimir&nbsp;Rakamaric</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Advances in computing crucially dependent on the use of multiple processing units that are programmed using parallel programming languages. As these parallel processing systems find uses in critical applications such as national security infrastructures, medical devices, airplanes, and computing installations that predict our weather, they must be highly reliable as well as energy efficient. Unfortunately, today?s parallel programming languages as well as multi-processors are extremely difficult to reliably employ, as well as efficiently program. In addition to the generally recognized difficulty of writing parallel programs, one of the central unresolved difficulties is that of a clearly defined and well specified shared memory semantics, which dictates how the computing elements exchange data, as well as how compilers can safely optimize parallel programs.  The main contribution of this project was exploring topics related to the analysis and verification of parallel programs while taking the semantics of shared memory into account. We devised several main contributions. First, we developed a formal model for a common and widely used parallel programming paradigm called pthreads. We implemented this model as an extension of our software verifier called SMACK. Using this extension, we were able to verify numerous real-world parallel programs, and in particular we focused on Linux device drivers. In the process, our tool automatically discovered several bugs related to parallelism and shared memory. Also, we competed twice in the annual software verification competition, and won several medals both times.  As our second main thrust, we carefully studied how shared memory operates in graphics processing units (GPUs). GPUs achieve higher computational efficiency than CPUs by employing simpler cores, and hide memory latency by switching away from stalled threads. GPUs also follow a weak memory consistency model, and as such are very hard to program correctly. While CPUs also have such weak memory rules,  the rules followed by GPUs are much less well understood. As a part of this project, we subjected a number of different GPU architecture to targeted stress-testing in order to observe potentially erroneous memory behaviors. In the process, we have shown that GPU code can in fact end up fetching stale data, which was not what programmers would expect.  Building on top of the lessons learned from our second thrust, we realized that despite the growing popularity of GPU parallel programming, there is a lack of portable synchronization primitives that are needed to perform certain kinds of parallel computations. Hence, as our final main contribution, we developed such a portable synchronization primitive. We empirically and formally validated its correctness under weak memory models as implemented by various GPU architectures. Since then, we have been working on trying to incorporate our primitive into the next GPU programming language standard, in order to make it available to a multitude of GPU programmers.  Finally, we explored improving the core search algorithm of the CIVL verification engine. We implemented several search strategies and we empirically evaluated them on a number of CIVL benchmarks. Our prototype implementation showed promising results, and we hope that it will be incorporated into the main CIVL engine.  We disseminated our research results by publishing around ten research papers and reports. We also implemented tools and extensions stemming from this research, and we disseminated those to the academic community as well as practitioners in national labs and industry. We made all of the developed tools publicly available and open-source projects. As a part of this project, we also trained a generation of students and post-doctoral researchers in the areas of parallel programming and shared memory semantics where the current shortage of such manpower is crippling national productivity as well as innovation.       Last Modified: 10/26/2016       Submitted by: Zvonimir Rakamaric]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
