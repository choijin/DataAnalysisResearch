<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Large: Collaborative Research: Complementary Situational Awareness for Human-Robot Partnerships</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>1260000.00</AwardTotalIntnAmount>
<AwardAmount>1279600</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This work will advance human-robot partnerships by establishing a new concept called complementary situational awareness (CSA), which is the simultaneous perception and use of the environment and operational constraints for task execution. The proposed CSA is transformative because it ushers in a new era of human-robot partnerships where robots act as our partners, not only in manipulation, but in perception and control. This research will establish the foundations for CSA to enable multifaceted human-robot partnerships. Three main research objectives guide this effort: 1) Real-time Sensing during Task Execution: design low-level control algorithms providing wire-actuated or flexible continuum robots with sensory awareness by supporting force sensing, exploration, and modulated force interaction in flexible unstructured environments; 2) Situational Awareness Modeling: prescribe information fusion and simultaneous localization and mapping (SLAM) algorithms suitable for surgical planning and in-vivo surgical plan adaptation; 3) Telemanipulation based on CSA: Design, construct, and integrate robotic testbeds with telemanipulation algorithms that use SLAM and exploration data for online adaptation of assistive telemanipulation virtual fixtures. This research also includes investigation of previously unaddressed questions on how sensory exploration and palpation data can be used to enable online-adaptation of assistive virtual fixtures based on force and stiffness data while also taking into account preoperative data and intraoperative correction of registration parameters.&lt;br/&gt;&lt;br/&gt;The proposed work will restore the situational awareness readily available in open surgery to minimally invasive surgery. This will benefit patients by enabling core technologies for effective and safe natural orifice surgery or single port access surgery. The societal impact of the proposed work on these two surgical paradigms is reduced pain for patients, shorter hospital stay, improved cosmesis and patients' self image, and lower costs. We also believe that CSA will impact manufacturing where its future will require people and robots working together in a shared space on collaborative tasks. Also, the same concepts of CSA apply to telemanipulation in constrained and unstructured environments and the proposed research has direct relevance to robot-human partnerships for space exploration. To ensure this broader impact will be achieved, an advisory board has been assembled with experts from medicine, manufacturing and aerospace. Finally, the PIs will facilitate collaboration in the medical robotics research community by making our software and hardware designs available on-line and using commercial-grade hardware available at multiple institutions.</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/05/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1327566</AwardID>
<Investigator>
<FirstName>Nabil</FirstName>
<LastName>Simaan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nabil Simaan</PI_FULL_NAME>
<EmailAddress>nabil.simaan@vanderbilt.edu</EmailAddress>
<PI_PHON>6153430470</PI_PHON>
<NSF_ID>000233602</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Vanderbilt University</Name>
<CityName>Nashville</CityName>
<ZipCode>372350002</ZipCode>
<PhoneNumber>6153222631</PhoneNumber>
<StreetAddress>Sponsored Programs Administratio</StreetAddress>
<StreetAddress2><![CDATA[PMB 407749 2301 Vanderbilt Place]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>965717143</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VANDERBILT UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004413456</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Vanderbilt University]]></Name>
<CityName>Nashville</CityName>
<StateCode>TN</StateCode>
<ZipCode>372350002</ZipCode>
<StreetAddress><![CDATA[2301 Vanderbilt PL]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~377894</FUND_OBLG>
<FUND_OBLG>2014~800552</FUND_OBLG>
<FUND_OBLG>2015~93154</FUND_OBLG>
<FUND_OBLG>2016~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>When operators telemanipulate robotic devices, they are hampered by perception barriers challenging their situational awareness. A surgeon telemanipulating a surgical robot is limited in their understanding of the surgical scene and of the robot's interaction with the anatomy. The robotics research community has dealt with these challenges by focusing on ways of providing force feedback to the surgeons and by providing assistive control laws (called virtual fixtures) that superimpose a safety barrier or help the surgeon follow a desired path while avoiding critical anatomy. These solutions are limited by the reliance on medical image registration (a process by which pre-operative anatomy images are related to the intraoperative scene). Moreover, these solutions relied predominantly on&nbsp;pre-operative geometric definitions of a surgical plan to construct the assistive virtual fixtures.</p> <p>Figure 1 shows the approach followed in this collaborative research along with some scenarios where the surgical perception is lacking. According to our approach, the robot is used for manipulation augmentation and for perception augmentation by fusing &nbsp;intraoperative sensory data and imaging (e.g. tissue stiffness, computer vision) with preoperative models and images of the anatomy. &nbsp;Figures 1b and 1c show scenarios where such perception and situational awareness augmentation would be critical for safe operation. In both images, the robot can have multiple contacts outside the surgeon's visual field of view and there is a need for a robot that can discern such contacts and a high-level controller that uses this information to adapt the telemanipulation behaviors to enable safe operation. &nbsp;</p> <p>The concept of robot situational awareness was investigated as part of a paradigm in which intraoperative sensory information was used to inform the update of a surgical plan and its corresponding virtual fixtures. In addition to using geometry, the use of force-controlled palpation and exploration of the anatomy has been explored and demonstrated to allow adaptive surgical plans. Figure 2 shows a robot using force-controlled scan of a mock organ to account for organ deformation relative to a pre-operative model of the organ. Using such intraoperative information, advanced statistical methods were used to take advantage of intraoperative sensing and preoperative information to improve the computer?s model of the patient?s anatomy and the surgical plan. Figure 3 shows a result of force-controlled exploration where an organ stiffness map is generated and used with geometry to inform the process of updating the model of the anatomy. Figure 4 shows steps in an efficient method for real-time stiffness mapping during telemanipulation of the robot. The method allows an interactive rate annotation of the anatomy model with stiffness information which could be used and in figure 3 for identifying possible locations of tumors or for identifying a hidden artery as in figure 4. &nbsp;</p> <p><span>These tools enabled a rigorous exploration of new hybrid assistive telemanipulation frameworks that allow the robot high-level controller to specify behaviors where the robot controls motion or regulates force while allowing the user to telemanipulate the robot tip for remote palpation. Figure 5 shows a subset of these conditions. Automated and semi-automated telemanipulation with superimposed end-effector excitations have been developed to allow the high level controller to discern information tantamount to that obtained during palpation. Ways of relaying this information to users have also been explored through a user study with the aim of determining the potential benefits of these approaches.&nbsp;</span></p> <p><span>We also focused on sensing using continuum robots to assist with surgical perception. To achieve this, a new approach to model force and motion transmission losses. It was shown that the high level controller can use these modeling techniques to allow assistive behaviors of palpation to support force-controlled virtual fixture model update and to enable regulation of force while carrying out tasks of ablation and knot tying. Figure 6 shows one of our robotic platforms used to test our new sensing and control approaches. The figure shows a new approach for hybrid force/motion control using estimation of tip forces and two sample use scenarios (force regulated knot tying and ablation). </span></p> <p>These contributions will facilitate future development of human-robot cooperative systems with applications for surgery, space robotics, search and rescue and potentially robot-worker collaboration in manufacturing.</p> <p>Other broader impacts of this award supported the training of two post-docs, eight Ph.D. students, and 6 undergraduate students. Thirty-nine archival publications were presented in national and international conferences and journals. Four Ph.D. dissertations were published. In addition, 37 high school female students received STEM and robotics training in three winter classes with each class spanning three weeks. Seven Ph.D.s and two postdocs trained on this program have joined industry research groups in medical robotics and research in human-robot collaboration. One of the Ph.D. students trained on this award started a tenure-track faculty position in the U.S..&nbsp;</p> <p>The project public page is http://nri-csa.vuse.vanderbilt.edu/joomla/ where there are also public data sets and computer code related to this project.</p><br> <p>            Last Modified: 12/22/2020<br>      Modified by: Nabil&nbsp;Simaan</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487795041_Figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487795041_Figure1--rgov-800width.jpg" title="Figure 1: Framework for telemanipulation with situational awareness"><img src="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487795041_Figure1--rgov-66x44.jpg" alt="Figure 1: Framework for telemanipulation with situational awareness"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 1: (a) A robotic surgery system used for manipulation and perception augmentation (b-c) a robot accessing deep into the anatomy can have contacts with the anatomy that may not be perceptible of visible to the surgeon.</div> <div class="imageCredit">Nabil Simaan, Russell H. Taylor, Howie Choset</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Nabil&nbsp;Simaan</div> <div class="imageTitle">Figure 1: Framework for telemanipulation with situational awareness</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487847149_Figure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487847149_Figure2--rgov-800width.jpg" title="Figure 2: robot palpation"><img src="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487847149_Figure2--rgov-66x44.jpg" alt="Figure 2: robot palpation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A robot using its tip to palpate tissue during telemanipulation. Information from force-controlled exploration of the organ surface is used to relate the geometry of the mock organ to its pre-operative imaging state through a process called deformable registration.</div> <div class="imageCredit">Nabil Simaan, Russell H. Taylor, Howie Choset</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Nabil&nbsp;Simaan</div> <div class="imageTitle">Figure 2: robot palpation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487932465_Figure3--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487932465_Figure3--rgov-800width.jpg" title="Figure 3: use of probing for registration and model update"><img src="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608487932465_Figure3--rgov-66x44.jpg" alt="Figure 3: use of probing for registration and model update"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 3: Use of intelligent probing for stiffness mapping and detection of mock tumors and hidden arteries. This information was shown to allow help with the process of updating the model of the anatomy (organ model registration and annotation)</div> <div class="imageCredit">Nabil Simaan, Russell H. Taylor, Howie Choset</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Nabil&nbsp;Simaan</div> <div class="imageTitle">Figure 3: use of probing for registration and model update</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488035898_Figure4--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488035898_Figure4--rgov-800width.jpg" title="Figure 4: stiffness mapping through palpation"><img src="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488035898_Figure4--rgov-66x44.jpg" alt="Figure 4: stiffness mapping through palpation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 4: An efficient process for interactive-rate use of palpation information to annotate the organ model with stiffness information.</div> <div class="imageCredit">nabil simaan</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Nabil&nbsp;Simaan</div> <div class="imageTitle">Figure 4: stiffness mapping through palpation</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488088691_Figure5--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488088691_Figure5--rgov-800width.jpg" title="Figure 5: user study"><img src="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488088691_Figure5--rgov-66x44.jpg" alt="Figure 5: user study"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 5: A subset of the assistive telemanipulation conditions explored through a user study: (a) unaided telemanipulation, (b) aided telemanipulation where the robot applies a lateral virtual fixture and overlays normal force information. Other conditions were explored where the robot automaticall</div> <div class="imageCredit">Nabil Simaan, Russell H. Taylor, Howie Choset</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Nabil&nbsp;Simaan</div> <div class="imageTitle">Figure 5: user study</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488160426_Figure6--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488160426_Figure6--rgov-800width.jpg" title="Figure 6: continuum robot with sensing and force regulation"><img src="/por/images/Reports/POR/2020/1327566/1327566_10277139_1608488160426_Figure6--rgov-66x44.jpg" alt="Figure 6: continuum robot with sensing and force regulation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Figure 6: (a) A continuum robot system with tip force estimation capabilities, (b) a new framework for hybrid force-motion control of continuum robots using force tip estimation, (c) The robot regulating the knot tightening force, (d) the robot regulating the normal force during a mock ablation task</div> <div class="imageCredit">Nabil Simaan, Russell H. Taylor, Howie Choset</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Nabil&nbsp;Simaan</div> <div class="imageTitle">Figure 6: continuum robot with sensing and force regulation</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ When operators telemanipulate robotic devices, they are hampered by perception barriers challenging their situational awareness. A surgeon telemanipulating a surgical robot is limited in their understanding of the surgical scene and of the robot's interaction with the anatomy. The robotics research community has dealt with these challenges by focusing on ways of providing force feedback to the surgeons and by providing assistive control laws (called virtual fixtures) that superimpose a safety barrier or help the surgeon follow a desired path while avoiding critical anatomy. These solutions are limited by the reliance on medical image registration (a process by which pre-operative anatomy images are related to the intraoperative scene). Moreover, these solutions relied predominantly on pre-operative geometric definitions of a surgical plan to construct the assistive virtual fixtures.  Figure 1 shows the approach followed in this collaborative research along with some scenarios where the surgical perception is lacking. According to our approach, the robot is used for manipulation augmentation and for perception augmentation by fusing  intraoperative sensory data and imaging (e.g. tissue stiffness, computer vision) with preoperative models and images of the anatomy.  Figures 1b and 1c show scenarios where such perception and situational awareness augmentation would be critical for safe operation. In both images, the robot can have multiple contacts outside the surgeon's visual field of view and there is a need for a robot that can discern such contacts and a high-level controller that uses this information to adapt the telemanipulation behaviors to enable safe operation.    The concept of robot situational awareness was investigated as part of a paradigm in which intraoperative sensory information was used to inform the update of a surgical plan and its corresponding virtual fixtures. In addition to using geometry, the use of force-controlled palpation and exploration of the anatomy has been explored and demonstrated to allow adaptive surgical plans. Figure 2 shows a robot using force-controlled scan of a mock organ to account for organ deformation relative to a pre-operative model of the organ. Using such intraoperative information, advanced statistical methods were used to take advantage of intraoperative sensing and preoperative information to improve the computer?s model of the patient?s anatomy and the surgical plan. Figure 3 shows a result of force-controlled exploration where an organ stiffness map is generated and used with geometry to inform the process of updating the model of the anatomy. Figure 4 shows steps in an efficient method for real-time stiffness mapping during telemanipulation of the robot. The method allows an interactive rate annotation of the anatomy model with stiffness information which could be used and in figure 3 for identifying possible locations of tumors or for identifying a hidden artery as in figure 4.    These tools enabled a rigorous exploration of new hybrid assistive telemanipulation frameworks that allow the robot high-level controller to specify behaviors where the robot controls motion or regulates force while allowing the user to telemanipulate the robot tip for remote palpation. Figure 5 shows a subset of these conditions. Automated and semi-automated telemanipulation with superimposed end-effector excitations have been developed to allow the high level controller to discern information tantamount to that obtained during palpation. Ways of relaying this information to users have also been explored through a user study with the aim of determining the potential benefits of these approaches.   We also focused on sensing using continuum robots to assist with surgical perception. To achieve this, a new approach to model force and motion transmission losses. It was shown that the high level controller can use these modeling techniques to allow assistive behaviors of palpation to support force-controlled virtual fixture model update and to enable regulation of force while carrying out tasks of ablation and knot tying. Figure 6 shows one of our robotic platforms used to test our new sensing and control approaches. The figure shows a new approach for hybrid force/motion control using estimation of tip forces and two sample use scenarios (force regulated knot tying and ablation).   These contributions will facilitate future development of human-robot cooperative systems with applications for surgery, space robotics, search and rescue and potentially robot-worker collaboration in manufacturing.  Other broader impacts of this award supported the training of two post-docs, eight Ph.D. students, and 6 undergraduate students. Thirty-nine archival publications were presented in national and international conferences and journals. Four Ph.D. dissertations were published. In addition, 37 high school female students received STEM and robotics training in three winter classes with each class spanning three weeks. Seven Ph.D.s and two postdocs trained on this program have joined industry research groups in medical robotics and research in human-robot collaboration. One of the Ph.D. students trained on this award started a tenure-track faculty position in the U.S..   The project public page is http://nri-csa.vuse.vanderbilt.edu/joomla/ where there are also public data sets and computer code related to this project.       Last Modified: 12/22/2020       Submitted by: Nabil Simaan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
