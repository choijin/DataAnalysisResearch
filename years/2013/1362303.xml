<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>A New Approach to Nonconvex Risk-Sensitive Stochastic Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>340000.00</AwardTotalIntnAmount>
<AwardAmount>340000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Georgia-Ann Klutke</SignBlockName>
<PO_EMAI>gaklutke@nsf.gov</PO_EMAI>
<PO_PHON>7032922443</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The research objective of this award is to develop a new framework for incorporating risk into sequential decision making under uncertainty.  The two pillars of the approach are cumulative prospect theory and dynamic risk measures.  The framework builds on both of these research streams to formulate a single theory that integrates subjective preferences in human behavior with normative decision-making objectives.  Existing utility-based dynamic models cannot handle the nonconvexity implied by the behavioral models of prospect theory, whereas the framework allows the probability weighting found in cumulative prospect theory to be combined with the usual outcome weighting of traditional expected utility formulations in a sequential decision-making model that incorporates both types of risk sensitivity. The framework will be used to develop efficient dynamic programming sampling and simulation-based methods for risk-sensitive optimization and control problems, and to investigate how the new modeling of risk-sensitivity affects the behavior of decision makers.&lt;br/&gt;&lt;br/&gt;If successful, the results of this research will provide an alternative framework for decision making under risk to currently existing approaches.  The framework unifies the predominantly descriptive research stream of prospect theory coming primarily from psychology and behavioral economics with the normative approaches generally associated with the microeconomics and operations research communities.  From this new approach arise a host of challenges, both theoretical and computational.  Algorithms will be developed that can be used to address practical operational and tactical decision-making problems arising in a wide variety of application areas, from manufacturing and supply chain management to service systems, including health care, transportation, and financial engineering.</AbstractNarration>
<MinAmdLetterDate>03/14/2014</MinAmdLetterDate>
<MaxAmdLetterDate>03/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1362303</AwardID>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Marcus</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven I Marcus</PI_FULL_NAME>
<EmailAddress>marcus@isr.umd.edu</EmailAddress>
<PI_PHON>3014057589</PI_PHON>
<NSF_ID>000367593</NSF_ID>
<StartDate>03/14/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Fu</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael C Fu</PI_FULL_NAME>
<EmailAddress>mfu@isr.umd.edu</EmailAddress>
<PI_PHON>3014052241</PI_PHON>
<NSF_ID>000280748</NSF_ID>
<StartDate>03/14/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1787</Code>
<Text>SERVICE ENTERPRISE SYSTEMS</Text>
</ProgramElement>
<ProgramElement>
<Code>5514</Code>
<Text>OPERATIONS RESEARCH</Text>
</ProgramElement>
<ProgramReference>
<Code>072E</Code>
<Text>NETWORKS &amp; QUEUING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>073E</Code>
<Text>OPTIMIZATION &amp; DECISION MAKING</Text>
</ProgramReference>
<ProgramReference>
<Code>076E</Code>
<Text>SERVICE ENTERPRISE SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>077E</Code>
<Text>SIMULATION MODELS</Text>
</ProgramReference>
<ProgramReference>
<Code>078E</Code>
<Text>ENTERPRISE DESIGN &amp; LOGISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>1787</Code>
<Text>SERVICE ENTERPRISE SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>5514</Code>
<Text>OPERATIONS RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>8023</Code>
<Text>Health Care Enterprise Systems</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~340000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main thrust of this research was to develop a new framework for incorporating risk into sequential decision making under uncertainty.&nbsp; The two pillars of the approach are cumulative prospect theory (CPT) and dynamic risk measures.&nbsp; The framework builds on both of these research streams to formulate a single theory that integrates subjective preferences in human behavior with normative decision-making objectives.&nbsp; Existing utility-based dynamic models could not handle the nonconvexity implied by the behavioral models of prospect theory.&nbsp; Hence, we have explored a new framework that allows the probability weighting found in cumulative prospect theory to be combined with the usual outcome weighting of traditional expected utility formulations in a sequential decision-making model that incorporates both types of risk sensitivity. Using the new framework, we developed efficient dynamic programming sampling and simulation-based methods for risk-sensitive optimization and control problems, and investigated how the new modeling of risk-sensitivity affects the behavior of decision makers.</p> <p>In particular, we have developed dynamic programming algorithms in this new framework and proved their convergence for finite and infinite horizon problems of this type. We have shown that, in general, the resulting feedback policies are randomized.</p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 9.0px Helvetica} --> <p class="p1">We have developed an approach to global optimization based on CPT, called Cumulative Weighting Optimization (CWO), which we prove convergent to an optimal solution and is stable under disturbances. Based on these results, we have designed a class of such algorithms for solving global optimization problems. We have proved that, under some additional assumptions, the simulation-based version of the CWO algorithm is also convergent and stable.</p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 9.0px Helvetica} --> <p class="p1">We have extended this idea to a risk-sensitive reinforcement learning (RL) setting and designed algorithms for both estimation and control.&nbsp; The estimation scheme that we designed uses the empirical distribution to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of a CPT-value optimization procedure that is based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation. We have proved theoretical convergence guarantees for all the proposed algorithms and also illustrated the usefulness of CPT-based criteria in a traffic signal control application.</p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 9.0px Helvetica} --> <p class="p1">In related work, we formulated two stochastic multi-armed bandit problems with distorted probabilities on the cost distributions: the classic K-armed bandit and the linearly parameterized bandit. In both settings, we designed algorithms that are inspired by the Upper Confidence Bound approach, incorporated cost distortions, and exhibited sublinear regret assuming H?lder continuous weight distortion functions. For both settings, we proved bounds on the regret of our algorithms. Numerical examples demonstrate the advantages resulting from using distortion-aware learning algorithms.</p> <p>As part of this research, we have also developed new approaches and algorithms for solving decision- making problems under uncertainty.&nbsp; In particular, we have studied the statistical ranking and selection problem of finding the best alternative when the performances of each alternative must be estimated by sampling.&nbsp; We have designed new policies that perform well relative to existing policies. In addition, we have formulated the fully sequential sampling and selection decision in statistical ranking and selection as a stochastic control problem, and derived an&nbsp;approximately optimal allocation policy. We have shown that this policy is not only computationally efficient but also possesses rigorous optimality under reasonable assumptions. Moreover, the proposed allocation policy is easily generalizable in the approximate dynamic programming paradigm.</p> <p class="p1">&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/21/2018<br>      Modified by: Steven&nbsp;I&nbsp;Marcus</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main thrust of this research was to develop a new framework for incorporating risk into sequential decision making under uncertainty.  The two pillars of the approach are cumulative prospect theory (CPT) and dynamic risk measures.  The framework builds on both of these research streams to formulate a single theory that integrates subjective preferences in human behavior with normative decision-making objectives.  Existing utility-based dynamic models could not handle the nonconvexity implied by the behavioral models of prospect theory.  Hence, we have explored a new framework that allows the probability weighting found in cumulative prospect theory to be combined with the usual outcome weighting of traditional expected utility formulations in a sequential decision-making model that incorporates both types of risk sensitivity. Using the new framework, we developed efficient dynamic programming sampling and simulation-based methods for risk-sensitive optimization and control problems, and investigated how the new modeling of risk-sensitivity affects the behavior of decision makers.  In particular, we have developed dynamic programming algorithms in this new framework and proved their convergence for finite and infinite horizon problems of this type. We have shown that, in general, the resulting feedback policies are randomized.  We have developed an approach to global optimization based on CPT, called Cumulative Weighting Optimization (CWO), which we prove convergent to an optimal solution and is stable under disturbances. Based on these results, we have designed a class of such algorithms for solving global optimization problems. We have proved that, under some additional assumptions, the simulation-based version of the CWO algorithm is also convergent and stable.  We have extended this idea to a risk-sensitive reinforcement learning (RL) setting and designed algorithms for both estimation and control.  The estimation scheme that we designed uses the empirical distribution to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of a CPT-value optimization procedure that is based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation. We have proved theoretical convergence guarantees for all the proposed algorithms and also illustrated the usefulness of CPT-based criteria in a traffic signal control application.  In related work, we formulated two stochastic multi-armed bandit problems with distorted probabilities on the cost distributions: the classic K-armed bandit and the linearly parameterized bandit. In both settings, we designed algorithms that are inspired by the Upper Confidence Bound approach, incorporated cost distortions, and exhibited sublinear regret assuming H?lder continuous weight distortion functions. For both settings, we proved bounds on the regret of our algorithms. Numerical examples demonstrate the advantages resulting from using distortion-aware learning algorithms.  As part of this research, we have also developed new approaches and algorithms for solving decision- making problems under uncertainty.  In particular, we have studied the statistical ranking and selection problem of finding the best alternative when the performances of each alternative must be estimated by sampling.  We have designed new policies that perform well relative to existing policies. In addition, we have formulated the fully sequential sampling and selection decision in statistical ranking and selection as a stochastic control problem, and derived an approximately optimal allocation policy. We have shown that this policy is not only computationally efficient but also possesses rigorous optimality under reasonable assumptions. Moreover, the proposed allocation policy is easily generalizable in the approximate dynamic programming paradigm.            Last Modified: 12/21/2018       Submitted by: Steven I Marcus]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
