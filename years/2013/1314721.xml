<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Large: Collaborative Research: HCPN: Hybrid Circuit/Packet Networking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>599444.00</AwardTotalIntnAmount>
<AwardAmount>599444</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Deepankar Medhi</SignBlockName>
<PO_EMAI>dmedhi@nsf.gov</PO_EMAI>
<PO_PHON>7032922935</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Ever-larger data centers are powering the cloud computing revolution, but the scale of these installations is currently limited by the ability to provide sufficient internal network connectivity. Delivering scalable packet-switched interconnects that can support the continually increasing data rates required between literally hundreds of thousands of servers is an extremely challenging problem that is only getting harder. This project leverages microsecond optical circuit-switch technology to develop a hybrid switching paradigm that spans the gap between traditional circuit switching and full-fledged packet switching, achieving a level of performance and scale not previously attainable. This will result in a hybrid switch whose optical switching capacity is orders of magnitude larger than the electrical packet switch, yet whose performance from an end-to-end perspective is largely indistinguishable from a giant (electrical) packet switch.&lt;br/&gt;&lt;br/&gt;The research provides a quantitative baseline for hybrid network design across a wide range of present and future technologies. The project will consist of five parts: i) traffic characterization to identify the class of network traffic that a circuit switch can support as well as the partitioning of the traffic between the circuit and packet portions of the network; ii) circuit scheduling to enable the circuit switch to rapidly multiplex a set of circuits across a large set of data center traffic flows; iii) traffic conditioning to reduce the variability of traffic at the end hosts, easing the demands placed on switch scheduling; iv) a prototype hybrid network that can use an optical circuit switch that operates three orders of magnitude faster than existing solutions; and v) a trend analysis to understand the tradeoffs resulting from potential future technology advances.&lt;br/&gt;&lt;br/&gt;The work stands to dramatically improve data center networks, significantly reducing operating costs and increasing energy efficiency. The research material will be incorporated into courses, helping to train the next generation of computer networking scientists and engineers. The PIs will also continue ongoing outreach to high school students, both through the UCSD COSMOS summer program and through talks delivered at local high schools.</AbstractNarration>
<MinAmdLetterDate>08/08/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1314721</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Andersen</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David G Andersen</PI_FULL_NAME>
<EmailAddress>dga@cs.cmu.edu</EmailAddress>
<PI_PHON>4122683064</PI_PHON>
<NSF_ID>000423704</NSF_ID>
<StartDate>08/08/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~391021</FUND_OBLG>
<FUND_OBLG>2015~208423</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Today's cloud services are power hungry, using energy sufficient to power every household in New York City twice over, and that percentage is expected to double in the next 10 years[1].&nbsp; The bills and carbon costs associated with today's massive datacenters are staggering [2].&nbsp; The enormous energy demands of these datacenters limits their growth potential and results in unnecessary operational expense. The good news is that there are many improvements that can be made in datacenters to raise their efficiency. In some cases, commercial providers run their systems at only 10 or 20% utilization due in large part to the inability for the underlying network to scale to meet bandwidth requirements. Fundamentally, the packet-switching technology underlying current data-center interconnects limits their ability to scale: implementing control- and data-planes necessary to forward packets individually is costly at present, and will rapidly cease to be feasible as link datarates continue to increase. The overall goal of this project has been to leverage microsecond optical circuit-switch technology to develop a hybrid switchingparadigm that spans the gap between traditional circuit switching and full-fledged packet switching, achieving a level of performance andscale not previously attainable. We have designed and built a hybrid switch whose optical switching capacity is orders of magnitudelarger than the electrical packet switch, yet whose performance from an end-to-end perspective is largely indistinguishable from a giant (electrical) packet switch. A key aspect of this project has been demonstrating a system-level control plane for hybrid networks capableof leveraging both circuit- and packet-switching. Our work has resulted in a characterization of commercial datacenter workloads, in particular multiple clusters at Facebook. Our findings have been used by our project as well as other projects to design an interconnect supporting commercial workloads. We have explored circuit switching in the context of a new, novel, non-crossbar &ldquo;Selector Switch&rdquo; architecture. A Selector Switch switches entire groups ofinput ports between entirely disjoint network matchings, rather than switching traffic from one input port to a single output port. We have studied the fundamental scaling limitations of MEMS-based optical switches, and found that they scale as a function of resolvable states, rather than ports. This means that by selecting among matchings, rather than individual input/output port matchings, we can build a Selector Switch that scales to 1000s of ports with existing technology, without requiring expensive optical amplification or telecom-grade transceivers. We have designed and build a 61-port Selector Switch prototype. Paired with a Selector Switch-based architecture, we have demonstrated how to build a Top-of-Rack switch that supports hybrid packet/circuit networks. The resulting prototype, REACToR, relies on a trio of mechanisms to support hybrid networks. First, it explicitly&ldquo;pulls&rdquo; packets from attached servers based on foreknowledge of which circuits will be established at a given time. This eliminates the need to maintain large packet buffers in the ToR. Second, REACToR classifies packets based on a centralized circuit schedule, directinglow-latency packets to a separate packet-switched network to reduce overall latency. Finally, REACToR plays a key role in ensuring datacenter-wide synchronization. We have designed and build a 16-port FPGA-based REACToR prototype. We have designed two centralized circuit-switch scheduling algorithms, Solstice and Albedo. Solstice operates by processing an inputdemand matrix, representing a snapshot of the overall datacenter workload demand. The Solstice algorithm is a computationally tractable heuristic for maximizing the bandwidth through the network, operating closely to non-tractable optimal solutions. We next designed Albedo, which extends the assumptions of Solstice by assuming that traffic could be indirected, or sent through intermediate nodes. By allowing some traffic to travel over what would otherwise be unused paths, Albedo is able to increase the total amount of traffic sent through the network as compared to Solstice. We have implemented both Solstice and Albedo in software. Finally, we have explored the software mechanisms needed by applications to take advantage of these high-bandwidth networks, in the form of support for ultra-fast, operating system stack-bypassing network cards.&nbsp; Our results have shown new and better ways for applications to be structured to capitalize on the network bandwidths our project seeks to realize.<br /><br />The net result of this project is a new strategy for supporting high bandwidth networks in the future, sidestepping the impending limitations of all-packet-switched networks. By switching some datacenter traffic optically, future datacenters will be able to support higher bandwidths, which will result in faster computations, fewer servers waiting for the data they need (and thus less server energyusage), and higher overall efficiency. The result of higher datacenter efficiency is a dramatic reduction in overall US energy usage (since datacenters account for 2.5% of US energy usage), and the removal of a barrier that companies encounter today, namely growing their networks to meet their customer demands.<br /><br />[1] https://energy.gov/eere/buildings/data-centers-and-servers<br />[2] https://www.nrdc.org/resources/americas-data-centers-consuming-and-wasting-growing-amounts-energy</p><br> <p>            Last Modified: 05/24/2019<br>      Modified by: David&nbsp;G&nbsp;Andersen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today's cloud services are power hungry, using energy sufficient to power every household in New York City twice over, and that percentage is expected to double in the next 10 years[1].  The bills and carbon costs associated with today's massive datacenters are staggering [2].  The enormous energy demands of these datacenters limits their growth potential and results in unnecessary operational expense. The good news is that there are many improvements that can be made in datacenters to raise their efficiency. In some cases, commercial providers run their systems at only 10 or 20% utilization due in large part to the inability for the underlying network to scale to meet bandwidth requirements. Fundamentally, the packet-switching technology underlying current data-center interconnects limits their ability to scale: implementing control- and data-planes necessary to forward packets individually is costly at present, and will rapidly cease to be feasible as link datarates continue to increase. The overall goal of this project has been to leverage microsecond optical circuit-switch technology to develop a hybrid switchingparadigm that spans the gap between traditional circuit switching and full-fledged packet switching, achieving a level of performance andscale not previously attainable. We have designed and built a hybrid switch whose optical switching capacity is orders of magnitudelarger than the electrical packet switch, yet whose performance from an end-to-end perspective is largely indistinguishable from a giant (electrical) packet switch. A key aspect of this project has been demonstrating a system-level control plane for hybrid networks capableof leveraging both circuit- and packet-switching. Our work has resulted in a characterization of commercial datacenter workloads, in particular multiple clusters at Facebook. Our findings have been used by our project as well as other projects to design an interconnect supporting commercial workloads. We have explored circuit switching in the context of a new, novel, non-crossbar "Selector Switch" architecture. A Selector Switch switches entire groups ofinput ports between entirely disjoint network matchings, rather than switching traffic from one input port to a single output port. We have studied the fundamental scaling limitations of MEMS-based optical switches, and found that they scale as a function of resolvable states, rather than ports. This means that by selecting among matchings, rather than individual input/output port matchings, we can build a Selector Switch that scales to 1000s of ports with existing technology, without requiring expensive optical amplification or telecom-grade transceivers. We have designed and build a 61-port Selector Switch prototype. Paired with a Selector Switch-based architecture, we have demonstrated how to build a Top-of-Rack switch that supports hybrid packet/circuit networks. The resulting prototype, REACToR, relies on a trio of mechanisms to support hybrid networks. First, it explicitly"pulls" packets from attached servers based on foreknowledge of which circuits will be established at a given time. This eliminates the need to maintain large packet buffers in the ToR. Second, REACToR classifies packets based on a centralized circuit schedule, directinglow-latency packets to a separate packet-switched network to reduce overall latency. Finally, REACToR plays a key role in ensuring datacenter-wide synchronization. We have designed and build a 16-port FPGA-based REACToR prototype. We have designed two centralized circuit-switch scheduling algorithms, Solstice and Albedo. Solstice operates by processing an inputdemand matrix, representing a snapshot of the overall datacenter workload demand. The Solstice algorithm is a computationally tractable heuristic for maximizing the bandwidth through the network, operating closely to non-tractable optimal solutions. We next designed Albedo, which extends the assumptions of Solstice by assuming that traffic could be indirected, or sent through intermediate nodes. By allowing some traffic to travel over what would otherwise be unused paths, Albedo is able to increase the total amount of traffic sent through the network as compared to Solstice. We have implemented both Solstice and Albedo in software. Finally, we have explored the software mechanisms needed by applications to take advantage of these high-bandwidth networks, in the form of support for ultra-fast, operating system stack-bypassing network cards.  Our results have shown new and better ways for applications to be structured to capitalize on the network bandwidths our project seeks to realize.  The net result of this project is a new strategy for supporting high bandwidth networks in the future, sidestepping the impending limitations of all-packet-switched networks. By switching some datacenter traffic optically, future datacenters will be able to support higher bandwidths, which will result in faster computations, fewer servers waiting for the data they need (and thus less server energyusage), and higher overall efficiency. The result of higher datacenter efficiency is a dramatic reduction in overall US energy usage (since datacenters account for 2.5% of US energy usage), and the removal of a barrier that companies encounter today, namely growing their networks to meet their customer demands.  [1] https://energy.gov/eere/buildings/data-centers-and-servers [2] https://www.nrdc.org/resources/americas-data-centers-consuming-and-wasting-growing-amounts-energy       Last Modified: 05/24/2019       Submitted by: David G Andersen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
