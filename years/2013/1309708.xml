<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: High Performance Cellular Simultaneous Recurrent Network based Pattern Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>146630.00</AwardTotalIntnAmount>
<AwardAmount>146630</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Radhakisan Baheti</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This is a collaborative proposal between a neural network researcher, addressing the issues of face recognition and image recognition in general, and a researcher on a new class of electronic chip based on memristors.&lt;br/&gt;       In recent years, new world records have been set in image recognition by convolutional neural networks, funded &lt;br/&gt;at other universities through the EFRI/COPN topic at NSF. At times, those systems have outperformed humans in those tasks. On the neural network side, this team plans to use a more general class of neural networks, the Cellular Simultaneous Neural Network (CSRN), to address benchmark challenges in face recognition where computers have yet to outperform humans. The CSRN may be viewed as&lt;br/&gt;a generalization of the convolutional network to add a kind of real-time recurrence or feedback, a kind of recurrence which is known to be crucial to the powers of biological brains.&lt;br/&gt;       On the electronic hardware side, this proposal addresses a crucial challenge in continuing Moore's Law. The speed of computing chips is not expected to grow as fast as it did in the past, but thanks to breakthroughs in lithography and the recent work in memristors, we can still expect progress towards thousand or even millions of active processors on a chip. In order to make full use of this emerging new capability, new efforts are needed to integrate device work and systems level work together, in developing new architectures of real practical use. If successful, this project could be an important step forward in that effort. Memristors for use in memory are already being well-funded by industry, but the extension to active processing and learning is more of a high risk&lt;br/&gt;breakthrough activity.&lt;br/&gt;     This project also includes a substantial component of education and outreach, including development of systems to stimulate K-8 children.</AbstractNarration>
<MinAmdLetterDate>05/20/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1309708</AwardID>
<Investigator>
<FirstName>Tarek</FirstName>
<LastName>Taha</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tarek M Taha</PI_FULL_NAME>
<EmailAddress>ttaha1@udayton.edu</EmailAddress>
<PI_PHON>9372293119</PI_PHON>
<NSF_ID>000287418</NSF_ID>
<StartDate>05/20/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Dayton</Name>
<CityName>DAYTON</CityName>
<ZipCode>454690104</ZipCode>
<PhoneNumber>9372292919</PhoneNumber>
<StreetAddress>300 COLLEGE PARK AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073134025</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF DAYTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073134025</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Dayton]]></Name>
<CityName>Dayton</CityName>
<StateCode>OH</StateCode>
<ZipCode>454690104</ZipCode>
<StreetAddress><![CDATA[300 COLLEGE PARK AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7607</Code>
<Text>EPCN-Energy-Power-Ctrl-Netwrks</Text>
</ProgramElement>
<ProgramReference>
<Code>102E</Code>
<Text>Quantum/high perform algorithm</Text>
</ProgramReference>
<ProgramReference>
<Code>1653</Code>
<Text>Adaptive &amp; intelligent systems</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~146630</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The key objective of this work was to accelerate neural algorithms (in particular Cellular Simultaneous Recurrent Networks) on neural hardware. The key outcomes of this work are:</p> <p>1. We accelerated Cellular Simultaneous Recurrent Networks (CSRNs). CSRNs are a class of neural algorithms that are useful for recognizing objects in camera and video images. These algorithms need to be taught what to recognize (training) and this is a highly compute intensive task. We looked at how to get these algorithms trained fast on a high performance computing system. We were able to accelerate the training process by over 500 times on a cluster of graphics processors compared to using a single traditional processor. This is important to enable the algorithms to be used efficiently as the more images a network can be trained with, the more capable it becomes.</p> <p>2. We examined the implementation of neural algorithms on neuromorphic hardware. These hardware are geared towards running neural algorithms efficiently and are becoming very popular in a large set of applications. For this study, we looked at the IBM TrueNorth Neurosynaptic system. We accelerated several neural algorithms on the TrueNorth processor including Convolutional Sparse Coding and Network intrusion detection using Deep Learning. We found that these types of hardware can enable very low power, yet high performance. This is important in modern power constrained systems such as cell phones.</p> <p>3. We developed novel extensions to neural algorithms for better recognition of objects in images and videos. Examples of this include combining CSRNs with CNNs to achieve better training, and combining recurrent networks with inception convolution networks.</p> <p>The results of these work were published in peer reviewed conferences and journals. Two graduate students were trained through this work.</p><br> <p>            Last Modified: 01/04/2018<br>      Modified by: Tarek&nbsp;M&nbsp;Taha</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The key objective of this work was to accelerate neural algorithms (in particular Cellular Simultaneous Recurrent Networks) on neural hardware. The key outcomes of this work are:  1. We accelerated Cellular Simultaneous Recurrent Networks (CSRNs). CSRNs are a class of neural algorithms that are useful for recognizing objects in camera and video images. These algorithms need to be taught what to recognize (training) and this is a highly compute intensive task. We looked at how to get these algorithms trained fast on a high performance computing system. We were able to accelerate the training process by over 500 times on a cluster of graphics processors compared to using a single traditional processor. This is important to enable the algorithms to be used efficiently as the more images a network can be trained with, the more capable it becomes.  2. We examined the implementation of neural algorithms on neuromorphic hardware. These hardware are geared towards running neural algorithms efficiently and are becoming very popular in a large set of applications. For this study, we looked at the IBM TrueNorth Neurosynaptic system. We accelerated several neural algorithms on the TrueNorth processor including Convolutional Sparse Coding and Network intrusion detection using Deep Learning. We found that these types of hardware can enable very low power, yet high performance. This is important in modern power constrained systems such as cell phones.  3. We developed novel extensions to neural algorithms for better recognition of objects in images and videos. Examples of this include combining CSRNs with CNNs to achieve better training, and combining recurrent networks with inception convolution networks.  The results of these work were published in peer reviewed conferences and journals. Two graduate students were trained through this work.       Last Modified: 01/04/2018       Submitted by: Tarek M Taha]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
