<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Neurodynamics of Tonality</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/17/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>18291.00</AwardTotalIntnAmount>
<AwardAmount>18291</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Cleary</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Music is a high-level cognitive capacity, a form of communication that relies on highly structured temporal sequences comparable in complexity to language.  Music is found among all human cultures, and musical "languages" vary among cultures and depend upon learning. For example, European melodies use different kinds of note combinations than Indian melodies, making it difficult for Westerners to understand Indian music, and vice versa. Unlike language, however, music rarely refers to the external world. It consists of self-contained patterns of sound, aspects of which are found universally among musical cultures. Therefore, while an understanding of the brain processes underlying language is still a distant goal, discovering the general principles of neural dynamics that underlie music may now be possible. Tonality refers to the stability relationships that are perceived among notes in a musical language. Although there are different kinds of tonality, tonality itself is a universal feature of music, found in virtually every musical language. The hypothesis of this research is that neural oscillation underlies tonal cognition and perception. Neural oscillation is periodic neural activity that, in the auditory system, becomes time-locked to incoming sounds. Neural oscillations can be complex, but there are now powerful mathematical tools for analyzing them. Mathematical analyses of time-locking auditory dynamics suggest constraints on what sorts of tonal relationships should be possible. They predict that fundamental principles of neural dynamics combined with fundamental principles of neural plasticity constrain what musical languages can be learned.&lt;br/&gt;&lt;br/&gt;To make detailed predictions, a sophisticated computer model of the auditory system will be built, based on the organization of the auditory system and general neurodynamic principles. Two simulations will be trained through passive exposure to European and North Indian melodies. These two are chosen because they represent two very different musical languages that are each relatively well-studied. The computer model will be used to predict neurophysiological and perceptual observations about music perception that have been collected over the past thirty-five years or so. Success of this model would imply the existence of a musical universal grammar. Universals predicted by intrinsic neurodynamics would provide a direct link to neurophysiology, and explain how brain changes during learning can establish different musical languages. This could lead to fundamental paradigm shifts in music theory, music cognition and related fields.  The success of this model would be equally influential in cognitive neuroscience. It would imply that high-level cognition and perception can arise from the interaction of acoustic signals with the physics of the auditory system. No neurodynamic approach has ever successfully captured such a high-level cognitive capacity. Researchers are currently struggling with the question of how to reconcile cognitive theories with neurodynamic principles and observations, and success in the musical domain could lead to new insights. This research will elucidate fundamental mechanisms of hearing and communication, and holds significant promise for understanding auditory system development. Identification of innate constraints shaping human communication behavior may have further implications for language learning. This research has implications for understanding a wide range of hearing and communication disorders. It has potential applicability to improving the design of neural prostheses, enhancing the perception of music and other sounds in cochlear implant patients.</AbstractNarration>
<MinAmdLetterDate>12/02/2013</MinAmdLetterDate>
<MaxAmdLetterDate>12/02/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1362417</AwardID>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Large</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward W Large</PI_FULL_NAME>
<EmailAddress>edward.large@oscilloscape.com</EmailAddress>
<PI_PHON>5617060863</PI_PHON>
<NSF_ID>000112586</NSF_ID>
<StartDate>12/02/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Connecticut</Name>
<CityName>Storrs</CityName>
<ZipCode>062691133</ZipCode>
<PhoneNumber>8604863622</PhoneNumber>
<StreetAddress>438 Whitney Road Ext.</StreetAddress>
<StreetAddress2><![CDATA[Unit 1133]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>614209054</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CONNECTICUT</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004534830</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Connecticut]]></Name>
<CityName>Storrs</CityName>
<StateCode>CT</StateCode>
<ZipCode>062691133</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~18291</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Music is a high-level cognitive capacity, a form of communication that relies on highly structured sound sequences comparable in complexity to spoken language. Music is found among all human cultures, and musical &lsquo;languages&rsquo; vary across cultures with learning. Unlike language, however, music rarely refers to the external world. It consists of self contained patterns of sound, and certain aspects these patterns are found universally, among all musical cultures.&nbsp;These two features &ndash; universality and independence &ndash; suggested to us that studying musical language may help us to discover and quantify general principles of brain function.&nbsp;</p> <p>Our research has revealed general principles of brain function that underlie music perception and musical communication. We have found that&nbsp;music speaks to the brain in its own language. The brain synchronizes with musical sounds, and one particular form of synchronization &ndash; called mode-locking &ndash; can give rise to expectancies for what events will come next in a melodic sequence. We also found that a form of neuroplasticity, called Hebbian plasticity, enables the brain to mold its intrinsic expectancies, so that it can learn new musical languages.&nbsp;</p> <p>We have been able to capture these principles of brain function &ndash; neural synchrony with sound, mode-locking, and Hebbian neuroplasticity &ndash; in a mathematical model of brain function. This mathematical model has been implemented as a computer program that is enabling researchers around the world to stimulate our models, and test our theories of brain function. This will enable new experiments and discoveries that will further&nbsp;enhance our understanding of the fundamental principles of brain function.</p> <p>This work has uncovered basic mechanisms of hearing, communication, and auditory system development. It has identified neural constraints that shape musical communication, and this has potentially wide-ranging implications for understanding how children learn language. Our results also have implications for understanding many different types of hearing and communication disorders. Based on our findings, we are now developing technologies that may eventually improve the design of neural prostheses such as hearing aids and cochlear implants, that can the perception of music and other sounds in hearing impaired listeners.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/13/2014<br>      Modified by: Edward&nbsp;W&nbsp;Large</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Music is a high-level cognitive capacity, a form of communication that relies on highly structured sound sequences comparable in complexity to spoken language. Music is found among all human cultures, and musical ælanguagesÆ vary across cultures with learning. Unlike language, however, music rarely refers to the external world. It consists of self contained patterns of sound, and certain aspects these patterns are found universally, among all musical cultures. These two features &ndash; universality and independence &ndash; suggested to us that studying musical language may help us to discover and quantify general principles of brain function.   Our research has revealed general principles of brain function that underlie music perception and musical communication. We have found that music speaks to the brain in its own language. The brain synchronizes with musical sounds, and one particular form of synchronization &ndash; called mode-locking &ndash; can give rise to expectancies for what events will come next in a melodic sequence. We also found that a form of neuroplasticity, called Hebbian plasticity, enables the brain to mold its intrinsic expectancies, so that it can learn new musical languages.   We have been able to capture these principles of brain function &ndash; neural synchrony with sound, mode-locking, and Hebbian neuroplasticity &ndash; in a mathematical model of brain function. This mathematical model has been implemented as a computer program that is enabling researchers around the world to stimulate our models, and test our theories of brain function. This will enable new experiments and discoveries that will further enhance our understanding of the fundamental principles of brain function.  This work has uncovered basic mechanisms of hearing, communication, and auditory system development. It has identified neural constraints that shape musical communication, and this has potentially wide-ranging implications for understanding how children learn language. Our results also have implications for understanding many different types of hearing and communication disorders. Based on our findings, we are now developing technologies that may eventually improve the design of neural prostheses such as hearing aids and cochlear implants, that can the perception of music and other sounds in hearing impaired listeners.                   Last Modified: 12/13/2014       Submitted by: Edward W Large]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
