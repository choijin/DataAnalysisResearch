<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>COLLABORATIVE RESEARCH: TIMING VARIATION RESILIENT SIGNAL PROCESSING: HARDWARE-ASSISTED CROSS-LAYER ADAPTATION</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>205029.00</AwardTotalIntnAmount>
<AwardAmount>205029</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In this research, a new vertically integrated cross-layer timing variation resilience methodology at the algorithm, microarchitecture and circuit levels, with "hardware-assistance" from the latter two levels is proposed. This addresses the effects of process variations and random delay defects in modern deeply scaled technologies as well as the effects of electrical bugs.  At the highest level of the layer stack, the project considers algorithmic level workload adaptation as well as adaptation to intermittent errors in the underlying hardware due to low-power/high-speed pipeline and arithmetic unit operation. A key contribution of this research is a novel way to accurately determine when the logic and arithmetic units of pipeline stages have finished computation. This uses concepts from wave pipelined operation of logic circuits and allows activity completion detection within nearly a single or a few gate delays. Such completion sensing allows pipeline stages to "borrow" or "lend" time much more effectively than currently used synchronously clocked pipelines. In addition, backup error detection mechanisms allow the processor pipeline to operate reliably even when there is some kind of malfunction in the completion sensing circuitry. A vertically integrated control algorithm is used to modulate power vs. performance vs. timing error resilience at the circuit, microarchitecture and algorithm (video compression) levels to deliver the desired quality of service at the required video throughput with minimum power consumption. &lt;br/&gt;&lt;br/&gt;Through this research effort, the development of courses at GaTech in embedded DSP design and test, and yield management research under extreme process variations will be greatly facilitated. The PIs will develop a set of teaching materials on power management and error resilience in real-time digital signal processing systems. The PIs will make maximum effort to involve undergraduate students from the Summer Undergraduate Research Experience for minorities (SURE) program in the proposed research. It will also be possible to involve senior undergraduate project students in this research through targeted advisement. They will participate in H.O.T. Days@ Georgia Tech, a one-week long summer program designed to introduce high school students to electrical and computer engineering concepts. The key involvement will be in working with robots (LEGO Mindstorm, simple functions). Both Georgia Tech and Auburn University aggressively encourage participation of undergraduate students, as well as women and minorities in research. Auburn's participation in the project will also improve the research capabilities of Alabama, an EPSCOR state. Additionally, several master's students from the Historically Black Tuskegee University located near Auburn will take graduate courses at Auburn University. The best prepared among these students will be encouraged to join the project and Ph.D. programs at the participating universities. Thus, funding for this project will support the goals of recruiting more U.S. citizens, women and minorities to graduate programs.</AbstractNarration>
<MinAmdLetterDate>08/21/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/21/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319529</AwardID>
<Investigator>
<FirstName>Adit</FirstName>
<LastName>Singh</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adit D Singh</PI_FULL_NAME>
<EmailAddress>adsingh@eng.auburn.edu</EmailAddress>
<PI_PHON>3348441847</PI_PHON>
<NSF_ID>000239664</NSF_ID>
<StartDate>08/21/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Auburn University</Name>
<CityName>Auburn</CityName>
<ZipCode>368320001</ZipCode>
<PhoneNumber>3348444438</PhoneNumber>
<StreetAddress>VPRED, Research &amp; Innovation Ctr</StreetAddress>
<StreetAddress2><![CDATA[540 Devall Drive, Suite 200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<StateCode>AL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066470972</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>AUBURN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>066470972</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Auburn University]]></Name>
<CityName>Auburn</CityName>
<StateCode>AL</StateCode>
<ZipCode>368490001</ZipCode>
<StreetAddress><![CDATA[200 Broun Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7945</Code>
<Text>DES AUTO FOR MICRO &amp; NANO SYST</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7945</Code>
<Text>DES AUTO FOR MICRO &amp; NANO SYST</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~205029</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this research project was to develop and validate innovative new design methodologies for complex digital electronic systems, such as processors and computers, that allow significantly improved performance while consuming reduced power compared to current designs. State-of-the-art computing systems, built in advanced technologies, experience significant manufacturing variations among each of the millions of microscopic electronic components fabricated within each integrated circuit. These variations, which are statistically random, are caused by unavoidable manufacturing inaccuracies associated with fabricating components with dimensions less than one tenth the wavelength of visible light. Their operational impact is to introduce random delays in the individual logic gates making up the electrical signal paths within a processor. Consequently, a large integrated circuit has millions of circuit paths with a wide statistical distribution of signal delays, with no two manufactured copies of the design having the same delays for corresponding paths. Inevitably, these will include a handful of extremely slow (worst case) outlier circuit paths in the tail of the statistical distributions. Because only a small subset of the circuit paths are active during each repeating computation cycle (clock cycle with a fixed&nbsp; time duration or period), the slowest paths often remain inactive during many clock cycles. However, it is impossible to know when a slow paths may be activated. Consequently, a large enough timing margin must be introduced into the clock period for each cycle so as to always accommodate the worst case delay along any computation circuit path. This is inefficient, wasteful of available power available to the circuit (often from a battery) and degrades computational performance.</p> <p>Two different design approaches were investigated in this project to mitigate this power and performance loss: (1) Better-than-worst-case timing designs where a more aggressive clock rate is employed in practice, along with the ability to detect the errors that occur on the rare occasions that a slow path is indeed activated, followed by recovery of the correct computation. (2) Asynchronous self-clocking pipelines that continuously change the computational cycle time based on sensing the completion of the computation, and only then activating the next clock cycle.&nbsp;</p> <p>A key outcome of this project was the demonstration that there are important computation applications where the better-than-worst-case timing design methodology can yield significantly better trade-offs between performance, power and silicon area. Major semiconductor designs companies such as ARM and Intel had earlier attempted building processors using the better-than-worst-case design approach. However, the power/performance gains they observed for general processor applications were limited to the elimination of the timing margins (approximately 20%) in the designs. However, once the timing margin is eliminated, the clock cycle time cannot be speeded up any further because it encounters a large number of circuit paths with virtually the same delay. This is due to the necessity to equalize critical path delays in pipelined processor designs. Once the timing margin is eliminated, any further speed-up of the clock results in such a large number of errors that the cost of error recovery in terms of recovery roll back time greatly exceeds any benefit from the increase clock rate. However, this project has shown that while better-than-worst-case timing design may not be effective in general purpose processor applications, there is an important class of very commonly used computational arithmetic designs where it can be extremely effective. Because of the need for propagation of the carry signal,&nbsp; arithmetic circuits inherently display very high variability in path delays that is dependent on the computational inputs. This can be exploited using better-than-worst-case designs. The innovative pipelined multiplier architecture developed as part of this project, and evaluated in an industrial setting fir advanced technology, has been shown to have performance comparable to the fastest traditional multiplier designs while requiring only half the circuit area and power consumption.&nbsp;The outcome of the research towards designing asynchronous self-clocking pipelines were less conclusive in validating the advantages of new methodology. Here switching/transition sensors were strategically incorporated throughout in the computational circuitry to sense the completion of all computation in the pipeline stages before the next clock pulse is activated. This allows the clock period to vary continuously, speeding up when path delays are relatively small, and slowing down when the worst case delay increases.&nbsp; While a novel timing variation tolerant pipeline design was developed, successfully implemented and evaluated, the benefits of such a design approach appear more limited. This was primarily on account of the large overhead of the completion sensing circuitry, as well as clock circuitry needed to distribute the self-adjusting clock signals throughout the design.</p> <p>The results of the research have been widely disseminated through publications and presentations at conferences around the world. They have also been included in graduate level courses at Auburn University and Georgia Tech. Furthermore, this project trained several students who are now successfully deploying this technology in industry.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/03/2019<br>      Modified by: Adit&nbsp;D&nbsp;Singh</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this research project was to develop and validate innovative new design methodologies for complex digital electronic systems, such as processors and computers, that allow significantly improved performance while consuming reduced power compared to current designs. State-of-the-art computing systems, built in advanced technologies, experience significant manufacturing variations among each of the millions of microscopic electronic components fabricated within each integrated circuit. These variations, which are statistically random, are caused by unavoidable manufacturing inaccuracies associated with fabricating components with dimensions less than one tenth the wavelength of visible light. Their operational impact is to introduce random delays in the individual logic gates making up the electrical signal paths within a processor. Consequently, a large integrated circuit has millions of circuit paths with a wide statistical distribution of signal delays, with no two manufactured copies of the design having the same delays for corresponding paths. Inevitably, these will include a handful of extremely slow (worst case) outlier circuit paths in the tail of the statistical distributions. Because only a small subset of the circuit paths are active during each repeating computation cycle (clock cycle with a fixed  time duration or period), the slowest paths often remain inactive during many clock cycles. However, it is impossible to know when a slow paths may be activated. Consequently, a large enough timing margin must be introduced into the clock period for each cycle so as to always accommodate the worst case delay along any computation circuit path. This is inefficient, wasteful of available power available to the circuit (often from a battery) and degrades computational performance.  Two different design approaches were investigated in this project to mitigate this power and performance loss: (1) Better-than-worst-case timing designs where a more aggressive clock rate is employed in practice, along with the ability to detect the errors that occur on the rare occasions that a slow path is indeed activated, followed by recovery of the correct computation. (2) Asynchronous self-clocking pipelines that continuously change the computational cycle time based on sensing the completion of the computation, and only then activating the next clock cycle.   A key outcome of this project was the demonstration that there are important computation applications where the better-than-worst-case timing design methodology can yield significantly better trade-offs between performance, power and silicon area. Major semiconductor designs companies such as ARM and Intel had earlier attempted building processors using the better-than-worst-case design approach. However, the power/performance gains they observed for general processor applications were limited to the elimination of the timing margins (approximately 20%) in the designs. However, once the timing margin is eliminated, the clock cycle time cannot be speeded up any further because it encounters a large number of circuit paths with virtually the same delay. This is due to the necessity to equalize critical path delays in pipelined processor designs. Once the timing margin is eliminated, any further speed-up of the clock results in such a large number of errors that the cost of error recovery in terms of recovery roll back time greatly exceeds any benefit from the increase clock rate. However, this project has shown that while better-than-worst-case timing design may not be effective in general purpose processor applications, there is an important class of very commonly used computational arithmetic designs where it can be extremely effective. Because of the need for propagation of the carry signal,  arithmetic circuits inherently display very high variability in path delays that is dependent on the computational inputs. This can be exploited using better-than-worst-case designs. The innovative pipelined multiplier architecture developed as part of this project, and evaluated in an industrial setting fir advanced technology, has been shown to have performance comparable to the fastest traditional multiplier designs while requiring only half the circuit area and power consumption. The outcome of the research towards designing asynchronous self-clocking pipelines were less conclusive in validating the advantages of new methodology. Here switching/transition sensors were strategically incorporated throughout in the computational circuitry to sense the completion of all computation in the pipeline stages before the next clock pulse is activated. This allows the clock period to vary continuously, speeding up when path delays are relatively small, and slowing down when the worst case delay increases.  While a novel timing variation tolerant pipeline design was developed, successfully implemented and evaluated, the benefits of such a design approach appear more limited. This was primarily on account of the large overhead of the completion sensing circuitry, as well as clock circuitry needed to distribute the self-adjusting clock signals throughout the design.  The results of the research have been widely disseminated through publications and presentations at conferences around the world. They have also been included in graduate level courses at Auburn University and Georgia Tech. Furthermore, this project trained several students who are now successfully deploying this technology in industry.          Last Modified: 06/03/2019       Submitted by: Adit D Singh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
