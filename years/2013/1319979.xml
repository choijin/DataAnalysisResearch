<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Optimal Iterative Estimation in Signal Processing, Information Theory and Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>416160.00</AwardTotalIntnAmount>
<AwardAmount>416160</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern imaging devices, sensors, data acquisition systems allow to gather data with unprecedented speed and&lt;br/&gt;accuracy. Most of the times, however, we are not interested in accumulating data per se, but rather to&lt;br/&gt;uncover some hidden patterns in the data. For instance, given a large network, we might want to discover &lt;br/&gt;a small subset of notes that are tightly connected to each other. Such highly connected substructures&lt;br/&gt;are of interest in biological datasets, but also in social network analysis, and in signal processing. &lt;br/&gt;Finding such patterns requires highly efficient algorithms that can process large amount of data and&lt;br/&gt;uncover tenuous statistical signatures. The investigators develop new algorithms that simultaneously optimize&lt;br/&gt;both metrics: statistical efficiency and computational efficiency.&lt;br/&gt;&lt;br/&gt;Consider in particular the problem of finding an anomalous submatrix in a large data matrix with independent &lt;br/&gt;random entries. If the anomalous submatrix has entries with a different distribution, this can be done via &lt;br/&gt;principal component analysis, as long as the submatrix has dimensions of the  order of the square root of&lt;br/&gt;the ambient dimensions. The investigators introduce a class of first order methods with linear complexity,&lt;br/&gt;and determine the optimal algorithm within this class. This appears to provably outperform existing approaches. &lt;br/&gt;The same framework is generalized to several other classes of high-dimensional estimation problems. &lt;br/&gt;Optimal iterative procedures are developed  under strict computational constraints.</AbstractNarration>
<MinAmdLetterDate>05/31/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/31/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319979</AwardID>
<Investigator>
<FirstName>Andrea</FirstName>
<LastName>Montanari</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrea Montanari</PI_FULL_NAME>
<EmailAddress>montanari@stanford.edu</EmailAddress>
<PI_PHON>6507232300</PI_PHON>
<NSF_ID>000107366</NSF_ID>
<StartDate>05/31/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943054100</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramElement>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~416160</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern data analysis challenges require to fit complex models with tens or hundreds of thousandsof parameters. Such parameters fitting procedures are carried out by sophisticated algorithms, andmay require dedicated hardware. In this context, the key bottleneck to carry out a specific statistical taskis not due to a lack of data, but rather to the feasibility (or infeasibility) of a certain computational task.While computational tools (both algorithms and hardware) increase in sophistication at a rapid pace,these computational barriers in statistical estimation cannot be surpassed by future improvements.Further, they are ubiquitous and arise in a large number of easy-to-state problems. A prototypicalexample is the `hidden clique problem' whereby we are asked to identify a dense subgraph in an otherwiserandom graph. This is the simplest example of a class of problems that require to estimate hidden structurein network or matrix data. Another classical example is sparse principal component analysis,whereby we want to estimate the main direction of variability of a cloud of datapoints, knowing that theunderlying signal is sparse.For these and similar problems, computational barriers correspond to sharp phase transitionsas the number of samples (data points) crosses a critical threshold. When we have less data than thethreshold, the information contained in those data is still sufficient to perform accurate statisticalinference, but extracting that information requires an exponentially large computational power and is thereforepractically impossible. This project characterized such barriers in a number of problems, and developed new algorithms&nbsp; that are guaranteed to achieve those barriers. The algorithms studied belong broadly tothree classes: semidefinite programming relaxations, iterative message passing algorithms, and gradient descentalgorithms. We find that often several algorithmic strategies are characterized by the same computational limits.</p><br> <p>            Last Modified: 10/13/2018<br>      Modified by: Andrea&nbsp;Montanari</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern data analysis challenges require to fit complex models with tens or hundreds of thousandsof parameters. Such parameters fitting procedures are carried out by sophisticated algorithms, andmay require dedicated hardware. In this context, the key bottleneck to carry out a specific statistical taskis not due to a lack of data, but rather to the feasibility (or infeasibility) of a certain computational task.While computational tools (both algorithms and hardware) increase in sophistication at a rapid pace,these computational barriers in statistical estimation cannot be surpassed by future improvements.Further, they are ubiquitous and arise in a large number of easy-to-state problems. A prototypicalexample is the `hidden clique problem' whereby we are asked to identify a dense subgraph in an otherwiserandom graph. This is the simplest example of a class of problems that require to estimate hidden structurein network or matrix data. Another classical example is sparse principal component analysis,whereby we want to estimate the main direction of variability of a cloud of datapoints, knowing that theunderlying signal is sparse.For these and similar problems, computational barriers correspond to sharp phase transitionsas the number of samples (data points) crosses a critical threshold. When we have less data than thethreshold, the information contained in those data is still sufficient to perform accurate statisticalinference, but extracting that information requires an exponentially large computational power and is thereforepractically impossible. This project characterized such barriers in a number of problems, and developed new algorithms  that are guaranteed to achieve those barriers. The algorithms studied belong broadly tothree classes: semidefinite programming relaxations, iterative message passing algorithms, and gradient descentalgorithms. We find that often several algorithmic strategies are characterized by the same computational limits.       Last Modified: 10/13/2018       Submitted by: Andrea Montanari]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
