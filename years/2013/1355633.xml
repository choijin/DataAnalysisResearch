<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Physio-linguistic Models of Deception Detection</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>315986</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Aidong Zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The goal of this Early-concept Grant for Exploratory Research  is to explore a new generation of computational tools for joint modeling of physiological and linguistic signals of human behavior.  The project is the first to investigate physio-linguistic models for deception analysis. To achieve this goal, the following three research objectives are pursued. First, a novel physio-linguistic dataset of deceit is built, covering several different domains. Second,  rule-based classifiers for deception detection are explored, using physiological features (e.g., heart rate, respiration rate, galvanic skin response, skin temperature), as well as linguistic features. Third, data-driven learning approaches for multimodal deception detection are developed,  taking advantage of the recent progress in early, late, and temporal fusion models.  &lt;br/&gt;&lt;br/&gt;The project is exploratory in nature, and acts as a catalyst for  novel research problems. First, it explores  rich sets of multimodal features extracted from physiological and linguistic modalities, analyzing their effectiveness in the recognition of deceit. Second, it also explores the integration of multiple physio-linguistic modalities, through experiments with rule-based and data-driven techniques that fuse multimodal features into joint deception analysis models. To address the challenges of multimodal research work, the team working on this project brings together experts from the fields of bio-sensors, computational linguistics, and physiology and behavioral sciences.&lt;br/&gt;&lt;br/&gt;The project has high potential payoffs, as models of deception detection have broad applicability, including: the development of critical tools for various applications in fields such as criminal justice, intelligence, and security; the enhancement of applications that can be negatively affected by the presence of deceit, such as opinion analysis or modeling of human communication; and a deeper understanding of fundamental aspects of human behavior, which can positively impact medical applications in psychiatry and psychology.  The tools and datasets produced during this project will be made freely available for the research community.&lt;br/&gt;&lt;br/&gt;For further information see the project web site at: http://web.eecs.umich.edu/~mihalcea/deceptiondetection/</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/23/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1355633</AwardID>
<Investigator>
<FirstName>Rada</FirstName>
<LastName>Mihalcea</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rada F Mihalcea</PI_FULL_NAME>
<EmailAddress>mihalcea@umich.edu</EmailAddress>
<PI_PHON>7346474088</PI_PHON>
<NSF_ID>000492026</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mihai</FirstName>
<LastName>Burzo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mihai Burzo</PI_FULL_NAME>
<EmailAddress>mburzo@umich.edu</EmailAddress>
<PI_PHON>8102376677</PI_PHON>
<NSF_ID>000610618</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Flint]]></Name>
<CityName>Flint</CityName>
<StateCode>MI</StateCode>
<ZipCode>485021950</ZipCode>
<StreetAddress><![CDATA[303 E. Kearsley St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~300000</FUND_OBLG>
<FUND_OBLG>2015~15986</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to break new ground in the task of multimodal deception detection, by building novel datasets and introducing new multimodal models for deception dataset. The project built and released the first two large multimodal datasets for deception detection: a multimodal dataset of real-life trial data annotated for deception, also including transcriptions and gesture annotations; and a crowdsourced dataset of linguistic deception covering a large number of users, and including age and gender information. It has also introduced new multimodal models for deception detection, leveraging physiological, visual, linguistic, and psychological signals obtained from the multimodal datasets, and using methods for early and late fusion. The project has made important contributions to the understanding of the connection between these multimodal inputs, and has led to significant improvements over models that used only one modality at a time. It has also brought new insights into the linguistic, visual, and physiological clues associated with deception, as well as the role played by demographic factors (in particular gender) in deception detection. A workshop was organized in conjunction with the International Conference on Multimodal Interaction (ICMI), which brought together researchers working on the problem of automatic deception detection, facilitating discussions around this research topic. All the publications and datasets that were produced and made available under this project can be accessed from http://deceptiondetection.eecs.umich.edu<br /><br /><br /></p><br> <p>            Last Modified: 11/29/2017<br>      Modified by: Rada&nbsp;F&nbsp;Mihalcea</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to break new ground in the task of multimodal deception detection, by building novel datasets and introducing new multimodal models for deception dataset. The project built and released the first two large multimodal datasets for deception detection: a multimodal dataset of real-life trial data annotated for deception, also including transcriptions and gesture annotations; and a crowdsourced dataset of linguistic deception covering a large number of users, and including age and gender information. It has also introduced new multimodal models for deception detection, leveraging physiological, visual, linguistic, and psychological signals obtained from the multimodal datasets, and using methods for early and late fusion. The project has made important contributions to the understanding of the connection between these multimodal inputs, and has led to significant improvements over models that used only one modality at a time. It has also brought new insights into the linguistic, visual, and physiological clues associated with deception, as well as the role played by demographic factors (in particular gender) in deception detection. A workshop was organized in conjunction with the International Conference on Multimodal Interaction (ICMI), which brought together researchers working on the problem of automatic deception detection, facilitating discussions around this research topic. All the publications and datasets that were produced and made available under this project can be accessed from http://deceptiondetection.eecs.umich.edu          Last Modified: 11/29/2017       Submitted by: Rada F Mihalcea]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
