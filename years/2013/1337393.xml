<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: DSD: A2MA - Algorithms and Architectures for Multiresolution Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>749801.00</AwardTotalIntnAmount>
<AwardAmount>749801</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this research project is to devise novel methodologies and&lt;br/&gt;devices for problems in computational science and engineering that&lt;br/&gt;require high-intensity of arithmetic operations (also known as&lt;br/&gt;Floating-point Operations Per Second, or ``FLOPS'').  Among the many&lt;br/&gt;hurdles faced in research and development of such compute-intensive&lt;br/&gt;technologies is achieving energy-efficient utilization of the&lt;br/&gt;available computing resources. Similar to the miles/gallon metric used&lt;br/&gt;in automotive design, one is interested in a metric that can be used&lt;br/&gt;in the design of new computing technologies: optimizing&lt;br/&gt;FLOPS/watt. This research will be on designing novel algorithms and&lt;br/&gt;architectures that optimize this metric. These algorithms and&lt;br/&gt;architectures will be customized to a particular class of scientific&lt;br/&gt;computing problems: tree-based finite element methods and N-body&lt;br/&gt;problems.&lt;br/&gt;&lt;br/&gt;It is possible to devise algorithms that parallelize well and are &lt;br/&gt;energy efficient (i.e., produce high ``percentage-of-peak''&lt;br/&gt;measurements). Often, however, such algorithms sacrifice work&lt;br/&gt;optimality. It is much more difficult to design algorithms that do so&lt;br/&gt;while achieving both work optimality and energy efficiency. This&lt;br/&gt;on-node utilization wall---a chronic problem since the early&lt;br/&gt;nineties---not only remains unresolved but has become more acute with&lt;br/&gt;the emergence of deeper memory hierarchies and manycore and&lt;br/&gt;heterogeneous architectures. At the same time, there is a large&lt;br/&gt;untapped potential by not only adapting algorithms to architectural&lt;br/&gt;changes, but instead driving architecture design from algorithm&lt;br/&gt;requirements. This research will identify the design space for&lt;br/&gt;tree-based algorithms (under the constraints of work-optimality and&lt;br/&gt;maximum concurrency), evaluate performance of state-of-the-art codes,&lt;br/&gt;and explore custom algorithm/hardware platforms.  A number of broader&lt;br/&gt;impacts are anticipated from this project. The target methodologies&lt;br/&gt;find applications in earth sciences, engineering, cosmology, biology,&lt;br/&gt;and data analysis.  Along with the research activities, an educational&lt;br/&gt;and dissemination program will be designed to communicate the results&lt;br/&gt;of this work to both students and researchers, as well as a more&lt;br/&gt;general audience of computational and application scientists.</AbstractNarration>
<MinAmdLetterDate>09/17/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/17/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1337393</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>van de Geijn</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert A van de Geijn</PI_FULL_NAME>
<EmailAddress>rvdg@cs.utexas.edu</EmailAddress>
<PI_PHON>5124719720</PI_PHON>
<NSF_ID>000336892</NSF_ID>
<StartDate>09/17/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lizy</FirstName>
<LastName>John</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lizy K John</PI_FULL_NAME>
<EmailAddress>ljohn@ece.utexas.edu</EmailAddress>
<PI_PHON>5122321455</PI_PHON>
<NSF_ID>000378662</NSF_ID>
<StartDate>09/17/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>George</FirstName>
<LastName>Biros</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George Biros</PI_FULL_NAME>
<EmailAddress>gbiros@gmail.com</EmailAddress>
<PI_PHON>5122329566</PI_PHON>
<NSF_ID>000209886</NSF_ID>
<StartDate>09/17/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andreas</FirstName>
<LastName>Gerstlauer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andreas Gerstlauer</PI_FULL_NAME>
<EmailAddress>gerstl@ece.utexas.edu</EmailAddress>
<PI_PHON>5122328294</PI_PHON>
<NSF_ID>000520598</NSF_ID>
<StartDate>09/17/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121539</ZipCode>
<StreetAddress><![CDATA[101 E. 27th Street, Stop A9000]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~749801</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project we studied fast and energy-efficient algorithms and architectures for tree-based methods in science and engineering. &nbsp;We aim at the creation of new classes of integrated hardware/algorithms concepts for tree-based finite element methods(FEM) and tree-based fast N-body methods. The data access patterns for these methods can be abstracted using ``tree traversals'', a particular set of methods that use tree-based data structures. This abstraction allows us to introduce specialized, hardware-level memory access and compute primitives that overcome the on-node utilization wall, are energy efficient, can be used in a black-box fashion, and are flexible to be used by a broad range of applications. &nbsp;Our contributions are summarized below.</p> <p>I<strong>INTELLECTUAL MERIT:</strong></p> <p>We developed detailed theoretical performance models that measure throughput and capture vertical memory transfer costs in terms of both time and power overheads for tree codes. &nbsp;We have extended our analysis to multidimensional treecodes that find applications in machine learning and data analytics. We developed &nbsp;novel algorithms for N-body methods for computational physics (fast multipole methods) and data analysis (kernel methods and hierarchical matrices). These algorithnms have been implemented in open source libraries and have been scaled using distributed and shared memory parallelism, as well as GPU and special-purpose accelerators, on high-end compute clusters with thousands of nodes.</p> <p>We carried out thorough performance evaluations of our N-body codes. Having this baseline information and the algorithm-design space we were able to customize and co-design our codes to explore alternative algorithm/hardware scenarios. Indeed, we completed extensive optimizations for N-body kernels, in particular pairwise interactions and nearest-neighbor finding. We created synergies with the critical computational kernels (linear algebra) completed integration with the Linear Algebra Processor (LAP), a hardware accelerator for basic linear algebra primitives.</p> <p>For co-design of new algorithms and architectures, we worked on co-designing novel hardware mechanisms using (i) enhancements to existing general purpose heterogeneous platforms, and by (ii)designing specialized processors to accelerate the chosen applications, while also considering memory transfers and memory reshuffles. One particular need that we have identified is the ability to generalize the LAP framework to a few additional computational kernels that appear in many problems with tree-codes.</p> <p>Using our N-body codes as a driving example of typical HPC computation patterns, we investigated the quantitative performance and energy efficiency tradeoffs of different accelerator integration and coupling approaches.<br /><br /><strong>BROADER IMPACTS:</strong></p> <p>Funding from this award has resulted in 25 peer reviewed publications and 24 presentations in conferences, workshops, and other institutions.<br />In addition, this award partially funded three software packages, which are listed below.</p> <p>1. libaskit, a distributed memory library for N-body based kernel methods in machine learning.&nbsp;<br />http://padas.ices.utexas.edu/libaskit&nbsp;</p> <p>2. PVFMM, a distributed memory 3D N-body code for computational physics.<br />https://github.com/dmalhotra/pvfmm</p> <p>3. MARSS, an x86-architecture simulator integrated with LAP&nbsp;special-purpose accelerator for matrix-matrix multiplication.<br />https://github.com/LAProc/marss</p> <p><strong>BROADER IMPACTS:</strong></p> <p>This award partially supported two postdoctoral researchers, four graduate student (one female student), and three undergraduate students for summer internships (two female students). The first postdoctoral researcher moved to industry and the second to an academic, tenure-track faculty position. &nbsp;During the course of the award, the graduate students went to &nbsp;industrial summer internships at QUALCOMM, AMD, ARM, INTEL, and NVIDIA for technology transfer. All graduate students and postdoctoral researchers gave presentations in high-performance computing conferences.</p><br> <p>            Last Modified: 01/23/2018<br>      Modified by: George&nbsp;Biros</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project we studied fast and energy-efficient algorithms and architectures for tree-based methods in science and engineering.  We aim at the creation of new classes of integrated hardware/algorithms concepts for tree-based finite element methods(FEM) and tree-based fast N-body methods. The data access patterns for these methods can be abstracted using ``tree traversals'', a particular set of methods that use tree-based data structures. This abstraction allows us to introduce specialized, hardware-level memory access and compute primitives that overcome the on-node utilization wall, are energy efficient, can be used in a black-box fashion, and are flexible to be used by a broad range of applications.  Our contributions are summarized below.  IINTELLECTUAL MERIT:  We developed detailed theoretical performance models that measure throughput and capture vertical memory transfer costs in terms of both time and power overheads for tree codes.  We have extended our analysis to multidimensional treecodes that find applications in machine learning and data analytics. We developed  novel algorithms for N-body methods for computational physics (fast multipole methods) and data analysis (kernel methods and hierarchical matrices). These algorithnms have been implemented in open source libraries and have been scaled using distributed and shared memory parallelism, as well as GPU and special-purpose accelerators, on high-end compute clusters with thousands of nodes.  We carried out thorough performance evaluations of our N-body codes. Having this baseline information and the algorithm-design space we were able to customize and co-design our codes to explore alternative algorithm/hardware scenarios. Indeed, we completed extensive optimizations for N-body kernels, in particular pairwise interactions and nearest-neighbor finding. We created synergies with the critical computational kernels (linear algebra) completed integration with the Linear Algebra Processor (LAP), a hardware accelerator for basic linear algebra primitives.  For co-design of new algorithms and architectures, we worked on co-designing novel hardware mechanisms using (i) enhancements to existing general purpose heterogeneous platforms, and by (ii)designing specialized processors to accelerate the chosen applications, while also considering memory transfers and memory reshuffles. One particular need that we have identified is the ability to generalize the LAP framework to a few additional computational kernels that appear in many problems with tree-codes.  Using our N-body codes as a driving example of typical HPC computation patterns, we investigated the quantitative performance and energy efficiency tradeoffs of different accelerator integration and coupling approaches.  BROADER IMPACTS:  Funding from this award has resulted in 25 peer reviewed publications and 24 presentations in conferences, workshops, and other institutions. In addition, this award partially funded three software packages, which are listed below.  1. libaskit, a distributed memory library for N-body based kernel methods in machine learning.  http://padas.ices.utexas.edu/libaskit   2. PVFMM, a distributed memory 3D N-body code for computational physics. https://github.com/dmalhotra/pvfmm  3. MARSS, an x86-architecture simulator integrated with LAP special-purpose accelerator for matrix-matrix multiplication. https://github.com/LAProc/marss  BROADER IMPACTS:  This award partially supported two postdoctoral researchers, four graduate student (one female student), and three undergraduate students for summer internships (two female students). The first postdoctoral researcher moved to industry and the second to an academic, tenure-track faculty position.  During the course of the award, the graduate students went to  industrial summer internships at QUALCOMM, AMD, ARM, INTEL, and NVIDIA for technology transfer. All graduate students and postdoctoral researchers gave presentations in high-performance computing conferences.       Last Modified: 01/23/2018       Submitted by: George Biros]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
