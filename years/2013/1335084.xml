<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Synthesis of User Interfaces for Collaborative Systems in Uncertain Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>230000.00</AwardTotalIntnAmount>
<AwardAmount>230000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jordan Berg</SignBlockName>
<PO_EMAI>jberg@nsf.gov</PO_EMAI>
<PO_PHON>7032925365</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The research objective of collaborative project is to synthesize observability and estimation techniques from hybrid control theory to design and validate user-interfaces for expensive, high-risk, and safety-critical collaborative systems (e.g. avionics and biomedical devices). The user interface provides a means for the user to observe the underlying automated system as well as for the user to apply inputs to the system.  This research will develop deterministic and stochastic methods for systems under both normal and abnormal operating conditions. The developed techniques and algorithms will be designed to accommodate environments that have complex information requirements, inevitable uncertainties and measurement errors, and nontrivial continuous dynamics, such as those found in air traffic control and aircraft flight management systems. Specifically, this research addresses the development of 1) observability-based conditions to assess the correctness of information content in user interfaces (modeled as hybrid systems with discrete push-button and continuous joystick-type inputs), 2) mode estimation techniques for fault detection (for operation under abnormal conditions) and a prioritization of display elements, 3) display design via sensor scheduling to determine the optimal combination of information for a given display mode, and 4) abstraction-based conditions for correctness of user-interface design (that assume the user interface is a reduced representation of the actual underlying system under standard and abnormal conditions). &lt;br/&gt;&lt;br/&gt;If successful, the results of this research could help identify human-automation interaction problems (such as automation surprises, inadequate or excessive information contained in the user interface, and mode confusion) in collaborative systems before they are built, tested, deployed, and possibly while they are operating.  The control algorithms and interface design aids developed through this research are generic, and have potential to improve the effectiveness in collaborative systems in a variety of domains.  This research plan will contribute to the formal methods in human computer interaction community, and to the control theory communities focused on the design of decision support aids.  Developed theory, code, and data will be disseminated in a timely manner.  Students will benefit through development of a new, multidisciplinary graduate course in control of collaborative systems, with particular emphasis on avionics and air traffic applications, as well as from mentoring and participation in this research.</AbstractNarration>
<MinAmdLetterDate>08/24/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/24/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1335084</AwardID>
<Investigator>
<FirstName>Inseok</FirstName>
<LastName>Hwang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Inseok Hwang</PI_FULL_NAME>
<EmailAddress>ihwang@purdue.edu</EmailAddress>
<PI_PHON>7654944600</PI_PHON>
<NSF_ID>000490016</NSF_ID>
<StartDate>08/24/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072045</ZipCode>
<StreetAddress><![CDATA[701 W. Stadium Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1632</Code>
<Text>CONTROL SYSTEMS</Text>
</ProgramElement>
<ProgramReference>
<Code>030E</Code>
<Text>CONTROL SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>031E</Code>
<Text>MECHATRONICS</Text>
</ProgramReference>
<ProgramReference>
<Code>034E</Code>
<Text>Dynamical systems</Text>
</ProgramReference>
<ProgramReference>
<Code>035E</Code>
<Text>NOISE, ACOUSTICS, VIBRATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>8024</Code>
<Text>Complex Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~230000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>When the human (e.g., pilot) interacts with the machine (e.g., automation), emergent safety-critical problems referred to as mode confusion or automation surprise could occur resulting in dysfunctional pilot- automation interaction. This problem occurs when the pilot loses his/her situational awareness about the current and future behavior of the aircraft, resulting in a loss of shared perception with the automation. Several aviation incidents/accidents as reported in the NASA Aviation Safety Reporting System can be attributed to mode confusion problems. Detecting such problems using learning based algorithms and verification and validation (V&amp;V) methods are new and important topics for safety of the aviation. Traditionally, this problem is studied by human factors and engineering psychology, which, although insightful to a certain degree, is not fool-proof in its analysis due to the lack of mathematical rigor. Furthermore, most existing works to detect such confusions neglect to consider the underlying hybrid nature of the pilot-automation system, leading to poor detection capability. In seeking to address these problems, we have focused on the following topics: 1) we have proposed a <em>formal framework</em> for <em>verification</em> of flight deck mode confusion under both deterministic and probabilistic conditions of the automation and pilot. This was motivated by the real aviation incidents/accidents where the expectations of the pilot differ from the actual behavior of the aircraft resulting in loss of situational awareness. To facilitate this, we have studied mathematical modeling methods for the automation and pilot and further proposed computationally efficient (predicate-based intent) abstraction procedure for feasible verification. 2) we have investigated the same flight deck mode confusion detection problem but under a different set of conditions. This was motivated by the consideration that obtaining detailed mathematical models of the autopilot and flight management system with a priori knowledge of system parameters (e.g., capture rates), automation logic, and sensor noise statistics are extremely difficult due to their proprietary nature of them. In order to relax this assumption, we have proposed a <em>learning based method</em> for the automation and pilot intent inference that works with the recorded flight data.</p> <p>The proposed formal V &amp;V framework to qualitatively and quantitatively detect the mode confusion takes an estimation and control theoretic point of view and has good scalability properties which makes it attractive for practical applications. The approach systematically harnesses both the physical behavior (e.g., aircraft) and the logical behavior (e.g., pilot) of the human-automation interaction, unlike existing works, thus increasing the <em>detection accuracy </em>of the confusion and enhancing the situational awareness for the pilot. Thus, the proposed research makes significant contribution to safety enhancement of our air travel. Our algorithm has been validated on several illustrative human-machine interaction problems that contain a variety of interaction issues such as the pilot may turn the MCP knob too much, or with a delay, or even in wrong direction, or the pilot may forget some details of the automation logic. The effectiveness of the analytical findings using the data generated by the Multi Aircraft Control System (MACS) and the scenarios inspired by well-documented NASA aviation incidents/accidents validate the significant advantages of our algorithm. The results have been published in national conferences and journals and have been selected for Best Graduate Student Paper Awards. Finally, since the developed V&amp;V methods are general enough to be applied to various human-machine interaction applications such as driver-automotive systems, robotic assistive technologies, and others, the tools developed in this project could help improve the safety of our society.</p> <p>In addition, we have also investigated the problem of developing a novel control framework to <em>simultaneously</em> attenuate for the destabilizing effects of lagged supervisory guidance (e.g., pilot reaction delays), delayed control (e.g., actuator lags, communication bandwidth), and system uncertainties (e.g., parameter variations, disturbances). Designing command tracking controllers with guaranteed transient and steady-state performance that are robust to all the above complexities, <em>simultaneously</em>, has remained largely intractable. Most existing control designs such as adaptive and robust control address only a subset of the above complexities and do not rigorously comment on their transient performance, thus limiting their applicability to practical systems such as industrial motion control systems and UAV tracking. In seeking to address these problems, we have developed predictor adaptive robust control framework which pulls together and seamlessly integrates ideas from several control schemes such as learning from the environment, predicting the system behavior a few seconds ahead of time, and attenuating time-varying disturbances into a unified control framework. The developed control framework has applicability for several practical control problems in automotive, aerospace, and robotic sectors and aims to enhance the safety of complex, human-machine collaborative systems.</p><br> <p>            Last Modified: 10/30/2017<br>      Modified by: Inseok&nbsp;Hwang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ When the human (e.g., pilot) interacts with the machine (e.g., automation), emergent safety-critical problems referred to as mode confusion or automation surprise could occur resulting in dysfunctional pilot- automation interaction. This problem occurs when the pilot loses his/her situational awareness about the current and future behavior of the aircraft, resulting in a loss of shared perception with the automation. Several aviation incidents/accidents as reported in the NASA Aviation Safety Reporting System can be attributed to mode confusion problems. Detecting such problems using learning based algorithms and verification and validation (V&amp;V) methods are new and important topics for safety of the aviation. Traditionally, this problem is studied by human factors and engineering psychology, which, although insightful to a certain degree, is not fool-proof in its analysis due to the lack of mathematical rigor. Furthermore, most existing works to detect such confusions neglect to consider the underlying hybrid nature of the pilot-automation system, leading to poor detection capability. In seeking to address these problems, we have focused on the following topics: 1) we have proposed a formal framework for verification of flight deck mode confusion under both deterministic and probabilistic conditions of the automation and pilot. This was motivated by the real aviation incidents/accidents where the expectations of the pilot differ from the actual behavior of the aircraft resulting in loss of situational awareness. To facilitate this, we have studied mathematical modeling methods for the automation and pilot and further proposed computationally efficient (predicate-based intent) abstraction procedure for feasible verification. 2) we have investigated the same flight deck mode confusion detection problem but under a different set of conditions. This was motivated by the consideration that obtaining detailed mathematical models of the autopilot and flight management system with a priori knowledge of system parameters (e.g., capture rates), automation logic, and sensor noise statistics are extremely difficult due to their proprietary nature of them. In order to relax this assumption, we have proposed a learning based method for the automation and pilot intent inference that works with the recorded flight data.  The proposed formal V &amp;V framework to qualitatively and quantitatively detect the mode confusion takes an estimation and control theoretic point of view and has good scalability properties which makes it attractive for practical applications. The approach systematically harnesses both the physical behavior (e.g., aircraft) and the logical behavior (e.g., pilot) of the human-automation interaction, unlike existing works, thus increasing the detection accuracy of the confusion and enhancing the situational awareness for the pilot. Thus, the proposed research makes significant contribution to safety enhancement of our air travel. Our algorithm has been validated on several illustrative human-machine interaction problems that contain a variety of interaction issues such as the pilot may turn the MCP knob too much, or with a delay, or even in wrong direction, or the pilot may forget some details of the automation logic. The effectiveness of the analytical findings using the data generated by the Multi Aircraft Control System (MACS) and the scenarios inspired by well-documented NASA aviation incidents/accidents validate the significant advantages of our algorithm. The results have been published in national conferences and journals and have been selected for Best Graduate Student Paper Awards. Finally, since the developed V&amp;V methods are general enough to be applied to various human-machine interaction applications such as driver-automotive systems, robotic assistive technologies, and others, the tools developed in this project could help improve the safety of our society.  In addition, we have also investigated the problem of developing a novel control framework to simultaneously attenuate for the destabilizing effects of lagged supervisory guidance (e.g., pilot reaction delays), delayed control (e.g., actuator lags, communication bandwidth), and system uncertainties (e.g., parameter variations, disturbances). Designing command tracking controllers with guaranteed transient and steady-state performance that are robust to all the above complexities, simultaneously, has remained largely intractable. Most existing control designs such as adaptive and robust control address only a subset of the above complexities and do not rigorously comment on their transient performance, thus limiting their applicability to practical systems such as industrial motion control systems and UAV tracking. In seeking to address these problems, we have developed predictor adaptive robust control framework which pulls together and seamlessly integrates ideas from several control schemes such as learning from the environment, predicting the system behavior a few seconds ahead of time, and attenuating time-varying disturbances into a unified control framework. The developed control framework has applicability for several practical control problems in automotive, aerospace, and robotic sectors and aims to enhance the safety of complex, human-machine collaborative systems.       Last Modified: 10/30/2017       Submitted by: Inseok Hwang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
