<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: A Novel Biomechatronic Interface Based on Wearable Dynamic Imaging Sensors</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2014</AwardEffectiveDate>
<AwardExpirationDate>01/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>995055.00</AwardTotalIntnAmount>
<AwardAmount>995055</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The problem of controlling biomechatronic systems, such as multiarticulating prosthetic hands, involves unique challenges in the science and engineering of Cyber Physical Systems (CPS), requiring integration between computational systems for recognizing human functional activity and intent and controlling prosthetic devices to interact with the physical world. Research on this problem has been limited by the difficulties in noninvasively acquiring robust biosignals that allow intuitive and reliable control of multiple degrees of freedom (DoF). The objective of this research is to investigate a new sensing paradigm based on ultrasonic imaging of dynamic muscle activity. The synergistic research plan will integrate novel imaging technologies, new computational methods for activity recognition and learning, and high-performance embedded computing to enable robust and intuitive control of dexterous prosthetic hands with multiple DoF. The interdisciplinary research team involves collaboration between biomedical engineers, electrical engineers and computer scientists. The specific aims are to: (1) research and develop spatio-temporal image analysis and pattern recognition algorithms to learn and predict different dexterous tasks based on sonographic patterns of muscle activity (2) develop a wearable image-based biosignal sensing system by integrating multiple ultrasound imaging sensors with a low-power heterogeneous multicore embedded processor and (3) perform experiments to evaluate the real-time control of a prosthetic hand.&lt;br/&gt;The proposed research methods are broadly applicable to assistive technologies where physical systems, computational frameworks and low-power embedded computing serve to augment human activities or to replace lost functionality. The research will advance CPS science and engineering through integration of portable sensors for image-based sensing of complex adaptive physical phenomena such as dynamic neuromuscular activity, and real-time sophisticated image understanding algorithms to interpret such phenomena running on low-power high performance embedded systems. The technological advances would enable practical wearable image-based biosensing, with applications in healthcare, and the computational methods would be broadly applicable to problems involving activity recognition from spatiotemporal image data, such as surveillance.&lt;br/&gt;&lt;br/&gt;This research will have societal impacts as well as train students in interdisciplinary methods relevant to CPS. About 1.6 million Americans live with amputations that significantly affect activities of daily living. The proposed project has the long-term potential to significantly improve functionality of upper extremity prostheses, improve quality of life of amputees, and increase the acceptance of prosthetic limbs. This research could also facilitate intelligent assistive devices for more targeted neurorehabilitation of stroke victims. This project will provide immersive interdisciplinary CPS-relevant training for graduate and undergraduate students to integrate computational methods with imaging, processor architectures, human functional activity and artificial devices for solving challenging public health problems. A strong emphasis will be placed on involving undergraduate students in research as part of structured programs at our institution. The research team will involve students with disabilities in research activities by leveraging an ongoing NSF-funded project. Bioengineering training activities will be part of a newly developed undergraduate curriculum and a graduate curriculum under development.&lt;br/&gt;&lt;br/&gt;The synergistic research plan has been designed to advance CPS science and engineering through the development of new computational methods for dynamic activity recognition and learning from image sequences, development of novel wearable imaging technologies including high-performance embedded computing, and real-time control of a physical system. The specific aims are to: &lt;br/&gt;(1) Research and develop spatio-temporal image analysis and pattern recognition algorithms to learn and predict different dexterous tasks based on sonographic patterns of muscle activity. The first aim has three subtasks designed to collect, analyze and understand image sequences associated with functional tasks.  (2) Develop a wearable image-based biosignal sensing system by integrating multiple ultrasound imaging sensors with a low-power heterogeneous multicore embedded processor. The second aim has two subtasks designed to integrate wearable imaging sensors with a real-time computational platform. (3) Perform experiments to evaluate the real-time control of a prosthetic hand. The third aim will integrate the wearable image acquisition system developed in Aim 2, and the image understanding algorithms developed in Aim 1, for real-time evaluation of the control of a prosthetic hand interacting with a virtual reality environment.&lt;br/&gt;&lt;br/&gt;Successful completion of these aims will result in a real-time system that acquires image data from complex neuromuscular activity, decodes activity intent from spatiotemporal image data using computational algorithms, and controls a prosthetic limb in a virtual reality environment in real time. Once developed and validated, this system can be the starting point for developing a new class of sophisticated control algorithms for intuitive control of advanced prosthetic limbs, new assistive technologies for neurorehabilitation, and wearable real-time imaging systems for smart health applications.</AbstractNarration>
<MinAmdLetterDate>08/30/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/30/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1329829</AwardID>
<Investigator>
<FirstName>Jana</FirstName>
<LastName>Kosecka</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jana Kosecka</PI_FULL_NAME>
<EmailAddress>kosecka@gmu.edu</EmailAddress>
<PI_PHON>7039931876</PI_PHON>
<NSF_ID>000207363</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Huzefa</FirstName>
<LastName>Rangwala</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Huzefa Rangwala</PI_FULL_NAME>
<EmailAddress>hrangwal@gmu.edu</EmailAddress>
<PI_PHON>7039933826</PI_PHON>
<NSF_ID>000515447</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Siddhartha</FirstName>
<LastName>Sikdar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Siddhartha Sikdar</PI_FULL_NAME>
<EmailAddress>ssikdar@gmu.edu</EmailAddress>
<PI_PHON>7039931539</PI_PHON>
<NSF_ID>000524662</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Houman</FirstName>
<LastName>Homayoun</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Houman Homayoun</PI_FULL_NAME>
<EmailAddress>hhomayoun@ucdavis.edu</EmailAddress>
<PI_PHON>9499439639</PI_PHON>
<NSF_ID>000624468</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Mason University</Name>
<CityName>FAIRFAX</CityName>
<ZipCode>220304422</ZipCode>
<PhoneNumber>7039932295</PhoneNumber>
<StreetAddress>4400 UNIVERSITY DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA11</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>077817450</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE MASON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>077817450</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Mason University]]></Name>
<CityName>Fairfax</CityName>
<StateCode>VA</StateCode>
<ZipCode>220304422</ZipCode>
<StreetAddress><![CDATA[4400 University Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA11</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7918</Code>
<Text>CPS-Cyber-Physical Systems</Text>
</ProgramElement>
<ProgramReference>
<Code>7918</Code>
<Text>CYBER-PHYSICAL SYSTEMS (CPS)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~995055</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There are over 50,000 individuals living with limb loss in the US. Loss of limbs has a profound impact on the ability of individuals to participate in activities of daily living. The function of the upper extremities is far more difficult to replace than that of the lower extremities. The primary functions of the upper extremities include not only gross and fine motor activities but also more complex combinations of activities, such as self-care, interaction with the environment and others, and self-expression.Thus, it is not surprising that persons with upper-limb amputation are generally less satisfied with the restoration of function provided by their prostheses relative to those with lower-limb amputation. Currently-available prosthetics only enable patients to perform a limited range of tasks. Although prosthetics have become more complex and more able to make complex movements, the means for patients to control these movements is inadequate. For the past 50 years, electromyography has been the predominant method for sensing electrical activity of the residual muscles in the forearm to infer the movement intent, a method known as myoelectric control. However, electromyography has a number of limitations that prevent robust sensing of the volitional intent of the user for the many different possible movements. Due to this, the current state of the art is moving towards more invasive approaches, such as targeted muscle reinnervation and peripheral nerve interfaces, to sense myoelectric signals closer to their source, which many amputees do not prefer. Therefore, there is an unmet need for a non-invasive control approach for prosthetic devices that enable many different movements/abilities.</p> <p>In this project, we proposed a paradigm shift in the field of prosthetic control using ultrasound imaging for sensing muscle deformation associated with volitional motor intent. This method is called sonomyography. The objective of this project was to develop a closed-loop cyber physical system that uses image analysis and machine learning to infer the volitional intent of the user in real time and control a terminal prosthetic device. We showed that this method can enable complex movements without the need for invasive approaches to amplify myoelectric signals, as ultrasound can resolve signals from deep within the tissue, and provides a rich signal source for decoding intent. We showed that sonomyography has significant advantages over conventional electromyography. As part of this project, we developed new machine learning algorithms to infer complex volitional movement from patterns of muscle deformation, developed new low-power wearable ultrasound imaging system, and evaluated closed loop control of virtual as well as physical prosthetic hands both in able bodied subjects as well as individuals with limb loss. This project brought together a multidisciplinary team of faculty and students from electrical engineering, bioengineering and computer science to develop technology that has the potential to significantly improve the lives of individuals living with limb loss.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/02/2019<br>      Modified by: Siddhartha&nbsp;Sikdar</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556842152341_Proportionalcontrol--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556842152341_Proportionalcontrol--rgov-800width.jpg" title="Proportional control of a TASKA hand using sonomyography"><img src="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556842152341_Proportionalcontrol--rgov-66x44.jpg" alt="Proportional control of a TASKA hand using sonomyography"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A subject controlling partial grasps of a TASKA multiarticulated prosthetic hand in real time using sonomypography.</div> <div class="imageCredit">Siddhartha Sikdar</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Siddhartha&nbsp;Sikdar</div> <div class="imageTitle">Proportional control of a TASKA hand using sonomyography</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841415107_Virtualenvironment--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841415107_Virtualenvironment--rgov-800width.jpg" title="Closed loop virtual evaluation environment"><img src="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841415107_Virtualenvironment--rgov-66x44.jpg" alt="Closed loop virtual evaluation environment"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A subject controlling a virtual prosthetic hand in real time using sonomyography.</div> <div class="imageCredit">Siddhartha Sikdar</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Siddhartha&nbsp;Sikdar</div> <div class="imageTitle">Closed loop virtual evaluation environment</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841500189_Prostheticablebodied--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841500189_Prostheticablebodied--rgov-800width.jpg" title="Controlling a physical prosthetic hand using sonomyography"><img src="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841500189_Prostheticablebodied--rgov-66x44.jpg" alt="Controlling a physical prosthetic hand using sonomyography"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A subject is controlling a multi-articulated prosthetic hand in real time using sonomyography</div> <div class="imageCredit">Siddhartha Sikdar</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Siddhartha&nbsp;Sikdar</div> <div class="imageTitle">Controlling a physical prosthetic hand using sonomyography</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841819510_TDSTASKA--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841819510_TDSTASKA--rgov-800width.jpg" title="Control of TASKA hand using wearable sonomyography system"><img src="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841819510_TDSTASKA--rgov-66x44.jpg" alt="Control of TASKA hand using wearable sonomyography system"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A subject controlling the TASKA multiarticulated hand using a wearable sonomyography system attached to their forearm.</div> <div class="imageCredit">Siddhartha Sikdar</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Siddhartha&nbsp;Sikdar</div> <div class="imageTitle">Control of TASKA hand using wearable sonomyography system</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841265117_Overallschematic--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841265117_Overallschematic--rgov-800width.jpg" title="Overall schematic demonstrating sonomyography"><img src="/por/images/Reports/POR/2019/1329829/1329829_10273021_1556841265117_Overallschematic--rgov-66x44.jpg" alt="Overall schematic demonstrating sonomyography"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The left panels show an ultrasound transducer used to image the forearm muscles as a subject performs different movements. The middle panel shows patterns of activity derived from image analysis. The right panels show a virtual prosthetic hand controlled by a machine learning algorithm.</div> <div class="imageCredit">Siddhartha Sikdar</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Siddhartha&nbsp;Sikdar</div> <div class="imageTitle">Overall schematic demonstrating sonomyography</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There are over 50,000 individuals living with limb loss in the US. Loss of limbs has a profound impact on the ability of individuals to participate in activities of daily living. The function of the upper extremities is far more difficult to replace than that of the lower extremities. The primary functions of the upper extremities include not only gross and fine motor activities but also more complex combinations of activities, such as self-care, interaction with the environment and others, and self-expression.Thus, it is not surprising that persons with upper-limb amputation are generally less satisfied with the restoration of function provided by their prostheses relative to those with lower-limb amputation. Currently-available prosthetics only enable patients to perform a limited range of tasks. Although prosthetics have become more complex and more able to make complex movements, the means for patients to control these movements is inadequate. For the past 50 years, electromyography has been the predominant method for sensing electrical activity of the residual muscles in the forearm to infer the movement intent, a method known as myoelectric control. However, electromyography has a number of limitations that prevent robust sensing of the volitional intent of the user for the many different possible movements. Due to this, the current state of the art is moving towards more invasive approaches, such as targeted muscle reinnervation and peripheral nerve interfaces, to sense myoelectric signals closer to their source, which many amputees do not prefer. Therefore, there is an unmet need for a non-invasive control approach for prosthetic devices that enable many different movements/abilities.  In this project, we proposed a paradigm shift in the field of prosthetic control using ultrasound imaging for sensing muscle deformation associated with volitional motor intent. This method is called sonomyography. The objective of this project was to develop a closed-loop cyber physical system that uses image analysis and machine learning to infer the volitional intent of the user in real time and control a terminal prosthetic device. We showed that this method can enable complex movements without the need for invasive approaches to amplify myoelectric signals, as ultrasound can resolve signals from deep within the tissue, and provides a rich signal source for decoding intent. We showed that sonomyography has significant advantages over conventional electromyography. As part of this project, we developed new machine learning algorithms to infer complex volitional movement from patterns of muscle deformation, developed new low-power wearable ultrasound imaging system, and evaluated closed loop control of virtual as well as physical prosthetic hands both in able bodied subjects as well as individuals with limb loss. This project brought together a multidisciplinary team of faculty and students from electrical engineering, bioengineering and computer science to develop technology that has the potential to significantly improve the lives of individuals living with limb loss.          Last Modified: 05/02/2019       Submitted by: Siddhartha Sikdar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
