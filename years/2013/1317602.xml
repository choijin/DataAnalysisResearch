<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Computation of Large-Scale, Multi-Dimensional Sparse Optimization Problems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>299999.00</AwardTotalIntnAmount>
<AwardAmount>299999</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yong Zeng</SignBlockName>
<PO_EMAI>yzeng@nsf.gov</PO_EMAI>
<PO_PHON>7032927902</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The proposed research largely lies in sparse optimization, a new distinct area of research in optimization for discovering sparse or other simple-structured solutions from dense datasets. Its development draws algorithmic techniques from classical nonlinear programming and is nurtured by the development in many other areas of data science. Today, the size, complexity, and diversity of instances have grown significantly. The proposed research addresses these new challenges in the following directions: data and variable splitting for handling multiple regularizers and for parallel and distributed optimization, efficient model path computation and regularization parameter selection, stochastic approximation, and coordinate descent methods for non-convex optimization. These investigations are expected to significantly reduce the running times of the existing algorithms, giving rise to novel algorithms to enable the solutions of a wide ranges of problems that are currently not solvable in data sciences. In particular, the expected results will fit machine learning models to data previously inaccessible (e.g., distributed data), enable the mining of data in much higher dimensions and across different modalities, as well as handle multiple regularizers in a computationally tractable way.&lt;br/&gt;&lt;br/&gt;Technological advances in data gathering have led to a rapid proliferation of big data in diverse areas such as the Internet, engineering, climate studies, cosmology, and medicine. In order for this massive amount of data to make sense, new computational approaches are being introduced to let scientists and engineers analyze their data. Among these approaches, sparse optimization and structured solutions have grown enormously important. Today, their scopes are quickly expanding. Beyond the sensing and processing of 1D signals and 2D images, high-dimensional quantities such as 3D video, 4D CT, and multi-way tensors have become the data or unknown variables in models. Beyond the sparsity structure, structures such as low-rankness, sparse graph, tree structure, linear representation of a few dictionary atoms, as well as their combinations, have debut as desired structures in various applications including genome mapping, protein structure study, social network analysis, stock price prediction, and text/speech mining. The proposed research will build on the recent successes and lead to new techniques for handling large-sized, diverse-typed data and variables, novel algorithms for pursuing a variety of structures in solutions, the extension of existing numerical methods to parallel and decentralized computing architectures, and the contributions to solving key problems in several aforementioned application areas.</AbstractNarration>
<MinAmdLetterDate>09/11/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317602</AwardID>
<Investigator>
<FirstName>Wotao</FirstName>
<LastName>Yin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wotao Yin</PI_FULL_NAME>
<EmailAddress>wotaoyin@math.ucla.edu</EmailAddress>
<PI_PHON>3108257764</PI_PHON>
<NSF_ID>000180296</NSF_ID>
<StartDate>09/11/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951555</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~105815</FUND_OBLG>
<FUND_OBLG>2014~105281</FUND_OBLG>
<FUND_OBLG>2015~88903</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="white-space: normal;">The focus of the research project was&nbsp;sparse optimization, a class of optimization models and numerical methods to produce solutions with simple structures. Among its many applications is the recovery of signals from a small number of their measurements by specifying their structures.&nbsp;The project addressed new challenges in sparse optimization. Specifically, the size, complexity, and diversity of instances grew signi?cantly. Beyond 1D signals and 2D images, high&shy;dimensional quantities such as video, 4D CT, and multi&shy;way tensors became the data or unknown variables in models. Beyond sparsity, structures such as low&shy;rankness, sparse graph, tree structure, linear representation of a few dictionary atoms, as well as their combinations, had&nbsp;debuted as desired structures in various applications.</p> <p style="white-space: normal;">The research activities of this project included the development of new algorithms and new lines of analysis, as well as software implementation&nbsp;and explorations of new applications. The educational activities of this project included the training of undergraduate and graduate students through research seminars and individual research projects. They completed reports and papers for publications in peer-reviewed journals, and attended workshops and conferences.</p> <p style="white-space: normal;">A selection of precise results are as follows. The citations are records of Google Scholar on December 1, 2016.</p> <ul> <li>A parallel algorithm for very large&shy;-scale sparse optimization. Demonstrated solving problems of 200GB in about 1 minute at the cost of $1 on Amazon EC2 cloud. Reference: Z. Peng, M. Yan, and W. Yin. Parallel and Distributed Sparse Optimization, Asilomar&rsquo;13, 2013. Citation: 59</li> <li>A parallel algorithm for very large&shy;scale general convex problems with multiple objectives and linear constraints. Demonstrated solving a 337GB basis&shy; pursuit problem on Amazon EC2 within 5 minutes at a cost of $1.22. Reference: W. Deng,M.&shy;J. Lai, Z. Peng, and W. Yin, Parallel Multi &shy;Block ADMM with o(1/k) Convergence, Journal of Scientific Computing, 2016. Citation: 79</li> <li>New interpretations and analysis to the Decentralized Gradient Descent (DGD) method for solving distributed consensus optimization problem. Established its tight parameter range for convergence and its convergence rates. Reference: K. Yuan, Q. Ling, and W. Yin, On the Convergence ofDecentralized Gradient Descent, SIAM Journal on Optimization, 2016. Citation: 60</li> <li>A novel decentralized exact first&shy;order algorithm (abbreviated as EXTRA) to solve the consensus optimization problem with differentiable objective functions, which has the best theoretical rate and practical performance. Reference: W. Shi, Q. Ling, G. Wu,and W. Yin, EXTRA: an Exact First&shy; Order Algorithm for Decentralized Consensus Optimization, SIAM Journal on Optimization, 2015. Citation: 79</li> <li>ARock, an asynchronous parallel algorithmic framework for finding a fixed point to a nonexpansive operator. It gives rise to novel and really fast algorithms for a variety of problems. Reference: Z. Peng, Y. Xu, M. Yan, and W. Yin, ARock: anAlgorithmic Framework for Asynchronous Parallel Coordinate Updates, SIAM Journal on Scientific Computing, 2016. Citation: 22</li> <li>The first three-&shy;operator splitting scheme for monotone operator inclusion problems.&nbsp;Previous ones are either two &shy;operator splitting schemes or reduce to one of them. Reference: D. Davis and W. Yin, A Three&shy;Operator Splitting Scheme and its Optimization Applications, under the review of Math OR, 2016. Citation: 31.</li> <li><span>A new model to recover a low-rank tensor by simultaneously performing low-rank&nbsp;</span><span>matrix factorizations to the all-mode matricizations of the underlying tensor. An alternating&nbsp;</span><span>minimization algorithm is applied to solve the model, along with two adaptive rank-adjusting&nbsp;</span><span>strategies when the exact rank is not known. Reference:&nbsp;<span>Y. Xu, R. Hao, W. Yin, and Z. Su,&nbsp;</span>Parallel matrix factorization for low-rank tensor completion,&nbsp;<span>Inverse Problems and Imaging, 2015. Citation: 29.</span></span></li> </ul> <p>Several software codes of these outcomes are implemented as open-source software codes.&nbsp;All papers and software codes are posted on the PI's homepage, UCLA CAM reports, arXiv, and Optimization Online.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/01/2016<br>      Modified by: Wotao&nbsp;Yin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The focus of the research project was sparse optimization, a class of optimization models and numerical methods to produce solutions with simple structures. Among its many applications is the recovery of signals from a small number of their measurements by specifying their structures. The project addressed new challenges in sparse optimization. Specifically, the size, complexity, and diversity of instances grew signi?cantly. Beyond 1D signals and 2D images, high&shy;dimensional quantities such as video, 4D CT, and multi&shy;way tensors became the data or unknown variables in models. Beyond sparsity, structures such as low&shy;rankness, sparse graph, tree structure, linear representation of a few dictionary atoms, as well as their combinations, had debuted as desired structures in various applications. The research activities of this project included the development of new algorithms and new lines of analysis, as well as software implementation and explorations of new applications. The educational activities of this project included the training of undergraduate and graduate students through research seminars and individual research projects. They completed reports and papers for publications in peer-reviewed journals, and attended workshops and conferences. A selection of precise results are as follows. The citations are records of Google Scholar on December 1, 2016.  A parallel algorithm for very large&shy;-scale sparse optimization. Demonstrated solving problems of 200GB in about 1 minute at the cost of $1 on Amazon EC2 cloud. Reference: Z. Peng, M. Yan, and W. Yin. Parallel and Distributed Sparse Optimization, Asilomar?13, 2013. Citation: 59 A parallel algorithm for very large&shy;scale general convex problems with multiple objectives and linear constraints. Demonstrated solving a 337GB basis&shy; pursuit problem on Amazon EC2 within 5 minutes at a cost of $1.22. Reference: W. Deng,M.&shy;J. Lai, Z. Peng, and W. Yin, Parallel Multi &shy;Block ADMM with o(1/k) Convergence, Journal of Scientific Computing, 2016. Citation: 79 New interpretations and analysis to the Decentralized Gradient Descent (DGD) method for solving distributed consensus optimization problem. Established its tight parameter range for convergence and its convergence rates. Reference: K. Yuan, Q. Ling, and W. Yin, On the Convergence ofDecentralized Gradient Descent, SIAM Journal on Optimization, 2016. Citation: 60 A novel decentralized exact first&shy;order algorithm (abbreviated as EXTRA) to solve the consensus optimization problem with differentiable objective functions, which has the best theoretical rate and practical performance. Reference: W. Shi, Q. Ling, G. Wu,and W. Yin, EXTRA: an Exact First&shy; Order Algorithm for Decentralized Consensus Optimization, SIAM Journal on Optimization, 2015. Citation: 79 ARock, an asynchronous parallel algorithmic framework for finding a fixed point to a nonexpansive operator. It gives rise to novel and really fast algorithms for a variety of problems. Reference: Z. Peng, Y. Xu, M. Yan, and W. Yin, ARock: anAlgorithmic Framework for Asynchronous Parallel Coordinate Updates, SIAM Journal on Scientific Computing, 2016. Citation: 22 The first three-&shy;operator splitting scheme for monotone operator inclusion problems. Previous ones are either two &shy;operator splitting schemes or reduce to one of them. Reference: D. Davis and W. Yin, A Three&shy;Operator Splitting Scheme and its Optimization Applications, under the review of Math OR, 2016. Citation: 31. A new model to recover a low-rank tensor by simultaneously performing low-rank matrix factorizations to the all-mode matricizations of the underlying tensor. An alternating minimization algorithm is applied to solve the model, along with two adaptive rank-adjusting strategies when the exact rank is not known. Reference: Y. Xu, R. Hao, W. Yin, and Z. Su, Parallel matrix factorization for low-rank tensor completion, Inverse Problems and Imaging, 2015. Citation: 29.   Several software codes of these outcomes are implemented as open-source software codes. All papers and software codes are posted on the PI's homepage, UCLA CAM reports, arXiv, and Optimization Online.                Last Modified: 12/01/2016       Submitted by: Wotao Yin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
