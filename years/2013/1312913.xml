<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  CDS&amp;E: Evolution of the High Redshift Galaxy and AGN Populations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>90000.00</AwardTotalIntnAmount>
<AwardAmount>90000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03020000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>AST</Abbreviation>
<LongName>Division Of Astronomical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Richard Barvainis</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The Cold Dark Matter (CDM) paradigm for the formation of structure in the universe has had many successes, from predicting the spectrum of fluctuations in the Cosmic Microwave Background to explaining the clustering of galaxies. But CDM theory is still at odds with the observed properties of galaxies on scales where the baryonic physics of gas cooling, star formation and feedback are important. Modeling these processes in the cosmological context is extremely difficult owing to the immense dynamic range and resolution needed, and requires the most advanced computational hardware available as well as new computing techniques and algorithms. The proposing team has developed smooth-particle hydrodynamics (SPH) codes addressing these processes at the galaxy scale, and plans to adapt their algorithms to new architectures in order extend them to cosmological volumes. In addition to moving their star formation and feedback algorithms from legacy message-passing interface (MPI) codes to a parallel language (CHARM++), they intend to implement faster algorithms that can achieve the needed dynamic range and take full advantage of the hundreds of thousands of processing cores on the Blue Waters system. The results will be processed by a parallel pipeline that creates simulated observations. The simulations will also be used in a program targeting science pre-majors from under-represented groups, introducing them to the use of computer simulations in astrophysics and encouraging them toward technically oriented majors.</AbstractNarration>
<MinAmdLetterDate>09/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1312913</AwardID>
<Investigator>
<FirstName>Laxmikant</FirstName>
<LastName>Kale</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laxmikant V Kale</PI_FULL_NAME>
<EmailAddress>kale@uiuc.edu</EmailAddress>
<PI_PHON>2172440094</PI_PHON>
<NSF_ID>000123469</NSF_ID>
<StartDate>09/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1217</Code>
<Text>EXTRAGALACTIC ASTRON &amp; COSMOLO</Text>
</ProgramElement>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramElement>
<ProgramReference>
<Code>1206</Code>
<Text>THEORETICAL &amp; COMPUTATIONAL ASTROPHYSICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~90000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-5a3cd9cd-231f-4958-95dd-5e43c6eca77d"> </span></p> <p dir="ltr"><span>Our models of structure formation on the largest scales in the Universe have been quite successful, explaining observations from the Cosmic Microwave Background to the distribution of galaxies seen in large surveys. &nbsp;Explaining the smaller structures is much more difficult because of more complicated dynamics, and the need to include more astrophysical processes in the modeling, e.g. star formation and supernovae.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Hence even performing this model required pushing the state-of-the-art in computational science. &nbsp;An idea of the difficulty can be realized from examining the first figure. &nbsp;Modeling this obviously inhomogeneous distribution of matter must be done on tens of thousands of processor cores. &nbsp;In the work reported on here, we used and enhanced our computer language designed for massively parallel systems, Charm++, to efficiently implement our modeling of astrophysics. &nbsp;Using Charm++, we were able to make effective use of over .5 million cores of the NSF Petascale computing facility, Blue Waters, on this highly inhomogeneous calculation. &nbsp;We have made these software improvements publicly available.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>We achieved scaling of our production simulation code to over 100,000 processor cores for these highly clustered simulations with a parallel efficiency of 50%. It resulted in a 3 fold improvement in execution time with multi-stepping over single-stepping. The improvement in execution time and parallel efficiency is primarily contributed by many improvements to the different stages of the simulation: dividing data into spatial domains, organizing the data, &nbsp;evenly balancing the work among processors, and the actual calculation of gravitational forces. To address the imbalance of work we used a combination of persistent hierarchical load balancing (across nodes) and transient load balancing (within a multi-processor node). </span></p> <p>&nbsp;</p> <p dir="ltr"><span>The evolution of modern high performance computers to collections of nodes, each with many processor cores, presents new opportunities for load balancing. &nbsp;Within-node load balancing can be realized in one of the two distinct ways. Firstly, using a parallel For loop construct provided by the Charm++ runtime system. And secondly, by using a parallel loop construct from a standard intra-node programming language, OpenMP, as Charm++ can interoperate with OpenMP. &nbsp;New runtime support in Charm++ was leveraged to deal with residual imbalance left after a global load balancing strategy migrates data across processors. &nbsp;The second figure demonstrates the success of this work.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The main astrophysical outcome of this work is a model for the brightness distribution of the galaxies seen at high redshift, corresponding to the early history of galaxy formation. &nbsp;Our models match surveys of high redshift galaxies performed with the Hubble Space Telescope (HST), but also predict a large population of galaxies too faint to be seen with HST. &nbsp;The existence of such a population, which might be confirmed with the next generation James Webb Space Telescope, could have significant consequences for the evolution of the inter-galactic gas.</span></p> <p><br /><span>The visual and scientific appeal of these work continued to attract good students into the field of computational astrophysics. &nbsp;At the University of Washington, the bulk of the funding was for a graduate student who has recently received her Ph.D. in Astronomy and now is employed as a postdoctoral fellow. &nbsp;Several early undergraduates were involved in the research through the University of Washington Pre-Major in Astronomy Program, which targets women and underrepresented minorities. &nbsp;One of the earlier participants in this research is now a graduating senior, and is applying to graduate Astronomy programs. &nbsp;At University of Illinois, one graduate student funded by this grant earned his PhD and has moved to industry. The second funded graduate student received her PhD and is a postdoctoral associate at Lawrence Livermore National Laboratory. </span></p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/22/2016<br>      Modified by: Laxmikant&nbsp;V&nbsp;Kale</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1312913/1312913_10275920_1482353463682_cosmogold--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1312913/1312913_10275920_1482353463682_cosmogold--rgov-800width.jpg" title="Figure 1: Dark Matter distribution on small scales"><img src="/por/images/Reports/POR/2016/1312913/1312913_10275920_1482353463682_cosmogold--rgov-66x44.jpg" alt="Figure 1: Dark Matter distribution on small scales"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Distribution of Dark matter on scales below 25 Megaparsecs.  The large cluster is roughly 10 times the mass of our Milky Way.</div> <div class="imageCredit">Fabio Governato and Thomas Quinn</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Laxmikant&nbsp;V&nbsp;Kale</div> <div class="imageTitle">Figure 1: Dark Matter distribution on small scales</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1312913/1312913_10275920_1482353686490_changacosmo25timeeffbw--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1312913/1312913_10275920_1482353686490_changacosmo25timeeffbw--rgov-800width.jpg" title="Figure 2: Speedup from Intranode Optimizations"><img src="/por/images/Reports/POR/2016/1312913/1312913_10275920_1482353686490_changacosmo25timeeffbw--rgov-66x44.jpg" alt="Figure 2: Speedup from Intranode Optimizations"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Wall clock times for computing gravity in a cosmological simulations as a function of the number of processor cores used.  The performance improvement of our intranode load balancing work can be seen from the decrease in the time to calculate gravity, particularly with many processors.</div> <div class="imageCredit">Harshitha Menon, Seonmyeong Bak, Lukasz Wesolowski, Gengbin Zheng, Pritish Jetley, Nitin Bhat</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Laxmikant&nbsp;V&nbsp;Kale</div> <div class="imageTitle">Figure 2: Speedup from Intranode Optimizations</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Our models of structure formation on the largest scales in the Universe have been quite successful, explaining observations from the Cosmic Microwave Background to the distribution of galaxies seen in large surveys.  Explaining the smaller structures is much more difficult because of more complicated dynamics, and the need to include more astrophysical processes in the modeling, e.g. star formation and supernovae.    Hence even performing this model required pushing the state-of-the-art in computational science.  An idea of the difficulty can be realized from examining the first figure.  Modeling this obviously inhomogeneous distribution of matter must be done on tens of thousands of processor cores.  In the work reported on here, we used and enhanced our computer language designed for massively parallel systems, Charm++, to efficiently implement our modeling of astrophysics.  Using Charm++, we were able to make effective use of over .5 million cores of the NSF Petascale computing facility, Blue Waters, on this highly inhomogeneous calculation.  We have made these software improvements publicly available.    We achieved scaling of our production simulation code to over 100,000 processor cores for these highly clustered simulations with a parallel efficiency of 50%. It resulted in a 3 fold improvement in execution time with multi-stepping over single-stepping. The improvement in execution time and parallel efficiency is primarily contributed by many improvements to the different stages of the simulation: dividing data into spatial domains, organizing the data,  evenly balancing the work among processors, and the actual calculation of gravitational forces. To address the imbalance of work we used a combination of persistent hierarchical load balancing (across nodes) and transient load balancing (within a multi-processor node).     The evolution of modern high performance computers to collections of nodes, each with many processor cores, presents new opportunities for load balancing.  Within-node load balancing can be realized in one of the two distinct ways. Firstly, using a parallel For loop construct provided by the Charm++ runtime system. And secondly, by using a parallel loop construct from a standard intra-node programming language, OpenMP, as Charm++ can interoperate with OpenMP.  New runtime support in Charm++ was leveraged to deal with residual imbalance left after a global load balancing strategy migrates data across processors.  The second figure demonstrates the success of this work.    The main astrophysical outcome of this work is a model for the brightness distribution of the galaxies seen at high redshift, corresponding to the early history of galaxy formation.  Our models match surveys of high redshift galaxies performed with the Hubble Space Telescope (HST), but also predict a large population of galaxies too faint to be seen with HST.  The existence of such a population, which might be confirmed with the next generation James Webb Space Telescope, could have significant consequences for the evolution of the inter-galactic gas.   The visual and scientific appeal of these work continued to attract good students into the field of computational astrophysics.  At the University of Washington, the bulk of the funding was for a graduate student who has recently received her Ph.D. in Astronomy and now is employed as a postdoctoral fellow.  Several early undergraduates were involved in the research through the University of Washington Pre-Major in Astronomy Program, which targets women and underrepresented minorities.  One of the earlier participants in this research is now a graduating senior, and is applying to graduate Astronomy programs.  At University of Illinois, one graduate student funded by this grant earned his PhD and has moved to industry. The second funded graduate student received her PhD and is a postdoctoral associate at Lawrence Livermore National Laboratory.              Last Modified: 12/22/2016       Submitted by: Laxmikant V Kale]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
