<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: First-Class Operating System Management of Computational Accelerators</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>455255.00</AwardTotalIntnAmount>
<AwardAmount>455255</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates operating system mechanisms to manage hardware accelerator resources in a safe, fair, and protected manner while maintaining high performance. Programmable vector processors including general-purpose graphical processing units (GP-GPUs) and other accelerators for encryption, compression, media transcoding, pattern matching, parsing, etc. are increasingly ubiquitous in computer systems. For the sake of safety and fairness, such accelerators must be managed by the operating system, but for the sake of performance, they must be accessible directly from user-level applications, without OS intervention. The conflict between these goals is exacerbated by the opacity of proprietary library/driver/hardware interfaces. This project seeks a balanced solution to these conflicting goals through: (1) an operating system resource management architecture that allows direct user-level access in the common case, but intercedes in the existing accelerator access path when necessary to delay and re-order requests; (2) a tool chain that uncovers hidden interface semantics required for resource management, together with a characterization of the information needed from vendors in the future; and (3) an integrated management and scheduling strategy across the full set of computational resources in a given system.&lt;br/&gt;&lt;br/&gt;By focusing on safe, fair, and efficient access to computational accelerators, the project aims to increase performance and power efficiency over a broad range of applications critical to today's digital economy and society. Broad dissemination is promoted through implementation in the Linux kernel, and open-source software release. Technology transfer is pursued through regular communication and collaboration with GPU industry vendors. Project research is integrated with education through curricular development and graduate student instruction.</AbstractNarration>
<MinAmdLetterDate>08/13/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/16/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319417</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Scott</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael L Scott</PI_FULL_NAME>
<EmailAddress>scott@cs.rochester.edu</EmailAddress>
<PI_PHON>5852757745</PI_PHON>
<NSF_ID>000343030</NSF_ID>
<StartDate>08/13/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kai</FirstName>
<LastName>Shen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kai Shen</PI_FULL_NAME>
<EmailAddress>kshen@cs.rochester.edu</EmailAddress>
<PI_PHON>5852755426</PI_PHON>
<NSF_ID>000489991</NSF_ID>
<StartDate>08/13/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041294109</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ROCHESTER</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041294109</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Rochester]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>146270226</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~455255</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Very fast I/O devices, networks, and computational accelerators are increasingly central to data center computing.&nbsp; Examples include high-end solid-state drives, Infiniband-class networks, and general-purpose graphical processing units (GPUs).&nbsp; Soon, terabyte-capacity nonvolatile memories are likely to be added to the list.&nbsp; Most of these devices have such high communication bandwidth that they cannot be kept busy by a single processor core; they are therefore designed with parallel ("multi-queue") interfaces that can be used concurrently -- and independently -- by applications running on separate cores.&nbsp; For such devices, the overhead of coordinating across cores (e.g., to make sure every application gets a fair share of overall bandwidth) is such a large fraction of total overhead that operating system (OS) designers have given up on fairness.</p> <p>In a similar vein, many devices now provide operations that are so fast -- on the order of tens of microseconds -- that the overhead of calling into the OS to ask for service is a significant fraction of the total operation latency.&nbsp; As a result, there is a growing trend toward accessing devices directly from user-level applications, bypassing the OS completely.</p> <p>The focus of this project was to put the OS "back in the loop" when accessing fast devices, so that it can fulfill its traditional responsibility for fair allocation of resources -- without imposing unacceptable overheads.&nbsp; The sponsored research made four principal contributions:</p> <p>(1) Black box inference techniques: these allow researchers to reverse engineer the (proprietary) interfaces of commercial GPUs, enabling experimentation with operating system and run-time library packages that manage the devices in ways not originally envisioned by the manufacturer.</p> <p>(2) A "disengaged" strategy for GPU scheduling: this allows applications to access the device unimpeded most of the time, but with the operating system intervening on an occasional basis to monitor usage and to throttle back applications that use more than their share of device capacity.</p> <p>(3) A new fair queueing system for multi-queue devices: this uses scalable concurrent data structures to track resource utilization across applications and cores, allowing OS-level software to enforce fairness without significantly impacting system-wide throughput.</p> <p>(4) Implementation techniques for protected user-level libraries: leveraging new hardware mechanisms available in mainstream processors, these allow a trusted library to be interposed between each application and its devices, without introducing the overhead of a system call on every operation.</p> <p>The overall result of these contributions has been a significant improvement in the performance, energy efficiency, and fairness of access to the I/O devices, networks, and computational accelerators of modern data centers.&nbsp; These improvements can in turn be expected to benefit the full range of data center applications, spanning commercial, non-profit, and scientific domains.</p> <p>Work on this project benefited greatly from collaboration with colleagues at Google.</p><br> <p>            Last Modified: 12/08/2018<br>      Modified by: Michael&nbsp;L&nbsp;Scott</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Very fast I/O devices, networks, and computational accelerators are increasingly central to data center computing.  Examples include high-end solid-state drives, Infiniband-class networks, and general-purpose graphical processing units (GPUs).  Soon, terabyte-capacity nonvolatile memories are likely to be added to the list.  Most of these devices have such high communication bandwidth that they cannot be kept busy by a single processor core; they are therefore designed with parallel ("multi-queue") interfaces that can be used concurrently -- and independently -- by applications running on separate cores.  For such devices, the overhead of coordinating across cores (e.g., to make sure every application gets a fair share of overall bandwidth) is such a large fraction of total overhead that operating system (OS) designers have given up on fairness.  In a similar vein, many devices now provide operations that are so fast -- on the order of tens of microseconds -- that the overhead of calling into the OS to ask for service is a significant fraction of the total operation latency.  As a result, there is a growing trend toward accessing devices directly from user-level applications, bypassing the OS completely.  The focus of this project was to put the OS "back in the loop" when accessing fast devices, so that it can fulfill its traditional responsibility for fair allocation of resources -- without imposing unacceptable overheads.  The sponsored research made four principal contributions:  (1) Black box inference techniques: these allow researchers to reverse engineer the (proprietary) interfaces of commercial GPUs, enabling experimentation with operating system and run-time library packages that manage the devices in ways not originally envisioned by the manufacturer.  (2) A "disengaged" strategy for GPU scheduling: this allows applications to access the device unimpeded most of the time, but with the operating system intervening on an occasional basis to monitor usage and to throttle back applications that use more than their share of device capacity.  (3) A new fair queueing system for multi-queue devices: this uses scalable concurrent data structures to track resource utilization across applications and cores, allowing OS-level software to enforce fairness without significantly impacting system-wide throughput.  (4) Implementation techniques for protected user-level libraries: leveraging new hardware mechanisms available in mainstream processors, these allow a trusted library to be interposed between each application and its devices, without introducing the overhead of a system call on every operation.  The overall result of these contributions has been a significant improvement in the performance, energy efficiency, and fairness of access to the I/O devices, networks, and computational accelerators of modern data centers.  These improvements can in turn be expected to benefit the full range of data center applications, spanning commercial, non-profit, and scientific domains.  Work on this project benefited greatly from collaboration with colleagues at Google.       Last Modified: 12/08/2018       Submitted by: Michael L Scott]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
