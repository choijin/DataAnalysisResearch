<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Understanding biological motion</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>536409.00</AwardTotalIntnAmount>
<AwardAmount>550705</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A major issue in the psychological sciences is how people can infer the intentions of others. Humans are remarkably adept at predicting the actions of other people and making inferences about their intention and goals. The present investigation examines how humans make such inferences from the physical movements of others. The work is guided by a computational theory of biological motion understanding that quantifies the action representations that allow people to make inferences in action recognition and prediction. The larger goal is to explain how perception and reasoning operate synergistically to infer hidden goals and intentions. &lt;br/&gt;&lt;br/&gt;The proposed research has broad impact in several domains. The inference capacity of most people exceeds that of today's best machine vision systems. For example, in the investigation of the bombing at the Boston marathon, extensive video from surveillance camera systems was available but it was the trained human eye that led to arrests. Human investigators scrutinized hundreds of hours of videos frame by frame and identified suspects who displayed suspicious behavioral patterns. Hence, understanding how humans make inferences and predictions about actions will play an important role in guiding the development of more advanced machine vision systems, useful in forensic sciences as well as many other real-world applications. In addition, individuals with autism or nonverbal learning disabilities often show difficulty in inferring the meaning of observed actions. Investigation of the key computational components underlying action understanding may potentially guide the development of behavioral interventions to facilitate compensatory strategies for understanding actions.</AbstractNarration>
<MinAmdLetterDate>04/15/2014</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1353391</AwardID>
<Investigator>
<FirstName>Hongjing</FirstName>
<LastName>Lu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hongjing Lu</PI_FULL_NAME>
<EmailAddress>hongjing@ucla.edu</EmailAddress>
<PI_PHON>3102062587</PI_PHON>
<NSF_ID>000498799</NSF_ID>
<StartDate>04/15/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951563</ZipCode>
<StreetAddress><![CDATA[Franz Hall, UCLA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1397</Code>
<Text>Cross-Directorate  Activities</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7298</Code>
<Text>International Research Collab</Text>
</ProgramElement>
<ProgramReference>
<Code>5946</Code>
<Text>UNITED KINGDOM</Text>
</ProgramReference>
<ProgramReference>
<Code>5979</Code>
<Text>Europe and Eurasia</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>7298</Code>
<Text>COLLABORATIVE RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<ProgramReference>
<Code>8819</Code>
<Text>Forensic Science</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~536409</FUND_OBLG>
<FUND_OBLG>2016~14296</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>People move their bodies constantly to discover the world and interact with others. Our physical actions are critical in determining how we think and reason about the world. But it remains unclear how actions are translated into understanding. This project systematically examined how humans make sophisticated inferences about actions (e.g., predict future actions, infer intentions of others from observed actions). Our goal was to identify the key computational components that connect action perception with reasoning about observed actions.&nbsp; With the support of the current grant, our research group published 22 journal articles and 3 conference proceedings papers, as well as supporting about 15 conference presentations. The funded project also provided training opportunities for 3 postdoctoral researchers, 3 graduate students, and about 50 undergraduate students who were involved in research activities.</p> <p>The project produced several major results that our research group has reported. First, meaningful social interactions receive priority in conscious perception. In other words, the human visual system amplifies socially-relevant sensory information at a very early stage of processing, and actively promotes social interactions to consciousness. We identified a set of important visual features that humans use to recognize different types of interactive actions. Second, humans perceive actions as more than mere body movements. People spontaneously make sense of the causal relations inherent in human actions. Our studies showed that humans use causality as a fundamental constraint that guides action perception and inference.</p> <p>Based on our empirical findings, we developed a new computational model that can account for how humans perceive interactions based on motion trajectories. This hierarchical Bayesian model makes use of the critical features identified in our empirical work. The hierarchical framework developed in the modeling work is likely to aid in developing more sophisticated computer systems, which might be able to identify suspicious actions captured in surveillance videos.</p> <p>Finally, our research group obtained both behavioral and neural evidence showing individual differences in action perception. Specifically, the ability to process action information in an adaptive manner, and to infer social intentions from actions, varies with the degree of autistic traits for people within the typically-developing population. These findings from the project shed light on the potential mechanisms underlying autism. In future work, the paradigms developed in our project could be adopted to enable transformative research on autism.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/29/2020<br>      Modified by: Hongjing&nbsp;Lu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ People move their bodies constantly to discover the world and interact with others. Our physical actions are critical in determining how we think and reason about the world. But it remains unclear how actions are translated into understanding. This project systematically examined how humans make sophisticated inferences about actions (e.g., predict future actions, infer intentions of others from observed actions). Our goal was to identify the key computational components that connect action perception with reasoning about observed actions.  With the support of the current grant, our research group published 22 journal articles and 3 conference proceedings papers, as well as supporting about 15 conference presentations. The funded project also provided training opportunities for 3 postdoctoral researchers, 3 graduate students, and about 50 undergraduate students who were involved in research activities.  The project produced several major results that our research group has reported. First, meaningful social interactions receive priority in conscious perception. In other words, the human visual system amplifies socially-relevant sensory information at a very early stage of processing, and actively promotes social interactions to consciousness. We identified a set of important visual features that humans use to recognize different types of interactive actions. Second, humans perceive actions as more than mere body movements. People spontaneously make sense of the causal relations inherent in human actions. Our studies showed that humans use causality as a fundamental constraint that guides action perception and inference.  Based on our empirical findings, we developed a new computational model that can account for how humans perceive interactions based on motion trajectories. This hierarchical Bayesian model makes use of the critical features identified in our empirical work. The hierarchical framework developed in the modeling work is likely to aid in developing more sophisticated computer systems, which might be able to identify suspicious actions captured in surveillance videos.  Finally, our research group obtained both behavioral and neural evidence showing individual differences in action perception. Specifically, the ability to process action information in an adaptive manner, and to infer social intentions from actions, varies with the degree of autistic traits for people within the typically-developing population. These findings from the project shed light on the potential mechanisms underlying autism. In future work, the paradigms developed in our project could be adopted to enable transformative research on autism.          Last Modified: 01/29/2020       Submitted by: Hongjing Lu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
