<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Decomposing Interviewer Variance in Standardized and Conversational Interviewing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>374151.00</AwardTotalIntnAmount>
<AwardAmount>374151</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Standardized interviewing procedures require survey interviewers to read questions as worded and provide only neutral or non-directive probes in response to questions from survey respondents.  Even though many major surveys in the government, non-profit, and private sectors use standardized interviewing in an effort to minimize the effects of interviewers on data quality, a substantial body of research has indicated that interviewers using standardized interviewing still influence the responses provided by individuals.  This variability among interviewers (or interviewer variance) in the types of survey responses collected reduces the precision of survey estimates and therefore has direct cost implications for survey data collection.  Conversational interviewing is known to handle respondent clarification requests in a more effective manner.  Interviewers are trained to read questions as worded initially and then say whatever is required to help respondents understand the questions.  Despite existing research showing that conversational interviewing produces noticeable decreases in the measurement error bias of survey estimates, survey researchers (and government agencies in particular) have been hesitant to employ it in practice, in part because of increased questionnaire administration time but also due to the fear of increased interviewer effects on the survey data due to the conversational style.  This research project will compare the interviewer variance, bias, and mean squared error arising in a variety of survey estimates from these two face-to-face interviewing techniques.  It will decompose the total interviewer variance introduced by each technique into measurement error variance (i.e., variance among interviewers in systematic measurement errors) and nonresponse error variance (i.e., variance among interviewers in the types of individuals recruited for the survey).  To meet these research objectives, this study will select a large random sample of persons from a unique economic database that contains known values for selected economic characteristics of interest.  The researchers will assign random subsets of this sample to professional interviewers who have been randomly assigned to receive training in one of the two interviewing techniques.  The two groups of interviewers will then administer a face-to-face survey to their assigned subsamples, collecting information on their economic characteristics.  Analyses of the collected data will employ innovative statistical modeling techniques that enable comparisons of the interviewer variance, bias, and mean squared error values for a variety of economic survey estimates across the two groups of interviewers as well as decompositions of the total interviewer variance in each group into the two aforementioned sources of variance among interviewers.  These analyses will provide survey researchers with the first empirical evidence of differences in these two techniques in terms of the overall quality of survey estimates and will uncover sources of the interviewer effects that can arise when using the two interviewing styles.&lt;br/&gt;&lt;br/&gt;As a scientific field, survey methodology, which studies the science of survey data collection, is still relatively nascent, but billions of dollars in public resources are dedicated to survey data collection every year.  This study will provide survey researchers with a more complete set of empirical evidence that will enable informed decisions about the face-to-face interviewing style that will yield survey estimates with the highest overall quality.  The resulting knowledge regarding which interviewing technique produces higher-quality estimates therefore will lead to higher-quality information being collected in surveys that employ face-to-face interviewing and higher-quality decisions being made by policy makers who use survey data.</AbstractNarration>
<MinAmdLetterDate>09/07/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1324689</AwardID>
<Investigator>
<FirstName>Frederick</FirstName>
<LastName>Conrad</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Frederick G Conrad</PI_FULL_NAME>
<EmailAddress>fconrad@umich.edu</EmailAddress>
<PI_PHON>7349361019</PI_PHON>
<NSF_ID>000163076</NSF_ID>
<StartDate>09/07/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brady T.</FirstName>
<LastName>West</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brady T. West</PI_FULL_NAME>
<EmailAddress>bwest@umich.edu</EmailAddress>
<PI_PHON>7346474615</PI_PHON>
<NSF_ID>000614418</NSF_ID>
<StartDate>09/07/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName>Ann Arbor</CityName>
<StateCode>MI</StateCode>
<ZipCode>481091271</ZipCode>
<StreetAddress><![CDATA[3003 South State Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~374151</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary objectives of this NSF-MMS-funded project were to:</p> <p>1) Collect original data from a large national sample of employed adults in Germany, using two different face-to-face survey interviewing techniques [<em>conversational interviewing (CI)</em>, where survey interviewers initially ask questions in a standardized fashion and are then given the flexibility to clarify ambiguous concepts or respondent confusion by saying whatever is needed to improve respondent comprehension, and <em>standardized interviewing (SI)</em>, where interviewers are only allowed to provide neutral probes (e.g., "Let me repeat the question...") in response to respondent confusion];</p> <p>2) Compare survey responses obtained using the two different interviewing techniques (CI and SI) in terms of the accuracy of the measures collected and the interviewer variance in the distributions of survey responses; and</p> <p>3) Decompose the interviewer variance introduced by the two techniques into nonresponse error variance and measurement error variance among interviewers, using validation information available from administrative data on employment and benefit receipt in Germany.</p> <p>From the perspective of <strong>intellectual merit</strong>, this research has examined whether the hesitation of survey researchers to employ CI in practice is empirically justified. We found that CI rarely increased the interviewer variance in survey responses to factual questions relative to SI, while also reducing response bias (as expected given the extant literature). These findings, which were <a href="http://onlinelibrary.wiley.com/doi/10.1111/rssa.12255/full">published in the <em>Journal of the Royal Statistical Society (Series A)</em></a>, provide empirical evidence that should mitigate concerns about employing this technique in practice. We have also found additional evidence of nonresponse error variance among interviewers contributing to interviewer variance in CI, where different interviewers may recruit individuals with different ages. While measurement error variance among interviewers was evident for both techniques, its contribution tended to be larger for CI, motivating studies of potential differences among interviewers in the implementation of CI. Given these results, which can be found in a forthcoming issue of the <em>Journal of Survey Statistics and Methodology</em>, we conducted in-depth analyses of what happens during CI by carefully analyzing real audio recordings of interviewer-respondent interactions in the CI and SI conditions.&nbsp;</p> <p>The results of our analyses of the audio recordings suggested that while conversational interviewers do make use of their flexibility to provide additional information to respondents when needed (e.g., to clarify any confusion that respondents may have), they may provide additional information when it is *not* needed, suggesting room for improvement in terms of the efficiency of CI, and they also tend to use neutral probes as much as standardized interviewers. These results, which will be published in a forthcoming issue of <em>Field Methods</em>, provide survey researchers with new knowledge about these interactions and point to possible training solutions for future surveys employing CI.</p> <p>In addition to these publications, we have produced a comprehensive synthesis of nearly 100 years of research on interviewer effects, focusing extensively on differences in interviewer behaviors (such as interviewing techniques) that could lead to interviewer effects, and presenting a research agenda for future work in this area. <a href="https://academic.oup.com/jssam/article/5/2/175/2452318/Explaining-Interviewer-Effects-A-Research">This article can be found in the <em>Journal of Survey Statistics and Methodology</em></a>. Finally, we have broken new ground in studying the impacts of interviewer effects on regression coefficients and related methods of accounting for these effects. Our work in this area is currently under second review for publication in the <em>Journal of Survey Statistics and Methodology</em>.</p> <p>In terms of the <strong>broader impacts</strong> of this work, the results from this project will allow survey researchers to make informed decisions about interviewing techniques that will optimize the quality of survey estimates while keeping costs low. Our results regarding the relative contributions of nonresponse error variance and measurement error variance among interviewers to total interviewer variance also have important implications for interviewer training. Our finding that interviewer variance in CI may arise from nonresponse error variance among interviewers in the ages of recruited respondents should encourage survey managers to carefully study the features of individuals being recruited by different interviewers as a possible source of variance in survey estimates. Three University of Michigan graduate students (Felicitas Mittereder, Jen Durow and Micha Fischer) have produced first- or co-authored articles from this work, and helped to present the work at 10 different national and international conferences on survey research methods.</p> <p><a href="https://www.xing-events.com/itsew2017.html;jsessionid=A794DBD485709036C5661CA615F82D73.amiando?page=1412021">We organized an international workshop in Nuremberg, Germany</a> where we disseminated our results and invited a panel of survey managers in Europe to discuss both the merits and drawbacks of these alternative interviewing techniques. We also disseminated our results at a workshop sponsored by the U.S. Bureau of Labor Statistics. Finally, we have incorporated results from this project into teaching material for the Joint Program in Survey Methodology (JPSM) and the Michigan Program in Survey Methodology (MPSM), and also into a <a href="https://www.coursera.org/learn/questionnaire-design">Coursera MOOC taught by two of our investigators (Dr. Kreuter and Dr. Conrad) on questionnaire design</a>.</p> <p>Contact: Brady T. West (bwest@umich.edu)</p><br> <p>            Last Modified: 09/19/2017<br>      Modified by: Brady T.&nbsp;West</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary objectives of this NSF-MMS-funded project were to:  1) Collect original data from a large national sample of employed adults in Germany, using two different face-to-face survey interviewing techniques [conversational interviewing (CI), where survey interviewers initially ask questions in a standardized fashion and are then given the flexibility to clarify ambiguous concepts or respondent confusion by saying whatever is needed to improve respondent comprehension, and standardized interviewing (SI), where interviewers are only allowed to provide neutral probes (e.g., "Let me repeat the question...") in response to respondent confusion];  2) Compare survey responses obtained using the two different interviewing techniques (CI and SI) in terms of the accuracy of the measures collected and the interviewer variance in the distributions of survey responses; and  3) Decompose the interviewer variance introduced by the two techniques into nonresponse error variance and measurement error variance among interviewers, using validation information available from administrative data on employment and benefit receipt in Germany.  From the perspective of intellectual merit, this research has examined whether the hesitation of survey researchers to employ CI in practice is empirically justified. We found that CI rarely increased the interviewer variance in survey responses to factual questions relative to SI, while also reducing response bias (as expected given the extant literature). These findings, which were published in the Journal of the Royal Statistical Society (Series A), provide empirical evidence that should mitigate concerns about employing this technique in practice. We have also found additional evidence of nonresponse error variance among interviewers contributing to interviewer variance in CI, where different interviewers may recruit individuals with different ages. While measurement error variance among interviewers was evident for both techniques, its contribution tended to be larger for CI, motivating studies of potential differences among interviewers in the implementation of CI. Given these results, which can be found in a forthcoming issue of the Journal of Survey Statistics and Methodology, we conducted in-depth analyses of what happens during CI by carefully analyzing real audio recordings of interviewer-respondent interactions in the CI and SI conditions.   The results of our analyses of the audio recordings suggested that while conversational interviewers do make use of their flexibility to provide additional information to respondents when needed (e.g., to clarify any confusion that respondents may have), they may provide additional information when it is *not* needed, suggesting room for improvement in terms of the efficiency of CI, and they also tend to use neutral probes as much as standardized interviewers. These results, which will be published in a forthcoming issue of Field Methods, provide survey researchers with new knowledge about these interactions and point to possible training solutions for future surveys employing CI.  In addition to these publications, we have produced a comprehensive synthesis of nearly 100 years of research on interviewer effects, focusing extensively on differences in interviewer behaviors (such as interviewing techniques) that could lead to interviewer effects, and presenting a research agenda for future work in this area. This article can be found in the Journal of Survey Statistics and Methodology. Finally, we have broken new ground in studying the impacts of interviewer effects on regression coefficients and related methods of accounting for these effects. Our work in this area is currently under second review for publication in the Journal of Survey Statistics and Methodology.  In terms of the broader impacts of this work, the results from this project will allow survey researchers to make informed decisions about interviewing techniques that will optimize the quality of survey estimates while keeping costs low. Our results regarding the relative contributions of nonresponse error variance and measurement error variance among interviewers to total interviewer variance also have important implications for interviewer training. Our finding that interviewer variance in CI may arise from nonresponse error variance among interviewers in the ages of recruited respondents should encourage survey managers to carefully study the features of individuals being recruited by different interviewers as a possible source of variance in survey estimates. Three University of Michigan graduate students (Felicitas Mittereder, Jen Durow and Micha Fischer) have produced first- or co-authored articles from this work, and helped to present the work at 10 different national and international conferences on survey research methods.  We organized an international workshop in Nuremberg, Germany where we disseminated our results and invited a panel of survey managers in Europe to discuss both the merits and drawbacks of these alternative interviewing techniques. We also disseminated our results at a workshop sponsored by the U.S. Bureau of Labor Statistics. Finally, we have incorporated results from this project into teaching material for the Joint Program in Survey Methodology (JPSM) and the Michigan Program in Survey Methodology (MPSM), and also into a Coursera MOOC taught by two of our investigators (Dr. Kreuter and Dr. Conrad) on questionnaire design.  Contact: Brady T. West (bwest@umich.edu)       Last Modified: 09/19/2017       Submitted by: Brady T. West]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
