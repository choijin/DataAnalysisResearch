<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Web Information Extraction: Integration and Scaling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>550000.00</AwardTotalIntnAmount>
<AwardAmount>563125</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hector Munoz-Avila</SignBlockName>
<PO_EMAI>hmunoz@nsf.gov</PO_EMAI>
<PO_PHON>7032924481</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project studies Web Information Extraction (WIE), the task of automatically extracting computer-understandable knowledge bases (KBs) from the World Wide Web.  The project addresses two key challenges in WIE. First, many different teams in academia and industry are pursuing WIE, but they lack methods for combining their KBs into a more powerful whole.  This project explores how to integrate knowledge automatically across WIE systems and approaches. Secondly, a long-standing goal for WIE is to construct systems that can scale to billions of facts, by continually improving themselves over time. This project is investigating new methods that continually optimize a WIE system with limited human intervention. The project's goal of scaling and integrating WIE systems promises to address needs in the research community, the computing industry, and the public. Methods that allow different WIE systems to seamlessly exchange knowledge could dramatically hasten the progress of Web extraction efforts currently underway in academia and industry. For the public, advances in Web extraction promise to enable improved search engines that can assist users with tasks and answer complex questions. Further, through application prototypes, the project will provide public-facing information retrieval tools that promise to help users retrieve, understand, and analyze the Web's knowledge more rapidly. The project's research is also integrated with an education plan that includes outreach to underrepresented groups.&lt;br/&gt;&lt;br/&gt;The technical solutions pursued in the project utilize probability distributions over natural language. For the integration challenge, the project is developing new Application Programming Interfaces (APIs) that leverage the expressiveness of natural language to automatically integrate current and future WIE systems, even when the systems extract from different types of corpora and represent knowledge in different ways.  For the scaling challenge, the project is developing ways to continually optimize new Statistical Language Models (SLMs) over text on the Web. The project investigates the SLM approach for WIE theoretically, asking what types of knowledge different SLMs can encode, and how much text is required to obtain the knowledge. Further, the project introduces new SLM capabilities, including methods for scaling to larger corpora and more semantic classes, and novel models that incorporate collocations, quantitative attributes, sense disambiguation, and actively-selected human input. The project web site (http://websail.eecs.northwestern.edu/wie/) provides additional information and access to results, including software, corpora, and evaluation data sets.</AbstractNarration>
<MinAmdLetterDate>07/29/2014</MinAmdLetterDate>
<MaxAmdLetterDate>07/05/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1351029</AwardID>
<Investigator>
<FirstName>Douglas</FirstName>
<LastName>Downey</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Douglas C Downey</PI_FULL_NAME>
<EmailAddress>dougd@allenai.org</EmailAddress>
<PI_PHON>8474913710</PI_PHON>
<NSF_ID>000534948</NSF_ID>
<StartDate>07/29/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Evanston</CityName>
<StateCode>IL</StateCode>
<ZipCode>602083109</ZipCode>
<StreetAddress><![CDATA[2145 Sheridan Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~108376</FUND_OBLG>
<FUND_OBLG>2015~105347</FUND_OBLG>
<FUND_OBLG>2016~121759</FUND_OBLG>
<FUND_OBLG>2017~112061</FUND_OBLG>
<FUND_OBLG>2018~115582</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our project was aimed at Web Information Extraction (WIE), the task of turning the Web's unstructured content into machine-processible knowledge.&nbsp; By making more of the Web's content understandable to machines, advances in the quality and scope of WIE could lead to exciting capabilities like improved Web search, automatic question answering, and better virtual assistants.&nbsp; The research supported in whole or in part by this award led to improvements in the quality and efficiency of WIE compared to prior art, as described below.&nbsp; This work was reported in more than a dozen research publications in reputable venues, and also contributed significantly to student training: eight PhD students participated in the work, with seven graduating during the award period, and the project also involved several undergraduate researchers.&nbsp; We released multiple resources for the research community, including open-source code bases, public prototypes, and data sets.</p> <p>Our technical approach used language models (LMs)---systems that estimate the probabilities of sequences of words---to create new ways to extract and integrate information using less human intervention.&nbsp; We also studied whether there are fundamental limitations of language model technology for extracting and representing knowledge.&nbsp; Our contributions, detailed below, fall into three primary areas: new WIE techniques, new methods for scaling-up WIE to large text data and models, and new analyses of LM limitations.</p> <p>Our new WIE techniques targeted both extracting information and integrating the extracted information with existing knowledge graphs.&nbsp; First, one rich source of knowledge on the Web is tabular data, such as lists of countries by GDP or companies by market cap.&nbsp; We developed a new integration technique that disambiguated the senses mentions of concepts in tables to their corresponding entries in a knowledge graph, which outperformed the prior state-of-the-art.&nbsp; Next, language modeling techniques are known to capture syntax and semantics of terms in their vector representations (so-called "word embeddings"), but exactly what information the embeddings capture is difficult to discern; we introduced a novel task called 'definition modeling' that attempts to make the information captured in an embedding explicit by generating a natural language definition of the term from the embedding.&nbsp; Selected examples of definitions automatically generated by our approach are shown in the attached image.&nbsp; We also built new extraction methods for other important challenges in artificial intelligence, targeting both commonsense properties of objects (e.g., typical size or tensile strength) and senses of entities in text (e.g. whether Chicago refers to a city or a film), even when the target senses are unknown in advance.</p> <p>To cost-effectively apply WIE methods in practice, they need to be computationally efficient and require only limited human input.&nbsp; We introduced a new model for automatically discovering large topic hierarchies from documents (schematically illustrated in the second image) that improved efficiency over previous work.&nbsp; We also developed new methods for scaling up neural language models, including a technique that can improve LMs trained on limited data by integrating efficient-to-acquire phrase statistics from larger text datasets, and a method for selecting the most informative training texts for a model.&nbsp; We also studied more effective ways to incorporate selected human input, experimenting with "active learning" (in which the machine learner helps direct its own learning) in the context of topic models, hierarchically-structured data, and common sense.&nbsp; Further, we introduced a way for researchers to better pool expensive-to-compute LM resources, by developing a platform for sharing word embeddings.</p> <p>Neural language models are the dominant paradigm in information extraction today; we studied whether this approach had fundamental limitations that need to be addressed before the models can deliver on their promise.&nbsp; We found that in the limited-training setting, the models struggle with certain concepts (like spatial and numeric relationships) that are second-nature to humans, and also that the models have a theoretical limitation, observed empirically in smaller models, that can prevent some words from ever being assigned high probability in any context (which we term the 'stolen probability' effect).&nbsp; Finally, we also observed that despite the success of large neural language models on certain common sense tasks, annotators who can interact with the models can relatively easily find model weaknesses to compose questions that stump the models, a design approach we used for constructing our CODAH commonsense data set.</p><br> <p>            Last Modified: 01/01/2021<br>      Modified by: Douglas&nbsp;C&nbsp;Downey</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1351029/1351029_10324822_1609302472930_def_model_examples--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1351029/1351029_10324822_1609302472930_def_model_examples--rgov-800width.jpg" title="Definition Modeling Examples"><img src="/por/images/Reports/POR/2020/1351029/1351029_10324822_1609302472930_def_model_examples--rgov-66x44.jpg" alt="Definition Modeling Examples"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Selected high-quality examples of definitions output by our definition model, from (Noraset et al., 2017)</div> <div class="imageCredit">Thanapon Noraset et al.</div> <div class="imageSubmitted">Douglas&nbsp;C&nbsp;Downey</div> <div class="imageTitle">Definition Modeling Examples</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1351029/1351029_10324822_1609303043849_sbt--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1351029/1351029_10324822_1609303043849_sbt--rgov-800width.jpg" title="Sparse Backoff Tree Example"><img src="/por/images/Reports/POR/2020/1351029/1351029_10324822_1609303043849_sbt--rgov-66x44.jpg" alt="Sparse Backoff Tree Example"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A schematic example of the sparse backoff tree data structure that represents a distribution over discrete latent variables (such as topics).  The numbered nodes are topics, lambdas and taus represent descendents backoff and total probabilities, respectively.  See ( Downey et al., 2015) for details.</div> <div class="imageCredit">Doug Downey et al.</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Douglas&nbsp;C&nbsp;Downey</div> <div class="imageTitle">Sparse Backoff Tree Example</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our project was aimed at Web Information Extraction (WIE), the task of turning the Web's unstructured content into machine-processible knowledge.  By making more of the Web's content understandable to machines, advances in the quality and scope of WIE could lead to exciting capabilities like improved Web search, automatic question answering, and better virtual assistants.  The research supported in whole or in part by this award led to improvements in the quality and efficiency of WIE compared to prior art, as described below.  This work was reported in more than a dozen research publications in reputable venues, and also contributed significantly to student training: eight PhD students participated in the work, with seven graduating during the award period, and the project also involved several undergraduate researchers.  We released multiple resources for the research community, including open-source code bases, public prototypes, and data sets.  Our technical approach used language models (LMs)---systems that estimate the probabilities of sequences of words---to create new ways to extract and integrate information using less human intervention.  We also studied whether there are fundamental limitations of language model technology for extracting and representing knowledge.  Our contributions, detailed below, fall into three primary areas: new WIE techniques, new methods for scaling-up WIE to large text data and models, and new analyses of LM limitations.  Our new WIE techniques targeted both extracting information and integrating the extracted information with existing knowledge graphs.  First, one rich source of knowledge on the Web is tabular data, such as lists of countries by GDP or companies by market cap.  We developed a new integration technique that disambiguated the senses mentions of concepts in tables to their corresponding entries in a knowledge graph, which outperformed the prior state-of-the-art.  Next, language modeling techniques are known to capture syntax and semantics of terms in their vector representations (so-called "word embeddings"), but exactly what information the embeddings capture is difficult to discern; we introduced a novel task called 'definition modeling' that attempts to make the information captured in an embedding explicit by generating a natural language definition of the term from the embedding.  Selected examples of definitions automatically generated by our approach are shown in the attached image.  We also built new extraction methods for other important challenges in artificial intelligence, targeting both commonsense properties of objects (e.g., typical size or tensile strength) and senses of entities in text (e.g. whether Chicago refers to a city or a film), even when the target senses are unknown in advance.  To cost-effectively apply WIE methods in practice, they need to be computationally efficient and require only limited human input.  We introduced a new model for automatically discovering large topic hierarchies from documents (schematically illustrated in the second image) that improved efficiency over previous work.  We also developed new methods for scaling up neural language models, including a technique that can improve LMs trained on limited data by integrating efficient-to-acquire phrase statistics from larger text datasets, and a method for selecting the most informative training texts for a model.  We also studied more effective ways to incorporate selected human input, experimenting with "active learning" (in which the machine learner helps direct its own learning) in the context of topic models, hierarchically-structured data, and common sense.  Further, we introduced a way for researchers to better pool expensive-to-compute LM resources, by developing a platform for sharing word embeddings.  Neural language models are the dominant paradigm in information extraction today; we studied whether this approach had fundamental limitations that need to be addressed before the models can deliver on their promise.  We found that in the limited-training setting, the models struggle with certain concepts (like spatial and numeric relationships) that are second-nature to humans, and also that the models have a theoretical limitation, observed empirically in smaller models, that can prevent some words from ever being assigned high probability in any context (which we term the 'stolen probability' effect).  Finally, we also observed that despite the success of large neural language models on certain common sense tasks, annotators who can interact with the models can relatively easily find model weaknesses to compose questions that stump the models, a design approach we used for constructing our CODAH commonsense data set.       Last Modified: 01/01/2021       Submitted by: Douglas C Downey]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
