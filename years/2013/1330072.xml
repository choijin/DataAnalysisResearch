<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Breakthrough: Reinforcement Learning Algorithms for Cyber-Physical Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>499760.00</AwardTotalIntnAmount>
<AwardAmount>499760</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Corman</SignBlockName>
<PO_EMAI>dcorman@nsf.gov</PO_EMAI>
<PO_PHON>7032928754</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project investigates new reinforcement learning algorithms to enable long-term real-time autonomous learning by cyber-physical systems (CPS). The complexity of CPS makes hand-programming safe and efficient controllers for them difficult. For CPS to meet their potential, they need methods that enable them to learn and adapt to novel situations that they were not programmed for. Reinforcement learning (RL) is a paradigm for learning sequential decision making processes with potential for solving this problem. However, existing RL algorithms do not meet all of the requirements of learning in CPS.  Efficacy of the new algorithms for CPS is evaluated in the context of smart buildings and autonomous vehicles.&lt;br/&gt;&lt;br/&gt;Cyber-physical systems (CPS) have the potential to revolutionize society by enabling smart buildings, transportation, medical technology, and electric grids. Success of this project could lead to a new generation of CPS that are capable of adapting to their situation and improving their performance autonomously over time. In addition to the traditional methods of dissemination, this project will develop and release open-source code implementing the new reinforcement learning algorithms.  Education and outreach activities associated with the project include a Freshman Research Initiative course, participation in  a UT Austin annual open house that draws in many underrepresented minorities to interest the public in computer science and science in general, and the department's annual summer school for high school girls called First Bytes.</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1330072</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Stone</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter H Stone</PI_FULL_NAME>
<EmailAddress>pstone@cs.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000156504</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121532</ZipCode>
<StreetAddress><![CDATA[101 E. 27th Street, Suite 5.300]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>7918</Code>
<Text>CYBER-PHYSICAL SYSTEMS (CPS)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~499760</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Reinforcement learning (RL) is a paradigm for learning sequential <br />decision making processes and could solve the problems of learning and <br />adaptation for CPS.&nbsp; In this research, we identifie seven key <br />properties of an RL algorithm that would make it generally applicable <br />to a broad range of CPS's: <br />&nbsp; <br />1. The algorithm must learn in very few samples (which may be <br />expensive or time-consuming). <br />&nbsp; <br />2. It must learn good policies even with unknown sensor or actuator <br />delays (i.e. selecting an action may not affect the environment <br />instantaneously). <br />&nbsp; <br />3. It must learn tasks with continuous state representations. <br />&nbsp; <br />4. It must handle multi-dimensional continuous actions. <br />&nbsp; <br />5. It must be computationally efficient enough to select actions <br />continually in real time. <br />&nbsp; <br />6. It must reason about and take advantage of the hierarchical <br />structure present in many tasks. <br />&nbsp; <br />7. It must reason about the agents who may be using it as well as <br />other CPS's that it must work with. <br />&nbsp; <br />Although prior RL algorithms successfully address one or more of these <br />challenges, no existing algorithm addressed all seven of them. In this <br />project, we helped to bridge this gap by developin novel reinforcement <br />learning algorithms that enable long-term real-time autonomous <br />learning by a broad range of cyber-physical systems.</p> <p>Specifically, we developed these new RL algorithms around an existing <br />algorithm, called TEXPLORE, which is sample-efficient, handles <br />continuous state and actuator delays, and acts in real-time <br />(Properties 1, 2, 3 and 5). From this algorithm, we achieved theh <br />project goals by successfully executing the following four steps in <br />order to develop new RL algorithms for autonomous learning on CPS's. <br />&nbsp; <br />1. We Extended TEXPLORE to handle multi-dimensional continuous actions. <br />&nbsp; <br />2. We Extended TEXPLORE to discover and reason about hierarchies and task <br />decomposition. <br />&nbsp; <br />3. We Incorporated explicit reasoning about other agents such as other <br />users and systems that the CPS must deal with. <br />&nbsp; <br />4. We implemented and tested the algorithms on multiple domains. <br />&nbsp; <br />The research addressed two of the CPS Research Target Areas: the <br />Science of CPS's and Technology for CPS's. RL algorithms present a new <br />way to model and control CPS's that bridge the gap between computation <br />and control, while enabling CPS's to learn, inter-operate, and handle <br />uncertainty.&nbsp;&nbsp; <br />&nbsp;<br /><br /><br /></p><br> <p>            Last Modified: 11/05/2017<br>      Modified by: Peter&nbsp;H&nbsp;Stone</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Reinforcement learning (RL) is a paradigm for learning sequential  decision making processes and could solve the problems of learning and  adaptation for CPS.  In this research, we identifie seven key  properties of an RL algorithm that would make it generally applicable  to a broad range of CPS's:     1. The algorithm must learn in very few samples (which may be  expensive or time-consuming).     2. It must learn good policies even with unknown sensor or actuator  delays (i.e. selecting an action may not affect the environment  instantaneously).     3. It must learn tasks with continuous state representations.     4. It must handle multi-dimensional continuous actions.     5. It must be computationally efficient enough to select actions  continually in real time.     6. It must reason about and take advantage of the hierarchical  structure present in many tasks.     7. It must reason about the agents who may be using it as well as  other CPS's that it must work with.     Although prior RL algorithms successfully address one or more of these  challenges, no existing algorithm addressed all seven of them. In this  project, we helped to bridge this gap by developin novel reinforcement  learning algorithms that enable long-term real-time autonomous  learning by a broad range of cyber-physical systems.  Specifically, we developed these new RL algorithms around an existing  algorithm, called TEXPLORE, which is sample-efficient, handles  continuous state and actuator delays, and acts in real-time  (Properties 1, 2, 3 and 5). From this algorithm, we achieved theh  project goals by successfully executing the following four steps in  order to develop new RL algorithms for autonomous learning on CPS's.     1. We Extended TEXPLORE to handle multi-dimensional continuous actions.     2. We Extended TEXPLORE to discover and reason about hierarchies and task  decomposition.     3. We Incorporated explicit reasoning about other agents such as other  users and systems that the CPS must deal with.     4. We implemented and tested the algorithms on multiple domains.     The research addressed two of the CPS Research Target Areas: the  Science of CPS's and Technology for CPS's. RL algorithms present a new  way to model and control CPS's that bridge the gap between computation  and control, while enabling CPS's to learn, inter-operate, and handle  uncertainty.               Last Modified: 11/05/2017       Submitted by: Peter H Stone]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
