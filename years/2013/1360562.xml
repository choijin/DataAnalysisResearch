<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Hybrid 4-Dimensional Augmented Reality Environments for Ubiquitous Markerless Context-Aware AEC/FM Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>273840.00</AwardTotalIntnAmount>
<AwardAmount>273840</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dennis Wenger</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The objective of this project is to test whether a framework proposed by the PIs can in near real-time read, write, and receive feedback from a model which fuses pictures from mobile devices and Building Information Models for the purpose of providing ubiquitous and marker-less contextual awareness for Architecture/ Engineering/ Construction and  Facility Management (AEC/FM) applications. According to the framework, field personnel can use mobile devices to take pictures that include specific project elements (e.g., column), touch or click on the elements in the image, and be presented with (or be able to add) a detailed list of information, such as architectural/structural plan related to the physical elements. The mobile device can use onboard GPS and other sensors to perform a rough calculation of the device's field-of-view and location. Initial image processing is done on the mobile device to extract and send feature points/descriptors, field-of-view, and location to the Hybrid 4-dimensional Augmented Reality (HD4AR) server. Based on a new computer vision method, the server uses this information from the phone to derive the mobile device's position at a resolution that is an order of magnitude more accurate than with current approaches based solely on GPS. The server uses the derived high-precision camera position to determine what cyber-information is in view of the device's camera. The extracted information, along with pixel coordinates of where each cyber-information item should appear in the photo, is returned to the mobile device and visualized in augmented reality format. &lt;br/&gt;&lt;br/&gt;If successful, the results of this research will provide the first feasible platform for context aware applications which does not require reliable and high accurate GPS/sensor-based location and orientation tracking and works based on existing image collections. It further assists field personnel through visualization of queried plan and actual site information in form of augmented reality, and supports interactions among project personnel and field information. By providing immediate access to information, the proposed framework automatically provides inexpensive, global and frequent reports from the field activities, and in turn can reduce downtime, rework, waste, and ultimately cost overrun. This project also involves educational and outreach activities to promote teaching and learning, engage undergraduate and graduate students, and reach out to underrepresented groups, K-12 students, and industry professionals. These activities include development of two course modules of "visual sensing for civil infrastructure engineering and management" and "mobile cyber-physical systems," as well as creating new software tools and hands-on outreach materials for context aware AEC/FM applications, which will be widely distributed among research and professional communities.</AbstractNarration>
<MinAmdLetterDate>09/26/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1360562</AwardID>
<Investigator>
<FirstName>Mani</FirstName>
<LastName>Golparvar-Fard</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mani Golparvar-Fard</PI_FULL_NAME>
<EmailAddress>mgolpar@illinois.edu</EmailAddress>
<PI_PHON>2173005226</PI_PHON>
<NSF_ID>000586985</NSF_ID>
<StartDate>09/26/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress><![CDATA[1901 South First St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1631</Code>
<Text>CIS-Civil Infrastructure Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>029E</Code>
<Text>INFRASTRUCTURE SYSTEMS MGT</Text>
</ProgramReference>
<ProgramReference>
<Code>036E</Code>
<Text>CIVIL INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>039E</Code>
<Text>STRUCTURAL SYSTEMS</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~273840</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Quick and easy access to updated project information is key to resolving many of the performance issues in the $1.1trillion construction industry. Real-time construction field reporting improves efficiency because it ensures that construction plans are followed and that resources are fully utilized. Despite its significance, the current methods of construction field reporting can be expensive, time-consuming and subjective. In many instances, monitoring and reporting work-in-progress is performed infrequently and completely manually. As a result, project personnel in many instances are left with incomplete performance data that is unsuitable to use for accurate analytics. Timely and accurate information on project progress and performance enables better communication and decision-making.</p> <p>Over the past decade, many context-aware techniques have been proposed to address these ongoing issues by delivering cyber-information, such as project specifications or drawings, to on-site users by intelligently interpreting their environment. However, these techniques only relied on Radio Frequency based location tracking technologies (e.g., GPS or Wireless Networks), which typically do not provide sufficient precision in congested construction sites or require additional hardware and custom mobile devices.</p> <p>To address these limitations, our project&nbsp;presents the underlying algorithms of a new vision-based mobile augmented reality system that allows field personnel to query and access 3D cyber-information on-site by using photographs taken from standard mobile devices. The system does not require any location tracking modules, external hardware attachments, and/or optical fiducial markers for localizing a user&rsquo;s position. Rather, the user&rsquo;s location and orientation are derived by comparing images from the user&rsquo;s mobile device to a 3D point cloud model generated from a set of pre-collected site photographs. A set of&nbsp;&nbsp;set of tools were also developed for analyzing, visualizing, and assessing architectural/construction progress with unordered photo collections captured via mobile devices and 3D building models. The prototype tools allow&nbsp;a user, such as a project manager or facility owner, to explore the construction site seamlessly in time, monitor the progress of construction, assess errors and deviations, and create photorealistic architectural visualizations, all through commodity smartphones and tablets without the need for significant computational power on these devices. In partnership with several leading construction companies, an exhaustive set of experiments were conducted on real-world construction projects to validate the accuracy of the proposed system, the integrated tools, and their impact on creating and preserving effective interfaces between onsite users and project information.</p> <p>Several aspects of the underlying technologies are reduced to practice. In construction, ongoing work is focused on integrating the developed prototype with several state-of-the-art solutions to bring the findings from this research into practice. The underlying methods and the developed prototypes are also applied to several industries beyond construction. For example, in retail settings, time to access information particularly is minimized, so the shoppers can compare prices, get access to other product information, or even leave comments about products through very simple to use image interfaces captured on commodity smartphones.</p> <p>From an educational perspective, the findings from this research project are integrated into the "Visual Sensing for Civil Infrastructure Engineering and Management" course offered at the University of Illinois at Urbana-Champaign. The entire course material is also shared through&nbsp;<a href="http://courses.engr.illinois.edu/cee598vsc/">http://courses.engr.illinois.edu/cee598vsc/</a>&nbsp;so that...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Quick and easy access to updated project information is key to resolving many of the performance issues in the $1.1trillion construction industry. Real-time construction field reporting improves efficiency because it ensures that construction plans are followed and that resources are fully utilized. Despite its significance, the current methods of construction field reporting can be expensive, time-consuming and subjective. In many instances, monitoring and reporting work-in-progress is performed infrequently and completely manually. As a result, project personnel in many instances are left with incomplete performance data that is unsuitable to use for accurate analytics. Timely and accurate information on project progress and performance enables better communication and decision-making.  Over the past decade, many context-aware techniques have been proposed to address these ongoing issues by delivering cyber-information, such as project specifications or drawings, to on-site users by intelligently interpreting their environment. However, these techniques only relied on Radio Frequency based location tracking technologies (e.g., GPS or Wireless Networks), which typically do not provide sufficient precision in congested construction sites or require additional hardware and custom mobile devices.  To address these limitations, our project presents the underlying algorithms of a new vision-based mobile augmented reality system that allows field personnel to query and access 3D cyber-information on-site by using photographs taken from standard mobile devices. The system does not require any location tracking modules, external hardware attachments, and/or optical fiducial markers for localizing a userÆs position. Rather, the userÆs location and orientation are derived by comparing images from the userÆs mobile device to a 3D point cloud model generated from a set of pre-collected site photographs. A set of  set of tools were also developed for analyzing, visualizing, and assessing architectural/construction progress with unordered photo collections captured via mobile devices and 3D building models. The prototype tools allow a user, such as a project manager or facility owner, to explore the construction site seamlessly in time, monitor the progress of construction, assess errors and deviations, and create photorealistic architectural visualizations, all through commodity smartphones and tablets without the need for significant computational power on these devices. In partnership with several leading construction companies, an exhaustive set of experiments were conducted on real-world construction projects to validate the accuracy of the proposed system, the integrated tools, and their impact on creating and preserving effective interfaces between onsite users and project information.  Several aspects of the underlying technologies are reduced to practice. In construction, ongoing work is focused on integrating the developed prototype with several state-of-the-art solutions to bring the findings from this research into practice. The underlying methods and the developed prototypes are also applied to several industries beyond construction. For example, in retail settings, time to access information particularly is minimized, so the shoppers can compare prices, get access to other product information, or even leave comments about products through very simple to use image interfaces captured on commodity smartphones.  From an educational perspective, the findings from this research project are integrated into the "Visual Sensing for Civil Infrastructure Engineering and Management" course offered at the University of Illinois at Urbana-Champaign. The entire course material is also shared through http://courses.engr.illinois.edu/cee598vsc/ so that researchers and graduate research students from various universities can build on top of the findings of this research project. Also incorporated assignments and content regarding uploading i...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
