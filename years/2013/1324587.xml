<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>The Expanded Hierarchical Rater Model: A Framework for the Analysis of Ratings</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>350001.00</AwardTotalIntnAmount>
<AwardAmount>350001</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Assessment of individuals' proficiency at complex tasks is often accomplished by observation and rating.  Teachers or testing agencies, for example, rate students' essays and their solutions to complex problems in mathematics and science.  School districts employ trained observers to rate teachers' performance in the classroom.  Experts rate radiologists' ability to classify x-ray images.  Ratings, however, may change over time due to changes in the way the rater perceives the work and/or changes in individuals' proficiency.  The material being rated also may reflect more than one dimension of proficiency.  Finally, summaries of these ratings may be misleading when the data collection design includes groupings (schools, hospitals, etc.) that introduce extraneous statistical dependence into the rating data.  This project will expand the Hierarchical Rater Model (HRM), a multilevel item response theory model that accounts for dependencies between multiple ratings of the same work, into a framework that will accommodate (a) variation in ratings over time; (b) multidimensional assessments; and (c) clusters and other hierarchical structure introduced by the data collection design.  This new framework will allow the HRM to provide estimates of the overall proficiencies of individuals on the rated tasks, as well as estimates of precision, accuracy, and other rater characteristics, under a broad variety of practical rating situations.  Analytical work, simulation studies, and real data applications will be used to explore and demonstrate the feasibility and applicability of the expanded HRM framework.  In particular, planned analysis of data from the Measures of Effective Teaching project (MET; Bill and Melinda Gates Foundation, 2012), a large study of class-room teaching in the United States, will demonstrate feasibility of the proposed methodological advancements to the HRM.  The research will culminate with a new HRM framework with unified notation and formulations so that researchers may specify and estimate special cases of the generalized model as needed.  The project also will provide computational tools including algorithms and source code, so that researchers can apply the framework with ease.&lt;br/&gt;&lt;br/&gt;The new HRM framework will advance scientific and practical knowledge in two ways.  It will enable researchers and practitioners to obtain high-quality estimates of proficiency that account and adjust for complex structure in the ratings.  It also will provide rich information about raters and the rating process.  Ratings of work, performance, and behavior are an increasing part of high-stakes decisions in many fields including human resources, medical diagnosis, and psychology.  The largest impact of this project may be in education policy and research, where ratings of teachers and students are increasingly common.  The new HRM framework will allow researchers and practitioners in these fields to produce more accurate assessments of individuals being rated, and to diagnose possible issues in the measurement and rating design, contributing to improved high-stakes decision making based on rating data.</AbstractNarration>
<MinAmdLetterDate>08/28/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1324587</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Junker</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian W Junker</PI_FULL_NAME>
<EmailAddress>brian@stat.cmu.edu</EmailAddress>
<PI_PHON>4122682718</PI_PHON>
<NSF_ID>000386415</NSF_ID>
<StartDate>08/28/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jodi</FirstName>
<LastName>Casabianca</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jodi Casabianca</PI_FULL_NAME>
<EmailAddress>jcasabianca@ets.org</EmailAddress>
<PI_PHON>9088126842</PI_PHON>
<NSF_ID>000640227</NSF_ID>
<StartDate>08/28/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~350001</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Constructed responses and other rich responses in assessment typically require human judgment during the scoring process. The subjective nature of human scoring elicits rater errors, such as bias or centrality, which degrade the validity and reliability of scores. The hierarchical rater model (HRM) and other item response theory (IRT) based models for ratings of rich responses were introduced to produce scores that account for these rater errors, thereby improving scores and the conclusions / decisions made about individuals. These models have been used in disciplines such as educational and psychological assessment and research. This project focused on expanding the original HRM into a flexible framework of models that can be applied in a wider variety of assessment and research scenarios. The original HRM was formulated to be applied in a simple assessment situation in which the assessment is unidimensional and administered in a cross-sectional design. Thus, the two main expansions of the HRM involved accommodating longitudinal data designs and assessments with a multidimensional internal structure. These advances, as well as the promotion of the HRM as a flexible framework of models with a large number of special cases and improving the accessibility of technical/analytic tools for researchers, were the goals of the project.</p> <p>Over the grant period, the project team developed, tested, and applied a longitudinal version of the HRM, which is appropriate to use when it is desired to have a latent proficiency or score estimate for each time point in the assessment design. This formulation was applied to several empirical datasets as well as to simulated data and was demonstrated in a didactic paper describing the details of model estimation, selection and evaluation. In addition, the project team also created a multidimensional version of the HRM for simple structured tests in which items relate to only one of multiple traits being measured. This formulation was also tested and demonstrated with empirical and simulated data. Simulations revealed that this multidimensional version of the HRM produces more accurate scores than a multidimensional IRT model that ignores rater effects and more accurate scores compared to scores estimated from a series of unidimensional HRMs. Importantly, other possible versions of these expansions and their corresponding assumptions were discussed as special cases of the larger HRM framework of models, and syntax for fitting these models were made available in workshops and on the project website: <a href="https://thehrmframework.wordpress.com/">https://thehrmframework.wordpress.com/</a> The project funded four graduate students in various capacities and for various lengths of time. All graduate students benefitted greatly from the opportunities made possible by the funding of this project; they learned about Bayesian and Markov chain Monte Carlo estimation, statistical model development and evaluation, research design, and hierarchical IRT models. One graduate student who worked on the project for four years, co-led the effort for the multidimensional expansion and is currently pursuing dissertation research studying and expanding the HRM with covariates.&nbsp;</p> <p>In terms of <strong>intellectual merit impact</strong>, the expansion of the HRM framework advances the methodological toolkit available for the analysis and scoring of constructed responses and other rich response formats. This is a major contribution to the field of statistics,&nbsp;particularly educational measurement and psychometrics. In addition, the methods and approaches that we illustrated in expanding the HRM framework are available to other researchers in statistical science looking to broaden and unify models. In terms of <strong>broader impact</strong>, the application of the HRM framework is relevant in all fields that require a human (or machine) to apply a rating to performance or behavior. That is, without restriction to a field of research or practice, the framework provides a sophisticated mechanism to properly analyze ratings and diagnose measurement processes; however, it will mostly be applied in small- or large-scale assessment settings and research settings in education, psychology, sociology, and medicine. The societal impact of the framework originates from the quality of the scores and rating information the HRM provides. Specifically, the HRM framework provides the capability to yield precise and consistent estimates adjusted for rater errors. Producing and reporting accurate representations of an individual?s work is pertinent to making high-stakes decisions such as college admissions, human resource decisions or medical diagnosis. Additionally, the utility of the model to provide information about raters and the rating process and to account for nuances in the measurement will allow researchers to improve the process by which they rate examinees and improve reliability in their measurement. Finally, in addition to publishing and presenting our work broadly and making technical resources available online, we have trained not only the four aforementioned doctoral students, but researchers and graduate students from a variety of disciplines have also been trained in workshops made possible by this grant.</p><br> <p>            Last Modified: 11/30/2018<br>      Modified by: Jodi&nbsp;Casabianca</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Constructed responses and other rich responses in assessment typically require human judgment during the scoring process. The subjective nature of human scoring elicits rater errors, such as bias or centrality, which degrade the validity and reliability of scores. The hierarchical rater model (HRM) and other item response theory (IRT) based models for ratings of rich responses were introduced to produce scores that account for these rater errors, thereby improving scores and the conclusions / decisions made about individuals. These models have been used in disciplines such as educational and psychological assessment and research. This project focused on expanding the original HRM into a flexible framework of models that can be applied in a wider variety of assessment and research scenarios. The original HRM was formulated to be applied in a simple assessment situation in which the assessment is unidimensional and administered in a cross-sectional design. Thus, the two main expansions of the HRM involved accommodating longitudinal data designs and assessments with a multidimensional internal structure. These advances, as well as the promotion of the HRM as a flexible framework of models with a large number of special cases and improving the accessibility of technical/analytic tools for researchers, were the goals of the project.  Over the grant period, the project team developed, tested, and applied a longitudinal version of the HRM, which is appropriate to use when it is desired to have a latent proficiency or score estimate for each time point in the assessment design. This formulation was applied to several empirical datasets as well as to simulated data and was demonstrated in a didactic paper describing the details of model estimation, selection and evaluation. In addition, the project team also created a multidimensional version of the HRM for simple structured tests in which items relate to only one of multiple traits being measured. This formulation was also tested and demonstrated with empirical and simulated data. Simulations revealed that this multidimensional version of the HRM produces more accurate scores than a multidimensional IRT model that ignores rater effects and more accurate scores compared to scores estimated from a series of unidimensional HRMs. Importantly, other possible versions of these expansions and their corresponding assumptions were discussed as special cases of the larger HRM framework of models, and syntax for fitting these models were made available in workshops and on the project website: https://thehrmframework.wordpress.com/ The project funded four graduate students in various capacities and for various lengths of time. All graduate students benefitted greatly from the opportunities made possible by the funding of this project; they learned about Bayesian and Markov chain Monte Carlo estimation, statistical model development and evaluation, research design, and hierarchical IRT models. One graduate student who worked on the project for four years, co-led the effort for the multidimensional expansion and is currently pursuing dissertation research studying and expanding the HRM with covariates.   In terms of intellectual merit impact, the expansion of the HRM framework advances the methodological toolkit available for the analysis and scoring of constructed responses and other rich response formats. This is a major contribution to the field of statistics, particularly educational measurement and psychometrics. In addition, the methods and approaches that we illustrated in expanding the HRM framework are available to other researchers in statistical science looking to broaden and unify models. In terms of broader impact, the application of the HRM framework is relevant in all fields that require a human (or machine) to apply a rating to performance or behavior. That is, without restriction to a field of research or practice, the framework provides a sophisticated mechanism to properly analyze ratings and diagnose measurement processes; however, it will mostly be applied in small- or large-scale assessment settings and research settings in education, psychology, sociology, and medicine. The societal impact of the framework originates from the quality of the scores and rating information the HRM provides. Specifically, the HRM framework provides the capability to yield precise and consistent estimates adjusted for rater errors. Producing and reporting accurate representations of an individual?s work is pertinent to making high-stakes decisions such as college admissions, human resource decisions or medical diagnosis. Additionally, the utility of the model to provide information about raters and the rating process and to account for nuances in the measurement will allow researchers to improve the process by which they rate examinees and improve reliability in their measurement. Finally, in addition to publishing and presenting our work broadly and making technical resources available online, we have trained not only the four aforementioned doctoral students, but researchers and graduate students from a variety of disciplines have also been trained in workshops made possible by this grant.       Last Modified: 11/30/2018       Submitted by: Jodi Casabianca]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
