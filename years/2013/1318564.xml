<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Collaborative Research: Enabling Cost-Effective Cloud HPC</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>311998.00</AwardTotalIntnAmount>
<AwardAmount>311998</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project examines novel services built on top of public cloud&lt;br/&gt;infrastructure to enable cost-effective high-performance computing.&lt;br/&gt;The PIs will explore the on-demand, elastic, and configurable features of&lt;br/&gt;cloud computing to complement the traditional supercomputer/cluster&lt;br/&gt;platforms. More specifically, this research aims to assess the efficacy&lt;br/&gt;of building dynamic cloud-based clusters leveraging the configurability&lt;br/&gt;and tiered pricing model of cloud instances. The scientific value of this&lt;br/&gt;proposal lies in the novel use of untapped features of cloud computing&lt;br/&gt;for HPC and the strategic adoption of small, cloud-based clusters for&lt;br/&gt;the purpose of developing/tuning applications for large supercomputers.&lt;br/&gt;&lt;br/&gt;Through this research, the PIs expect to answer key research questions&lt;br/&gt;regarding: (1) automatic workload-specific cloud cluster configuration,&lt;br/&gt;(2) cost-aware and contention-aware data and task co-scheduling,&lt;br/&gt;and (3) adaptive, integrated cloud instance provisioning and job&lt;br/&gt;scheduling, plus workload aggregation for cloud instance rental cost&lt;br/&gt;reduction. If successful, this research will result in tools that&lt;br/&gt;adaptively aggregate, configure, and re-configure cloud resources for&lt;br/&gt;different HPC needs, with the purpose of offering low-cost R&amp;D&lt;br/&gt;environments for scalable parallel applications.</AbstractNarration>
<MinAmdLetterDate>08/08/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1318564</AwardID>
<Investigator>
<FirstName>Mladen</FirstName>
<LastName>Vouk</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mladen A Vouk</PI_FULL_NAME>
<EmailAddress>vouk@csc.ncsu.edu</EmailAddress>
<PI_PHON>9195157886</PI_PHON>
<NSF_ID>000251077</NSF_ID>
<StartDate>10/21/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Xiaosong</FirstName>
<LastName>Ma</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaosong Ma</PI_FULL_NAME>
<EmailAddress>ma@csc.ncsu.edu</EmailAddress>
<PI_PHON>9195137577</PI_PHON>
<NSF_ID>000300080</NSF_ID>
<StartDate>08/08/2013</StartDate>
<EndDate>10/21/2013</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Xiaosong</FirstName>
<LastName>Ma</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaosong Ma</PI_FULL_NAME>
<EmailAddress>ma@csc.ncsu.edu</EmailAddress>
<PI_PHON>9195137577</PI_PHON>
<NSF_ID>000300080</NSF_ID>
<StartDate>10/21/2013</StartDate>
<EndDate>07/27/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>North Carolina State University</Name>
<CityName>Raleigh</CityName>
<ZipCode>276957514</ZipCode>
<PhoneNumber>9195152444</PhoneNumber>
<StreetAddress>2601 Wolf Village Way</StreetAddress>
<StreetAddress2><![CDATA[Admin. III, STE 240]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042092122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTH CAROLINA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[North Carolina State University]]></Name>
<CityName>Raleigh</CityName>
<StateCode>NC</StateCode>
<ZipCode>276958206</ZipCode>
<StreetAddress><![CDATA[890 Oval Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~311998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the project was to explore cost-effectiveness of High Performance Computing (HPC) in the cloud, and how to improve it. While large-scale "embarassingly parallel"&nbsp; (asynchronous, low coupling) computations run well in the clouds, this is not always the case with applications that contain tightly coupled computations.&nbsp; Those run much better on dedicated HPC clusters and supercomputers that offer very low latency interconnects, and nowdays also considerable amount of memory and storage. We investigated different aspects of the HPC-in-the-cloud cost-effectiveness and overhead: from&nbsp; hardware investment and changes in the cloud architecture, to on-demand resource charges, to batch queue wait times, to storage size and data transfer overhead and rates, to security.</p> <p>- How fast and at what cost one can access storage, and what type of storage, is important. Especially it this day and age of growing awareness of "bigdata" applications and anlytics. For example, deciding the optimal server flash cache (SFC) allocation among Virtual Machines (VM) or VM disks is of considerable importance. Our work validates the growing applicability of automated techniques to solve real-time resource management problems traditionally addressed using heuristics, such as continuous and automated optimization of SFC space partitioning and Input/Output scheduling across applications.</p> <p>- We also explored the concept of "reverse HPC cloud engineering", i.e., operating a cloud in a supercomputer hardware matrix. The concept works very well for both HPC and traditional cloud computing, i.e., both gain considerably in performance. Unfortunately, in practice it is probably not cost-effective to run clouds on expensive, state-of-the-art, supercomputing architectures. However, multi-core processor, and GPU accelerators, offer archiectural options that provide many performance enhancement for both the classical cloud computing and the HPC in the cloud. They also seem to be cost-effective. We are in the process of benchmarking several of the options.</p> <p>- Finally, our work shows that as public (commerical) clouds mature, they are beginning to successfully compete with HPC enhanced private clouds. We show that the&nbsp; public cloud option is more cost effective than the private cloud for HPC type applications of short duration or with intermittent need for cloud computation resources. However, as the need for extended or prolonged access to computation resources grows, at some point in time (60 to 180 days in our experience) it becomes more cost effective to assign HPC applications to a private cloud provider. Another issue is transfer of large quantities of data&nbsp; from public cloud providers to external locations. Depending on the public cloud vendor, and the quantity of data transferred, the break-even cost between using a public versus private cloud vendor exhibits tiered pricing points at which became more effective to either use a private cloud vendor or to leave the application on a supercomputer. Comparing the costs for the various tiers helped us identify when and where to build virtual clusters that may lower the per-job monetary cost through workload aggregation while maintaining the elastic and on-demand nature of cloud resource management. Our third major observation concerns spot market pricing. We show under what operational conditions a public cloud option may be a more cost effective alternative for HPC type applications using the option of bidding on unused Central Processing Unit (CPU) cycles under the constraint that the application would run on the cloud instance as long as bid price exceeded the current spot price. Our study examined hourly spot pricing of two Amazon regions and five availability zones. Our results indicate that there were periods of time when pairs of availability zones exhibited strong anti-correlated behavior of their spot instances. In effect, the spot price in one zone would be substantively different than the spot price in the other zone. Information like this may be a deciding factor as to where and when to run an HPC job in a cloud, and for how  long.</p><br> <p>            Last Modified: 11/26/2017<br>      Modified by: Mladen&nbsp;A&nbsp;Vouk</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the project was to explore cost-effectiveness of High Performance Computing (HPC) in the cloud, and how to improve it. While large-scale "embarassingly parallel"  (asynchronous, low coupling) computations run well in the clouds, this is not always the case with applications that contain tightly coupled computations.  Those run much better on dedicated HPC clusters and supercomputers that offer very low latency interconnects, and nowdays also considerable amount of memory and storage. We investigated different aspects of the HPC-in-the-cloud cost-effectiveness and overhead: from  hardware investment and changes in the cloud architecture, to on-demand resource charges, to batch queue wait times, to storage size and data transfer overhead and rates, to security.  - How fast and at what cost one can access storage, and what type of storage, is important. Especially it this day and age of growing awareness of "bigdata" applications and anlytics. For example, deciding the optimal server flash cache (SFC) allocation among Virtual Machines (VM) or VM disks is of considerable importance. Our work validates the growing applicability of automated techniques to solve real-time resource management problems traditionally addressed using heuristics, such as continuous and automated optimization of SFC space partitioning and Input/Output scheduling across applications.  - We also explored the concept of "reverse HPC cloud engineering", i.e., operating a cloud in a supercomputer hardware matrix. The concept works very well for both HPC and traditional cloud computing, i.e., both gain considerably in performance. Unfortunately, in practice it is probably not cost-effective to run clouds on expensive, state-of-the-art, supercomputing architectures. However, multi-core processor, and GPU accelerators, offer archiectural options that provide many performance enhancement for both the classical cloud computing and the HPC in the cloud. They also seem to be cost-effective. We are in the process of benchmarking several of the options.  - Finally, our work shows that as public (commerical) clouds mature, they are beginning to successfully compete with HPC enhanced private clouds. We show that the  public cloud option is more cost effective than the private cloud for HPC type applications of short duration or with intermittent need for cloud computation resources. However, as the need for extended or prolonged access to computation resources grows, at some point in time (60 to 180 days in our experience) it becomes more cost effective to assign HPC applications to a private cloud provider. Another issue is transfer of large quantities of data  from public cloud providers to external locations. Depending on the public cloud vendor, and the quantity of data transferred, the break-even cost between using a public versus private cloud vendor exhibits tiered pricing points at which became more effective to either use a private cloud vendor or to leave the application on a supercomputer. Comparing the costs for the various tiers helped us identify when and where to build virtual clusters that may lower the per-job monetary cost through workload aggregation while maintaining the elastic and on-demand nature of cloud resource management. Our third major observation concerns spot market pricing. We show under what operational conditions a public cloud option may be a more cost effective alternative for HPC type applications using the option of bidding on unused Central Processing Unit (CPU) cycles under the constraint that the application would run on the cloud instance as long as bid price exceeded the current spot price. Our study examined hourly spot pricing of two Amazon regions and five availability zones. Our results indicate that there were periods of time when pairs of availability zones exhibited strong anti-correlated behavior of their spot instances. In effect, the spot price in one zone would be substantively different than the spot price in the other zone. Information like this may be a deciding factor as to where and when to run an HPC job in a cloud, and for how  long.       Last Modified: 11/26/2017       Submitted by: Mladen A Vouk]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
