<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>347216.00</AwardTotalIntnAmount>
<AwardAmount>347216</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today's architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.&lt;br/&gt;The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.&lt;br/&gt;MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities.</AbstractNarration>
<MinAmdLetterDate>08/29/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1339820</AwardID>
<Investigator>
<FirstName>George</FirstName>
<LastName>Bosilca</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George M Bosilca</PI_FULL_NAME>
<EmailAddress>bosilca@icl.utk.edu</EmailAddress>
<PI_PHON>8659746321</PI_PHON>
<NSF_ID>000348594</NSF_ID>
<StartDate>08/29/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName/>
<StateCode>TN</StateCode>
<ZipCode>379960003</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8005</Code>
<Text>Scientific Software Elements</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~347216</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As one of the most widely used parallel programming paradigms, message-passing interface (MPI) is an essential element of the software infrastructure necessary to support the nation&rsquo;s scientific endeavors, and, consequently, is of critical interest in all scientific computing fields. The ADAPT project focused on enhancing and modernizing one of the major MPI implementations, Open MPI, in the context of the ongoing revolution in processor architecture and system design. We sought to create a viable foundation for a new generation of Open MPI components so as to enable the rapid exploration of new physical capabilities, provide greatly improved performance portability, and pave the way toward full interoperability between classes of components. More specifically, we provided fundamental software support for manycore processor systems, which are made for a high degree of parallel processing. Our efforts allowed these systems to efficiently execute MPI-based applications and to tolerate different types of process interruptions and failures at scales ranging from current systems to the extreme scales that will soon be built. We integrated hardware architecture information as a core component and provided an application program interface (API) to allow all layers of the software stack full access to the information. We adapted algorithms to take advantage of process placement, distributed topologies, file access, and point-to-point and collective communication information to enable improved application portability and, more importantly, better scalability and performance. By updating the multithreading capabilities of the Open MPI library, we improved the message injection rate by orders of magnitude, achieved fully asynchronous data transfers, and provided support for other asynchronous behaviors. In addition to enhancing the Open MPI software stack, our work impacted applications&mdash;especially at large scale and in the types of platforms that will fill the gap as we progress to exascale, the level at which computing systems that will be at least 50 time faster than the nation&rsquo;s most powerful supercomputers in use today.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/31/2017<br>      Modified by: George&nbsp;M&nbsp;Bosilca</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As one of the most widely used parallel programming paradigms, message-passing interface (MPI) is an essential element of the software infrastructure necessary to support the nation?s scientific endeavors, and, consequently, is of critical interest in all scientific computing fields. The ADAPT project focused on enhancing and modernizing one of the major MPI implementations, Open MPI, in the context of the ongoing revolution in processor architecture and system design. We sought to create a viable foundation for a new generation of Open MPI components so as to enable the rapid exploration of new physical capabilities, provide greatly improved performance portability, and pave the way toward full interoperability between classes of components. More specifically, we provided fundamental software support for manycore processor systems, which are made for a high degree of parallel processing. Our efforts allowed these systems to efficiently execute MPI-based applications and to tolerate different types of process interruptions and failures at scales ranging from current systems to the extreme scales that will soon be built. We integrated hardware architecture information as a core component and provided an application program interface (API) to allow all layers of the software stack full access to the information. We adapted algorithms to take advantage of process placement, distributed topologies, file access, and point-to-point and collective communication information to enable improved application portability and, more importantly, better scalability and performance. By updating the multithreading capabilities of the Open MPI library, we improved the message injection rate by orders of magnitude, achieved fully asynchronous data transfers, and provided support for other asynchronous behaviors. In addition to enhancing the Open MPI software stack, our work impacted applications&mdash;especially at large scale and in the types of platforms that will fill the gap as we progress to exascale, the level at which computing systems that will be at least 50 time faster than the nation?s most powerful supercomputers in use today.          Last Modified: 01/31/2017       Submitted by: George M Bosilca]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
