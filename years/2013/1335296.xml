<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Computationally Efficient Algorithms for Markov Decision Processes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>285000.00</AwardTotalIntnAmount>
<AwardAmount>285000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Georgia-Ann Klutke</SignBlockName>
<PO_EMAI>gaklutke@nsf.gov</PO_EMAI>
<PO_PHON>7032922443</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The research objective of this award is to develop computationally efficient algorithms for solving certain classes of Markov Decision Processes (MDPs).  MDPs, also known under the name of stochastic dynamic programming, provide major operations research methods and tools for dynamic optimization of stochastic systems. They are broadly used for the optimization of production, service, telecommunication, and military systems. In addition to operations research, MDPs are used in many other disciplines including electrical engineering, computer science, and economics.  The project will investigate the problems with the two most often used objective criteria: average costs per unit of time and the expected total discounted costs as well as risk sensitive criteria.  In addition to problems with a single objective criterion, it will investigate problems with multiple criteria and constraints.  The computational efficiency will be studied with respect to the parameters of an MDP and with respect to the parameters of the initial problem formulation. &lt;br/&gt; &lt;br/&gt;If successful, the results of this research will provide new methodologies and computer algorithms for finding exact and approximate solutions to MDPs modeling applications to production and service systems.  These applications include inventory and queueing control, scheduling, and resource allocation. This project will also contribute to the development of human resources in science and engineering. First, it will support Ph.D. students at the Stony Brook University to conduct research related to this project. Second, it will create research and educational projects for graduate and undergraduate students including minority and female students in science and engineering.  The results of this project will be used in applied probability and dynamic programming courses that the PI teaches.  The results of this project will be disseminated via journal publications, the internet, and included into the text "Introduction to Markov Decision Processes" the PI is currently working on.</AbstractNarration>
<MinAmdLetterDate>07/26/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1335296</AwardID>
<Investigator>
<FirstName>Eugene</FirstName>
<LastName>Feinberg</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eugene A Feinberg</PI_FULL_NAME>
<EmailAddress>eugene.feinberg@sunysb.edu</EmailAddress>
<PI_PHON>6316327189</PI_PHON>
<NSF_ID>000096894</NSF_ID>
<StartDate>07/26/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Stony Brook]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>117943600</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5514</Code>
<Text>OPERATIONS RESEARCH</Text>
</ProgramElement>
<ProgramReference>
<Code>072E</Code>
<Text>NETWORKS &amp; QUEUING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>073E</Code>
<Text>OPTIMIZATION &amp; DECISION MAKING</Text>
</ProgramReference>
<ProgramReference>
<Code>077E</Code>
<Text>SIMULATION MODELS</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~285000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Markov Decision Processes (MDPs), also known under the name of stochastic dynamic programming, provide major operations research methods and tools for dynamic optimization of stochastic systems. They are broadly used for optimization of production, service, telecommunication, and military systems. MDPs are used in many other disciplines including electrical engineering, computer science, and economics.&nbsp; The project dealt with the development of computationally efficient methods for finding optimal policies for MDPs.</p> <p>&nbsp;</p> <p>There are two major methods for solving MDPs: value iterations and linear programming.&nbsp; These two methods can be applied to problems with expected discounted total costs and to problems with average costs per unit time.&nbsp; The project was analyzing efficiency of value iteration and linear programming algorithms, developing new solution techniques, analyzing the structure of MDP models, studying and advancing the related mathematical methods, and studying particular applications.</p> <p>&nbsp;</p> <p>The major outcomes are:</p> <p>&nbsp;</p> <ol> <li>We constructed examples demonstrating that the value iteration algorithm is weakly polynomial for discounted problems. This result is surprising because it is known that discounted MDPs can be solved in strongly polynomial time by simplex method, which is a linear programming algorithm.&nbsp; Thus the project provided the theoretical proof that linear programming is a better method for finding exact solutions to discounted MDPs than value iterations.&nbsp;&nbsp; We also provided examples demonstrating that modified policy iteration algorithms are also less efficient for solving MDPs than the simplex method.&nbsp; These examples are also surprising because modified policy iteration algorithms were introduced many years years ago to improve the efficiency of the simplex method applied to MDPs.</li> <li>We developed methods for converting average-cost problems into problems with discounted costs.&nbsp; By using this conversion, we developed strongly polynomial algorithms for solving certain classes of MDPs with average costs per unit time.</li> <li>We formulated and proved optimality conditions for partially observable Markov Decision Processes (POMDPs) with infinite state, action, and observation sets.&nbsp; Finding such conditions was a long-standing open problem.</li> <li>In order to investigate MDPs, the project team investigated several mathematical problems and obtained significant results.&nbsp; In particular, we found generalizations of two classic mathematical facts, Berge&rsquo;s maximum theorem and Fatou&rsquo;s lemma. For Berge&rsquo;s maximum theorem, we found its generalization to noncompact decision sets.&nbsp; For Fatou&rsquo;s lemma, we described its uniform version.&nbsp; This version of Fatou&rsquo;s lemma was used for the analysis of POMDPs.&nbsp; We also constructed counter-examples to some conjectures relevant to the Tauberian and Hardy-Littlewood theorems.&nbsp; These theorems are important for studying average costs per unit time as normalized limits of discounted costs as the discount factor increases to one.&nbsp; We also described solutions to backward and forward Kolmogorov&rsquo;s equations for jump Markov processes.</li> <li>We used our results for MDPs to develop new methods for analyzing and solving inventory control problems.&nbsp; By using these methods, we found new solutions to the classic periodic-review inventory control problem with backorders and setup costs.</li> <li>The project developed applications to the voltage control problem for distribution electric systems, to optimal pricing, to control of queues, and to filtering and identification problems.</li> </ol> <p>The results of the projects were included into graduate courses on dynamic programming and probability theory at Stony Brook University.&nbsp; The project was used to train four graduate students, one of whom is female.&nbsp; After completion of her studies, she received an academic job.&nbsp; The results of the project also were used to develop and implement voltage control algorithms applied to the data provided by the local electric utility company. The results of this implementation were reported to the US Department of Energy.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/27/2017<br>      Modified by: Eugene&nbsp;A&nbsp;Feinberg</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Markov Decision Processes (MDPs), also known under the name of stochastic dynamic programming, provide major operations research methods and tools for dynamic optimization of stochastic systems. They are broadly used for optimization of production, service, telecommunication, and military systems. MDPs are used in many other disciplines including electrical engineering, computer science, and economics.  The project dealt with the development of computationally efficient methods for finding optimal policies for MDPs.     There are two major methods for solving MDPs: value iterations and linear programming.  These two methods can be applied to problems with expected discounted total costs and to problems with average costs per unit time.  The project was analyzing efficiency of value iteration and linear programming algorithms, developing new solution techniques, analyzing the structure of MDP models, studying and advancing the related mathematical methods, and studying particular applications.     The major outcomes are:     We constructed examples demonstrating that the value iteration algorithm is weakly polynomial for discounted problems. This result is surprising because it is known that discounted MDPs can be solved in strongly polynomial time by simplex method, which is a linear programming algorithm.  Thus the project provided the theoretical proof that linear programming is a better method for finding exact solutions to discounted MDPs than value iterations.   We also provided examples demonstrating that modified policy iteration algorithms are also less efficient for solving MDPs than the simplex method.  These examples are also surprising because modified policy iteration algorithms were introduced many years years ago to improve the efficiency of the simplex method applied to MDPs. We developed methods for converting average-cost problems into problems with discounted costs.  By using this conversion, we developed strongly polynomial algorithms for solving certain classes of MDPs with average costs per unit time. We formulated and proved optimality conditions for partially observable Markov Decision Processes (POMDPs) with infinite state, action, and observation sets.  Finding such conditions was a long-standing open problem. In order to investigate MDPs, the project team investigated several mathematical problems and obtained significant results.  In particular, we found generalizations of two classic mathematical facts, Berge?s maximum theorem and Fatou?s lemma. For Berge?s maximum theorem, we found its generalization to noncompact decision sets.  For Fatou?s lemma, we described its uniform version.  This version of Fatou?s lemma was used for the analysis of POMDPs.  We also constructed counter-examples to some conjectures relevant to the Tauberian and Hardy-Littlewood theorems.  These theorems are important for studying average costs per unit time as normalized limits of discounted costs as the discount factor increases to one.  We also described solutions to backward and forward Kolmogorov?s equations for jump Markov processes. We used our results for MDPs to develop new methods for analyzing and solving inventory control problems.  By using these methods, we found new solutions to the classic periodic-review inventory control problem with backorders and setup costs. The project developed applications to the voltage control problem for distribution electric systems, to optimal pricing, to control of queues, and to filtering and identification problems.   The results of the projects were included into graduate courses on dynamic programming and probability theory at Stony Brook University.  The project was used to train four graduate students, one of whom is female.  After completion of her studies, she received an academic job.  The results of the project also were used to develop and implement voltage control algorithms applied to the data provided by the local electric utility company. The results of this implementation were reported to the US Department of Energy.          Last Modified: 09/27/2017       Submitted by: Eugene A Feinberg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
