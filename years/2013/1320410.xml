<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Integrating low-level speech features into a model of speech perception</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>179724</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Why are humans so much better than machines at recognizing speech?  This research aims to measure differences between humans and machines in how they compute similarities between sounds.  A computational model of speech perception will be trained on speech production data, using several types of features that are typically used in speech recognitions systems.  It will then be tested on its ability to predict human listeners' responses in speech sound discrimination tasks.  Results are expected to provide information about how the speech that listeners hear shapes their perception of sounds, as well as how well the information used by automatic speech recognition systems matches the information used by human listeners.&lt;br/&gt;&lt;br/&gt;By allowing us to compare how humans and speech recognition systems use information when perceiving speech, this research will provide a tool that can help make speech recognition systems more human-like.  Reverse engineering human perception can improve the way these systems generalize to new dialects, talkers, and noise conditions.  This has the potential to facilitate the construction of systems for low-resource languages, broadening the impact of speech recognition technologies.&lt;br/&gt;&lt;br/&gt;[Supported by SBE/BCS/PAC and CISE/IIS/RI]</AbstractNarration>
<MinAmdLetterDate>07/27/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/16/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320410</AwardID>
<Investigator>
<FirstName>Naomi</FirstName>
<LastName>Feldman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Naomi Feldman</PI_FULL_NAME>
<EmailAddress>nhf@umd.edu</EmailAddress>
<PI_PHON>3014056269</PI_PHON>
<NSF_ID>000622351</NSF_ID>
<StartDate>07/27/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~179724</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Human listeners easily outperform speech recognition systems in challenging listening conditions, such as perceiving speech in noise, understanding accented speech, and generalizing across different speakers.&nbsp; The goal of this project was to develop an approach for measuring how closely the representations used in automatic speech recognition systems match those of human listeners.&nbsp; In the long term, this approach can help us understand how human speech perception achieves such robustness and can help us create more robust language technologies.<br /><br />The project was a success in terms of scientific advances, training of students, and integration of the cognitive science and engineering research communities.<br /><br />A major component of the project involved developing a new method for training models of human speech perception directly on large collections of speech recordings, rather than on small, artificial datasets.&nbsp; This increases the models' ecological validity and was key to comparing representations from automatic speech recognition, which are computed over large speech datasets, with human perception.&nbsp; Using this method, several feature types commonly used in speech recognition systems were tested, providing quantitative data on how the speaker normalization algorithms used to create these feature types compare with human listeners in generalizing across talkers and dialects.<br /><br />Additional results shed light on the computational strategies that human listeners use when perceiving speech.&nbsp; Studies revealed parallels in how listeners compensate for perceptual uncertainty when perceiving consonants and vowels, and showed that listeners are quite selective in choosing features of the speech signal to attend to when making perceptual judgments.<br /><br />Graduate, undergraduate, and high school students received training through this project, learning to integrate methods from cognitive science and electrical engineering.&nbsp; In two cases, students' work on this project won them recognition in prestigious national competitions: one student won an NSF Graduate Research Fellowship, and another was named an Intel Science Talent Search semi-finalist.<br /><br />Longer-term outcomes include new courses that train students to integrate knowledge across disciplines, new interdisciplinary collaborations within and outside of the University of Maryland, and the designation of "flexible automatic speech recognition" as a focus area in the university's Language Science Center.</p><br> <p>            Last Modified: 11/19/2016<br>      Modified by: Naomi&nbsp;Feldman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Human listeners easily outperform speech recognition systems in challenging listening conditions, such as perceiving speech in noise, understanding accented speech, and generalizing across different speakers.  The goal of this project was to develop an approach for measuring how closely the representations used in automatic speech recognition systems match those of human listeners.  In the long term, this approach can help us understand how human speech perception achieves such robustness and can help us create more robust language technologies.  The project was a success in terms of scientific advances, training of students, and integration of the cognitive science and engineering research communities.  A major component of the project involved developing a new method for training models of human speech perception directly on large collections of speech recordings, rather than on small, artificial datasets.  This increases the models' ecological validity and was key to comparing representations from automatic speech recognition, which are computed over large speech datasets, with human perception.  Using this method, several feature types commonly used in speech recognition systems were tested, providing quantitative data on how the speaker normalization algorithms used to create these feature types compare with human listeners in generalizing across talkers and dialects.  Additional results shed light on the computational strategies that human listeners use when perceiving speech.  Studies revealed parallels in how listeners compensate for perceptual uncertainty when perceiving consonants and vowels, and showed that listeners are quite selective in choosing features of the speech signal to attend to when making perceptual judgments.  Graduate, undergraduate, and high school students received training through this project, learning to integrate methods from cognitive science and electrical engineering.  In two cases, students' work on this project won them recognition in prestigious national competitions: one student won an NSF Graduate Research Fellowship, and another was named an Intel Science Talent Search semi-finalist.  Longer-term outcomes include new courses that train students to integrate knowledge across disciplines, new interdisciplinary collaborations within and outside of the University of Maryland, and the designation of "flexible automatic speech recognition" as a focus area in the university's Language Science Center.       Last Modified: 11/19/2016       Submitted by: Naomi Feldman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
