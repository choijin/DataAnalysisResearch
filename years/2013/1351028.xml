<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Apprenticeship Learning for Robotic Manipulation of Deformable Objects</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2014</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Reid Simmons</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project considers the problem of apprenticeship learning, in which a robot first gets access to demonstrations of a task and ought to learn from these demonstrations how to perform that task in new, yet similar, situations.  This line of work has already shown significant promise, including in helicopter control where it enabled autonomous helicopter aerobatics at the level of the best human pilots.  However, fundamental limitations remain, and robotic capabilities to manipulate deformable objects are currently still well below human level. The approach followed builds on, and extends, non-rigid registration algorithms, which can capture how scenes with deformable objects relate to each other.  Such registration is extrapolated to morph a demonstrated manipulation trajectory into a good trajectory for a new scene.  New machine learning algorithms are developed to enable choosing the optimal training demonstration and the optimal morphing objective while accounting for external constraints, such as avoiding collisions and satisfying joint limits. Infrastructure is being built for large-scale data collection of demonstrations and theoretical and empirical characterizations are developed for how much data is needed for a given task.  Concrete challenge tasks considered are knot tying, cloth and fabric manipulation, surgical suturing, and small surgical procedures. Results will be incorporated into the PI's graduate robotics course and the source code will be shared with the robotics community.</AbstractNarration>
<MinAmdLetterDate>03/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>03/10/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1351028</AwardID>
<Investigator>
<FirstName>Pieter</FirstName>
<LastName>Abbeel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pieter Abbeel</PI_FULL_NAME>
<EmailAddress>pabbeel@cs.berkeley.edu</EmailAddress>
<PI_PHON>5106428109</PI_PHON>
<NSF_ID>000511407</NSF_ID>
<StartDate>03/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947201776</ZipCode>
<StreetAddress><![CDATA[Soda Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-9895e3e1-7fff-7a4f-99e0-32b5e865ce9e"> <p dir="ltr"><span>This project has had 6 main thrusts:</span></p> <br /> <p dir="ltr"><span>(1) Fundamental Research to Enable Robots to Learn More Effectively from Demonstrations (Imitation Learning) and from Their Own Trial and Error (Reinforcement Learning) -- together called Apprenticeship Learning.&nbsp;</span></p> <br /> <p dir="ltr"><span>We studied, and advanced, non-rigid registration algorithms that can capture how scenes with deformable objects relate to each other.&nbsp; After relating two scenes to each other, if a demonstration is available for completing the task in the first scene, the relationship between scenes can be used to understand how to complete the task in the second scene.&nbsp; Key starting point was thin-plate-spline-robust-point-matching.  Among other things, we incorporated: lookahead; the ability to trade off path following vs. force imitation; accounting for mismatch between robot body and human demonstrator; better scene registration through considering normals and through deep learning.&nbsp; The result of these developments was a robot that could meaningfully manipulate rope and tie knots across a wide range of scenarios, as well as fold towels.</span></p> <br /> <p dir="ltr"><span>We investigated imitation learning that leverages recent advances in deep supervised learning and virtual reality commoditization.&nbsp; We showed  that behavioral cloning, with high-quality demonstrations, is surprisingly effective. We developed a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration.&nbsp;</span></p> <br /> <p dir="ltr"><span>As we saw our systems ever more capable of imitation, we started to shift focus towards reinforcement learning (RL), where a robot learns from its own trial and error (sometimes from scratch, sometimes bootstrapped off demonstrations).&nbsp; We proposed an RL approach that enabled robots to learn to manipulate rope on their own.  We developed a new neural net architecture called Value Iteration Networks which allows RL to learn new skills more efficiently through the ability to plan. &nbsp; We adapted the Value Iteration Network ideas to the continuous control setting, enabling new robotic assembly skills.  In a close collaboration with NASA, we studied autonomous control for tensegrity robots.  We also made advances in considering safety during learning and in ensuring the robot is communicative throughout its behavior.&nbsp;</span></p> <br /> <p dir="ltr"><span>These research advances were shared with the public through presentation and publication at the top robotics and machine learning venues:&nbsp; ICRA 2015 (3x), IROS 2015 (4x), NeurIPS 2016, ICRA 2017 (4x), RSS 2017, CoRL 2017, ICRA 2018 (2x)&nbsp;</span></p> <br /> <p dir="ltr"><span>(2) Fundamental Research and Development of a New Robotic Arm that is Capable, Safe, Low-Cost.</span></p> <br /> <p dir="ltr"><span>Robots must cost less and must be force-controlled to enable widespread, safe deployment in unconstrained human environments. We proposed Quasi-Direct Drive actuation as a capable paradigm for robotic force-controlled manipulation in human environments at low-cost. Our prototype - Blue - is a human scale 7 Degree of Freedom arm with 2kg payload. Blue has a bill of materials of only $3,000. This could open up many new opportunities for distribution of robot arms to researchers and hobbyists, and from there (as software further matures) could result in affordable robots that can help people in their homes, e.g. enabling longer independent living.</span></p> <br /> <p dir="ltr"><span>The work on the BLUE project was published at ICRA 2019 (arm) and CASE 2019 (gripper).</span></p> <br /> <p dir="ltr"><span>The project page is here: http://rll.berkeley.edu/blue/</span></p> <p dir="ltr"><span>It was also covered extensively in the popular media, representative listing of outlets here:</span></p> <p dir="ltr"><a href="https://www.berkeleyopenarms.org/in-the-press"><span>https://www.berkeleyopenarms.org/in-the-press</span></a><span>.&nbsp;&nbsp;</span></p> <p dir="ltr"><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span><span> </span></span><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span>(3) Training of the next generation of AI / robotics researchers.&nbsp;&nbsp;</span></p> <br /> <p dir="ltr"><span>This project directly contributed to the research training of 5 Post-docs, 8 PhD students, 1 Masters student, 18 Undergraduate students, and 1 High School student.</span></p> <br /> <p dir="ltr"><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span>(4) Dissemination: The work was disseminated through 18 peer-reviewed publications, all available from the PI&rsquo;s website.&nbsp; The work under this grant has also been part of the PIs numerous public presentations over the past 5 years, estimated at about 25 (public) talks per year.&nbsp; </span><span><span> </span></span><span><span> </span></span></p> <br /> <p dir="ltr"><span>(5) Teaching Artificial Intelligence (a) at Berkeley, (b) worldwide through Massive Open Online Courses (MOOCs), (c) worldwide through sharing the Berkeley AI course materials with instructors elsewhere.</span></p> <br /> <p dir="ltr"><span>This project directly allowed the PI to further build out the UC Berkeley CS188 Artificial Intelligence course.</span></p> <br /> <p dir="ltr"><span>The PI also offered the course for free online through edX.&nbsp;</span></p> <br /> <p dir="ltr"><span>In addition, all the course and instructional materials are available to all instructors (and students) at ai.berkeley.edu.&nbsp; By now the materials are used at over 100 other universities.</span><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span>(6) K-12 Outreach to Encourage STEM:&nbsp;&nbsp;</span></p> <p dir="ltr"><span><span> </span></span><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span>There also has been substantial outreach, which we hope will inspire the next generation of scientists and engineers.&nbsp; We hosted over 75 groups.  http://rll.berkeley.edu/outreach/ documents some of the visits we hosted.</span></p> <br /><span>The PI was also faculty sponsor of a student-initiated robotics course and competition for underprivileged high schools, called Pioneers in Engineering (pioneers.berkeley.edu).</span><span><span> </span></span><span><span> </span></span></span></p> <p>&nbsp;</p><br> <p>            Last Modified: 06/30/2019<br>      Modified by: Pieter&nbsp;Abbeel</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941056934_tele-op--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941056934_tele-op--rgov-800width.jpg" title="Teaching Robots through VR Teleop"><img src="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941056934_tele-op--rgov-66x44.jpg" alt="Teaching Robots through VR Teleop"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be use</div> <div class="imageCredit">Pieter Abbeel</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel</div> <div class="imageTitle">Teaching Robots through VR Teleop</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941212494_David_with_floating_BLUE(1)--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941212494_David_with_floating_BLUE(1)--rgov-800width.jpg" title="Meet Blue: Berkeley Low-Cost, Capable, Safe Robot Arm"><img src="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941212494_David_with_floating_BLUE(1)--rgov-66x44.jpg" alt="Meet Blue: Berkeley Low-Cost, Capable, Safe Robot Arm"></a> <div class="imageCaptionContainer"> <div class="imageCaption">In this project we developed Blue: a low-cost, capable, safe robot arm.</div> <div class="imageCredit">Stephen McKinley</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel</div> <div class="imageTitle">Meet Blue: Berkeley Low-Cost, Capable, Safe Robot Arm</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941355762_blue--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941355762_blue--rgov-800width.jpg" title="Blue Robot"><img src="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941355762_blue--rgov-66x44.jpg" alt="Blue Robot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Meet Blue, the Berkeley robot designed from the ground up with AI / ML in mind.</div> <div class="imageCredit">Stephen McKinley</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel</div> <div class="imageTitle">Blue Robot</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941508122_ScreenShot2019-06-30at5.20.21PM--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941508122_ScreenShot2019-06-30at5.20.21PM--rgov-800width.jpg" title="Robot ties knots"><img src="/por/images/Reports/POR/2019/1351028/1351028_10293978_1561941508122_ScreenShot2019-06-30at5.20.21PM--rgov-66x44.jpg" alt="Robot ties knots"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Robot executing knot ties. Top row to bottom row: figure-eight knot, double-overhand knot,square knot, and clove hitch. Figure 1 already illustrated the overhand knot.</div> <div class="imageCredit">Pieter Abbeel</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Pieter&nbsp;Abbeel</div> <div class="imageTitle">Robot ties knots</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  This project has had 6 main thrusts:   (1) Fundamental Research to Enable Robots to Learn More Effectively from Demonstrations (Imitation Learning) and from Their Own Trial and Error (Reinforcement Learning) -- together called Apprenticeship Learning.    We studied, and advanced, non-rigid registration algorithms that can capture how scenes with deformable objects relate to each other.  After relating two scenes to each other, if a demonstration is available for completing the task in the first scene, the relationship between scenes can be used to understand how to complete the task in the second scene.  Key starting point was thin-plate-spline-robust-point-matching.  Among other things, we incorporated: lookahead; the ability to trade off path following vs. force imitation; accounting for mismatch between robot body and human demonstrator; better scene registration through considering normals and through deep learning.  The result of these developments was a robot that could meaningfully manipulate rope and tie knots across a wide range of scenarios, as well as fold towels.   We investigated imitation learning that leverages recent advances in deep supervised learning and virtual reality commoditization.  We showed  that behavioral cloning, with high-quality demonstrations, is surprisingly effective. We developed a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration.    As we saw our systems ever more capable of imitation, we started to shift focus towards reinforcement learning (RL), where a robot learns from its own trial and error (sometimes from scratch, sometimes bootstrapped off demonstrations).  We proposed an RL approach that enabled robots to learn to manipulate rope on their own.  We developed a new neural net architecture called Value Iteration Networks which allows RL to learn new skills more efficiently through the ability to plan.   We adapted the Value Iteration Network ideas to the continuous control setting, enabling new robotic assembly skills.  In a close collaboration with NASA, we studied autonomous control for tensegrity robots.  We also made advances in considering safety during learning and in ensuring the robot is communicative throughout its behavior.    These research advances were shared with the public through presentation and publication at the top robotics and machine learning venues:  ICRA 2015 (3x), IROS 2015 (4x), NeurIPS 2016, ICRA 2017 (4x), RSS 2017, CoRL 2017, ICRA 2018 (2x)    (2) Fundamental Research and Development of a New Robotic Arm that is Capable, Safe, Low-Cost.   Robots must cost less and must be force-controlled to enable widespread, safe deployment in unconstrained human environments. We proposed Quasi-Direct Drive actuation as a capable paradigm for robotic force-controlled manipulation in human environments at low-cost. Our prototype - Blue - is a human scale 7 Degree of Freedom arm with 2kg payload. Blue has a bill of materials of only $3,000. This could open up many new opportunities for distribution of robot arms to researchers and hobbyists, and from there (as software further matures) could result in affordable robots that can help people in their homes, e.g. enabling longer independent living.   The work on the BLUE project was published at ICRA 2019 (arm) and CASE 2019 (gripper).   The project page is here: http://rll.berkeley.edu/blue/ It was also covered extensively in the popular media, representative listing of outlets here: https://www.berkeleyopenarms.org/in-the-press.          (3) Training of the next generation of AI / robotics researchers.     This project directly contributed to the research training of 5 Post-docs, 8 PhD students, 1 Masters student, 18 Undergraduate students, and 1 High School student.         (4) Dissemination: The work was disseminated through 18 peer-reviewed publications, all available from the PI?s website.  The work under this grant has also been part of the PIs numerous public presentations over the past 5 years, estimated at about 25 (public) talks per year.       (5) Teaching Artificial Intelligence (a) at Berkeley, (b) worldwide through Massive Open Online Courses (MOOCs), (c) worldwide through sharing the Berkeley AI course materials with instructors elsewhere.   This project directly allowed the PI to further build out the UC Berkeley CS188 Artificial Intelligence course.   The PI also offered the course for free online through edX.    In addition, all the course and instructional materials are available to all instructors (and students) at ai.berkeley.edu.  By now the materials are used at over 100 other universities.       (6) K-12 Outreach to Encourage STEM:       There also has been substantial outreach, which we hope will inspire the next generation of scientists and engineers.  We hosted over 75 groups.  http://rll.berkeley.edu/outreach/ documents some of the visits we hosted.  The PI was also faculty sponsor of a student-initiated robotics course and competition for underprivileged high schools, called Pioneers in Engineering (pioneers.berkeley.edu).            Last Modified: 06/30/2019       Submitted by: Pieter Abbeel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
