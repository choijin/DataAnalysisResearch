<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Energy-Efficient Architectures for Emerging Big-Data Workloads</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>873286.00</AwardTotalIntnAmount>
<AwardAmount>889286</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In modern server architectures, the processor socket and the memory system&lt;br/&gt;are implemented as separate modules.  Data exchange between these modules&lt;br/&gt;is expensive -- it is slow, it consumes a large amount of energy, and there&lt;br/&gt;are long wait times for narrow data links.  Emerging big-data workloads will&lt;br/&gt;require especially large amounts of data movement between the processor&lt;br/&gt;and memory.  To reduce the cost of data movement for big-data workloads,&lt;br/&gt;the project attempts to design new server architectures that can leverage&lt;br/&gt;3D stacking technology.  The proposed approach, referred to as Near Data&lt;br/&gt;Computing (NDC), reduces the distance between a subset of computational&lt;br/&gt;units and a subset of memory, and can yield high efficiency for workloads&lt;br/&gt;that exhibit locality.  The project will also develop new big-data &lt;br/&gt;algorithms and runtime systems that can exploit the properties of the&lt;br/&gt;new architectures.&lt;br/&gt;&lt;br/&gt;The project will lead to technologies that can boost performance and&lt;br/&gt;reduce the energy demands of big-data workloads.  Several reports have&lt;br/&gt;cited the importance of these workloads to national, industrial, and&lt;br/&gt;scientific computing infrastructures.  The project outcomes will be&lt;br/&gt;integrated into University of Utah curricula and will play a significant&lt;br/&gt;role in a new degree program on datacenter design and operation.  The&lt;br/&gt;PIs will broaden their impact by publicly distributing parts of their&lt;br/&gt;software infrastructure and by engaging in outreach programs that&lt;br/&gt;involve minorities and K-12 students.</AbstractNarration>
<MinAmdLetterDate>06/25/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302663</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Davis</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan L Davis</PI_FULL_NAME>
<EmailAddress>ald@cs.utah.edu</EmailAddress>
<PI_PHON>8015813991</PI_PHON>
<NSF_ID>000424298</NSF_ID>
<StartDate>06/25/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mary</FirstName>
<LastName>Hall</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mary Hall</PI_FULL_NAME>
<EmailAddress>mhall@cs.utah.edu</EmailAddress>
<PI_PHON>8015851039</PI_PHON>
<NSF_ID>000367228</NSF_ID>
<StartDate>06/25/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rajeev</FirstName>
<LastName>Balasubramonian</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rajeev Balasubramonian</PI_FULL_NAME>
<EmailAddress>rajeev@cs.utah.edu</EmailAddress>
<PI_PHON>8015814553</PI_PHON>
<NSF_ID>000136755</NSF_ID>
<StartDate>06/25/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Feifei</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Feifei Li</PI_FULL_NAME>
<EmailAddress>lifeifei@cs.utah.edu</EmailAddress>
<PI_PHON>8015856673</PI_PHON>
<NSF_ID>000598994</NSF_ID>
<StartDate>06/25/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName/>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~209943</FUND_OBLG>
<FUND_OBLG>2014~436176</FUND_OBLG>
<FUND_OBLG>2016~243167</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project's central objective was to explore hardware-software techniques to move computations closer to data, thus reducing the cost (latency, bandwidth, and energy) of data movement for big-data workloads.</p> <p>&nbsp;On the hardware/database front, the project explored new 3D-stacked memory devices and augmented DIMMs that are equipped with simple cores.&nbsp; MapReduce workloads were then parallelized across the many cores spread across a large memory system to extract more than an order of magnitude speedup.</p> <p>&nbsp;The project also explored machine learning workloads.&nbsp; An extreme form of near-data processing was explored by leveraging memristor devices.&nbsp; The project showed that memristor devices can be used not only to store machine learning parameters, but also to perform in-situ dot-product operations.&nbsp; This reduces data movement, and offers unprecedented levels of compute/storage density.&nbsp; The project explored various techniques to reduce the overhead of the required analog units.&nbsp; The eventual design outperformed state-of-the-art machine learning accelerators by 8X.</p> <p>&nbsp;The project also augmented in-memory graph analytics by leveraging the concept of in-memory concurrent transaction processing for big graphs. By integrating trasaction processing and in-memory computing techniques, the proposed design significantly outperformed other parallel processing models for large-scale graph analytics.</p> <p>To show applicability for a broader set of workloads, the project also introduced an in-memory sampling technique for large spatial and spatio-temporal data. These samples were then used for various complex data analytics tasks.</p> <p>In addition, the project explored compiler extensions to reduce data movement.&nbsp; GPU implementations of sparse graph algorithms, such as the Stochastic Gradient Descent algorithm used in recommendation engines, were used as target workloads.&nbsp; A custom data representation was employed to to achieve locality and wavefront parallelism.</p> <p>&nbsp;Overall, the project showed that near-data processing, with help from the compiler and new memory devices, can achieve an order of magnitude improvement on analytics workloads dealing with massive datasets.</p> <p>&nbsp;The PIs also engaged in several outreach and educational activities for students ranging from elementary schools to graduate schools.&nbsp; Workshops and magazine special issues on near-data processing were organized.&nbsp; The project has produced many graduates with key skills to drive the nation's technological workforce.</p><br> <p>            Last Modified: 10/28/2018<br>      Modified by: Rajeev&nbsp;Balasubramonian</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project's central objective was to explore hardware-software techniques to move computations closer to data, thus reducing the cost (latency, bandwidth, and energy) of data movement for big-data workloads.   On the hardware/database front, the project explored new 3D-stacked memory devices and augmented DIMMs that are equipped with simple cores.  MapReduce workloads were then parallelized across the many cores spread across a large memory system to extract more than an order of magnitude speedup.   The project also explored machine learning workloads.  An extreme form of near-data processing was explored by leveraging memristor devices.  The project showed that memristor devices can be used not only to store machine learning parameters, but also to perform in-situ dot-product operations.  This reduces data movement, and offers unprecedented levels of compute/storage density.  The project explored various techniques to reduce the overhead of the required analog units.  The eventual design outperformed state-of-the-art machine learning accelerators by 8X.   The project also augmented in-memory graph analytics by leveraging the concept of in-memory concurrent transaction processing for big graphs. By integrating trasaction processing and in-memory computing techniques, the proposed design significantly outperformed other parallel processing models for large-scale graph analytics.  To show applicability for a broader set of workloads, the project also introduced an in-memory sampling technique for large spatial and spatio-temporal data. These samples were then used for various complex data analytics tasks.  In addition, the project explored compiler extensions to reduce data movement.  GPU implementations of sparse graph algorithms, such as the Stochastic Gradient Descent algorithm used in recommendation engines, were used as target workloads.  A custom data representation was employed to to achieve locality and wavefront parallelism.   Overall, the project showed that near-data processing, with help from the compiler and new memory devices, can achieve an order of magnitude improvement on analytics workloads dealing with massive datasets.   The PIs also engaged in several outreach and educational activities for students ranging from elementary schools to graduate schools.  Workshops and magazine special issues on near-data processing were organized.  The project has produced many graduates with key skills to drive the nation's technological workforce.       Last Modified: 10/28/2018       Submitted by: Rajeev Balasubramonian]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
