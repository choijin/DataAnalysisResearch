<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>147236.00</AwardTotalIntnAmount>
<AwardAmount>147236</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today's architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.&lt;br/&gt;The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.&lt;br/&gt;MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities.</AbstractNarration>
<MinAmdLetterDate>08/29/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1339763</AwardID>
<Investigator>
<FirstName>Edgar</FirstName>
<LastName>Gabriel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edgar Gabriel</PI_FULL_NAME>
<EmailAddress>gabriel@cs.uh.edu</EmailAddress>
<PI_PHON>7137433857</PI_PHON>
<NSF_ID>000316336</NSF_ID>
<StartDate>08/29/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036837920</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042916627</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>772042015</ZipCode>
<StreetAddress><![CDATA[4800 Calhoun Blvd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8005</Code>
<Text>Scientific Software Elements</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~147236</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Open MPI is an open source implementation of the Message Passing Interface (MPI) specification &ndash; one of the most widely used parallel programming paradigms on High Performance Computing (HPC). The library is jointly developed and maintained by a consortium of academic, research, and industrial partners. The Open MPI consortium combines the expertise, technologies, and resources from numerous members of the High Performance Computing community in order to build a production-quality MPI library, offering different advantages to system and software vendors, application developers and computer science researchers. The goal of this project was to enhance, harden, and modernize the Open MPI library in the context of the ongoing developments in processor architecture and system design.</p> <p>The specific contributions and achievements of the University of Houston in this project were focused on parallel I/O. Parallel I/O refers the ability of multiple processes to access (different parts of) the same file in an efficient and consistent manner. Since most large problems solved on High Performance Computing systems operate on very large data files &ndash; often exceeding tens or even hundreds of Gigabytes in overall size - the time spent in reading and writing these files represent a significant fraction of the overall execution time of these applications. Thus, minimizing the time spent in reading and writing these large files are crucial for running large applications.</p> <p>OMPIO is a parallel I/O library developed by the University of Houston as part of the Open MPI project. It consists of a set of frameworks to separate the parallel I/O functionality into separate subgroups and provide different components each targeting different application scenarios and file systems. In addition, each framework contains sophisticated selection logic to decide which component to use in a particular scenario.</p> <p>The key achievements of the ADAPT project is making OMPIO ready for utilization in every day production environments by significantly extending and hardening the code base both from the functionality as well as the stability perspective. OMPIO is now the default parallel I/O library on most file systems in Open MPI, starting from the 2.0.0 release in July 2016. Thus, the project enabled transforming a research project into a tool that enables research by a wide range of scientists in the national laboratories and universities.</p> <p>Further contributions include adding support for asynchronous/non-blocking read/write operations, design and evaluation of new aggregator selection logic for collective I/O operations, and extending the list of file systems supported by OMPIO.</p><br> <p>            Last Modified: 10/03/2016<br>      Modified by: Edgar&nbsp;Gabriel</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Open MPI is an open source implementation of the Message Passing Interface (MPI) specification &ndash; one of the most widely used parallel programming paradigms on High Performance Computing (HPC). The library is jointly developed and maintained by a consortium of academic, research, and industrial partners. The Open MPI consortium combines the expertise, technologies, and resources from numerous members of the High Performance Computing community in order to build a production-quality MPI library, offering different advantages to system and software vendors, application developers and computer science researchers. The goal of this project was to enhance, harden, and modernize the Open MPI library in the context of the ongoing developments in processor architecture and system design.  The specific contributions and achievements of the University of Houston in this project were focused on parallel I/O. Parallel I/O refers the ability of multiple processes to access (different parts of) the same file in an efficient and consistent manner. Since most large problems solved on High Performance Computing systems operate on very large data files &ndash; often exceeding tens or even hundreds of Gigabytes in overall size - the time spent in reading and writing these files represent a significant fraction of the overall execution time of these applications. Thus, minimizing the time spent in reading and writing these large files are crucial for running large applications.  OMPIO is a parallel I/O library developed by the University of Houston as part of the Open MPI project. It consists of a set of frameworks to separate the parallel I/O functionality into separate subgroups and provide different components each targeting different application scenarios and file systems. In addition, each framework contains sophisticated selection logic to decide which component to use in a particular scenario.  The key achievements of the ADAPT project is making OMPIO ready for utilization in every day production environments by significantly extending and hardening the code base both from the functionality as well as the stability perspective. OMPIO is now the default parallel I/O library on most file systems in Open MPI, starting from the 2.0.0 release in July 2016. Thus, the project enabled transforming a research project into a tool that enables research by a wide range of scientists in the national laboratories and universities.  Further contributions include adding support for asynchronous/non-blocking read/write operations, design and evaluation of new aggregator selection logic for collective I/O operations, and extending the list of file systems supported by OMPIO.       Last Modified: 10/03/2016       Submitted by: Edgar Gabriel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
