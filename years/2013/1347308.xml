<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Information Procuration via Adaptive Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>99714.00</AwardTotalIntnAmount>
<AwardAmount>99714</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balasubramanian Kalyanasundaram</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The Principal Investigator (PI) formulates the information procuration problem of converting unstructured data into structured information as one of using limited resources (such as processing time and collection costs) among several available strategies for information acquisition, extraction, collation and aggregation in a sequential and adaptive manner. The proposal aims to build a Markov decision process (MDP) for which both the states and the rewards will be learned, and from which an optimal adaptive strategy for effective information procuration will be extracted.&lt;br/&gt;&lt;br/&gt;Recent methods for designing adaptive strategies for multi-armed bandit problems and budgeted learning approaches by the PI will be extended for this purpose, as well as techniques from inverse reinforcement learning. Moreover, given the intended size of the application data sets, the focus will be on on scalable algorithms for these problems. Due to the centrality of the problem, new approaches to making better sense of unstructured data will have much impact both in terms of developing new methods and in practice. The proposed synthesis of methods from Operations Research, Approximation Algorithms and Machine Learning is novel in this context. This proposal will increase the cross-fertilization of ideas between Operations Research and Machine Learning, via a collaboration team formed at this intersection.</AbstractNarration>
<MinAmdLetterDate>08/21/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/21/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1347308</AwardID>
<Investigator>
<FirstName>Ramamoorthi</FirstName>
<LastName>Ravi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ramamoorthi Ravi</PI_FULL_NAME>
<EmailAddress>ravi@cmu.edu</EmailAddress>
<PI_PHON>4122683694</PI_PHON>
<NSF_ID>000110479</NSF_ID>
<StartDate>08/21/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~99714</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>An important aspect of curating data into useful, structured information is to provide recommendation links that expose relevant yet largely unexplored pages in a robust and reduandant way that maximizes the discoverability of the less visited pages.</p> <p>The process of searcing for relevant information in the world-wide web is greatly aided by the presence of recommendations that link information pages to others that bear closely related information. Such recommendations are also ubiquitous in retail sites, video and movie streaming services as well as in social question-answering sites. The choice of relevant pages to recommend has been well studied with most existing methods using the textual content of the originating and destination pages, as well as some additional information about other pages that are linked to or from them.</p> <p>In this work, we have re-cast the problem of choosing pages to recommend as a problem of exposing as many of undiscovered pages as possible. Typical web-sites that hold information have a very small fraction of highly visited and popular pages and a very large unexplored set of undiscovered pages. Given the paucity of how many pages we can recommend from each popular page, and the very large number of undiscovered pages, we consider the choice of recommendations as a two step process: First, a large list of relevant undiscovered pages are chosen per popular page as candidates using traditional recommendation methods (typically running to the hundreds per page). Second, we choose a much smaller list of candidate pages to recommend (about ten or so among the hundreds) so that the undiscovered pages chosen are redundantly linked from at least one or more popular pages. The precise requirements of this recommendation design problem are specified by how many links can be put out from each popular page, as well as the requirement of how redundantly each undiscovered page must be covered for it to be counted.</p> <p>Our work formulates this class of recommendation subgraph design problems and presents three highly scalable algorithms for providing good solutions. We also analyze these methods on simple theoretical models of random generation of the underlying data, and perform simulations on such data to verify our conclusions abut the relative effectiveness of the three methods depending on the problem settings.Our methods have been useful in the deployment of effective recommendations in many web relevance engines.</p> <p>A full report is publicly available here: http://arxiv.org/abs/1409.2042&nbsp;</p><br> <p>            Last Modified: 10/07/2014<br>      Modified by: Ramamoorthi&nbsp;Ravi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ An important aspect of curating data into useful, structured information is to provide recommendation links that expose relevant yet largely unexplored pages in a robust and reduandant way that maximizes the discoverability of the less visited pages.  The process of searcing for relevant information in the world-wide web is greatly aided by the presence of recommendations that link information pages to others that bear closely related information. Such recommendations are also ubiquitous in retail sites, video and movie streaming services as well as in social question-answering sites. The choice of relevant pages to recommend has been well studied with most existing methods using the textual content of the originating and destination pages, as well as some additional information about other pages that are linked to or from them.  In this work, we have re-cast the problem of choosing pages to recommend as a problem of exposing as many of undiscovered pages as possible. Typical web-sites that hold information have a very small fraction of highly visited and popular pages and a very large unexplored set of undiscovered pages. Given the paucity of how many pages we can recommend from each popular page, and the very large number of undiscovered pages, we consider the choice of recommendations as a two step process: First, a large list of relevant undiscovered pages are chosen per popular page as candidates using traditional recommendation methods (typically running to the hundreds per page). Second, we choose a much smaller list of candidate pages to recommend (about ten or so among the hundreds) so that the undiscovered pages chosen are redundantly linked from at least one or more popular pages. The precise requirements of this recommendation design problem are specified by how many links can be put out from each popular page, as well as the requirement of how redundantly each undiscovered page must be covered for it to be counted.  Our work formulates this class of recommendation subgraph design problems and presents three highly scalable algorithms for providing good solutions. We also analyze these methods on simple theoretical models of random generation of the underlying data, and perform simulations on such data to verify our conclusions abut the relative effectiveness of the three methods depending on the problem settings.Our methods have been useful in the deployment of effective recommendations in many web relevance engines.  A full report is publicly available here: http://arxiv.org/abs/1409.2042        Last Modified: 10/07/2014       Submitted by: Ramamoorthi Ravi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
