<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Identifying Linguistic Factors Associated with  Differential Student Performance on Middle School Science Assessments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1102618.00</AwardTotalIntnAmount>
<AwardAmount>1102618</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gregg Solomon</SignBlockName>
<PO_EMAI>gesolomo@nsf.gov</PO_EMAI>
<PO_PHON>7032928333</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project, conducted by the American Association for the Advancement of Science (AAAS), seeks to improve assessment of science understanding in K-12 students.  The project will cover a wide range of 16 topics within science education.  When a student takes a science test, performance will depend on cognitive factors, what the student knows, and linguistic factors, related to language.  This project aims to distinguish cognitive and linguistic factors, with a focus on English language learners.  &lt;br/&gt;&lt;br/&gt;The first stage of the project involves data analysis, using more than 800 assessment items previously developed by AAAS to align with middle and high school science standards, and tested on more than 100,000 students.  This stage involves extensive analyses of the test questions (e.g.,, the language used) as well as student performance.  After it is determined what are the factors that affect differential performance based on English language learner status, the second stage of the project involves developing and testing new versions of the science assessments. &lt;br/&gt;&lt;br/&gt;This project addresses the important issue highlighted by the 2012 National Research Council report, A Framework for K-12 Science Education, that assessment developers must remove barriers to science participation due to English language learner status.  In other words, this project aims to produce science tests that accurately assess science knowledge for both  English language learners and non-English language learners.  &lt;br/&gt;&lt;br/&gt;This work helps the Education and Human Resources directorate, and the Division of Research on Learning, pursue the mission of supporting science, technology, engineering and mathematics (STEM) education research.  In particular, this project focuses on improving understanding of STEM learning and STEM assessment, as well as broadening participation in STEM education and ultimately the STEM workforce.</AbstractNarration>
<MinAmdLetterDate>03/25/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1348622</AwardID>
<Investigator>
<FirstName>George</FirstName>
<LastName>DeBoer</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George E DeBoer</PI_FULL_NAME>
<EmailAddress>gdeboer@aaas.org</EmailAddress>
<PI_PHON>2023266624</PI_PHON>
<NSF_ID>000201452</NSF_ID>
<StartDate>03/25/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sharon</FirstName>
<LastName>Nelson-Barber</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sharon S Nelson-Barber</PI_FULL_NAME>
<EmailAddress>snelson@wested.org</EmailAddress>
<PI_PHON>6504528267</PI_PHON>
<NSF_ID>000225822</NSF_ID>
<StartDate>03/25/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Chun-Wei</FirstName>
<LastName>Huang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Chun-Wei Huang</PI_FULL_NAME>
<EmailAddress>chuang@wested.org</EmailAddress>
<PI_PHON>4156153162</PI_PHON>
<NSF_ID>000649335</NSF_ID>
<StartDate>03/25/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>American Association for the Advancement of Science</Name>
<CityName>Washington</CityName>
<ZipCode>200053928</ZipCode>
<PhoneNumber>2023266400</PhoneNumber>
<StreetAddress>1200 NEW YORK AVENUE, N.W.</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>077795672</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>AMERICAN ASSOCIATION FOR THE ADVANCEMENT OF SCIENCE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>077795672</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[American Association For Advancement Science]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200053928</ZipCode>
<StreetAddress><![CDATA[1200 New York Avenue, NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7625</Code>
<Text>REAL</Text>
</ProgramElement>
<ProgramElement>
<Code>7980</Code>
<Text>ECR-EHR Core Research</Text>
</ProgramElement>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0415</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0416</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0417</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~276171</FUND_OBLG>
<FUND_OBLG>2015~269658</FUND_OBLG>
<FUND_OBLG>2016~285438</FUND_OBLG>
<FUND_OBLG>2017~271351</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Given that English learners (ELs) are such a rapidly growing segment of the U.S. student population and that their performance on large-scale assessments is widely acknowledged to be lower than that of non-EL students, understanding what factors differentially affect EL student performance is critical. One explanation for the lower EL test scores is that linguistic features of a test item make interpreting what is being asked more difficult than necessary for EL students and that modifying how test items are written might produce a more accurate measure of EL students' knowledge in science.</p> <p>To explore this premise, a team of scientists, education researchers, linguists, and statisticians from the American Association for the Advancement of Science (AAAS) and WestEd used a large data set involving more than 100,000 students and nearly 1,000 assessment items aligned to middle and early high school content standards in science. The items were previously designed by AAAS's Project 2061 with funding from the National Science Foundation (ESI-0352473). &nbsp;</p> <p>Over the course of this project, the research team identified and then compared the relative importance of linguistic features of an item (e.g., its vocabulary, grammar, and coherence) versus cognitive complexity features of an item (e.g., the type of knowledge and mental processing that is needed to respond) when applied to the performance of EL and non-EL students on science tests. The team then proposed strategies for eliminating linguistic features that might result in an underestimation of EL students' science knowledge and used the strategies to develop a set of revised items. In a validation study and follow-up focus group interviews with native-English and native-Spanish speaking students, the team investigated whether the changes made in the items actually narrowed the performance gap between EL and non-EL students.</p> <p>In the end, the research team was unable to find anything that could reliably explain the performance difference between the two groups of students. None of the cognitive or linguistic measures developed for the study proved to be statistically significant predictors of the performance of native-English-speakers, English learners, or the difference between them. The team concluded that a difference in knowledge of the science content itself was such a strong, determining factor in how students performed on these test items that it overwhelmed the likely smaller impact of any of the item features that were examined. This finding has enormous implications for science education and points to the need to address the lower performance of EL students at the level of instruction.</p> <p>It's not that language doesn't matter, but it may matter more at the level of opportunity to learn than at the level of testing what students have learned. Students need opportunities to learn science content and associated vocabulary. But they also need to learn general academic vocabulary and ways of using language in the classroom--two aspects of language proficiency that are of huge importance to the academic success ELs. This is particularly important as new science standards call for instruction that expects students to make greater use of science practices that require sophisticated language skills, such as providing evidence-based explanations as they make sense of phenomena and solve engineering problems. By many measures, schools are falling short on providing all students--not just ELs--with these essential learning opportunities. How best to address these deficits presents serious challenges to policy makers, educators, and citizens. Findings from this study can be used to help inform their decision making process.</p> <p><span style="text-decoration: underline;"><br /></span></p><br> <p>            Last Modified: 08/06/2019<br>      Modified by: George&nbsp;E&nbsp;Deboer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Given that English learners (ELs) are such a rapidly growing segment of the U.S. student population and that their performance on large-scale assessments is widely acknowledged to be lower than that of non-EL students, understanding what factors differentially affect EL student performance is critical. One explanation for the lower EL test scores is that linguistic features of a test item make interpreting what is being asked more difficult than necessary for EL students and that modifying how test items are written might produce a more accurate measure of EL students' knowledge in science.  To explore this premise, a team of scientists, education researchers, linguists, and statisticians from the American Association for the Advancement of Science (AAAS) and WestEd used a large data set involving more than 100,000 students and nearly 1,000 assessment items aligned to middle and early high school content standards in science. The items were previously designed by AAAS's Project 2061 with funding from the National Science Foundation (ESI-0352473).    Over the course of this project, the research team identified and then compared the relative importance of linguistic features of an item (e.g., its vocabulary, grammar, and coherence) versus cognitive complexity features of an item (e.g., the type of knowledge and mental processing that is needed to respond) when applied to the performance of EL and non-EL students on science tests. The team then proposed strategies for eliminating linguistic features that might result in an underestimation of EL students' science knowledge and used the strategies to develop a set of revised items. In a validation study and follow-up focus group interviews with native-English and native-Spanish speaking students, the team investigated whether the changes made in the items actually narrowed the performance gap between EL and non-EL students.  In the end, the research team was unable to find anything that could reliably explain the performance difference between the two groups of students. None of the cognitive or linguistic measures developed for the study proved to be statistically significant predictors of the performance of native-English-speakers, English learners, or the difference between them. The team concluded that a difference in knowledge of the science content itself was such a strong, determining factor in how students performed on these test items that it overwhelmed the likely smaller impact of any of the item features that were examined. This finding has enormous implications for science education and points to the need to address the lower performance of EL students at the level of instruction.  It's not that language doesn't matter, but it may matter more at the level of opportunity to learn than at the level of testing what students have learned. Students need opportunities to learn science content and associated vocabulary. But they also need to learn general academic vocabulary and ways of using language in the classroom--two aspects of language proficiency that are of huge importance to the academic success ELs. This is particularly important as new science standards call for instruction that expects students to make greater use of science practices that require sophisticated language skills, such as providing evidence-based explanations as they make sense of phenomena and solve engineering problems. By many measures, schools are falling short on providing all students--not just ELs--with these essential learning opportunities. How best to address these deficits presents serious challenges to policy makers, educators, and citizens. Findings from this study can be used to help inform their decision making process.          Last Modified: 08/06/2019       Submitted by: George E Deboer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
