<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Enhancing external validity in existing STEM evaluations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>796205.00</AwardTotalIntnAmount>
<AwardAmount>796205</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gregg Solomon</SignBlockName>
<PO_EMAI>gesolomo@nsf.gov</PO_EMAI>
<PO_PHON>7032928333</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In light of increasing accountability pressures, the PI proposes to develop, describe and test statistical methods for reducing external validity bias in random assignment evaluations of STEM programs carried out in a non-representative samples of sites.  A series of simulations will be conducted using real-world information to test the conditions under  which different statistical methods can reduce the external validity bias.&lt;br/&gt;&lt;br/&gt;The methodological approach examines real world data from four kinds of studies: regression with interactions; Bayesian additive regression trees (BART); inverse probablility of selection weighting (IPSW); and Subclassification.&lt;br/&gt;&lt;br/&gt;The use of real world data in the proposed simulation studies will test the efficacy of each in reducing such bias. Four products will result from the work that will be highly useful to STEM evaluators and researchers: (1) a better understanding of the types of schools and districts that participate in impact evaluation; (2) analysis methods that researchers can use to improve the external validity of existing evaluations; (3) guidance on types of data to collect in future evaluations; and a data set and framework researchers can use to conduct simulation studies to investigate other methodological questions.</AbstractNarration>
<MinAmdLetterDate>09/24/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/24/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1335843</AwardID>
<Investigator>
<FirstName>Elizabeth</FirstName>
<LastName>Stuart</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elizabeth A Stuart</PI_FULL_NAME>
<EmailAddress>estuart@jhsph.edu</EmailAddress>
<PI_PHON>4105168668</PI_PHON>
<NSF_ID>000085528</NSF_ID>
<StartDate>09/24/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Olsen</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert B Olsen</PI_FULL_NAME>
<EmailAddress>Rob_Olsen@abtassoc.com</EmailAddress>
<PI_PHON>6174927100</PI_PHON>
<NSF_ID>000642328</NSF_ID>
<StartDate>09/24/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212052103</ZipCode>
<StreetAddress><![CDATA[624 North Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<Appropriation>
<Code>0413</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~796205</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Most evaluations of social and behavioral interventions, including those in education, are conducted in groups of people, schools, or communities that do not look like the people, schools, or communities for which policy decisions need to be made on the basis of those evaluations.&nbsp; This project has helped researchers and decision makers understand how to design and analyze studies in ways that will help ensure that the results are relevant for the decision makers that need to act for their communities.&nbsp; We documented that the types of school districts that participate in typical large-scale evaluations of education programs are quite larger and more disadvantaged than typical districts across the United States. We also suggested and examined the performance of alternative study designs and analysis procedures that can help adjust the samples to make them more relevant for policy decisions. &nbsp;Finally, we explored the usefulness of using existing data to help estimate the effects of interventions in target populations, showing that it can be difficult to find evaluation and population data that have comparable measures and thus the importance of recent efforts that aim for consistency of measures across studies.&nbsp; Given those data challenges, we also developed statistical methods to adjust estimates of program effectiveness in populations to account for potential unobserved factors that differ between the individuals in the evaluation and those in the population of interest. The results of the study are relevant for the full range of STEM interventions, across elementary, middle, and high school, and for other fields, such as social work, public health, and medicine. The resulting better understanding of how to design and analyze studies for estimating effects of interventions in populations could have broad impacts on society, by helping decision makers better understand what they can (and can&rsquo;t) learn from existing evaluations, and helping ensure that evaluations are conducted in ways that are most useful for the decisions that need to be made.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/08/2017<br>      Modified by: Elizabeth&nbsp;A&nbsp;Stuart</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Most evaluations of social and behavioral interventions, including those in education, are conducted in groups of people, schools, or communities that do not look like the people, schools, or communities for which policy decisions need to be made on the basis of those evaluations.  This project has helped researchers and decision makers understand how to design and analyze studies in ways that will help ensure that the results are relevant for the decision makers that need to act for their communities.  We documented that the types of school districts that participate in typical large-scale evaluations of education programs are quite larger and more disadvantaged than typical districts across the United States. We also suggested and examined the performance of alternative study designs and analysis procedures that can help adjust the samples to make them more relevant for policy decisions.  Finally, we explored the usefulness of using existing data to help estimate the effects of interventions in target populations, showing that it can be difficult to find evaluation and population data that have comparable measures and thus the importance of recent efforts that aim for consistency of measures across studies.  Given those data challenges, we also developed statistical methods to adjust estimates of program effectiveness in populations to account for potential unobserved factors that differ between the individuals in the evaluation and those in the population of interest. The results of the study are relevant for the full range of STEM interventions, across elementary, middle, and high school, and for other fields, such as social work, public health, and medicine. The resulting better understanding of how to design and analyze studies for estimating effects of interventions in populations could have broad impacts on society, by helping decision makers better understand what they can (and can?t) learn from existing evaluations, and helping ensure that evaluations are conducted in ways that are most useful for the decisions that need to be made.          Last Modified: 11/08/2017       Submitted by: Elizabeth A Stuart]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
