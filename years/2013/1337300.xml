<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: DSD: Collaborative Research: NeoNexus: The Next-generation Information Processing System across Digital and Neuromorphic Computing Domains</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>275884.00</AwardTotalIntnAmount>
<AwardAmount>275884</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>tao li</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The explosion of "big data" applications imposes severe challenges of data processing speed and scalability on traditional computer systems. The performance of traditional Von Neumann machines is greatly hindered by the increasing performance gap between CPU and memory, motivating the active research on new or alternative computing architectures. By imitating brain's naturally massive parallel architecture with closely coupled memory and computing as well as the unique analog domain operations, neuromorphic computing systems are anticipated to deliver superior speed for applications in image recognition and natural language understanding.&lt;br/&gt;&lt;br/&gt;The objective of this research is to establish the fundamental framework and design methodology for NeoNexus -- the next-generation information processing system inspired by human neocortex. It integrates neuromorphic computing accelerators with conventional computing resources by leveraging large scale inference-based data processing and computing acceleration technique atop memristor crossbar arrays. The computation and data exchange will be carefully coordinated and supported by the innovative interconnect architecture, i.e., a hierarchical network-on-chip (NoC). The software-hardware co-design platform will be developed to address the various design challenges. The project will help computer architecture and high-performance computing communities to overcome the ever-increasing technical challenges of traditional architectures and accelerate the fusion between conventional computing technology and cognitive computing model. It will also promote the applications of artificial intelligence technology advances in modern computer architectures and motivate the inventions at both software and hardware levels. Undergraduate and graduate students involved in this research will be trained for the next-generation semiconductor industry workforce.</AbstractNarration>
<MinAmdLetterDate>09/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1337300</AwardID>
<Investigator>
<FirstName>Qinru</FirstName>
<LastName>Qiu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Qinru Qiu</PI_FULL_NAME>
<EmailAddress>qiqiu@syr.edu</EmailAddress>
<PI_PHON>3154431836</PI_PHON>
<NSF_ID>000489809</NSF_ID>
<StartDate>09/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Syracuse University</Name>
<CityName>SYRACUSE</CityName>
<ZipCode>132441200</ZipCode>
<PhoneNumber>3154432807</PhoneNumber>
<StreetAddress>OFFICE OF SPONSORED PROGRAMS</StreetAddress>
<StreetAddress2><![CDATA[211 Lyman Hall]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY24</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002257350</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SYRACUSE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002257350</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Syracuse University]]></Name>
<CityName>Syracuse</CityName>
<StateCode>NY</StateCode>
<ZipCode>132441200</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY24</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~275884</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this XPS project, we studied the characteristics of bio-inspired machine intelligence models and explored their implementation on massive parallel platforms.&nbsp;</p> <p>Three types of machine intelligence models were investigated. They include: (1) a recurrent belief propagation network for Bayesian inference. This network is trained using statistical information extracted from the training data. (2) a recurrent deep neural network. This network is trained based on standard stochastic gradient descent algorithm. (3) a biological plausible Spiking Neural Network (SNN) model. This network is trained based on Spike-timing-dependent plasticity (STDP), which is a biological process that adjusts the strength of connections between neurons.</p> <p>Four hardware platforms are investigates. They include: (1) Multi-core general purpose computing platform such as Xeon Phi processor and GPGPU, (2) FPGA based hardware accelerator, (3) neurosynaptic processor with programmable physical neuron cores and inter-core connections, (4) memristor based crossbar arrays.</p> <p>Our study reveals the intrinsic relation between an STDP learned network and a Bayesian network. We showed that the synaptic weight learned using STDP rule is highly correlated to the conditional probability of the firing of the pre-synaptic neuron given the status of the post-synaptic neuron, which is also the definition of weight of links in a Bayesian network. This indicates that the two networks are essentially similar except that one of them encode information using the rate of the spike train.</p> <p>We also show that asynchronous neuron activities not only simplifies hardware design, improves throughput, but also improves recall accuracy for recurrent belief propagation networks. The uncertainty in the neuron evaluation order caused by racing among neurons introduces randomness, which may bring the system out from local optimum, hence gives 5% higher recall accuracy than a system that follows fixed neuron evaluation order.</p> <p>To efficiently map a belief propagation network to a crossbar based memristor array, we developed a generalized sparse matrix reordering (GSMR) technique to compress its sparse connectivity matrix. The GSMR technique re-orders the rows and columns in a sparse matrix so that minimum size non-zero block cover can be achieved. The compression leads to 90% performance improvements and 44% energy reduction compared to an implementation without GSMR compression.&nbsp;</p> <p>We further explored techniques to map the three bio-inspired neural networks to a neurosynaptic processor, which features crossbar array based neuron cores and asynchronous network-on-chip inter-core communications. The tradeoffs among hardware cost, performance, and accuracy have been investigated. We show that in a recurrent neural network, feedback variables such as cell state has higher impact to the model accuracy than other variables, and hence should be implemented with higher precision. Our constrained-then-train approach and encoding schedule can improve the accuracy of a Long Short Term Memory from 58% to 96%.</p> <p>Five PhD students worked on this project. The PI and her colleagues organized the IEEE Transactions on Multi-Scale Computing Systems (TMSCS) special issue on Design and Applications of Neuromorphic Computing System and served as the guest editor. A new course on &ldquo;Machine intelligence with Deep Learning&rdquo; was prepared and offered in Fall 2017.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/20/2017<br>      Modified by: Qinru&nbsp;Qiu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this XPS project, we studied the characteristics of bio-inspired machine intelligence models and explored their implementation on massive parallel platforms.   Three types of machine intelligence models were investigated. They include: (1) a recurrent belief propagation network for Bayesian inference. This network is trained using statistical information extracted from the training data. (2) a recurrent deep neural network. This network is trained based on standard stochastic gradient descent algorithm. (3) a biological plausible Spiking Neural Network (SNN) model. This network is trained based on Spike-timing-dependent plasticity (STDP), which is a biological process that adjusts the strength of connections between neurons.  Four hardware platforms are investigates. They include: (1) Multi-core general purpose computing platform such as Xeon Phi processor and GPGPU, (2) FPGA based hardware accelerator, (3) neurosynaptic processor with programmable physical neuron cores and inter-core connections, (4) memristor based crossbar arrays.  Our study reveals the intrinsic relation between an STDP learned network and a Bayesian network. We showed that the synaptic weight learned using STDP rule is highly correlated to the conditional probability of the firing of the pre-synaptic neuron given the status of the post-synaptic neuron, which is also the definition of weight of links in a Bayesian network. This indicates that the two networks are essentially similar except that one of them encode information using the rate of the spike train.  We also show that asynchronous neuron activities not only simplifies hardware design, improves throughput, but also improves recall accuracy for recurrent belief propagation networks. The uncertainty in the neuron evaluation order caused by racing among neurons introduces randomness, which may bring the system out from local optimum, hence gives 5% higher recall accuracy than a system that follows fixed neuron evaluation order.  To efficiently map a belief propagation network to a crossbar based memristor array, we developed a generalized sparse matrix reordering (GSMR) technique to compress its sparse connectivity matrix. The GSMR technique re-orders the rows and columns in a sparse matrix so that minimum size non-zero block cover can be achieved. The compression leads to 90% performance improvements and 44% energy reduction compared to an implementation without GSMR compression.   We further explored techniques to map the three bio-inspired neural networks to a neurosynaptic processor, which features crossbar array based neuron cores and asynchronous network-on-chip inter-core communications. The tradeoffs among hardware cost, performance, and accuracy have been investigated. We show that in a recurrent neural network, feedback variables such as cell state has higher impact to the model accuracy than other variables, and hence should be implemented with higher precision. Our constrained-then-train approach and encoding schedule can improve the accuracy of a Long Short Term Memory from 58% to 96%.  Five PhD students worked on this project. The PI and her colleagues organized the IEEE Transactions on Multi-Scale Computing Systems (TMSCS) special issue on Design and Applications of Neuromorphic Computing System and served as the guest editor. A new course on "Machine intelligence with Deep Learning" was prepared and offered in Fall 2017.              Last Modified: 11/20/2017       Submitted by: Qinru Qiu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
