<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Small: Collaborative Research: Immersive Visualization and 3D Interaction for Volume Data Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>249955.00</AwardTotalIntnAmount>
<AwardAmount>249955</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This collaborative research project (IIS-1320046, IIS-1319606) designs a 3-dimensional immersive visualization environment for volume data that is critical in a variety of application domains, such as medicine, engineering, geophysical exploration, and biomechanics. For example, biomechanics researchers examine volumes derived from insect scans to understand how form relates to function, particularly in regard to how insects create internal fluid flows. For effective analysis of a 3D volume, scientists and other users need to integrate various views, peer inside the volume, and separate various structures in the data. However, despite many advances in volume rendering algorithms, neither traditional displays nor traditional interaction techniques are sufficient for efficient and accurate analysis of complex volume datasets. This project develops an approach for interactively exploring and segmenting volume datasets by combining and extending: (1) utilization of advanced, high-fidelity displays based on virtual reality (VR) technologies for improving the visual analysis of volume data, and (2) the use of natural, gesture-based 3D techniques. Using controlled, empirical studies with real-world volume datasets from biomechanics and other biological sciences, the investigators are determining what characteristics of advanced displays can lead to faster, more accurate visual analysis. Iterative design and evaluation methods are being used to develop usable and natural 3D interaction techniques with which users can explore the interior of volume datasets. Beyond the empirical findings of these studies, an important outcome of the project is the design of a next-generation volume data analysis system that can be used by scientists and researchers to improve the efficiency and accuracy of their work.&lt;br/&gt;&lt;br/&gt;The expected results will provide a deep understanding of visualization principles fostering further advancements in the realm of volume data analysis. Easier, more accurate, and faster analysis can lead to improvements in healthcare, breakthroughs in science, and advances in education. For example, this work may lead to insights into fundamental physiological mechanisms of feeding, breathing, and circulation in insects - one of the most important animal groups on earth. There are millions of insect species living in almost every habitat, and their lifestyles have profound impacts on human societies. Their effects in areas such as agriculture and health can be both positive (e.g., pollination) and negative (e.g., crop damage, disease), and understanding their fundamental physiologies is critical to controlling their impact. The project provides opportunities for interdisciplinary educational and research activities for graduate and undergraduate students, and outreach activities to underrepresented students. The results of this work will be disseminated broadly via publication in archival journals, peer-reviewed conferences, and online forums. The project website (https://research.cs.vt.edu/3di/node/188) will provide access to research results, including data sets and software.</AbstractNarration>
<MinAmdLetterDate>08/07/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319606</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Laidlaw</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David H Laidlaw</PI_FULL_NAME>
<EmailAddress>dhl@cs.brown.edu</EmailAddress>
<PI_PHON>4013542819</PI_PHON>
<NSF_ID>000229227</NSF_ID>
<StartDate>08/07/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brown University</Name>
<CityName>Providence</CityName>
<ZipCode>029129002</ZipCode>
<PhoneNumber>4018632777</PhoneNumber>
<StreetAddress>BOX 1929</StreetAddress>
<StreetAddress2><![CDATA[350 Eddy Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<StateCode>RI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>RI01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001785542</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BROWN UNIVERSITY IN PROVIDENCE IN THE STATE OF RHODE ISLAND AND PROVIDENCE PLANTATIONS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001785542</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>Providence</CityName>
<StateCode>RI</StateCode>
<ZipCode>029129002</ZipCode>
<StreetAddress><![CDATA[155 Waterman Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>RI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~36360</FUND_OBLG>
<FUND_OBLG>2014~115373</FUND_OBLG>
<FUND_OBLG>2015~98222</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-a5e23d92-7a44-20b6-6227-fd40a58f56fc"> </span></p> <p><span id="docs-internal-guid-a5e23d92-7d8c-05cf-aaaf-792d03ad268c">&nbsp;</span></p> <p dir="ltr"><span>Our work has advanced the state-of-the-art in using VR for visualization.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>First, we have shown that the visual fidelity of VR display systems has significant influence on user effectiveness in spatially complex data exploration tasks. We found that the use of stereo displays in particular improves user performance in visual search and spatial judgment tasks. The effects have been verified through experimental evaluations across multiple VR platforms and exploration tasks to allow for a generalization of the results to other display systems. We used these results to compile guidelines for the effective use of VR for data analysis tasks. Based on these experiments and a broad survey of international researchers working with volume data we have created a taxonomy of volume data analysis tasks. Our taxonomy lists generalized tasks such as search, pattern recognition, spatial understanding, quantitative estimation, and shape description. Defining these task groups makes it possible to generalize the findings across different experiments and enables the transition of interaction techniques and strategies from one domain to another.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Second, we have contributed to the field of 3D interaction design by developing and evaluating two techniques for 3D volume manipulation and exploration. The Volume Cracker technique allows the user to open up the volume data like a book and leaf through its &ldquo;pages&rdquo; looking for features and patterns. The hand-held Capsule device provides intuitive interaction methods for 3D orientation, selection and interaction tasks streamlined for the exploration of biological data (e.g. CT scans, protein models).</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Third, we have have extended the range of VR applications through active ongoing collaboration projects with domain experts from various fields at Brown University and Virginia Tech. Our projects with biologists at both institutions have lead to novel VR applications for the analysis of micro-CT scans of spatially complex beetle trachea data and for fluid visualizations to investigate the creation of dinosaur footprints. Together with medical experts we have created an experimental application to evaluate the uses of VR in the diagnosis of a life-threatening vascular disease during twin pregnancies. Understanding the individual requirements of expert users in each of these applications has allowed us to draw parallels to our previously created task taxonomy and allowed us to improve the guidelines for effective VR usage.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Finally, over the course of the award we have introduced a large number of undergraduate and graduate researchers to the field of VR research through courses and independent study projects offered at both institutions. The award helped to create multiple large-scale VR environments, including the YURT display room at Brown University. These systems allowed us to form an inclusive VR research community leading to persistent collaboration projects. Our efforts led to ten publications at conference and major journals in the field.</span></p> <div><span><br /></span></div> <p>&nbsp;</p> <p dir="ltr">&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/02/2017<br>      Modified by: David&nbsp;H&nbsp;Laidlaw</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640013947_system1--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640013947_system1--rgov-800width.jpg" title="Prenatal Medical Imaging Visualization in Virtual Reality Yurt"><img src="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640013947_system1--rgov-66x44.jpg" alt="Prenatal Medical Imaging Visualization in Virtual Reality Yurt"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A user interacting with medical MRI scan data in Brown?s YURT display. Our high-fidelity YURT VR system was the one of the core systems for interaction experiments and the development of novel applications like the dinosaur track viewer.</div> <div class="imageCredit">Brown University, David Laidlaw, Wesley Miller, Johannes Novotny, Joshua Tveite</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">David&nbsp;H&nbsp;Laidlaw</div> <div class="imageTitle">Prenatal Medical Imaging Visualization in Virtual Reality Yurt</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640209930_bhvc--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640209930_bhvc--rgov-800width.jpg" title="Bare-hand Volume Cracker Interaction"><img src="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640209930_bhvc--rgov-66x44.jpg" alt="Bare-hand Volume Cracker Interaction"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Bare hand interaction with a volumetric data visualization using our volume cracker interface. Our two-handed gestures for volumetric data exploration significant advantages over standard interaction techniques in search and pattern recognition tasks.</div> <div class="imageCredit">Virginia Tech, Doug Bowman, Wallace Lages</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">David&nbsp;H&nbsp;Laidlaw</div> <div class="imageTitle">Bare-hand Volume Cracker Interaction</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640302598_capsule-table--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640302598_capsule-table--rgov-800width.jpg" title="Prototype &quot;Capsule&quot; Volume Interaction Device"><img src="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640302598_capsule-table--rgov-66x44.jpg" alt="Prototype &quot;Capsule&quot; Volume Interaction Device"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The prototype of our ?Capsule? user input device for interacting with volumetric biomedical data. Position and rotation of a visualization follows the physical orientation of the capsule, while the sliding covers allow for zooming, slicing and selection interactions.</div> <div class="imageCredit">Virginia Tech, Doug Bowman, Wallace Lages</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">David&nbsp;H&nbsp;Laidlaw</div> <div class="imageTitle">Prototype "Capsule" Volume Interaction Device</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640430202_VRuserstudy-small--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640430202_VRuserstudy-small--rgov-800width.jpg" title="Volume Rendering in Virtual Reality User Study"><img src="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640430202_VRuserstudy-small--rgov-66x44.jpg" alt="Volume Rendering in Virtual Reality User Study"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A user viewing an iso-surface visualization of spatially complex beetle trachea, as part of our display fidelity experiment at the Brown University CAVE display. The experiment covered a wide range of exploration tasks like tracing and feature comparison.</div> <div class="imageCredit">Brown University, David Laidlaw, Wesley Miller, Johannes Novotny, Joshua Tveite</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">David&nbsp;H&nbsp;Laidlaw</div> <div class="imageTitle">Volume Rendering in Virtual Reality User Study</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640630746_motion2--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640630746_motion2--rgov-800width.jpg" title="Counting Task Stimulus for Interation Modality Experiment."><img src="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509640630746_motion2--rgov-66x44.jpg" alt="Counting Task Stimulus for Interation Modality Experiment."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Example visualizations of the datasets used in a counting task in our interaction modality experiment. Study participants had to find and compare features on the surface of the 3D shape by either walking around the object or turning it with a hand controller.</div> <div class="imageCredit">Virginia Tech, Doug Bowman, Wallace Lages</div> <div class="imageSubmitted">David&nbsp;H&nbsp;Laidlaw</div> <div class="imageTitle">Counting Task Stimulus for Interation Modality Experiment.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509644688988_58870941-wide--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509644688988_58870941-wide--rgov-800width.jpg" title="Dinosaur Footprint Formation Visualization in Virtual Reality"><img src="/por/images/Reports/POR/2017/1319606/1319606_10264987_1509644688988_58870941-wide--rgov-66x44.jpg" alt="Dinosaur Footprint Formation Visualization in Virtual Reality"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An example visualization of our VR application for the analysis of fluid dynamics involved in generation dinosaur tracks. Apart from particle simulation visualizations the viewer also supports streamline visualizations as well as filtering and clustering techniques to support domain scientists.</div> <div class="imageCredit">Brown University, David Laidlaw, Stephen Gatesy, Joshua Tveite</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">David&nbsp;H&nbsp;Laidlaw</div> <div class="imageTitle">Dinosaur Footprint Formation Visualization in Virtual Reality</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      Our work has advanced the state-of-the-art in using VR for visualization.    First, we have shown that the visual fidelity of VR display systems has significant influence on user effectiveness in spatially complex data exploration tasks. We found that the use of stereo displays in particular improves user performance in visual search and spatial judgment tasks. The effects have been verified through experimental evaluations across multiple VR platforms and exploration tasks to allow for a generalization of the results to other display systems. We used these results to compile guidelines for the effective use of VR for data analysis tasks. Based on these experiments and a broad survey of international researchers working with volume data we have created a taxonomy of volume data analysis tasks. Our taxonomy lists generalized tasks such as search, pattern recognition, spatial understanding, quantitative estimation, and shape description. Defining these task groups makes it possible to generalize the findings across different experiments and enables the transition of interaction techniques and strategies from one domain to another.    Second, we have contributed to the field of 3D interaction design by developing and evaluating two techniques for 3D volume manipulation and exploration. The Volume Cracker technique allows the user to open up the volume data like a book and leaf through its "pages" looking for features and patterns. The hand-held Capsule device provides intuitive interaction methods for 3D orientation, selection and interaction tasks streamlined for the exploration of biological data (e.g. CT scans, protein models).    Third, we have have extended the range of VR applications through active ongoing collaboration projects with domain experts from various fields at Brown University and Virginia Tech. Our projects with biologists at both institutions have lead to novel VR applications for the analysis of micro-CT scans of spatially complex beetle trachea data and for fluid visualizations to investigate the creation of dinosaur footprints. Together with medical experts we have created an experimental application to evaluate the uses of VR in the diagnosis of a life-threatening vascular disease during twin pregnancies. Understanding the individual requirements of expert users in each of these applications has allowed us to draw parallels to our previously created task taxonomy and allowed us to improve the guidelines for effective VR usage.    Finally, over the course of the award we have introduced a large number of undergraduate and graduate researchers to the field of VR research through courses and independent study projects offered at both institutions. The award helped to create multiple large-scale VR environments, including the YURT display room at Brown University. These systems allowed us to form an inclusive VR research community leading to persistent collaboration projects. Our efforts led to ten publications at conference and major journals in the field.                 Last Modified: 11/02/2017       Submitted by: David H Laidlaw]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
