<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Fundamental Issues in International Data Placement for Data-Intensive Applications, a Laboratory Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>299999.00</AwardTotalIntnAmount>
<AwardAmount>299999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Researchers in an expanding number of scientific disciplines, ranging from astronomy to medicine and from economics, to botany, would like to move large volumes of data across institutional and national boundaries. Placing data at different locations expands data processing capacity, facilitates collaborative analysis and accelerates translation of gained insight into scientific discoveries. Yet, we have little understanding of how a complex ensemble of interrelated factors affect the cost, performance, and resource consumption of these data movements. Factors include the number and size distributions of files in a data set, performance characteristics of the source and destination file/storage systems, time-of-day/week fluctuations of the network carrying capacity between sites, availability of IPv6 paths.  This  project will explore models that capture both the characteristics and interplay of facets that are involved in placing data at locations separated by local, national, or international networks. Exploratory work will take place to establish organizational and functional foundations for an international data placement laboratory (iDPL) to support at-scale end-to-end data placement experiments.&lt;br/&gt;&lt;br/&gt;The iDPL will be constructed with international partners and is intended to be an extensible facility.  The UCSD and UW-Madison team bring together extensive experience in software tools (HTCondor, Open Science Grid, Rocks Clustering), high-performance campus-area networks (Quartzite and Prism@UCSD), storage systems (Data Oasis), and engaged science communities.  The iDPL core software architecture is to treat data placement experiments as workflows and then use HTCondor?s DAGMan and Metronome continuous testing framework to author, instantiate and manage experiments.   Synthetic data sets will be offered to facilitate diagnostics.  Whenever possible, international networking providers will be engaged to correlate infrastructure view of the network with user experiences.&lt;br/&gt;&lt;br/&gt;The broad and long term impact is to lay the groundwork for a persistent, data and information sharing vehicle that can be easily expanded beyond the initial international group of institutions</AbstractNarration>
<MinAmdLetterDate>08/15/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/15/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1339508</AwardID>
<Investigator>
<FirstName>Philip</FirstName>
<LastName>Papadopoulos</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Philip M Papadopoulos</PI_FULL_NAME>
<EmailAddress>ppapadopoulos@ucsd.edu</EmailAddress>
<PI_PHON>8588223628</PI_PHON>
<NSF_ID>000462781</NSF_ID>
<StartDate>08/15/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7369</Code>
<Text>International Res Ret Connect</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~299999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Larger datasets are becoming commonplace for computationally intensive science. When coupled with the fact that computional engines such as department clusters, academic resource centers and commerical clouds are frequently utilized by computational scientists, one is faced with reality that data and compute are not always colocated.&nbsp; For the application to work properly, the required data must be "placed" or transferred to where the analysis will be undertaken. This results in a common sequence of events: 1) move data to where analysis will occur, 2) perform computational analyis and 3) transfer results back to a user.&nbsp; When data is small, the time taken to perform the data movement steps are relatively unimportant. When data is large,&nbsp; these transfer steps can dominate the total time for analysis.&nbsp; Ideally, one would like to be able to predict how much time each of the three steps will consume.</p> <p>The international data placement laboratory (iDPL) represents a prototype for regular measurement and understanding of <em>data placement </em>using a variety of data transfer technologies and algorithms.&nbsp; The approach for iDPL was to enable a system that could test both raw network performance (like perfSONAR) and easily incorporate different algorithms for disk-to-remote-disk copy performance. Over years of measurement, some data placement is limited by raw network performance, while other placement is limited by disk performance.&nbsp; Routine tests among&nbsp; four anchor institutions of the iDPL: University of California, San Diego, University of Wisconsin Madison, Chinese Academy of sciences, Beijing and Beihang University, Beijing could capture limitations due to distance, firewall traversal problems, and periodic (regular) performance degradation all withiin the context of national and international networks. iDPL has also been extended for routine tests to two commercial cloud providers: Amazon AWS and Google Compute Engine.</p> <p>iDPL is open source and uses the widely-deployed HTCondor system for orchestration of repeated tests.&nbsp; Internally, a data placement is an abstraction that moves data from a source to a destination.&nbsp; The data may be "in memory", which allows iDPL to easily handle network-only tests.&nbsp; Data can also be disk-resident, but the protocol utilized to effect the transfer can change. iDPL supports so-called "data movers" that include: raw socket (netcat), secure copy (SCP), explicitly parallel (FDT from the Monalisa project and GridFTP from Globus), and udp-only (UDT).&nbsp; &nbsp;Both IPv4 and IPv6 are supported.&nbsp; iDPL provides the mechanics to set up server processes (like a user-space secure shell daemon) for the duration of a single test event and then tear it down. This structure dramatically reduces the installed software requirements for nodes to participate in the laboratory.&nbsp; Using HTCondor, the results of a placement test can be scheduled among any two nodes within the laboratory, and the results of that test are returned back to the user.&nbsp;</p> <p>Data from iDPL experiments are extracted from log files and added to a time-series database from the Graphite Project. Leveraging HTCondor for orchestration and Graphite for data presentation enabled the project to focus on core issues of multi-protocol data placement testing.&nbsp;</p> <p>See:&nbsp;https://github.com/idpl&nbsp; for all code and configuration.&nbsp;</p><br> <p>            Last Modified: 12/13/2017<br>      Modified by: Philip&nbsp;M&nbsp;Papadopoulos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Larger datasets are becoming commonplace for computationally intensive science. When coupled with the fact that computional engines such as department clusters, academic resource centers and commerical clouds are frequently utilized by computational scientists, one is faced with reality that data and compute are not always colocated.  For the application to work properly, the required data must be "placed" or transferred to where the analysis will be undertaken. This results in a common sequence of events: 1) move data to where analysis will occur, 2) perform computational analyis and 3) transfer results back to a user.  When data is small, the time taken to perform the data movement steps are relatively unimportant. When data is large,  these transfer steps can dominate the total time for analysis.  Ideally, one would like to be able to predict how much time each of the three steps will consume.  The international data placement laboratory (iDPL) represents a prototype for regular measurement and understanding of data placement using a variety of data transfer technologies and algorithms.  The approach for iDPL was to enable a system that could test both raw network performance (like perfSONAR) and easily incorporate different algorithms for disk-to-remote-disk copy performance. Over years of measurement, some data placement is limited by raw network performance, while other placement is limited by disk performance.  Routine tests among  four anchor institutions of the iDPL: University of California, San Diego, University of Wisconsin Madison, Chinese Academy of sciences, Beijing and Beihang University, Beijing could capture limitations due to distance, firewall traversal problems, and periodic (regular) performance degradation all withiin the context of national and international networks. iDPL has also been extended for routine tests to two commercial cloud providers: Amazon AWS and Google Compute Engine.  iDPL is open source and uses the widely-deployed HTCondor system for orchestration of repeated tests.  Internally, a data placement is an abstraction that moves data from a source to a destination.  The data may be "in memory", which allows iDPL to easily handle network-only tests.  Data can also be disk-resident, but the protocol utilized to effect the transfer can change. iDPL supports so-called "data movers" that include: raw socket (netcat), secure copy (SCP), explicitly parallel (FDT from the Monalisa project and GridFTP from Globus), and udp-only (UDT).   Both IPv4 and IPv6 are supported.  iDPL provides the mechanics to set up server processes (like a user-space secure shell daemon) for the duration of a single test event and then tear it down. This structure dramatically reduces the installed software requirements for nodes to participate in the laboratory.  Using HTCondor, the results of a placement test can be scheduled among any two nodes within the laboratory, and the results of that test are returned back to the user.   Data from iDPL experiments are extracted from log files and added to a time-series database from the Graphite Project. Leveraging HTCondor for orchestration and Graphite for data presentation enabled the project to focus on core issues of multi-protocol data placement testing.   See: https://github.com/idpl  for all code and configuration.        Last Modified: 12/13/2017       Submitted by: Philip M Papadopoulos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
