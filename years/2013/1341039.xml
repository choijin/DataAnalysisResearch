<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC-NIE Networking Infrastructure: 100 Gb/s Science DMZ</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The University of California Santa Cruz (UCSC) is home to leading research centers in a number of areas, including astrophysics, genomics, earth and planetary science, and particle physics. Being aggressive adopters of computational components in their research programs, these centers all have current and growing requirements for high-performance networking. These research activities involve the transfer of big data across the network, and are currently limited by UCSC's connectivity to regional, national, and international research networks. The motivation for this project is to re-architect UCSC's research network so that it is optimized for these high performance scientific applications.&lt;br/&gt;&lt;br/&gt;To better support these big data research activities UCSC adds a Science DMZ component to the UCSC network that is optimized for high-performance applications. The Science DMZ is a portion of the campus network that segregates these big science data flows from production network traffic. UCSC is provisioning 100 Gb/s external connectivity from the Science DMZ to regional, national, and international research networks, and 10 Gb/s to the computational infrastructure supporting these projects. UCSC is deploying enhanced measurement and monitoring capabilities to ensure optimal use of the new infrastructure, and the experimental OpenFlow technology to enable experiments in enhanced services for these users. To ensure researchers are able to make optimal use of the new services UCSC dedicates network engineering personnel to act as a bridge between the researchers and central campus IT staff to ensure researchers make maximal use of the new infrastructure.</AbstractNarration>
<MinAmdLetterDate>09/05/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/05/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1341039</AwardID>
<Investigator>
<FirstName>Bradley</FirstName>
<LastName>Smith</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bradley R Smith</PI_FULL_NAME>
<EmailAddress>brad@curatednetworks.com</EmailAddress>
<PI_PHON>8313453409</PI_PHON>
<NSF_ID>000531883</NSF_ID>
<StartDate>09/05/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mary</FirstName>
<LastName>Doyle</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mary Doyle</PI_FULL_NAME>
<EmailAddress>mdoyle1@ucsc.edu</EmailAddress>
<PI_PHON>8314595278</PI_PHON>
<NSF_ID>000541930</NSF_ID>
<StartDate>09/05/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Cruz</Name>
<CityName>Santa Cruz</CityName>
<ZipCode>950641077</ZipCode>
<PhoneNumber>8314595278</PhoneNumber>
<StreetAddress>1156 High Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>125084723</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA CRUZ</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Cruz]]></Name>
<CityName>Santa Cruz</CityName>
<StateCode>CA</StateCode>
<ZipCode>950641077</ZipCode>
<StreetAddress><![CDATA[1156 High St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the UCSC 100 Gb/s Science DMZ project was to create a Science DMZ in the campus network to provide enhanced network connectivity for network intensive research efforts, to assist these research activities to use the new infrastructure, and to engage with the broader research network community to exchange lessons learned in these activities.</p> <p>We completed construction of the Science DMZ as described in the proposal (see image 1). Components of the Science DMZ include: a core switch connected to the CENIC High Performance Research (HPR) Network at 100 Gbps, to distribution Science DMZ switches in the data center and science and engineering buildings housing network intensive research activities around campus, and to a perfSonar test node provisioned for visibility into the Science DMZ core. In addition we have deployed data transfer and additional perfSonar nodes around the Science DMZ.</p> <p>We coordinated with a number of research projects whose activities were network limited in some critical way. Examples of this include work we did with Prof James Ackman and the Santa Cruz Institute for Particle Physics (SCIPP). The core research of Prof James Ackman in MCD Biology involves the transfer of high-resolution TIFF images of brain activity patterns in mice to large computational facilities for analysis and to/from colleagues for collaboration in their research to understand cerebral connectivity. Prior to the the Science DMZ Prof Acman&rsquo;s data transfers (120GB/day) took 15 hours/day, leaving little room for anticipated 10x growth in these transfers. After initial work we were able to increase bandwidth by a factor of 15, providing room to grow for the near-to midterm research. We continue to work with Prof Ackman to find ways of improving the workflow, and to keep in touch regarding the possible need for additional network capacity.</p> <p>SCIPP participates in the ATLAS experiment at the Large Hadron Collider (LHC) in Cern, Switzerland. When the instrument is operational, SCIPP&rsquo;s workflow involves transferring their portion of data generated by the ATLAS instrument each day, processing this data for new insights for use in tuning the next day&rsquo;s experiments, and subsequent meetings of the experiment team for defining experiments for the following day. Before the Science DMZ the transfer of their data took 12 hours, leaving little time for processing and analysis, significantly handicapping their ability to provide input to the next day&rsquo;s experiments. After collaboration with the Science DMZ team, including consultation with ESnet&rsquo;s networking team, we were able to reduce their transfer time to 3 hours, giving them the breathing room needed to analyze the data in time to provide input to the next day&rsquo;s experiments. We have continued collaboration with the group to integrate a new AAA (<a title="link" href="https://arxiv.org/pdf/1508.01443.pdf" target="_blank">link</a>) &ldquo;Any Data, Any Time, Anywhere&rdquo;) system developed at the San Diego Supercomputer Center to provide a new, caching architecture for improved access to LHC data.</p> <p>To share the results of our work, as well as to learn from the work of other Science DMZ projects, we are active participants in the technical advisory committees of the Corporation for Educational Network Initiatives in California (CENIC). These committees have monthly conference calls and quarterly in-person meetings to provide input to the development of the CENIC networks, as well as to share lessons learned at the various campuses. From these interactions, broader discussions developed among researchers at the various institutions that culminated in the creation and funding of the Pacific Research Platform project housed at UC San Diego. Our project has been an active participant in the PRP since it&rsquo;s inception, actively participating in experiments run over the PRP infrastructure as well as coordinating UCSC faculty participation in new PRP research activities. As a result our Science DMZ houses equipment used in a number of PRP activities including GPU-based deep learning and new high-performance data transfer node architectures.</p> <p>Looking to the future we have been awarded an NSF DNI CI Engineer award to fund a new Cyberinfrastructure Engineer for UCSC. This position will be instrumental in continuing the development and outreach efforts for our Science DMZ during the coming years. In addition, as a part of the collaboration with the PRP, as well as engagement with the local storage research group who developed the open-source Ceph distributed filesystem, we are in the process of enhancing our Science DMZ (see image 2) to implement the next generation architecture developed by the ESnet engineers (<a title="link" href="http://meetings.internet2.edu/media/medialibrary/2015/10/05/20151005-dart-science-dmz-futures-v3.pdf" target="_blank">link</a>) to improve the usability, performance, and security of this infrastructure for use by researchers.</p> <p>In summary, the NSF CC-NIE grant has allowed UCSC to dramatically upgrade its research network infrastructure to provide an order-of-magnitude improvement in communications capacity to campus research with infrastructure that can easily evolve to meet future needs.</p><br> <p>            Last Modified: 03/28/2017<br>      Modified by: Bradley&nbsp;R&nbsp;Smith</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1341039/1341039_10275173_1490639185926_SciDMZAfter--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1341039/1341039_10275173_1490639185926_SciDMZAfter--rgov-800width.jpg" title="UCSC 100Gbps Science DMZ"><img src="/por/images/Reports/POR/2017/1341039/1341039_10275173_1490639185926_SciDMZAfter--rgov-66x44.jpg" alt="UCSC 100Gbps Science DMZ"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Original DMZ Design</div> <div class="imageCredit">Brad Smith</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Bradley&nbsp;R&nbsp;Smith</div> <div class="imageTitle">UCSC 100Gbps Science DMZ</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1341039/1341039_10275173_1490639244987_ScienceDMZCeph--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1341039/1341039_10275173_1490639244987_ScienceDMZCeph--rgov-800width.jpg" title="Science DMZ with Ceph distributed filesystem"><img src="/por/images/Reports/POR/2017/1341039/1341039_10275173_1490639244987_ScienceDMZCeph--rgov-66x44.jpg" alt="Science DMZ with Ceph distributed filesystem"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Science DMZ with Ceph distributed filesystem</div> <div class="imageCredit">Brad Smith</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Bradley&nbsp;R&nbsp;Smith</div> <div class="imageTitle">Science DMZ with Ceph distributed filesystem</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the UCSC 100 Gb/s Science DMZ project was to create a Science DMZ in the campus network to provide enhanced network connectivity for network intensive research efforts, to assist these research activities to use the new infrastructure, and to engage with the broader research network community to exchange lessons learned in these activities.  We completed construction of the Science DMZ as described in the proposal (see image 1). Components of the Science DMZ include: a core switch connected to the CENIC High Performance Research (HPR) Network at 100 Gbps, to distribution Science DMZ switches in the data center and science and engineering buildings housing network intensive research activities around campus, and to a perfSonar test node provisioned for visibility into the Science DMZ core. In addition we have deployed data transfer and additional perfSonar nodes around the Science DMZ.  We coordinated with a number of research projects whose activities were network limited in some critical way. Examples of this include work we did with Prof James Ackman and the Santa Cruz Institute for Particle Physics (SCIPP). The core research of Prof James Ackman in MCD Biology involves the transfer of high-resolution TIFF images of brain activity patterns in mice to large computational facilities for analysis and to/from colleagues for collaboration in their research to understand cerebral connectivity. Prior to the the Science DMZ Prof Acman?s data transfers (120GB/day) took 15 hours/day, leaving little room for anticipated 10x growth in these transfers. After initial work we were able to increase bandwidth by a factor of 15, providing room to grow for the near-to midterm research. We continue to work with Prof Ackman to find ways of improving the workflow, and to keep in touch regarding the possible need for additional network capacity.  SCIPP participates in the ATLAS experiment at the Large Hadron Collider (LHC) in Cern, Switzerland. When the instrument is operational, SCIPP?s workflow involves transferring their portion of data generated by the ATLAS instrument each day, processing this data for new insights for use in tuning the next day?s experiments, and subsequent meetings of the experiment team for defining experiments for the following day. Before the Science DMZ the transfer of their data took 12 hours, leaving little time for processing and analysis, significantly handicapping their ability to provide input to the next day?s experiments. After collaboration with the Science DMZ team, including consultation with ESnet?s networking team, we were able to reduce their transfer time to 3 hours, giving them the breathing room needed to analyze the data in time to provide input to the next day?s experiments. We have continued collaboration with the group to integrate a new AAA (link) "Any Data, Any Time, Anywhere") system developed at the San Diego Supercomputer Center to provide a new, caching architecture for improved access to LHC data.  To share the results of our work, as well as to learn from the work of other Science DMZ projects, we are active participants in the technical advisory committees of the Corporation for Educational Network Initiatives in California (CENIC). These committees have monthly conference calls and quarterly in-person meetings to provide input to the development of the CENIC networks, as well as to share lessons learned at the various campuses. From these interactions, broader discussions developed among researchers at the various institutions that culminated in the creation and funding of the Pacific Research Platform project housed at UC San Diego. Our project has been an active participant in the PRP since it?s inception, actively participating in experiments run over the PRP infrastructure as well as coordinating UCSC faculty participation in new PRP research activities. As a result our Science DMZ houses equipment used in a number of PRP activities including GPU-based deep learning and new high-performance data transfer node architectures.  Looking to the future we have been awarded an NSF DNI CI Engineer award to fund a new Cyberinfrastructure Engineer for UCSC. This position will be instrumental in continuing the development and outreach efforts for our Science DMZ during the coming years. In addition, as a part of the collaboration with the PRP, as well as engagement with the local storage research group who developed the open-source Ceph distributed filesystem, we are in the process of enhancing our Science DMZ (see image 2) to implement the next generation architecture developed by the ESnet engineers (link) to improve the usability, performance, and security of this infrastructure for use by researchers.  In summary, the NSF CC-NIE grant has allowed UCSC to dramatically upgrade its research network infrastructure to provide an order-of-magnitude improvement in communications capacity to campus research with infrastructure that can easily evolve to meet future needs.       Last Modified: 03/28/2017       Submitted by: Bradley R Smith]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
