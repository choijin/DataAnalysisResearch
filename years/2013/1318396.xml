<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Small: Automated Diagnosis and Root Cause Analysis of Internet Problems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>499351.00</AwardTotalIntnAmount>
<AwardAmount>499351</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Reliable Internet performance and availability are essential for many existing and future network applications. While the Internet works well enough most of the time for most people, nearly everyone has experienced outages and service degradation that make the network unusable, and we are far from five nines of reliability that critical services require. Improving Internet connectivity requires action against all sources of unavailability and poor performance.  &lt;br/&gt;&lt;br/&gt;The research community has made substantial progress toward understanding and developing technologies to address short-term outages due to BGP (border gateway protocol) routing convergence. However, much less progress has been made at reducing the impact of long-term outages and route misconfiguration.  Despite being rare, these events have a large impact on overall network availability because repairs happen on a human timescale.  Additionally, many users suffer from the use of sub-optimal (high latency or lossy) paths to network services due to misconfigurations and ineffective route selection.  Operators at an affected ISP or service often encounter stumbling blocks at each step: identifying that a problem exists, localizing the root cause of the problem, and affecting a repair. &lt;br/&gt;&lt;br/&gt;The researchers on this project will develop a system to transform this largely manual troubleshooting process into a fully automated one.  The goal of the research is that persistent outages and performance problems can be identified in real-time, rather than today's matter of hours.  While automated diagnosis and identification of root cause is fundamentally hard, the project will benefit from dramatic recent progress in Internet measurement technologies, specifically reverse path measurement that provides a much more complete picture of the Internet topology than ever before.&lt;br/&gt;&lt;br/&gt;Intellectual Merit: The goal of the research project is to change the paradigm of network diagnosis on the Internet -- from blind to informed.  The state of art with network troubleshooting is to use ad-hoc techniques.  For instance, it is common occurrence on the NANOG (North American Network Operators? Group) mailing list for operators to post requests asking other operators to manually issue traceroutes and report them in order to identify network anomalies.  The network could thus benefit from a continuously operated service that can not only detect network problems in realtime but also identify misbehaving network elements at the granularity of routers.  There are also a number of challenges to deploying a functional diagnosis system, and the researchers will address them using the following key components.  First, the project will produce a scalable measurement system that will synthesize measurements from different techniques to provide snapshots of routing behavior in real-time.  Second, the research will focus on developing a general theory of Internet path changes that will help model the propagation of routing events and identify the candidate set of responsible ASes (autonomous systems). Third, the researchers will develop inference techniques that will operate on measured data and identify the origin of failures and path changes in the wide area even when the measurement data is incomplete or subject to transient dynamics.&lt;br/&gt;&lt;br/&gt;Broader Impact: Our society is increasingly relying on the Internet for critical telecommunications services, such as home health monitoring, e-911, smart grids, and so forth.  It is no longer simply an inconvenience when the Internet is unavailable or inefficient.  If this project is successful, it will help operators address the major sources of unavailability and misconfigurations in the Internet, benefiting all of its users.  In addition, because of a lack of automated tools, operators currently spend huge amounts of time chasing down individual outages and performance misconfigurations; this raises the barrier to entry for small ISPs, ultimately raising the costs of Internet service for everyone.</AbstractNarration>
<MinAmdLetterDate>08/20/2013</MinAmdLetterDate>
<MaxAmdLetterDate>12/18/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1318396</AwardID>
<Investigator>
<FirstName>Arvind</FirstName>
<LastName>Krishnamurthy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arvind Krishnamurthy</PI_FULL_NAME>
<EmailAddress>arvind@cs.washington.edu</EmailAddress>
<PI_PHON>2066160957</PI_PHON>
<NSF_ID>000488256</NSF_ID>
<StartDate>08/20/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Choffnes</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David R Choffnes</PI_FULL_NAME>
<EmailAddress>choffnes@ccs.neu.edu</EmailAddress>
<PI_PHON>8479779302</PI_PHON>
<NSF_ID>000629991</NSF_ID>
<StartDate>08/20/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress><![CDATA[185 Stevens Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~499351</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our end goal is that network problems can be identified in realtime, rather than today's matter of hours. While automated diagnosis and identification of root cause is fundamentally hard, our project benefits from dramatic recent progress in Internet measurement technologies, specifically reverse path measurement, that provides a much more complete picture of the Internet topology than ever before.</p> <p>&nbsp;</p> <p>We obtained the following results:</p> <p><br />(a) Probabilistic analysis of root cause and AS relationship inference: We developed a new algorithm for inferring business relationships between Autonomous Systems (ASes) from publicly available BGP paths. Unlike previous approaches, our algorithm does not depend on pre-defined characteristics of BGP policies, such as valley-free routing. Instead, we build a probabilistic model by learning patterns in historical BGP data. We identify a key set of route features that are highly predictive. Our algorithm, which makes relationship predictions based on a weighted sum of these route features, consistently achieves 99% prediction accuracy (less than 1% error rate) over 3-4 years of routing data, reducing significantly the error rate of state-of-the-art algorithm. The predictiveness of these features leads us to believe that we have identified underlying structures of AS relationships---particular hybrid AS relationships---that were not previously understood.</p> <p>(b) Automated diagnosis of performance problems due to problems in the optical network: To confirm and quantify physical layer over-engineering in today's datacenters, we conduct what to our knowledge is the first large-scale study of operational optical links. We analyzed over 300K links across more than 20 datacenters of a large cloud provider over a period of 10 months. We find a remarkably conservative state of affairs---99.9% of the links have incoming optical signal quality that is higher than the minimum threshold for BER, while the median is 6 times higher! We then designed a practical system that builds multiple virtual topologies on the same physical topology, where the class of the topology offers a bound on the maximum packet error rate (i.e., grayness) on any path in it. The first-class topology does not have any gray paths; hence, it offers the same path packet error rate guarantee as current DCN designs offer. Other classes increasingly use more gray links. Each application uses the virtual topology that meets its needs. Thus, loss tolerant applications use virtual topologies that may have more gray paths. To support applications, such as large transfers that are otherwise loss-tolerant but suffer when the transport protocol (e.g., TCP) is sensitive to losses, Rail uses a transparent coding-based error correction scheme. We develop an efficient algorithm to compute virtual topologies that leverages the topological structure of DCNs. Rail is easily deployable as it requires no changes to the switch or transceiver hardware. We evaluated Rail using simulations-based analysis and a testbed. Even at the maximum stretched reach level we consider, we find 95% of all paths are as reliable as today. Furthermore, Rail successfully protects loss-sensitive applications from gray paths.</p> <p>(c) Analysis of packet losses of microbursts inside datacenters: Our primary result is to provide a high-resolution characterization of a production data center network. To do so, we developed a custom high-resolution counter collection framework on top of the data center operator's in-house switch platform. This framework is able to poll switch statistics at a 10s to 100s of microseconds granularity with minimal impact on regular switch operations. With the framework, we proceed to perform a data-driven analysis of various counters (including packet counters and buffer utilization statistics) from Top-of-Rack (ToR) switches in multiple clusters running multiple applications. While our measurements are limited to ToR switches, our measurements and prior work indicate that the majority of congestion occurs at that layer. Our main findings include: (1) micro-bursts, periods of high utilization lasting less than 1ms, exist in production data centers, and in fact, they encompass most congestion events. (2) Link utilization is multimodal; when bursts occur, they are generally intense. (3) At small timescales, many multi-statistic features become possible to measure: load can be very unbalanced, packets tend to be larger inside bursts than outside, and buffers are related to simultaneous bursts in a nonlinear fashion.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2017<br>      Modified by: Arvind&nbsp;Krishnamurthy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our end goal is that network problems can be identified in realtime, rather than today's matter of hours. While automated diagnosis and identification of root cause is fundamentally hard, our project benefits from dramatic recent progress in Internet measurement technologies, specifically reverse path measurement, that provides a much more complete picture of the Internet topology than ever before.     We obtained the following results:   (a) Probabilistic analysis of root cause and AS relationship inference: We developed a new algorithm for inferring business relationships between Autonomous Systems (ASes) from publicly available BGP paths. Unlike previous approaches, our algorithm does not depend on pre-defined characteristics of BGP policies, such as valley-free routing. Instead, we build a probabilistic model by learning patterns in historical BGP data. We identify a key set of route features that are highly predictive. Our algorithm, which makes relationship predictions based on a weighted sum of these route features, consistently achieves 99% prediction accuracy (less than 1% error rate) over 3-4 years of routing data, reducing significantly the error rate of state-of-the-art algorithm. The predictiveness of these features leads us to believe that we have identified underlying structures of AS relationships---particular hybrid AS relationships---that were not previously understood.  (b) Automated diagnosis of performance problems due to problems in the optical network: To confirm and quantify physical layer over-engineering in today's datacenters, we conduct what to our knowledge is the first large-scale study of operational optical links. We analyzed over 300K links across more than 20 datacenters of a large cloud provider over a period of 10 months. We find a remarkably conservative state of affairs---99.9% of the links have incoming optical signal quality that is higher than the minimum threshold for BER, while the median is 6 times higher! We then designed a practical system that builds multiple virtual topologies on the same physical topology, where the class of the topology offers a bound on the maximum packet error rate (i.e., grayness) on any path in it. The first-class topology does not have any gray paths; hence, it offers the same path packet error rate guarantee as current DCN designs offer. Other classes increasingly use more gray links. Each application uses the virtual topology that meets its needs. Thus, loss tolerant applications use virtual topologies that may have more gray paths. To support applications, such as large transfers that are otherwise loss-tolerant but suffer when the transport protocol (e.g., TCP) is sensitive to losses, Rail uses a transparent coding-based error correction scheme. We develop an efficient algorithm to compute virtual topologies that leverages the topological structure of DCNs. Rail is easily deployable as it requires no changes to the switch or transceiver hardware. We evaluated Rail using simulations-based analysis and a testbed. Even at the maximum stretched reach level we consider, we find 95% of all paths are as reliable as today. Furthermore, Rail successfully protects loss-sensitive applications from gray paths.  (c) Analysis of packet losses of microbursts inside datacenters: Our primary result is to provide a high-resolution characterization of a production data center network. To do so, we developed a custom high-resolution counter collection framework on top of the data center operator's in-house switch platform. This framework is able to poll switch statistics at a 10s to 100s of microseconds granularity with minimal impact on regular switch operations. With the framework, we proceed to perform a data-driven analysis of various counters (including packet counters and buffer utilization statistics) from Top-of-Rack (ToR) switches in multiple clusters running multiple applications. While our measurements are limited to ToR switches, our measurements and prior work indicate that the majority of congestion occurs at that layer. Our main findings include: (1) micro-bursts, periods of high utilization lasting less than 1ms, exist in production data centers, and in fact, they encompass most congestion events. (2) Link utilization is multimodal; when bursts occur, they are generally intense. (3) At small timescales, many multi-statistic features become possible to measure: load can be very unbalanced, packets tend to be larger inside bursts than outside, and buffers are related to simultaneous bursts in a nonlinear fashion.          Last Modified: 11/30/2017       Submitted by: Arvind Krishnamurthy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
