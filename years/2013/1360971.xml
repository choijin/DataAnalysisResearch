<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Probabilistic Hashing for Efficient Search Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/28/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>475149.00</AwardTotalIntnAmount>
<AwardAmount>475149</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Numerous applications involve massive, high-dimensional datasets. For example, the search industry routinely deals with billions of web pages, where each page is often represented as a binary vector in 2^64 dimensions. In computer vision, images are often represented as non-binary vectors in millions of dimensions. Algorithms which are capable of efficiently compressing, retrieving, and mining these datasets are of high practical importance. Mathematically rigorous and computationally efficient hashing methods will be developed to dramatically reduce ultra-high-dimensional datasets. These algorithms will be integrated with a variety of learning techniques including classification, clustering, near-neighbor search, matrix factorizations, etc. &lt;br/&gt;&lt;br/&gt;The project builds on and extends minwise hashing, and b-bit minwise hashing which are standard hashing techniques in search applications. The project aims to (i) rigorously analyze b-bit minwise hashing and develop, analyze, and apply significantly more efficient (and more accurate) to problems in search and learning; (ii) develop  a unified framework of probabilistic hashing which essentially consists of one permutation followed by (at most) one random projection; (iii) develop a unified theory of summary statistics under a variety of engineering constraints (storage space, computational speed, indexing capability, adaptation to streaming, etc.). &lt;br/&gt;&lt;br/&gt;Hashing algorithms developed under this framework are expected to be substantially much more efficient and more accurate than existing popular algorithms such as random projections and minwise hashing.  This general framework allows the design algorithms to accommodate many different data types (sparse or dense data, binary or real-valued data, static or streaming data), many different engineering needs (computing inner products or lp distances, kernel learning or linear learning), and different storage requirements. Anticipated results of the proposed research include rigorous and computationally efficient hashing algorithms for dealing with ultra-high-dimensional datasets, the integration of the resulting hashing algorithms into with a variety of learning techniques for classification, clustering, near-neighbor search, singular value decompositions, matrix factorization, etc; and rigorous experimental evaluation of the resulting methods on big (e.g., TeraByte or potentially PetaByte) data of the order of up to 2^64 dimensions. &lt;br/&gt;&lt;br/&gt;Broader Impacts: Effective approaches to building predictive models from extremely high dimensional data can impact many areas of science that rely on machine learning as the primary methodology for knowledge acquisition from data. The PI's education and outreach efforts aim to broaden the participation of women and underrepresented groups. The publications, software, and datasets resulting from the project will be freely disseminated to the larger scientific community.</AbstractNarration>
<MinAmdLetterDate>09/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/14/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1360971</AwardID>
<Investigator>
<FirstName>Ping</FirstName>
<LastName>Li</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ping Li</PI_FULL_NAME>
<EmailAddress>pingli@stat.rutgers.edu</EmailAddress>
<PI_PHON>8484457667</PI_PHON>
<NSF_ID>000083097</NSF_ID>
<StartDate>09/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<StreetAddress2><![CDATA[2nd Floor East Wing]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001912864</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University]]></Name>
<CityName>New Brunswick</CityName>
<StateCode>NJ</StateCode>
<ZipCode>089018559</ZipCode>
<StreetAddress><![CDATA[3 RUTGERS PLAZA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~316562</FUND_OBLG>
<FUND_OBLG>2014~158587</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Numerous applications involve massive, high-dimensional datasets. Algorithms which are capable of efficiently compressing, retrieving, and mining these datasets are of high practical importance. For this NSF project, a wide variety of mathematically rigorous and computationally efficient hashing methods have been developed, in several major categories:</span></p> <p><span>(</span>1) Densified one permutation hashing which dramatically improved the processing bottleneck in the widely used b-bit minwise hashing algorithm, with papers published in ICML'14 and UAI'14.&nbsp;</p> <p>(2) MIPS (maximum inner product search). This line of work has a significant impact in both research and practice, because searching for maximum inner products is a common operation in machine learning optimization and infomational retrieval. The initital paper in NIPS'14 won the best paper award and follow-up papers in UAI'15 and WWW'15 improved the initial version.</p> <p>(3) Generalized consistent weighted sampling (GCWS). This line of work provides efficient hashing methods for non-binary data and is a natural generalization of minwise hashing (for 0/1 data only). A series of papers in KDD'15, KDD'17, WWW'17 etc, provide practical procedures and theories for GCWS with applications in machine learning and search. Surprisingly, GCWS as a simple method demostrates competitive performance in many machine learning tasks compared to complex models such as deep learning and tree methods.&nbsp;</p> <p>The project has (partially) supported multiple Ph.D. students and postdoctoral researchers. Some of them have become tenure-track faculty.&nbsp;</p><br> <p>            Last Modified: 12/17/2018<br>      Modified by: Ping&nbsp;Li</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Numerous applications involve massive, high-dimensional datasets. Algorithms which are capable of efficiently compressing, retrieving, and mining these datasets are of high practical importance. For this NSF project, a wide variety of mathematically rigorous and computationally efficient hashing methods have been developed, in several major categories:  (1) Densified one permutation hashing which dramatically improved the processing bottleneck in the widely used b-bit minwise hashing algorithm, with papers published in ICML'14 and UAI'14.   (2) MIPS (maximum inner product search). This line of work has a significant impact in both research and practice, because searching for maximum inner products is a common operation in machine learning optimization and infomational retrieval. The initital paper in NIPS'14 won the best paper award and follow-up papers in UAI'15 and WWW'15 improved the initial version.  (3) Generalized consistent weighted sampling (GCWS). This line of work provides efficient hashing methods for non-binary data and is a natural generalization of minwise hashing (for 0/1 data only). A series of papers in KDD'15, KDD'17, WWW'17 etc, provide practical procedures and theories for GCWS with applications in machine learning and search. Surprisingly, GCWS as a simple method demostrates competitive performance in many machine learning tasks compared to complex models such as deep learning and tree methods.   The project has (partially) supported multiple Ph.D. students and postdoctoral researchers. Some of them have become tenure-track faculty.        Last Modified: 12/17/2018       Submitted by: Ping Li]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
