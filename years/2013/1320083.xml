<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Functional Scene Representation for Image Understanding</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>What does it mean to "understand" an image? One popular answer is simply naming the objects seen in the image. During the last decade most computer vision researchers have focused on this "object naming" problem. While there has been great progress in detecting things like "cars" and "people", such a level of understanding still cannot answer even basic questions about an image such as "What is the geometric structure of the scene?" "Where in the image can I walk"?&lt;br/&gt; &lt;br/&gt;The goal of this project is to develop a geometric and functional representation of our visual world for scene understanding. This project aims to develop this functional representation by learning relationships between the physical/visual representation of the scene and the space of the interactions an agent can perform in that scene. The key advantage of functional scene representation is that it is subjective, explicitly task-based and takes into account the agent?s capabilities.&lt;br/&gt; &lt;br/&gt;This project is anticipated to result in major advances within the image understanding community, bringing it closer to researchers in robotics. It is anticipated to result in improvements in: (a) 3D Scene Understanding; (b) Recognition; (c) Human Activity Understanding, and hence could be a critical enabling technology for applications such as autonomous systems, surveillance, and personal robotics. This project is also expected to contribute to education through course development, student projects, workshops, and tutorials involving a broader audience as well as using popular online media (e.g., YouTube) and interactive web demos to involve young children.</AbstractNarration>
<MinAmdLetterDate>08/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320083</AwardID>
<Investigator>
<FirstName>Abhinav</FirstName>
<LastName>Gupta</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abhinav Gupta</PI_FULL_NAME>
<EmailAddress>abhinavg@cs.cmu.edu</EmailAddress>
<PI_PHON>4122681161</PI_PHON>
<NSF_ID>000602038</NSF_ID>
<StartDate>08/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we focused on developing a physical and functional representation of the scene. Below we summarize some key outcomes and achievements from the project:</p> <p>1. We developed a data-driven representation for 3D Scene Understanding. We showed how one can estimate surface normals from a single image.</p> <p>2. We developed reasoning approaches that help to in improving 3D Scene Understanding performance.</p> <p>3. We developed deep learning approaches for surface normal estimation, 3d voxel representation and predicting human affordances.</p> <p>4. We investigated how we can use robots for physical interactions and learning a functional representation of the scene. Specifically, we developed a self-supervised robot learning framework. We showed how this framework can be used to learn policy for grasping, drone navigation etc.</p> <p>5. We investigated multi-task learning such that self-supervised learning can scale across multiple tasks.</p> <p>6. We also investigated curriculum learning to reduce the data requirements in high dimensional control space.</p> <p>7. Finally, we developed an adversarial learning framework. This framework has been crucial to learn robust functional representation.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>This project has led to several publications (see below) and one best student paper award at ICRA 2016.</p> <p><span>[1] David Fouhey, Abhinav Gupta and Martial Hebert. Data-Driven 3D Primitives for Single Image Understanding. In International Conference on Computer Vision (ICCV) 2013.</span></p> <p><span>[2] David Fouhey, Abhinav Gupta and Martial Hebert. Unfolding an Indoor Origami World. Accepted in European Conference on Computer Vision (ECCV) 2014.</span></p> <p><span>[3]&nbsp;</span><span>D. Fouhey</span><span>, V. Delaitre, A. Gupta, A. Efros, I. Laptev, and J. Sivic. People Watching: Human Actions as a Cue for Single View Geometry. In International Journal of Computer Vision (IJCV) 2014.</span></p> <p><span>[4] A. Shrivastava and A. Gupta. Building Part-based Object Detectors via 3D Geometry. In International Conference on Computer Vision (ICCV) 2013.</span></p> <p>[5]&nbsp;<strong>Xiaolong Wang</strong>, David F. Fouhey, and Abhinav Gupta.&nbsp;<em>Designing Deep Networks for Surface Normal Estimation.</em>&nbsp;Proc. of IEEE Conference on Computer Vision and Pattern Recognition&nbsp;<strong>(CVPR)</strong>, 2015.</p> <p>[6] David Fouhey, Xiaolong Wang and Abhinav Gupta. In defense of the Direct Perception of Affordances.&nbsp;<a rel="nofollow" href="http://arxiv.org/abs/1505.01085"><strong>arXiv:1505.01085</strong></a></p> <p>[7] David Fouhey, Abhinav Gupta and Martial Hebert. Singe Image 3D without a single 3D Image. In ICCV 2015.</p> <p>[8]&nbsp; Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, Abhinav Gupta, Learning&nbsp;a Predictable and Generative Vector Representation for Objects, ECCV 2016.</p> <p>[9] Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, Abhinav Gupta, The Curious Robot: Learning Visual Representations via Physical Interactions, ECCV 2016</p> <p>[10] Lerrel Pinto and Abhinav Gupta, Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours, ICRA 2016 (Best Student Paper Award).</p> <p><span>[11]&nbsp;&nbsp;</span>Dhiraj Gandhi, Lerrel Pinto, Abhinav Gupta. Learning to fly by crashing. IROS 2017.</p> <p class="p1">[12] Lerrel Pinto, James Davidson and Abhinav Gupta. Supervision via Competition: RobotAdversaries for Learning Tasks. ICRA 2017</p> <p class="p1">[13] Lerrel Pinto and Abhinav Gupta. Learning to Push by Grasping: Using multiple tasksfor effective learning. ICRA 2017</p> <p class="p1">[14] Adithyavairavan Murali,&nbsp;Lerrel Pinto, Dhiraj Gandhi,&nbsp;Abhinav Gupta.&nbsp;CASSL: Curriculum Accelerated Self-Supervised Learning. Under Submission.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/20/2017<br>      Modified by: Abhinav&nbsp;Gupta</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we focused on developing a physical and functional representation of the scene. Below we summarize some key outcomes and achievements from the project:  1. We developed a data-driven representation for 3D Scene Understanding. We showed how one can estimate surface normals from a single image.  2. We developed reasoning approaches that help to in improving 3D Scene Understanding performance.  3. We developed deep learning approaches for surface normal estimation, 3d voxel representation and predicting human affordances.  4. We investigated how we can use robots for physical interactions and learning a functional representation of the scene. Specifically, we developed a self-supervised robot learning framework. We showed how this framework can be used to learn policy for grasping, drone navigation etc.  5. We investigated multi-task learning such that self-supervised learning can scale across multiple tasks.  6. We also investigated curriculum learning to reduce the data requirements in high dimensional control space.  7. Finally, we developed an adversarial learning framework. This framework has been crucial to learn robust functional representation.        This project has led to several publications (see below) and one best student paper award at ICRA 2016.  [1] David Fouhey, Abhinav Gupta and Martial Hebert. Data-Driven 3D Primitives for Single Image Understanding. In International Conference on Computer Vision (ICCV) 2013.  [2] David Fouhey, Abhinav Gupta and Martial Hebert. Unfolding an Indoor Origami World. Accepted in European Conference on Computer Vision (ECCV) 2014.  [3] D. Fouhey, V. Delaitre, A. Gupta, A. Efros, I. Laptev, and J. Sivic. People Watching: Human Actions as a Cue for Single View Geometry. In International Journal of Computer Vision (IJCV) 2014.  [4] A. Shrivastava and A. Gupta. Building Part-based Object Detectors via 3D Geometry. In International Conference on Computer Vision (ICCV) 2013.  [5] Xiaolong Wang, David F. Fouhey, and Abhinav Gupta. Designing Deep Networks for Surface Normal Estimation. Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.  [6] David Fouhey, Xiaolong Wang and Abhinav Gupta. In defense of the Direct Perception of Affordances. arXiv:1505.01085  [7] David Fouhey, Abhinav Gupta and Martial Hebert. Singe Image 3D without a single 3D Image. In ICCV 2015.  [8]  Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, Abhinav Gupta, Learning a Predictable and Generative Vector Representation for Objects, ECCV 2016.  [9] Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, Abhinav Gupta, The Curious Robot: Learning Visual Representations via Physical Interactions, ECCV 2016  [10] Lerrel Pinto and Abhinav Gupta, Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours, ICRA 2016 (Best Student Paper Award).  [11]  Dhiraj Gandhi, Lerrel Pinto, Abhinav Gupta. Learning to fly by crashing. IROS 2017. [12] Lerrel Pinto, James Davidson and Abhinav Gupta. Supervision via Competition: RobotAdversaries for Learning Tasks. ICRA 2017 [13] Lerrel Pinto and Abhinav Gupta. Learning to Push by Grasping: Using multiple tasksfor effective learning. ICRA 2017 [14] Adithyavairavan Murali, Lerrel Pinto, Dhiraj Gandhi, Abhinav Gupta. CASSL: Curriculum Accelerated Self-Supervised Learning. Under Submission.             Last Modified: 10/20/2017       Submitted by: Abhinav Gupta]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
