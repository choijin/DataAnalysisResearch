<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Exploring Adapting Language Technology Across a Network of Domains</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Much of the most successful software for processing and understanding natural language is based on learning from labeled examples. However, applications to diverse genres such as social media and historical documents have demonstrated the limitations of this approach since the application data differs dramatically from the training examples. Labeling training datasets for each new genre is prohibitively expensive.  Methods that adapt the software between the original source domain and the target --- for example, from 20th century newspapers to Shakespearean drama --- are an attractive alternative and an active research area. However, language does not naturally fall into a few source and target domains; rather, documents exist in a multidimensional field of similarity and difference, based on metadata attributes such as the date of publication. In addition, binary source/target adaptation ignores vast amounts of unlabeled data that may bridge the gap between, say, the 20th and 17th centuries, or between text from the Wall Street Journal and text entered on Twitter.&lt;br/&gt;&lt;br/&gt;This EAGER award explores a new approach to adapting language technology to new application domains. Using explicit document metadata such as date of authorship (for historical documents) or product type (for online reviews), documents are situated in a network of fine-grained domains. Micro-adaptation is then performed between adjacent nodes in the network, which are expected to be more similar to each other than (distant) the source and target domains. These micro-adaptations can then be propagated across the domain graph, yielding an adaptation path from source to target. Empirical evaluations will compare this approach to the current state-of-the-art practices: adapting directly from source to target, and adapting from the source to a broader set of non-source documents.  In addition, a theoretical analysis will identify conditions under which this approach is likely to succeed.&lt;br/&gt;&lt;br/&gt;Language technology already impacts society by facilitating the retrieval, organization, and summarization of information, but its inability to transcend a small set of training domains is one of the most critical obstacles to more widespread adoption. Key application domains such as social media, patient medical records, and legal documents differ substantially from available training corpora, and the development of effective technology for these areas depends on bridging the domain gap. In addition, the sociocultural variation found in online language dramatically reduces the performance of state-of-the-art systems, creating a "language gap" between standard and minority dialects. This research is not tied to any specific language processing task; rather, it promises to build a more robust foundation that can apply across many tasks, bringing the benefits of language technology to new users and settings.</AbstractNarration>
<MinAmdLetterDate>08/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1349837</AwardID>
<Investigator>
<FirstName>Jacob</FirstName>
<LastName>Eisenstein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jacob Eisenstein</PI_FULL_NAME>
<EmailAddress>jacobe@gatech.edu</EmailAddress>
<PI_PHON>6179132859</PI_PHON>
<NSF_ID>000572169</NSF_ID>
<StartDate>08/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Ave., NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>EAGER: Exploring Adapting Language Technology Across a Network of Domains</p> <p><br />Jacob Eisenstein, Georgia Institute of Technology</p> <p>NSF Directorate/Division: CISE / Division of Information and Intelligent Systems</p> <p>NSF Award Number: 1349837</p> <p>Statistical natural language processing has made great strides in training language technology software from labeled examples, with improvements in machine learning and feature engineering offering increasing accuracies on increasingly complex linguistic tasks. However, a fundamental limitation to this approach is that labeled examples may differ in important ways from the target data: for example, a user may be interested in extracting mentions of peopleand places from Twitter text or historical documents, but labeledtraining data for this task may exist only for the domain of 20th century newspapers. The resulting <em>domain shift</em> will cause performance in these target domains to be much worse than in the original source domain.</p> <p><br /><em>Unsupervised domain adaptation</em> has been proposed as a solution tothis problem. The core idea is to use unlabeled data in the target domain to somehow <em>adapt</em> a classifier to perform well in the target domain. Approaches such as Structural Correspondence Learning (SCL) have made some progress towards reducing domain mismatch, typically by identifying a latent representation in which the source and target domains are similar. However, these approaches have several limitations.&nbsp;</p> <ol> <li>Most importantly, they treat domain adaptation in binary terms:&nbsp; adapting from a single source domain to a single target domain. &nbsp;In&nbsp; reality, all data is best described by a number of different *domain&nbsp; attributes*, such as year, genre, geographical origin, etc. The&nbsp; cross-product of these attributes may yield a huge number of domains,&nbsp; so it is crucial to learn the specific properties of each domain attribute.</li> <li>Furthermore, many domain attributes are scalar in nature: for&nbsp; example, if we have labeled data from the 20th century and our target&nbsp; data is in the 17th century, we should leverage data from the&nbsp; intervening centuries so as to better understand the underlying&nbsp; manifold of linguistic change over time.</li> <li>Approaches such as SCL and marginalized denoising autoencoders (mDA)&nbsp; tend to focus on learning representations that minimize some&nbsp; least-squares error on the target domain data. This is suitable for&nbsp; applications such as computer vision, but is a poor fit for many&nbsp; language technology settings, such as named entity recognition&nbsp; (NER), where we encounter large, sparse, structured feature spaces.&nbsp; The result is that these systems are hard to scale (requiring&nbsp; heuristics for selecting "pivot features"), and optimize a criterion&nbsp; (least squares) that is not appropriate for the underlying data.</li> </ol> <p>Our work in this project resulted in <strong>Feature Embeddings</strong>, a novel representation learning approach for domain adaptation in structured feature spaces. Like prior work in representation learning, ourapproach learns dense features that are more robust to domain shift. However, rather than performing representation learning by reconstructing pivot features, Feature Embeddings use techniques from neural language models to obtain low-dimensional embeddings directly (see the attached figure).</p> <p><br />We have applied Feature Embeddings to the classical language technology of part-of-speech tagging: assigning a syntactic class (e.g., "noun", "verb") to each word in a sentence. The standard training set is the Penn Treebank, which contains syntactic annotations on a million tokens of newstext fromthe late 20th century. We consider two adaptation settings:</p> <ol> <li>Target domains in web text, including genres of email, search&nbsp...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ EAGER: Exploring Adapting Language Technology Across a Network of Domains   Jacob Eisenstein, Georgia Institute of Technology  NSF Directorate/Division: CISE / Division of Information and Intelligent Systems  NSF Award Number: 1349837  Statistical natural language processing has made great strides in training language technology software from labeled examples, with improvements in machine learning and feature engineering offering increasing accuracies on increasingly complex linguistic tasks. However, a fundamental limitation to this approach is that labeled examples may differ in important ways from the target data: for example, a user may be interested in extracting mentions of peopleand places from Twitter text or historical documents, but labeledtraining data for this task may exist only for the domain of 20th century newspapers. The resulting domain shift will cause performance in these target domains to be much worse than in the original source domain.   Unsupervised domain adaptation has been proposed as a solution tothis problem. The core idea is to use unlabeled data in the target domain to somehow adapt a classifier to perform well in the target domain. Approaches such as Structural Correspondence Learning (SCL) have made some progress towards reducing domain mismatch, typically by identifying a latent representation in which the source and target domains are similar. However, these approaches have several limitations.   Most importantly, they treat domain adaptation in binary terms:  adapting from a single source domain to a single target domain.  In  reality, all data is best described by a number of different *domain  attributes*, such as year, genre, geographical origin, etc. The  cross-product of these attributes may yield a huge number of domains,  so it is crucial to learn the specific properties of each domain attribute. Furthermore, many domain attributes are scalar in nature: for  example, if we have labeled data from the 20th century and our target  data is in the 17th century, we should leverage data from the  intervening centuries so as to better understand the underlying  manifold of linguistic change over time. Approaches such as SCL and marginalized denoising autoencoders (mDA)  tend to focus on learning representations that minimize some  least-squares error on the target domain data. This is suitable for  applications such as computer vision, but is a poor fit for many  language technology settings, such as named entity recognition  (NER), where we encounter large, sparse, structured feature spaces.  The result is that these systems are hard to scale (requiring  heuristics for selecting "pivot features"), and optimize a criterion  (least squares) that is not appropriate for the underlying data.   Our work in this project resulted in Feature Embeddings, a novel representation learning approach for domain adaptation in structured feature spaces. Like prior work in representation learning, ourapproach learns dense features that are more robust to domain shift. However, rather than performing representation learning by reconstructing pivot features, Feature Embeddings use techniques from neural language models to obtain low-dimensional embeddings directly (see the attached figure).   We have applied Feature Embeddings to the classical language technology of part-of-speech tagging: assigning a syntactic class (e.g., "noun", "verb") to each word in a sentence. The standard training set is the Penn Treebank, which contains syntactic annotations on a million tokens of newstext fromthe late 20th century. We consider two adaptation settings:  Target domains in web text, including genres of email, search  queries, and forums. Target domains in historical Portuguese text, from the 18th to 16th  centuries, and including literature, letters, and academic  dissertations. Each century and genre is modeled as a separate  domain attribute. We choose historical Portuguese because it is one of the few languages for wh...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
