<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Collaborative Research: Integrating Cognitive and Computational Models of Narrative</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>146950.00</AwardTotalIntnAmount>
<AwardAmount>146950</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The primary objective of this research is to develop new, cognitively informed computational models of the generation of narrative that is told within three-dimensional virtual environments.  Motivated by theoretic models of narrative structure and psychological models of narrative comprehension, techniques will be developed for creating accounts of sequences of events and the techniques needed to convey them to users. These techniques will use these models to search for narratives that are at once coherent and effective at communicating the underlying event structure.  The project will explore how computational models of the mental processes performed by people when experiencing film or machinima can inform an automatic process used to generate the films themselves.  Extensive empirical studies will provide a comprehensive evaluation of the effectiveness of the models.&lt;br/&gt;&lt;br/&gt;The research program has three major thrusts: (1) Integrating generative models of character plans with narrative theoretic structural models to create storylines that reflect both rich character goal structures and recognizable narrative elements. (2) Developing methods for shot sequence selection that build on pragmatic models from linguistic communication to effectively convey characters' plans and goals. (3) Developing and then evaluating a system that integrates these parts to search for narratives that are both coherent and effective.&lt;br/&gt;&lt;br/&gt;The project will contribute to the infrastructure of science and education by training new researchers (graduate research assistants) in an area that is broadly multidisciplinary (computer science, cognitive science and psychology). These new researchers will gain from the project a unique integrated view of the contributing disciplines.  Team members will participate in the dissemination of results through journal articles and presentations at national and international conferences on creativity, artificial intelligence, human-computer interaction and psychology. It is expected that the work will have a significant impact on the theory and understanding of creativity, particularly in the context of narrative, serving as a foundation for a new generation of tools that support the creative process.</AbstractNarration>
<MinAmdLetterDate>08/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/27/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319974</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Magliano</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph P Magliano</PI_FULL_NAME>
<EmailAddress>jmagliano@gsu.edu</EmailAddress>
<PI_PHON>6307509366</PI_PHON>
<NSF_ID>000276220</NSF_ID>
<StartDate>08/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northern Illinois University</Name>
<CityName>De Kalb</CityName>
<ZipCode>601152828</ZipCode>
<PhoneNumber>8157531581</PhoneNumber>
<StreetAddress>301 Lowden Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL16</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001745512</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHERN ILLINOIS UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001745512</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northern Illinois University]]></Name>
<CityName>DeKalb</CityName>
<StateCode>IL</StateCode>
<ZipCode>601152860</ZipCode>
<StreetAddress><![CDATA[Department of Psychology]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~44677</FUND_OBLG>
<FUND_OBLG>2014~102273</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There four sets of outcomes that were accomplished with this project.&nbsp; The first was to develop a database of the actions and cinematic features associated with a type of scene extant in cinema that could be used by a computational film making system to create scenes of this nature.&nbsp; The second outcome pertained to advances in theory and research associated with the processing of visual films. This outcome has implications for the first because it was intended to identify useful methodologies for exploring the validity of computational film making systems.&nbsp; &nbsp;A third outcome was to develop a framework for collaborations between cognitive scientist and filmmakers.&nbsp; Finally, the fourth outcome pertained to student training.&nbsp;</p> <p>&nbsp;</p> <p>Outcome 1: An important barrier to computational filmmaking is identifying a set of actions that are stereotypically of a type of scene that could be produced by the system.&nbsp; We developed a corpus of 30 films that we have described in term of actions that comprise the scenes.&nbsp; We have also developed a simple system for classifying the cinematic features of the how the actions are shot.&nbsp; This corpus is available on open science framework and is described in Winer et al. (in press). Winer et al., describes the process of developing the corpus so that other research groups could apply this approach.</p> <p>&nbsp;</p> <p>Outcome 2: Relative to narrative text, there is little research and theory on how we process and comprehend visual narratives.&nbsp; This project funded a series of studies that were conducted to better understand how film and graphic narratives are processed.&nbsp; This research has led to the proposal of Scene Perception and Comprehension Theory (SPECT; Loschky et al., in press), which describes how visual attention, scene perception, and comprehension processes are coordinated to support the understanding of visual narratives.&nbsp; Several studies are reported in the final report that were motivated by this framework.&nbsp; Within these studies, we have conducted research on computer generate films that provides a multipronged approach that can be applied to testing computational systems.&nbsp; This approach has been reported in conference proceedings, but a article is being prepared for publication that describes it.</p> <p>&nbsp;</p> <p>One important insight form this research was that the use of cinematic devices can create strong exogenous control of visual attention.&nbsp; We have learned that editing, for example, creates a situation in which there is attentional synchronicity in an audience. That is, a group of individuals will tend to look at the same regions of the screen at the same time as a scene unfolds, but still have qualitatively different ways of comprehending and reacting to the scene.&nbsp; This underscores the importance of intelligent film making systems in creating cinema that mirrors the same level of control in naturalistic films.&nbsp;</p> <p>&nbsp;</p> <p>Outcome 3. With respect to a model of research practitioner collaborations, we have been developing an approach to this with Tom Ackerman, who was a consultant on the project.&nbsp; Mr. Ackerman is a professional cinematographer.&nbsp; We are modeling our collaborations from a research &ndash; practitioner collaboration in the learning sciences.&nbsp; Clinton et al. (under review) describes this partnership.</p> <p>&nbsp;</p> <p>Outcome 4: Finally, one graduate student and one undergraduate student worked on the project.&nbsp; Dr. James Clinton&rsquo;s dissertation research (Ph.D, 2016) was supported by the grant, and was recently accepted for publication (Clinton et al., in press).&nbsp; Mr. Aidan Osterby was an undergraduate working on this project for all years of it (M.A., 2017).&nbsp; His honors thesis was supported by the project and we are continuing that research project.&nbsp; He is now in a graduate program at Kansas State University, and working with Dr. Lester Loschky.&nbsp; Dr. Loschky was a collaborator on some of the research supported by this project.&nbsp;&nbsp;</p><br> <p>            Last Modified: 09/12/2017<br>      Modified by: Joseph&nbsp;P&nbsp;Magliano</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There four sets of outcomes that were accomplished with this project.  The first was to develop a database of the actions and cinematic features associated with a type of scene extant in cinema that could be used by a computational film making system to create scenes of this nature.  The second outcome pertained to advances in theory and research associated with the processing of visual films. This outcome has implications for the first because it was intended to identify useful methodologies for exploring the validity of computational film making systems.   A third outcome was to develop a framework for collaborations between cognitive scientist and filmmakers.  Finally, the fourth outcome pertained to student training.      Outcome 1: An important barrier to computational filmmaking is identifying a set of actions that are stereotypically of a type of scene that could be produced by the system.  We developed a corpus of 30 films that we have described in term of actions that comprise the scenes.  We have also developed a simple system for classifying the cinematic features of the how the actions are shot.  This corpus is available on open science framework and is described in Winer et al. (in press). Winer et al., describes the process of developing the corpus so that other research groups could apply this approach.     Outcome 2: Relative to narrative text, there is little research and theory on how we process and comprehend visual narratives.  This project funded a series of studies that were conducted to better understand how film and graphic narratives are processed.  This research has led to the proposal of Scene Perception and Comprehension Theory (SPECT; Loschky et al., in press), which describes how visual attention, scene perception, and comprehension processes are coordinated to support the understanding of visual narratives.  Several studies are reported in the final report that were motivated by this framework.  Within these studies, we have conducted research on computer generate films that provides a multipronged approach that can be applied to testing computational systems.  This approach has been reported in conference proceedings, but a article is being prepared for publication that describes it.     One important insight form this research was that the use of cinematic devices can create strong exogenous control of visual attention.  We have learned that editing, for example, creates a situation in which there is attentional synchronicity in an audience. That is, a group of individuals will tend to look at the same regions of the screen at the same time as a scene unfolds, but still have qualitatively different ways of comprehending and reacting to the scene.  This underscores the importance of intelligent film making systems in creating cinema that mirrors the same level of control in naturalistic films.      Outcome 3. With respect to a model of research practitioner collaborations, we have been developing an approach to this with Tom Ackerman, who was a consultant on the project.  Mr. Ackerman is a professional cinematographer.  We are modeling our collaborations from a research &ndash; practitioner collaboration in the learning sciences.  Clinton et al. (under review) describes this partnership.     Outcome 4: Finally, one graduate student and one undergraduate student worked on the project.  Dr. James Clinton?s dissertation research (Ph.D, 2016) was supported by the grant, and was recently accepted for publication (Clinton et al., in press).  Mr. Aidan Osterby was an undergraduate working on this project for all years of it (M.A., 2017).  His honors thesis was supported by the project and we are continuing that research project.  He is now in a graduate program at Kansas State University, and working with Dr. Lester Loschky.  Dr. Loschky was a collaborator on some of the research supported by this project.         Last Modified: 09/12/2017       Submitted by: Joseph P Magliano]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
