<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Scaling up Modeling and Statistical Inference for Massive Collections of Time Series</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2014</AwardEffectiveDate>
<AwardExpirationDate>05/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>549178.00</AwardTotalIntnAmount>
<AwardAmount>549178</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hector Munoz-Avila</SignBlockName>
<PO_EMAI>hmunoz@nsf.gov</PO_EMAI>
<PO_PHON>7032924481</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Consider the task of predicting influenza rates at a very large set of spatial locations. Modeling each region independently does not leverage the information from related regions and can lead to poor predictions, especially in the presence of missing observations. Likewise, imagine estimating the value of every house in the United States. Capturing trends within a neighborhood is key; however, each neighborhood only has a few recent house sales. The challenges presented by these increasingly prevalent massive time series are endemic to a wide range of applications, from crime modeling for police resource allocation to forecasting consumer trends and social networks: the individual data streams often include only infrequent observations such that each alone does not provide sufficient data for accurate inferences. However, the structured relationships between them offer an opportunity to share information.  A key question is how to discover these relationships.  &lt;br/&gt;&lt;br/&gt;This project takes a computationally-driven Bayesian nonparametric approach, trading off flexibility and scalability, to address the challenges of massive collections of infrequently observed time series. Our approaches exploit correlation among the data streams, e.g., among related regions, while enabling data-driven discovery of sparse dependencies. The multi-resolution and modular forms also allow incorporation of heterogeneous side information. Key to the success of the proposed methods is scalable Bayesian posterior inference. We focus on (i) parallel computations exploiting sparse graph dependencies, (ii) multi-resolution inference, and (iii) online algorithms for dependent data.&lt;br/&gt;&lt;br/&gt;This project represents an ambitious cross-disciplinary effort, integrating ideas from machine learning, systems, engineering, and statistics. The work addresses a largely ignored question in the discussion on big data: How to cope with modeling and computational issues when the data has crucial structure across time, especially arising from individually sparse and disparate measurement sources. The tools developed will significantly broaden the scope of scientific questions that can be addressed. Results from this work will be publicly disseminated, including through open source software, and our industry partners aim to transition the technology into real-world systems. This project also involves developing (i) exciting and intensive programs harnessing existing infrastructure, UW DawgBytes, to increase the exposure of K-12 students, and especially girls, to machine learning; and (ii) curriculum training students in both statistical and computational thinking.&lt;br/&gt;&lt;br/&gt;For further information, see the project website at http://www.stat.washington.edu/~ebfox/CAREER.html.</AbstractNarration>
<MinAmdLetterDate>06/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>03/31/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1350133</AwardID>
<Investigator>
<FirstName>Emily</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emily B Fox</PI_FULL_NAME>
<EmailAddress>ebfox@uw.edu</EmailAddress>
<PI_PHON>2062219341</PI_PHON>
<NSF_ID>000514440</NSF_ID>
<StartDate>06/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981954322</ZipCode>
<StreetAddress><![CDATA[Department of Statistics]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~99342</FUND_OBLG>
<FUND_OBLG>2015~102852</FUND_OBLG>
<FUND_OBLG>2016~110880</FUND_OBLG>
<FUND_OBLG>2017~115564</FUND_OBLG>
<FUND_OBLG>2018~120540</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Intellectual Merit<br /><br /></strong>Most current approaches to time series modeling fail to scale to the structure of modern, large data streams. One major challenge is how to handle a large collection of individually sparse data streams. In such situations, each time series alone does not provide sufficient data for accurate inferences. However, the structured relationships between them presents an opportunity to share information. A key question is how to do this in a scalable manner and in the presence of disparate observation types. For example, there is insufficient data or computational resources to infer the parameters associated with a massive joint model for the data stream. We term this the "big p, infrequent n" problem.</p> <p>The goal of this project represents a significant paradigm shift in dynamical modeling: we take a computationally-&shy;driven Bayesian approach, trading off between flexibility and scalability, to address the challenges of big p, infrequent n time series. This approach provides efficient sharing of information, while enabling data-&shy;driven discovery of sparse dependencies. Another key component to the project is the development of scalable Bayesian posterior inference algorithms for models of dependent data.</p> <p>Our research focused on several key thrusts:</p> <ol> <li>Bayesian dynamical models for individually sparse time series, leveraging clusters and hierarchies to share information</li> <li>Learning directed, undirected, and factor-based networks in large, messy time series</li> <li>Deep learning models combined with sparsity-inducing penalties to handle limited data scenarios and get at notions of interpretability</li> <li>Large-&shy;scale Bayesian inference algorithms, including for time series</li> <li>Applying our methods to real-world datasets, including from neuroscience, genomics, an analysis of housing prices and homelessness, and motion capture data.</li> <li>Creating and disseminating open-&shy;source software.</li> </ol> <p>Results of our analyses include:</p> <ul> <li>Forming a housing index robustly estimated at a local level, even finer-scale than census tracts</li> <li>Analyzing the dynamics of homelessness and the relationship between rent levels and rates of homelessness</li> <li>Studying networks in the brain from MEG data</li> <li>Segmenting a human chromatin sequence in under an hour, whereas past methods took days</li> <li>Rapidly parsing weeks of intracranial EEG data of seizures into interpretable states.</li> </ul> <p>This work was disseminated to the community through presentations and publications, including 10 papers in top statistics and machine learning journals, 7 papers at top machine learning conferences, and 6 papers at peer-reviewed machine learning workshops.&nbsp; Open source software was released on GitHub and as a package in R.<br /><br /><strong>Broader Impact</strong><br /><br />The PI co-developed a 4-course Coursera Machine Learning Specialization. The specialization was developed to be accessible to people with no ML or Stats background and only basic programming knowledge. The courses use a case studies approach, grounding all presented material in a series of real-world applications that students explore in hands-on exercises.&nbsp; The Coursera Machine Learning Specialization has been very successful, seeing enrollment of hundreds of thousands of learners. Importantly, the courses gained worldwide traction, with learners in all but a handful of countries.&nbsp; The learners cover age brackets 18-24 to 65+, are primarily not full-time or part-time students, have significant representation from those without a college degree, and include those unemployed and looking for work.&nbsp; The diversity of learners clearly indicates the broad reach of this specialization and its impact across demographics, well beyond the walls of academia.</p> <p>The PI mentored numerous graduate and undergraduate students and postdocs throughout the project, and each had opportunities to present their work and attend conferences to advance their training.&nbsp; Multiple projects were in collaboration with Zillow, an industry partner, and involved student and postdoc participation.&nbsp; One of the projects studying the dynamics of homelessness targetted informing policy at the metro level.</p> <p>The PI also led and participated in numerous events aimed at increasing participating and retention of women in machine learning, statistics, and computing more broadly.&nbsp;&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/11/2021<br>      Modified by: Emily&nbsp;B&nbsp;Fox</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit  Most current approaches to time series modeling fail to scale to the structure of modern, large data streams. One major challenge is how to handle a large collection of individually sparse data streams. In such situations, each time series alone does not provide sufficient data for accurate inferences. However, the structured relationships between them presents an opportunity to share information. A key question is how to do this in a scalable manner and in the presence of disparate observation types. For example, there is insufficient data or computational resources to infer the parameters associated with a massive joint model for the data stream. We term this the "big p, infrequent n" problem.  The goal of this project represents a significant paradigm shift in dynamical modeling: we take a computationally-&shy;driven Bayesian approach, trading off between flexibility and scalability, to address the challenges of big p, infrequent n time series. This approach provides efficient sharing of information, while enabling data-&shy;driven discovery of sparse dependencies. Another key component to the project is the development of scalable Bayesian posterior inference algorithms for models of dependent data.  Our research focused on several key thrusts:  Bayesian dynamical models for individually sparse time series, leveraging clusters and hierarchies to share information Learning directed, undirected, and factor-based networks in large, messy time series Deep learning models combined with sparsity-inducing penalties to handle limited data scenarios and get at notions of interpretability Large-&shy;scale Bayesian inference algorithms, including for time series Applying our methods to real-world datasets, including from neuroscience, genomics, an analysis of housing prices and homelessness, and motion capture data. Creating and disseminating open-&shy;source software.   Results of our analyses include:  Forming a housing index robustly estimated at a local level, even finer-scale than census tracts Analyzing the dynamics of homelessness and the relationship between rent levels and rates of homelessness Studying networks in the brain from MEG data Segmenting a human chromatin sequence in under an hour, whereas past methods took days Rapidly parsing weeks of intracranial EEG data of seizures into interpretable states.   This work was disseminated to the community through presentations and publications, including 10 papers in top statistics and machine learning journals, 7 papers at top machine learning conferences, and 6 papers at peer-reviewed machine learning workshops.  Open source software was released on GitHub and as a package in R.  Broader Impact  The PI co-developed a 4-course Coursera Machine Learning Specialization. The specialization was developed to be accessible to people with no ML or Stats background and only basic programming knowledge. The courses use a case studies approach, grounding all presented material in a series of real-world applications that students explore in hands-on exercises.  The Coursera Machine Learning Specialization has been very successful, seeing enrollment of hundreds of thousands of learners. Importantly, the courses gained worldwide traction, with learners in all but a handful of countries.  The learners cover age brackets 18-24 to 65+, are primarily not full-time or part-time students, have significant representation from those without a college degree, and include those unemployed and looking for work.  The diversity of learners clearly indicates the broad reach of this specialization and its impact across demographics, well beyond the walls of academia.  The PI mentored numerous graduate and undergraduate students and postdocs throughout the project, and each had opportunities to present their work and attend conferences to advance their training.  Multiple projects were in collaboration with Zillow, an industry partner, and involved student and postdoc participation.  One of the projects studying the dynamics of homelessness targetted informing policy at the metro level.  The PI also led and participated in numerous events aimed at increasing participating and retention of women in machine learning, statistics, and computing more broadly.                  Last Modified: 06/11/2021       Submitted by: Emily B Fox]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
