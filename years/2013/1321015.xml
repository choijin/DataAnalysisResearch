<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Multi-View Learning of Acoustic Features for Speech Recognition Using Articulatory Measurements</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>444859.00</AwardTotalIntnAmount>
<AwardAmount>444859</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project explores techniques for learning acoustic features for speech recognition, based on multi-view learning using acoustic and articulatory recordings.  Recent work has shown recognition improvements using this strategy via linear and nonlinear canonical correlation analysis, in which transformations of acoustic features are learned so as to maximize correlation with (transformations of) articulatory measurements.  Prior work has been limited to a single database and a single language.&lt;br/&gt;&lt;br/&gt;The main goals of this project are to learn better universal features for arbitrary speakers and languages and to develop improved multi-view techniques.  Project activities include: learning time-varying projections; multi-view techniques based on neural networks; "many-view" learning using articulation, video, labels, etc.; efficient implementations; new input features such as spectro-temporal filters; and visualization tools for related research and education.&lt;br/&gt;&lt;br/&gt;A critical component of automatic speech recognition is a representation of the audio signal that encapsulates useful information while discarding acoustic noise, speaker identity, and so on.  This project aims to automatically learn improved representations using statistical analysis of audio recordings paired with positions of the speech articulators (lips, tongue, etc.) and other measurements.  The project starts with basic statistical techniques, and develops new techniques that address challenges and opportunities specific to speech and related signals.&lt;br/&gt;&lt;br/&gt;The project's impact extends beyond speech processing.  Applications of multi-view representation learning include neurology, meteorology, chemometrics, computer vision, and text processing; all of these can benefit from the improved techniques.  The work impacts education by generating materials for a Speech Technologies course and visualization tools for speech and other signals.</AbstractNarration>
<MinAmdLetterDate>07/31/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/01/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1321015</AwardID>
<Investigator>
<FirstName>Karen</FirstName>
<LastName>Livescu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karen Livescu</PI_FULL_NAME>
<EmailAddress>klivescu@ttic.edu</EmailAddress>
<PI_PHON>7738342549</PI_PHON>
<NSF_ID>000512036</NSF_ID>
<StartDate>07/31/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Toyota Technological Institute at Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372803</ZipCode>
<PhoneNumber>7738340409</PhoneNumber>
<StreetAddress>6045 S Kenwood Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>127228927</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Toyota Technological Institute at Chicago]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>606372902</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~185675</FUND_OBLG>
<FUND_OBLG>2014~185350</FUND_OBLG>
<FUND_OBLG>2015~73834</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project explores methods for improving automatic speech recognition by developing new representations of the speech signal.&nbsp; In particular, the project uses multi-view machine learning methods to automatically learn improved speech representations from data consisting of speech recordings paired with simultaneous physiological measurements of motions of the articulators (lips, tongue, etc.).&nbsp; Knowledge of the articulatory motions can improve speech recognizer performance, but articulatory measurements are not normally available when speech recognizers are deployed.&nbsp; Therefore, the articulatory measurements are used together with audio training data to inform the learning algorithm about meaningful dimensions of the speech signal.</p> <p><br />The methods developed in the project are mainly based on canonical correlation analysis (CCA), a classifical statistical technique.&nbsp; Prior work had shown that CCA-based representations can improve speech recognition in certain constrained settings.&nbsp; This project has developed several new CCA-based methods that automatically learn nonlinear representations, especially using deep neural networks.&nbsp; In addition to CCA-based techniques, the project has also explored other approaches such as contrastive learning.&nbsp; The project has focused on making the techniques broadly applicable to arbitrary speakers and domains, as well as on efficient implementations.</p> <p><br />The project results have shown that speech representations learned in this setting can improve speech recognition performance across speakers and domains.&nbsp; In addition, while the main focus has been speech recognition and articulatory data, the project has also demonstrated the broader applicability of the methods by applying them to other tasks.&nbsp; For example, the project has included learning improved multilingual word embeddings, jointly learning representations of images and captions, and learning acoustic word embeddings.</p> <p><br />In addition to these main findings, the project has had broader impact on neighboring fields and on education.&nbsp; Techniques developed under this project have been used, for example, by researchers in computer vision, music retrieval, neuroscience, and medical imaging.&nbsp; The project has also raised theoretical questions about optimization in multi-view representation learning, and these theoretical questions have been subsequently pursued by other researchers.&nbsp; Data and software created for this project have been used by other researchers, in workshop tutorials, and in the PI's courses on speech technologies and unsupervised machine learning.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/29/2017<br>      Modified by: Karen&nbsp;Livescu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project explores methods for improving automatic speech recognition by developing new representations of the speech signal.  In particular, the project uses multi-view machine learning methods to automatically learn improved speech representations from data consisting of speech recordings paired with simultaneous physiological measurements of motions of the articulators (lips, tongue, etc.).  Knowledge of the articulatory motions can improve speech recognizer performance, but articulatory measurements are not normally available when speech recognizers are deployed.  Therefore, the articulatory measurements are used together with audio training data to inform the learning algorithm about meaningful dimensions of the speech signal.   The methods developed in the project are mainly based on canonical correlation analysis (CCA), a classifical statistical technique.  Prior work had shown that CCA-based representations can improve speech recognition in certain constrained settings.  This project has developed several new CCA-based methods that automatically learn nonlinear representations, especially using deep neural networks.  In addition to CCA-based techniques, the project has also explored other approaches such as contrastive learning.  The project has focused on making the techniques broadly applicable to arbitrary speakers and domains, as well as on efficient implementations.   The project results have shown that speech representations learned in this setting can improve speech recognition performance across speakers and domains.  In addition, while the main focus has been speech recognition and articulatory data, the project has also demonstrated the broader applicability of the methods by applying them to other tasks.  For example, the project has included learning improved multilingual word embeddings, jointly learning representations of images and captions, and learning acoustic word embeddings.   In addition to these main findings, the project has had broader impact on neighboring fields and on education.  Techniques developed under this project have been used, for example, by researchers in computer vision, music retrieval, neuroscience, and medical imaging.  The project has also raised theoretical questions about optimization in multi-view representation learning, and these theoretical questions have been subsequently pursued by other researchers.  Data and software created for this project have been used by other researchers, in workshop tutorials, and in the PI's courses on speech technologies and unsupervised machine learning.          Last Modified: 11/29/2017       Submitted by: Karen Livescu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
