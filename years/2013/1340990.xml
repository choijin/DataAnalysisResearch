<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC-NIE Networking Infrastructure: Accelerating Research Data Transit Between the Scientist's Desktop, Campus, and National Cyberinfrastructure</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>499935.00</AwardTotalIntnAmount>
<AwardAmount>499935</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project accelerates transformative research in numerous data bound STEM fields through acquisition, deployment and efficient sustained operation of high capability research network infrastructure. Notre Dame scientists are too often: working on local systems of insufficient capacity because remote access to national resources is of unreliable performance; deploying 100s of TB of local storage because remote data access is too slow for local computation; moving data between campuses at 10Mbps despite fiber connectivity at 10Gbps; decoupled from access to experimental data taken in disparate campus facilities; or worse, simply not sharing available data due to the complexity and poor reliability of network transfers for their large datasets. In partnership with the University's central enterprise IT organization, the Center for Research Computing identified those unfunded components most critical to science at the University today and essential for supporting the increasingly data centric science of tomorrow. The proposed system addresses the bottlenecks, Accelerating Research Data Transit (ARDT) between the scientist's desktop, campus, and national cyberinfrastructure resources. ARDT extends Notre Dame's current infrastructure with three new components: a new science DMZ that provides an optimized external network path clear of traditional campus network appliances which hinder campus bridging; a new core switch and router for the research data center which adds many more higher speed lower latency uplinks; and endpoint building network upgrades for faculty in engineering, physical, and social sciences providing over 100 10Gb and 500 1Gb new network ports.</AbstractNarration>
<MinAmdLetterDate>09/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/09/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1340990</AwardID>
<Investigator>
<FirstName>Joannes</FirstName>
<LastName>Westerink</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joannes J Westerink</PI_FULL_NAME>
<EmailAddress>jjw@nd.edu</EmailAddress>
<PI_PHON>5746316475</PI_PHON>
<NSF_ID>000181253</NSF_ID>
<StartDate>05/09/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Bensman</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward L Bensman</PI_FULL_NAME>
<EmailAddress>ebensman@nd.edu</EmailAddress>
<PI_PHON>5746317432</PI_PHON>
<NSF_ID>000504387</NSF_ID>
<StartDate>09/06/2013</StartDate>
<EndDate>05/09/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jaroslaw</FirstName>
<LastName>Nabrzyski</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jaroslaw Nabrzyski</PI_FULL_NAME>
<EmailAddress>naber@nd.edu</EmailAddress>
<PI_PHON>5746312400</PI_PHON>
<NSF_ID>000510570</NSF_ID>
<StartDate>09/06/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Brenner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul Brenner</PI_FULL_NAME>
<EmailAddress>paul.r.brenner@nd.edu</EmailAddress>
<PI_PHON>5746317432</PI_PHON>
<NSF_ID>000547652</NSF_ID>
<StartDate>09/06/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ron</FirstName>
<LastName>Kraemer</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ron D Kraemer</PI_FULL_NAME>
<EmailAddress>rkraemer@nd.edu</EmailAddress>
<PI_PHON>5746319700</PI_PHON>
<NSF_ID>000644456</NSF_ID>
<StartDate>09/06/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Notre Dame</Name>
<CityName>NOTRE DAME</CityName>
<ZipCode>465565708</ZipCode>
<PhoneNumber>5746317432</PhoneNumber>
<StreetAddress>940 Grace Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>824910376</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NOTRE DAME DU LAC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>048994727</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Notre Dame]]></Name>
<CityName>Notre Dame</CityName>
<StateCode>IN</StateCode>
<ZipCode>465565612</ZipCode>
<StreetAddress><![CDATA[Center for Research Computing]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~499935</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project centered on network upgrades to the ND research data center core switch, 40Gbps connectivity between the colleges of engineering and science, and the partial upgrade of social science facility networks via three network end point packages.&nbsp; The PIs were able to add 100Gb components to the core router and build a Science DMZ connected to peer universities and laboratories at 100Gbps via I-Light, Internet2 and ESnet. &nbsp;NSF and ND&rsquo;s investment had a secondary effect, helping I-Light improve the northern state ring which serves multiple smaller predominantly undergraduate universities such as Indiana University South Bend, Bethel, Trine, Valpariso, Purdue North Central and Indiana Purdue Fort Wayne.&nbsp;</p> <p>Participants attended the CC-NIE workshop held in Austin TX and shared best practices with peer institutions.&nbsp; The operational sustainability components and collaborations outlined in the original proposal are in place.&nbsp; We expect the NSF equipment investments to be operational for at least another 5 years and the newly enabled scientific projects and collaborations to continue for many years beyond the respective equipment lifetime.</p> <p>Science Driver Success:</p> <p>The Notre Dame High Energy Physics (HEP) Compact Muon Solenoid (CMS) faculty, students and scientific staff operate CMS Tier 3 infrastructure in collaboration with the Center for Research Computing and the Cooperative Computing Lab.&nbsp; This collaboration allows them opportunistic access to campus cluster resources averaging over 5,000 cores and bursting above 20,000 cores.&nbsp; The computational capacity would provide research capacity on par with the US Tier 2 sites were it not for limitations in network bandwidth and data storage resources.</p> <p>The NSF funds provided not only a core datacenter switch capable of handling 100Gb but also the addition of all 100Gb card, transceiver and routing software license requirements.</p> <p>The transit of CMS data into the 5,000+ available compute cores was made possible by a secure bridge between the 100Gbps Science DMZ VLAN and the CRC data center primary VLAN.&nbsp; 5 XRootD cache servers each configured with 12 2TB disks (or larger) and a 10Gb network link provide over 100TB of cache space.&nbsp; With collaborative support from colleagues at UCSD we have tuned each cache server to deliver 6-7Gbps of data transfer for production workflows.&nbsp; To date we have achieved sustained 30-minute average bandwidth measures of over 25Gbps for CMS data transfers.&nbsp; This is &gt;2.5X the maximum observed when data transit was limited to the campus enterprise 10Gb connection.&nbsp; With additional XRootD cache servers and further tuning we hope to reach 50Gbps sustained in the coming year.&nbsp; This allows our CMS team to fully utilize the available computational cores on campus while reducing their costs/requirement to provision storage for duplicate data stored at ND.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/16/2016<br>      Modified by: Paul&nbsp;Brenner</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project centered on network upgrades to the ND research data center core switch, 40Gbps connectivity between the colleges of engineering and science, and the partial upgrade of social science facility networks via three network end point packages.  The PIs were able to add 100Gb components to the core router and build a Science DMZ connected to peer universities and laboratories at 100Gbps via I-Light, Internet2 and ESnet.  NSF and ND?s investment had a secondary effect, helping I-Light improve the northern state ring which serves multiple smaller predominantly undergraduate universities such as Indiana University South Bend, Bethel, Trine, Valpariso, Purdue North Central and Indiana Purdue Fort Wayne.   Participants attended the CC-NIE workshop held in Austin TX and shared best practices with peer institutions.  The operational sustainability components and collaborations outlined in the original proposal are in place.  We expect the NSF equipment investments to be operational for at least another 5 years and the newly enabled scientific projects and collaborations to continue for many years beyond the respective equipment lifetime.  Science Driver Success:  The Notre Dame High Energy Physics (HEP) Compact Muon Solenoid (CMS) faculty, students and scientific staff operate CMS Tier 3 infrastructure in collaboration with the Center for Research Computing and the Cooperative Computing Lab.  This collaboration allows them opportunistic access to campus cluster resources averaging over 5,000 cores and bursting above 20,000 cores.  The computational capacity would provide research capacity on par with the US Tier 2 sites were it not for limitations in network bandwidth and data storage resources.  The NSF funds provided not only a core datacenter switch capable of handling 100Gb but also the addition of all 100Gb card, transceiver and routing software license requirements.  The transit of CMS data into the 5,000+ available compute cores was made possible by a secure bridge between the 100Gbps Science DMZ VLAN and the CRC data center primary VLAN.  5 XRootD cache servers each configured with 12 2TB disks (or larger) and a 10Gb network link provide over 100TB of cache space.  With collaborative support from colleagues at UCSD we have tuned each cache server to deliver 6-7Gbps of data transfer for production workflows.  To date we have achieved sustained 30-minute average bandwidth measures of over 25Gbps for CMS data transfers.  This is &gt;2.5X the maximum observed when data transit was limited to the campus enterprise 10Gb connection.  With additional XRootD cache servers and further tuning we hope to reach 50Gbps sustained in the coming year.  This allows our CMS team to fully utilize the available computational cores on campus while reducing their costs/requirement to provision storage for duplicate data stored at ND.          Last Modified: 09/16/2016       Submitted by: Paul Brenner]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
