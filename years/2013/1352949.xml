<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Assessing Assessments: A Historical and Philosophical Study of Scientific Assessments for Environmental Policy in the Late 20th Century</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>104949.00</AwardTotalIntnAmount>
<AwardAmount>145470</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Frederick Kronz</SignBlockName>
<PO_EMAI>fkronz@nsf.gov</PO_EMAI>
<PO_PHON>7032927283</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Assessing Assessments: A Historical and Philosophical Study of Scientific &lt;br/&gt;Assessments for Environmental Policy in the late 20th Century&lt;br/&gt;&lt;br/&gt;Intellectual merit&lt;br/&gt;&lt;br/&gt;This project investigates the process by which scientists attempt to assess and summarize--accurately, fairly, objectively--the scientific knowledge that society needs to make informed judgments about complex issues. How do scientists evaluate the reliability of their colleagues? research, understand its limits and degrees of uncertainty, and come to consensus (or not)?  How do scientists respond to the subtle or overt pressures that arise when they know the world is watching the outcome of the process? What factors internal to the process may lead to systematic bias, error, or distortion? &lt;br/&gt;Periodically, scientists come together to present scientific information at the request of policy-makers and for the benefit of the general public, particularly in the area of environmental science relevant to policy.  Recent examples include the National Acid Precipitation Assessment Program, the Ozone Trends Panel, and the Intergovernmental Panel on Climate Change, organized to summarize and assess the state of the art with respect to acid rain, global ozone depletion, and anthropogenic climate change, respectively.  Such assessments consume large amounts of time, money, and manpower, and they can play a major role in policy debates. Therefore, it is important to understand how they work (or fail to work). Yet, few scholars have studied them.  &lt;br/&gt;&lt;br/&gt;Broader Impact&lt;br/&gt;&lt;br/&gt;Understanding the internal dynamics of the assessment process puts us in a better position to judge the quality of any particular assessment, as well as to suggest potential means of improvement, and, ultimately, to provide an improved foundation for science-based policy. The presumption that assessments should influence policy assumes that the quality of these assessments is high, but that assumption has rarely been closely examined.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/11/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/11/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1352949</AwardID>
<Investigator>
<FirstName>Naomi</FirstName>
<LastName>Oreskes</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Naomi Oreskes</PI_FULL_NAME>
<EmailAddress>oreskes@fas.harvard.edu</EmailAddress>
<PI_PHON>6174953480</PI_PHON>
<NSF_ID>000146828</NSF_ID>
<StartDate>09/11/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021383846</ZipCode>
<StreetAddress><![CDATA[1350 Holyoke Center]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7603</Code>
<Text>STS-Sci, Tech &amp; Society</Text>
</ProgramElement>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~104948</FUND_OBLG>
<FUND_OBLG>2012~40522</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>2014 Annual Report</strong></p> <p>&nbsp;</p> <p>Project Title: Collaborative Research: Assessing Assessments: A Historical and Philosophical Study of Scientific Assessments for Environmental Policy in the Late 20th Century&nbsp;<br />PI: Naomi Oreskes&nbsp;<br />Awardee: Harvard University&nbsp;<br />Award Number: 1352949</p> <p>&nbsp;</p> <p>Large-scale, organized and formalized assessments of the state of scientific knowledge are now an important part of the scientific and policy landscape, particularly in the earth and environmental sciences.&nbsp;Structured assessments have played a major role in debates over, and policy decisions about, acid rain, ozone depletion, and global warming; they have also engaged large numbers of scientists and cost a great deal of money.&nbsp;&nbsp;The human and financial resources devoted over the past thirty years to the Intergovernmental Panel on Climate Change, for example, runs into hundreds of thousands of man-hours and tens of millions of dollars of direct expenses.&nbsp;Yet, there has been relatively little consideration of how&mdash;or how well&mdash;these assessments work. This project sought to assess these assessments: to explore their internal dynamics and understand how the scientists involved make the judgments they do.&nbsp;&nbsp;&nbsp;We have studied how scientists involved in assessments understand their role, try to manage personal bias, and produce objective results that can offer a valid basis for decision-making about matters that require scientific information and understanding.&nbsp;</p> <p>&nbsp;</p> <p>The project is now complete. In addition to numerous scientific papers and conference presentations, already reported, we report here on the completion of our final product:&nbsp;<em>Discerning Experts: The Practices of Scientific Assessment for Environmental Policy,</em>published by the University of Chicago Press in February 2019.&nbsp;&nbsp;</p> <p>&nbsp;</p> <p>Among our findings and recommendations are the following:&nbsp;</p> <p>&nbsp;</p> <p>1)&nbsp;&nbsp;&nbsp;Assessments are not just summaries of existing knowledge; they also produce new knowledge and drive research agenda. This may be good&mdash;more knowledge is better than less&mdash;but it also may raise questions about the stability and reliability of the outcomes. Assessments should reflect settled knowledge. In most cases they do, but not all.</p> <p>&nbsp;</p> <p>2)&nbsp;&nbsp;&nbsp;Scientists in assessments generally strive for consensus and often consider it necessary. We suggest that this may be counter-productive, leading to &ldquo;least common denominator&rdquo; results, and that scientists should consider alternative methods for accurately reporting out both consensus and dissent.&nbsp;</p> <p>&nbsp;</p> <p>3)&nbsp;&nbsp;&nbsp;Scientists in assessments often strive to achieve objectivity through a &ldquo;balance of bias&rdquo;, in which the participants represent a variety of viewpoints. Typically, demographic diversity is used as a proxy for perspectival diversity. We believe that such demographic diversity is important, but it will only work as intended if those diverse voices are truly heard.&nbsp;&nbsp;More work is needed to understand if, for example, the IPCC emphasis on gender and demographic diversity is working as intended.&nbsp;</p> <p>&nbsp;</p> <p>4)&nbsp;&nbsp;&nbsp;Assessors almost always strive for &ldquo;policy neutrality,&rdquo; but it may not be possible or even desirable. We believe that scientists should not be afraid to make policy recommendations when they fall out as logical consequences from scientific findings, e.g., the need to control CFCs to prevent ozone depletion or the need to control GHGs to prevent disruptive climate change.&nbsp;&nbsp;However, we also think that, when scientists do so, it is incumbent upon them to comment only in areas where they have true expertise. We introduce the notion of &ldquo;proximate expertise&rdquo; as a means to conceptualize when scientists should be speaking up on policy implications and when they should not. A climate scientist may be warranted in making recommendations on climate policy, but not, for example, on vaccination safety.&nbsp;</p> <p>&nbsp;</p> <p>Assessments are an important locus of scientific knowledge production, and are likely to remain so for the foreseeable future.&nbsp;&nbsp;Governments, organizations running assessments, and scientists involved in them should find ways to ensure that the assessment process continues to be innovative, and that the assessment economy does not crowd out other forms of scientific knowledge production.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/17/2019<br>      Modified by: Naomi&nbsp;Oreskes</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ 2014 Annual Report     Project Title: Collaborative Research: Assessing Assessments: A Historical and Philosophical Study of Scientific Assessments for Environmental Policy in the Late 20th Century  PI: Naomi Oreskes  Awardee: Harvard University  Award Number: 1352949     Large-scale, organized and formalized assessments of the state of scientific knowledge are now an important part of the scientific and policy landscape, particularly in the earth and environmental sciences. Structured assessments have played a major role in debates over, and policy decisions about, acid rain, ozone depletion, and global warming; they have also engaged large numbers of scientists and cost a great deal of money.  The human and financial resources devoted over the past thirty years to the Intergovernmental Panel on Climate Change, for example, runs into hundreds of thousands of man-hours and tens of millions of dollars of direct expenses. Yet, there has been relatively little consideration of how&mdash;or how well&mdash;these assessments work. This project sought to assess these assessments: to explore their internal dynamics and understand how the scientists involved make the judgments they do.   We have studied how scientists involved in assessments understand their role, try to manage personal bias, and produce objective results that can offer a valid basis for decision-making about matters that require scientific information and understanding.      The project is now complete. In addition to numerous scientific papers and conference presentations, already reported, we report here on the completion of our final product: Discerning Experts: The Practices of Scientific Assessment for Environmental Policy,published by the University of Chicago Press in February 2019.       Among our findings and recommendations are the following:      1)   Assessments are not just summaries of existing knowledge; they also produce new knowledge and drive research agenda. This may be good&mdash;more knowledge is better than less&mdash;but it also may raise questions about the stability and reliability of the outcomes. Assessments should reflect settled knowledge. In most cases they do, but not all.     2)   Scientists in assessments generally strive for consensus and often consider it necessary. We suggest that this may be counter-productive, leading to "least common denominator" results, and that scientists should consider alternative methods for accurately reporting out both consensus and dissent.      3)   Scientists in assessments often strive to achieve objectivity through a "balance of bias", in which the participants represent a variety of viewpoints. Typically, demographic diversity is used as a proxy for perspectival diversity. We believe that such demographic diversity is important, but it will only work as intended if those diverse voices are truly heard.  More work is needed to understand if, for example, the IPCC emphasis on gender and demographic diversity is working as intended.      4)   Assessors almost always strive for "policy neutrality," but it may not be possible or even desirable. We believe that scientists should not be afraid to make policy recommendations when they fall out as logical consequences from scientific findings, e.g., the need to control CFCs to prevent ozone depletion or the need to control GHGs to prevent disruptive climate change.  However, we also think that, when scientists do so, it is incumbent upon them to comment only in areas where they have true expertise. We introduce the notion of "proximate expertise" as a means to conceptualize when scientists should be speaking up on policy implications and when they should not. A climate scientist may be warranted in making recommendations on climate policy, but not, for example, on vaccination safety.      Assessments are an important locus of scientific knowledge production, and are likely to remain so for the foreseeable future.  Governments, organizations running assessments, and scientists involved in them should find ways to ensure that the assessment process continues to be innovative, and that the assessment economy does not crowd out other forms of scientific knowledge production.                    Last Modified: 02/17/2019       Submitted by: Naomi Oreskes]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
