<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Object and Activity Recognition as the Maximum Weight Subgraph Problem with Mutual Exclusion Constraints</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>493800.00</AwardTotalIntnAmount>
<AwardAmount>525800</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>It has been widely acknowledged that recognizing objects in images, and human activities in video - the basic problems in computer vision - can be significantly improved by accounting for object (activity) parts, context, and their spatiotemporal relationships. This is because these constraints facilitate resolving ambiguous hypotheses in the face of uncertainty. Since parts and contexts can be efficiently modeled by graphical models (e.g., Conditional Random Field), object and activity recognition are often formulated as probabilistic inference of graphical models. The project develops a new theoretical framework of graphical models that explicitly encodes high-order, spatiotemporal, hierarchical, and contextual interactions among objects (activities) as Quadratic Mutual-Exclusion Constraints (QMCs), for the purposes of object and activity recognition in images and video.&lt;br/&gt;&lt;br/&gt;The key contributions of the project work include: 1) Approaches to view-invariant object and activity recognition; 2) Formulations of learning and inference of graphical models representing objects and human activities, as finding a maximum weight subgraph (MWS) under the QMCs; 3) Polynomial-time algorithms for solving the MWS problem subject to QMCs; and 4) Explicit performance bounds and theoretical guarantees of tightness and convergence of the proposed learning and inference algorithms. &lt;br/&gt;&lt;br/&gt;The project framework encodes hard constraints from the domain of interest that have never been used in prior work, and uses principled, polynomial-time algorithms for learning and inference. The research of this project advances the state of the art in object and activity recognition, and enables new applications including video surveillance, retrieval from large datasets, and perception of mobile robots.</AbstractNarration>
<MinAmdLetterDate>04/26/2013</MinAmdLetterDate>
<MaxAmdLetterDate>03/22/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302164</AwardID>
<Investigator>
<FirstName>Longin Jan</FirstName>
<LastName>Latecki</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Longin Jan Latecki</PI_FULL_NAME>
<EmailAddress>latecki@temple.edu</EmailAddress>
<PI_PHON>2152045781</PI_PHON>
<NSF_ID>000227657</NSF_ID>
<StartDate>04/26/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Temple University</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191226003</ZipCode>
<PhoneNumber>2157077547</PhoneNumber>
<StreetAddress>1801 N. Broad Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>057123192</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>057123192</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Temple University]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191226094</ZipCode>
<StreetAddress><![CDATA[1805 North Broad St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~133405</FUND_OBLG>
<FUND_OBLG>2014~149374</FUND_OBLG>
<FUND_OBLG>2015~227021</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project has advanced object and activity recognition in images and videos by reducing uncertainty with Quadratic Mutual-Exclusion Constraints (QMCs) about object and activity classes, which are learned directly from training data. Visual recognition has been cast as the problem of finding a maximum weight subgraph (MWS) subject to QMCs, and new, polynomial-time algorithms have been developed for solving this constrained problem. Explicit performance bounds and theoretical guarantees of tightness and convergence of the proposed learning and inference algorithms have been derived. New directions have been explored by integrating QMCs with recent deep learning approaches for weakly supervised semantic segmentation of images, fine-grained object detection in images, and estimating boundary flow (i.e., motions of object boundaries) in videos. Since accounting for QMCs, in general, increases computational complexity, as a tradeoff for improving accuracy of recognition, the project has also studied improving performance under a limited time budget. The project has produced: over 20 publications presented in high-impact journals and at top vision conferences, open- source software, and served as a research material for four successfully defended PhD Theses, a number of MS theses. It has also provided training of seven undergraduate REU students (among those two minority) at Oregon State University and Temple University.</p><br> <p>            Last Modified: 09/06/2019<br>      Modified by: Longin Jan&nbsp;Latecki</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project has advanced object and activity recognition in images and videos by reducing uncertainty with Quadratic Mutual-Exclusion Constraints (QMCs) about object and activity classes, which are learned directly from training data. Visual recognition has been cast as the problem of finding a maximum weight subgraph (MWS) subject to QMCs, and new, polynomial-time algorithms have been developed for solving this constrained problem. Explicit performance bounds and theoretical guarantees of tightness and convergence of the proposed learning and inference algorithms have been derived. New directions have been explored by integrating QMCs with recent deep learning approaches for weakly supervised semantic segmentation of images, fine-grained object detection in images, and estimating boundary flow (i.e., motions of object boundaries) in videos. Since accounting for QMCs, in general, increases computational complexity, as a tradeoff for improving accuracy of recognition, the project has also studied improving performance under a limited time budget. The project has produced: over 20 publications presented in high-impact journals and at top vision conferences, open- source software, and served as a research material for four successfully defended PhD Theses, a number of MS theses. It has also provided training of seven undergraduate REU students (among those two minority) at Oregon State University and Temple University.       Last Modified: 09/06/2019       Submitted by: Longin Jan Latecki]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
