<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Exploring the Role of Coarray Fortran for Highly Parallel Structured Adaptive Mesh Refinement Calculations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>306776.00</AwardTotalIntnAmount>
<AwardAmount>306776</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Parallel Adaptive Mesh Refinement (AMR) calculations carried out on structured meshes play an exceedingly important role in several areas of science and engineering. Software and hardware infrastructure, and the computational capabilities they enable, go hand in hand. Innovations in software infrastructure simplify once-difficult computational problems. The first goal of this proposal is to capitalize on one such recent innovation (Coarray Fortran, henceforth CAF) in facilitating seamless implementation of parallel AMR. The project further wishes to make CAF-based AMR ready for PetaScale computations on NSF?s high-end supercomputers. Its second goal is to develop a highly parallel CAF-based AMR framework for solving elliptic, parabolic and hyperbolic PDEs. The result of this exploratory proposal will be a small, capable and light-weight framework for PetaScale class AMR applications.&lt;br/&gt;&lt;br/&gt;The lead PI is writing a textbook on "Numerical PDEs for Scientists and Engineers" and several of the applications from the text will be made available in this AMR framework. Along with the text, he has developed a website that freely disseminates polished lectures on PDEs to anyone over the web --  http://www.nd.edu/~dbalsara/Numerical-PDE-Course . The Co-PI is working with the original inventor of CAF to write a book on "Parallel Programming in Fortran with Coarrays". Both the textbook-writing efforts, as well as the associated website development, will greatly enhance the educational resources available to advanced undergraduates and graduates in STEM disciplines.</AbstractNarration>
<MinAmdLetterDate>08/05/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1307369</AwardID>
<Investigator>
<FirstName>Dinshaw</FirstName>
<LastName>Balsara</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dinshaw S Balsara</PI_FULL_NAME>
<EmailAddress>dbalsara@nd.edu</EmailAddress>
<PI_PHON>5746319639</PI_PHON>
<NSF_ID>000304329</NSF_ID>
<StartDate>08/05/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Notre Dame</Name>
<CityName>NOTRE DAME</CityName>
<ZipCode>465565708</ZipCode>
<PhoneNumber>5746317432</PhoneNumber>
<StreetAddress>940 Grace Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>824910376</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NOTRE DAME DU LAC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>048994727</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Notre Dame]]></Name>
<CityName>Notre Dame</CityName>
<StateCode>IN</StateCode>
<ZipCode>465565612</ZipCode>
<StreetAddress><![CDATA[940 Grace Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1217</Code>
<Text>EXTRAGALACTIC ASTRON &amp; COSMOLO</Text>
</ProgramElement>
<ProgramElement>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramElement>
<ProgramReference>
<Code>1206</Code>
<Text>THEORETICAL &amp; COMPUTATIONAL ASTROPHYSICS</Text>
</ProgramReference>
<ProgramReference>
<Code>1217</Code>
<Text>EXTRAGALACTIC ASTRON &amp; COSMOLO</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~306776</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The President has identified several "moonshot" projects. These are projects where the US should fast-track its science and technology development so that it either remains ahead of other countries or does not get outstripped by them. High performance computing is one of those strategic directions identified by the President. This project is aligned with that crucial goal.</p> <p>Computing on the largest supercomputers in the world is a daunting exercise. This is because one has to solve several difficult computational problems in order to extract peak performance from a PetaScale supercomputer. The US has only a handful of machines that can sustain several PetaFlops of performance. The methods for achieving high performance are difficult and have usually been within the purview of the specialist. To give but one example, the recent Pessage Passing Interface -3 specification that came out two years ago is a massive document that spans over 800 pages. It is unlikely that myriad scientists will learn all the details and incorporate the novel features of MPI-3 in their code. This is where our project comes in. The Co-Array Fortran (CAF) standard is much smaller, compact and easy to digest. CAF has also been incorporated into public domain compilers, like the GNU compiler that is freely available to anyone. The goal of the present project was to demonstrate that by using CAF we can simultaneously achieve the often-conflicting goals of extreme high performance along with ease of use.</p> <p>To demonstrate this fact to the community via this exploratory project we wrote two entire codes. One was an AMR code written in CAF for a range of applications areas. The other was a completely functionally equivalent code written in MPI-3 which used the best one-sided communication strategies available in the MPI-3 standard. We were able to show via a set of papers that the CAF-based codes often outperformed the much more turgid MPI-3 codes. Furthermore, the CAF-based codes are simpler to understand, more intuitive and easier to maintain. It also turned out that in order to get the full advantage of MPI-3 we found that the MPI code had to be structured in a fashion that was very close to the CAF code! This increases the motivation to adopt CAF and brings PetaScale computing closer to the masses.</p> <p>As part of the broader impact, it has spurred some improvements in the GNU compilers, which because of their public-domain nature, should reach the entire community. We have also developed websites that teach high performance computing via CAF as well as methods from computational sciences to the entire science and technology community. Our educational websites get over 50,000 hits per year which means that we are helping a lot of people worldwide. The PI has been the lead speaker at two recent summer schools, one in France and another in Russia. A broad clientele of students attended the summer school including several students from the US. The project has, therefore, contributed to the freely-available educational resources that are available to the international research community.</p><br> <p>            Last Modified: 08/01/2016<br>      Modified by: Dinshaw&nbsp;S&nbsp;Balsara</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The President has identified several "moonshot" projects. These are projects where the US should fast-track its science and technology development so that it either remains ahead of other countries or does not get outstripped by them. High performance computing is one of those strategic directions identified by the President. This project is aligned with that crucial goal.  Computing on the largest supercomputers in the world is a daunting exercise. This is because one has to solve several difficult computational problems in order to extract peak performance from a PetaScale supercomputer. The US has only a handful of machines that can sustain several PetaFlops of performance. The methods for achieving high performance are difficult and have usually been within the purview of the specialist. To give but one example, the recent Pessage Passing Interface -3 specification that came out two years ago is a massive document that spans over 800 pages. It is unlikely that myriad scientists will learn all the details and incorporate the novel features of MPI-3 in their code. This is where our project comes in. The Co-Array Fortran (CAF) standard is much smaller, compact and easy to digest. CAF has also been incorporated into public domain compilers, like the GNU compiler that is freely available to anyone. The goal of the present project was to demonstrate that by using CAF we can simultaneously achieve the often-conflicting goals of extreme high performance along with ease of use.  To demonstrate this fact to the community via this exploratory project we wrote two entire codes. One was an AMR code written in CAF for a range of applications areas. The other was a completely functionally equivalent code written in MPI-3 which used the best one-sided communication strategies available in the MPI-3 standard. We were able to show via a set of papers that the CAF-based codes often outperformed the much more turgid MPI-3 codes. Furthermore, the CAF-based codes are simpler to understand, more intuitive and easier to maintain. It also turned out that in order to get the full advantage of MPI-3 we found that the MPI code had to be structured in a fashion that was very close to the CAF code! This increases the motivation to adopt CAF and brings PetaScale computing closer to the masses.  As part of the broader impact, it has spurred some improvements in the GNU compilers, which because of their public-domain nature, should reach the entire community. We have also developed websites that teach high performance computing via CAF as well as methods from computational sciences to the entire science and technology community. Our educational websites get over 50,000 hits per year which means that we are helping a lot of people worldwide. The PI has been the lead speaker at two recent summer schools, one in France and another in Russia. A broad clientele of students attended the summer school including several students from the US. The project has, therefore, contributed to the freely-available educational resources that are available to the international research community.       Last Modified: 08/01/2016       Submitted by: Dinshaw S Balsara]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
