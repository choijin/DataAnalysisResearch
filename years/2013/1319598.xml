<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Head Activated Technology for Off-the-Shelf Mobile Devices</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>496020.00</AwardTotalIntnAmount>
<AwardAmount>496020</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Mobile computing devices offer increasingly rich human-computer interaction opportunities through the use of new sensor technology such as multi-touch surfaces, microphones, and cameras.  These advances provide users with richer sets of interaction modalities and motivate new and novel human-centric interfaces.  Among the new input modalities, touch-based interaction is the most advanced and widely deployed, while speech is gaining increased attention.  However, even with recent breakthroughs in computer vision and depth and motion sensing technology, the imaging modality remains the least developed for mobile devices.  And yet image-based sensing (a) would permit richer interaction opportunities for a general user population and (b) holds special promise for individuals with disabilities who find touch-based interfaces very challenging to use.&lt;br/&gt; &lt;br/&gt;This project focuses on the development of Head-Activated Technology for off-the- shelf Mobile Devices (HAT-MD).  HAT-MD systems will employ the next-generation imaging technology that will be included in a wide array of mobile devices.  Advanced algorithms will detect and track user head and facial features, and map specific movements to specific interface controls and application actions.  HAT-MD is initially targeted towards individuals with physical disabilities, and will both (a) extend to mobile platforms the head control interaction techniques that are often employed on desktop computers and (b) broaden the set of head and face movements that can be employed.&lt;br/&gt;&lt;br/&gt;Project deliverables include:  (a) HAT-MD Algorithms:  Next-generation imaging sensors will be used for detection, tracking, and 3D reconstruction.  Head position and facial features will be detected, tracked, and mapped to interface controls.  Computation will be distributed between the mobile devices and cloud computing services.  (b) HAT-MD Applications:  Head activated interfaces will be developed to address the challenges that individuals with physical disabilities have when using contemporary mobile platforms.  (c) HAT-MD Evaluations:  Rigorous human subject studies will evaluate the effectiveness and ease-of-use of the general HAT-MD methodology as well as specific interface and applications that are developed.  Algorithms will be benchmarked to current state-of-the-art detection, tracking, and reconstruction methods.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The increased accessibility provided by the HAT-MD project will create new opportunities for people to interact with mobile devices, especially for individuals with physical disabilities who currently have limited independent control of such devices.  The project will also feature direct involvement by individuals with disabilities at the research, development, and evaluation phases in order to focus the intellectual development in a truly useful direction that will complement the broader impact.</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319598</AwardID>
<Investigator>
<FirstName>Kenneth</FirstName>
<LastName>Barner</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kenneth Barner</PI_FULL_NAME>
<EmailAddress>barner@eecis.udel.edu</EmailAddress>
<PI_PHON>3028316937</PI_PHON>
<NSF_ID>000094999</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jingyi</FirstName>
<LastName>Yu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jingyi Yu</PI_FULL_NAME>
<EmailAddress>yu@cis.udel.edu</EmailAddress>
<PI_PHON>3028310345</PI_PHON>
<NSF_ID>000487324</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Delaware</Name>
<CityName>Newark</CityName>
<ZipCode>197160099</ZipCode>
<PhoneNumber>3028312136</PhoneNumber>
<StreetAddress>210 Hullihen Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<StateCode>DE</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DE00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>059007500</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF DELAWARE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>059007500</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Delaware]]></Name>
<CityName/>
<StateCode>DE</StateCode>
<ZipCode>197162553</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DE00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~496020</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>The Head Activated Technology for off&ndash;the&ndash;shelf Mobile Devices (HAT&ndash;MD) employs next&ndash;generation stereo imaging anticipated to be included in a wide array of mobile devices. Advanced algorithms detect and track user head and facial features, mapping specific movements to mobile device interface controls and application-specific actions.</p> <p>HAT&ndash;MD is initially targeted towards individuals with physical disabilities, extending the head&ndash;control methodology often employed on desktop computers to mobile platforms and broadening the set of head and face movements that can be employed. Additional modules including saliency detection, dynamic refocusing, gaze tracking, etc. are implemented based on the platform.</p> <p>More specifically, project deliverables include: HAT&ndash;MD Algorithms, HAT&ndash;MD Applications and HAT&ndash;MD Educations. The project&rsquo;s <strong>intellectual merit </strong>focuses on development and implementation of fundamental HAT-MD Algorithms (stereo detection and tracking; stereo 3D reconstruction, pose, gaze and mouth shape estimation, stereo assisted face recognition) in the mobile&ndash;cloud settings. We present a touchless, head activated virtual keyboard (HACK-Board) that can be utilizes face detection as interface for assistive typing for handicapped individuals. The HACK-Board is considered a Natural User Interface (NUI) for it requires the natural appendages of the user as well as the implementation of targeted HAT-MD applications. The greater accessibility provided by the HAT-MD project yield significant <strong>broader impact</strong>, particularly for individuals with physical disabilities who currently have limited independent control of mobile devices. The project also features direct involvement by individuals with disabilities at the research, development, and evaluation phases in order to focus the intellectual development in a truly useful direction and complement the broader impact. The results from this project (algorithms, apps, and benchmarks) are made widely available to students.</p> <p>Significant Results are listed as follows:</p> <p>1) A new depth-guided refocus synthesis technique particularly tailored for mobile devices is developed. Our technique takes coarse depth maps as inputs and applies novel depth-aware pseudo ray tracing. Comprehensive experiments show that our approach can produce very high quality DoF comparable to the ones produced by DSLR and the Lytro light field cameras.</p> <p>2) A new saliency detection algorithm tailored for light fields is developed. Experiments show that our saliency detection scheme can robustly handle challenging scenarios such as similar foreground and background, cluttered background, complex occlusions, etc., and achieve high accuracy and robustness.</p> <p>3) We have shown that heterogeneity in saliency is common and critical for reliable saliency pre- diction. Our study also produces the first database of personalized saliency maps (PSMs). We model PSM based on universal saliency map (USM) shared by different participants and adopt a multi- task CNN framework to estimate the discrepancy between PSM and USM. Comprehensive experiments demonstrate that our new PSM model and prediction scheme are effective and reliable. We are preparing a journal paper based on the results.</p> <p>4) A robust framework for detecting siblings from a pair of images, based upon how closely one image&rsquo;s feature set matches that of another is proposed.</p> <p>5) A label consistent recursive least squares dictionary learning algorithm, LC-RLSDLA, is proposed to learn discriminative dictionaries for image classification based on sparse coding. The class label information and a label consistency term are used in the cost function to enforce discriminability among the sparse codes. Experiments performed on two face recognition databases demonstrate that the proposed method outperforms state-of-the-art sparse coding algorithms.</p> <p>6) An efficient transfer learning-based smile detection approach to leverage the large amount of labeled data from face recognition datasets and to alleviate overfitting on smile detection is proposed. A well-trained deep face recognition model is explored and fine-tuned for smile detection in the wild, unlike previous works which use either hand-engineered features or train deep convolutional networks from scratch. Experiments represented in a conference paper in the 13th IEEE international conference on automatic face and Gesture recognition show that the proposed approach achieves improved state-of-the-art performance</p> <p>7) Hybrid networks that incorporate global scene features, skeleton features of the group, visual attentions, and local facial features are developed to recognition group-level emotions in the wild. Specifically, deep convolutional neural networks (CNNs) are first trained on the faces of the group, the whole images, the skeletons of the group and visual attentions, and then fused to perform the group-level emotion prediction. The efficiency and performance of the algorithms scored the second place in the 5th Emotion Recognition in the Wild (EmotiW 2017) Challenge, and the first place in the 6th Emotion Recognition in the Wild (EmotiW 2018) Challenge, both in the category of Group-Level Emotion Recognition sub-challenge.</p> <p>8) Second place in the 5th Emotion Recognition in the Wild (EmotiW 2017) Challenge, in the category of Group-Level Emotion Recognition sub-challenge.</p> <p>9) First place in the 6th Emotion Recognition in the Wild (EmotiW 2018) Challenge, in the category of Group-Level Emotion Recognition sub-challenge (to be released in ICMI 2018).</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/23/2019<br>      Modified by: Kenneth&nbsp;Barner</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    The Head Activated Technology for off&ndash;the&ndash;shelf Mobile Devices (HAT&ndash;MD) employs next&ndash;generation stereo imaging anticipated to be included in a wide array of mobile devices. Advanced algorithms detect and track user head and facial features, mapping specific movements to mobile device interface controls and application-specific actions.  HAT&ndash;MD is initially targeted towards individuals with physical disabilities, extending the head&ndash;control methodology often employed on desktop computers to mobile platforms and broadening the set of head and face movements that can be employed. Additional modules including saliency detection, dynamic refocusing, gaze tracking, etc. are implemented based on the platform.  More specifically, project deliverables include: HAT&ndash;MD Algorithms, HAT&ndash;MD Applications and HAT&ndash;MD Educations. The project?s intellectual merit focuses on development and implementation of fundamental HAT-MD Algorithms (stereo detection and tracking; stereo 3D reconstruction, pose, gaze and mouth shape estimation, stereo assisted face recognition) in the mobile&ndash;cloud settings. We present a touchless, head activated virtual keyboard (HACK-Board) that can be utilizes face detection as interface for assistive typing for handicapped individuals. The HACK-Board is considered a Natural User Interface (NUI) for it requires the natural appendages of the user as well as the implementation of targeted HAT-MD applications. The greater accessibility provided by the HAT-MD project yield significant broader impact, particularly for individuals with physical disabilities who currently have limited independent control of mobile devices. The project also features direct involvement by individuals with disabilities at the research, development, and evaluation phases in order to focus the intellectual development in a truly useful direction and complement the broader impact. The results from this project (algorithms, apps, and benchmarks) are made widely available to students.  Significant Results are listed as follows:  1) A new depth-guided refocus synthesis technique particularly tailored for mobile devices is developed. Our technique takes coarse depth maps as inputs and applies novel depth-aware pseudo ray tracing. Comprehensive experiments show that our approach can produce very high quality DoF comparable to the ones produced by DSLR and the Lytro light field cameras.  2) A new saliency detection algorithm tailored for light fields is developed. Experiments show that our saliency detection scheme can robustly handle challenging scenarios such as similar foreground and background, cluttered background, complex occlusions, etc., and achieve high accuracy and robustness.  3) We have shown that heterogeneity in saliency is common and critical for reliable saliency pre- diction. Our study also produces the first database of personalized saliency maps (PSMs). We model PSM based on universal saliency map (USM) shared by different participants and adopt a multi- task CNN framework to estimate the discrepancy between PSM and USM. Comprehensive experiments demonstrate that our new PSM model and prediction scheme are effective and reliable. We are preparing a journal paper based on the results.  4) A robust framework for detecting siblings from a pair of images, based upon how closely one image?s feature set matches that of another is proposed.  5) A label consistent recursive least squares dictionary learning algorithm, LC-RLSDLA, is proposed to learn discriminative dictionaries for image classification based on sparse coding. The class label information and a label consistency term are used in the cost function to enforce discriminability among the sparse codes. Experiments performed on two face recognition databases demonstrate that the proposed method outperforms state-of-the-art sparse coding algorithms.  6) An efficient transfer learning-based smile detection approach to leverage the large amount of labeled data from face recognition datasets and to alleviate overfitting on smile detection is proposed. A well-trained deep face recognition model is explored and fine-tuned for smile detection in the wild, unlike previous works which use either hand-engineered features or train deep convolutional networks from scratch. Experiments represented in a conference paper in the 13th IEEE international conference on automatic face and Gesture recognition show that the proposed approach achieves improved state-of-the-art performance  7) Hybrid networks that incorporate global scene features, skeleton features of the group, visual attentions, and local facial features are developed to recognition group-level emotions in the wild. Specifically, deep convolutional neural networks (CNNs) are first trained on the faces of the group, the whole images, the skeletons of the group and visual attentions, and then fused to perform the group-level emotion prediction. The efficiency and performance of the algorithms scored the second place in the 5th Emotion Recognition in the Wild (EmotiW 2017) Challenge, and the first place in the 6th Emotion Recognition in the Wild (EmotiW 2018) Challenge, both in the category of Group-Level Emotion Recognition sub-challenge.  8) Second place in the 5th Emotion Recognition in the Wild (EmotiW 2017) Challenge, in the category of Group-Level Emotion Recognition sub-challenge.  9) First place in the 6th Emotion Recognition in the Wild (EmotiW 2018) Challenge, in the category of Group-Level Emotion Recognition sub-challenge (to be released in ICMI 2018).          Last Modified: 07/23/2019       Submitted by: Kenneth Barner]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
