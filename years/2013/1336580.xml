<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS:CLCCA:LigHTS: Lagging-Hardware Tolerant Systems" in the system.</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>749854.00</AwardTotalIntnAmount>
<AwardAmount>749854</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>tao li</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>With the advent of scalable parallel computing, thousands of devices are connected and managed collectively.  This era is confronted with a new challenge: performance failure; systems often perform worse than expected due to large-scale management issues such as hardware failures, software bugs, and configuration mistakes.  This project targets one overlooked cause of performance failure: "lagging hardware" -- hardware whose performance degrades significantly compared to its specification.  Many reports indicate that a single lagging hardware can easily cascade and make the performance of a whole cluster collapse.  Here, parallelism is unexploited, productivity is reduced, the system is underutilized, and energy is wasted. The goal of the LigHTS project is to transform computing systems into Lagging-Hardware Tolerant Systems.  The LigHTS project will bring many direct benefits to the society; users from many areas (science, healthcare, business, education, military, and government) increasingly use large-scale storage and computation services.  Here, predictable performance is a key to success, and in this context lagging-hardware tolerant computing is a critical ingredient. &lt;br/&gt;&lt;br/&gt;The LigHTS project consists of three major objectives.  The first is lagging-hardware data analysis and instrumentation. To improve the robustness of future parallel systems, it is crucial to study lagging characteristics exhibited by modern hardware and to devise new instrumentation methodologies that can collect cases of lagging hardware in deployment.  The second is lagging-failure system analysis.  It is important to rigorously analyze the impact of lagging hardware (including disk, network, processor) to currently deployed systems. The results will unearth design flaws and provide valuable reevaluations of how deployed systems should evolve.  The last is LigHTS principles, design, and implementation.  There is a need to establish foundational principles of lagging-hardware tolerant computing and apply the principles in building prototypes of cross-layer LigHTS systems spanning distributed storage, computing framework, operating and runtime systems.</AbstractNarration>
<MinAmdLetterDate>09/08/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/14/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1336580</AwardID>
<Investigator>
<FirstName>Haryadi</FirstName>
<LastName>Gunawi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Haryadi Gunawi</PI_FULL_NAME>
<EmailAddress>haryadi@cs.uchicago.edu</EmailAddress>
<PI_PHON>7737025772</PI_PHON>
<NSF_ID>000626546</NSF_ID>
<StartDate>09/08/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Chien</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew A Chien</PI_FULL_NAME>
<EmailAddress>achien@cs.uchicago.edu</EmailAddress>
<PI_PHON>7738340117</PI_PHON>
<NSF_ID>000295194</NSF_ID>
<StartDate>09/08/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Ross</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Ross</PI_FULL_NAME>
<EmailAddress>rross@mcs.anl.gov</EmailAddress>
<PI_PHON>6305254588</PI_PHON>
<NSF_ID>000259978</NSF_ID>
<StartDate>09/08/2013</StartDate>
<EndDate>07/14/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dries</FirstName>
<LastName>Kimpe</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dries Kimpe</PI_FULL_NAME>
<EmailAddress>dries@northwestern.edu</EmailAddress>
<PI_PHON>8474913003</PI_PHON>
<NSF_ID>000635607</NSF_ID>
<StartDate>09/08/2013</StartDate>
<EndDate>07/07/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606375418</ZipCode>
<StreetAddress><![CDATA[1100 E 58th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~749854</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><br />The LIGHTS (Lagging-Hardware Tolerant Systems) project has advanced storage and distributed systems to analyze, anticipate, and tolerate lagging hardware devices.&nbsp; The LIGHTS projects produces several outcomes given the execution of the sub-projects below. <br /><br />With the Limpware project [HotCloud '13], we highlight the problem of lagging hardware, providing anecdotes of lagging hardware in production systems, including lagging CPUs, network hardware, memory, and storage devices. <br /><br />With the Limplock project [SoCC '13], we unearth the fact that many modern distributed systems such as Hadoop, HDFS, ZooKeeper, Cassandra, and HBase are not fully equipped in dealing with lagging hardware. <br /><br />With the CBS project [SoCC '14], we study thousands of bugs in six popular cloud-scale distributed systems and found several cases of software bugs that incorrectly handled lagging hardware. <br /><br />With the SPV project [HotCloud '15], we show that lagging-hardware related performance bugs can be detected prior to deployment using performance model checking tool such as Colored Petri Nets. <br /><br />With the PBSE project [SoCC '17], we enhance the basic speculative execution technique in many data-parallel frameworks such as Hadoop and Spark.&nbsp; We introduce Path-Based Speculative Execution that can robustly detect and failover from degraded NICs. <br /><br />With the Tail at Store project [FAST '16], we study millions of hours of SSD and disk performance and show that disks and SSDs exhibit a lagging behavior compared to their other peers within the RAID systems. <br /><br />With the Tiny-Tail Flash project [FAST '17 and TOS '17], we achieve a near-perfect elimination of garbage collection tail latencies in NAND SSDs. <br /><br />With the tail-tolerant flash array project, we adopt techniques in the tiny-tail flash project into the context of flash array.&nbsp; The tail-tolerant flash array introduces a simple interface in which commodity SSDs can collaborate with the RAID layer to cut garbage collection tail latencies. <br /><br />Finally, with the fail-slow at scale project, we show 100 real cases of lagging hardware occurred in large-scale production systems including their detailed characteristics and chains of events. <br /><br /><br />Broader Impact: The LIGHT project places significant value on technology transfer; the outcomes of the project have led to direct industrial impact.&nbsp; For example, approaches from PBSE and Tiny-Tail Flash projects have been adopted by several industries that deploy data-parallel frameworks and SSD managements.&nbsp; In addition, Predictable performance is a key to success of multi-billion dollar computing, and we believe LIGHTS projects and the approaches introduced will be an important ingredient. Users from many areas (science, healthcare, business, education, military, and government) are increasingly use large-scale storage and computing services, and the outcomes of our project will improve the reliability and availability of these services. <br /><br /></p><br> <p>            Last Modified: 11/10/2017<br>      Modified by: Haryadi&nbsp;Gunawi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The LIGHTS (Lagging-Hardware Tolerant Systems) project has advanced storage and distributed systems to analyze, anticipate, and tolerate lagging hardware devices.  The LIGHTS projects produces several outcomes given the execution of the sub-projects below.   With the Limpware project [HotCloud '13], we highlight the problem of lagging hardware, providing anecdotes of lagging hardware in production systems, including lagging CPUs, network hardware, memory, and storage devices.   With the Limplock project [SoCC '13], we unearth the fact that many modern distributed systems such as Hadoop, HDFS, ZooKeeper, Cassandra, and HBase are not fully equipped in dealing with lagging hardware.   With the CBS project [SoCC '14], we study thousands of bugs in six popular cloud-scale distributed systems and found several cases of software bugs that incorrectly handled lagging hardware.   With the SPV project [HotCloud '15], we show that lagging-hardware related performance bugs can be detected prior to deployment using performance model checking tool such as Colored Petri Nets.   With the PBSE project [SoCC '17], we enhance the basic speculative execution technique in many data-parallel frameworks such as Hadoop and Spark.  We introduce Path-Based Speculative Execution that can robustly detect and failover from degraded NICs.   With the Tail at Store project [FAST '16], we study millions of hours of SSD and disk performance and show that disks and SSDs exhibit a lagging behavior compared to their other peers within the RAID systems.   With the Tiny-Tail Flash project [FAST '17 and TOS '17], we achieve a near-perfect elimination of garbage collection tail latencies in NAND SSDs.   With the tail-tolerant flash array project, we adopt techniques in the tiny-tail flash project into the context of flash array.  The tail-tolerant flash array introduces a simple interface in which commodity SSDs can collaborate with the RAID layer to cut garbage collection tail latencies.   Finally, with the fail-slow at scale project, we show 100 real cases of lagging hardware occurred in large-scale production systems including their detailed characteristics and chains of events.    Broader Impact: The LIGHT project places significant value on technology transfer; the outcomes of the project have led to direct industrial impact.  For example, approaches from PBSE and Tiny-Tail Flash projects have been adopted by several industries that deploy data-parallel frameworks and SSD managements.  In addition, Predictable performance is a key to success of multi-billion dollar computing, and we believe LIGHTS projects and the approaches introduced will be an important ingredient. Users from many areas (science, healthcare, business, education, military, and government) are increasingly use large-scale storage and computing services, and the outcomes of our project will improve the reliability and availability of these services.          Last Modified: 11/10/2017       Submitted by: Haryadi Gunawi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
