<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Sparse Modeling and Estimation with High-dimensional Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>217757.00</AwardTotalIntnAmount>
<AwardAmount>217757</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With the recent advances in science and technology, high dimensional data are becoming a commonplace in diverse fields. The goal of this proposed research is to develop methods and theory for several basic classes of statistical problems associated with this type of data. Among the central questions are the nature of sparsity in different contexts, and how it determines our ability or inability to deal with high dimensional data. The investigator studies a reproducing kernel Hilbert space based framework to exploit sparsity for general predictive problems. The framework underpins the connections among various popular methods that encourage sparsity, and provides an opportunity to study them in a unified fashion, which in turn will foster the development of improved methods and algorithms. The investigator will also consider the problem of covariance matrix estimation and selection. The research concentrates on understanding the nature of and connection among various notions of sparsity for large covariance matrix, and their relationship with Gaussian graphical models.&lt;br/&gt;&lt;br/&gt;From the world's most powerful telescopes to the finest atomic force microscopes, from the flourishing financial market to the fast-growing World-Wide Web, high dimensional and massive data are being produced at an astonishing rate. Immediate access to copious amount of interesting and important information presents unprecedented opportunities, but also creates unique challenges, to mathematicians in general and statisticians in particular. Development of statistical theory to understand the nature of their fundamental characteristics, and methodology to address the associated issues, including those discussed in this proposal, will advance our intellectual exploration and knowledge, and undoubtedly benefit a multitude of scientific and technological fields -- genomics, medical imaging, communication networks, and finance are just a few well known examples.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>05/17/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/04/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1321692</AwardID>
<Investigator>
<FirstName>Ming</FirstName>
<LastName>Yuan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ming Yuan</PI_FULL_NAME>
<EmailAddress>ming.yuan@columbia.edu</EmailAddress>
<PI_PHON>2128512143</PI_PHON>
<NSF_ID>000061701</NSF_ID>
<StartDate>05/17/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Morgridge Institute for Research, Inc.</Name>
<CityName>Madison</CityName>
<ZipCode>537151542</ZipCode>
<PhoneNumber>6083164335</PhoneNumber>
<StreetAddress>330 North Orchard Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>012420082</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MORGRIDGE INSTITUTE FOR RESEARCH, INC., THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Morgridge Institute for Research]]></Name>
<CityName/>
<StateCode>WI</StateCode>
<ZipCode>537151542</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~44829</FUND_OBLG>
<FUND_OBLG>2012~84871</FUND_OBLG>
<FUND_OBLG>2013~88057</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>With the recent advances in science and technology, large datasets are becoming ubiquitous. How to effectively extract information for such data is the ultimate goal of the proposed project. Towards this goal, the PI has enagaged in the following research activities:</p> <div class="page" title="Page 1"> <div class="layoutArea"> <div class="column"> <p><span>Structured Sparsity in Regression. </span><span>In many high dimensional regression problems, the regressors are inherently related, for example, as groups or as main and interaction ef- fects. Appropriately accounting for such structures could lead to improved prediction and enhanced interpretability. The PI, along with his collaborators, has developed and studied a number of popular sparse regularization techniques for such purposes. </span></p> <p><span>High Dimensional Kernel Methods. </span><span>Reproducing kernel Hilbert space based methods provide a unified yet flexible framework for a large number of machine learning and statistical tasks. Challenges of high dimensionality in kernel methods have mostly been treated on a case-by-case basis. The PI, along with his co-authors, has recently introduced a general framework of sparse regularization for kernel methods and studies its pros and cons in several general settings such as functional ANOVA, multiple kernel learning and support vector machines. </span></p> <p><span>Large Covariance Matrix Estimation. </span><span>Covariance matrix estimation plays a central role in multivariate data analysis. The problem of estimating a large covariance matrix has therefore attracted considerable amount of interests in recent years. The PI, along with his collaborators, has developed several popular methods for such purposes and stud- ied the theoretical properties of these methods along with the fundamental limitation in estimating large covariance matrices. </span></p> <p><span>Functional Data Analysis. </span><span>The PI, together with collaborators, has studied problems for functional data analysis through a framework of reproducing kernel Hilbert space. The results we obtained reveal the fundamental properties of estimating the mean function, covariance function, Principal components, as well as the slope function in functional linear regression. </span></p> <p><span>Applications and Broad Impact. </span><span>The aforementioned methodological and theoretical research has also resulted in better understanding of problems from biology and medicine, finance, and engineering.&nbsp;</span></p> </div> </div> </div> <p>In addition, the PI has developed multiple graduate courses to disseminate these findings along with other recent development, and trained four PhD students to work on problems related to the current project.</p><br> <p>            Last Modified: 09/15/2015<br>      Modified by: Ming&nbsp;Yuan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ With the recent advances in science and technology, large datasets are becoming ubiquitous. How to effectively extract information for such data is the ultimate goal of the proposed project. Towards this goal, the PI has enagaged in the following research activities:     Structured Sparsity in Regression. In many high dimensional regression problems, the regressors are inherently related, for example, as groups or as main and interaction ef- fects. Appropriately accounting for such structures could lead to improved prediction and enhanced interpretability. The PI, along with his collaborators, has developed and studied a number of popular sparse regularization techniques for such purposes.   High Dimensional Kernel Methods. Reproducing kernel Hilbert space based methods provide a unified yet flexible framework for a large number of machine learning and statistical tasks. Challenges of high dimensionality in kernel methods have mostly been treated on a case-by-case basis. The PI, along with his co-authors, has recently introduced a general framework of sparse regularization for kernel methods and studies its pros and cons in several general settings such as functional ANOVA, multiple kernel learning and support vector machines.   Large Covariance Matrix Estimation. Covariance matrix estimation plays a central role in multivariate data analysis. The problem of estimating a large covariance matrix has therefore attracted considerable amount of interests in recent years. The PI, along with his collaborators, has developed several popular methods for such purposes and stud- ied the theoretical properties of these methods along with the fundamental limitation in estimating large covariance matrices.   Functional Data Analysis. The PI, together with collaborators, has studied problems for functional data analysis through a framework of reproducing kernel Hilbert space. The results we obtained reveal the fundamental properties of estimating the mean function, covariance function, Principal components, as well as the slope function in functional linear regression.   Applications and Broad Impact. The aforementioned methodological and theoretical research has also resulted in better understanding of problems from biology and medicine, finance, and engineering.      In addition, the PI has developed multiple graduate courses to disseminate these findings along with other recent development, and trained four PhD students to work on problems related to the current project.       Last Modified: 09/15/2015       Submitted by: Ming Yuan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
