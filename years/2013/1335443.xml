<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS:CLCCA:Collaborative Research:Harnessing Highly Threaded Hardware for Server Workloads</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>349999.00</AwardTotalIntnAmount>
<AwardAmount>365999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Data centers provide the computational and storage infrastructure required to meet today's ever increasing demand for Internet-based services. Web servers deliver a vast range of information on demand, ranging from static content such as files, images, video and audio streaming services, to dynamic content created via scripting languages (e.g,. PHP) or stand alone C/C++ applications (e.g., search results).  Server performance, scaling and energy efficiency (throughput/Watt) are crucial factors in reducing total cost of ownership (TCO) in today's server-based industries.  Unfortunately, current system designs based on commodity multicore processors may not be the most power/energy efficient for all server workloads.  &lt;br/&gt;&lt;br/&gt;Today's massively parallel accelerators (e.g., GPUs) provide exceptional performance per Watt for certain workloads versus conventional many core CPUs. Unfortunately, these devices have not found wide-spread general purpose use outside the high-performance computing domain. This project expands the use of massively parallel accelerators to server and operating system-intensive workloads by innovating across the application, runtime, operating system, and architecture layers.&lt;br/&gt;&lt;br/&gt;This research builds on the observation that server workload execution patterns are not completely unique across multiple requests. The goal of this project is to develop computer systems (software and hardware) that exploit similarity across requests to improve server performance and power/energy efficiency by launching data parallel executions for request cohorts.  The three primary aspects of this research are 1) mapping traditional thread/task parallel workloads onto data parallel hardware, 2) developing a new accelerator-centric operating system architecture, and 3) developing new architectural mechanisms to support this new class of accelerator workloads and operating system software.</AbstractNarration>
<MinAmdLetterDate>09/11/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/17/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1335443</AwardID>
<Investigator>
<FirstName>Alvin</FirstName>
<LastName>Lebeck</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alvin R Lebeck</PI_FULL_NAME>
<EmailAddress>alvy@cs.duke.edu</EmailAddress>
<PI_PHON>9196606551</PI_PHON>
<NSF_ID>000335353</NSF_ID>
<StartDate>09/11/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1359</Code>
<Text>RES EXP FOR TEACHERS(RET)-SITE</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<ProgramReference>
<Code>1359</Code>
<Text>RET SITE-Res Exp for Tchr Site</Text>
</ProgramReference>
<ProgramReference>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~349999</FUND_OBLG>
<FUND_OBLG>2014~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-02780365-7fff-745e-cf90-33018437d9a2"> <p dir="ltr"><span>Today's massively parallel accelerators (e.g., GPUs) provide exceptional performance per Watt for certain workloads compared with conventional many core CPUs.&nbsp; Unfortunately, these devices have not found wide-spread general purpose use outside the high-performance computing domain.  This project has developed prototype systems to use parallel accelerators for server-type workloads by innovating across the application, runtime, operating system, and architecture layers.</span></p> <p dir="ltr"><span>GPUs lack software abstractions to direct the flow of data within a system or control the scheduling of tasks on the GPU, leaving the developer with only coarse-grain control of scheduling and low-level control over I/O Therefore, certain classes of applications that could benefit from GPU's computational density require unacceptable development costs to realize their full cost performance potential.</span></p> <p dir="ltr"><span>Major project accomplishments include the Rhythm system, the GPUnet framework, a novel similarity search technique for query processing on GPUs, and a multi-tenant GPU scheduling framework. Rhythm exploits similarity across requests to improve server performance and power/energy efficiency by enabling data parallel executions for server code. The GPUnet framework for developing GPU programs that directly control the network.</span></p> <p dir="ltr"><span>The intellectual merit of Rhythm arises from the observation that server workload execution patterns are not completely unique across multiple requests. Rhythm is designed for high throughput servers that exploit similarity across requests to improve server performance and power/energy efficiency by launching data parallel executions for request cohorts. An implementation of the SPECWeb Banking workload using Rhythm on NVIDIA GPUs provides a basis for evaluating both software and hardware for future cohort-based servers. Our evaluation of Rhythm on future server platforms shows that it achieves 4&times; the throughput (reqs/sec) of a core i7 at efficiencies (reqs/Joule) comparable to a dual core ARM Cortex A9. A Rhythm implementation that generates transposed responses achieves 8&times; the i7 throughput while processing 2.5&times; more requests/Joule compared to the A9.</span></p> <p dir="ltr"><span>The intellectual merit of GPUnet comes from defining a native GPU networking layer that provides a socket abstraction and high-level networking APIs to GPU programs. GPUnet enables individual threads in one GPU to communicate with threads in other GPUs or CPUs via standard and familiar socket interfaces, regardless of whether they are in the same or different machines. Native GPU networking cuts the CPU out of GPU-NIC interactions, simplifying code and increasing performance. It also unifies application compute and I/O logic within the GPU program, providing a simpler programming model. GPUnet uses advanced NIC and GPU hardware capabilities and applies sophisticated code optimizations that yield high application performance equal to or exceeding hand-tuned traditional implementations. We have quantitatively evaluated GPUnet in two major ways: by building an in-memory MapReduce framework and a face verification server. For word count and K-means MapReduce workloads our system scales to four GPUs providing speedups of 2.9-3.5x over a single GPU. The face verification application exceeds the throughput of a 6-core Intel CPU and a CUDA-based server by 1.5x and 2.3x respectively, while maintaining 3x lower latency than the CPU and requiring half as much code than the other versions.</span></p> <p dir="ltr"><span>The intellectual merit of our similarity search comes from optimized implementations for both the host processor and the accelerator, and enabling lower total cost of ownership in data centers for these workloads.&nbsp; Our optimizations utilize novel structures for a sparse matrix multiplication fused with a top-k search to eliminate unnecessary processing and data movement overhead.  Augmenting existing Xeon servers with GPUs running our implementation results in a 3X improvement in throughput per machine, resulting in a more than 2.5X reduction in cost of ownership, even for discounted Xeon servers. Replacing a Xeon based cluster with an accelerator based cluster for similarity search reduces the total cost of ownership by more than 6X to 16X while consuming significantly less power than an ARM based cluster.</span></p> <p dir="ltr"><span>The intellectual merit form our GPU multi-tenant scheduling arises from the requirement for an efficient and low-overhead method for sharing the device among multiple users that improves system throughput while adapting to workload change. We developed adaptive simultaneous multi-tenancy to control the resources allocated programs running on the GPU, and an efficient policy to make decisions about this allocation. Our system dynamically adjusts the program&rsquo;s parameters at run-time when a new task arrives or a running task ends. Evaluations using our prototype implementation show that, compared to sequential execution, system throughput is improved by an average of 9.8% (and up to 22.4%) for combinations of tasks that include at least one low-utilization task.</span></p> <p dir="ltr"><span>The broader impacts of the project come from professional development for students, including two doctoral theses, five masters theses, and three undergraduate projects. The ideas in the project have influenced corporate directions at NVIDIA.&nbsp; Finally, work from the project has been integrated into graduate and undergraduate computer systems curricula.</span></p> <div></div> </span></p><br> <p>            Last Modified: 10/09/2019<br>      Modified by: Alvin&nbsp;R&nbsp;Lebeck</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Today's massively parallel accelerators (e.g., GPUs) provide exceptional performance per Watt for certain workloads compared with conventional many core CPUs.  Unfortunately, these devices have not found wide-spread general purpose use outside the high-performance computing domain.  This project has developed prototype systems to use parallel accelerators for server-type workloads by innovating across the application, runtime, operating system, and architecture layers. GPUs lack software abstractions to direct the flow of data within a system or control the scheduling of tasks on the GPU, leaving the developer with only coarse-grain control of scheduling and low-level control over I/O Therefore, certain classes of applications that could benefit from GPU's computational density require unacceptable development costs to realize their full cost performance potential. Major project accomplishments include the Rhythm system, the GPUnet framework, a novel similarity search technique for query processing on GPUs, and a multi-tenant GPU scheduling framework. Rhythm exploits similarity across requests to improve server performance and power/energy efficiency by enabling data parallel executions for server code. The GPUnet framework for developing GPU programs that directly control the network. The intellectual merit of Rhythm arises from the observation that server workload execution patterns are not completely unique across multiple requests. Rhythm is designed for high throughput servers that exploit similarity across requests to improve server performance and power/energy efficiency by launching data parallel executions for request cohorts. An implementation of the SPECWeb Banking workload using Rhythm on NVIDIA GPUs provides a basis for evaluating both software and hardware for future cohort-based servers. Our evaluation of Rhythm on future server platforms shows that it achieves 4&times; the throughput (reqs/sec) of a core i7 at efficiencies (reqs/Joule) comparable to a dual core ARM Cortex A9. A Rhythm implementation that generates transposed responses achieves 8&times; the i7 throughput while processing 2.5&times; more requests/Joule compared to the A9. The intellectual merit of GPUnet comes from defining a native GPU networking layer that provides a socket abstraction and high-level networking APIs to GPU programs. GPUnet enables individual threads in one GPU to communicate with threads in other GPUs or CPUs via standard and familiar socket interfaces, regardless of whether they are in the same or different machines. Native GPU networking cuts the CPU out of GPU-NIC interactions, simplifying code and increasing performance. It also unifies application compute and I/O logic within the GPU program, providing a simpler programming model. GPUnet uses advanced NIC and GPU hardware capabilities and applies sophisticated code optimizations that yield high application performance equal to or exceeding hand-tuned traditional implementations. We have quantitatively evaluated GPUnet in two major ways: by building an in-memory MapReduce framework and a face verification server. For word count and K-means MapReduce workloads our system scales to four GPUs providing speedups of 2.9-3.5x over a single GPU. The face verification application exceeds the throughput of a 6-core Intel CPU and a CUDA-based server by 1.5x and 2.3x respectively, while maintaining 3x lower latency than the CPU and requiring half as much code than the other versions. The intellectual merit of our similarity search comes from optimized implementations for both the host processor and the accelerator, and enabling lower total cost of ownership in data centers for these workloads.  Our optimizations utilize novel structures for a sparse matrix multiplication fused with a top-k search to eliminate unnecessary processing and data movement overhead.  Augmenting existing Xeon servers with GPUs running our implementation results in a 3X improvement in throughput per machine, resulting in a more than 2.5X reduction in cost of ownership, even for discounted Xeon servers. Replacing a Xeon based cluster with an accelerator based cluster for similarity search reduces the total cost of ownership by more than 6X to 16X while consuming significantly less power than an ARM based cluster. The intellectual merit form our GPU multi-tenant scheduling arises from the requirement for an efficient and low-overhead method for sharing the device among multiple users that improves system throughput while adapting to workload change. We developed adaptive simultaneous multi-tenancy to control the resources allocated programs running on the GPU, and an efficient policy to make decisions about this allocation. Our system dynamically adjusts the program?s parameters at run-time when a new task arrives or a running task ends. Evaluations using our prototype implementation show that, compared to sequential execution, system throughput is improved by an average of 9.8% (and up to 22.4%) for combinations of tasks that include at least one low-utilization task. The broader impacts of the project come from professional development for students, including two doctoral theses, five masters theses, and three undergraduate projects. The ideas in the project have influenced corporate directions at NVIDIA.  Finally, work from the project has been integrated into graduate and undergraduate computer systems curricula.         Last Modified: 10/09/2019       Submitted by: Alvin R Lebeck]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
