<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EarthCube Building Blocks:  Earth System Bridge:  Spanning Scientific Communities with Interoperable Modeling Frameworks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>1699997.00</AwardTotalIntnAmount>
<AwardAmount>1699997</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>06010000</Code>
<Directorate>
<Abbreviation>GEO</Abbreviation>
<LongName>Directorate For Geosciences</LongName>
</Directorate>
<Division>
<Abbreviation>ICER</Abbreviation>
<LongName>ICER</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Eva Zanzerkia</SignBlockName>
<PO_EMAI>ezanzerk@nsf.gov</PO_EMAI>
<PO_PHON>7032924734</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This EarthCube project possesses deep, disciplinary and interdisciplinary expertise in the development, implementation and support of geoscientific modeling architectures and in the promotion and utilization of community standards in model development and in data management. Combined, this team will integrate existing model architectures, model coupling standards and data standards into a set of open source Earth System Bridge building blocks that will transform the process of Earth system model coupling and bridge the present technological gap. In collaboration with the community of framework developers, the team will create a Framework Definition Language (FDL) which characterizes Earth system coupling technologies in terms of their metadata usage, architecture, protocols for interaction, and implementation, and use this as a basis for understanding potential framework inter-connections. They will use the FDL to develop a set of ?bridges? that connect leading software frameworks from the federal modeling enterprise and from the academic geoscientific modeling enterprise, for the sake of creating seamless environmental and impacts prediction tools, and develop new services to improve the integration of inter-agency, four-dimensional databases with more heterogeneous academic databases for use in earth system models and by modeling groups. Demonstration of the  new modeling and data integration architecture will be in two case studies: the first, prediction of the local impacts of the Hurricane Sandy landfall; and the second, the integration of deep Earth process with surface dynamics. These problems span disciplines, agencies, and research and operational communities, and require addressing scientific, technical, and cultural issues.&lt;br/&gt;&lt;br/&gt;The overarching goal is to bridge this present technological gap in Earth System modeling, thereby illustrating how a common cyberinfrastructure can be used in different Earth science disciplines and communities, and how improved cyberinfrastructure can directly address pressing cross-disciplinary science questions.&lt;br/&gt;&lt;br/&gt;The tools developed by the Earth System Bridge have the potential to serve many different disciplinary communities. A more interconnected and capable modeling community will significantly increase the speed at which knowledge is currently transferred between the research and operational communities, Connecting the academic and operational infrastructure will reduce inefficiencies and gaps that exists in the system today. The technology developed will place increased capabilities in the hands of a broader set of geoscientists, lowering the barriers to more scientists participating in multi-disciplinary research that is needed to address today?s policy issues and increase our national resilience to a wide variety of natural hazards.</AbstractNarration>
<MinAmdLetterDate>08/26/2013</MinAmdLetterDate>
<MaxAmdLetterDate>03/09/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.050</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1343811</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Hooper</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard P Hooper</PI_FULL_NAME>
<EmailAddress>richard.hooper@tufts.edu</EmailAddress>
<PI_PHON>6176273211</PI_PHON>
<NSF_ID>000212728</NSF_ID>
<StartDate>03/09/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gary</FirstName>
<LastName>Egbert</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gary D Egbert</PI_FULL_NAME>
<EmailAddress>egbert@oce.orst.edu</EmailAddress>
<PI_PHON>5417372947</PI_PHON>
<NSF_ID>000415824</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate>02/06/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Scott</FirstName>
<LastName>Peckham</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Scott D Peckham</PI_FULL_NAME>
<EmailAddress>Scott.Peckham@colorado.edu</EmailAddress>
<PI_PHON>3034926752</PI_PHON>
<NSF_ID>000473356</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Cecelia</FirstName>
<LastName>Deluca</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Cecelia Deluca</PI_FULL_NAME>
<EmailAddress>cecelia.deluca@noaa.gov</EmailAddress>
<PI_PHON>3034973604</PI_PHON>
<NSF_ID>000152536</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Gochis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Gochis</PI_FULL_NAME>
<EmailAddress>gochis@ucar.edu</EmailAddress>
<PI_PHON>3034971000</PI_PHON>
<NSF_ID>000566098</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Arrigo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer Arrigo</PI_FULL_NAME>
<EmailAddress>jarrigo@cuahsi.org</EmailAddress>
<PI_PHON>2027777302</PI_PHON>
<NSF_ID>000587388</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate>03/09/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anna</FirstName>
<LastName>Kelbert</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anna Kelbert</PI_FULL_NAME>
<EmailAddress>akelbert@usgs.gov</EmailAddress>
<PI_PHON>3032738448</PI_PHON>
<NSF_ID>000670371</NSF_ID>
<StartDate>02/06/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803090572</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, room 497]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~1699997</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Observational data and predictive models (observations &amp; predictions, or experimentation &amp; theory) are the two pillars of science. &nbsp;Sophisticated instruments are used to measure values of scientific variables that are then stored in data sets.&nbsp; Physically-based, mathematical models summarize our current best knowledge of how physical systems operate, and are therefore used to build computational models that predict future system states from initial conditions.&nbsp; Data sets and computational models are therefore the two fundamental, digital resource types geoscientists use to study problems of societal interest, such as resource management, environmental hazards and climate change.&nbsp; While modern geoscientists have online access to an abundance of data sets and models, these resources differ from each other in myriad ways and this heterogeneity hinders interoperability. Interoperability is critically important because a single data set or model is insufficient to tackle a nontrivial geoscience problem. For example, models typically require several types of input data, obtained by reading measured data from files or as output from other models. Coupling many different heterogeneous resources into computational workflows is therefore essential, and geoscientists spend a large fraction of their time setting up and executing these workflows. An extensive, empirical analysis of scientific workflows (Garijo et al., 2013) found that geoscientists typically spend between 60% and 80% of their time dealing with these "data friction" issues, leaving the remaining time for the science.</p> <p>To address this problem, sophisticated software architectures known as "modeling frameworks" or "modeling systems" have been created to simplify the task of coupling heterogeneous digital resources into executable workflows.&nbsp; The coupled models typically run on supercomputers, exchanging values of variables as they run.&nbsp; Geoscience variables (e.g. sea surface temperature) typically vary over space (i.e. a geographic region spanned by a grid) and/or time (i.e. a time period).&nbsp; Modeling frameworks treat each digital resource as a reusable, plug-and-play component and achieve interoperability through a combination of (1) standardized component APIs, (2) standardized metadata for describing resources and (3) the use of mediators or brokers to transform data as needed before passing it to another resource. A variety of modeling frameworks have been developed both within academia and at federal agencies. &nbsp;Examples from the academic community include the NSF-funded CSDMS project (Community Surface Dynamics Modeling System), which serves the Earth surface process modeling community (Peckham et al., 2013), and Pyre, from the deep-Earth process community.&nbsp; Two examples from the federal or operational modeling community include:</p> <ul> <li>ESMF (Earth System Modeling Framework), used primarily by the atmosphere and ocean modeling community (Hill et al., 2004), and</li> </ul> <ul> <li>OMS (Object Modeling System), developed by the USDA (US Department of Agriculture) primarily for agricultural modeling (David et al., 2002)</li> </ul> <p>Despite many similarities, each framework is customized to the needs of the community it serves.&nbsp; This has led to a critical gap between the frameworks and coupling technologies used by mission-driven federal agencies, tasked with environmental forecasts, and those used by academic communities, who advance the science and technology that underpin the forecasts.&nbsp; This gap hinders the interchange between research and operations that is essential to improving predictions of environmental hazards as well as to mitigating the impacts of those hazards.</p> <p>The Earth System Bridge (ESB) project, funded by NSF's EarthCube initiative, brought together developers of several modeling frameworks to develop a theoretical basis and practical, streamlined approach to cross-framework interoperability.&nbsp; Specific technologies developed by this project include:</p> <ul> <li>Earth System Framework Description Language (ES-FDL) for fully describing the capabilities of a modeling framework.</li> </ul> <ul> <li>Extensions to the Geoscience Standard Names (GSN) ontology, which supports cross-framework, domain-agnostic semantic alignment.&nbsp; With community input, variable names from numerous models, existing vocabularies (e.g. CF Standard Names) and geoscience domains were mapped into the GSN.</li> </ul> <ul> <li>A Model Component Metadata schema for deep description of computational models.</li> </ul> <ul> <li>A set of adapters, so that any model that implements the framework-agnostic Basic Model Interface (BMI) can be deployed without change as a component in multiple modeling frameworks (e.g. ESMF/NUOPC, OMS, OpenMI and Pyre).</li> </ul> <ul> <li>Extensions to the NUOPC "cap", a simplified method for deploying models as ESMF components.</li> </ul> <ul> <li>A prototype web and mobile app to simplify the collection of standardized metadata for models from developers and advanced users.</li> </ul> <p>These technologies were then used to demonstrate how frameworks can be connected to solve societally important problems.&nbsp; In one demonstration, the WRF-Hydro model from academia was integrated into the NOAA Environmental Modeling System (NEMS) -- part of the Earth System Prediction Suite (Theurich et al., 2016) -- to simulate the hydrologic impacts of Hurricane Irene and tropical storm Lee during August 2011. &nbsp;WRF-Hydro was also deployed in the National Water Model.&nbsp; In another, a deep-Earth process model, SNAC, was deployed within the CSDMS and Pyre frameworks. In short, ESB technologies allow models developed by different communities to be reused and rapidly deployed across the divide between science domains and between research and operations.</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/16/2018<br>      Modified by: Scott&nbsp;D&nbsp;Peckham</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1343811/1343811_10270833_1518800126183_ESB_Figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1343811/1343811_10270833_1518800126183_ESB_Figure1--rgov-800width.jpg" title="WRF-Hydro in NFIE Modeling Workflow"><img src="/por/images/Reports/POR/2018/1343811/1343811_10270833_1518800126183_ESB_Figure1--rgov-66x44.jpg" alt="WRF-Hydro in NFIE Modeling Workflow"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A conceptual diagram of the 2015 NFIE modeling workflow, showing WRF-Hydro connected to federal models.</div> <div class="imageCredit">David Gochis</div> <div class="imageSubmitted">Scott&nbsp;D&nbsp;Peckham</div> <div class="imageTitle">WRF-Hydro in NFIE Modeling Workflow</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Observational data and predictive models (observations &amp; predictions, or experimentation &amp; theory) are the two pillars of science.  Sophisticated instruments are used to measure values of scientific variables that are then stored in data sets.  Physically-based, mathematical models summarize our current best knowledge of how physical systems operate, and are therefore used to build computational models that predict future system states from initial conditions.  Data sets and computational models are therefore the two fundamental, digital resource types geoscientists use to study problems of societal interest, such as resource management, environmental hazards and climate change.  While modern geoscientists have online access to an abundance of data sets and models, these resources differ from each other in myriad ways and this heterogeneity hinders interoperability. Interoperability is critically important because a single data set or model is insufficient to tackle a nontrivial geoscience problem. For example, models typically require several types of input data, obtained by reading measured data from files or as output from other models. Coupling many different heterogeneous resources into computational workflows is therefore essential, and geoscientists spend a large fraction of their time setting up and executing these workflows. An extensive, empirical analysis of scientific workflows (Garijo et al., 2013) found that geoscientists typically spend between 60% and 80% of their time dealing with these "data friction" issues, leaving the remaining time for the science.  To address this problem, sophisticated software architectures known as "modeling frameworks" or "modeling systems" have been created to simplify the task of coupling heterogeneous digital resources into executable workflows.  The coupled models typically run on supercomputers, exchanging values of variables as they run.  Geoscience variables (e.g. sea surface temperature) typically vary over space (i.e. a geographic region spanned by a grid) and/or time (i.e. a time period).  Modeling frameworks treat each digital resource as a reusable, plug-and-play component and achieve interoperability through a combination of (1) standardized component APIs, (2) standardized metadata for describing resources and (3) the use of mediators or brokers to transform data as needed before passing it to another resource. A variety of modeling frameworks have been developed both within academia and at federal agencies.  Examples from the academic community include the NSF-funded CSDMS project (Community Surface Dynamics Modeling System), which serves the Earth surface process modeling community (Peckham et al., 2013), and Pyre, from the deep-Earth process community.  Two examples from the federal or operational modeling community include:  ESMF (Earth System Modeling Framework), used primarily by the atmosphere and ocean modeling community (Hill et al., 2004), and   OMS (Object Modeling System), developed by the USDA (US Department of Agriculture) primarily for agricultural modeling (David et al., 2002)   Despite many similarities, each framework is customized to the needs of the community it serves.  This has led to a critical gap between the frameworks and coupling technologies used by mission-driven federal agencies, tasked with environmental forecasts, and those used by academic communities, who advance the science and technology that underpin the forecasts.  This gap hinders the interchange between research and operations that is essential to improving predictions of environmental hazards as well as to mitigating the impacts of those hazards.  The Earth System Bridge (ESB) project, funded by NSF's EarthCube initiative, brought together developers of several modeling frameworks to develop a theoretical basis and practical, streamlined approach to cross-framework interoperability.  Specific technologies developed by this project include:  Earth System Framework Description Language (ES-FDL) for fully describing the capabilities of a modeling framework.   Extensions to the Geoscience Standard Names (GSN) ontology, which supports cross-framework, domain-agnostic semantic alignment.  With community input, variable names from numerous models, existing vocabularies (e.g. CF Standard Names) and geoscience domains were mapped into the GSN.   A Model Component Metadata schema for deep description of computational models.   A set of adapters, so that any model that implements the framework-agnostic Basic Model Interface (BMI) can be deployed without change as a component in multiple modeling frameworks (e.g. ESMF/NUOPC, OMS, OpenMI and Pyre).   Extensions to the NUOPC "cap", a simplified method for deploying models as ESMF components.   A prototype web and mobile app to simplify the collection of standardized metadata for models from developers and advanced users.   These technologies were then used to demonstrate how frameworks can be connected to solve societally important problems.  In one demonstration, the WRF-Hydro model from academia was integrated into the NOAA Environmental Modeling System (NEMS) -- part of the Earth System Prediction Suite (Theurich et al., 2016) -- to simulate the hydrologic impacts of Hurricane Irene and tropical storm Lee during August 2011.  WRF-Hydro was also deployed in the National Water Model.  In another, a deep-Earth process model, SNAC, was deployed within the CSDMS and Pyre frameworks. In short, ESB technologies allow models developed by different communities to be reused and rapidly deployed across the divide between science domains and between research and operations.          Last Modified: 02/16/2018       Submitted by: Scott D Peckham]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
