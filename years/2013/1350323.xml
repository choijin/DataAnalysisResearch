<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Large-scale Appearance Modeling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/15/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>473479.00</AwardTotalIntnAmount>
<AwardAmount>473479</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The visual appearance of the world around us is the result of complex light interactions between different surfaces and material properties that comprise a scene.  Despite staggering advances in data-driven appearance modeling, the creation of accurate models of large environments remains an open problem.  The reliance of most current appearance modeling methods on active lighting to probe different slices of a scene's appearance precludes their use in environments where there is limited or no control over the incident ambient lighting.  Furthermore, to facilitate calibration, many appearance modeling techniques estimate the appearance of a scene from a fixed vantage point, excluding scenes too large to fit in a single view with sufficient detail.  In this research, the PI will investigate two novel appearance modeling paradigms designed expressly for large-scale environments under uncontrolled ambient lighting: appearance-from-motion and appearance-by-similarity.  The former exploits relations between observations from different viewpoints to infer the full reflectance behavior, while the latter seeks to identify the best match from a library of pre-existing appearance instances to a possibly under-constrained set of observations.  To support these two paradigms, a novel appearance model will be developed that builds upon our intuitions regarding scene appearance.  The work will focus on two common types of input: community photo-collections and targeted video sequences.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This research will pave the way towards practical techniques for in-situ appearance modeling of large-scale environments, while stimulating new research in computer vision and in data-driven appearance modeling in computer graphics by answering fundamental questions as to whether we can model appearance from motion and/or by exploiting similarity.  The project will have far-reaching impact not only on computer science but also on diverse fields ranging from metropolitan planning to cultural heritage to entertainment.  The ability to model existing environments will be beneficial to various security and safety training programs (for example, virtual fire drill simulations of existing buildings and sites could help train and prepare firefighters and first responders).  The emerging field of virtual reality therapy will also benefit from this research, by making it easier to create digital models of large-scale environments (so that, for example, patients who have suffered a stroke can practice motor rehabilitation skills in virtual reproductions of environments they encounter in their daily lives, while autistic children can train to improve their social interactions in virtual reproductions of places such as classrooms which they encounter in their daily lives).</AbstractNarration>
<MinAmdLetterDate>01/10/2014</MinAmdLetterDate>
<MaxAmdLetterDate>03/06/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1350323</AwardID>
<Investigator>
<FirstName>Pieter</FirstName>
<LastName>Peers</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pieter Peers</PI_FULL_NAME>
<EmailAddress>ppeers@cs.wm.edu</EmailAddress>
<PI_PHON>7572213466</PI_PHON>
<NSF_ID>000554726</NSF_ID>
<StartDate>01/10/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>College of William and Mary</Name>
<CityName>Williamsburg</CityName>
<ZipCode>231878795</ZipCode>
<PhoneNumber>7572213966</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 8795]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>074762238</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLLEGE OF WILLIAM &amp; MARY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>074762238</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[College of William and Mary]]></Name>
<CityName>Williamsburg</CityName>
<StateCode>VA</StateCode>
<ZipCode>231878795</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~85832</FUND_OBLG>
<FUND_OBLG>2015~90864</FUND_OBLG>
<FUND_OBLG>2016~94741</FUND_OBLG>
<FUND_OBLG>2017~98846</FUND_OBLG>
<FUND_OBLG>2018~103196</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The visual appearance of the world around us is the result of complex light interactions between different surfaces and material properties that comprise the scene. Modeling all of these properties efficiently and accurately, such that the scene can be revisualized from a novel viewpoint or under novel lighting, is a difficult and active research problem. In particular, the creation of accurate models of large environments is challenging due to the presence of uncontrolled factors, such as lighting, and the absence/difficulty of careful calibration of the measurements.&nbsp; Therefore, the research performed under this award focused on the in-situ digitization of object appearance under uncontrolled acquisition conditions using off-the-shelf hardware with minimal calibration.&nbsp; Our investigations focused on two novel appearance modeling paradigms: "appearance-from-motion", and "appearance-by-similarity".</p> <p><br />In "appearance-from-motion" we exploit the change in object appearance when moving the object in the lighting environment to infer the full (visual) material properties of the object.&nbsp; In our first work in this direction, we assumed prior knowledge of the object shape, and recovered the material properties from a video of the object rotating around its axis in an unknown environment (in other words, we also did not have knowledge of the lighting present in the scene). The proposed method proved robust enough to be able to recover the material properties from a (fully uncalibrated) YouTube video of a rotating object.&nbsp; In follow up work, we eliminated the assumption of prior knowledge of the shape, and recovered both the shape and material properties from a video of a rotating object.</p> <p>The goal of "appearance-by-similarity" is to leverage knowledge from a large library of material properties in order to reduce the amount of data that needs to be captured from a scene.&nbsp; Intuitively, if we have a very similar material in the database, then we only need to identify the most similar material in the database which should require less information/photographs than directly recovering the material properties.&nbsp; We developed a novel deep learning based method that can estimate plausible material properties from a single photograph of a planar material sample captured under unknown lighting.&nbsp; In order to leverage deep learning (and deep convolutional neural networks in particular), a sufficiently large training dataset is required.&nbsp; Such a dataset did not exist at that time. We therefore, developed a novel"self-augmentation" learning strategy that uses knowledge of the inverse function (namely visualizing the measured material properties), together with a dataset of photographs of materials under unknown lighting (this is essentially an exemplar dataset of possible inputs a user might provide the network) and a small initial dataset of fully measured materials, to self-augment the accuracy of the neural network. In other words, the neural network learns to improve itself by leveraging the inverse process of what it is trying to learn.&nbsp; In subsequent work we further reduced the need for a initial dataset, and show that a satisfactory accurate neural network can be trained using only photographs of materials under unknown lighting without the need for a small fully measured material dataset.&nbsp; While inferring the material properties from a single photograph makes appearance modeling accessible to non-expert users, and greatly facilitates in-situ acquisition, it can only offer limited accuracy. In subsequent work, we developed a method that can estimate the material properties of a planar material sample from an arbitrary number of input photographs from arbitrary viewpoints captured with a cell phone with its flash turned on. If the user only provides a few input photographs, a plausible estimate of the material properties is inferred. As the user acquires more photographs of the material, the accuracy of the estimates improves.</p> <p><br />Besides the above key research results, we also investigated a number of related research problems relevant to in-situ appearance modeling under uncontrolled conditions, ranging from: a single image radiometric calibration method which converts pixel values in photographs to physically correct quantities, a method for learning the distribution of shapes from community photo collections (such as Google images) such that we can afterwards generate any 3D shape seen in the image collection, and a number of methods for estimating material properties of homogeneous objects under uncontrolled lighting.</p> <p>Our research has made advances in appearance modeling and acquisition in the computer graphics and the computer vision field.&nbsp; Our results have been published in several scientific journals and conferences.This project has supported the development of multiple undergraduate and graduate students, and its findings were part of the central thesis of several doctoral dissertations. Its results have been integrated in the relevant undergraduate and graduate courses at the College of William &amp; Mary. The developed methods have potential applications in the many fields that rely on visualizations of digital environments. The ability to model existing environments has the potential to benefit cultural heritage preservation, training simulations, and virtual reality therapy.</p><br> <p>            Last Modified: 01/04/2020<br>      Modified by: Pieter&nbsp;Peers</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The visual appearance of the world around us is the result of complex light interactions between different surfaces and material properties that comprise the scene. Modeling all of these properties efficiently and accurately, such that the scene can be revisualized from a novel viewpoint or under novel lighting, is a difficult and active research problem. In particular, the creation of accurate models of large environments is challenging due to the presence of uncontrolled factors, such as lighting, and the absence/difficulty of careful calibration of the measurements.  Therefore, the research performed under this award focused on the in-situ digitization of object appearance under uncontrolled acquisition conditions using off-the-shelf hardware with minimal calibration.  Our investigations focused on two novel appearance modeling paradigms: "appearance-from-motion", and "appearance-by-similarity".   In "appearance-from-motion" we exploit the change in object appearance when moving the object in the lighting environment to infer the full (visual) material properties of the object.  In our first work in this direction, we assumed prior knowledge of the object shape, and recovered the material properties from a video of the object rotating around its axis in an unknown environment (in other words, we also did not have knowledge of the lighting present in the scene). The proposed method proved robust enough to be able to recover the material properties from a (fully uncalibrated) YouTube video of a rotating object.  In follow up work, we eliminated the assumption of prior knowledge of the shape, and recovered both the shape and material properties from a video of a rotating object.  The goal of "appearance-by-similarity" is to leverage knowledge from a large library of material properties in order to reduce the amount of data that needs to be captured from a scene.  Intuitively, if we have a very similar material in the database, then we only need to identify the most similar material in the database which should require less information/photographs than directly recovering the material properties.  We developed a novel deep learning based method that can estimate plausible material properties from a single photograph of a planar material sample captured under unknown lighting.  In order to leverage deep learning (and deep convolutional neural networks in particular), a sufficiently large training dataset is required.  Such a dataset did not exist at that time. We therefore, developed a novel"self-augmentation" learning strategy that uses knowledge of the inverse function (namely visualizing the measured material properties), together with a dataset of photographs of materials under unknown lighting (this is essentially an exemplar dataset of possible inputs a user might provide the network) and a small initial dataset of fully measured materials, to self-augment the accuracy of the neural network. In other words, the neural network learns to improve itself by leveraging the inverse process of what it is trying to learn.  In subsequent work we further reduced the need for a initial dataset, and show that a satisfactory accurate neural network can be trained using only photographs of materials under unknown lighting without the need for a small fully measured material dataset.  While inferring the material properties from a single photograph makes appearance modeling accessible to non-expert users, and greatly facilitates in-situ acquisition, it can only offer limited accuracy. In subsequent work, we developed a method that can estimate the material properties of a planar material sample from an arbitrary number of input photographs from arbitrary viewpoints captured with a cell phone with its flash turned on. If the user only provides a few input photographs, a plausible estimate of the material properties is inferred. As the user acquires more photographs of the material, the accuracy of the estimates improves.   Besides the above key research results, we also investigated a number of related research problems relevant to in-situ appearance modeling under uncontrolled conditions, ranging from: a single image radiometric calibration method which converts pixel values in photographs to physically correct quantities, a method for learning the distribution of shapes from community photo collections (such as Google images) such that we can afterwards generate any 3D shape seen in the image collection, and a number of methods for estimating material properties of homogeneous objects under uncontrolled lighting.  Our research has made advances in appearance modeling and acquisition in the computer graphics and the computer vision field.  Our results have been published in several scientific journals and conferences.This project has supported the development of multiple undergraduate and graduate students, and its findings were part of the central thesis of several doctoral dissertations. Its results have been integrated in the relevant undergraduate and graduate courses at the College of William &amp; Mary. The developed methods have potential applications in the many fields that rely on visualizations of digital environments. The ability to model existing environments has the potential to benefit cultural heritage preservation, training simulations, and virtual reality therapy.       Last Modified: 01/04/2020       Submitted by: Pieter Peers]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
