<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Acquisition of Big-Data Private-Cloud Research Cyberinfrastructure (BDPC)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Proposal #: 13-38099&lt;br/&gt;PI(s):  Vardi, Moshe; Allen, Genevera; Bradshaw, Stephen; Karaki, Lydia; Veeraraghavan, Ashok&lt;br/&gt;Institution:      Rice University&lt;br/&gt;Title: MRI/Acq.: Big-Data Private Cloud Research Cyberinfrastructure (BDPC)            &lt;br/&gt;Project Proposed:&lt;br/&gt;This project, acquiring a novel cyber-infrastructure instrument for big data cloud computing designed as a loosely coupled computations system with large memory requirements, enables a significant range of application domains as well as research into infrastructures for cloud computing. The domain sciences addressed range from development of big-data enabling software technologies spanning fundamental computer science work, to the analysis of electronic medical records, twitter streams, and hurricane evacuation strategies. Additional benefits are expected in understanding disease and therapeutic treatments, and in the development and application of mathematical models in the areas of machine learning, optimization, compressed sensing, image processing, and statistical analysis and data mining. The instrument will also help bridge the gap between numerical models and observations in astrophysics.&lt;br/&gt;Broader Impacts:  &lt;br/&gt;The broader impacts on society, and especially in education and training (including for members of underrepresented groups) are all compelling. The instrument will directly impact the educational experience for all students taking classes in computing and computational problem solving. The targeted research communities are diverse and broad, including the underrepresented groups, with strong empirical and experimental components. The proposed instrument is highly suitable for training and education.</AbstractNarration>
<MinAmdLetterDate>08/30/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/30/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1338099</AwardID>
<Investigator>
<FirstName>Moshe</FirstName>
<LastName>Vardi</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Moshe Y Vardi</PI_FULL_NAME>
<EmailAddress>vardi@rice.edu</EmailAddress>
<PI_PHON>7133485977</PI_PHON>
<NSF_ID>000462277</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lydia</FirstName>
<LastName>Kavraki</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lydia Kavraki</PI_FULL_NAME>
<EmailAddress>kavraki@cs.rice.edu</EmailAddress>
<PI_PHON>7133484834</PI_PHON>
<NSF_ID>000489055</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ashok</FirstName>
<LastName>Veeraraghavan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ashok Veeraraghavan</PI_FULL_NAME>
<EmailAddress>Ashok.Veeraraghavan@gmail.com</EmailAddress>
<PI_PHON>7133484820</PI_PHON>
<NSF_ID>000583333</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Genevera</FirstName>
<LastName>Allen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Genevera Allen</PI_FULL_NAME>
<EmailAddress>gallen@rice.edu</EmailAddress>
<PI_PHON>7133484820</PI_PHON>
<NSF_ID>000605885</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stephen</FirstName>
<LastName>Bradshaw</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephen Bradshaw</PI_FULL_NAME>
<EmailAddress>stephen.bradshaw@rice.edu</EmailAddress>
<PI_PHON>7133484045</PI_PHON>
<NSF_ID>000642042</NSF_ID>
<StartDate>08/30/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 Main St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Today, computation is a cornerstone of scientific inquiry, as important to science and engineering as the traditional approaches of theory and experimentation. To support computational research and scholarship across science and engineering, we procured, deployed and are now operating a high-throughput computing resource at Rice. This newly deployed system replaced old inefficient, end-of-life infrastructure with limited capabilities which rendered it too costly to operate for what it delivered and inadequate for supporting the forward looking federally funded research and development by researchers at Rice University and their collaborators.</p> <p>&nbsp;The deployed system, co-funded by Rice University, consists of a total of 88 dual-socket HPE Linux-based server nodes interconnected with a 10gb/s network. The system is supported by a 220 TeraByte (TB) Lustre scratch file system and a 50 TB NFS file system for longer term data storage. Rice plans to operate this resource for 5 years (60 months).</p> <p>&nbsp;After 18 months in full production, the system has delivered over 12 million core hours supporting the computationally demanding research of fifty faculty members and their associated three hundred plus users. The user community is continuing to grow and we expect, based on past experience, that within the next 6-12 months the number of faculty members and users supported will likely double. Faculty members supporting users on the system today are developing computational techniques to advance science and engineering with direct impact on many disciplines, including astrophysics, computer-aided verification, machine vision, robotics, and statistical machine learning. The system has already enabled significant student training, not only though serving as a platform for accelerating research and discovery, but by serving as a resource used in seven classes across engineering (reaching over 250 students). While usage by classes is very small as a percentage of total time, it is a critical part of preparing students for careers that are increasingly dependent on computing skills.</p> <p>&nbsp;While national supercomputing resources, such as XSEDE, can provide the research community with substantial computing capabilities, the investigators needed a local modest-size resource that would complement XSEDE in order to facilitate development, small scale experimentation, training and support the long tail of research computing. In particular, the higher bandwidth and lower latency to the desktop from a local system better supports both code development and interactive data analysis using tools with graphical user interfaces.</p> <p>&nbsp;This system has enabled Rice to successfully deploy and operate an on-premises computational infrastructure in support of expanding demand for throughput computing to facilitate rapidly growing simulation, modeling, big-data data, and analytics computational experiments. Massive throughput computing is a type of computational workload that is generally too large to be efficiently executed on an individual workstation but too small to be competitive or well suited for running on an XSEDE resource. This class of computing needs is currently ideally served by the on-premises infrastructure such as the infrastructure procured and deployed under this award.</p><br> <p>            Last Modified: 12/02/2016<br>      Modified by: Moshe&nbsp;Y&nbsp;Vardi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today, computation is a cornerstone of scientific inquiry, as important to science and engineering as the traditional approaches of theory and experimentation. To support computational research and scholarship across science and engineering, we procured, deployed and are now operating a high-throughput computing resource at Rice. This newly deployed system replaced old inefficient, end-of-life infrastructure with limited capabilities which rendered it too costly to operate for what it delivered and inadequate for supporting the forward looking federally funded research and development by researchers at Rice University and their collaborators.   The deployed system, co-funded by Rice University, consists of a total of 88 dual-socket HPE Linux-based server nodes interconnected with a 10gb/s network. The system is supported by a 220 TeraByte (TB) Lustre scratch file system and a 50 TB NFS file system for longer term data storage. Rice plans to operate this resource for 5 years (60 months).   After 18 months in full production, the system has delivered over 12 million core hours supporting the computationally demanding research of fifty faculty members and their associated three hundred plus users. The user community is continuing to grow and we expect, based on past experience, that within the next 6-12 months the number of faculty members and users supported will likely double. Faculty members supporting users on the system today are developing computational techniques to advance science and engineering with direct impact on many disciplines, including astrophysics, computer-aided verification, machine vision, robotics, and statistical machine learning. The system has already enabled significant student training, not only though serving as a platform for accelerating research and discovery, but by serving as a resource used in seven classes across engineering (reaching over 250 students). While usage by classes is very small as a percentage of total time, it is a critical part of preparing students for careers that are increasingly dependent on computing skills.   While national supercomputing resources, such as XSEDE, can provide the research community with substantial computing capabilities, the investigators needed a local modest-size resource that would complement XSEDE in order to facilitate development, small scale experimentation, training and support the long tail of research computing. In particular, the higher bandwidth and lower latency to the desktop from a local system better supports both code development and interactive data analysis using tools with graphical user interfaces.   This system has enabled Rice to successfully deploy and operate an on-premises computational infrastructure in support of expanding demand for throughput computing to facilitate rapidly growing simulation, modeling, big-data data, and analytics computational experiments. Massive throughput computing is a type of computational workload that is generally too large to be efficiently executed on an individual workstation but too small to be competitive or well suited for running on an XSEDE resource. This class of computing needs is currently ideally served by the on-premises infrastructure such as the infrastructure procured and deployed under this award.       Last Modified: 12/02/2016       Submitted by: Moshe Y Vardi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
