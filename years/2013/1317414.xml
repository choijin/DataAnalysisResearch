<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Visual Cortex on Silicon</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>219918.00</AwardTotalIntnAmount>
<AwardAmount>219918</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The human vision system understands and interprets complex scenes for a wide range of visual tasks in real-time while consuming less than 20 Watts of power. This Expeditions-in-Computing project explores holistic design of machine vision systems that have the potential to approach and eventually exceed the capabilities of human vision systems. This will enable the next generation of machine vision systems to not only record images but also understand visual content. Such smart machine vision systems will have a multi-faceted impact on society, including visual aids for visually impaired persons, driver assistance for reducing automotive accidents, and augmented reality for enhanced shopping, travel, and safety. The transformative nature of the research will inspire and train a new generation of students in inter-disciplinary work that spans neuroscience, computing and engineering discipline.&lt;br/&gt;&lt;br/&gt;While several machine vision systems today can each successfully perform one or a few human tasks ? such as detecting human faces in point-and-shoot cameras ? they are still limited in their ability to perform a wide range of visual tasks, to operate in complex, cluttered environments, and to provide reasoning for their decisions.  In contrast, the mammalian visual cortex excels in a broad variety of goal-oriented cognitive tasks, and is at least three orders of magnitude more energy efficient than customized state-of-the-art machine vision systems. The proposed research envisions a holistic design of a machine vision system that will approach the cognitive abilities of the human cortex, by developing a comprehensive solution consisting of vision algorithms, hardware design, human-machine interfaces, and information storage. The project aims to understand the fundamental mechanisms used in the visual cortex to enable the design of new vision algorithms and hardware fabrics that can improve power, speed, flexibility, and recognition accuracies relative to existing machine vision systems. Towards this goal, the project proposes an ambitious inter-disciplinary research agenda that will (i) understand goal-directed visual attention mechanisms in the brain to design task-driven vision algorithms; (ii) develop vision theory and algorithms that scale in performance with increasing complexity of a scene; (iii) integrate complementary approaches in biological and machine vision techniques; (iv) develop a new-genre of computing architectures inspired by advances in both the understanding of the visual cortex and the emergence of electronic devices; and (v) design human-computer interfaces that will effectively assist end-users while preserving privacy and maximizing utility. These advances will allow us to replace current-day cameras with cognitive visual systems that more intelligently analyze and understand complex scenes, and dynamically interact with users.&lt;br/&gt;&lt;br/&gt;Machine vision systems that understand and interact with their environment in ways similar to humans will enable new transformative applications. The project will develop experimental platforms to: (1) assist visually impaired people; (2) enhance driver attention; and (3) augment reality to provide enhanced experience for retail shopping or a vacation visit, and enhanced safety for critical public infrastructure. This project will result in education and research artifacts that will be disseminated widely through a web portal and via online lecture delivery. The resulting artifacts and prototypes will enhance successful ongoing outreach programs to under-represented minorities and the general public, such as museum exhibits, science fairs, and a summer camp aimed at K-12 students. It will also spur similar new outreach efforts at other partner locations. The project will help identify and develop course material and projects directed at instilling interest in computing fields for students in four-year colleges. Partnerships with two Hispanic serving institutes, industry, national labs and international projects are also planned.</AbstractNarration>
<MinAmdLetterDate>09/17/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317414</AwardID>
<Investigator>
<FirstName>Greg</FirstName>
<LastName>Link</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Greg Link</PI_FULL_NAME>
<EmailAddress>glink@ycp.edu</EmailAddress>
<PI_PHON>7178151346</PI_PHON>
<NSF_ID>000618010</NSF_ID>
<StartDate>09/17/2013</StartDate>
<EndDate>05/29/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Wayne</FirstName>
<LastName>Blanding</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wayne Blanding</PI_FULL_NAME>
<EmailAddress>wblandin@ycp.edu</EmailAddress>
<PI_PHON>7178156651</PI_PHON>
<NSF_ID>000694893</NSF_ID>
<StartDate>05/29/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>York College of Pennsylvania</Name>
<CityName>York</CityName>
<ZipCode>174033651</ZipCode>
<PhoneNumber>7178151346</PhoneNumber>
<StreetAddress>441 Country Club Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>068673102</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>YORK COLLEGE OF PENNSYLVANIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068673102</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[York College of Pennsylvania]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>174033651</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7723</Code>
<Text>Expeditions in Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7723</Code>
<Text>EXPERIMENTAL EXPEDITIONS</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~83462</FUND_OBLG>
<FUND_OBLG>2015~136456</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The major objective of the project has been to develop educational content and tools along with outreach experiences that share the advancements of the Expedition program.&nbsp; Our approach has been to create hands-on learning experiences around the specification and implementation of an advanced visual prosthetic device.&nbsp; The prosthetic device is intended to help a visually impaired person while grocery shopping without human assistance.&nbsp; The system consists of a wearable headset and a glove outfitted with haptic actuators.&nbsp; The headset navigates the visually impaired shopper indoor and detects grocery items on shelves.&nbsp; Once an item is detected, the glove provides haptic feedback to the user to guide their hand to the location on the shelf. The system incorporates developments from the primary thrusts of the Expedition including advanced computer vision, deep learning and inferencing, digital and analog hardware architectures, and human computer interfaces (HCI). &nbsp;</p> <p>Several graduate and undergraduate students have participated in the development of the many iterations of the prototype.&nbsp; During the academic year, undergraduate students at Penn State and York College of Engineering collaborated and contributed significantly to the project.&nbsp; Several of these students were able to incorporate their work into their honor&rsquo;s or master&rsquo;s theses.&nbsp; During the 2015 and 2016 summers, in conjunction with the Summer Research Opportunities Program (SROP) at Penn State, several students participated in research internships with topics directly related to the development of the device prototype.&nbsp; Students participated in a myriad of tasks that required them to push the limits of their creativity and ingenuity.&nbsp; During the 2016 summer, interns visiting from the University of Puerto Rico Mayaguez, were tasked with collecting an image dataset from the local grocery store.&nbsp; By outfitting a shopping cart with various cameras and a custom battery power supply, the students were able to collect a large instore image dataset within two hours.&nbsp;</p> <p>During the 2015 and 2016 summers, a high-school science teacher joined the project with the objective that she would create new learning experiences for her students. &nbsp;She was assigned projects that involved quantitatively evaluating traditional and state-of-the-art computer vision techniques. &nbsp;She was guided through the process of collecting training images, creating ground-truth labels, performing data augmentation, training a Convolution Neural Network, and quantifying the performance of the recognition system.&nbsp; Ultimately, she was able to incorporate her experiences into activities in her science class and science club curriculum.</p> <p>Throughout the project, students learned best practices in team coordination and project management using industry tools and methodologies.&nbsp; With such a large team of students, opportunities to meet in-person were scarce.&nbsp; The students independently utilized online tools including Skype and Slack to maintain communication when in-person meetings were not possible.&nbsp; Students learned how to develop detailed project specifications and implementation plans for their individual/group projects.</p> <p>Over the course of the Expedition, numerous opportunities were seized to highlight the project to external audiences.&nbsp; Most notably, students from Penn State and York College won second place in the 2017 IEEE Global Student Challenge for their entry &ldquo;Computer Vision for Good&rdquo; which highlighted the visual prosthetic device prototype.&nbsp; Other outreach efforts targeting young audiences included the annual ScienceU Exploration events and the 2018 Art of Discovery exhibition in State College, Pennsylvania.&nbsp; Demonstrations of the prototype device were exhibited at the 2017 National Federation of the Blind (NFB) meeting in State College, Pennsylvania.&nbsp; In 2016 the haptic glove was demonstrated along with other project artifacts at the Coalition for National Science Funding (CNSF) on Capitol Hill. &nbsp;Collaborators from Penn State, SiliconScapes and IBM demonstrated hardware accelerators for vision at the 2016 OpenPOWER and SuperComputing conferences.</p> <p>In summary, the Visual Cortex on Silicon Expedition has provided a valuable learning vehicle for students and investigators.&nbsp; Students have participated in authentic research and development efforts with objectives that were clear and consistent throughout the duration of the project. Students have been exposed to state-of-the-art advancements in computer vision, machine learning, and computer architecture. &nbsp;The project investigators have identified new and creative methodologies for distilling and disseminating highly technical content to learners of varying interest and ability.&nbsp; &nbsp;</p><br> <p>            Last Modified: 12/12/2018<br>      Modified by: Wayne&nbsp;Blanding</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The major objective of the project has been to develop educational content and tools along with outreach experiences that share the advancements of the Expedition program.  Our approach has been to create hands-on learning experiences around the specification and implementation of an advanced visual prosthetic device.  The prosthetic device is intended to help a visually impaired person while grocery shopping without human assistance.  The system consists of a wearable headset and a glove outfitted with haptic actuators.  The headset navigates the visually impaired shopper indoor and detects grocery items on shelves.  Once an item is detected, the glove provides haptic feedback to the user to guide their hand to the location on the shelf. The system incorporates developments from the primary thrusts of the Expedition including advanced computer vision, deep learning and inferencing, digital and analog hardware architectures, and human computer interfaces (HCI).    Several graduate and undergraduate students have participated in the development of the many iterations of the prototype.  During the academic year, undergraduate students at Penn State and York College of Engineering collaborated and contributed significantly to the project.  Several of these students were able to incorporate their work into their honor?s or master?s theses.  During the 2015 and 2016 summers, in conjunction with the Summer Research Opportunities Program (SROP) at Penn State, several students participated in research internships with topics directly related to the development of the device prototype.  Students participated in a myriad of tasks that required them to push the limits of their creativity and ingenuity.  During the 2016 summer, interns visiting from the University of Puerto Rico Mayaguez, were tasked with collecting an image dataset from the local grocery store.  By outfitting a shopping cart with various cameras and a custom battery power supply, the students were able to collect a large instore image dataset within two hours.   During the 2015 and 2016 summers, a high-school science teacher joined the project with the objective that she would create new learning experiences for her students.  She was assigned projects that involved quantitatively evaluating traditional and state-of-the-art computer vision techniques.  She was guided through the process of collecting training images, creating ground-truth labels, performing data augmentation, training a Convolution Neural Network, and quantifying the performance of the recognition system.  Ultimately, she was able to incorporate her experiences into activities in her science class and science club curriculum.  Throughout the project, students learned best practices in team coordination and project management using industry tools and methodologies.  With such a large team of students, opportunities to meet in-person were scarce.  The students independently utilized online tools including Skype and Slack to maintain communication when in-person meetings were not possible.  Students learned how to develop detailed project specifications and implementation plans for their individual/group projects.  Over the course of the Expedition, numerous opportunities were seized to highlight the project to external audiences.  Most notably, students from Penn State and York College won second place in the 2017 IEEE Global Student Challenge for their entry "Computer Vision for Good" which highlighted the visual prosthetic device prototype.  Other outreach efforts targeting young audiences included the annual ScienceU Exploration events and the 2018 Art of Discovery exhibition in State College, Pennsylvania.  Demonstrations of the prototype device were exhibited at the 2017 National Federation of the Blind (NFB) meeting in State College, Pennsylvania.  In 2016 the haptic glove was demonstrated along with other project artifacts at the Coalition for National Science Funding (CNSF) on Capitol Hill.  Collaborators from Penn State, SiliconScapes and IBM demonstrated hardware accelerators for vision at the 2016 OpenPOWER and SuperComputing conferences.  In summary, the Visual Cortex on Silicon Expedition has provided a valuable learning vehicle for students and investigators.  Students have participated in authentic research and development efforts with objectives that were clear and consistent throughout the duration of the project. Students have been exposed to state-of-the-art advancements in computer vision, machine learning, and computer architecture.  The project investigators have identified new and creative methodologies for distilling and disseminating highly technical content to learners of varying interest and ability.          Last Modified: 12/12/2018       Submitted by: Wayne Blanding]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
