<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Characterizing Object Recognition Machinery in a Newborn Visual System</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/15/2014</AwardEffectiveDate>
<AwardExpirationDate>03/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>559086.00</AwardTotalIntnAmount>
<AwardAmount>559086</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Vishton</SignBlockName>
<PO_EMAI>pvishton@nsf.gov</PO_EMAI>
<PO_PHON>7032928132</PO_PHON>
</ProgramOfficer>
<AbstractNarration>How does early experience shape how we process and interpret visual information? Two major limitations have made this question difficult to answer. First, researchers can typically collect only a few data points from newborns, which prevents precise measurement of the infants' visual cognitive abilities. Second, human infants cannot ethically be raised in controlled environments from birth, which prevents researchers from studying how specific experiences shape the newborn mind. &lt;br/&gt;&lt;br/&gt;To overcome these limitations, Dr. Wood has developed a new controlled-rearing method using a non-human animal model. This method can be used to measure all of a newborn's behavior (24 hours/day, 7 days/week) with high precision (9 samples/second) within strictly controlled environments. With support from this NSF CAREER award, Dr. Wood will use the new controlled-rearing method to characterize how newborns recognize objects at the onset of visual object experience. &lt;br/&gt;&lt;br/&gt;Dr. Wood's laboratory will use a two-pronged approach. First, the lab will perform a series of controlled-rearing experiments with newborn chickens. Studies of chickens can inform human cognitive development because chickens and humans have similar neural processing systems for sensory information. These controlled-rearing experiments will reveal how specific visual experiences shape newborns' object recognition abilities. The findings will provide the foundation for a new, publicly-accessible database that describes how specific sensory experiences relate to specific behaviors in a newborn organism. &lt;br/&gt;&lt;br/&gt;Second, the lab will build biologically-inspired computational models of newborns' object recognition behavior, using state-of-the-art techniques from artificial intelligence. These models will make predictions that can be compared to the data from the controlled-rearing experiments. This will help identify how the visual system processes objects. This approach integrates ideas from developmental psychology, vision science, and computational neuroscience, providing a unified framework for studying the origins of object recognition and other visual cognitive abilities.</AbstractNarration>
<MinAmdLetterDate>04/04/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1351892</AwardID>
<Investigator>
<FirstName>Justin</FirstName>
<LastName>Wood</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Justin Wood</PI_FULL_NAME>
<EmailAddress>justin.wood@usc.edu</EmailAddress>
<PI_PHON>2137402210</PI_PHON>
<NSF_ID>000622951</NSF_ID>
<StartDate>04/04/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900891061</ZipCode>
<StreetAddress><![CDATA[3620 South McClintock Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1698</Code>
<Text>DS -Developmental Sciences</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1698</Code>
<Text>DS-Developmental Sciences</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~109795</FUND_OBLG>
<FUND_OBLG>2015~112988</FUND_OBLG>
<FUND_OBLG>2016~235135</FUND_OBLG>
<FUND_OBLG>2017~101168</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>One of the great unsolved mysteries in science concerns the origins of intelligence. What are the building blocks of cognition, and how does experience shape cognition over time? Although scientists have pondered these questions for centuries, methodological barriers prevented detailed analysis of how intelligence emerges in newborn brains. With support from this award, my lab invented a powerful set of automated controlled-rearing methods for studying newborn cognition, using newborn chicks as a model system. We raise chicks in strictly controlled virtual worlds and record their behavior 24/7 as they learn to perceive and understand their environment. Fueled by interactive video game engines, we can explore how foundational cognitive abilities emerge in newborn brains. For this project, we focused on object perception: a core ability underlying biological intelligence. We made two major discoveries.</p> <p>First, object perception emerges rapidly in newborn brains. Newborns can segment objects from backgrounds, build abstract object concepts that generalize across new viewing situations, and remember objects that disappear from view (object permanence). Newborns can also perform "one-shot learning," building abstract object concepts from a single image of an object. From an artificial intelligence perspective, these are impressive feats. State-of-the-art machine learning systems typically require thousands to millions of labeled training images to develop object perception, whereas newborns develop this ability from input of a single object. Understanding how newborn brains develop object perception could therefore lead to more efficient learning algorithms in artificial intelligence.</p> <p>Second, newborn brains need experience with a natural visual world in order to learn correctly. Our experiments revealed two main constraints on learning: slowness and smoothness. Changes in object views need to occur slowly and smoothly over time, adhering to the spatiotemporal properties of objects in the real world<em>. </em>Without slow and smooth visual input, newborns fail to develop object perception. These two discoveries indicate that newborn brains learn to see by leveraging the slow and smooth input from natural visual environments.</p> <p>These new methods and results provide the foundations for reverse engineering visual intelligence. By using high-powered methods to study the origins of object perception, we can collect precise data showing how newborn brains transform sensory inputs into behavioral outputs. These input-output patterns can serve as benchmarks for building end-to-end (pixels-to-actions) artificial agents that learn how to see like newborn animals.</p> <p>To this end, my lab developed a machine learning platform for linking biological intelligence to artificial intelligence. This platform allows us to raise newborn animals and artificial agents in the same environments, and test whether they develop the same abilities when provided with the same experiences. For each controlled-rearing experiment performed in the lab, we build a computer-simulated "virtual experiment" for measuring learning in autonomous artificial agents. The agent's brain can be equipped with different biologically-inspired learning algorithms (e.g., deep reinforcement learning, curiosity-driven learning), so by comparing the animals and agents, we can discover which learning mechanisms underlie object perception. This machine learning platform will be publicly-accessible, providing a tool for researchers and students to explore whether their artificial brains learn like newborn brains.</p> <p>In summary, we now have two essential tools for exploring the origins of intelligence: (1) high-powered methods for studying newborn cognition and (2) autonomous artificial agents for modeling newborn cognition. These two innovations link biological intelligence to artificial intelligence, providing a unified framework for reverse engineering newborn cognition and building artificial brains that learn like newborn brains.</p><br> <p>            Last Modified: 07/28/2019<br>      Modified by: Justin&nbsp;Wood</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ One of the great unsolved mysteries in science concerns the origins of intelligence. What are the building blocks of cognition, and how does experience shape cognition over time? Although scientists have pondered these questions for centuries, methodological barriers prevented detailed analysis of how intelligence emerges in newborn brains. With support from this award, my lab invented a powerful set of automated controlled-rearing methods for studying newborn cognition, using newborn chicks as a model system. We raise chicks in strictly controlled virtual worlds and record their behavior 24/7 as they learn to perceive and understand their environment. Fueled by interactive video game engines, we can explore how foundational cognitive abilities emerge in newborn brains. For this project, we focused on object perception: a core ability underlying biological intelligence. We made two major discoveries.  First, object perception emerges rapidly in newborn brains. Newborns can segment objects from backgrounds, build abstract object concepts that generalize across new viewing situations, and remember objects that disappear from view (object permanence). Newborns can also perform "one-shot learning," building abstract object concepts from a single image of an object. From an artificial intelligence perspective, these are impressive feats. State-of-the-art machine learning systems typically require thousands to millions of labeled training images to develop object perception, whereas newborns develop this ability from input of a single object. Understanding how newborn brains develop object perception could therefore lead to more efficient learning algorithms in artificial intelligence.  Second, newborn brains need experience with a natural visual world in order to learn correctly. Our experiments revealed two main constraints on learning: slowness and smoothness. Changes in object views need to occur slowly and smoothly over time, adhering to the spatiotemporal properties of objects in the real world. Without slow and smooth visual input, newborns fail to develop object perception. These two discoveries indicate that newborn brains learn to see by leveraging the slow and smooth input from natural visual environments.  These new methods and results provide the foundations for reverse engineering visual intelligence. By using high-powered methods to study the origins of object perception, we can collect precise data showing how newborn brains transform sensory inputs into behavioral outputs. These input-output patterns can serve as benchmarks for building end-to-end (pixels-to-actions) artificial agents that learn how to see like newborn animals.  To this end, my lab developed a machine learning platform for linking biological intelligence to artificial intelligence. This platform allows us to raise newborn animals and artificial agents in the same environments, and test whether they develop the same abilities when provided with the same experiences. For each controlled-rearing experiment performed in the lab, we build a computer-simulated "virtual experiment" for measuring learning in autonomous artificial agents. The agent's brain can be equipped with different biologically-inspired learning algorithms (e.g., deep reinforcement learning, curiosity-driven learning), so by comparing the animals and agents, we can discover which learning mechanisms underlie object perception. This machine learning platform will be publicly-accessible, providing a tool for researchers and students to explore whether their artificial brains learn like newborn brains.  In summary, we now have two essential tools for exploring the origins of intelligence: (1) high-powered methods for studying newborn cognition and (2) autonomous artificial agents for modeling newborn cognition. These two innovations link biological intelligence to artificial intelligence, providing a unified framework for reverse engineering newborn cognition and building artificial brains that learn like newborn brains.       Last Modified: 07/28/2019       Submitted by: Justin Wood]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
