<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: A Local-Global Approach Towards Omnipresent Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>184416.00</AwardTotalIntnAmount>
<AwardAmount>184416</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project constructs an Omnipresent Vision system - a computational system that allows us to navigate, share, enhance, and understand the visual data captured by a slew of fixed and moving cameras. The society is flooded with various cameras. Almost every cell phone has a video camera and wearable cameras are starting to permeate our lives. These local cameras capture visual experiences from personal perspectives. Static cameras at various outdoor and indoor locations are also constantly capturing videos. These fixed-view cameras offer global, persistent looks into our daily lives. The key idea of this project is to fully leverage the combination of these local and global cameras to enable new visual experiences and facilitate the understanding of the scene and the people within. This is achieved with novel algorithms and computational tools that bring together the local and global views into an integrated platform, model the dynamic scene by joining those two sets of perspectives, and recognize the actions and events in them. &lt;br/&gt;&lt;br/&gt;The research, at a personal level, enables the spatio-temporal and contextual expansion of the person's view, and at a scene level, it enables the interpretation of the scene at various scales of spatial and temporal resolutions. It also provides new means to understand people and scenes. For instance, it facilitates the understanding of people who cannot communicate their intentions. The research activities also furnish graduate and undergraduate students educational opportunities to take part in spawning this new area of research.</AbstractNarration>
<MinAmdLetterDate>09/04/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/04/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1353235</AwardID>
<Investigator>
<FirstName>Ko</FirstName>
<LastName>Nishino</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ko Nishino</PI_FULL_NAME>
<EmailAddress>kon@drexel.edu</EmailAddress>
<PI_PHON>2158952678</PI_PHON>
<NSF_ID>000429327</NSF_ID>
<StartDate>09/04/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Drexel University</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191021119</ZipCode>
<PhoneNumber>2158956342</PhoneNumber>
<StreetAddress>1505 Race St, 10th Floor</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002604817</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DREXEL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002604817</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Drexel University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>191042737</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~184416</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this research program was to establish the theoretical and computational foundation for realizing an ``Omnipresent Vision'' system--a computational system that allows us to navigate, share, enhance, and understand the visual data captured by a number of fixed and moving cameras. The key idea is the integration of ``local'' and ``global'' views. The proliferation of wearable cameras has elicited a growing body of research that makes use of these local views (e.g., ego-centric vision). Our world, however, is also dominated with static cameras that feed persistent observations of specific locations. These ``global'' views share overlapping visual information with local views and also provide direct observations of their movements in the scene (e.g., trajectories of people wearing cameras). The global camera thus yields indispensable visual information to ``anchor'' the local views in the same scene. By integrating the local and global views, we can accurately localize all views in space and time, which opens new means to consolidate and enrich the visual data captured by the two distinct sets of cameras. The two sets of views also provide different perspectives on the actions and events prompted by the social behavior and interactions of the people in the scene. By leveraging the disparate but coherent visual cues captured in these different views, we may delineate and recognize the scene activities and interpret them to deduce the semantical context of the dynamic scene. In other words, the multi-faceted visual observation gives us a better chance of accurately understanding the people and the scene.<br />The research program let the investigators explore how to establish a space-time platform to anchor the local and global views to enable visual analysis of human behavior in the space, including those who are and are not wearing cameras. This research led to the derivation of a novel framework for reconstructing a simplified 3D geometric representation of the space from the camera views. The research program also enabled the investigators to explore how the different camera perspectives, including dynamic cameras worn and capturing personal perspectives and static scene-fixed cameras capturing overhead perspectives, can be combined to better model the body movements and underlying intentions of people in the space. Finally, research on recovering other physical properties of the scene, particularly pertaining to what the scene surfaces are made of (i.e., materials), have also been studied as part of this research program. Collectively, these research efforts have significantly advanced our understanding of how various camera perspectives, most importantly dynamic ego-centric and static surveillance views, can be combined to visually analyze the behavior of people. These research results provide a sound foundation to develop novel means for monitoring people to ensure their safety (e.g., the well being of elderlies at home) and security (e.g., detecting malicious activities in shared public spaces).&nbsp;<br /><br /></p><br> <p>            Last Modified: 11/30/2016<br>      Modified by: Ko&nbsp;Nishino</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1353235/1353235_10274675_1480521758917_surface_all--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1353235/1353235_10274675_1480521758917_surface_all--rgov-800width.jpg" title="Simplified 3D scene model of a bookstore"><img src="/por/images/Reports/POR/2016/1353235/1353235_10274675_1480521758917_surface_all--rgov-66x44.jpg" alt="Simplified 3D scene model of a bookstore"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A simplified 3D representation of a shared scene computed from head-worn and scene fixed cameras.</div> <div class="imageCredit">Ko Nishino</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ko&nbsp;Nishino</div> <div class="imageTitle">Simplified 3D scene model of a bookstore</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1353235/1353235_10274675_1480521876488_localglobalaction--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1353235/1353235_10274675_1480521876488_localglobalaction--rgov-800width.jpg" title="Local and global actions"><img src="/por/images/Reports/POR/2016/1353235/1353235_10274675_1480521876488_localglobalaction--rgov-66x44.jpg" alt="Local and global actions"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Combinations of local (using body parts) and global (using whole body) actions of people recognized by a combination of head-worn and surveillance cameras.</div> <div class="imageCredit">Ko Nishino</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ko&nbsp;Nishino</div> <div class="imageTitle">Local and global actions</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this research program was to establish the theoretical and computational foundation for realizing an ``Omnipresent Vision'' system--a computational system that allows us to navigate, share, enhance, and understand the visual data captured by a number of fixed and moving cameras. The key idea is the integration of ``local'' and ``global'' views. The proliferation of wearable cameras has elicited a growing body of research that makes use of these local views (e.g., ego-centric vision). Our world, however, is also dominated with static cameras that feed persistent observations of specific locations. These ``global'' views share overlapping visual information with local views and also provide direct observations of their movements in the scene (e.g., trajectories of people wearing cameras). The global camera thus yields indispensable visual information to ``anchor'' the local views in the same scene. By integrating the local and global views, we can accurately localize all views in space and time, which opens new means to consolidate and enrich the visual data captured by the two distinct sets of cameras. The two sets of views also provide different perspectives on the actions and events prompted by the social behavior and interactions of the people in the scene. By leveraging the disparate but coherent visual cues captured in these different views, we may delineate and recognize the scene activities and interpret them to deduce the semantical context of the dynamic scene. In other words, the multi-faceted visual observation gives us a better chance of accurately understanding the people and the scene. The research program let the investigators explore how to establish a space-time platform to anchor the local and global views to enable visual analysis of human behavior in the space, including those who are and are not wearing cameras. This research led to the derivation of a novel framework for reconstructing a simplified 3D geometric representation of the space from the camera views. The research program also enabled the investigators to explore how the different camera perspectives, including dynamic cameras worn and capturing personal perspectives and static scene-fixed cameras capturing overhead perspectives, can be combined to better model the body movements and underlying intentions of people in the space. Finally, research on recovering other physical properties of the scene, particularly pertaining to what the scene surfaces are made of (i.e., materials), have also been studied as part of this research program. Collectively, these research efforts have significantly advanced our understanding of how various camera perspectives, most importantly dynamic ego-centric and static surveillance views, can be combined to visually analyze the behavior of people. These research results provide a sound foundation to develop novel means for monitoring people to ensure their safety (e.g., the well being of elderlies at home) and security (e.g., detecting malicious activities in shared public spaces).          Last Modified: 11/30/2016       Submitted by: Ko Nishino]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
