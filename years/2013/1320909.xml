<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Effective Augmented Reality Depth Representation Methods and Accuracy Evaluations Inspired by Medical Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>498233.00</AwardTotalIntnAmount>
<AwardAmount>498233</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Augmented reality (AR) systems, which are computer systems that enhance the viewing of physical objects in the world with computer data, are currently held back from widespread use for many real-world applications because of the unsolved human-computer interaction problem of how to accurately convey to a person how far away from that person a computer-generated object is intended to appear.  People using AR systems routinely misjudge the depth of AR-presented objects.   This is especially true for AR objects that should appear to be located behind opaque occluding surfaces; in this case AR should produce an "x-ray vision" perceptual experience that makes the occluding surface appear to become transparent.  The perceptual phenomena that underlie this problem relate to (a) conflicting depth cues that naturally arise with AR technology, especially incorrect occlusion cues in optical "x-ray vision" AR, (b) conflicting findings from techniques that have been developed to measure depth perception within reaching distance, and (c) the role of practice and feedback in training to correct these depth misjudgments.&lt;br/&gt;&lt;br/&gt;This project will evaluate AR depth representation methods and explain the underlying phenomena, with an emphasis on medical AR tasks and applications.  The project will develop and evaluate a head-worn haploscope to allow researchers to study the depth cues of accommodation and vergence AR.  The project will create and evaluate vergence-based methods for rendering AR information in depth; that is, techniques in which people can control the appearance of computer data inside of a physical object by rotating their eyes as is needed to look at near and far objects.  The researchers on this project will collaborate with experts on the use of AR for medical applications to develop new vergence-based techniques for AR "x-ray vision"  in the medical domain.  &lt;br/&gt;&lt;br/&gt;Broader Impacts: Vergence-based AR applications have the potential to improve health outcomes for a broad array of medical procedures, and also to improve human capabilities in task domains such as manufacturing and equipment maintenance.  This project will hasten the timeframe for successfully developing and deploying such applications.  Students working on this project will be trained in an interdisciplinary context that rigorously studies the intimate interplay between computer graphics and human perception.  The interdisciplinary and human-centered aspects of the project will help to recruit students who might otherwise be less likely to gravitate to computer science.</AbstractNarration>
<MinAmdLetterDate>09/03/2013</MinAmdLetterDate>
<MaxAmdLetterDate>03/15/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320909</AwardID>
<Investigator>
<FirstName>J. Edward</FirstName>
<LastName>Swan II</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>J. Edward Swan II</PI_FULL_NAME>
<EmailAddress>swan@cse.msstate.edu</EmailAddress>
<PI_PHON>6623124411</PI_PHON>
<NSF_ID>000356075</NSF_ID>
<StartDate>09/03/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Mississippi State University</Name>
<CityName>MISSISSIPPI STATE</CityName>
<ZipCode>397629662</ZipCode>
<PhoneNumber>6623257404</PhoneNumber>
<StreetAddress>PO Box 6156</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Mississippi</StateName>
<StateCode>MS</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MS03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>075461814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MISSISSIPPI STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>075461814</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Mississippi State University]]></Name>
<CityName/>
<StateCode>MS</StateCode>
<ZipCode>397629637</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Mississippi</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MS03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~170439</FUND_OBLG>
<FUND_OBLG>2014~327794</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project studied&nbsp;<em>augmented reality</em> (AR), a computer display technique that places virtual, computer-generated objects into real world settings.&nbsp;&nbsp;For example, modern smartphones allow virtual furniture to be placed in a house, and answer questions such as "will this table fit in this part of my kitchen?"&nbsp;&nbsp;This project addressed similar questions that arise from medical AR applications, where the virtual objects come from medical scans and medical equipment, and the goal is for doctors to see the virtual objects within a patient's body.&nbsp;&nbsp;If this succeeds, the doctor has a kind of x-ray vision, and the human body appears to be transparent.</p> <p>The project involved designing and building AR displays, which allowed questions to be asked about how the human visual system reacts to seeing virtual AR objects.&nbsp;&nbsp;The project also used commercially-available AR displays.&nbsp;A series of experiments was conducted, which measured some of the ways that the human visual system sees AR objects.&nbsp;With x-ray vision, a long-standing problem is how to represent virtual objects so that they appear to be located at the right depth within a human body (or other solid object).&nbsp;&nbsp;The experiments determined that the angle of an observer's eyes exhibits a constant error under certain common visual conditions, but that this error can be successfully eliminated.&nbsp;&nbsp;</p> <p>When human observers see a real, solid object, such as a coffee cup, the object appears to have a definite spatial location.&nbsp;&nbsp;Perhaps it is sitting on a desk.&nbsp;&nbsp;For AR, another long-standing problem is how to draw a virtual AR object that has a similarly definite spatial location.&nbsp;&nbsp;Perhaps we wish to put a virtual AR coffee cup next to the real coffee cup, and we want the location of both cups to appear equally solid and real.&nbsp;&nbsp;For medical AR applications, we want to place virtual medical data and virtual medical instruments in locations that also appear solid and real.&nbsp;&nbsp;This project developed and experimentally validated new methods for calibrating AR displays, which led to increased accuracy in the perceived location of AR objects.&nbsp;</p> <p>The intellectual merit of this project was to develop additional knowledge about how the human visual system perceives the location of virtual AR objects, especially within the reaching distances used for most medical procedures.&nbsp;&nbsp;This kind of knowledge was developed, and was applied to validated methods for placing virtual AR objects in depth and at specific locations.&nbsp;</p> <p>A broader impact was for AR medical applications to have improved depth and location accuracy.&nbsp;&nbsp;During the time period of this project, commercial AR technology advanced significantly.&nbsp;&nbsp;Out of many AR displays, the Microsoft HoloLens and Magic Leap One have had the largest impact.&nbsp;&nbsp;These devices, and the instructions given to AR application developers, suggest that findings from this project, as well as related projects within the community of applied perceptual AR research, have positively influenced the design of this generation of AR devices.&nbsp;&nbsp;It is likely that these devices display more accurately-perceived virtual AR objects than what would otherwise have been the case.&nbsp;</p> <p>Another important broader impact has been the education of the students who were supported by this project.&nbsp;Nine graduate students were supported, all of whom studied computer science.&nbsp;&nbsp;Four have pursued PhD degrees (one has graduated), and six MS degrees (five have graduated).&nbsp;&nbsp;The graduated students are working at Google, Radiance Technologies, Green Mountain Technology, and Torch Technology.&nbsp;&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/04/2021<br>      Modified by: J. Edward&nbsp;Swan Ii</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736340139_Image-1-200--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736340139_Image-1-200--rgov-800width.jpg" title="The Augmented Reality (AR) haploscope."><img src="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736340139_Image-1-200--rgov-66x44.jpg" alt="The Augmented Reality (AR) haploscope."></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Augmented Reality (AR) haploscope.  The physical design allows independent adjustment of vergence angle and focal distance.</div> <div class="imageCredit">Gurjot Singh, Stephen R. Ellis, J. Edward Swan II</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">J. Edward&nbsp;Swan Ii</div> <div class="imageTitle">The Augmented Reality (AR) haploscope.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736413838_Image-2--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736413838_Image-2--rgov-800width.jpg" title="Ray diagram of one side of the haploscope."><img src="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736413838_Image-2--rgov-66x44.jpg" alt="Ray diagram of one side of the haploscope."></a> <div class="imageCaptionContainer"> <div class="imageCaption">This diagram showcases the path of the virtual object as it is generated by the monitor, shrunk by the minimization lens, collimated by the collimating lens, set to a specific focal distance by the accommodation lens, and finally reflected directly into a user's eye by the beamsplitter.</div> <div class="imageCredit">Nate Phillips, Kristen Massey, Mohammed Safayet Arefin, J. Edward Swan II</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">J. Edward&nbsp;Swan Ii</div> <div class="imageTitle">Ray diagram of one side of the haploscope.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736492407_Image-3--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736492407_Image-3--rgov-800width.jpg" title="Model of constant change in vergence angle."><img src="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736492407_Image-3--rgov-66x44.jpg" alt="Model of constant change in vergence angle."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Model of constant change in vergence angle. When an AR object is seen through a display focused at infinity, which is common for head-up AR displays in some domains, the vergence angle of the eyes is biased by a constant angular amount, which results in larger depth errors at farther distances.</div> <div class="imageCredit">Gurjot Singh, Stephen R. Ellis, J. Edward Swan II</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">J. Edward&nbsp;Swan Ii</div> <div class="imageTitle">Model of constant change in vergence angle.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736571674_Image-4--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736571674_Image-4--rgov-800width.jpg" title="Evidence for model of constant change in vergence angle."><img src="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736571674_Image-4--rgov-66x44.jpg" alt="Evidence for model of constant change in vergence angle."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Evidence for model of constant change in vergence angle. When an AR object was seen through a display focused at infinity, 30 observers exhibited a constant change in vergence angle at tested distances.</div> <div class="imageCredit">Gurjot Singh, Stephen R. Ellis, J. Edward Swan II</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">J. Edward&nbsp;Swan Ii</div> <div class="imageTitle">Evidence for model of constant change in vergence angle.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736657256_Image-5--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736657256_Image-5--rgov-800width.jpg" title="Wearable AR hardware to study AR calibration methods."><img src="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736657256_Image-5--rgov-66x44.jpg" alt="Wearable AR hardware to study AR calibration methods."></a> <div class="imageCaptionContainer"> <div class="imageCaption">Two generations of an AR display system that developed a novel eye calibration method based on eye-tracking cameras.  The method was compared to standard manual calibration techniques.</div> <div class="imageCredit">Kenneth R. Moser, Yuta Itoh, J. Edward Swan II</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">J. Edward&nbsp;Swan Ii</div> <div class="imageTitle">Wearable AR hardware to study AR calibration methods.</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736746216_Image-6--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736746216_Image-6--rgov-800width.jpg" title="Measuring eye location through AR calibration methods."><img src="/por/images/Reports/POR/2021/1320909/1320909_10274443_1609736746216_Image-6--rgov-66x44.jpg" alt="Measuring eye location through AR calibration methods."></a> <div class="imageCaptionContainer"> <div class="imageCaption">The purpose of AR calibration is to measure the eye locations relative to the AR display. This experiment measured eye locations from different postures, distributions, and number of calibration alignments.  Environmental distribution causes notable errors along the Y axis.</div> <div class="imageCredit">Kenneth R. Moser, Mohammed Safayet Arefin, J. Edward Swan II</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">J. Edward&nbsp;Swan Ii</div> <div class="imageTitle">Measuring eye location through AR calibration methods.</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project studied augmented reality (AR), a computer display technique that places virtual, computer-generated objects into real world settings.  For example, modern smartphones allow virtual furniture to be placed in a house, and answer questions such as "will this table fit in this part of my kitchen?"  This project addressed similar questions that arise from medical AR applications, where the virtual objects come from medical scans and medical equipment, and the goal is for doctors to see the virtual objects within a patient's body.  If this succeeds, the doctor has a kind of x-ray vision, and the human body appears to be transparent.  The project involved designing and building AR displays, which allowed questions to be asked about how the human visual system reacts to seeing virtual AR objects.  The project also used commercially-available AR displays. A series of experiments was conducted, which measured some of the ways that the human visual system sees AR objects. With x-ray vision, a long-standing problem is how to represent virtual objects so that they appear to be located at the right depth within a human body (or other solid object).  The experiments determined that the angle of an observer's eyes exhibits a constant error under certain common visual conditions, but that this error can be successfully eliminated.    When human observers see a real, solid object, such as a coffee cup, the object appears to have a definite spatial location.  Perhaps it is sitting on a desk.  For AR, another long-standing problem is how to draw a virtual AR object that has a similarly definite spatial location.  Perhaps we wish to put a virtual AR coffee cup next to the real coffee cup, and we want the location of both cups to appear equally solid and real.  For medical AR applications, we want to place virtual medical data and virtual medical instruments in locations that also appear solid and real.  This project developed and experimentally validated new methods for calibrating AR displays, which led to increased accuracy in the perceived location of AR objects.   The intellectual merit of this project was to develop additional knowledge about how the human visual system perceives the location of virtual AR objects, especially within the reaching distances used for most medical procedures.  This kind of knowledge was developed, and was applied to validated methods for placing virtual AR objects in depth and at specific locations.   A broader impact was for AR medical applications to have improved depth and location accuracy.  During the time period of this project, commercial AR technology advanced significantly.  Out of many AR displays, the Microsoft HoloLens and Magic Leap One have had the largest impact.  These devices, and the instructions given to AR application developers, suggest that findings from this project, as well as related projects within the community of applied perceptual AR research, have positively influenced the design of this generation of AR devices.  It is likely that these devices display more accurately-perceived virtual AR objects than what would otherwise have been the case.   Another important broader impact has been the education of the students who were supported by this project. Nine graduate students were supported, all of whom studied computer science.  Four have pursued PhD degrees (one has graduated), and six MS degrees (five have graduated).  The graduated students are working at Google, Radiance Technologies, Green Mountain Technology, and Torch Technology.            Last Modified: 01/04/2021       Submitted by: J. Edward Swan Ii]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
