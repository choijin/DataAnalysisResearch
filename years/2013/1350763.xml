<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Towards Human Centered Visual Understanding: Exploring the Intended and Interpreted Meaning of Images in Social Multimedia</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>199170.00</AwardTotalIntnAmount>
<AwardAmount>199170</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project explores a new direction in computer vision, which is to model the context dependent visual semantics associated with images in social multimedia. The context dependent visual semantics, e.g., the intended and perceived sentiment of an image in social multimedia, are dynamically formed based on the various contextual information associated with it. This is different from the static visual semantics that conventional computer vision research focused on studying, such as the object category presented in the image.&lt;br/&gt;&lt;br/&gt;The project develops a set of new networked and context aware probabilistic latent semantic models, which integrate situated contextual information into visual content analysis for modeling context dependent visual semantics. The research team is verifying two hypotheses: 1) the context dependent semantics needs to be holistically modeled and jointly inferred from a collection of related images; and 2) related context dependent visual semantics, such as intended and perceived meaning of an image, also needs to be jointly modeled for more robust recognition.&lt;br/&gt;&lt;br/&gt;The project is integrated with education through training graduate and undergraduate students. The outcome of the research can be applied to many domains, such as targeted online advertisements; open source information analysis and social event prediction; and social multimedia security.</AbstractNarration>
<MinAmdLetterDate>09/06/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/09/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1350763</AwardID>
<Investigator>
<FirstName>Hong</FirstName>
<LastName>Man</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hong Man</PI_FULL_NAME>
<EmailAddress>hman@stevens.edu</EmailAddress>
<PI_PHON>2012165038</PI_PHON>
<NSF_ID>000431444</NSF_ID>
<StartDate>09/14/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gang</FirstName>
<LastName>Hua</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gang Hua</PI_FULL_NAME>
<EmailAddress>gang.hua@stevens.edu</EmailAddress>
<PI_PHON>2012168073</PI_PHON>
<NSF_ID>000602979</NSF_ID>
<StartDate>09/06/2013</StartDate>
<EndDate>09/14/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stevens Institute of Technology</Name>
<CityName>HOBOKEN</CityName>
<ZipCode>070305991</ZipCode>
<PhoneNumber>2012168762</PhoneNumber>
<StreetAddress>CASTLE POINT ON HUDSON</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>064271570</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>STEVENS INSTITUTE OF TECHNOLOGY (INC)</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>064271570</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stevens Institute of Technology]]></Name>
<CityName>Hoboken</CityName>
<StateCode>NJ</StateCode>
<ZipCode>070305991</ZipCode>
<StreetAddress><![CDATA[Castle Point on Hudson]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~199170</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This research aims at developing a computational framework for recognizing the intended and interpreted meaning of images in social media. The main focus of this project is the systematic study of a set of networked and context aware latent topic models, which integrate situated contextual information into visual content analysis for modeling context dependent visual semantics.</p> <p>&nbsp;</p> <p>The outcomes of this study consist of three major components, including a new mixture statistic metric learning (MSML) method for recognition of human actions and facial expressions in images and short video clips, a new multi-scale text analysis method based on set of sentiment parts (SSP) model for text sentiment analysis on short social media messages, and a joint visual and textual sentiment analysis framework to effectively bridge sentiment semantics between visual and textual modalities.</p> <p>Environmental objects and background textures in real-world video sequences often pose great challenges for human action and facial expression recognition. A mixture statistic metric learning (MSML) is introduced for recognizing human actions and facial expressions in realistic ``in the wild'' scenarios. In this proposed method, multiple statistics, including temporal means and covariance matrices as well as parameters of spatial Gaussian mixture distributions, are explicitly mapped to or generated on symmetric positive definite (SPD) Riemannian manifolds. An implicit mixture of Mahalanobis metrics is learned from the Riemannian manifolds. The learned metrics confine similar input pairs into local neighborhoods and separate dissimilar input pairs to relatively orthogonal areas on a regularized manifold. The proposed metric learning method also explores the prior distributions within the multiple statistics in the video sequences. This method is tested on five action video datasets (KTH, Weizmann, UCF101, HMDB51, and YouTube) and three facial expression datasets (CK+, MMI, and AFEW). The proposed method consistently outperforms many state-of-the-art methods on all action and facial expression recognition tasks in both recognition accuracy and computational efficiency.</p> <p>&nbsp;</p> <p>Text messages with complex sentiment expressions generally pose great challenges in sentiment analysis. In this study, the MSML approach, in which each concept/topic is modeled through a manifold embedding followed by an optimal features/metrics learning, is also adapted to sentiment classification within multiscale partitions of sentences and short paragraphs. A Set of Sentiment Parts (SSP) is first introduced to express sentiment features in different contexts of varying scales. A statistic combination is then determined by analyzing canonical correlations on Riemannian manifolds. A metric learning method is designed to keep the orthogonality within Riemannian point pairs. The nearest neighbor method is finally used to classify sentiments of SSP. Five sentiment datasets (sentiment140, Amazon-cells, yelp, Internet Movie Database and opinosis1.0), and two topic categorization text datasets (Reuters-21578 data set and SMS spam message data set) are used in our experiments. Highly competitive performance is achieved by the proposed method.</p> <p>&nbsp;</p> <p>Finally a joint visual-textual descriptor learning method is proposed that can effectively fuse the sentiment information from these two modalities. A set of affective visual features is firstly explored with the guidance from the theories of art and psychology. An affective visual descriptor, namely bag of affective words (BoAW), is then introduced to provide basic visual sentiment cues. A structured random forest model incorporated with the class-activation-mapping (CAM) technique is proposed to estimate the conditional probabilities of adjectives and nouns pairs (ANPs) given an image represented by it BoAWs. Subsequently, a set of sentiment parts (SSP) feature for text sentiment analysis is constructed at the same level of abstraction as the BoAWs. The resulting visual BoAWs and textual SSP descriptors are represented on a unified manifold space. Metric learning on the manifold kernels is applied for final sentiment analysis. In this study, the convolutional neural network (CNN) based CAM is pre-trained on ILSVRC 2014 dataset. It is then re-trained on the ANP labeled MVSO affective visual dataset. The global average pooling (GAP) layer of CAM is used for localizing discriminative image regions, and the fully-connected layer is for generating objective visual descriptors. The proposed method is evaluated on the MVSO affective visual image dataset, and a manually collected dataset of 300 tweets with mixed images and text messages in the experimental study. This new method performs comparably with the best state-of-the-art method on MVSO visual only dataset in producing the correct ANP pairs, and it outperforms all available benchmark methods on the mixed visual and text tweets dataset.&nbsp;&nbsp;</p> <p>&nbsp;</p> <p>In summary, a novel framework for joint visual and textual semantic presentation and inference has been developed for sentiment analysis. Considering the facts that vision is the main source in human knowledge acquisition and text is the main medium for human knowledge dissemination, this research has a great potential to advance the field of general artificial intelligence, which will be beneficial to education, community building and general economic and social developments.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/29/2018<br>      Modified by: Hong&nbsp;Man</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This research aims at developing a computational framework for recognizing the intended and interpreted meaning of images in social media. The main focus of this project is the systematic study of a set of networked and context aware latent topic models, which integrate situated contextual information into visual content analysis for modeling context dependent visual semantics.     The outcomes of this study consist of three major components, including a new mixture statistic metric learning (MSML) method for recognition of human actions and facial expressions in images and short video clips, a new multi-scale text analysis method based on set of sentiment parts (SSP) model for text sentiment analysis on short social media messages, and a joint visual and textual sentiment analysis framework to effectively bridge sentiment semantics between visual and textual modalities.  Environmental objects and background textures in real-world video sequences often pose great challenges for human action and facial expression recognition. A mixture statistic metric learning (MSML) is introduced for recognizing human actions and facial expressions in realistic ``in the wild'' scenarios. In this proposed method, multiple statistics, including temporal means and covariance matrices as well as parameters of spatial Gaussian mixture distributions, are explicitly mapped to or generated on symmetric positive definite (SPD) Riemannian manifolds. An implicit mixture of Mahalanobis metrics is learned from the Riemannian manifolds. The learned metrics confine similar input pairs into local neighborhoods and separate dissimilar input pairs to relatively orthogonal areas on a regularized manifold. The proposed metric learning method also explores the prior distributions within the multiple statistics in the video sequences. This method is tested on five action video datasets (KTH, Weizmann, UCF101, HMDB51, and YouTube) and three facial expression datasets (CK+, MMI, and AFEW). The proposed method consistently outperforms many state-of-the-art methods on all action and facial expression recognition tasks in both recognition accuracy and computational efficiency.     Text messages with complex sentiment expressions generally pose great challenges in sentiment analysis. In this study, the MSML approach, in which each concept/topic is modeled through a manifold embedding followed by an optimal features/metrics learning, is also adapted to sentiment classification within multiscale partitions of sentences and short paragraphs. A Set of Sentiment Parts (SSP) is first introduced to express sentiment features in different contexts of varying scales. A statistic combination is then determined by analyzing canonical correlations on Riemannian manifolds. A metric learning method is designed to keep the orthogonality within Riemannian point pairs. The nearest neighbor method is finally used to classify sentiments of SSP. Five sentiment datasets (sentiment140, Amazon-cells, yelp, Internet Movie Database and opinosis1.0), and two topic categorization text datasets (Reuters-21578 data set and SMS spam message data set) are used in our experiments. Highly competitive performance is achieved by the proposed method.     Finally a joint visual-textual descriptor learning method is proposed that can effectively fuse the sentiment information from these two modalities. A set of affective visual features is firstly explored with the guidance from the theories of art and psychology. An affective visual descriptor, namely bag of affective words (BoAW), is then introduced to provide basic visual sentiment cues. A structured random forest model incorporated with the class-activation-mapping (CAM) technique is proposed to estimate the conditional probabilities of adjectives and nouns pairs (ANPs) given an image represented by it BoAWs. Subsequently, a set of sentiment parts (SSP) feature for text sentiment analysis is constructed at the same level of abstraction as the BoAWs. The resulting visual BoAWs and textual SSP descriptors are represented on a unified manifold space. Metric learning on the manifold kernels is applied for final sentiment analysis. In this study, the convolutional neural network (CNN) based CAM is pre-trained on ILSVRC 2014 dataset. It is then re-trained on the ANP labeled MVSO affective visual dataset. The global average pooling (GAP) layer of CAM is used for localizing discriminative image regions, and the fully-connected layer is for generating objective visual descriptors. The proposed method is evaluated on the MVSO affective visual image dataset, and a manually collected dataset of 300 tweets with mixed images and text messages in the experimental study. This new method performs comparably with the best state-of-the-art method on MVSO visual only dataset in producing the correct ANP pairs, and it outperforms all available benchmark methods on the mixed visual and text tweets dataset.       In summary, a novel framework for joint visual and textual semantic presentation and inference has been developed for sentiment analysis. Considering the facts that vision is the main source in human knowledge acquisition and text is the main medium for human knowledge dissemination, this research has a great potential to advance the field of general artificial intelligence, which will be beneficial to education, community building and general economic and social developments.          Last Modified: 11/29/2018       Submitted by: Hong Man]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
