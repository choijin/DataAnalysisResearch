<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Impact of the Summer Institutes on Faculty Teaching and Student Achievement</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>572235.00</AwardTotalIntnAmount>
<AwardAmount>572235</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Connie Della-Piana</SignBlockName>
<PO_EMAI>cdellapi@nsf.gov</PO_EMAI>
<PO_PHON>7032925309</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This collaborative project, designed to determine the outcomes of a long standing faculty development effort, includes four institutions, Yale University, the University of Colorado, Boulder, Cornell University, and the University of Connecticut. Over the last decade, the National Academies Summer Institutes (SIs) have trained almost 1,000 faculty and instructional staff in 'scientific teaching,' an approach to STEM instruction based on evidence about how people learn (1, 2). The SIs are designed to teach effective use of active learning and assessment with attention to fostering learning by diverse students. They feature an iterative approach to examining the learning associated with newly introduced teaching practices. At present few studies systematically examine the outcomes of faculty development efforts. This project is a Phase III assessment of the impact of the SI training on participants' teaching practices and their students' outcomes. It entails developing a general system for asking questions about the outcomes of faculty development efforts and using the SIs as a base for testing that system. &lt;br/&gt;&lt;br/&gt;Intellectual Merit: This project is an important large-scale assessment of scientific teaching practices and their impact on student outcomes. The project is unique because it employs a novel systems evaluation approach to design and applies measures for the adoption of teaching practices and student achievement. This work will create knowledge about educational practices, instructor behavior change, and student outcomes, while developing universal evaluation tools that will be widely disseminated to enable others to evaluate their education initiatives.&lt;br/&gt;&lt;br/&gt;Broader Impacts: The results from the study of the institutes will help inform future faculty development efforts.  In addition, the resulting evaluation system will provide tools for designing and conducting evaluations of complex projects.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;1. J. Handelsman, S. Miller, C. Pfund, Scientific Teaching.  (WH Freeman &amp; Co, 2007).&lt;br/&gt;2. C. Pfund et al., Professional development. Summer institute to improve university science teaching. Science (New York, N.Y.) 324, 470 (2009).</AbstractNarration>
<MinAmdLetterDate>12/18/2013</MinAmdLetterDate>
<MaxAmdLetterDate>12/18/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1322861</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Trochim</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William M Trochim</PI_FULL_NAME>
<EmailAddress>wmt1@cornell.edu</EmailAddress>
<PI_PHON>6072550887</PI_PHON>
<NSF_ID>000315066</NSF_ID>
<StartDate>12/18/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148502820</ZipCode>
<StreetAddress><![CDATA[3M14 MVR Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7512</Code>
<Text>TUES-Type 3 Project</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~572235</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of NSF Award 1322861 was to &nbsp;support the Cornell Office for Research on Evaluation (CORE) in facilitating the Systems Evaluation Protocol (SEP) (see <a href="https://core.human.cornell.edu/">https://core.human.cornell.edu</a>) with the National Academy of Sciences Summer Institute (NAS SI). The purpose of completing the SEP was to collaboratively create a large-scale multi-phase evaluation plan for the student outcomes of the NAS SI. We facilitated all steps of developing an evaluation plan for NAS SI, resulting in a very complex cross-institutional program model, as well as plans for measurement, data collection, and analysis. As a result of this multi-institutional work, we concluded with considering how a protocol &ndash; similar to our SEP -&nbsp; and the products of working through that protocol, can benefit the management of science.</p> <p>Outcome 1 &ndash; Development of the NAS SI program model and evaluation plan. The steps of the SEP walk the program staff through several activities that contribute to creating a program model (such as thinking about stakeholder priorities and how to identify outcomes), developing evaluation or research questions, identifying relevant measures and literature, developing a data management and analysis plan, and determining how to use and report the findings.&nbsp;</p> <p>Evaluation planning starts with a logic and/or pathway model. A model is a depiction of the theory of how a program works.&nbsp; Logic models list program inputs, outputs, activities and outcomes in columns, but does not show which activities are related to which outcomes. Pathway models offer the detail on these relationships &ndash; where boxes for each activity are connected by an arrow to one or more short-term outcomes, and the arrow indicates causation or some influence resulting in that outcome.&nbsp; The outcomes connect progressively to other outcomes until the ultimate long-term outcome has been reached</p> <p>For this project, the model started with the NAS Summer Institute with undergraduate faculty outcomes, which lead to undergraduate student outcomes and undergraduate institution outcomes, which combine to build into a stronger workforce and globally competitive industry and research. The SEP helped program staff identify key outcomes and pathways that were important to study and research questions were identified. This set the NAS SI up so that multiple phases of evaluation can show the gradual movement of Scientific Teaching effects across the multi-level and interconnected nature of the program outcomes. In the current environment of transdisciplinary science, program managers are increasingly aware of the interconnected nature of their programs. Having a resource, like the SEP, will give them a structured method for planning that will make knowledge generation more feasible, and result in models useful for tracking evidence of success and communication with stakeholders. &nbsp;</p> <p>Outcome 2 &ndash; Three partner institutions were introduced first-hand to the SEP.&nbsp; At least one of those three continued to use the SEP and other evaluation resources moving forward. Twenty (20) undergraduate faculty across the country who had graduated from the NAS SI were exposed to the SEP modeling process and resources.&nbsp; Many of them found the methodology for understanding their programs useful, and they may continue to use these resources for their own class evaluations.</p> <p>Outcome 3 &ndash; Staff at NAS SI had put some previous efforts into modeling the different parts of their work.&nbsp; Pathway modeling started out by looking for relationships between all the sub-models, and pulling them together into a single &ldquo;uber&rdquo; model.&nbsp; The protocol for doing this in a consistent manner was tracked and documented.&nbsp; This may be helpful because there are multiple instances where organizations model different programs, but they may want to pull them together into a larger overarching program.&nbsp; Without keeping track of each step, it is easy to get lost in the process and have to start over at the beginning.&nbsp; A Protocol is a tool to help keep one on track.</p> <p>Outcome 4- In an effort to look at the relationship between the NAS SI model of student outcomes and Undergraduate class models of student outcomes, an in-depth qualitative analysis was conducted between: 1. student outcomes from three project-developed models of Undergraduate science courses; and 2. the student outcomes of the overarching NAS SI model. It was determined that the time commitment required for three reviewers to sufficiently review the 4,730 activity or outcome pairs was not beneficial in knowledge gained. Therefore, the process was not continued with the additional 17 class models.&nbsp; &nbsp;</p> <p>Outcome 5 - Finally, this work supported the development and improvement of some of the documents that are used to assist practitioners with developing a program evaluation plan, and which are available freely and publicly online at <a href="http://evaluationnetway.com/">http://evaluationnetway.com</a>. These can be used to result in improved evaluation of STEM programs generally.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/23/2019<br>      Modified by: William&nbsp;M&nbsp;Trochim</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of NSF Award 1322861 was to  support the Cornell Office for Research on Evaluation (CORE) in facilitating the Systems Evaluation Protocol (SEP) (see https://core.human.cornell.edu) with the National Academy of Sciences Summer Institute (NAS SI). The purpose of completing the SEP was to collaboratively create a large-scale multi-phase evaluation plan for the student outcomes of the NAS SI. We facilitated all steps of developing an evaluation plan for NAS SI, resulting in a very complex cross-institutional program model, as well as plans for measurement, data collection, and analysis. As a result of this multi-institutional work, we concluded with considering how a protocol &ndash; similar to our SEP -  and the products of working through that protocol, can benefit the management of science.  Outcome 1 &ndash; Development of the NAS SI program model and evaluation plan. The steps of the SEP walk the program staff through several activities that contribute to creating a program model (such as thinking about stakeholder priorities and how to identify outcomes), developing evaluation or research questions, identifying relevant measures and literature, developing a data management and analysis plan, and determining how to use and report the findings.   Evaluation planning starts with a logic and/or pathway model. A model is a depiction of the theory of how a program works.  Logic models list program inputs, outputs, activities and outcomes in columns, but does not show which activities are related to which outcomes. Pathway models offer the detail on these relationships &ndash; where boxes for each activity are connected by an arrow to one or more short-term outcomes, and the arrow indicates causation or some influence resulting in that outcome.  The outcomes connect progressively to other outcomes until the ultimate long-term outcome has been reached  For this project, the model started with the NAS Summer Institute with undergraduate faculty outcomes, which lead to undergraduate student outcomes and undergraduate institution outcomes, which combine to build into a stronger workforce and globally competitive industry and research. The SEP helped program staff identify key outcomes and pathways that were important to study and research questions were identified. This set the NAS SI up so that multiple phases of evaluation can show the gradual movement of Scientific Teaching effects across the multi-level and interconnected nature of the program outcomes. In the current environment of transdisciplinary science, program managers are increasingly aware of the interconnected nature of their programs. Having a resource, like the SEP, will give them a structured method for planning that will make knowledge generation more feasible, and result in models useful for tracking evidence of success and communication with stakeholders.    Outcome 2 &ndash; Three partner institutions were introduced first-hand to the SEP.  At least one of those three continued to use the SEP and other evaluation resources moving forward. Twenty (20) undergraduate faculty across the country who had graduated from the NAS SI were exposed to the SEP modeling process and resources.  Many of them found the methodology for understanding their programs useful, and they may continue to use these resources for their own class evaluations.  Outcome 3 &ndash; Staff at NAS SI had put some previous efforts into modeling the different parts of their work.  Pathway modeling started out by looking for relationships between all the sub-models, and pulling them together into a single "uber" model.  The protocol for doing this in a consistent manner was tracked and documented.  This may be helpful because there are multiple instances where organizations model different programs, but they may want to pull them together into a larger overarching program.  Without keeping track of each step, it is easy to get lost in the process and have to start over at the beginning.  A Protocol is a tool to help keep one on track.  Outcome 4- In an effort to look at the relationship between the NAS SI model of student outcomes and Undergraduate class models of student outcomes, an in-depth qualitative analysis was conducted between: 1. student outcomes from three project-developed models of Undergraduate science courses; and 2. the student outcomes of the overarching NAS SI model. It was determined that the time commitment required for three reviewers to sufficiently review the 4,730 activity or outcome pairs was not beneficial in knowledge gained. Therefore, the process was not continued with the additional 17 class models.     Outcome 5 - Finally, this work supported the development and improvement of some of the documents that are used to assist practitioners with developing a program evaluation plan, and which are available freely and publicly online at http://evaluationnetway.com. These can be used to result in improved evaluation of STEM programs generally.          Last Modified: 04/23/2019       Submitted by: William M Trochim]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
