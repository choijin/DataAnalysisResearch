<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small:  Learning and Testing Classes of Distributions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>471875.00</AwardTotalIntnAmount>
<AwardAmount>471875</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A long and successful line of research in machine learning deals with algorithms that learn from "labeled" data, where a target function is assumed to provide a label for each data point.  A major focus of theoretical work has been to develop efficient algorithms for learning different classes of target functions.  Recent years have witnessed a data explosion across many domains of science and society, but much of this newly available data consists simply of example points (DNA sequences, sensor readings, smartphone user locations, etc) without any labels.  A natural model of such scenarios is that data points are generated according to some unknown probability distribution (typically over an extremely large domain).  The goal of the proposed work is to study the learnability of different classes of probability distributions given access to samples drawn from the distributions.  This is closely analogous to the framework of learning from labeled data sketched above, but with probability distributions playing the role of functions as the objects to be learned.&lt;br/&gt;&lt;br/&gt;In this project, the PI will perform theoretical research on developing computationally efficient algorithms for learning and testing various natural types of probability distributions over extremely large domains. (Testing algorithms are algorithms which, instead of trying to accurately model an unknown distribution, have the more modest goal of testing whether or not the distribution has some property of interest.) Specific problems the PI will address include: (1) Developing efficient algorithms to learn and test univariate probability distributions that satisfy various natural kinds of "shape constraints" on the underlying probability density function.  Preliminary results suggest that dramatic improvements in efficiency may be possible for algorithms that are designed to exploit this type of structure.  (2) Developing efficient algorithms for learning and testing complex distributions that result from the aggregation of many independent simple sources of randomness.&lt;br/&gt;&lt;br/&gt;The algorithms that the PI will work to develop can provide useful modelling tools in data-rich environments and may serve as a "computational substrate" on which large-scale machine learning applications can be developed for real-world problems spanning a broad range of application areas.  Other important focuses of the grant are to train graduate students through research collaboration, disseminate research results through seminar talks, survey articles and other publications, and to continue ongoing outreach activities aimed at increasing interest in theoretical computer science topics in elementary school students.</AbstractNarration>
<MinAmdLetterDate>06/03/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319788</AwardID>
<Investigator>
<FirstName>Rocco</FirstName>
<LastName>Servedio</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rocco A Servedio</PI_FULL_NAME>
<EmailAddress>rocco@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397065</PI_PHON>
<NSF_ID>000232661</NSF_ID>
<StartDate>06/03/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York, NY</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~471875</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><pre>A well-known class of problems in statistics, usually referred to as  "density estimation" problems, involve inferring an unknown probability  distribution on the basis of samples drawn from the distribution.  The  main goal of this project was to study problems of this sort from the  vantage point of theoretical computer science.  The statistics  perspective typically focuses on how much data is required to perform  the relevant inference tasks; in theoretical computer science, the  emphasis is not only on data requirements, but on the computational  efficiency of algorithms to solve the relevant problems.  This project  aimed to develop provably correct and efficient algorithms for inferring  various types of probability distributions.  Below we describe two of  the main results achieved in the course of this project.  One significant result was a new and general approach for learning  "univariate" probability distributions (these are simply distributions  where each draw from the distribution is a single number; a very simple  example of such a distribution is the outcome obtained from rolling a  die, since the possible distinct outcomes are simply the numbers one  through six).  The PI developed a general algorithm that can efficiently  learn any univariate distribution whose probability density function can  be approximated by a collection of polynomial curves over different  portions of the domain (i.e. any distribution that can be approximated  by a piecewise polynomial function).  The algorithm obtained is provably  best possible in terms of the amount of data that it requires in order  to come up with a hypothesis distribution achieving a given accuracy  level.  The PI also showed that a wide range of different well-studied  types of probability distributions (such as k-modal distributions,  monotone hazard rate distributions, various types of mixture  distributions, etc) can be approximated by these piecewise polynomial  functions, so the new general method has broad applicability.  Another significant result was for learning probability distributions  that are obtained by aggregating many simple independent distributions.   As a concrete example, suppose that N (a very large number) of people  each separately roll their own k-sided die, where each die is labeled  with the numbers 1,...,k but may be skewed in an unknown and arbitrary  way (one die may output 2 one-quarter of the time and 4 three-quarters  of the time; another may be fair and output all values 1,...,k  equiprobably; etc).  The total of all die rolls is added up, and this  comprises a single draw from the aggregate distribution.  Is it possible  to learn this aggregate distribution efficiently?  The PI gave an  efficient algorithm to learn this aggregate distribution to high  accuracy from an essentially optimal (minimal) number of samples ---  perhaps surprisingly, the running time and amount of data required by  the algorithm is completely independent of the number N of component  independent distributions!  Ongoing work is underway to extend this  result to even more general types of aggregated distributions.    The efficient algorithms described above for learning different types of  probability distributions may serve as a useful "computational  substrate" for systems that do large-scale analysis of probabilistic  data.  In terms of broader impacts, besides the scientific impact of the  work, this award contributed to the development of human resources for  the STEM and academic workforce.  A number of Ph.D. students at Columbia  University received partial support and research training as a result of  this grant award. </pre> <p>&nbsp;</p><br> <p>            Last Modified: 08/23/2016<br>      Modified by: Rocco&nbsp;A&nbsp;Servedio</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[A well-known class of problems in statistics, usually referred to as  "density estimation" problems, involve inferring an unknown probability  distribution on the basis of samples drawn from the distribution.  The  main goal of this project was to study problems of this sort from the  vantage point of theoretical computer science.  The statistics  perspective typically focuses on how much data is required to perform  the relevant inference tasks; in theoretical computer science, the  emphasis is not only on data requirements, but on the computational  efficiency of algorithms to solve the relevant problems.  This project  aimed to develop provably correct and efficient algorithms for inferring  various types of probability distributions.  Below we describe two of  the main results achieved in the course of this project.  One significant result was a new and general approach for learning  "univariate" probability distributions (these are simply distributions  where each draw from the distribution is a single number; a very simple  example of such a distribution is the outcome obtained from rolling a  die, since the possible distinct outcomes are simply the numbers one  through six).  The PI developed a general algorithm that can efficiently  learn any univariate distribution whose probability density function can  be approximated by a collection of polynomial curves over different  portions of the domain (i.e. any distribution that can be approximated  by a piecewise polynomial function).  The algorithm obtained is provably  best possible in terms of the amount of data that it requires in order  to come up with a hypothesis distribution achieving a given accuracy  level.  The PI also showed that a wide range of different well-studied  types of probability distributions (such as k-modal distributions,  monotone hazard rate distributions, various types of mixture  distributions, etc) can be approximated by these piecewise polynomial  functions, so the new general method has broad applicability.  Another significant result was for learning probability distributions  that are obtained by aggregating many simple independent distributions.   As a concrete example, suppose that N (a very large number) of people  each separately roll their own k-sided die, where each die is labeled  with the numbers 1,...,k but may be skewed in an unknown and arbitrary  way (one die may output 2 one-quarter of the time and 4 three-quarters  of the time; another may be fair and output all values 1,...,k  equiprobably; etc).  The total of all die rolls is added up, and this  comprises a single draw from the aggregate distribution.  Is it possible  to learn this aggregate distribution efficiently?  The PI gave an  efficient algorithm to learn this aggregate distribution to high  accuracy from an essentially optimal (minimal) number of samples ---  perhaps surprisingly, the running time and amount of data required by  the algorithm is completely independent of the number N of component  independent distributions!  Ongoing work is underway to extend this  result to even more general types of aggregated distributions.    The efficient algorithms described above for learning different types of  probability distributions may serve as a useful "computational  substrate" for systems that do large-scale analysis of probabilistic  data.  In terms of broader impacts, besides the scientific impact of the  work, this award contributed to the development of human resources for  the STEM and academic workforce.  A number of Ph.D. students at Columbia  University received partial support and research training as a result of  this grant award.           Last Modified: 08/23/2016       Submitted by: Rocco A Servedio]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
