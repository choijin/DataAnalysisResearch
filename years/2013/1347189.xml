<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Eager:  HPC Virtualization with SR-IOV</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>98291.00</AwardTotalIntnAmount>
<AwardAmount>98291</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The recently introduced Single Root I/O Virtualization (SR-IOV)&lt;br/&gt;technique for InfiniBand and High Speed Ethernet provides native I/O&lt;br/&gt;virtualization capabilities and enables us to provision the internal&lt;br/&gt;PCI bus interface between multiple Virtual Machines (VMs). However,&lt;br/&gt;achieving near native throughput for HPC applications that use both&lt;br/&gt;point-to-point and collective operations on virtualized multi-core&lt;br/&gt;systems with SR-IOV presents a new set of challenges for the designers&lt;br/&gt;of high performance middleware, such as MPI.  In order to solve this&lt;br/&gt;problem, this project aims to address the following set of challenges:&lt;br/&gt;1) How to redesign MPI communication library to achieve efficient&lt;br/&gt;locality-aware communication and facilitate fair resource sharing on&lt;br/&gt;modern virtualized high performance clusters, with SR-IOV? 2) Can&lt;br/&gt;communication libraries be designed to deliver the best communication&lt;br/&gt;performance across different VM subscription policies and network&lt;br/&gt;communication modes? 3) What are the the challenges involved in&lt;br/&gt;designing support for advanced features such as, live migration,&lt;br/&gt;Quality of Service, and I/O storage virtualization?  and 4) What kind&lt;br/&gt;of benefits, in terms of performance and scalability, can be achieved&lt;br/&gt;by the proposed approach for HPC applications? A synergistic and&lt;br/&gt;comprehensive research plan is proposed to address the above&lt;br/&gt;challenges for HPC Virtualization on clusters with SR-IOV and study&lt;br/&gt;its impact for a set of HPC applications.</AbstractNarration>
<MinAmdLetterDate>09/05/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/05/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1347189</AwardID>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dhabaleswar K Panda</PI_FULL_NAME>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142925199</PI_PHON>
<NSF_ID>000487085</NSF_ID>
<StartDate>09/05/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName/>
<StateCode>OH</StateCode>
<ZipCode>432101206</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~98291</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Virtualized clusters with multi-core processors, high-performance<br />interconnects/protocols (such as InfiniBand, RDMA over Converged Enhanced Ethernet (RoCE) and 10-Gigabit Ethernet with iWARP), and Solid State Drives (SSDs) have been widely used in the enterprise computing domain for easy system management and efficient resource utilization. But in the High Performance Computing (HPC) domain, the adoption of virtualization still remains lower. One of the biggest hurdles in realizing this objective comes from lower performance of virtualized I/O devices, offered by virtualized computing environments. The recently introduced Single Root I/O Virtualization (SR-IOV) technique for InfiniBand and High Speed Ethernet provides native I/O virtualization capabilities and enables us to provision the internal PCI bus interfaces among multiple virtual machines.&nbsp; However, achieving near native throughput for HPC applications that use both point-to-point and collective operations in MPI on the upcoming virtualized multi-/many-core systems presents a new set of challenges for the designers of high performance middleware.</p> <p>To address the challenges for this project, we have investigated and designed novel approaches to utilize the underlying system capabilities to improve MPI communication and application performance and scalability with SR-IOV. The main goals we have achieved for this project are categorized as: 1) Efficient network virtualization with SR-IOV for MPI; 2) Locality aware communication for point-to-point and collective operations; 3) Integrated evaluation with HPC applications; and 4) Exploratory researches on storage virtualization and I/O scheduling, end-to-end Quality of Service, and live migration.&nbsp; We have designed and implemented locality aware point-to-point/collective communication for high-performance MVAPICH2 library over SR-IOV enabled InfiniBand clusters. We have systematically evaluated our design with MPI point-to-point, RMA,<br />collectives operations and typical HPC end applications or benchmarks. Compared with MPI over default SR-IOV, our design can significantly improve MPI communication performance in virtualized environments. The improvement for point-to-point can be up to 84% and 158% in terms of latency and bandwidth. In the meanwhile, our design can also deliver near native performance with only 3%-8% overhead in virtualized environments. For different message sizes, our design can improve the latency of various collectives, such as MPI_Bcast, MPI_Allgather, MPI_Allreduce, and MPI_Alltoall, by up to 68%, 76%, 61%, 29%, respectively. For NAS and P3DFFT, the benefits of our design brings can be up to 43% and 33%, respectively.&nbsp; We have also done many studies by technical survey and experiments with our new design for exploratory research tasks.</p> <p>The new SR-IOV-based designs are planned to be integrated into the MVAPICH2 library and made available to the HPC Virtualization community in the near future.&nbsp; In addition to the software distribution, the results have been presented at various conferences and events, such as Euro-Par &rsquo;14, HiPC &rsquo;14, and HP-CAST &rsquo;14.</p><br> <p>            Last Modified: 01/07/2015<br>      Modified by: Dhabaleswar&nbsp;K&nbsp;Panda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Virtualized clusters with multi-core processors, high-performance interconnects/protocols (such as InfiniBand, RDMA over Converged Enhanced Ethernet (RoCE) and 10-Gigabit Ethernet with iWARP), and Solid State Drives (SSDs) have been widely used in the enterprise computing domain for easy system management and efficient resource utilization. But in the High Performance Computing (HPC) domain, the adoption of virtualization still remains lower. One of the biggest hurdles in realizing this objective comes from lower performance of virtualized I/O devices, offered by virtualized computing environments. The recently introduced Single Root I/O Virtualization (SR-IOV) technique for InfiniBand and High Speed Ethernet provides native I/O virtualization capabilities and enables us to provision the internal PCI bus interfaces among multiple virtual machines.  However, achieving near native throughput for HPC applications that use both point-to-point and collective operations in MPI on the upcoming virtualized multi-/many-core systems presents a new set of challenges for the designers of high performance middleware.  To address the challenges for this project, we have investigated and designed novel approaches to utilize the underlying system capabilities to improve MPI communication and application performance and scalability with SR-IOV. The main goals we have achieved for this project are categorized as: 1) Efficient network virtualization with SR-IOV for MPI; 2) Locality aware communication for point-to-point and collective operations; 3) Integrated evaluation with HPC applications; and 4) Exploratory researches on storage virtualization and I/O scheduling, end-to-end Quality of Service, and live migration.  We have designed and implemented locality aware point-to-point/collective communication for high-performance MVAPICH2 library over SR-IOV enabled InfiniBand clusters. We have systematically evaluated our design with MPI point-to-point, RMA, collectives operations and typical HPC end applications or benchmarks. Compared with MPI over default SR-IOV, our design can significantly improve MPI communication performance in virtualized environments. The improvement for point-to-point can be up to 84% and 158% in terms of latency and bandwidth. In the meanwhile, our design can also deliver near native performance with only 3%-8% overhead in virtualized environments. For different message sizes, our design can improve the latency of various collectives, such as MPI_Bcast, MPI_Allgather, MPI_Allreduce, and MPI_Alltoall, by up to 68%, 76%, 61%, 29%, respectively. For NAS and P3DFFT, the benefits of our design brings can be up to 43% and 33%, respectively.  We have also done many studies by technical survey and experiments with our new design for exploratory research tasks.  The new SR-IOV-based designs are planned to be integrated into the MVAPICH2 library and made available to the HPC Virtualization community in the near future.  In addition to the software distribution, the results have been presented at various conferences and events, such as Euro-Par Æ14, HiPC Æ14, and HP-CAST Æ14.       Last Modified: 01/07/2015       Submitted by: Dhabaleswar K Panda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
