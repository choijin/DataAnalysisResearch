<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: SDA: Collaborative Research: A Scalable and Distributed System Framework for Compute-Intensive and Data-Parallel Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>375000.00</AwardTotalIntnAmount>
<AwardAmount>375000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tevfik Kosar</SignBlockName>
<PO_EMAI>tkosar@nsf.gov</PO_EMAI>
<PO_PHON>7032927992</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Whereas traditional high-performance computing (HPC) applications are computationally intensive, recent HPC applications require more data-intensive analysis and visualization to extract knowledge. In many cases, these applications execute the same computational algorithm as in the past (e.g., parallel search or parallel rendering) but now must do so for significantly larger data sets. For example, the life sciences, along with the cross-cutting area of scientific visualization, constitute an emerging category of HPC applications that not only perform sophisticated calculations but also ingest a sea of data. Running these new HPC data-parallel applications on today's computing platforms imposes new challenges and demands additional functionality.&lt;br/&gt;&lt;br/&gt;However, today's HPC platforms still adopt a compute-centric model and do not handle these new challenges well. Such a model often moves a large amount of data to various parallel computational processes. Consequently, long CPU wait times for I/O to complete and enormous data-movement overhead become major stumbling blocks to high performance and scalability. This project encompasses the creation of a scalable cross-layer software framework to enable both computationally intensive and data-intensive parallel HPC applications to run on distributed file systems. This framework consists of two interwoven research tasks: (1) an adaptive, data locality-aware, middleware system that dynamically schedules compute processes to access local data by monitoring physical data locations and (2) a framework that captures the computation and data I/O processing relationship from parallel applications and coordinates the scheduling of the corresponding process and I/O execution for maximum parallel efficiency. The success of this project contributes enhanced productivity and return on investment on HPC resources via the elimination of both CPU wait time and network transfer of frequently accessed data in scientific applications. An open-source, sustainable, and reusable software framework is delivered to speed-up the discovery and innovation process in areas such as bioinformatics, climate, high-energy physics, cosmology, astrophysics, and chromodynamics. The synergy in the two proposing institutions, Virginia Tech and the University of Central Florida, and their collaborating DOE national laboratories, will catalyze new and beneficial perspectives in the graduate education of students and prepare a 21st-century workforce in HPC.</AbstractNarration>
<MinAmdLetterDate>08/22/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/22/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1337244</AwardID>
<Investigator>
<FirstName>Jun</FirstName>
<LastName>Wang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jun Wang</PI_FULL_NAME>
<EmailAddress>Jun.Wang@ucf.edu</EmailAddress>
<PI_PHON>4078230449</PI_PHON>
<NSF_ID>000277527</NSF_ID>
<StartDate>08/22/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>The University of Central Florida Board of Trustees</Name>
<CityName>Orlando</CityName>
<ZipCode>328168005</ZipCode>
<PhoneNumber>4078230387</PhoneNumber>
<StreetAddress>4000 CNTRL FLORIDA BLVD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>150805653</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Central Florida]]></Name>
<CityName/>
<StateCode>FL</StateCode>
<ZipCode>328162362</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~375000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many scientists are exploring the possibilities of deploying applications with large scale of data on cloud computing platforms such as Amazon EC2 and Windows Azure. Recently, the successful deployment of eScience applications on clouds motivates us to deploy HPC analytics applications to the cloud, especially MapReduce enabled. The reason behind this lies in a fact that eScience applications and HPC analytics applications share some important features: terascale or peta-scale data size and high cost to run on single or several supercomputers or large platforms. However, HPC analytics applications bear some distinct characteristics such as complex data access patterns, interest locality, and which pose new challenges to its adoption of clouds.</p> <p>&nbsp;</p> <p>Apart from the applications, which deal with information retrieval and processing over the Internet (For example Google Search Application), scientific applications are also following the same trend of generating and processing petabytes of data. These applications come from a diverse range of fields such as Cosmology, Bioinformatics, Geology, Medicine, Astronomy, etc. If we assume that one physicist will run one simulation per day for a five day work week, then the approximate savings per year would be $19,100 per physicist. Since this unified storage framework avoids many unnecessary data transfer and guarantees minimum data movement, we estimate this unified framework could save millions of dollars of physicists&rsquo; labor in Los Alamos National Lab.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/05/2018<br>      Modified by: Jun&nbsp;Wang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many scientists are exploring the possibilities of deploying applications with large scale of data on cloud computing platforms such as Amazon EC2 and Windows Azure. Recently, the successful deployment of eScience applications on clouds motivates us to deploy HPC analytics applications to the cloud, especially MapReduce enabled. The reason behind this lies in a fact that eScience applications and HPC analytics applications share some important features: terascale or peta-scale data size and high cost to run on single or several supercomputers or large platforms. However, HPC analytics applications bear some distinct characteristics such as complex data access patterns, interest locality, and which pose new challenges to its adoption of clouds.     Apart from the applications, which deal with information retrieval and processing over the Internet (For example Google Search Application), scientific applications are also following the same trend of generating and processing petabytes of data. These applications come from a diverse range of fields such as Cosmology, Bioinformatics, Geology, Medicine, Astronomy, etc. If we assume that one physicist will run one simulation per day for a five day work week, then the approximate savings per year would be $19,100 per physicist. Since this unified storage framework avoids many unnecessary data transfer and guarantees minimum data movement, we estimate this unified framework could save millions of dollars of physicists? labor in Los Alamos National Lab.          Last Modified: 09/05/2018       Submitted by: Jun Wang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
