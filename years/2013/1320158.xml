<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Optimal Monte Carlo Estimation via Randomized Multilevel Methods</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>209999.00</AwardTotalIntnAmount>
<AwardAmount>209999</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research project will investigate a comprehensive set of tools to enable efficient and unbiased Monte Carlo methods in a wide range of settings such as: steady-state computations and stochastic differential equations (SDEs). The PIs extend the applicability and power of a recently introduced technique called multilevel Monte Carlo (MLMC), which has rapidly grown in popularity and has shown to be highly successful, particularly in the context of numerical solutions to SDEs. The PIs strategy rests on two basic ingredients. First, they abstract the main ideas of MLMC. This abstraction makes it clear that MLMC can be applied to many problem settings (beyond the SDE context), for example in problems such as: estimating steady-state expectations of Markov random fields, and solving distributional fixed point equations. Second, the PIs introduce a simple, yet powerful, extra randomization step. This randomization step will permit to not only completely delete the bias, which so far is present in every single application of the multilevel method, but it will also permit to more easily optimize parameters (often user-defined) that arise in classical multilevel applications. At the core of our abstraction of the MLMC method lies the construction of a suitable sequence of strong (almost sure) approximations under some metric. The freedom that is implicit in constructing such approximations yields a rich research program that touches upon many of the elements of modern probability, including random matrices, Markov random fields, mean field fixed point equations and Lyapunov stability.&lt;br/&gt;    &lt;br/&gt;    The PIs will investigate a methodology that enables high-performance computing in the context of simulation of stochastic systems. The PIs methodology will substantially extend a recently developed approach, called Multilevel Monte Carlo (MLMC), which has typically been applied only to compute numerical solutions of stochastic differential equations (SDEs). More generally, this research project addresses a wide range of problems that lie at the center of modern scientific computing, beyond the important setting of SDEs which arise in virtually all areas of modeling in engineering and science. For example, the PIs will generalize the MLMC approach to accurately perform so-called steady-state simulation for Markov chains indexed by trees. These computational problems arise very often in statistical inference applications, ranging from imaging to classification problems. The PIs research also improves upon the classical MLMC technique by optimizing its design and allowing the study of, for example, steady-state analysis of SDEs (i.e. combining traditional areas of study with new methodological applications). The PIs will in particular apply these optimized computational techniques to solve problems in service and manufacturing engineering. The PIs plan to develop a new jointly designed course, on the topic of this proposal, and the course material will be made available online to increase the dissemination and the potential applicability of the project's findings. The PIs will attempt to recruit high-quality personnel from under-represented groups and will disseminate the scientific output of the research via open access sites, in addition to the standard vehicles such as conferences and journal publications.</AbstractNarration>
<MinAmdLetterDate>07/23/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320158</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Glynn</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter W Glynn</PI_FULL_NAME>
<EmailAddress>glynn@stanford.edu</EmailAddress>
<PI_PHON>6507251624</PI_PHON>
<NSF_ID>000463253</NSF_ID>
<StartDate>07/23/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943054121</ZipCode>
<StreetAddress><![CDATA[475 Via Ortega, RM 357]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~68658</FUND_OBLG>
<FUND_OBLG>2014~70701</FUND_OBLG>
<FUND_OBLG>2015~70640</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 3"> <div class="section" style="background-color: rgb(100.000000%, 100.000000%, 100.000000%);"> <div class="layoutArea"> <div class="column"> <p><span style="font-size: 7.000000pt; font-family: 'ArialMT'; color: rgb(20.000000%, 20.000000%, 20.000000%);">The general goal of this project is to investigate a new computational technique, called "Randomized Multilevel Monte Carlo" (RMLMC), which allows producing unbiased estimators with optimal rate of convergence. The research combines a wide range of advanced tools in probability to obtain algorithms which overcome difficulties which have been unsolved for many years. </span></p> <p><span style="font-size: 7.000000pt; font-family: 'ArialMT'; color: rgb(20.000000%, 20.000000%, 20.000000%);">For example, the PIs deploy the theory of rough paths (which lies at the core of contemporary stochastic analysis), in combination with RMLMC to obtain the first procedure with simulates exactly and without any bias realizations of multidimensional stochastic differential equations. Prior to PIs research, only one-dimensional exact samplers existed, the first of which was produced in 2004. So, it took more than a dozen years and the tools developed in this research to understand how to exactly encode, in a computer, a multidimensional differential equation driven by white noise. Because these processes arise in a wide range of scientific and engineering applications, this proposal can potentially impact various areas of science and engineering in future years</span><span style="font-size: 7.000000pt; font-family: 'ArialMT'; color: rgb(20.000000%, 20.000000%, 20.000000%);">.</span></p> <div class="page" title="Page 3"></div> <p><span style="font-size: 7.000000pt; font-family: 'ArialMT'; color: rgb(20.000000%, 20.000000%, 20.000000%);">&nbsp;</span></p> <div class="page" title="Page 3"></div> <p><span style="font-size: 7.000000pt; font-family: 'ArialMT'; color: rgb(20.000000%, 20.000000%, 20.000000%);">&nbsp;</span></p> <p><span style="font-size: 7.000000pt; font-family: 'ArialMT'; color: rgb(20.000000%, 20.000000%, 20.000000%);">Another outcome which is important to emphasize from this project is the development of a technique to provide unbiased estimators for solutions of stochastic optimization problems. This is particularly interesting in Big Data applications in which a statistical model is fitted in the presence of massive data sets. Often, one uses a small universe (a random subsample) from the original massive data set and fits the model for the subsample. After averaging many subsamples one obtains an estimate of the statistical model, which represents the information on the whole universe (but with random noise from the sampling procedure). Although reasonable, this approach, unfortunately, is biased because the statistical method will overfit on every sample systematically. So, this systematic effect, applied to every subsample, is not removed after averaging. The research of this proposal develops a method which completely removes the systematic bias, therefore allowing one to fit statistical models to arbitrarily large data sets using easy parallelization. In simple words, the proposal introduces a subsampling procedure which does not suffer from systemic biases, therefore producing parameter estimates which can be averaged to produce an estimate which is equivalent to optimizing over the whole universe of the data. </span></p> </div> </div> </div> </div><br> <p>            Last Modified: 12/14/2018<br>      Modified by: Peter&nbsp;W&nbsp;Glynn</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     The general goal of this project is to investigate a new computational technique, called "Randomized Multilevel Monte Carlo" (RMLMC), which allows producing unbiased estimators with optimal rate of convergence. The research combines a wide range of advanced tools in probability to obtain algorithms which overcome difficulties which have been unsolved for many years.   For example, the PIs deploy the theory of rough paths (which lies at the core of contemporary stochastic analysis), in combination with RMLMC to obtain the first procedure with simulates exactly and without any bias realizations of multidimensional stochastic differential equations. Prior to PIs research, only one-dimensional exact samplers existed, the first of which was produced in 2004. So, it took more than a dozen years and the tools developed in this research to understand how to exactly encode, in a computer, a multidimensional differential equation driven by white noise. Because these processes arise in a wide range of scientific and engineering applications, this proposal can potentially impact various areas of science and engineering in future years.          Another outcome which is important to emphasize from this project is the development of a technique to provide unbiased estimators for solutions of stochastic optimization problems. This is particularly interesting in Big Data applications in which a statistical model is fitted in the presence of massive data sets. Often, one uses a small universe (a random subsample) from the original massive data set and fits the model for the subsample. After averaging many subsamples one obtains an estimate of the statistical model, which represents the information on the whole universe (but with random noise from the sampling procedure). Although reasonable, this approach, unfortunately, is biased because the statistical method will overfit on every sample systematically. So, this systematic effect, applied to every subsample, is not removed after averaging. The research of this proposal develops a method which completely removes the systematic bias, therefore allowing one to fit statistical models to arbitrarily large data sets using easy parallelization. In simple words, the proposal introduces a subsampling procedure which does not suffer from systemic biases, therefore producing parameter estimates which can be averaged to produce an estimate which is equivalent to optimizing over the whole universe of the data.            Last Modified: 12/14/2018       Submitted by: Peter W Glynn]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
