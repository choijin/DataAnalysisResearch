<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Collaborative Research: Self-Coordination in Cooperative Smart Camera Networks Incorporating System-On-Chip Reconfiguration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>12/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>340766.00</AwardTotalIntnAmount>
<AwardAmount>340766</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The number of cameras in our lives and the scale of camera systems are continuously increasing as technological advances and falling prices in camera systems create new opportunities and applications. In addition to personal uses, cameras are widely employed in military, public and commercial applications for surveillance and statistics gathering. There are an estimated 30 million surveillance cameras in the U.S. capturing 4 billion hours of footage a week. Besides the traditional use of cameras for surveillance purposes, projects such as Google Glass are driving the development of miniature and low-cost cameras with local processing and communication capabilities. For future camera systems, local intelligence and autonomous collaboration among components will provide the capability to solve more complex tasks, which requires a unifying perspective to simultaneously address the challenges of hardware/software co-design, real-time operation, high accuracy and self-coordination and self-adaptation in run-time.&lt;br/&gt;&lt;br/&gt;This project provides a holistic and novel approach for the design, deployment and self-coordination of a set of collaborative embedded smart cameras, with the goal of monitoring large areas with the highest accuracy and smallest latency. One objective is designing  synthesis approaches and computing infrastructure for the embedded smart cameras that allow hardware restructuring and systematic swapping of tasks between hardware and software on-the-fly. Another objective is to develop self-configuration approaches to autonomously adapt system behavior and optimally deal with run-time environmental changes, including node failures.&lt;br/&gt;&lt;br/&gt;This research is expected to enable development of new real-time, fully automated, collaborative and highly accurate camera systems by providing a systematic approach for the design and deployment of such systems, and testing new methods at laboratory and campus scales. Potential applications include smart surveillance systems, multi-camera-based driver assistance systems, assistance in nursing homes, quality control on production lines based on 3D reconstruction, and remote surgery. The project also integrates research with the undergraduate and graduate programs of two institutions and contributes towards increasing the involvement of under-represented groups through the University of Arkansas Engineering Career Awareness Program, Arkansas Louis Stokes Alliance for Minority Participation and George Washington Carver Project, and the WiSE program at Syracuse University.  Students from under-represented groups are to be recruited and involved in the design, implementation, and deployment of collaborative multi-camera networks.</AbstractNarration>
<MinAmdLetterDate>09/26/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302559</AwardID>
<Investigator>
<FirstName>Senem</FirstName>
<LastName>Velipasalar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Senem Velipasalar</PI_FULL_NAME>
<EmailAddress>svelipas@syr.edu</EmailAddress>
<PI_PHON>3154434418</PI_PHON>
<NSF_ID>000111075</NSF_ID>
<StartDate>09/26/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Syracuse University</Name>
<CityName>SYRACUSE</CityName>
<ZipCode>132441200</ZipCode>
<PhoneNumber>3154432807</PhoneNumber>
<StreetAddress>OFFICE OF SPONSORED PROGRAMS</StreetAddress>
<StreetAddress2><![CDATA[211 Lyman Hall]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY24</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002257350</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SYRACUSE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002257350</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Syracuse University]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>132441200</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>24</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY24</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~340766</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>During this project, research was conducted on self-coordinated target assignment and camera handoff in a distributed network of embedded smart cameras, person detection and re-identification across multiple images and videos, and improving target re-identification across non-overlapping camera views.</span></p> <p><span>We presented a new approach for self-coordination of cameras in a distributed network by using an analytically exact approach, derived from Boolean satisfiability, for object-to-camera assignment and handoff decision. We encoded both the target assignment and the camera handoff as a logic problem that is solved through a systematic search over a set of potential solutions. We leverage Answer Set Programming (ASP) to formally capture the problem of target assignment in a distributed network as a mapping problem of a set of tasks (targets) onto a set of resources (cameras). The method uses a novel version of the Answer Set Programing model to capture collaboration with incremental optimization and was implemented on an FPGA-based system-on-chip. While system performance in current camera assignment methodologies suffers from an increasing number of cameras and objects at run-time, ASP-based methods have been proven to achieve satisfactory performance for the synthesis of problems with hundreds of nodes.</span></p> <p><span>Person re-identification is indispensable for consistent labeling across different camera views. Most existing studies use static cameras, apply background subtraction to detect moving people, and then focus on the matching of detection results. However, if cameras are mobile or only single image frames (not videos) are available, then background subtraction cannot be used, and human detection needs to be performed on entire images. In this work, we focused on a crowd-sourcing scenario to find and follow person(s) of interest in the collected images/videos. We proposed an approach combining person detection with the GPU implementation of color histogram and Speeded Up Robust&nbsp;Features-based re-identification. Moreover, GeoTags were extracted from the videos captured by smart phones, and displayed on a map together with the timestamps.</span></p> <p><span>Person re-identification across cameras with disjoint views is a challenging task due to several reasons including illumination changes, and variations in camera viewpoint, camera intrinsic parameters and target appearance. Brightness transfer function (BTF) was introduced for inter-camera color calibration, and to improve the performance of person re-identification methods. We presented a novel method to model the appearance variation across different camera views. We proposed building a codebook of BTFs composed of the most representative BTFs for a camera pair. We also proposed an ordering and trimming criteria to avoid using all possible combinations of codewords for different color channels. Different experiments have been performed to demonstrate the performance improvement provided by the proposed method. The presented method outperforms other BTF-based approaches, including weighted BTF, cumulative BTF and mean BTF. Results show that the performance improvement provided by the proposed method becomes more pronounced when the training dataset size becomes smaller. Any target re-identification approach, which relies on appearance and color or brightness histograms, can benefit from the proposed approach thanks to obtaining an improved brightness transfer. Using the proposed method for brightness transfer beforehand, and then incorporating other features and using a variety of different distance metrics are expected to increase the accuracy even further. In order to support this argument, we have performed extensive experiments, and incorporated the proposed codebook of BTF into different state-of-the-art person re-identification approaches. Results show that the proposed method performs an improved brightness transfer across different camera views, and provides increased top ranked matching rate.</span></p> <p><span>In addition to our aforementioned work, we also developed an autonomous altitude measurement and landing area detection for indoor applications of unmanned aerial vehicles (UAVs). Fully autonomous navigation of UAVs, without relying on pre-installed tags or markers, still remains a challenge especially for GPS-denied areas and complex indoor environments. Robust altitude control and safe landing zone detection are two important tasks for indoor UAV applications. In this project, we proposed an approach for UAVs to control their altitudes, and autonomously detect safe landing zones without relying on any markers, special setups, or assuming that the environment is known. The proposed method employs both depth data and camera images to detect and also track the safe landing zones. We also developed a method for detecting heat leakages for building envelope inspection using UAVs. Accurate energy audits are essential to maximize energy savings and improvements in buildings. The efficiency of thermal insulation of buildings has direct impact on power consumption for heating and ventilation. Thermal leaks are critical defects on building insulation. In this project, we presented a methodology for employing UAVs to conduct rapid building envelope performance diagnostics and perform aerial mapping of energy flows. We proposed an algorithm that autonomously detects heat leakages from thermal images of buildings.</span></p> <p><span>The principal investigators on this project together edited a book titled "Distributed Embedded Smart Cameras: Architectures, Design and Applications", which was published in 2014.</span></p><br> <p>            Last Modified: 04/23/2018<br>      Modified by: Senem&nbsp;Velipasalar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ During this project, research was conducted on self-coordinated target assignment and camera handoff in a distributed network of embedded smart cameras, person detection and re-identification across multiple images and videos, and improving target re-identification across non-overlapping camera views.  We presented a new approach for self-coordination of cameras in a distributed network by using an analytically exact approach, derived from Boolean satisfiability, for object-to-camera assignment and handoff decision. We encoded both the target assignment and the camera handoff as a logic problem that is solved through a systematic search over a set of potential solutions. We leverage Answer Set Programming (ASP) to formally capture the problem of target assignment in a distributed network as a mapping problem of a set of tasks (targets) onto a set of resources (cameras). The method uses a novel version of the Answer Set Programing model to capture collaboration with incremental optimization and was implemented on an FPGA-based system-on-chip. While system performance in current camera assignment methodologies suffers from an increasing number of cameras and objects at run-time, ASP-based methods have been proven to achieve satisfactory performance for the synthesis of problems with hundreds of nodes.  Person re-identification is indispensable for consistent labeling across different camera views. Most existing studies use static cameras, apply background subtraction to detect moving people, and then focus on the matching of detection results. However, if cameras are mobile or only single image frames (not videos) are available, then background subtraction cannot be used, and human detection needs to be performed on entire images. In this work, we focused on a crowd-sourcing scenario to find and follow person(s) of interest in the collected images/videos. We proposed an approach combining person detection with the GPU implementation of color histogram and Speeded Up Robust Features-based re-identification. Moreover, GeoTags were extracted from the videos captured by smart phones, and displayed on a map together with the timestamps.  Person re-identification across cameras with disjoint views is a challenging task due to several reasons including illumination changes, and variations in camera viewpoint, camera intrinsic parameters and target appearance. Brightness transfer function (BTF) was introduced for inter-camera color calibration, and to improve the performance of person re-identification methods. We presented a novel method to model the appearance variation across different camera views. We proposed building a codebook of BTFs composed of the most representative BTFs for a camera pair. We also proposed an ordering and trimming criteria to avoid using all possible combinations of codewords for different color channels. Different experiments have been performed to demonstrate the performance improvement provided by the proposed method. The presented method outperforms other BTF-based approaches, including weighted BTF, cumulative BTF and mean BTF. Results show that the performance improvement provided by the proposed method becomes more pronounced when the training dataset size becomes smaller. Any target re-identification approach, which relies on appearance and color or brightness histograms, can benefit from the proposed approach thanks to obtaining an improved brightness transfer. Using the proposed method for brightness transfer beforehand, and then incorporating other features and using a variety of different distance metrics are expected to increase the accuracy even further. In order to support this argument, we have performed extensive experiments, and incorporated the proposed codebook of BTF into different state-of-the-art person re-identification approaches. Results show that the proposed method performs an improved brightness transfer across different camera views, and provides increased top ranked matching rate.  In addition to our aforementioned work, we also developed an autonomous altitude measurement and landing area detection for indoor applications of unmanned aerial vehicles (UAVs). Fully autonomous navigation of UAVs, without relying on pre-installed tags or markers, still remains a challenge especially for GPS-denied areas and complex indoor environments. Robust altitude control and safe landing zone detection are two important tasks for indoor UAV applications. In this project, we proposed an approach for UAVs to control their altitudes, and autonomously detect safe landing zones without relying on any markers, special setups, or assuming that the environment is known. The proposed method employs both depth data and camera images to detect and also track the safe landing zones. We also developed a method for detecting heat leakages for building envelope inspection using UAVs. Accurate energy audits are essential to maximize energy savings and improvements in buildings. The efficiency of thermal insulation of buildings has direct impact on power consumption for heating and ventilation. Thermal leaks are critical defects on building insulation. In this project, we presented a methodology for employing UAVs to conduct rapid building envelope performance diagnostics and perform aerial mapping of energy flows. We proposed an algorithm that autonomously detects heat leakages from thermal images of buildings.  The principal investigators on this project together edited a book titled "Distributed Embedded Smart Cameras: Architectures, Design and Applications", which was published in 2014.       Last Modified: 04/23/2018       Submitted by: Senem Velipasalar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
