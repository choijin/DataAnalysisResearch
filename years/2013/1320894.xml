<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: Statistical ranking theory without a canonical loss</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>223572.00</AwardTotalIntnAmount>
<AwardAmount>223572</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The problem of ranking objects occupies a central place in key technologies such as web search and recommendation systems. These technologies have a tremendous daily impact on the lives of millions of people. Moreover, the enormous scale of data on the web makes the use of machine learning especially attractive in constructing ranking algorithms.  A huge amount of research effort has been devoted to developing efficient ranking algorithms that can deal with a variety of data sets encountered in web search and recommendation systems.&lt;br/&gt;&lt;br/&gt;This project develops unifying mathematical theory that will provide a basis for understanding and categorizing existing algorithms and, more importantly, lead to deeper insights and new algorithms for the problem of learning to rank. The investigators also apply ranking algorithms to new domains.  For example, ranking chemical reactions based on their plausibility will help chemists discover much-needed reaction bases for technologies such as carbon dioxide reduction, and conversion of natural gas into gasoline. &lt;br/&gt;&lt;br/&gt;Fundamental advances in the statistical theory of ranking will be incorporated into undergraduate and graduate courses. Data sets and software developed will be made freely available to the scientific community. The investigators will also organize a workshop with a focus on interdisciplinary participation and involvement of under-represented groups in computer science and statistics.&lt;br/&gt;&lt;br/&gt;The primary technical challenge in developing statistical ranking theory is the absence of a universally agreed-upon loss functions for ranking. This is in contrast to classic machine learning problems such as classification and regression, where there are only a few natural possibilities for the loss function and these are well-understood theoretically. The project addresses this gap by investigating how different loss functions for ranking affect fundamental theoretical properties such as learnability, and by creating a theory of convex surrogates that is applicable when loss functions abound.  The project re-examines existing statistical literature on ranking with a computational lens.  This will enable development of flexible and efficient plug-in decision rules that model the conditional probability of labels given inputs.&lt;br/&gt;&lt;br/&gt;By incorporating the results of this research into courses and survey articles, the PIs help train a new generation of machine learning researchers and practitioners who will view ranking as a learning problem on par with classification and regression in mathematical depth as well as practical importance.  Theoretical guidance for practitioners formulating new algorithms for ranking will improve the most common applications on the web.</AbstractNarration>
<MinAmdLetterDate>08/08/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320894</AwardID>
<Investigator>
<FirstName>Pradeep</FirstName>
<LastName>Ravikumar</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pradeep K Ravikumar</PI_FULL_NAME>
<EmailAddress>pradeepr@cs.cmu.edu</EmailAddress>
<PI_PHON>4124775625</PI_PHON>
<NSF_ID>000553653</NSF_ID>
<StartDate>08/08/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787121532</ZipCode>
<StreetAddress><![CDATA[101 E. 27th Street, Suite 5.300]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~223572</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There have been tremendous recent successes in the use of machine learning methods across application domains. Most of these successful applications are in classification tasks, where the goal is to predict a categorical response, given input features. These successes are due in part to the well-developed statistical learning theory of classification, particularly binary classification, where we predict a binary response, with the 0-1 loss, which assigns a cost of zero if the prediction is correct, and one otherwise. The 0-1 loss is an instance of a loss function, which quantifies the goodness of any candidate prediction function, or classifier, given training data, and accordingly, can be used to come up with the ``optimal'' classifier given training data. What are the optimal classifiers given the loss function, and how do we estimate these from data in a computationally tractable manner, are some of the questions such a statistical learning theory enables us to answer.</p> <p><br />In this project, we aimed to extend this well-developed statistical theory for binary classification with the 0-1 loss, to the setting of ranking objects. Here, the goal is prediction as well, but in contrast to predicting a binary response (is it a cat or not?), we aim to predict a ranking (in what order should these five documents be ranked, in response to a query?). This task is difficult in part because the set of possible orders quickly becomes intractable as a function of the number of objects. For instance, the number of orderings of 60 objects is more than the number of atoms in the universe. But such ranking problems are nonetheless increasingly important in our digital world: ranging from ranking web-pages in response to queries, and ranking ads, to ranking posts in a user news-feed. Due in part to its importance, this has led to an entire sub-field of machine learning, called learning to rank. In contrast to binary classification, with its canonical 0-1 loss function, there is a huge (and growing) diversity of loss functions in the context of ranking. Accordingly, any analytical tools we develop that are highly specialized to a single evaluation metric is unlikely to be useful in the learning to rank landscape with its large number of equally popular loss functions.&nbsp;</p> <p><br />A key technical challenge of the project was thus to develop a unified framework to answer the following question: given any loss function, what are the optimal prediction functions, and how to estimate these optimal prediction function given training data in a consistent manner; so that with increasing samples, the estimator would converge to the optimal prediction function. Another crucial desideratum is that the estimator be computationally practical. We made the following contributions towards these technical challenges.</p> <p><br />We characterized optimal classifiers, as well as provided practical algorithms for estimating these from data, for a broad class of classification problems, including complex loss functions for binary classification, for multi-label problems, where any input could simultaneously have multiple labels, as well as for general structured prediction problems, where the goal is to predict a response that takes values in a finite but large set that has some structure. We also provided scalable classification algorithms for such general loss functions, even under the setting where the number of labels is extremely large. We also provided novel scalable algorithms for the classical problem of empirical risk minimization, for general loss functions: this is the standard approach used to estimate the optimal predictor given a loss function, and we provided approaches to make these computationally practical for general loss functions.</p> <p><br />We also provided a representation theory for listwise ranking functions: these are prediction functions that output a ranking of a set of inputs, by predicting ``scores'' for each of the inputs, such that a higher score entails that the input has a higher rank. We identified a curious phenomenon: most ranking functions used in practice were pointwise, that is, they scored each input independently, which unsurprisingly, is not optimal for general ranking loss functions. We provided a systematic representation theory (and corresponding parameterizations) of general listwise ranking functions.</p> <p><br />We also proposed and analyzed a new evaluation measure for clustering along the lines of the classical zero-one loss for classification. Clustering is the task of grouping a set of points, and might seem qualitatively different from the task of classification, which is the task of predicting a categorical response. Indeed, such clustering is an instance of what is known as unsupervised learning, since the training data does not come with labels specifying any target response, which in this case, would be the cluster or group assignments. We nonetheless provided commonalities with the task of classification, and extended some of our earlier results to this setting as well.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/24/2017<br>      Modified by: Pradeep&nbsp;K&nbsp;Ravikumar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There have been tremendous recent successes in the use of machine learning methods across application domains. Most of these successful applications are in classification tasks, where the goal is to predict a categorical response, given input features. These successes are due in part to the well-developed statistical learning theory of classification, particularly binary classification, where we predict a binary response, with the 0-1 loss, which assigns a cost of zero if the prediction is correct, and one otherwise. The 0-1 loss is an instance of a loss function, which quantifies the goodness of any candidate prediction function, or classifier, given training data, and accordingly, can be used to come up with the ``optimal'' classifier given training data. What are the optimal classifiers given the loss function, and how do we estimate these from data in a computationally tractable manner, are some of the questions such a statistical learning theory enables us to answer.   In this project, we aimed to extend this well-developed statistical theory for binary classification with the 0-1 loss, to the setting of ranking objects. Here, the goal is prediction as well, but in contrast to predicting a binary response (is it a cat or not?), we aim to predict a ranking (in what order should these five documents be ranked, in response to a query?). This task is difficult in part because the set of possible orders quickly becomes intractable as a function of the number of objects. For instance, the number of orderings of 60 objects is more than the number of atoms in the universe. But such ranking problems are nonetheless increasingly important in our digital world: ranging from ranking web-pages in response to queries, and ranking ads, to ranking posts in a user news-feed. Due in part to its importance, this has led to an entire sub-field of machine learning, called learning to rank. In contrast to binary classification, with its canonical 0-1 loss function, there is a huge (and growing) diversity of loss functions in the context of ranking. Accordingly, any analytical tools we develop that are highly specialized to a single evaluation metric is unlikely to be useful in the learning to rank landscape with its large number of equally popular loss functions.    A key technical challenge of the project was thus to develop a unified framework to answer the following question: given any loss function, what are the optimal prediction functions, and how to estimate these optimal prediction function given training data in a consistent manner; so that with increasing samples, the estimator would converge to the optimal prediction function. Another crucial desideratum is that the estimator be computationally practical. We made the following contributions towards these technical challenges.   We characterized optimal classifiers, as well as provided practical algorithms for estimating these from data, for a broad class of classification problems, including complex loss functions for binary classification, for multi-label problems, where any input could simultaneously have multiple labels, as well as for general structured prediction problems, where the goal is to predict a response that takes values in a finite but large set that has some structure. We also provided scalable classification algorithms for such general loss functions, even under the setting where the number of labels is extremely large. We also provided novel scalable algorithms for the classical problem of empirical risk minimization, for general loss functions: this is the standard approach used to estimate the optimal predictor given a loss function, and we provided approaches to make these computationally practical for general loss functions.   We also provided a representation theory for listwise ranking functions: these are prediction functions that output a ranking of a set of inputs, by predicting ``scores'' for each of the inputs, such that a higher score entails that the input has a higher rank. We identified a curious phenomenon: most ranking functions used in practice were pointwise, that is, they scored each input independently, which unsurprisingly, is not optimal for general ranking loss functions. We provided a systematic representation theory (and corresponding parameterizations) of general listwise ranking functions.   We also proposed and analyzed a new evaluation measure for clustering along the lines of the classical zero-one loss for classification. Clustering is the task of grouping a set of points, and might seem qualitatively different from the task of classification, which is the task of predicting a categorical response. Indeed, such clustering is an instance of what is known as unsupervised learning, since the training data does not come with labels specifying any target response, which in this case, would be the cluster or group assignments. We nonetheless provided commonalities with the task of classification, and extended some of our earlier results to this setting as well.             Last Modified: 11/24/2017       Submitted by: Pradeep K Ravikumar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
