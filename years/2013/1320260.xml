<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: Towards Modeling Source Separation from Measured Cortical Responses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>269993.00</AwardTotalIntnAmount>
<AwardAmount>269993</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will use new technologies for measuring brain activity to understand in detail how human listeners are able to separate competing, overlapping voices, and thereby to help design automatic systems capable of the same feat. Natural environments are full of overlapping sounds, and successful audio processing by both humans and machines relies on a fundamental ability to separate out sound sources of interest. This is commonly referred to as the "cocktail party effect," based on the ability of people to hear what a single person is saying despite the noisy background audio from other speakers. Despite the long history of research in hearing, this exceptional human capability for sound source separation is still poorly understood, and efforts to automatically separate overlapping voices by machine are correspondingly crude: although great advances have been made in robust processing of noisy speech by machine, separation of complex natural sounds (such as overlapping voices) remains a challenge. Advances in sensor technology now enable the modeling of this function in humans, giving an unprecedented, detailed view of sound representation processing in the brain.  This project works specifically with measurements of neuroelectric response made directly on the surface of the human cortex (currently with a 256-electrode sensor array) for patients awaiting neurosurgery. Using such measurements made for controlled mixtures of voices, the project will endeavor to both develop models of voice separation in the human cortex by reconstructing an approximation to the acoustic stimulus from the neural population response, and in the process learning the linear mapping between the neural response back to a spectrogram measure of the stimulus. To attempt to significantly improve the ability of machine algorithms to mimic human source separation capability, the project will also focus on a signal processing framework that supports experiments with different combinations of cues and strategies to optimize agreement with the recordings of neural activity. The engineering model is based on the Computational Auditory Scene Analysis (CASA) framework, a family of approaches that have shown competitive results for handling sound mixtures.</AbstractNarration>
<MinAmdLetterDate>09/08/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/08/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320260</AwardID>
<Investigator>
<FirstName>Nelson</FirstName>
<LastName>Morgan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nelson Morgan</PI_FULL_NAME>
<EmailAddress>morgan@icsi.berkeley.edu</EmailAddress>
<PI_PHON>5106662931</PI_PHON>
<NSF_ID>000185837</NSF_ID>
<StartDate>09/08/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Ellis</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel P Ellis</PI_FULL_NAME>
<EmailAddress>dpwe@ee.columbia.edu</EmailAddress>
<PI_PHON>2128548928</PI_PHON>
<NSF_ID>000093617</NSF_ID>
<StartDate>09/08/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>International Computer Science Institute</Name>
<CityName>Berkeley</CityName>
<ZipCode>947041345</ZipCode>
<PhoneNumber>5106662900</PhoneNumber>
<StreetAddress>2150 Shattuck Ave, Suite 1100</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>187909478</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>INTERNATIONAL COMPUTER SCIENCE INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[International Computer Science Institute]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947041159</ZipCode>
<StreetAddress><![CDATA[1947 Center Street, Suite 600]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~269993</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was a collaboration between the Chang Lab at UCSF (a research lab focused on understanding the mechanisms underlying human speech perception and production) and the Speech Group at ICSI (which has for many years worked to improve automatic speech recognition in the presence of obfuscating factors). In this collaboration, we studied and contrasted the behavior of neural circuits (in major language areas of the human cortex in awake human patients) with modern automatic speech recognition methods for speech. The ultimate goal would be to learn about the human mechanism for separating one person&rsquo;s speech from another&rsquo;s, which is commonly called the &ldquo;cocktail party effect&rdquo;, named for the ability of human hearing to focus on one person&rsquo;s speech from an acoustic mixture present in natural situations.</p> <p>In this project, specifically what we were able to do was to demonstrate that features extracted from electrocorticographic (ECoG) recordings (taken from sensors directly on the surface of an epilepsy patient&rsquo;s cortex) could classify speech sounds of a speaker the subject is focused on with significant robustness to the addition of an additional speaker. This would be in contrast to now-standard machine methods of speech sound classification, which degrade significantly in the presence of a second speaker&rsquo;s audio.</p> <p>The table below summarizes this result, showing the phone error rate for these cases:</p> <p>&nbsp;</p> <table border="1" cellspacing="0" cellpadding="0"> <tbody> <tr> <td width="148" valign="top"> <p>&nbsp;</p> </td> <td width="148" valign="top"> <p>Acoustic features</p> </td> <td width="148" valign="top"> <p>ECoG-based features</p> </td> </tr> <tr> <td width="148" valign="top"> <p>Single source</p> </td> <td width="148" valign="top"> <p>4.0%</p> </td> <td width="148" valign="top"> <p>56.8%</p> </td> </tr> <tr> <td width="148" valign="top"> <p>Mixed source</p> </td> <td width="148" valign="top"> <p>54.3%%</p> </td> <td width="148" valign="top"> <p>64.5%%</p> </td> </tr> </tbody> </table> <p>&nbsp;</p> <p>While the error rates for the ECoG-based features are, overall, higher than for the features taken directly from the acoustics, the relative degradation of the error rates for the mixed source (attending to one speaker in the presence of two) is much smaller for the neural case.</p> <p>Additional outcomes of this work included the development of a mixed-speaker multi-pitch tracker, and a preliminary system for speech separation using pitches that might come from such a tracker. However, the primary result was the one described above.</p> <p>&nbsp;The ECoG recordings and the ECoG-based features were generated by our UCSF partners, while the machine recognition methods were done within the ICSI lab. In the course of this work an important process was learning about the issues of a collaboration between a neuroscience laboratory and a speech engineering one. One challenge was overcoming technology transfer issues between two quite different laboratories. One positive outcome was learning that the obvious interests of each lab were not exclusive. That is, the lab ostensibly focused on neuroscience was really quite interested in the speech engineering results, while the speech engineering lab was quite interested in learning about brain function.</p> <p>Another important challenge was the limitation of the data &ndash; it is not straightforward to collect the kinds of data quantities that those of us in the speech community are used to, because in this case there are clinical constraints. Without much more data it is difficult to draw strong conclusions about the actual brain mechanisms involved, beyond the robustness that we were able to demonstrate. I hope that future work will be able to expand these results using larger data corpora, should they prove feasible to collect.</p><br> <p>            Last Modified: 09/26/2016<br>      Modified by: Nelson&nbsp;Morgan</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1320260/1320260_10276709_1474903501107_block_diagram--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1320260/1320260_10276709_1474903501107_block_diagram--rgov-800width.jpg" title="Process block diagram"><img src="/por/images/Reports/POR/2016/1320260/1320260_10276709_1474903501107_block_diagram--rgov-66x44.jpg" alt="Process block diagram"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Speech from the TIMIT and CRM databases were played to patients, ECoG was collected, and neural and acoustic signals were used to recognize phone strings.</div> <div class="imageCredit">Generated by our research team</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Nelson&nbsp;Morgan</div> <div class="imageTitle">Process block diagram</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was a collaboration between the Chang Lab at UCSF (a research lab focused on understanding the mechanisms underlying human speech perception and production) and the Speech Group at ICSI (which has for many years worked to improve automatic speech recognition in the presence of obfuscating factors). In this collaboration, we studied and contrasted the behavior of neural circuits (in major language areas of the human cortex in awake human patients) with modern automatic speech recognition methods for speech. The ultimate goal would be to learn about the human mechanism for separating one person?s speech from another?s, which is commonly called the "cocktail party effect", named for the ability of human hearing to focus on one person?s speech from an acoustic mixture present in natural situations.  In this project, specifically what we were able to do was to demonstrate that features extracted from electrocorticographic (ECoG) recordings (taken from sensors directly on the surface of an epilepsy patient?s cortex) could classify speech sounds of a speaker the subject is focused on with significant robustness to the addition of an additional speaker. This would be in contrast to now-standard machine methods of speech sound classification, which degrade significantly in the presence of a second speaker?s audio.  The table below summarizes this result, showing the phone error rate for these cases:              Acoustic features    ECoG-based features      Single source    4.0%    56.8%      Mixed source    54.3%%    64.5%%         While the error rates for the ECoG-based features are, overall, higher than for the features taken directly from the acoustics, the relative degradation of the error rates for the mixed source (attending to one speaker in the presence of two) is much smaller for the neural case.  Additional outcomes of this work included the development of a mixed-speaker multi-pitch tracker, and a preliminary system for speech separation using pitches that might come from such a tracker. However, the primary result was the one described above.   The ECoG recordings and the ECoG-based features were generated by our UCSF partners, while the machine recognition methods were done within the ICSI lab. In the course of this work an important process was learning about the issues of a collaboration between a neuroscience laboratory and a speech engineering one. One challenge was overcoming technology transfer issues between two quite different laboratories. One positive outcome was learning that the obvious interests of each lab were not exclusive. That is, the lab ostensibly focused on neuroscience was really quite interested in the speech engineering results, while the speech engineering lab was quite interested in learning about brain function.  Another important challenge was the limitation of the data &ndash; it is not straightforward to collect the kinds of data quantities that those of us in the speech community are used to, because in this case there are clinical constraints. Without much more data it is difficult to draw strong conclusions about the actual brain mechanisms involved, beyond the robustness that we were able to demonstrate. I hope that future work will be able to expand these results using larger data corpora, should they prove feasible to collect.       Last Modified: 09/26/2016       Submitted by: Nelson Morgan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
