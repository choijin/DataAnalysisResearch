<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC-NIE Networking Infrastructure: Accelerating Science for Nebraska</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>491871.00</AwardTotalIntnAmount>
<AwardAmount>491871</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project team, composed of campus administration, computer scientists and domain scientists, is working to bring 100 Gbps networking, an improved Science DMZ, and improved network research and development capabilities to the UNL campus. This serves projects at the Holland Computing Center (HCC), which provides centrally managed computational research resources and expert personnel for the support of hundreds of computational scientists across the state of Nebraska.&lt;br/&gt;&lt;br/&gt;The proposal team is upgrading UNL's network capacity to 100 Gbps to Internet2 from the Great Plains Network in Kansas City to HCC's Lincoln data center. This provides greatly enhanced access to HCC's existing high performance computing as well as over a petabyte of high-energy physics data for U.S. CMS (Compact Muon Solenoid).&lt;br/&gt;&lt;br/&gt;One driver for increased network bandwidth is the "Any Data, Anytime, Anywhere" project, led at UNL, which enables unorganized data analysis to individual U.S. CMS physicists through remote data access. Additional areas of work benefiting from the improvement include: Brain Imaging and Analysis, Next Generation Sequencing, Traffic Engineering, the Open Science Grid and MobilityFirst Future Internet architecture. The project aligns well with other local network research; for example, allowing greater WAN bandwidth to be dedicated to projects such as Lark, a prior CC-NIE award focusing on integrating the network and computing layers. This also funds hardware improvements to the local Science DMZ for monitoring (perfSonar-PS) and security (Bro IDS).</AbstractNarration>
<MinAmdLetterDate>09/04/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/04/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1341031</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Swanson</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David R Swanson</PI_FULL_NAME>
<EmailAddress>dswanson4@unl.edu</EmailAddress>
<PI_PHON>4024725006</PI_PHON>
<NSF_ID>000385633</NSF_ID>
<StartDate>09/04/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Byravamurthy</FirstName>
<LastName>Ramamurthy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Byravamurthy Ramamurthy</PI_FULL_NAME>
<EmailAddress>byrav@cse.unl.edu</EmailAddress>
<PI_PHON>4024727791</PI_PHON>
<NSF_ID>000486807</NSF_ID>
<StartDate>09/04/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Bockelman</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian P Bockelman</PI_FULL_NAME>
<EmailAddress>bbockelman@morgridge.org</EmailAddress>
<PI_PHON>4027504235</PI_PHON>
<NSF_ID>000539290</NSF_ID>
<StartDate>09/04/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Askren</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Askren</PI_FULL_NAME>
<EmailAddress>mark.askren@unl.edu</EmailAddress>
<PI_PHON>4024721825</PI_PHON>
<NSF_ID>000644640</NSF_ID>
<StartDate>09/04/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName/>
<StateCode>NE</StateCode>
<ZipCode>685880430</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~491871</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project delivered a significant network capacity upgrade to the University of Nebraska-Lincoln; during the lifetime of the project, wide-area-network capacity to research computing resources increased tenfold, from 10Gbps to 100Gbps.</p> <p>This purchase provided support for the purchase of network cards and optics necessary for 100Gbps service. &nbsp;It performed a one-time defraying of connection fees costs for 100Gbps access to the nation's R&amp;D networks. &nbsp;It also provided for deployment of an "intrusion detection system" (IDS), viewed as critical to operating a secure high-speed network where firewalls are not pragmatic. &nbsp;Each were critical elements in the long-term campus cyberinfrastructure plan.</p> <p>Beyond direct hardware purchases, the project served as a "watering hole" between campus IT, computing center staff, and computer science researchers. &nbsp;This allowed notable progress to be made on supporting the next generation networking technology, IPv6, at a production level. &nbsp;The staff has also started to participate in national and worldwide network performance monitoring activities.</p> <p>Within the campus, the 100Gbps capacity was extended all the way to the Holland Computing Center (HCC). &nbsp;HCC provides the research computing infrastructure for the campuses of the University of Nebraska system. &nbsp;Hence, the 100Gbps link forms the backbone of Nebraska's "Science DMZ" - a dedicated network segment optimized for scientific workflows.</p> <p>All local users of research computing benefit from the upgraded network: even if they do not transfer large volumes of data, they do not encounter contention for access to Internet2 and other R&amp;D networks &nbsp;(e.g. see Incoming Flow Rate image). &nbsp;The project that enjoys the most direct benefit is the CMS experiment on the Large Hadron Collider (LHC). &nbsp;CMS daily utilizes more than 10Gbps of bandwidth, transferring multiple petabytes per year. &nbsp;This has allowed CMS to explore new data management models that trade increased WAN usage for local disk.</p> <p>Another illustration of the benefit of the increased network capacity delivered by this project is opportunistic users of the Open Science Grid (OSG). &nbsp;A prominent physics experiment, LIGO, was able to store data at the local grid storage resource and opportunistically use Nebraska network capacity to provide input for workflows running across the OSG (a national computing infrastructure). &nbsp;LIGO needed to sustain a transfer rate of 10Gbps; prior to the networking upgrade, this would have been 100% of the campus bandwidth. &nbsp;Thus, this project provided LIGO with the capability to run production, data intensive science on the OSG for the first time (e.g. see Outgoing Flow Rate image).</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/16/2015<br>      Modified by: David&nbsp;R&nbsp;Swanson</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/1341031/1341031_10274966_1449805816678_incomingFlow18Gbps--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/1341031/1341031_10274966_1449805816678_incomingFlow18Gbps--rgov-800width.jpg" title="Incoming Flow 18Gbps"><img src="/por/images/Reports/POR/2015/1341031/1341031_10274966_1449805816678_incomingFlow18Gbps--rgov-66x44.jpg" alt="Incoming Flow 18Gbps"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Aggregate incoming flow rate network graph for HC...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project delivered a significant network capacity upgrade to the University of Nebraska-Lincoln; during the lifetime of the project, wide-area-network capacity to research computing resources increased tenfold, from 10Gbps to 100Gbps.  This purchase provided support for the purchase of network cards and optics necessary for 100Gbps service.  It performed a one-time defraying of connection fees costs for 100Gbps access to the nation's R&amp;D networks.  It also provided for deployment of an "intrusion detection system" (IDS), viewed as critical to operating a secure high-speed network where firewalls are not pragmatic.  Each were critical elements in the long-term campus cyberinfrastructure plan.  Beyond direct hardware purchases, the project served as a "watering hole" between campus IT, computing center staff, and computer science researchers.  This allowed notable progress to be made on supporting the next generation networking technology, IPv6, at a production level.  The staff has also started to participate in national and worldwide network performance monitoring activities.  Within the campus, the 100Gbps capacity was extended all the way to the Holland Computing Center (HCC).  HCC provides the research computing infrastructure for the campuses of the University of Nebraska system.  Hence, the 100Gbps link forms the backbone of Nebraska's "Science DMZ" - a dedicated network segment optimized for scientific workflows.  All local users of research computing benefit from the upgraded network: even if they do not transfer large volumes of data, they do not encounter contention for access to Internet2 and other R&amp;D networks  (e.g. see Incoming Flow Rate image).  The project that enjoys the most direct benefit is the CMS experiment on the Large Hadron Collider (LHC).  CMS daily utilizes more than 10Gbps of bandwidth, transferring multiple petabytes per year.  This has allowed CMS to explore new data management models that trade increased WAN usage for local disk.  Another illustration of the benefit of the increased network capacity delivered by this project is opportunistic users of the Open Science Grid (OSG).  A prominent physics experiment, LIGO, was able to store data at the local grid storage resource and opportunistically use Nebraska network capacity to provide input for workflows running across the OSG (a national computing infrastructure).  LIGO needed to sustain a transfer rate of 10Gbps; prior to the networking upgrade, this would have been 100% of the campus bandwidth.  Thus, this project provided LIGO with the capability to run production, data intensive science on the OSG for the first time (e.g. see Outgoing Flow Rate image).          Last Modified: 12/16/2015       Submitted by: David R Swanson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
